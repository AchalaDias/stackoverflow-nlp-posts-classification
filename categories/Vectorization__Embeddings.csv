Title,Description,category,combined_text
How to understand hidden_states of the returns in BertModel?,"<blockquote>
<p>Returns last_hidden_state (torch.FloatTensor of shape (batch_size,
sequence_length, hidden_size)): Sequence of hidden-states at the
output of the last layer of the model.</p>
<p>pooler_output (torch.FloatTensor: of shape (batch_size, hidden_size)):
Last layer hidden-state of the first token of the sequence
(classification token) further processed by a Linear layer and a Tanh
activation function. The Linear layer weights are trained from the
next sentence prediction (classification) objective during
pre-training.</p>
<p>This output is usually not a good summary of the semantic content of
the input, you’re often better with averaging or pooling the sequence
of hidden-states for the whole input sequence.</p>
<p>hidden_states (tuple(torch.FloatTensor), optional, returned when
config.output_hidden_states=True): Tuple of torch.FloatTensor (one for
the output of the embeddings + one for the output of each layer) of
shape (batch_size, sequence_length, hidden_size).</p>
<p>Hidden-states of the model at the output of each layer plus the
initial embedding outputs.</p>
<p>attentions (tuple(torch.FloatTensor), optional, returned when
config.output_attentions=True): Tuple of torch.FloatTensor (one for
each layer) of shape (batch_size, num_heads, sequence_length,
sequence_length).</p>
<p>Attentions weights after the attention softmax, used to compute the
weighted average in the self-attention heads.</p>
</blockquote>
<p>This is from <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertmodel"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/bert.html#bertmodel</a>. Although the description in the document is clear, I still don't understand the <strong>hidden_states</strong> of returns. There is a tuple, one for the output of the embeddings, and the other for the output of each layer.</p>
<p>Please tell me how to distinguish them, or what is the meaning of them.</p>
",Vectorization & Embeddings,understand hidden state return bertmodel return last hidden state torch floattensor shape batch size sequence length hidden size sequence hidden state output last layer model pooler output torch floattensor shape batch size hidden size last layer hidden state first token sequence classification token processed linear layer tanh activation function linear layer weight trained next sentence prediction classification objective pre training output usually good summary semantic content input often better averaging pooling sequence hidden state whole input sequence hidden state tuple torch floattensor optional returned config output hidden state true tuple torch floattensor one output embeddings one output layer shape batch size sequence length hidden size hidden state model output layer plus initial embedding output attention tuple torch floattensor optional returned config output attention true tuple torch floattensor one layer shape batch size num head sequence length sequence length attention weight attention softmax used compute weighted average self attention head although description document clear still understand hidden state return tuple one output embeddings output layer please tell distinguish meaning
No attention output in jinaai/jina-embeddings-v3 embedding model,"<p>When I use this model like so -</p>
<pre><code>from transformers import AutoModel, AutoTokenizer

model_id = &quot;jinaai/jina-embeddings-v3&quot;
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModel.from_pretrained(model_id, trust_remote_code=True)

inputs = tokenizer([
    &quot;The weather is lovely today.&quot;,
    &quot;It's so sunny outside!&quot;,
    &quot;He drove to the stadium.&quot;
], return_tensors=&quot;pt&quot;, padding=True, truncation=True)

outputs = model(**inputs, output_attentions=True)

attentions = outputs.attentions
</code></pre>
<p>I get this warning which seems contradictory -</p>
<pre><code>flash_attn is not installed. Using PyTorch native attention implementation.
Flash attention implementation does not support kwargs: output_attentions
</code></pre>
<p>attentions is None</p>
<p>I tried it with other models and it works as expected.</p>
",Vectorization & Embeddings,attention output jinaai jina embeddings v embedding model use model like get warning seems contradictory attention none tried model work expected
How to replace an Embedding layer with a Time Distributed Dense after training?,"<p>I have the following problem:</p>

<ol>
<li><p>I want to use a LSTM network for text classification. In order to speed up training and make code more clear I want to use an <code>Embedding</code> layer along <code>keras.Tokenizer</code> in order to train my model.</p></li>
<li><p>Once I trained my model - I want to compute a saliency map of output w.r.t. the input. To do that I decided to replace an <code>Embedding</code> layer with a <code>TimeDistributedDense</code>.</p></li>
</ol>

<p>Do you have any idea what is the best way to do that. For a simple model I'm able to simply rebuild model with a known weights - but I want to make it as generic as possible - e.g. to replace the model structure the future and make my framework as model agnostic as possible.</p>
",Vectorization & Embeddings,replace embedding layer time distributed dense training following problem want use lstm network text classification order speed training make code clear want use layer along order train model trained model want compute saliency map output w r input decided replace layer idea best way simple model able simply rebuild model known weight want make generic possible e g replace model structure future make framework model agnostic possible
How to Identify Similar Code Parts Using CodeBERT Embeddings?,"<p>I'm using CodeBERT to compare how similar two pieces of code are. For example:</p>
<pre><code># Code 1
def calculate_area(radius):
return 3.14 * radius * radius
</code></pre>
<pre><code># Code 2
def compute_circle_area(r):
return 3.14159 * r * r
</code></pre>
<p>CodeBERT creates &quot;embeddings,&quot; which are like detailed descriptions of the code as numbers. I then compare these numerical descriptions to see how similar the codes are. This works well for telling me how much the codes are alike.</p>
<p>However, I can't tell which parts of the code CodeBERT thinks are similar. Because the &quot;embeddings&quot; are complex, I can't easily see what CodeBERT is focusing on. Comparing the code word-by-word doesn't work here.</p>
<p>My question is: How can I figure out which specific parts of two code snippets CodeBERT considers similar, beyond just getting a general similarity score?</p>
<p>I tried simple diff methods but that defeats the purpose of purely using CodeBERT.
I want to know if it's possible using CodeBERT alone.</p>
",Vectorization & Embeddings,identify similar code part using codebert embeddings using codebert compare similar two piece code example codebert creates embeddings like detailed description code number compare numerical description see similar code work well telling much code alike however tell part code codebert think similar embeddings complex easily see codebert focusing comparing code word word work question figure specific part two code snippet codebert considers similar beyond getting general similarity score tried simple diff method defeat purpose purely using codebert want know possible using codebert alone
Comparing the similarity of spoken and written form text,"<p>I'm converting spoken form text to its written form. For example, &quot;he owes me two-thousand dollars&quot; should be converted to &quot;he owes me $2,000&quot; . I want an automatic check, to judge if the conversion was right or not. Can i use sentence transformers to compare the embeddings of &quot;two-thousand dollars&quot; to &quot;$2,000&quot; to check if the spoken to written conversion was right? For example, if the cosine similarity of the embeddings is close to 1, that would mean right conversion. Is there any other better way to do this?</p>
",Vectorization & Embeddings,comparing similarity spoken written form text converting spoken form text written form example owes two thousand dollar converted owes want automatic check judge conversion wa right use sentence transformer compare embeddings two thousand dollar check spoken written conversion wa right example cosine similarity embeddings close would mean right conversion better way
Upserting in Pinecone takes too long,"<p>I'm trying to upsert reviews that i've scraped into pinecone. For the embedding model im using <code>jina-embedding-v3</code>. For 204 reviews this takes around <strong>2.5 hours!</strong> in Colab. Tried using GPU but the embeddings arent using GPU.
Am i doing something wrong? Is there a way that i can speed up the process? The code is below:</p>
<p>Initialising DB:</p>
<pre><code>if index_name not in pc.list_indexes().names():
  pc.create_index(
    name=index_name,
    dimension=1024,
    metric=&quot;cosine&quot;,
    spec=ServerlessSpec(
        cloud=&quot;aws&quot;,
        region=&quot;us-east-1&quot;
    )
)
</code></pre>
<p>Embedding &amp; Upserting:</p>
<pre><code>device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Load the Jina embedding model and tokenizer from Hugging Face
model_name = &quot;jinaai/jina-embeddings-v3&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, trust_remote_code=True)

from langchain.text_splitter import SpacyTextSplitter
text_splitter = SpacyTextSplitter(chunk_size=500)

# Function to generate embeddings
def generate_embeddings(text, task='retrieval.passage'):
    return model.encode(text, convert_to_tensor=True, task=task).numpy()

for review_id, review in enumerate(all_reviews[:2]):
    chunks = text_splitter.split_text(review)

    for chunk_index, chunk in enumerate(chunks):
        embedding = generate_embeddings(chunk)

        unique_id = f&quot;{review_id}_{chunk_index}&quot;

        metadata = {&quot;review_id&quot;: review_id, &quot;chunk_index&quot;: chunk_index, &quot;text&quot;: chunk}

        index.upsert([(unique_id, embedding, metadata)])

# Generate and store embeddings
for review_id, review in enumerate(all_reviews):
    chunks = text_splitter.split_text(review)

    for chunk_index, chunk in enumerate(chunks):
        embedding = generate_embeddings(chunk)

        unique_id = f&quot;{review_id}_{chunk_index}&quot;

        metadata = {&quot;review_id&quot;: review_id, &quot;chunk_index&quot;: chunk_index, &quot;text&quot;: chunk}

        index.upsert([(unique_id, embedding, metadata)])
</code></pre>
",Vectorization & Embeddings,upserting pinecone take long trying upsert review scraped pinecone embedding model im using review take around hour colab tried using gpu embeddings arent using gpu something wrong way speed process code initialising db embedding upserting
TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0] while using RF classifier?,"<p>I am learning about random forests in scikit learn and as an example I would like to use Random forest classifier for text classification, with my own dataset. So first I vectorized the text with tfidf and for classification:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier(n_estimators=10) 
classifier.fit(X_train, y_train)           
prediction = classifier.predict(X_test)
</code></pre>

<p>When I run the classification I got this:</p>

<pre><code>TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.
</code></pre>

<p>then I used the <code>.toarray()</code> for <code>X_train</code> and I got the following:</p>

<pre><code>TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]
</code></pre>

<p>From a previous <a href=""https://stackoverflow.com/questions/21689141/classifying-text-documents-with-random-forests"">question</a> as I understood I need to reduce the dimensionality of the numpy array so I do the same:</p>

<pre><code>from sklearn.decomposition.truncated_svd import TruncatedSVD        
pca = TruncatedSVD(n_components=300)                                
X_reduced_train = pca.fit_transform(X_train)               

from sklearn.ensemble import RandomForestClassifier                 
classifier=RandomForestClassifier(n_estimators=10)                  
classifier.fit(X_reduced_train, y_train)                            
prediction = classifier.predict(X_testing) 
</code></pre>

<p>Then I got this exception:</p>

<pre><code>  File ""/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py"", line 419, in predict
    n_samples = len(X)
  File ""/usr/local/lib/python2.7/site-packages/scipy/sparse/base.py"", line 192, in __len__
    raise TypeError(""sparse matrix length is ambiguous; use getnnz()""
TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]
</code></pre>

<p>The I tried the following:</p>

<pre><code>prediction = classifier.predict(X_train.getnnz()) 
</code></pre>

<p>And got this:</p>

<pre><code>  File ""/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py"", line 419, in predict
    n_samples = len(X)
TypeError: object of type 'int' has no len()
</code></pre>

<p>Two questions were raised from this: How can I use Random forests to classify correctly? and what's happening with <code>X_train</code>?. </p>

<p>Then I tried the following:</p>

<pre><code>df = pd.read_csv('/path/file.csv',
header=0, sep=',', names=['id', 'text', 'label'])



X = tfidf_vect.fit_transform(df['text'].values)
y = df['label'].values



from sklearn.decomposition.truncated_svd import TruncatedSVD
pca = TruncatedSVD(n_components=2)
X = pca.fit_transform(X)

a_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.33, random_state=42)

from sklearn.ensemble import RandomForestClassifier

classifier=RandomForestClassifier(n_estimators=10)
classifier.fit(a_train, b_train)
prediction = classifier.predict(a_test)

from sklearn.metrics.metrics import precision_score, recall_score, confusion_matrix, classification_report
print '\nscore:', classifier.score(a_train, b_test)
print '\nprecision:', precision_score(b_test, prediction)
print '\nrecall:', recall_score(b_test, prediction)
print '\n confussion matrix:\n',confusion_matrix(b_test, prediction)
print '\n clasification report:\n', classification_report(b_test, prediction)
</code></pre>
",Vectorization & Embeddings,typeerror sparse matrix length ambiguous use getnnz shape using rf classifier learning random forest scikit learn example would like use random forest classifier text classification dataset first vectorized text tfidf classification run classification got used got following previous href understood need reduce dimensionality numpy array p got exception tried following got two question raised use random forest classify correctly happening tried following
semantic similarity between sentences,"<p>I'm doing a project. I need any opensource tool or technique to find the semantic similarity of two sentences, where I give two sentences as an input, and receive score (i.e.,semantic similarity) as an output. Any help?</p>
",Vectorization & Embeddings,semantic similarity sentence project need opensource tool technique find semantic similarity two sentence give two sentence input receive score e semantic similarity output help
Understanding word2vec text representation,"<p>I would like to implement the <code>distance</code> part of word2vec in my program. Unfortunately it's not in C/C++ or Python, but first I did not understand the non-binary representation. This is how I get the file
<code>./word2vec -train text8-phrase -output vectorsphrase.txt -cbow 0 -size 300 -window 10 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 0</code></p>

<p>When I check vectorsphrase.txt file for france all I get is:</p>

<pre><code> france -0.062591 0.264201 0.236335 -0.072601 -0.094313 -0.202659 -0.373314 0.074684 -0.262307 0.139383 -0.053648 -0.154181 0.126962 0.432593 -0.039440 0.108096 0.083703 0.148991 0.062826 0.048151 0.005555 0.066885 0.004729 -0.013939 -0.043947 0.057280 -0.005259 -0.223302 0.065608 -0.013932 -0.199372 -0.054966 -0.026725 0.012510 0.076350 -0.027816 -0.187357 0.248191 -0.085087 0.172979 -0.116789 0.014136 0.131571 0.173892 0.316052 -0.045492 0.057584 0.028944 -0.193623 0.043965 -0.166696 0.111058 0.145268 -0.119645 0.091659 0.056593 0.417965 -0.002927 -0.081579 -0.021356 0.030447 0.052507 -0.109058 -0.011124 -0.136975 0.104396 0.069319 0.030266 -0.193283 -0.024614 -0.025636 -0.100761 0.032366 0.069175 0.200204 -0.042976 -0.045123 -0.090475 0.090071 -0.037075 0.182373 0.151529 0.080198 -0.024067 -0.196623 -0.204863 0.154429 -0.190242 -0.063265 -0.323000 -0.109863 0.102366 -0.085017 0.198042 -0.033342 0.119225 0.176891 0.214628 0.031771 0.168739 0.063246 -0.147353 -0.003526 0.138835 -0.172460 -0.133294 -0.369451 0.063572 0.076098 -0.116277 0.208374 0.015783 0.145032 0.090530 -0.090470 0.109325 0.119184 0.024838 0.101194 -0.184154 -0.161648 -0.039929 0.079321 0.029462 -0.016193 -0.005485 0.197576 -0.118860 0.019042 -0.137174 -0.047933 -0.008472 0.092360 0.165395 0.013253 -0.099013 -0.017355 -0.048332 -0.077228 0.034320 -0.067505 -0.050190 -0.320440 -0.040684 -0.106958 -0.169634 -0.014216 0.225693 0.345064 0.135220 -0.181518 -0.035400 -0.095907 -0.084446 0.025784 0.090736 -0.150824 -0.351817 0.174671 0.091944 -0.112423 -0.140281 0.059532 0.002152 0.127812 0.090834 -0.130366 -0.061899 -0.280557 0.076417 -0.065455 0.205525 0.081539 0.108110 0.013989 0.133481 -0.256035 -0.135460 0.127465 0.113008 0.176893 -0.018049 0.062527 0.093005 -0.078622 -0.109232 0.065856 0.138583 0.097186 -0.124459 0.011706 0.113376 0.024505 -0.147662 -0.118035 0.129616 0.114539 0.165050 -0.134871 -0.036298 -0.103552 -0.108726 0.025950 0.053895 -0.173731 0.201482 -0.198697 -0.339452 0.166154 -0.014059 0.022529 0.212491 -0.051978 0.057627 0.198319 0.092990 -0.171989 -0.060376 0.084172 -0.034411 -0.065443 0.054581 -0.024187 0.072550 0.113017 0.080476 -0.170499 0.148091 -0.010503 0.158095 0.111080 0.007598 0.042551 -0.161005 -0.078712 0.318305 -0.011473 0.065593 0.121385 0.087249 -0.011178 0.053639 -0.100713 0.168689 0.120121 -0.058025 -0.161788 -0.101135 -0.080533 0.120502 -0.099477 0.187640 -0.054496 0.180532 -0.097961 0.049633 -0.019596 0.145623 0.284261 0.039761 0.053866 0.089779 -0.000676 -0.081653 0.082819 0.263937 -0.141818 0.011605 -0.028248 -0.020573 0.091329 -0.080264 -0.358647 -0.134252 0.115414 -0.066107 0.150770 -0.018897 0.168325 0.111375 -0.091567 -0.152783 -0.034834 -0.418656 -0.091504 -0.134671 0.051754 -0.129495 0.230855 -0.339259 0.208410 0.191621 0.007837 -0.016602 -0.131502 -0.059481 -0.185196 0.303028 0.017646 -0.047340 
</code></pre>

<p>So apart from cosine values I don't get anything else, and when I run the distance and type france I get </p>

<h2>               Word       Cosine distance</h2>

<pre><code>            spain              0.678515
          belgium              0.665923
      netherlands              0.652428
            italy              0.633130
      switzerland              0.622323
       luxembourg              0.610033
         portugal              0.577154
           russia              0.571507
          germany              0.563291
        catalonia              0.534176
</code></pre>

<p>So, from the given probabilities, how do I link it to other words, and how do I know which one belongs to which?</p>
",Vectorization & Embeddings,understanding word vec text representation would like implement part word vec program unfortunately c c python first understand non binary representation get file check vectorsphrase txt file france get apart cosine value get anything else run distance type france get word cosine distance given probability link word know one belongs
What is the concept of negative-sampling in word2vec?,"<p>I'm reading the 2014 paper <em><a href=""https://arxiv.org/pdf/1402.3722v1.pdf"" rel=""nofollow noreferrer"">word2vec Explained: Deriving Mikolov et al.’s
Negative-Sampling Word-Embedding Method</a></em> (note: direct download link) and it references the concept of &quot;negative-sampling&quot;:</p>
<blockquote>
<p>Mikolov et al. present the negative-sampling approach as a more efficient
way of deriving word embeddings. While negative-sampling is based on the
skip-gram model, it is in fact optimizing a different objective.</p>
</blockquote>
<p>I have some issue understanding the concept of negative-sampling.</p>
<p><a href=""https://arxiv.org/pdf/1402.3722v1.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1402.3722v1.pdf</a></p>
<p>Can anyone explain in layman's terms what negative-sampling is?</p>
",Vectorization & Embeddings,concept negative sampling word vec reading paper word vec explained deriving mikolov et al negative sampling word embedding method note direct download link reference concept negative sampling mikolov et al present negative sampling approach efficient way deriving word embeddings negative sampling based skip gram model fact optimizing different objective issue understanding concept negative sampling anyone explain layman term negative sampling
Does Word2Vec has a hidden layer?,"<p>When I am reading one of papers of Tomas Mikolov: <a href=""http://arxiv.org/pdf/1301.3781.pdf"" rel=""nofollow noreferrer"">http://arxiv.org/pdf/1301.3781.pdf</a></p>

<p>I have one concern on the Continuous Bag-of-Words Model section：</p>

<blockquote>
  <p>The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged).</p>
</blockquote>

<p>I find some people mention that there is a hidden layer in Word2Vec model, but from my understanding, there is only one projection layer in that model. Does this projection layer do the same work as hidden layer?</p>

<p>The another question is that how to project input data into the projection layer? </p>

<p>""the projection layer is shared for all words (not just the projection matrix)"", what does that mean?</p>
",Vectorization & Embeddings,doe word vec ha hidden layer reading one paper tomas mikolov one concern continuous bag word model section first proposed architecture similar feedforward nnlm non linear hidden layer removed projection layer shared word projection matrix thus word get projected position vector averaged find people mention hidden layer word vec model understanding one projection layer model doe projection layer work hidden layer another question project input data projection layer projection layer shared word projection matrix doe mean
Understanding Word2Vec&#39;s Skip-Gram Structure and Output,"<p>I have a two-fold question about the Skip-Gram model in Word2Vec:</p>
<ul>
<li><p>The first part is about structure: as far as I understand it, the Skip-Gram model is based on one neural network with one input weight matrix <strong>W</strong>, one hidden layer of size N, and C output weight matrices <strong>W'</strong> each used to produce one of the C output vectors. Is this correct?</p>
</li>
<li><p>The second part is about the output vectors: as far as I understand it, each output vector is of size V and is a result of a Softmax function. Each output vector <em>node</em> corresponds to the index of a word in the vocabulary, and the value of each node is the probability that the corresponding word occurs at that context location (for a given input word). The target output vectors are not, however, one-hot encoded, even if the training instances are. Is this correct?</p>
</li>
</ul>
<p>The way I imagine it is something along the following lines (made-up example):</p>
<p>Assuming the vocabulary ['quick', 'fox', 'jumped', 'lazy', 'dog'] and a context of C=1, and assuming that for the input word 'jumped' I see the two output vectors looking like this:</p>
<p>[0.2 <strong>0.6</strong> 0.01 0.1 0.09]</p>
<p>[0.2 0.2 0.01 0.16 <strong>0.43</strong>]</p>
<p>I would interpret this as 'fox' being the most likely word to show up before 'jumped' (p=0.6), and 'dog' being the most likely to show up after it (p=0.43).</p>
<p>Do I have this right? Or am I completely off?</p>
",Vectorization & Embeddings,understanding word vec skip gram structure output two fold question skip gram model word vec first part structure far understand skip gram model based one neural network one input weight matrix w one hidden layer size n c output weight matrix w used produce one c output vector correct second part output vector far understand output vector size v result softmax function output vector node corresponds index word vocabulary value node probability corresponding word occurs context location given input word target output vector however one hot encoded even training instance correct way imagine something along following line made example assuming vocabulary quick fox jumped lazy dog context c assuming input word jumped see two output vector looking like would interpret fox likely word show jumped p dog likely show p right completely
what actually word embedding dimensions values represent?,"<p>I have a doubt in word2vec and word embedding, I have downloaded GloVe pre-trained word embedding (shape 40,000 x 50)  and using this function to extract information from that:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
def loadGloveModel(gloveFile):
    print (&quot;Loading Glove Model&quot;)
    f = open(gloveFile,'r')
    model = {}
    for line in f:
        splitLine = line.split()
        word = splitLine[0]
        embedding = np.array([float(val) for val in splitLine[1:]])
        model[word] = embedding
    print (&quot;Done.&quot;,len(model),&quot; words loaded!&quot;)
    return model
</code></pre>
<p>Now if I call this function for word <code>'python'</code>, something like:</p>
<pre class=""lang-py prettyprint-override""><code>print(loadGloveModel('glove.6B.100d.txt')['python'])
</code></pre>
<p>it gives me 1x50 shape vector like this:</p>
<pre><code>[ 0.24934    0.68318   -0.044711  -1.3842    -0.0073079  0.651
 -0.33958   -0.19785   -0.33925    0.26691   -0.033062   0.15915
  0.89547    0.53999   -0.55817    0.46245    0.36722    0.1889
  0.83189    0.81421   -0.11835   -0.53463    0.24158   -0.038864
  1.1907     0.79353   -0.12308    0.6642    -0.77619   -0.45713
 -1.054     -0.20557   -0.13296    0.12239    0.88458    1.024
  0.32288    0.82105   -0.069367   0.024211  -0.51418    0.8727
  0.25759    0.91526   -0.64221    0.041159  -0.60208    0.54631
  0.66076    0.19796   -1.1393     0.79514    0.45966   -0.18463
 -0.64131   -0.24929   -0.40194   -0.50786    0.80579    0.53365
  0.52732    0.39247   -0.29884    0.009585   0.99953   -0.061279
  0.71936    0.32901   -0.052772   0.67135   -0.80251   -0.25789
  0.49615    0.48081   -0.68403   -0.012239   0.048201   0.29461
  0.20614    0.33556   -0.64167   -0.64708    0.13377   -0.12574
 -0.46382    1.3878     0.95636   -0.067869  -0.0017411  0.52965
  0.45668    0.61041   -0.11514    0.42627    0.17342   -0.7995
 -0.24502   -0.60886   -0.38469   -0.4797   ]
</code></pre>
<p>I need help in understanding the output matrix. What does these value represent and there significance in generating new word?</p>
",Vectorization & Embeddings,actually word embedding dimension value represent doubt word vec word embedding downloaded glove pre trained word embedding shape x using function extract information call function word something like give x shape vector like need help understanding output matrix doe value represent significance generating new word
Why does word2vec use 2 representations for each word?,"<p>I am trying to understand why word2vec's skipgram model has 2 representations for each word (the hidden representation which is the word embedding) and the output representation (also called context word embedding) . Is this just for generality where the context can be anything (not just words) or is there a more fundamental reason </p>
",Vectorization & Embeddings,doe word vec use representation word trying understand word vec skipgram model ha representation word hidden representation word embedding output representation also called context word embedding generality context anything word fundamental reason
How to add index to python FAISS incrementally,"<p>I am using Faiss to index my huge dataset embeddings, embedding generated from bert model. I want to add the embeddings incrementally, it is working fine if I only add it with faiss.IndexFlatL2 , but the problem is while saving it the size of it is too large.
So I tried with faiss.IndexIVFPQ, but it needs to train embeddings before I add the data, so I can not add it incrementally, I have to compute all embeddings first and then train and add it, it is having issue because all the data should be kept in RAM till I write it. Is there any way to do this incrementally.
Here is my code:</p>
<pre><code>    # It is working fine when using with IndexFlatL2
    def __init__(self, sentences, model):
        self.sentences = sentences
        self.model = model
        self.index = faiss.IndexFlatL2(768)

    def process_sentences(self):
        result = self.model(self.sentences)
        self.sentence_ids = []
        self.token_ids = []
        self.all_tokens = []
        for i, (toks, embs) in enumerate(tqdm(result)):
            # initialize all_embeddings for every new sentence (INCREMENTALLY)
            all_embeddings = []
            for j, (tok, emb) in enumerate(zip(toks, embs)):
                self.sentence_ids.append(i)
                self.token_ids.append(j)
                self.all_tokens.append(tok)
                all_embeddings.append(emb)

            all_embeddings = np.stack(all_embeddings) # Add embeddings after every sentence
            self.index.add(all_embeddings)

        faiss.write_index(self.index, &quot;faiss_Model&quot;)
</code></pre>
<p>ANd when using with IndexIVFPQ:</p>
<pre><code>   def __init__(self, sentences, model):
       self.sentences = sentences
       self.model = model
       self.quantizer = faiss.IndexFlatL2(768)
       self.index = faiss.IndexIVFPQ(self.quantizer, 768, 1000, 16, 8)

   def process_sentences(self):
       result = self.model(self.sentences)
       self.sentence_ids = []
       self.token_ids = []
       self.all_tokens = []
       all_embeddings = []
       for i, (toks, embs) in enumerate(tqdm(result)):
           for j, (tok, emb) in enumerate(zip(toks, embs)):
               self.sentence_ids.append(i)
               self.token_ids.append(j)
               self.all_tokens.append(tok)
               all_embeddings.append(emb)

       all_embeddings = np.stack(all_embeddings)
       self.index.train(all_embeddings) # Train
       self.index.add(all_embeddings) # Add to index
       faiss.write_index(self.index, &quot;faiss_Model_mini&quot;)
</code></pre>
",Vectorization & Embeddings,add index python faiss incrementally using faiss index huge dataset embeddings embedding generated bert model want add embeddings incrementally working fine add faiss indexflatl problem saving size large tried faiss indexivfpq need train embeddings add data add incrementally compute embeddings first train add issue data kept ram till write way incrementally code using indexivfpq
Using an AWS service to execute a python script that will extract keywords from text using keyBERT?,"<p>I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords.</p>
<p>Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.</p>
",Vectorization & Embeddings,using aws service execute python script extract keywords text using keybert simple python script given two block text extract keywords using keybert compare list keywords sort two list depending list share keywords aws service would best fit need want able esentially spin needed give block text execute return result want integrate project use python attempted use lambda concerned potential cost running thanks
NoneType&#39; object is not iterable for Vectorizer sklearn,"<p>I have imported text data into pandas dataframe. I would like to implement Vectorizer. So i use sklearn to do tfidf and so on  </p>

<p>So the first step i did. clean the text. </p>

<pre><code>#Creating Function
from nltk.corpus import stopwords
def text_process(sms):  
nonpunc = [char for char in sms if char not in string.punctuation]
nonpunc = ''.join(nonpunc)
return[word for word in nonpunc.split() if word.lower() not in stopwords.words('english')]
</code></pre>

<p>Next</p>

<pre><code>data['sms'].head(5).apply(text_process)
</code></pre>

<p>Next</p>

<pre><code>from sklearn.feature_extraction.text import  CountVectorizer
bow_transformer = CountVectorizer(analyzer = text_process).fit(data['sms'])
</code></pre>

<p>I receive an error. </p>

<pre><code>  ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-84-f1812582c7e1&gt; in &lt;module&gt;
      1 #Step 1
      2 from sklearn.feature_extraction.text import  CountVectorizer
----&gt; 3 bow_transformer = CountVectorizer(analyzer = text_process).fit(data['sms'])

~\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py in fit(self, raw_documents, y)
    976         self
    977         """"""
--&gt; 978         self.fit_transform(raw_documents)
    979         return self
    980 

~\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py in fit_transform(self, raw_documents, y)
   1010 
   1011         vocabulary, X = self._count_vocab(raw_documents,
-&gt; 1012                                           self.fixed_vocabulary_)
   1013 
   1014         if self.binary:

~\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py in _count_vocab(self, raw_documents, fixed_vocab)
    920         for doc in raw_documents:
    921             feature_counter = {}
--&gt; 922             for feature in analyze(doc):
    923                 try:
    924                     feature_idx = vocabulary[feature]

&lt;ipython-input-82-4149ae75d7bf&gt; in text_process(sms)
      3 def text_process(sms):
      4 
----&gt; 5     nonpunc = [char for char in sms if char not in string.punctuation]
      6     nonpunc = ''.join(nonpunc)
      7     return[word for word in nonpunc.split() if word.lower() not in stopwords.words('english')]

TypeError: 'NoneType' object is not iterable
</code></pre>
",Vectorization & Embeddings,nonetype object iterable vectorizer sklearn imported text data panda dataframe would like implement vectorizer use sklearn tfidf first step clean text next next receive error
Is it possible to evaluate Machine Translations using Sentence BERT?,"<p>I'm not referring to <a href=""https://arxiv.org/abs/1904.09675"" rel=""nofollow noreferrer"">BERTScore</a>. BERTScore uses token-level word embeddings, you compute pairwise cosine similarity of word embeddings and obtain scores using greedy matching.</p>
<p>I'm referring to <a href=""https://arxiv.org/abs/1908.10084"" rel=""nofollow noreferrer"">Sentence BERT</a>. I.e., pure cosine similarity to compare semantic similarity of sentences, not precision, recall or f1-measure. <strong>The question is, if we do this on document level, i.e., several sentences, do we then just compute the mean cosine similarity or is this metric not suitable as a machine translation evaluation metric (alternative to BLEU)?</strong></p>
<p>Because for individual sentences, it does make sense as it capture semantic similarity. Sentences that mean the same thing but are phrased differently get penalized by BLEU but get rather high values with Sentence BERT, which is exactly what I want. However, I could not find the use of Sentence BERT in recent WMT Shared Metric Task papers, so I assume there is a catch I am missing which explains why people do not use this approach.</p>
",Vectorization & Embeddings,possible evaluate machine translation using sentence bert referring bertscore bertscore us token level word embeddings compute pairwise cosine similarity word embeddings obtain score using greedy matching referring sentence bert e pure cosine similarity compare semantic similarity sentence precision recall f measure question document level e several sentence compute mean cosine similarity metric suitable machine translation evaluation metric alternative bleu individual sentence doe make sense capture semantic similarity sentence mean thing phrased differently get penalized bleu get rather high value sentence bert exactly want however could find use sentence bert recent wmt shared metric task paper assume catch missing explains people use approach
Why is there a gap between the rotary embedding in LLaMA and my custom implementation?,"<p>I’m encountering an issue when applying rotary position encoding outside the LLaMA model in the Transformers library (version 4.46.0).
The problem arises in the following code block:</p>
<pre><code>from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, apply_rotary_pos_emb

# q_unrope/k_unrope/q/k are saved inside the model

rotary_emb = LlamaRotaryEmbedding(config=config, device=device)
rotary_emb.training = False

bsz, head_num, seq_len, head_dim = k.shape
position_ids = torch.arange(seq_len).unsqueeze(0).expand(bsz, seq_len).to(device)

cos, sin = rotary_emb(k, position_ids)

q_rope, k_rope = apply_rotary_pos_emb(q_unrope, k_unrope, cos, sin)

print(f&quot;{torch.max(torch.abs(k_rope - k))=}&quot;)
print(f&quot;{torch.max(torch.abs(q_rope - q))=}&quot;)
</code></pre>
<p>I noticed a gap between the q_rope computed externally and the roped q inside the LLaMA model:</p>
<pre><code>torch.max(torch.abs(k_rope - k)) = tensor(0.0039, device='cuda:0', dtype=torch.float16)
torch.max(torch.abs(q_rope - q)) = tensor(0.0078, device='cuda:0', dtype=torch.float16)
</code></pre>
<p>However, when I save and reload the rotary_emb from the model and use it instead of initializing with</p>
<pre><code>rotary_emb = LlamaRotaryEmbedding(config=config, device=device)
</code></pre>
<p>the gap reduces to zero.</p>
<p>What might be causing this discrepancy? Any help or insights would be greatly appreciated. Thank you!</p>
",Vectorization & Embeddings,gap rotary embedding llama custom implementation encountering issue applying rotary position encoding outside llama model transformer library version problem arises following code block noticed gap q rope computed externally roped q inside llama model however save reload rotary emb model use instead initializing gap reduces zero might causing discrepancy help insight would greatly appreciated thank
Problem in tqdm function in a Doc2Vec model,"<p>I am using this article <a href=""https://actsusanli.medium.com/"" rel=""nofollow noreferrer"">https://actsusanli.medium.com/</a> to implement the Doc2Vec model and I have a problem in the training step.</p>
<pre><code>model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs = 40)
</code></pre>
<p>As you can see, I am using the tqdm function. When I ran the code the tqdm is 100%, after some minutes, but the algorithm still runs in the same shell for a long time.</p>
<p><strong>Do you have any idea if this is a problem of tqdm function or something else?</strong></p>
",Vectorization & Embeddings,problem tqdm function doc vec model using article implement doc vec model problem training step see using tqdm function ran code tqdm minute algorithm still run shell long time idea problem tqdm function something else
What&#39;s the major difference between glove and word2vec?,"<p>What is the difference between word2vec and glove? 
Are both the ways to train a word embedding? if yes then how can we use both?</p>
",Vectorization & Embeddings,major difference glove word vec difference word vec glove way train word embedding yes use
similarity from word to sentence after doing words Embedding,"<p>I have dataframe with 1000 text rows.</p>
<p>I did word2vec .</p>
<p>Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word &quot;king&quot;.</p>
<p>I thought about taking in each sentence the 4 closet words to the word king  and make average of them.
maybe by using <code>model.wv.similarity</code>.
the avg of each sentnce will be in the field df['king']</p>
<p>I will glad to know how to do that or to hear about another method.</p>
<p>example data:</p>
<pre><code>    data = {
    'text': [
        &quot;The king sat on the throne with wisdom.&quot;,
        &quot;A queen ruled the kingdom alongside the king.&quot;,
        &quot;Knights were loyal to their king.&quot;,
        &quot;The empire prospered under the rule of a wise monarch.&quot;
    ]
}
df = pd.DataFrame(data)
df['text']=df['text'].str.split()    

model = Word2Vec(df['text'], vector_size=100, window=2, min_count=1 )

model.wv.similarity('Knights','king')
</code></pre>
<p><strong>edit</strong>:</p>
<p>My mission is:</p>
<p>I have 1000 text rows (people that complain about something)
I want to catalog them into 4 words.
Lets say that word 1 is king. Word 2 is castle…
I want to know about each sentence which word from the  4 words most represent the sentence.
In order to do that I thought about taking each word from the 4 words and calculate <code>model.wv.similarity</code> to all of the words in  df['text'].
After that, for each sentence, take the 3  words that have  the highest score to word king  (and to the word  castle and ets..)  .
calculate mean of the 3 highest score and that would be the value of df['king'] for the sentence</p>
",Vectorization & Embeddings,similarity word sentence word embedding dataframe text row word vec want create new field give distance sentence word want let say word king thought taking sentence closet word word king make average maybe using avg sentnce field df king glad know hear another method example data edit mission text row people complain something want catalog word let say word king word castle want know sentence word word represent sentence order thought taking word word calculate word df text sentence take word highest score word king word castle ets calculate mean highest score would value df king sentence
Langchain Chatbot with Memory + Vector Database,"<p>In Langchain, what is the suggested way to build a chatbot with memory and retrieval from a vector embedding database at the same time?</p>
<p>The examples in the <a href=""https://python.langchain.com/en/latest/modules/memory/how_to_guides.html"" rel=""nofollow noreferrer"">docs</a> add memory modules to chains that do not have a vector database. Related <a href=""https://github.com/hwchase17/langchain/issues/2185"" rel=""nofollow noreferrer"">issue</a>.</p>
",Vectorization & Embeddings,langchain chatbot memory vector database langchain suggested way build chatbot memory retrieval vector embedding database time example doc add memory module chain vector database related issue
Normalization of token embeddings in BERT encoder blocks,"<p>Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?</p>
",Vectorization & Embeddings,normalization token embeddings bert encoder block following multi headed attention layer bert encoder block layer normalization done separately embedding token e one mean variance per token embedding concatenated vector token embeddings mean variance embeddings
Get attention masks from HF pipelines,"<p>How should returned attention masks be accessed from the FeatureExtractionPipeline in Huggingface?</p>
<p>The code below takes an embedding model, distributes it and a huggingface dataset across 8 GPUs on a single node, and performs inference on the inputs. The code requires the attention masks for mean pooling.</p>
<p>Code example:</p>
<pre class=""lang-py prettyprint-override""><code>from accelerate import Accelerator
from accelerate.utils import tqdm
from transformers import AutoTokenizer, AutoModel
from optimum.bettertransformer import BetterTransformer

import torch

from datasets import load_dataset

from transformers import pipeline

accelerator = Accelerator()

model_name = &quot;BAAI/bge-large-en-v1.5&quot;

tokenizer = AutoTokenizer.from_pretrained(model_name,)

model = AutoModel.from_pretrained(model_name,)

pipe = pipeline(
    &quot;feature-extraction&quot;,
    model=model,
    tokenizer=tokenizer,
    max_length=512,
    truncation=True,
    padding=True,
    pad_to_max_length=True,
    batch_size=256,
    framework=&quot;pt&quot;,
    return_tensors=True,
    return_attention_mask=True,
    device=(accelerator.device)
)

dataset = load_dataset(
    &quot;wikitext&quot;,
    &quot;wikitext-2-v1&quot;,
    split=&quot;train&quot;,
)

#Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0] #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


# Assume 8 processes

with accelerator.split_between_processes(dataset[&quot;text&quot;]) as data:

    for out in pipe(data):

        sentence_embeddings = mean_pooling(out, out[&quot;attention_mask&quot;])
</code></pre>
<p>I need the attention maks from pipe to use for mean pooling.</p>
<p>Best,</p>
<p>Enrico</p>
",Vectorization & Embeddings,get attention mask hf pipeline returned attention mask accessed featureextractionpipeline huggingface code take embedding model distributes huggingface dataset across gpus single node performs inference input code requires attention mask mean pooling code example need attention mak pipe use mean pooling best enrico
CBOW and Skip-Gram learning perspective,"<p>I want to know if I am having a correct understanding of those methods</p>
<p>In CBOW we use nearby words to predict the target word whereas in skip gram we use target word to predict the nearby words,so does that mean that in CBOW we are creating embedding based on what words come often together like &quot;I&quot; will have &quot;am&quot; closely mapped in the dimensional space ,whereas embedding in Skip gram it is more about the meaning of that word like what &quot;am&quot; is used for . Am I correct?</p>
",Vectorization & Embeddings,cbow skip gram learning perspective want know correct understanding method cbow use nearby word predict target word whereas skip gram use target word predict nearby word doe mean cbow creating embedding based word come often together like closely mapped dimensional space whereas embedding skip gram meaning word like used correct
Haystack: save InMemoryDocumentStore and load it in retriever later to save embedding generation time,"<p>I am using InMemory Document Store and an Embedding retriever for the Q/A pipeline.</p>
<pre><code>from haystack.document_stores import InMemoryDocumentStore
document_store = InMemoryDocumentStore(embedding_dim =768,use_bm25=True) 
document_store.write_documents(docs_processed)
     
from haystack.nodes import EmbeddingRetriever
retriever_model_path ='downloaded_models\local\my_local_multi-qa-mpnet-base-dot-v1'
retriever = EmbeddingRetriever(document_store=document_store,
                              embedding_model=retriever_model_path,
                              use_gpu=True)

document_store.update_embeddings(retriever=retriever)
</code></pre>
<p>As the embedding takes a while, I want to load the embeddings and later use them again in the retriever. (in rest API side). I don't want to use ElasticSearch or Faiss. How can I achieve this using In Memory Store? I tried to use Pickle, but there is no way to store the embeddings. Again, in the embedding retriever, there is no load function.</p>
<p>I tried to do the following:</p>
<pre><code>with open(&quot;document_store_res.pkl&quot;, &quot;wb&quot;) as f:
    pickle.dump(document_store.get_all_documents(), f)
</code></pre>
<p>And in the rest API, I am trying to load the document store :</p>
<pre><code>def reader_retriever():
# Load the pickled model        
        with open(os.path.join(settings.BASE_DIR,'\downloaded_models\document_store_res.pkl'), 'rb') as f:
            document_store_new = pickle.load(f)

            retriever_model_path = os.path.join(settings.BASE_DIR, '\downloaded_models\my_local_multi-qa-mpnet-base-dot-v1')

            retriever = EmbeddingRetriever(document_store=document_store_new,
                               embedding_model=retriever_model_path,
                               use_gpu=True)

            document_store_new.update_embeddings(retriever=retriever,
                                batch_size=100)
            farm_reader_path = os.path.join(settings.BASE_DIR, '\downloaded_models\my_local_bert-large-uncased-whole-word-masking-squad2')

            reader = FARMReader(model_name_or_path=farm_reader_path,
                                    use_gpu=True)
            

            return reader, retriever
</code></pre>
",Vectorization & Embeddings,haystack save inmemorydocumentstore load retriever later save embedding generation time using inmemory document store embedding retriever q pipeline embedding take want load embeddings later use retriever rest api side want use elasticsearch faiss achieve using memory store tried use pickle way store embeddings embedding retriever load function tried following rest api trying load document store
Using sub-classes as anchors and classes as positives and negatives in a Siamese network with triplet loss?,"<p>I’m experimenting with a Siamese network using triplet loss to categorize sub-classes into broader classes. My setup differs from traditional triplet loss models: It involves using the sub-class as the anchor and the broader class as the positive (where the sub-class fits) and a different class as the negative (where it doesn’t fit). The goal is to position each sub-class embedding closer to its relevant class and farther from unrelated classes. Would this architecture make sense for capturing context-dependent relationships between sub-classes and classes? Are there any limitations I should be aware of?</p>
<p>I havent written any code. I'm curious about the theorhetical possibility.</p>
",Vectorization & Embeddings,using sub class anchor class positive negative siamese network triplet loss experimenting siamese network using triplet loss categorize sub class broader class setup differs traditional triplet loss model involves using sub class anchor broader class positive sub class fit different class negative fit goal position sub class embedding closer relevant class farther unrelated class would architecture make sense capturing context dependent relationship sub class class limitation aware havent written code curious theorhetical possibility
Is it possible to get embeddings from NV-Embed using Candle?,"<p>What I want to do is a CLI program that outputs embeddings of an arbitrary input.
To do that, I want to do an inference with an embeddings model, and I chose <code>NV-Embed-v2</code>. My framework of choice is <a href=""https://github.com/huggingface/candle"" rel=""nofollow noreferrer"">Candle</a>, but I also looked at <a href=""https://github.com/EricLBuehler/mistral.rs"" rel=""nofollow noreferrer"">Mistral-RS</a>.</p>
<p>Basically, what I'm trying to do is this code fragment:
<a href=""https://huggingface.co/nvidia/NV-Embed-v2"" rel=""nofollow noreferrer"">https://huggingface.co/nvidia/NV-Embed-v2</a>
but with Rust and Candle.</p>
<p>What I tried is to start off with <a href=""https://github.com/huggingface/candle/blob/main/candle-examples/examples/mistral/main.rs"" rel=""nofollow noreferrer"">Mistral Candle's example</a> because the NV-Embed's HF page says: <code>Model Details / Base Decoder-only LLM: Mistral-7B-v0.1</code>.</p>
<p>I replaced the model id in the original code with <code>nvidia/NV-Embed-v2</code>, and was able to download the weights from Hugging Face, but upon loading the config, I got this:</p>
<pre><code>Error: missing field `vocab_size` at line 101 column 1
</code></pre>
<p>Then I hardcoded the values from the JSON config loaded from HF to a newly created <code>candle_transformers::models::mistral::Config</code> instance. And after that, <code>Mistral::new(&amp;config, vb)</code> fails with:</p>
<pre><code>Error: cannot find tensor model.embed_tokens.weight
</code></pre>
<p>Is there a way around that — maybe there are some other Candle-based open source works that I could use as an inspiration? Or, maybe that's a common mistake that could easily be diagnosed?</p>
",Vectorization & Embeddings,possible get embeddings nv embed using candle want cli program output embeddings arbitrary input want inference embeddings model chose framework choice candle also looked mistral r basically trying code fragment rust candle tried start mistral candle example nv embed hf page say replaced model id original code wa able download weight hugging face upon loading config got hardcoded value json config loaded hf newly created instance fails way around maybe candle based open source work could use inspiration maybe common mistake could easily diagnosed
How to reliably find synonyms within a corpus of data?,"<p>I'm working on a project where I need to identify synonyms for terms across a specific corpus of product data. The data is a list of products, and I'm trying to find similar terms that appear repeatedly in the titles, descriptions and other fields. For example searching for &quot;baby&quot; should return &quot;newborn&quot;, &quot;toddler&quot;, &quot;infant&quot; if they're available in the corpus.</p>
<p>I've experimented with some pre-trained embeddings in <code>gensim</code>, specifically the <code>&quot;glove-wiki-gigaword-100&quot;</code> model. This got me some reasonable results, but it missed a few obvious synonyms and included some unrelated terms as well. I also tried using Word2Vec, but my corpus of products is too small to train a high-quality custom model, so it didn't yield very useful results either.</p>
<p>Here's a simplified version of my most successful code:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.downloader import load as gensim_load
from typing import List, Set, Dict
from .models import AlgoProduct

def get_corpus_words(products: List[AlgoProduct]) -&gt; Set[str]:
    corpus_words = set()
    for p in products:
        corpus_words.add(p.object.lower())
        corpus_words.update([m.lower() for m in p.words_of_interest])
    return corpus_words

def load_pretrained_model():
    model = gensim_load(&quot;glove-wiki-gigaword-100&quot;)
    return model

def get_similar_words(model, word: str, corpus_words: Set[str], topn: int = 10) -&gt; Set[str]:
    similar_words = set()
    try:
        top_similar = model.most_similar(word.lower(), topn=topn)
        for similar_word, similarity in top_similar:
            if similar_word in corpus_words:
                similar_words.add(similar_word)
    except KeyError:
        pass
    return similar_words

def get_synonyms(products: List[AlgoProduct]) -&gt; Dict[str, Set[str]]:
    synonyms: Dict[str, Set[str]] = {}
    corpus_words = get_corpus_words(products)
    model = load_pretrained_model()

    for word in corpus_words:
        synonyms[word] = get_similar_words(model, word, corpus_words)

    return synonyms
</code></pre>
<p>This function builds a dictionary with each term in the corpus mapped to its synonyms. But the results are inconsistent—many entries have empty sets or miss expected synonyms:</p>
<pre class=""lang-py prettyprint-override""><code>{'baby': {'girl', 'toddler', 'newborn'}, 'bibs': set(), 'shoes': {'boots'}, 'newborn': {'baby', 'toddler'}, ... }
</code></pre>
<p>In some of my previous iterations, I was getting overly broad matches or just odd, unrelated terms:</p>
<pre class=""lang-py prettyprint-override""><code>{'Nest': [], 'baby': ['for', 'toddler', 'boots', 'sleeping', 'strap'], 'nappy': ['blanket', 'hipseat', 'shoes'], ...}
</code></pre>
<p>The current iteration is okay, depending on the product list it can get pretty close, but I am looking for ways to further optimize and improve the reliability. I am certain that this is not the most optimal approach.</p>
<p>To summarize, I basically need to find synonyms in my own corpus of data, in theory I can just find all synonyms and then figure out what appears in my corpus, but that seems relatively inefficient and I doubt it would be as reliable.</p>
",Vectorization & Embeddings,reliably find synonym within corpus data working project need identify synonym term across specific corpus product data data list product trying find similar term appear repeatedly title description field example searching baby return newborn toddler infant available corpus experimented pre trained embeddings specifically model got reasonable result missed obvious synonym included unrelated term well also tried using word vec corpus product small train high quality custom model yield useful result either simplified version successful code function build dictionary term corpus mapped synonym result inconsistent many entry empty set miss expected synonym previous iteration wa getting overly broad match odd unrelated term current iteration okay depending product list get pretty close looking way optimize improve reliability certain optimal approach summarize basically need find synonym corpus data theory find synonym figure appears corpus seems relatively inefficient doubt would reliable
Geometric visulalization of Cosine Similarity,"<p>I have calculated the cosine similarity between two documents in a very basic way by using TF-IDF vectorization in Python.
But I want to visualize the documents as a vectorized graph in a 3D space. Like this:</p>
<p><a href=""https://i.sstatic.net/pBFMUIOf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pBFMUIOf.png"" alt=""example of graph"" /></a></p>
<p>Here's the code I used for calculating cosine similarity:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import numpy as np

# Sample documents
documents = [
    &quot;This is a sample document.&quot;,
    &quot;This document is another example.&quot;
]

# Vectorizing the documents
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# Calculating cosine similarity
cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])
print(&quot;Cosine Similarity:&quot;, cosine_sim)
</code></pre>
",Vectorization & Embeddings,geometric visulalization cosine similarity calculated cosine similarity two document basic way using tf idf vectorization python want visualize document vectorized graph space like code used calculating cosine similarity
word2vec download taking too long,"<p>Windows10, IDLE, The download time is too long for the below code.</p>
<pre><code>import gensim.downloader as api
word_vec_list = api.load('word2vec-google-news-300')
</code></pre>
<ol>
<li>Is it possible that I download only fewer words?</li>
<li>Is it possible to download the dataset as csv and use it later in the program?</li>
</ol>
",Vectorization & Embeddings,word vec download taking long window idle download time long code possible download fewer word possible download dataset csv use later program
Bert sentence embeddings,"<p>Im trying to obtain sentence embeddings for Bert but Im not quite sure if Im doing it properly... and yes Im aware that exist such tools already such as bert-as-service but I want to do it myself and understand how it works.</p>

<p>Lets say I want to extract a sentence embedding from word embeddings from the following sentence ""I am."". As I understood Bert outputs in the form of (12, seq_lenght, 768). I extracted each word embedding from the last encoder layer in the form of (1, 768). My doubt now lies in extracting the sentence from these two word vectors. If I have (2,768) should I sum the dim=1 and obtain a vector of (1,768)? Or maybe concatenate the two words (1, 1536) and applying a (mean) pooling and get the sentence vector in shape of (1, 768). Im not sure what is the right approach is to obtain the sentence vector for this given example is. </p>
",Vectorization & Embeddings,bert sentence embeddings im trying obtain sentence embeddings bert im quite sure im properly yes im aware exist tool already bert service want understand work let say want extract sentence embedding word embeddings following sentence understood bert output form seq lenght extracted word embedding last encoder layer form doubt lie extracting sentence two word vector sum dim obtain vector maybe concatenate two word applying mean pooling get sentence vector shape im sure right approach obtain sentence vector given example
What&#39;s going wrong in these weighted jaccard sum calculations for comparing the pronunciation of consonant clusters?,"<h2>Context</h2>
<p>I have this code for my attempt to create a &quot;similarity mapping&quot; between consonants (or consonant clusters), to the same set of consonants/clusters (basically a cross product mapping), like this form:</p>
<pre><code>// ... all possible consonants/clusters ...
type ClusterKey = 'b' | 'c' | 'd' | 'f' | 'br' | 'bl'

type ClusterMappings = Record&lt;ClusterKey, ClusterMapping&gt;

type ClusterMapping = Record&lt;string, number&gt;
</code></pre>
<p>The ClusterKey values are basically the keys in <a href=""https://github.com/termsurf/talk/blob/ea9e2791c6bd5dbf729362159c5f2f75ba7b38a1/make/rhymes/features.ts#L147-L335"" rel=""nofollow noreferrer"">this object</a>:</p>
<pre><code>export const CONSONANTS: Record&lt;string, Float32Array&gt; = {
  m: consonant({ bilabial: 0.9, nasal: 1.0 }),
  N: consonant({ retroflex: 1.0, nasal: 0.8 }),
  n: consonant({ alveolar: 1.0, nasal: 0.9 }),
  q: consonant({ velar: 1.0, nasal: 0.8 }),
  'G~': consonant({ velarization: 1.0 }),
  G: consonant({ velar: 1.0, fricative: 0.8 }),
  'g?': consonant({ plosive: 1.0, velar: 1.0, implosive: 0.5 }),
  g: consonant({ plosive: 1.0, velar: 1.0 }),
  &quot;'&quot;: consonant({ plosive: 1.0, glottal: 0.9 }),
  Q: consonant({ pharyngeal: 1.0, fricative: 0.9 }),
  'd?': consonant({ dental: 1.0, plosive: 1.0, implosive: 0.5 }),
  'd!': consonant({ dental: 1.0, plosive: 1.0, ejective: 0.5 }),
  'd*': consonant({ click: 1.0 }),
  ...
}
</code></pre>
<h2>Full Code</h2>
<p>The full code I have been working on is here, which defines a bunch of consonant vectors, maps them to each other, and stringifies it into somewhat human readable JSON:</p>
<pre><code>import fs from &quot;fs&quot;;

import merge from &quot;lodash/merge&quot;;

export const CONSONANT_FEATURE_NAMES = [
  &quot;affricate&quot;,
  &quot;alveolar&quot;,
  &quot;approximant&quot;,
  &quot;aspiration&quot;,
  &quot;bilabial&quot;,
  &quot;click&quot;,
  &quot;coarticulated&quot;,
  &quot;dental&quot;,
  &quot;dentalization&quot;,
  &quot;ejective&quot;,
  &quot;fricative&quot;,
  &quot;glottal&quot;,
  &quot;glottalization&quot;,
  &quot;implosive&quot;,
  &quot;labialization&quot;,
  &quot;labiodental&quot;,
  &quot;labiovelar&quot;,
  &quot;lateral&quot;,
  &quot;nasal&quot;,
  &quot;nasalization&quot;,
  &quot;palatal&quot;,
  &quot;palatalization&quot;,
  &quot;pharyngeal&quot;,
  &quot;pharyngealization&quot;,
  &quot;plosive&quot;,
  &quot;postalveolar&quot;,
  &quot;retroflex&quot;,
  &quot;sibilant&quot;,
  &quot;stop&quot;,
  &quot;tap&quot;,
  &quot;tense&quot;,
  &quot;uvular&quot;,
  &quot;velar&quot;,
  &quot;velarization&quot;,
  &quot;voiced&quot;,
] as const;

export type ConsonantFeatureName = (typeof CONSONANT_FEATURE_NAMES)[number];

export type ConsonantWithFeatures = Partial&lt;
  Record&lt;ConsonantFeatureName, number&gt;
&gt;;

export const VOWEL_FEATURE_NAMES = [
  &quot;back&quot;, // Back vowel (tongue towards the back)
  &quot;central&quot;, // Central vowel (tongue in the center)
  &quot;front&quot;, // Front vowel (tongue towards the front)
  &quot;closed&quot;, // High tongue position
  &quot;long&quot;, // Vowel length (long or short)
  &quot;open&quot;, // Low tongue position
  &quot;mid&quot;, // Mid tongue position
  &quot;nasalization&quot;, // Whether the vowel is nasalized
  &quot;rounded&quot;, // Whether the lips are rounded
  &quot;stress&quot;, // Whether the vowel is stressed
  &quot;unrounded&quot;,
  &quot;near&quot;,
] as const;

export type VowelFeatureName = (typeof VOWEL_FEATURE_NAMES)[number];

export type VowelWithFeatures = Partial&lt;Record&lt;VowelFeatureName, number&gt;&gt;;

export const VOWELS: Record&lt;string, Float32Array&gt; = {};

const NASALS = [0, 0.4];
const STRESSES = [0, 0.3];

NASALS.forEach((nasalization) =&gt; {
  STRESSES.forEach((stress) =&gt; {
    const keys: Array&lt;string&gt; = [];
    if (nasalization) {
      keys.push(`&amp;`);
    }
    if (stress) {
      keys.push(&quot;^&quot;);
    }
    const key = keys.join(&quot;&quot;);
    merge(VOWELS, {
      [`i${key}`]: vowel({
        closed: 1.0,
        front: 1.0,
        unrounded: 1.0,
        nasalization,
        stress,
      }),
      [`e${key}`]: vowel({
        mid: 1.0,
        front: 1.0,
        unrounded: 1.0,
        nasalization,
        stress,
      }),
      [`a${key}`]: vowel({
        open: 1.0,
        front: 0.5,
        unrounded: 1.0,
        nasalization,
        stress,
      }),
      [`o${key}`]: vowel({
        closed: 1.0,
        mid: 1.0,
        back: 1.0,
        rounded: 1.0,
        nasalization,
        stress,
      }),
      [`u${key}`]: vowel({
        closed: 1.0,
        back: 1.0,
        rounded: 1.0,
        nasalization,
        stress,
      }),
      [`I${key}`]: vowel({
        near: 1.0,
        closed: 1.0,
        front: 1.0,
        unrounded: 1.0,
        nasalization,
        stress,
      }),
      [`E${key}`]: vowel({
        open: 1.0,
        mid: 1.0,
        front: 1.0,
        unrounded: 1.0,
        nasalization,
        stress,
      }),
      [`A${key}`]: vowel({
        near: 1.0,
        open: 1.0,
        front: 1.0,
        unrounded: 0.9,
        nasalization,
        stress,
      }),
      [`O${key}`]: vowel({
        near: 1.0,
        mid: 1.0,
        back: 1.0,
        unrounded: 1.0,
        nasalization,
        stress,
      }),
      [`U${key}`]: vowel({
        mid: 1.0,
        central: 1.0,
        nasalization,
        stress,
      }),
      [`i$${key}`]: vowel({
        front: 1.0,
        closed: 1.0,
        rounded: 1.0,
        nasalization,
        stress,
      }),
      [`e$${key}`]: vowel({
        closed: 1.0,
        mid: 1.0,
        front: 1.0,
        rounded: 1.0,
        nasalization,
        stress,
      }),
      [`a$${key}`]: vowel({
        open: 1.0,
        mid: 1.0,
        front: 1.0,
        rounded: 1.0,
        nasalization,
        stress,
      }),
      [`o$${key}`]: vowel({
        open: 1.0,
        mid: 1.0,
        back: 1.0,
        rounded: 1.0,
        nasalization,
        stress,
      }),
      [`u$${key}`]: vowel({
        open: 1.0,
        mid: 1.0,
        central: 1.0,
        unrounded: 1.0,
        nasalization,
        stress,
      }),
    });
  });
});

export const VOWEL_KEYS = Object.keys(VOWELS);

export const VOWEL_VECTOR_PLACEHOLDER = vowel();

// https://en.wikipedia.org/wiki/IPA_consonant_chart_with_audio
export const CONSONANTS: Record&lt;string, Float32Array&gt; = {
  m: consonant({ bilabial: 0.9, nasal: 1.0 }),
  N: consonant({ retroflex: 1.0, nasal: 0.8 }),
  n: consonant({ alveolar: 1.0, nasal: 0.9 }),
  q: consonant({ velar: 1.0, nasal: 0.8 }),
  &quot;G~&quot;: consonant({ velarization: 1.0 }),
  G: consonant({ velar: 1.0, fricative: 0.8 }),
  &quot;g?&quot;: consonant({ plosive: 1.0, velar: 1.0, implosive: 0.5 }),
  g: consonant({ plosive: 1.0, velar: 1.0 }),
  &quot;'&quot;: consonant({ plosive: 1.0, glottal: 0.9 }),
  Q: consonant({ pharyngeal: 1.0, fricative: 0.9 }),
  &quot;d?&quot;: consonant({ dental: 1.0, plosive: 1.0, implosive: 0.5 }),
  &quot;d!&quot;: consonant({ dental: 1.0, plosive: 1.0, ejective: 0.5 }),
  &quot;d*&quot;: consonant({ click: 1.0 }),
  &quot;d.&quot;: consonant({ dental: 1.0, plosive: 1.0, stop: 0.2 }),
  D: consonant({ dental: 1.0, retroflex: 0.9, plosive: 1.0 }),
  &quot;dQ~&quot;: consonant({
    dental: 1.0,
    plosive: 1.0,
    pharyngealization: 0.8,
  }),
  d: consonant({ dental: 1.0, plosive: 1.0 }),
  &quot;b?&quot;: consonant({
    bilabial: 1.0,
    voiced: 1.0,
    plosive: 1.0,
    implosive: 0.5,
  }),
  &quot;b!&quot;: consonant({
    bilabial: 1.0,
    voiced: 1.0,
    plosive: 1.0,
    ejective: 0.5,
  }),
  b: consonant({ bilabial: 1.0, voiced: 1.0, plosive: 1.0 }),
  &quot;p!&quot;: consonant({ bilabial: 1.0, plosive: 1.0, ejective: 0.5 }),
  &quot;p*&quot;: consonant({ bilabial: 1.0, click: 1.0 }),
  &quot;p.&quot;: consonant({ bilabial: 1.0, plosive: 1.0, stop: 0.2 }),
  &quot;p@&quot;: consonant({ bilabial: 1.0, plosive: 1.0, tense: 0.1 }),
  p: consonant({ bilabial: 1.0, plosive: 1.0 }),
  // ...
};

export const CONSONANT_KEYS = Object.keys(CONSONANTS);

export const CONSONANT_VECTOR_PLACEHOLDER = consonant();

function consonant(
  mappings: Partial&lt;Record&lt;ConsonantFeatureName, number&gt;&gt; = {}
) {
  const consonant = new Float32Array(CONSONANT_FEATURE_NAMES.length);
  CONSONANT_FEATURE_NAMES.forEach((name, i) =&gt; {
    const number =
      name in mappings
        ? typeof mappings[name] === &quot;number&quot;
          ? mappings[name]
          : mappings[name] === true
          ? 1
          : 0
        : 0;
    consonant[i] = number;
  });
  return consonant;
}

function vowel(mappings: Partial&lt;Record&lt;VowelFeatureName, number&gt;&gt; = {}) {
  const consonant = new Float32Array(VOWEL_FEATURE_NAMES.length);
  VOWEL_FEATURE_NAMES.forEach((name, i) =&gt; {
    const number =
      name in mappings
        ? typeof mappings[name] === &quot;number&quot;
          ? mappings[name]
          : mappings[name] === true
          ? 1
          : 0
        : 0;
    consonant[i] = number;
  });
  return consonant;
}

const startingConsonants = CONSONANT_KEYS.map((key) =&gt; [{ key }]).concat(
  `bl
br
cl
cr
dr
fl
fr
gl
gr
kl
kr
pl
pr
sl
sm
sn
sp
st
tr
tw
sk
sw
sr
sc
th
sh
ch
ph
thw
sch
spl
spr
str
skr
slj
trj
blj
blw
drw
brw
kw
grw
kn
gn
zl
zn
vl
vr
ps
pt
pn
skh
sth
sf
ks
zh
zv
spn
shl
skl
smn
shr
chv
thn
klh
ql
phn
zr
brl
grk
ndr
ndl
sdr
skn
slz
zj
zd
rz
tsr
tn
prn
skj
svk
dj
trl
khr
chr
tsw
thr
ghn`
    .trim()
    .split(/\n+/)
    .map((text) =&gt; text.split(&quot;&quot;).map((key) =&gt; ({ key })))
);

export type Substitution = {
  key: string;
  vector: Float32Array;
};

export type SubstitutionList = {
  list: Array&lt;Substitution&gt;;
  vector: Float32Array;
};

export type Cluster = {
  key: string;
};

export type Substitutions = Record&lt;string, SubstitutionList&gt;;

const startingConsonantSubstitutions = startingConsonants.reduce&lt;Substitutions&gt;(
  (map, cluster) =&gt; {
    const list = startingConsonants.map(consonant);
    const { key, vector } = consonant(cluster);

    map[key] = { list, vector };

    return map;
  },
  {}
);

const substitutions = {
  ...startingConsonantSubstitutions,
};

fs.writeFileSync(`make/rhymes/substitutions.json`, stringify(substitutions));

function stringify(substitutions: Substitutions) {
  const text = [];

  text.push(`{`);

  let n = Object.keys(substitutions).length;
  let i = 0;
  for (const key in substitutions) {
    const sub = substitutions[key]!;
    text.push(`  &quot;${key}&quot;: {`);
    sub.list.forEach((item, i) =&gt; {
      const tail = i === sub.list.length - 1 ? &quot;&quot; : &quot;,&quot;;
      const sum = weightedJaccard(sub.vector, item.vector);
      text.push(`    &quot;${item.key}&quot;: ${sum}${tail}`);
    });
    const tail = i === n - 1 ? &quot;&quot; : &quot;,&quot;;
    text.push(`  }${tail}`);
    i++;
  }

  text.push(`}`);
  return text.join(&quot;\n&quot;);
}

// function consonant(symbols: Array&lt;Cluster&gt;) {
//   const vector = new Float32Array(
//     4 * CONSONANT_VECTOR_PLACEHOLDER.length,
//   )
//   symbols.forEach((symbol, i) =&gt; {
//     const consonant = CONSONANTS[symbol.key]!
//     let j = 0
//     while (j &lt; consonant.length) {
//       vector[i * 4 + j] = consonant[j]!
//       j++
//     }
//   })
//   return { key: symbols.map(({ key }) =&gt; key).join(''), vector }
// }
function consonant(symbols: Array&lt;Cluster&gt;) {
  const vector = combineConsonantClusterVectors(
    symbols.map((symbol) =&gt; symbol.key)
  );

  return { key: symbols.map(({ key }) =&gt; key).join(&quot;&quot;), vector };
}

export function combineConsonantClusterVectors(
  clusters: Array&lt;string&gt;
): Float32Array {
  const vector = new Float32Array(CONSONANT_VECTOR_PLACEHOLDER.length);

  clusters.forEach((cluster) =&gt; {
    const consonantVector =
      CONSONANTS[cluster] ||
      new Float32Array(CONSONANT_VECTOR_PLACEHOLDER.length);

    for (let i = 0; i &lt; vector.length; i++) {
      vector[i]! += consonantVector[i]!;
    }
  });

  return normalizeVector(vector);
}

export function normalizeVector(vector: Float32Array): Float32Array {
  const max = Math.max(...vector);
  if (max === 0) {
    return vector;
  } // Avoid division by zero if the vector is all zeros.
  return vector.map((value) =&gt; value / max);
}

export function weightedJaccard(a: Float32Array, b: Float32Array): number {
  let intersectionSum = 0;
  let unionSum = 0;

  for (let i = 0; i &lt; a.length; i++) {
    const valueA = a[i]!;
    const valueB = b[i]!;

    // Weighted Intersection: take the minimum of both values
    intersectionSum += Math.min(valueA, valueB);

    // Weighted Union: take the maximum of both values
    unionSum += Math.max(valueA, valueB);
  }

  // Return the Jaccard similarity, or 0 if the union is 0 to avoid division by zero
  return unionSum === 0 ? 0 : intersectionSum / unionSum;
}
</code></pre>
<p>Toward the bottom of that code block is this:</p>
<pre><code>export type Substitution = {
  key: string
  vector: Float32Array
}

export type SubstitutionList = {
  list: Array&lt;Substitution&gt;
  vector: Float32Array
}

export type Cluster = {
  key: string
}

export type Substitutions = Record&lt;string, SubstitutionList&gt;

const startingConsonantSubstitutions =
  startingConsonants.reduce&lt;Substitutions&gt;((map, cluster) =&gt; {
    const list = startingConsonants.map(consonant)
    const { key, vector } = consonant(cluster)

    map[key] = { list, vector }

    return map
  }, {})

const substitutions = {
  ...startingConsonantSubstitutions,
}
</code></pre>
<p>That is where the final result is generated of the substitution mapping JSON.</p>
<h2>Results</h2>
<p>Currently, (writing that <code>substitutions</code> out to a JSON file), it currently results in mappings like this (full example <a href=""https://github.com/termsurf/talk/blob/ea9e2791c6bd5dbf729362159c5f2f75ba7b38a1/make/rhymes/substitutions.json"" rel=""nofollow noreferrer"">here</a>):</p>
<pre><code>{
  &quot;m&quot;: {
    &quot;m&quot;: 1,
    &quot;N&quot;: 0.27586207534413565,
    &quot;n&quot;: 0.3103448219163239,
    &quot;q&quot;: 0.27586207534413565,
    &quot;G~&quot;: 0,
    &quot;G&quot;: 0,
    &quot;g?&quot;: 0,
    &quot;g&quot;: 0,
    &quot;'&quot;: 0,
    &quot;Q&quot;: 0,
    &quot;d?&quot;: 0,
    &quot;d!&quot;: 0,
    &quot;d*&quot;: 0,
    &quot;d.&quot;: 0,
    &quot;D&quot;: 0,
    &quot;dQ~&quot;: 0,
    &quot;d&quot;: 0,
    &quot;b?&quot;: 0.19999999470180935,
    &quot;b!&quot;: 0.19999999470180935,
    &quot;b&quot;: 0.22499999403953552,
    &quot;p!&quot;: 0.25714285033089773,
    &quot;p*&quot;: 0.29999999205271405,
    &quot;p.&quot;: 0.28124999228748493,
    &quot;p@&quot;: 0.2903225728146864,
    &quot;p&quot;: 0.29999999205271405,
    &quot;T!&quot;: 0,
    &quot;T&quot;: 0,
    &quot;t!&quot;: 0,
    &quot;t*&quot;: 0,
    &quot;tQ~&quot;: 0,
    &quot;t@&quot;: 0,
    &quot;t.&quot;: 0,
    &quot;t&quot;: 0,
    &quot;k!&quot;: 0,
    &quot;k.&quot;: 0,
    &quot;k*&quot;: 0,
    &quot;K!&quot;: 0,
    &quot;K&quot;: 0,
    &quot;k&quot;: 0,
    &quot;H!&quot;: 0,
    &quot;H&quot;: 0,
    &quot;h~&quot;: 0,
    &quot;h!&quot;: 0,
    &quot;h&quot;: 0,
    &quot;J&quot;: 0,
    &quot;j!&quot;: 0,
    &quot;j&quot;: 0,
    &quot;S!&quot;: 0,
    &quot;s!&quot;: 0,
    &quot;S&quot;: 0,
    &quot;sQ~&quot;: 0,
    &quot;s@&quot;: 0,
    &quot;s&quot;: 0,
    &quot;F&quot;: 0.29999999205271405,
    &quot;f!&quot;: 0,
    &quot;f&quot;: 0,
    &quot;V&quot;: 0.22499999403953552,
    &quot;v&quot;: 0,
    &quot;z!&quot;: 0,
    &quot;zQ~&quot;: 0,
    &quot;z&quot;: 0,
    &quot;Z!&quot;: 0,
    &quot;Z&quot;: 0,
    &quot;CQ~&quot;: 0,
    &quot;C&quot;: 0,
    &quot;cQ~&quot;: 0,
    &quot;c&quot;: 0,
    &quot;L&quot;: 0,
    &quot;l*&quot;: 0,
    &quot;lQ~&quot;: 0,
    &quot;l&quot;: 0,
    &quot;R&quot;: 0,
    &quot;rQ~&quot;: 0,
    &quot;r&quot;: 0,
    &quot;x!&quot;: 0,
    &quot;X!&quot;: 0,
    &quot;X&quot;: 0,
    &quot;x@&quot;: 0,
    &quot;x&quot;: 0,
    &quot;W&quot;: 0,
    &quot;w!&quot;: 0,
    &quot;w~&quot;: 0,
    &quot;w&quot;: 0,
    &quot;y~&quot;: 0,
    &quot;y&quot;: 0,
    &quot;bl&quot;: 0.14999999602635702,
    &quot;br&quot;: 0.14999999602635702,
    &quot;cl&quot;: 0,
    &quot;cr&quot;: 0,
    &quot;dr&quot;: 0,
    &quot;fl&quot;: 0,
    &quot;fr&quot;: 0,
    &quot;gl&quot;: 0,
    &quot;gr&quot;: 0,
    &quot;kl&quot;: 0,
    &quot;kr&quot;: 0,
    &quot;pl&quot;: 0.1799999952316284,
    &quot;pr&quot;: 0.1799999952316284,
    &quot;sl&quot;: 0,
    &quot;sm&quot;: 0.3877550990618253,
    &quot;sn&quot;: 0.11538461303334734,
    &quot;sp&quot;: 0.14999999602635702,
    &quot;st&quot;: 0,
    &quot;tr&quot;: 0,
    &quot;tw&quot;: 0,
    &quot;sk&quot;: 0,
    &quot;sw&quot;: 0,
    &quot;sr&quot;: 0,
    &quot;sc&quot;: 0,
    &quot;th&quot;: 0,
    &quot;sh&quot;: 0,
    &quot;ch&quot;: 0,
    &quot;ph&quot;: 0.1799999952316284,
    &quot;thw&quot;: 0,
    &quot;sch&quot;: 0,
    &quot;spl&quot;: 0.11249999701976776,
    &quot;spr&quot;: 0.10204081682302911,
    &quot;str&quot;: 0,
    &quot;skr&quot;: 0,
    &quot;slj&quot;: 0,
    &quot;trj&quot;: 0,
    &quot;blj&quot;: 0.09999999735090467,
    &quot;blw&quot;: 0.0925925930014036,
    &quot;drw&quot;: 0,
    &quot;brw&quot;: 0.09999999735090467,
    &quot;kw&quot;: 0,
    &quot;grw&quot;: 0,
    &quot;kn&quot;: 0.1836734654157671,
    &quot;gn&quot;: 0.1836734654157671,
    &quot;zl&quot;: 0,
    &quot;zn&quot;: 0.1836734654157671,
    &quot;vl&quot;: 0,
    &quot;vr&quot;: 0,
    &quot;ps&quot;: 0.14999999602635702,
    &quot;pt&quot;: 0.147058824560634,
    &quot;pn&quot;: 0.44999998807907104,
    &quot;skh&quot;: 0,
    &quot;sth&quot;: 0,
    &quot;sf&quot;: 0,
    &quot;ks&quot;: 0,
    &quot;zh&quot;: 0,
    &quot;zv&quot;: 0,
    &quot;spn&quot;: 0.21590908936971476,
    &quot;shl&quot;: 0,
    &quot;skl&quot;: 0,
    &quot;smn&quot;: 0.3589743550555789,
    &quot;shr&quot;: 0,
    &quot;chv&quot;: 0,
    &quot;thn&quot;: 0.10227272511760065,
    &quot;klh&quot;: 0,
    &quot;ql&quot;: 0.16326530934968925,
    &quot;phn&quot;: 0.29999999205271405,
    &quot;zr&quot;: 0,
    &quot;brl&quot;: 0.11249999701976776,
    &quot;grk&quot;: 0,
    &quot;ndr&quot;: 0.10227272511760065,
    &quot;ndl&quot;: 0.13043477960405067,
    &quot;sdr&quot;: 0,
    &quot;skn&quot;: 0.09183673270788355,
    &quot;slz&quot;: 0,
    &quot;zj&quot;: 0,
    &quot;zd&quot;: 0,
    &quot;rz&quot;: 0,
    &quot;tsr&quot;: 0,
    &quot;tn&quot;: 0.132352938598415,
    &quot;prn&quot;: 0.24358974202223155,
    &quot;skj&quot;: 0,
    &quot;svk&quot;: 0,
    &quot;dj&quot;: 0,
    &quot;trl&quot;: 0,
    &quot;khr&quot;: 0,
    &quot;chr&quot;: 0,
    &quot;tsw&quot;: 0,
    &quot;thr&quot;: 0,
    &quot;ghn&quot;: 0.13043477960405067
  },
  ...
}
</code></pre>
<h2>Analysis</h2>
<p><strong>The only one that seems correct is <code>m:m</code> (<code>m</code> mapped to itself, which is <code>1</code>).</strong> That makes sense, it's an exact match. However, we should find <code>m:n</code>, and <code>m:N</code> (at least), to be like 0.9 or 0.95, since they are both nasal consonants which sound extremely similar.</p>
<p>It looks like all the consonant <em>clusters</em> matching <code>*n</code> have at least some value, but not sure if it makes sense what it ends up being. I don't have in mind exactly what these final <code>weightedJaccard</code> sums should be even (I am brand new to all these NLP/ML algorithms, trying to wire them together carefully).</p>
<p>All the rest of the consonant clusters are <code>0</code>, which probably makes sense, since if they don't have <code>m</code> or <code>n</code>, then they are probably not a good match (though not 100% sure yet if that is desired, I'd have to tinker with phonetic relationships more manually first).</p>
<p><strong>Then <code>p</code> and <code>b</code> are &quot;labial&quot; consonants (like <code>m</code> and <code>n</code>), so they have some value, but I would expect it to be slightly higher than 0.1-0.3.</strong></p>
<p>Finally, <code>F</code> and <code>V</code>, which are both technically &quot;bilabial&quot; consonants, they are &quot;fricatives&quot; however (like blowing wind through your lips), whereas <code>b</code> and <code>p</code> are explosions of sound (&quot;plosives&quot;), not streams of sound.</p>
<p>Here are all the key characters that relate to <code>m</code> (at least as far as this data turned out):</p>
<ul>
<li><code>F</code>: bilabial fricative</li>
<li><code>V</code>: voiced bilabial fricative</li>
<li><code>b</code>: voiced bilabial plosive</li>
<li><code>p</code>: bilabial plosive</li>
<li><code>m</code>: bilabial nasal</li>
<li><code>n</code>: alveolar nasal</li>
<li><code>N</code>: retroflex nasal</li>
</ul>
<p>I would expect it to be more like this number wise:</p>
<ul>
<li><code>F</code>: 0</li>
<li><code>V</code>: 0</li>
<li><code>b</code>: 0.5</li>
<li><code>p</code>: 0.3</li>
<li><code>m</code>: 1</li>
<li><code>n</code>: 0.95</li>
<li><code>N</code>: 0.9</li>
</ul>
<p>That is how I personally (subjectively) would give them values. The main thing there is <code>F</code> and <code>V</code> (while technically being &quot;bilabial&quot;), have no audible relationship to the sound <code>m</code> (like sounds &quot;f&quot; and &quot;v&quot; basically).</p>
<h2>Question</h2>
<p>Any ideas what I'm doing wrong? It seems that my feature vector mappings (done onto the <code>CONSONANTS</code> map object) are not going to result in the desired relationships/similarities between sounds. There are like 1-4 props per vector there, and it seems like &quot;why wouldn't any vector with 4 defined props with all values <code>1.0</code> result in the same vector embedding (so basically, it wouldn't respect the fact that these properties represent totally different things). So I'm confused as to how this will ever get close to resulting in a &quot;similarity mapping&quot;.</p>
<ul>
<li>Where can I debug better what might be going wrong in my sample code (which <a href=""https://github.com/termsurf/talk/blob/ea9e2791c6bd5dbf729362159c5f2f75ba7b38a1/make/rhymes/features.ts"" rel=""nofollow noreferrer"">currently</a> is spread across multiple modules in reality)? <em>But I combined and pasted the code in that big snippet above.</em></li>
<li>Am I doing the &quot;weighted jaccard&quot; calculations properly? Am I building the vectors properly? What could I be doing wrong here?</li>
</ul>
<p><em>How can I even begin to make features and assign floating point number values to each feature, so that I can get these sounds which I subjectively think are similar, to in fact be correlated in the numbers?</em></p>
<h2>Summary</h2>
<p>The basic flow of the code is:</p>
<ul>
<li><strong>Define consonant vectors</strong> (forget about vowels for now). There are about 35 keys (ML &quot;features&quot;?) in that <code>CONSONANT_FEATURE_NAMES</code> variable, which are all boolean values in reality, but I gave them a <code>number</code> value to make it sort of <em>weighted</em> instead of binary. I am pretty sure I'm doing this step correctly, am I not?</li>
<li><strong>Map consonants/clusters to themselves (cross-product).</strong></li>
<li><strong>For each mapping item, calculate each item's consonant cluster vector union for the weighted jaccard sum later.</strong> Not sure if I'm doing this step properly.</li>
<li><strong>Then taking that calculated jaccard sum for each item being mapped, call <code>weightedJaccard(source.vector, item.vector)</code></strong>, to create the mapping of the source consonant cluster to each target cluster. Also not sure if this step is correct.</li>
</ul>
<p>Looking to better understand where I'm going wrong in calculating the similarity between consonant clusters using the <strong>weight jaccard</strong> approach.</p>
<p><em><strong>Note</strong>: Got the idea for using the jaccard sum from this paper: <a href=""https://arxiv.org/abs/2109.14796"" rel=""nofollow noreferrer"">arxiv.org/abs/2109.14796</a>. Not sure if I'm doing it right, or even if that's the best approach to my overall problem. But for this question, we can assume we want to use the weight jaccard sum idea, unless you feel strongly otherwise.</em></p>
<h2>Update</h2>
<p><strong>Note</strong>: Plugging this into Claude AI, it gave what seems like a helpful suggestion, to completely change my base consonant vector system, sort of like this:</p>
<pre><code>// Define a more detailed feature space
const FEATURE_SPACE = {
  placeOfArticulation: {
    features: ['bilabial', 'labiodental', 'dental', 'alveolar', 'postalveolar', 'retroflex', 'palatal', 'velar', 'uvular', 'pharyngeal', 'glottal'],
    weight: 0.3,
    // Define relationships between places of articulation
    similarity: (a: string, b: string) =&gt; {
      const order = ['bilabial', 'labiodental', 'dental', 'alveolar', 'postalveolar', 'retroflex', 'palatal', 'velar', 'uvular', 'pharyngeal', 'glottal'];
      const distance = Math.abs(order.indexOf(a) - order.indexOf(b));
      return 1 - (distance / (order.length - 1));
    }
  },
  manner: {
    features: ['plosive', 'nasal', 'trill', 'tap', 'fricative', 'lateral', 'approximant', 'affricate'],
    weight: 0.3,
    // Define relationships between manners of articulation
    similarity: (a: string, b: string) =&gt; {
      const groups = [
        ['plosive', 'affricate'],
        ['fricative', 'approximant'],
        ['nasal', 'lateral'],
        ['trill', 'tap']
      ];
      if (a === b) return 1;
      if (groups.some(group =&gt; group.includes(a) &amp;&amp; group.includes(b))) return 0.5;
      return 0;
    }
  },
  voicing: {
    features: ['voiced', 'voiceless'],
    weight: 0.2,
    similarity: (a: string, b: string) =&gt; a === b ? 1 : 0
  },
  airflow: {
    features: ['oral', 'nasal'],
    weight: 0.1,
    similarity: (a: string, b: string) =&gt; a === b ? 1 : 0
  },
  release: {
    features: ['aspirated', 'unaspirated'],
    weight: 0.1,
    similarity: (a: string, b: string) =&gt; a === b ? 1 : 0
  }
};

type FeatureSpace = typeof FEATURE_SPACE;

function createDetailedFeatureVector(consonant: string): number[] {
  const features = CONSONANTS[consonant];
  const vector: number[] = [];

  Object.entries(FEATURE_SPACE).forEach(([category, { features, weight }]) =&gt; {
    features.forEach(feature =&gt; {
      const value = features[feature] || 0;
      vector.push(value * weight);
    });
  });

  return vector;
}

function calculateFeatureSimilarity(feature1: string, feature2: string, category: keyof FeatureSpace): number {
  const { similarity } = FEATURE_SPACE[category];
  return similarity(feature1, feature2);
}

function consonantSimilarity(c1: string, c2: string): number {
  let totalSimilarity = 0;
  let totalWeight = 0;

  Object.entries(FEATURE_SPACE).forEach(([category, { features, weight }]) =&gt; {
    const f1 = features.find(f =&gt; CONSONANTS[c1][f]) || '';
    const f2 = features.find(f =&gt; CONSONANTS[c2][f]) || '';
    
    if (f1 &amp;&amp; f2) {
      const similarity = calculateFeatureSimilarity(f1, f2, category as keyof FeatureSpace);
      totalSimilarity += similarity * weight;
      totalWeight += weight;
    }
  });

  return totalSimilarity / totalWeight;
}

// Example usage
console.log(consonantSimilarity('m', 'n')); // Should be high (both nasals)
console.log(consonantSimilarity('p', 'b')); // Should be high (differ only in voicing)
console.log(consonantSimilarity('p', 'k')); // Should be moderate (both voiceless plosives, different place)
console.log(consonantSimilarity('p', 's')); // Should be lower (different manner and place)
</code></pre>
<p>Not sure why my system won't work, and why this might be better. <em>Slightly another way of looking at the problem perhaps, maybe better too.</em></p>
<p>Should I be doing something more like that to organize my features? Maybe that's why I'm getting unexpected results...</p>
",Vectorization & Embeddings,going wrong weighted jaccard sum calculation comparing pronunciation consonant cluster context code attempt create similarity mapping consonant consonant cluster set consonant cluster basically cross product mapping like form clusterkey value basically key object full code full code working defines bunch consonant vector map stringifies somewhat human readable json toward bottom code block final result generated substitution mapping json result currently writing json file currently result mapping like full example analysis one seems correct mapped make sense exact match however find least like since nasal consonant sound extremely similar look like consonant cluster matching least value sure make sense end mind exactly final sum even brand new nlp ml algorithm trying wire together carefully rest consonant cluster probably make sense since probably good match though sure yet desired tinker phonetic relationship manually first labial consonant like value would expect slightly higher finally technically bilabial consonant fricative however like blowing wind lip whereas explosion sound plosive stream sound key character relate least far data turned bilabial fricative voiced bilabial fricative voiced bilabial plosive bilabial plosive bilabial nasal alveolar nasal retroflex nasal would expect like number wise personally subjectively would give value main thing technically bilabial audible relationship sound like sound f v basically question idea wrong seems feature vector mapping done onto map object going result desired relationship similarity sound like prop per vector seems like vector defined prop value result vector embedding basically respect fact property represent totally different thing confused ever get close resulting similarity mapping debug better might going wrong sample code currently spread across multiple module reality combined pasted code big snippet weighted jaccard calculation properly building vector properly could wrong even begin make feature assign floating point number value feature get sound subjectively think similar fact correlated number summary basic flow code define consonant vector forget vowel key ml feature variable boolean value reality gave value make sort weighted instead binary pretty sure step correctly map consonant cluster cross product mapping item calculate item consonant cluster vector union weighted jaccard sum later sure step properly taking calculated jaccard sum item mapped call create mapping source consonant cluster target cluster also sure step correct looking better understand going wrong calculating similarity consonant cluster using weight jaccard approach note got idea using jaccard sum paper arxiv org ab sure right even best approach overall problem question assume want use weight jaccard sum idea unless feel strongly otherwise update note plugging claude ai gave seems like helpful suggestion completely change base consonant vector system sort like sure system work might better slightly another way looking problem perhaps maybe better something like organize feature maybe getting unexpected result
How do I freeze only some embedding indices with tied embeddings?,"<p>I found in <a href=""https://stackoverflow.com/questions/54924582/is-it-possible-to-freeze-only-certain-embedding-weights-in-the-embedding-layer-i"">Is it possible to freeze only certain embedding weights in the embedding layer in pytorch?</a> a nice way to freeze only some indices of an embedding layer.
However, while including it in a BERT model, I cannot find a way to tie those embeddings. Can anyone help me?</p>
<p>HF Transformers <a href=""https://github.com/huggingface/transformers/blob/e782e95e3465b66a377c0a7fe95f8f10cd87459d/src/transformers/modeling_utils.py#L2002"" rel=""nofollow noreferrer"">uses</a> the copy of the embedding layer weights in the decoder matrix. However it is not possible to do it if my Embedding layer is a nn.Module instead of a nn.Embedding. Substituting the decoder layer with the custom module is not performing the weight transposition.</p>
",Vectorization & Embeddings,freeze embedding index tied embeddings found us copy embedding layer weight decoder matrix however possible embedding layer nn module instead nn embedding substituting decoder layer custom module performing weight transposition
"InvalidArgumentError: indices[120,0] = 3080 is not in [0, 32) [[{{node embedding_6/embedding_lookup}}]]","<p>I have seen others have posted similar questions. But the difference is I'm running a Keras Functional API instead of a sequential model.</p>

<pre><code>from keras.models import Model
from keras import layers
from keras import Input
text_vocabulary_size = 10000
question_vocabulary_size = 10000
answer_vocabulary_size = 500

text_input = Input(shape=(None,), dtype='int32', name='text')
embedded_text = layers.Embedding(64, text_vocabulary_size)(text_input)
encoded_text = layers.LSTM(32)(embedded_text)
question_input = Input(shape=(None,), dtype='int32', name='question')
embedded_question = layers.Embedding( 32, question_vocabulary_size)(question_input)
encoded_question = layers.LSTM(16)(embedded_question)

concatenated = layers.concatenate([encoded_text, encoded_question],axis=-1) 

## Concatenates the encoded question and encoded text

answer = layers.Dense(answer_vocabulary_size, activation='softmax')(concatenated)
model = Model([text_input, question_input], answer)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])
</code></pre>

<h2>Feeding data to a multi-input model</h2>

<pre><code>import numpy as np
num_samples = 1000
max_length = 100

text = np.random.randint(1, text_vocabulary_size, size=(num_samples, max_length))

question = np.random.randint(1, question_vocabulary_size,  size=(num_samples, max_length))

answers = np.random.randint(0, 1, size=(num_samples, answer_vocabulary_size)) 
</code></pre>

<h2>Fitting using a list of inputs</h2>

<pre><code>model.fit([text, question], answers, epochs=10, batch_size=128)
</code></pre>

<p>The error I get while trying into fitting the model is as follows.</p>

<pre><code>InvalidArgumentError: indices[120,0] = 3080 is not in [0, 32)
     [[{{node embedding_6/embedding_lookup}}]]
</code></pre>
",Vectorization & Embeddings,invalidargumenterror index node embedding embedding lookup seen others posted similar question difference running kera functional api instead sequential model feeding data multi input model fitting using list input error get trying fitting model follows
extracting word embeddings from log probabilities,"<p>I am solving an image captioning related issue and eventually I have extract the embeddings of the tokens. One possible way is to extract the embeddings using the tokens. But I cannot do do that because in my case the whole process needs to be differentiable and the tokens are not differentiable.</p>
<p>what I am currently doing is the following -</p>
<pre><code>embeddings = self.model.encoder_decoder.model.tgt_embed[0].lut
embedded_sentence_pred = torch.matmul(seq_logps, embeddings.weight)
</code></pre>
<p>I am not entirely sure if I am doing it correctly. Can you provide any insight?</p>
",Vectorization & Embeddings,extracting word embeddings log probability solving image captioning related issue eventually extract embeddings token one possible way extract embeddings using token case whole process need differentiable token differentiable currently following entirely sure correctly provide insight
How to use pretrained BERT word embedding vector to finetune (initialize) other networks?,"<p>When I used to do classification work with textcnn, I had experience finetuning textcnn using pretrained word embedding with like Word2Vec and fasttext. And I use this process:</p>
<ol>
<li>Create an embedding layer in textcnn</li>
<li>Load the embedding matrix of the words used this time by Word2Vec or
fasttext</li>
<li>Since the vector value of the embedding layer will change during training, the network is
being finetuning.</li>
</ol>
<p>Recently I also want to try BERT to do this. I thought, 'As there should be few differences to use BERT pretrained embedding to initial other networks' embedding layer and finetuning, it should be easy!' But in fact yesterday I tried all day and still cannot do it.<br>
The fact I found is that, as BERT's embedding is a contextual embedding, especially when extracting the word embeddings, the vector of each word from each sentence will vary, so it seems that there is no way to use that embedding to initialize the embedding layer of another network as usual...<br><br>
Finally, I thought up one method to 'finetuning', as the following steps:</p>
<ol>
<li>First, do not define an embedding layer in textcnn.</li>
<li>Instead of using embedding layer, in the network training part, I
firstly pass sequence tokens to the pretrained BERT model and get
the word embeddings for each sentence.</li>
<li>Put the BERT word embedding from 2. into textcnn and train the
textcnn network.</li>
</ol>
<p>By using this method I was finally able to train, but thinking seriously, I don't think I'm doing a finetuning at all...<br>
Because as you can see, every time when I start a new training loop, the word embedding generated from BERT is always the same vector, so just input these unchanged vectors to the textcnn wouldn't let the textcnn be finetuned at all, right?
<br><br>
<strong>UPDATE:</strong>
I thought up a new method to use the BERT embeddings and 'train' BERT and textcnn together.<br>
Some part of my code is:</p>
<pre class=""lang-py prettyprint-override""><code>    BERTmodel = AutoModel.from_pretrained('bert- 
                base-uncased',output_hidden_states=True).to(device)
    TextCNNmodel = TextCNN(EMBD_DIM, CLASS_NUM, KERNEL_NUM, 
                   KERNEL_SIZES).to(device)
    optimizer = torch.optim.Adam(TextCNNmodel.parameters(), lr=LR)
    loss_func = nn.CrossEntropyLoss()
</code></pre>
<pre class=""lang-py prettyprint-override""><code>  for epoch in range(EPOCH):
    TextCNNmodel.train()
    BERTmodel.train()
    for step, (token_batch, seg_batch, y_batch) in enumerate(train_loader):
        token_batch = token_batch.to(device)
        y_batch = y_batch.to(device)

        BERToutputs = BERTmodel(token_batch)
        # I want to use the second-to-last hidden layer as the embedding, so
        x_batch = BERToutputs[2][-2]

        output = TextCNNmodel(x_batch)
        output = output.squeeze()
        loss = loss_func(output, y_batch)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
</code></pre>
<p>I think by enable BERTmodel.train() and delete torch.no_grad() when get the embedding, the loss gradient could be backward to BERTmodel, too. The training process of TextCNNmodel also went smoothly. <br>
To use this model later, I saved the parameters of both TextCNNmodel and BERTmodel.<br>
Then to experiment whether the BERTmodel was really being trained and changed, in another program I load the BERTModel, and input a sentence to test that whether the BERTModel was really being trained. <br>
However, I found that the output (the embedding) of original 'bert-base-uncased' model and my 'BERTmodel' are the same, which is disappointing...<br>
I really have no idea why the BERTmodel part did not change...</p>
",Vectorization & Embeddings,use pretrained bert word embedding vector finetune initialize network used classification work textcnn experience finetuning textcnn using pretrained word embedding like word vec fasttext use process create embedding layer textcnn load embedding matrix word used time word vec fasttext since vector value embedding layer change training network finetuning recently also want try bert thought difference use bert pretrained embedding initial network embedding layer finetuning easy fact yesterday tried day still fact found bert embedding contextual embedding especially extracting word embeddings vector word sentence vary seems way use embedding initialize embedding layer another network usual finally thought one method finetuning following step first define embedding layer textcnn instead using embedding layer network training part firstly pas sequence token pretrained bert model get word embeddings sentence put bert word embedding textcnn train textcnn network using method wa finally able train thinking seriously think finetuning see every time start new training loop word embedding generated bert always vector input unchanged vector textcnn let textcnn finetuned right update thought new method use bert embeddings train bert textcnn together part code think enable bertmodel train delete torch grad get embedding loss gradient could backward bertmodel training process textcnnmodel also went smoothly use model later saved parameter textcnnmodel bertmodel experiment whether bertmodel wa really trained changed another program load bertmodel input sentence test whether bertmodel wa really trained however found output embedding original bert base uncased model bertmodel disappointing really idea bertmodel part change
Vector search to get a uniqueness score based on context,"<p>I have a single blog post with a title and description, and I want to compare its uniqueness against multiple blog entries in a CSV file. The CSV contains several blogs, each with a title and meta description.</p>
<p>I am currently using TF-IDF vectorization and cosine similarity to compare the single blog with all the entries in the CSV file. However, this approach only matches based on the exact words and not the context.</p>
",Vectorization & Embeddings,vector search get uniqueness score based context single blog post title description want compare uniqueness multiple blog entry csv file csv contains several blog title meta description currently using tf idf vectorization cosine similarity compare single blog entry csv file however approach match based exact word context
where can i download a pretrained word2vec map?,"<p>I have been learning about NLP models and came across word embedding, and saw the examples in which it is possible to see relations between words by calculating their dot products and such.</p>

<p>What I am looking for is just a dictionary, mapping words to their representative vectors, so I can play around with it. I know that I can build a model and train it and create my own map but I just want the already trained map as a python variable. </p>
",Vectorization & Embeddings,download pretrained word vec map learning nlp model came across word embedding saw example possible see relation word calculating dot product looking dictionary mapping word representative vector play around know build model train create map want already trained map python variable
Clustering for SBERT embedding,"<p>I have a set of sentences which I have transformed into vectors using SBERT embedding. I would like to cluster these vectors.</p>
<p>When looking for informations online, I keep seeing post telling to do some classical clustering, either using KMeans (which then use the Euclidean distance), or using (1-cosine) as the distance (eg nltk.cluster.cosine_distance() ) which is definitely not a distance.
I don't understand the validity of these methods: in the first case, the distance used is not a distance that is meaningful for similarity, and in the second case, it is not a distance.</p>
<p>Are these methods valid? If yes, why? And if not, what would be a good approach to cluster?</p>
",Vectorization & Embeddings,clustering sbert embedding set sentence transformed vector using sbert embedding would like cluster vector looking information online keep seeing post telling classical clustering either using kmeans use euclidean distance using cosine distance eg nltk cluster cosine distance definitely distance understand validity method first case distance used distance meaningful similarity second case distance method valid yes would good approach cluster
"Layer expects 2 input(s), but it received 1 input tensors","<p>I am trying to build model to predict posts likes, the model takes text and content type which is one hot encoded column.</p>
<p>I have made a TensorFlow dataset but when trying to fit the model I got this error:</p>
<pre><code>Layer &quot;functional_13&quot; expects 2 input(s), but it received 1 input tensors. 
Inputs received: [&lt;tf.Tensor 'data:0' shape=(None, 1000) dtype=int64&gt;]
</code></pre>
<p>Here is some snippets of my code:</p>
<pre><code>dataset = tf.data.Dataset.from_tensor_slices((vectorized_text,
                                             content,
                                             df['likes_rate']))

dataset= dataset.cache()
dataset= dataset.shuffle(160000)
dataset= dataset.batch(16)
dataset= dataset.prefetch(8)
</code></pre>
<p>This is my model</p>
<pre><code>from tensorflow.keras.layers import Input, Embedding, Concatenate,LSTM,Bidirectional
text_input= Input(shape=(1000,))
content_input=Input(shape=(3,))

text_embeddings = tf.keras.layers.Embedding(Max_Features+1, 32)(text_input)  # Adjust embedding dim
lstm= Bidirectional(LSTM(32,activation='tanh'))(text_embeddings)
# Concatenate text embeddings and content features
combined_features = tf.keras.layers.Concatenate()([lstm, content_input])

# Hidden layers (adjust number/activation functions)
x = tf.keras.layers.Dense(256, activation='relu')(combined_features)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dropout(0.1)(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
# Output layer for likes prediction
output = tf.keras.layers.Dense(1, activation='linear')(x)
</code></pre>
<p>I want to make an embedding layer to for the text then pass it to LSTM then combine the output of the LSTM and the contnent to Dense layers.</p>
<p>When trying to fit the model I am getting the problem above.</p>
<pre><code>model = tf.keras.models.Model(inputs=[text_input, content_input], outputs=output)
model.compile(loss='mse',optimizer='Adam')
model.fit(dataset,epochs=10)
</code></pre>
<p>If I iterate over the dataset. The code works correctly. But <code>.fit</code> every time make a random weights so the model make no progress.</p>
<pre><code>for text_batch, content_batch, y_batch in dataset:
    # Train model on the current batch
    model.fit(x=[text_batch, content_batch], y=y_batch)
</code></pre>
",Vectorization & Embeddings,layer expects input received input tensor trying build model predict post like model take text content type one hot encoded column made tensorflow dataset trying fit model got error snippet code model want make embedding layer text pas lstm combine output lstm contnent dense layer trying fit model getting problem iterate dataset code work correctly every time make random weight model make progress
"Input 0 of layer &quot;lstm_1&quot; is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 256)","<p>I am trying to build a generative LSTM model using tensorflow2. I am new to using LSTM layer in tensorflow. the code is given below::</p>
<pre><code>vec = layers.TextVectorization(output_sequence_length=maxlen,
                               max_tokens=379)
vec.adapt(use_for_vocab(train)) 
# here i just convert the train dataset into a form suitable to use for vectorization 

voc = vec.get_vocabulary()
voc_size = len(voc)

embed = layers.Embedding(input_dim=voc_size,
                         output_dim=256,
                         mask_zero=True)

inp_word = layers.Input(shape=(maxlen+2,), # maxlen is the maximum length of the sentence in the text
                   name=&quot;word_input&quot;)      # 2 is added to accommodate start_token and end_token
x_word = embed(inp_word)
x_word = layers.Dropout(0.5)(x_word)
x_word = layers.LSTM(256, return_sequences=True)(x_word)
ops_word = layers.GlobalAveragePooling1D(name=&quot;word_gap&quot;)(x_word)

</code></pre>
<p>Here is the model summary:</p>
<p><strong>Model: &quot;word_model&quot;</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Layer (type)</th>
<th>Output Shape</th>
<th>Param #</th>
</tr>
</thead>
<tbody>
<tr>
<td>word_input (InputLayer)</td>
<td>[(None, 35)]</td>
<td>0</td>
</tr>
<tr>
<td>embedding_1 (Embedding)</td>
<td>(None, 35, 128)</td>
<td>45184</td>
</tr>
<tr>
<td>dropout_6 (Dropout)</td>
<td>(None, 35, 128)</td>
<td>0</td>
</tr>
<tr>
<td>lstm_5 (LSTM)</td>
<td>(None, 35, 256)</td>
<td>394240</td>
</tr>
<tr>
<td>word_gap (GlobalAveragePooling1D)</td>
<td>(None, 256)</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>Total params: 439,424
Trainable params: 439,424
Non-trainable params: 0</p>
<hr />
<p>I have created a prefetched dataset for input. This is how I have built it using the function below:</p>
<pre><code>from tensorflow.data import Dataset, AUTOTUNE

def format_dataset(x, y):
  y = Dataset.from_tensor_slices(y)
  data = Dataset.zip((x, y))
  return data.batch(32).prefetch(AUTOTUNE)
</code></pre>
<p>I am able to compile the model without any error. But, while fitting the model, it is giving the following error:</p>
<pre><code> File &quot;/usr/local/lib/python3.9/dist-packages/keras/engine/training.py&quot;, line 1284, in train_function  *
        return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.9/dist-packages/keras/engine/training.py&quot;, line 1268, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.9/dist-packages/keras/engine/training.py&quot;, line 1249, in run_step  **
        outputs = model.train_step(data)
    File &quot;/usr/local/lib/python3.9/dist-packages/keras/engine/training.py&quot;, line 1050, in train_step
        y_pred = self(x, training=True)
    File &quot;/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py&quot;, line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File &quot;/usr/local/lib/python3.9/dist-packages/keras/engine/input_spec.py&quot;, line 235, in assert_input_compatibility
        raise ValueError(

    ValueError: Exception encountered when calling layer 'caption_model' (type Functional).
    
    Input 0 of layer &quot;lstm_5&quot; is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128)

</code></pre>
<p>Cannot understand why input sequence still is two-dimensional in spite of making return_sequences true. Any help will be appreciated.</p>
",Vectorization & Embeddings,input layer lstm incompatible layer expected ndim found ndim full shape received none trying build generative lstm model using tensorflow new using lstm layer tensorflow code given model summary model word model layer type output shape param word input inputlayer none embedding embedding none dropout dropout none lstm lstm none word gap globalaveragepooling none total params trainable params non trainable params created prefetched dataset input built using function able compile model without error fitting model giving following error understand input sequence still two dimensional spite making return sequence true help appreciated
How does OpenAIEmbeddings() work? Is it creating a single vector of size 1536 for whole text corpus?,"<p>I'm working with the <code>OpenAIEmbeddings()</code> class from <code>OpenAI</code>, which uses the <code>text-embedding-3-small</code> model. According to the <a href=""https://platform.openai.com/docs/guides/embeddings/what-are-embeddings"" rel=""nofollow noreferrer"">documentation</a>, it generates a 1536-dimensional vector for any input text.</p>
<p>However, I'm a bit confused about how this works:</p>
<ul>
<li>Is the 1536-dimensional vector generated for the entire input text?</li>
<li>If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs?</li>
</ul>
<p><strong>I was expecting this:</strong></p>
<p>If there are 100 words in my input text, i expected that OpenAIEmbeddings() would output 100 vectors, each having size 1536.</p>
<p>But the output is a single vector of size 1536 for the whole input text.</p>
<p>Why I expected this?</p>
<p>Because in my learning, i've understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus. How does this differ from the approach taken by OpenAIEmbeddings?</p>
<p>I'm trying to understand whether there's a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input.</p>
<p>Any insights or examples would be greatly appreciated!</p>
",Vectorization & Embeddings,doe openaiembeddings work creating single vector size whole text corpus working class us model according documentation generates dimensional vector input text however bit confused work dimensional vector generated entire input text dimensional vector represents entire input text doe model handle individual word versus longer text like sentence paragraph wa expecting word input text expected openaiembeddings would output vector size output single vector size whole input text expected learning understood embeddings like word vec glove provide vector word corpus doe differ approach taken openaiembeddings trying understand whether way extract embeddings individual word using model output always single vector representing whole input insight example would greatly appreciated
Error when using inputs_embeds with generate method,"<p>I'm encountering a problem when trying to use inputs_embeds to pass the embedding to my model:</p>
<pre><code>ValueError: You passed `inputs_embeds` to `.generate()`, but the model class LlamaForCausalLM doesn't have its forwarding implemented. See the GPT2 implementation for an example (https://github.com/huggingface/transformers/pull/21405), and feel free to open a PR with it!
</code></pre>
<p>Can someone help me understand what is going on and how to fix this issue?</p>
<p>I have an embedding with the shape <code>torch.Size([1, 46, 4096])</code> and I want to pass it to my model. Here is my code:</p>
<pre><code>if True:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = &quot;/content/drive/My Drive/finetuneunslothllama&quot;, 
        max_seq_length = max_seq_length,`your text`
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model) # Activer native 2x faster inference

outputs = model.generate(inputs_embeds=embeddings, max_new_tokens=64, use_cache=True)

generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)
print(generated_text)
</code></pre>
",Vectorization & Embeddings,error using input embeds generate method encountering problem trying use input embeds pas embedding model someone help understand going fix issue embedding shape want pas model code
Why Is My Skip-Gram Implementation Producing Incorrect Results?,"<p>I'm implementing a Skip-Gram model for Word2Vec using Python. However, my model doesn't seem to be working correctly, as indicated by the resulting embeddings and their visualization. Here is an example of the 3D plot of the embeddings, which shows words clustered together and overlapping, making it difficult to distinguish between them:</p>
<p>I suspect that the issue lies in my implementation rather than the plotting function.</p>
<pre><code>import numpy as np
from nltk.corpus import stopwords
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import re

np.random.seed(10)

def softmax(x):
    '''
                 (xi - max{x1,x2,...,xv})
                e
    xi =    --------------
                  (xj - max{x1,x2,...,xv})
             ∑j  e
    '''
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()

class SkipGram:
    def __init__(self,ws=2,dim=8) -&gt; None:
        self.X = []
        self.N = dim
        self.Y = []
        self.window_size = ws
        self.alpha = 0.1
        self.vocab = {}
        self.vocab_size = 0
        
    def __create_vocabulary(self,corpus):
        stop_words = set(stopwords.words(&quot;english&quot;))
        filtered_corpus = []
        self.vocab_size = 0
        for i,sentence in enumerate(corpus):
            if isinstance(sentence,str) :
                corpus[i] = sentence.split()
            filtered_corpus.append([])
            j = 0
            for word in corpus[i]:
                w = re.sub(r'[^a-z]+','',word.lower())
                if w != '' and w not in stop_words:
                    corpus[i][j] = w
                    filtered_corpus[i].append(w)
                else:
                    continue
                if corpus[i][j].lower() not in self.vocab:
                    self.vocab[corpus[i][j].lower()] = self.vocab_size
                    self.vocab_size += 1
                j += 1
        return filtered_corpus
        
    def __create_context_and_center_words(self,processed_corpus):
        for sentence in processed_corpus:
            for i,word in enumerate(sentence):
                center_word = np.zeros((self.vocab_size,1))
                center_word[self.vocab[word]][0] = 1
                context = np.zeros((self.vocab_size,1))
                
                for j in range(i-self.window_size,i + self.window_size + 1):
                    if j != i and j &gt;= 0 and j &lt; len(sentence):
                        context[self.vocab[sentence[j]]][0] += 1
                self.X.append(center_word)
                self.Y.append(context)
        self.X = np.array(self.X)
        self.Y = np.array(self.Y)
                
        
    def initialize(self,corpus):
        corpus = self.__create_vocabulary(corpus)
        self.__create_context_and_center_words(corpus)
        self.W1 = np.random.rand(self.vocab_size,self.N)
        self.W2 = np.random.rand(self.N,self.vocab_size)
    
    def feed_forward(self,x):
        h = np.dot(self.W1.T,x) # N V . V 1 -&gt; N 1
        u = np.dot(self.W2.T,h) # V N . N 1 -&gt; V 1
        y = softmax(u)
        return h,u,y
    
    def backpropagate(self,x,y_actual,y_result,h):
        e = y_result - y_actual # V 1
        dw2 = np.dot(h,e.T) # N 1 . 1 V -&gt;  N V
        eh = np.dot(self.W2,e) # N x V . V x 1 -&gt;  N x 1
        dw1 = np.dot(x,eh.T) # V x 1 . 1 x N -&gt; V x N
        return dw1,dw2
        
    def train(self,epochs):
        for i in range(epochs):
            loss = 0
            dw1,dw2 = np.zeros_like(self.W1),np.zeros_like(self.W2)
            for j in range(len(self.X)):
                h,_,y = self.feed_forward(self.X[j])
                a,b = self.backpropagate(self.X[j],self.Y[j],y,h)
                dw1 += a
                dw2 += b
                loss -=  np.sum(self.Y[j] * np.log(y+1e-08))
            loss /= len(self.X)
            [dw1,dw2] = [dw1/len(self.X), dw2/len(self.X)]
            self.W1 -= self.alpha * dw1
            self.W2 -= self.alpha * dw2
            print(f'Epoch : {i+1}, Loss = {loss}')
            
    def get_similar_words(self,word,n):
        if word in self.vocab:
            x = np.zeros((self.vocab_size,))
            x[self.vocab[word]] = 1
            _,_,y = self.feed_forward(x)
            output = {}
            for i in range(self.vocab_size):
                output[y[i]] = i
            words = {i:word for i,word in enumerate(self.vocab.keys())}
            context = []
            for k in sorted(output,reverse=True):
                context.append(words[output[k]])
                if len(context) == n:
                    break
            return context
        else:
            print(&quot;Given Word not found&quot;)
            
    def get_vector(self,word):
        return self.W1[self.vocab[word]]
            
    def plot(self):
        tsne = TSNE(n_components=3,random_state=0,perplexity=self.vocab_size-1)
        vectors_3d = tsne.fit_transform(self.W1)
        fig = plt.figure(figsize=(12,8))
        ax = fig.add_subplot(111,projection='3d')
        ax.scatter(vectors_3d[:,0],vectors_3d[:,1],vectors_3d[:,2],marker='o',edgecolors='k')
        for word,i in self.vocab.items():
            ax.text(vectors_3d[i,0],vectors_3d[i,1],vectors_3d[i,2],word)
        ax.set_title('Word2Vec Word Embeddings')
        ax.set_xlabel('Dimension 1')
        ax.set_ylabel('Dimension 2')
        ax.set_zlabel('Dimension 3')
        plt.show()
        
</code></pre>
<pre><code>#main.py
from nltk.corpus import gutenberg

corpus = gutenberg.sents()[:40]
w2v = SkipGram(3,20)
w2v.initialize(corpus)
w2v.train(200)
w2v.plot()
</code></pre>
<p><img src=""https://i.sstatic.net/TCoZiNJj.png"" alt=""Output of my model"" /></p>
<p>I have tried adjusting the learning rate and initializing weights with different values, but the issue persists.</p>
<p>What might be going wrong with my implementation?</p>
<p>Reviewed the code for generating vocabulary and context words.
Checked the weight initialization and learning rate settings.</p>
",Vectorization & Embeddings,skip gram implementation producing incorrect result implementing skip gram model word vec using python however model seem working correctly indicated resulting embeddings visualization example plot embeddings show word clustered together overlapping making difficult distinguish suspect issue lie implementation rather plotting function tried adjusting learning rate initializing weight different value issue persists might going wrong implementation reviewed code generating vocabulary context word checked weight initialization learning rate setting
Hugging Face Transformers embedding layer position and token_type embeddings,"<p>I am trying to evaluate different Integrated Gradients methods on my RoBERTa based model, and I came to a paper introducing &quot;Sequential Integrated Gradients&quot; with this github repo:
<a href=""https://github.com/josephenguehard/time_interpret/blob/main/experiments/nlp/utils.py"" rel=""nofollow noreferrer"">text</a></p>
<p>I understand to get the IGs we need to pass input_embeddings not ids to the model so it's able to compute gradients but in the given code the author is producing the embeddings like:</p>
<p><code>getattr(model, model_name).embeddings.word_embeddings(input_ids)</code></p>
<p>and later generate positional and type embeddings (also attention mask) manually and finally summing them and applying the LayerNorm and dropout layers to pass to the model using this wrapper:</p>
<pre><code>class ForwardModel(nn.Module):
    def __init__(self, model, model_name):
        super().__init__()
        self.model = model
        self.model_name = model_name

    def forward(
        self,
        input_embed,
        attention_mask=None,
        position_embed=None,
        type_embed=None,
        return_all_logits=False,
    ):
        embeds = input_embed + position_embed
        if type_embed is not None:
            embeds += type_embed

        # Get predictions
        embeds = getattr(self.model, self.model_name).embeddings.dropout(
            getattr(self.model, self.model_name).embeddings.LayerNorm(embeds)
        )
        pred = self.model(
            inputs_embeds=embeds,
            attention_mask=attention_mask,
        )[0]

        # Return all logits or just maximum class
        if return_all_logits:
            return pred
        else:
            return pred.max(1).values
</code></pre>
<p>however when I pass the model just &quot;embeds&quot; itself using argument &quot;inputs_embeds&quot; I get the same output as passing &quot;input_ids&quot; using &quot;input_ids&quot; argument, it means the model handles the position and type embeddings itself. So adding them manually to input_embeddings brings a different output. It seems they did it maybe because the model bypasses those embeddings if the input_embeddings instead of ids are passed. Can someone clarify?</p>
",Vectorization & Embeddings,hugging face transformer embedding layer position token type embeddings trying evaluate different integrated gradient method roberta based model came paper introducing sequential integrated gradient github repo text understand get ig need pas input embeddings id model able compute gradient given code author producing embeddings like later generate positional type embeddings also attention mask manually finally summing applying layernorm dropout layer pas model using wrapper however pas model embeds using argument input embeds get output passing input id using input id argument mean model handle position type embeddings adding manually input embeddings brings different output seems maybe model bypass embeddings input embeddings instead id passed someone clarify
word-embedding: Convert supervised model into unsupervised model,"<p>I want to load an pre-trained embedding to initialize my own unsupervise FastText model and retrain with my dataset.</p>
<p>The trained embedding file I have loads fine with <code>gensim.models.KeyedVectors.load_word2vec_format('model.txt')</code>. But when I try:
<code>FastText.load_fasttext_format('model.txt')</code> I get: <code>NotImplementedError: Supervised fastText models are not supported</code>.</p>
<p>Is there any way to convert supervised KeyedVectors to unsupervised FastText? And if possible, is it a bad idea?</p>
<p>I know that has an great difference between supervised and unsupervised models. But I really wanna try use/convert this and retrain it. I'm not finding a trained unsupervised model to load for my case (it's a portuguese dataset), and the best model I find <a href=""http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc"" rel=""nofollow noreferrer"">is that</a></p>
",Vectorization & Embeddings,word embedding convert supervised model unsupervised model want load pre trained embedding initialize unsupervise fasttext model retrain dataset trained embedding file load fine try get way convert supervised keyedvectors unsupervised fasttext possible bad idea know ha great difference supervised unsupervised model really wan na try use convert retrain finding trained unsupervised model load case portuguese dataset best model find
Difference between CBOW and Skipgram gradients in word2vec?,"<p>Why are <code>f</code> values that are greater than or lower than <code>MAX_EXP</code> taken into account during the updates in CBOW, but ignored in Skipgram? </p>

<p>I'm specifically looking at the Google implementation of word2vec, but the same functionality has been replicated throughout many other projects, one of which is <a href=""https://github.com/dav/word2vec/blob/master/src/word2vec.c"" rel=""nofollow noreferrer"">here</a>, for larger context.</p>

<pre><code>// CBOW negative sampling gradient calculations  
f = 0;
l2 = target * layer1_size;
for (c = 0; c &lt; layer1_size; c++) f += neu1[c] * syn1neg[c + l2];
// ** here, we still update, but essentially round the value to 1 or 0
if (f &gt; MAX_EXP) g = (label - 1) * alpha;
else if (f &lt; -MAX_EXP) g = (label - 0) * alpha;
else g = (label - expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;

// ---------------------------

// Skipgram hierarchical softmax gradient calculations
f = 0;
l2 = vocab[word].point[d] * layer1_size;
for (c = 0; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1[c + l2];
// ** here, we don't update if f is outside the range given by MAX_EXP **
if (f &lt;= -MAX_EXP) continue;
else if (f &gt;= MAX_EXP) continue;
else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];
g = (1 - vocab[word].code[d] - f) * alpha;
</code></pre>
",Vectorization & Embeddings,difference cbow skipgram gradient word vec value greater lower taken account update cbow ignored skipgram specifically looking google implementation word vec functionality ha replicated throughout many project one larger context
"How can I convert user query to sql for a search engine to get exact matching documents from db, the query is in japanese","<p>I am using tfidf as a search algorithm but tfidf doesn't give exact matches so could you suggest a different algo  that can match exactly with the database .With tfidf when we search for show me domestic stocks domestic bonds also come up in the search and I want only domestic stocks in answers.</p>
<p>We have tried tfidf also llm for our use case but llm also gets confused and gives extra filters in answers</p>
",Vectorization & Embeddings,convert user query sql search engine get exact matching document db query japanese using tfidf search algorithm tfidf give exact match could suggest different algo match exactly database tfidf search show domestic stock domestic bond also come search want domestic stock answer tried tfidf also llm use case llm also get confused give extra filter answer
Classifying texts using word embeddings,"<p>I'm trying to train a model that is able to classify short texts (200-600 words per text). I got a training set with their corresponding labels, and a text might have one or more labels.</p>
<p>My first approach was to use TF-IDF with Naive Bayes but it wasn't performing very well. But after replacing Naive Bayes with a logistic regeression, the results improved a bit.</p>
<p>However, what are some of the more state-of-the-art methods that I could try? For example, can a text embedding model from OpenAI be of use? Compared to TF-IDF, embeddings would be able to handle synonyms, etc. But I'm struggling to put it all together, how should the vector (or vectors) for each text be calculated? It feels like embedding are better suited for sentence length classification or comparisons.</p>
",Vectorization & Embeddings,classifying text using word embeddings trying train model able classify short text word per text got training set corresponding label text might one label first approach wa use tf idf naive bayes performing well replacing naive bayes logistic regeression result improved bit however state art method could try example text embedding model openai use compared tf idf embeddings would able handle synonym etc struggling put together vector vector text calculated feel like embedding better suited sentence length classification comparison
CUDA error: device-side assert triggered Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions,"<p>I am trying to convert my text into its embeddings using a bert model , when i apply this to my dataset it works fine for some of my inputs then stops and gives that error</p>
<p>I have set TORCH_USE_CUDA_DSA and CUDA_LAUNCH_BLOCKING to 1 and the inputs are not exceeding the number of token limit also and this is my embedding code i have tried to free the memory of my gpu and still it didn't work.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import numpy as np

def embeddings(text):
    if len(text) &gt; 4000:
        flag = text.split(&quot;.&quot;)
        t1 = flag[:len(flag) // 2]
        t2 = flag[len(flag) // 2:]
        t1 = &quot;.&quot;.join(t1)
        t2 = &quot;.&quot;.join(t2)
        emb_avg = np.mean([embeddings(t1), embeddings(t2)], axis=0)
        return emb_avg
    else:
        with torch.no_grad(): 
            encoded_input = tokenizer(text, return_tensors='pt')
            encoded_input.to(device)

            output = model(**encoded_input)
            emb = output.encoder_last_hidden_state
            emb_np = emb.cpu().numpy()  
           
            del emb
            del output
            del encoded_input
            gc.collect()
            
            torch.cuda.empty_cache()  
        emb_avg = np.mean(emb_np, axis=1)
        emb_avg = emb_avg.flatten()
        torch.cuda.empty_cache()
        return emb_avg
</code></pre>
<p>and I'm applying to my data set</p>
<pre><code>from tqdm.auto import tqdm
tqdm.pandas()  
df['emb'] = df['abstract'].progress_apply(embeddings)
</code></pre>
",Vectorization & Embeddings,cuda error device side assert triggered compile enable device side assertion trying convert text embeddings using bert model apply dataset work fine input stop give error set torch use cuda dsa cuda launch blocking input exceeding number token limit also embedding code tried free memory gpu still work applying data set
Transformer models for contextual word embedding in large datasets,"<p>I'm interested in using contextual word embeddings generated by a transformer-based model to explore the similarity of certain words in a large dataset.</p>
<p>Most transformer models only allow up to 512 tokens of input. So presumably I would need to break the dataset down into individual sentences (&lt;512t) &amp; feed them into the model. That would give me a list of word embeddings per sentence. I'm struggling to understand how I could best translate this list into meaningful embeddings per word across the whole dataset.</p>
<p>The immediately obvious approach would be to find the average embedding for each word. However, the point of contextual embedding is that it can identify different uses/meanings for the same word. 'Bank' as a noun and 'bank' as an adjective may have very different embeddings and therefore I'm not sure that the average would have a great deal of meaning. Is this a genuine concern? Is there a better approach?</p>
<p>In such a use case is there any value in using a contextual transformer model over a static one?</p>
",Vectorization & Embeddings,transformer model contextual word embedding large datasets interested using contextual word embeddings generated transformer based model explore similarity certain word large dataset transformer model allow token input presumably would need break dataset individual sentence feed model would give list word embeddings per sentence struggling understand could best translate list meaningful embeddings per word across whole dataset immediately obvious approach would find average embedding word however point contextual embedding identify different us meaning word bank noun bank adjective may different embeddings therefore sure average would great deal meaning genuine concern better approach use case value using contextual transformer model static one
BERT embedding cosine similarities look very random and useless,"<p>I thought you can use BERT embeddings to determine semantic similarity. I was trying to group some words in categories using this, but the results were very bad.</p>
<p>E.g. here is a small example with animals and fruits. Notice that the highest similarity is between cat and banana?</p>
<pre><code>import torch
from transformers import BertTokenizer, BertModel
from sklearn.metrics.pairwise import cosine_similarity

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True).eval()

def gen_embedding(word):
    encoding = tokenizer(word, return_tensors='pt')
    with torch.no_grad():
        outputs = model(**encoding)

    token_embeddings = outputs.last_hidden_state.squeeze()
    token_embeddings = token_embeddings[1 : -1]
    word_embedding = token_embeddings.mean(dim=0)
    return word_embedding

words = [
    'cat',
    'seagull',
    'mango',
    'banana'
]

embs = [gen_embedding(word) for word in words]

print(cosine_similarity(embs))

# array([[1.        , 0.33929926, 0.7086487 , 0.79372996],
#        [0.33929926, 1.0000001 , 0.29915804, 0.4000572 ],
#        [0.7086487 , 0.29915804, 1.        , 0.7659105 ],
#        [0.79372996, 0.4000572 , 0.7659105 , 0.99999976]], dtype=float32)
</code></pre>
<p>Am I doing something wrong?</p>
",Vectorization & Embeddings,bert embedding cosine similarity look random useless thought use bert embeddings determine semantic similarity wa trying group word category using result bad e g small example animal fruit notice highest similarity cat banana something wrong
BartForConditionalGeneration: Adding additional layers of embedding,"<p>In this file:</p>
<pre><code>https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/modeling_bart.py
</code></pre>
<p>I think-</p>
<blockquote>
<p>Inside the BartEncoder, on this <a href=""https://github.com/huggingface/transformers/blob/e0c3cee17085914bbe505c159beeb8ae39bc37dd/src/transformers/models/bart/modeling_bart.py#L1166"" rel=""nofollow noreferrer"">line</a>, I have to add other embeddings</p>
</blockquote>
<p>Could anyone tell me - are there any other places I need to modify to add additional layers of embedding?
Could anyone tell me - are there any other places I need to modify to add additional layers of embedding (e.g., 2 other embedding with positional embedding)?</p>
",Vectorization & Embeddings,bartforconditionalgeneration adding additional layer embedding file think inside bartencoder line add embeddings could anyone tell place need modify add additional layer embedding could anyone tell place need modify add additional layer embedding e g embedding positional embedding
Vector Embedding using Spark for compute,"<p>I have some large parquet files of data in <code>Iceberg</code> (which I have stored using <code>Spark</code>). My objective now is to pull these down using <code>Spark</code>, convert them into a <code>spark dataframe</code>, perform <em>vector embedding</em> to transform the dataframe into a new dataframe with the <em>embedded vector columns</em>, and then store this vector-column into a vector database like <code>qdrant</code>.</p>
<p>I have had problems making things work so far, and online documentation on this specific topic is limited. I tried <code>Spark NLP</code>, but it appears incompatible with the <code>qdrant-spart connector</code> I used to allow qdrant to be a target for Spark. So I guess I am looking for what the conventional way is to do the following two:</p>
<ol>
<li>Perform vector embedding on a Spark dataframe using a model like BERT (Word2Vec is insufficient for my needs), extending the dataframe with a vector column.</li>
<li>Take the produces vector-embeddings column and store it in a vector database like qdrant.</li>
</ol>
<p>I feel like the <em>distributed</em> nature of Spark is a big obstacle here.</p>
",Vectorization & Embeddings,vector embedding using spark compute large parquet file data stored using objective pull using convert perform vector embedding transform dataframe new dataframe embedded vector column store vector column vector database like problem making thing work far online documentation specific topic limited tried appears incompatible used allow qdrant target spark guess looking conventional way following two perform vector embedding spark dataframe using model like bert word vec insufficient need extending dataframe vector column take produce vector embeddings column store vector database like qdrant feel like distributed nature spark big obstacle
Generating Vector Embeddings for Organization Names,"<p>I have seen couple of Word2Vec Models that can generate embeddings for Company Names, and performs well when the different formats of the same company names are given.
But what I want to do is a bit different. For example, I have a list of company names like: [&quot;abc informatics&quot;, &quot;xyz communications&quot;, &quot;intra soft&quot;, &quot;gigabyte&quot; ]
Now, if a new company name comes up I want to check if it already matches with the existing company names by a threshold of 80% (probably through cosine similiarity or any other approach). Since the embedding models are trained on international companies it kind of performs poorly for local companies. Another problem is Word2Vec reflects on semantics while generating embeddings, for example &quot;Plants ltd&quot; and &quot;Trees Ltd&quot; will generate similiar embedings, but in reality both of them are quite different from one another!!!</p>
<p>I am open to any other solutions if embedding similiarity search doesnot work well.</p>
<p>This question is probably a duplicate to <a href=""https://stackoverflow.com/questions/77305303/create-embeddings-for-string-matching"">Create embeddings for string matching</a> , but since it didnt receieve any good answers I am asking the question here anyways.</p>
",Vectorization & Embeddings,generating vector embeddings organization name seen couple word vec model generate embeddings company name performs well different format company name given want bit different example list company name like abc informatics xyz communication intra soft gigabyte new company name come want check already match existing company name threshold probably cosine similiarity approach since embedding model trained international company kind performs poorly local company another problem word vec reflects semantics generating embeddings example plant ltd tree ltd generate similiar embedings reality quite different one another open solution embedding similiarity search doesnot work well question probably duplicate href embeddings string matching since didnt receieve good answer asking question anyways
Speed up semantic search for large dataset with custom search function,"<p>I am building a semantic search engine for short sentences where I want to know which sentence have the more similar sentences. For that, I need to know based on a threshold on the similarity score, how many similar sentences I have for each sentence, and so I need an almost complete view of the similarities between all the sentence.</p>
<p>Given the quality of my free text, I'm not able to achieve good performances by only using a classical transformer embedding. To get the best out of my data composed both of free texts and metadata, I have come up with an hybrid solution using sentence transformer embedding, TD-IDF embedding, and an encoding of my metadata.</p>
<p>Once the free texts is embedded, I perform a custom search function where I use weights to combine the cosine similarity between the sentence transformer embeddings, the cosine similarity between the TF-IDF embedding, and a pairwise matching between my encoding of metadata.</p>
<p>However, my dataset is quite large with more than 500k documents. The compute for the embeddings is not a problem in terms of ressources, but the search function is. As I'm not able to store a 500k*500k matrix containing all the similarity scores between all my documents, I have tried to do it by smaller batches, for example computing 100*500k matrix one after the other. This solve the memory issue, but the processing time is really high.</p>
<p>I have looked a bit about packages libraries such as chromadb and faiss, but it seems that they don't provide custom search features.</p>
<p>Any idea?</p>
",Vectorization & Embeddings,speed semantic search large dataset custom search function building semantic search engine short sentence want know sentence similar sentence need know based threshold similarity score many similar sentence sentence need almost complete view similarity sentence given quality free text able achieve good performance using classical transformer embedding get best data composed free text metadata come hybrid solution using sentence transformer embedding td idf embedding encoding metadata free text embedded perform custom search function use weight combine cosine similarity sentence transformer embeddings cosine similarity tf idf embedding pairwise matching encoding metadata however dataset quite large k document compute embeddings problem term ressources search function able store k k matrix containing similarity score document tried smaller batch example computing k matrix one solve memory issue processing time really high looked bit package library chromadb faiss seems provide custom search feature idea
Llama Index custom embeddings - difference between getting text embeddings vs query embeddings?,"<p>I'm looking here at the Llama index documentation to create custom embeddings: <a href=""https://docs.llamaindex.ai/en/stable/examples/embeddings/custom_embeddings.html"" rel=""nofollow noreferrer"">https://docs.llamaindex.ai/en/stable/examples/embeddings/custom_embeddings.html</a></p>
<p>I'm confused at the documentation - what's the difference betweeen the two methods <code>_get_query_embedding()</code> and <code>_get_text_embedding</code>? They look exactly the same to me</p>
<pre><code>from typing import Any, List
from InstructorEmbedding import INSTRUCTOR
from llama_index.embeddings.base import BaseEmbedding


class InstructorEmbeddings(BaseEmbedding):
    def __init__(
        self,
        instructor_model_name: str = &quot;hkunlp/instructor-large&quot;,
        instruction: str = &quot;Represent the Computer Science documentation or question:&quot;,
        **kwargs: Any,
    ) -&gt; None:
        self._model = INSTRUCTOR(instructor_model_name)
        self._instruction = instruction
        super().__init__(**kwargs)

        def _get_query_embedding(self, query: str) -&gt; List[float]:
            embeddings = self._model.encode([[self._instruction, query]])
            return embeddings[0]

        def _get_text_embedding(self, text: str) -&gt; List[float]:
            embeddings = self._model.encode([[self._instruction, text]])
            return embeddings[0]

        def _get_text_embeddings(self, texts: List[str]) -&gt; List[List[float]]:
            embeddings = self._model.encode(
                [[self._instruction, text] for text in texts]
            )
            return embeddings
</code></pre>
",Vectorization & Embeddings,llama index custom embeddings difference getting text embeddings v query embeddings looking llama index documentation create custom embeddings confused documentation difference betweeen two method look exactly
"Incorrect similarity by Embeddings, How to fix it","<p>I have one problem with texts similarities.</p>
<p>I have a query</p>
<pre><code>&quot;eight point two&quot;
</code></pre>
<p>and when i make vector search, i get such results</p>
<pre><code>sentences = [test]
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

embeddings = model.encode(sentences)
res = client.search(
    collection_name=&quot;test3&quot;,
    search_params=models.SearchParams(hnsw_ef=128, exact=False),
    query_vector=embeddings[0],
)
res


[ScoredPoint(score=0.74708945,'vector_data': 'eight point two'},
ScoredPoint(score=0.7437991,, 'vector_data': 'two point eight'}),  
ScoredPoint(score=0.70646083,  'vector_data': eight point two p1 '}, vector=None),
</code></pre>
<p>how to take into account word order?
i want to see</p>
<pre><code>'eight point two'
'eight point two p1 '
'two point eight'
</code></pre>
",Vectorization & Embeddings,incorrect similarity embeddings fix one problem text similarity query make vector search get result take account word order want see
What does GlobalAveragePooling1D do in keras?,"<p>In the embedding example here:
<a href=""https://www.tensorflow.org/text/guide/word_embeddings"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/guide/word_embeddings</a></p>
<pre><code>result = embedding_layer(tf.constant([[0, 1, 2], [3, 4, 5]]))
result.shape
TensorShape([2, 3, 5])
</code></pre>
<p>Then it explains:</p>
<blockquote>
<p>When given a batch of sequences as input, an embedding layer returns a 3D floating point tensor, of shape (samples, sequence_length, embedding_dimensionality). To convert from this sequence of variable length to a fixed representation there are a variety of standard approaches. You could use an RNN, Attention, or pooling layer before passing it to a Dense layer. This tutorial uses pooling because it's the simplest.</p>
<p>The GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.</p>
</blockquote>
<p>Then the code:</p>
<pre><code>embedding_dim=16

model = Sequential([
  vectorize_layer,
  Embedding(vocab_size, embedding_dim, name=&quot;embedding&quot;),
  GlobalAveragePooling1D(),
  Dense(16, activation='relu'),
  Dense(1)
])
</code></pre>
<p>The GlobalAveragePooling1D should calculate a single integer for each word's embedding of dimension = n. I don't understand this part:</p>
<blockquote>
<p>This allows the model to handle input of variable length, in the simplest way possible.</p>
</blockquote>
<p>Similarly:</p>
<blockquote>
<p>To convert from this sequence of variable length to a fixed representation there are a variety of standard approaches.</p>
</blockquote>
<p>In each embedding layer, input length is already fixed by the parameter 'input_length'. Truncation and padding are used to ensure the fixed length of the input. So what does it mean by saying GlobalAveragePooling1D is used to convert from this sequence of variable length to a fixed representation? What does the 'variable length' mean here?</p>
",Vectorization & Embeddings,doe globalaveragepooling kera embedding example explains given batch sequence input embedding layer return floating point tensor shape sample sequence length embedding dimensionality convert sequence variable length fixed representation variety standard approach could use rnn attention pooling layer passing dense layer tutorial us pooling simplest globalaveragepooling layer return fixed length output vector example averaging sequence dimension allows model handle input variable length simplest way possible code globalaveragepooling calculate single integer word embedding dimension n understand part allows model handle input variable length simplest way possible similarly convert sequence variable length fixed representation variety standard approach embedding layer input length already fixed parameter input length truncation padding used ensure fixed length input doe mean saying globalaveragepooling used convert sequence variable length fixed representation doe variable length mean
Longformer get last_hidden_state,"<p>I am trying to follow this example in the huggingface documentation here <a href=""https://huggingface.co/transformers/model_doc/longformer.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/longformer.html</a>:</p>
<pre><code>import torch
from transformers import LongformerModel, LongformerTokenizer
model = LongformerModel.from_pretrained('allenai/longformer-base-4096')
tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')
SAMPLE_TEXT = ' '.join(['Hello world! '] * 1000)  # long input document
input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0)  # batch of size 1
# Attention mask values -- 0: no attention, 1: local attention, 2: global attention
attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to local attention
global_attention_mask = torch.zeros(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to global attention to be deactivated for all tokens
global_attention_mask[:, [1, 4, 21,]] = 1  # Set global attention to random tokens for the sake of this example
                                    # Usually, set global attention based on the task. For example,
                                    # classification: the &lt;s&gt; token
                                    # QA: question tokens
                                    # LM: potentially on the beginning of sentences and paragraphs
outputs = model(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask, output_hidden_states= True)
sequence_output = outputs[0].last_hidden_state
pooled_output = outputs.pooler_output
</code></pre>
<p>I suppose that this would return a document embedding for the sample text.
However, I run into the following error:</p>
<pre><code>AttributeError: 'Tensor' object has no attribute 'last_hidden_state'
</code></pre>
<p>Why isnt it possible to call last_hidden_state?</p>
",Vectorization & Embeddings,longformer get last hidden state trying follow example huggingface documentation suppose would return document embedding sample text however run following error isnt possible call last hidden state
Text classification. TFIDF and Naive Bayes?,"<p>I'm attempting a text classification task, where I have training data of around 500 restaurant reviews that are labelled across 12 categories. I spent longer than I should have implementing TF.IDF and cosine similarity for the classification of test data, only to get some very poor results (0.4 F-measure). With time not on my side now, I need to implement something significantly more effective that doesn't have a steep learning curve. I am considering using the TF.IDF values in conjunction with Naive Bayes. Does this sound sensible? I know if I can get my data in the right format, I can do this with Scikit learn. Is there anything else you recommend I consider?</p>
",Vectorization & Embeddings,text classification tfidf naive bayes attempting text classification task training data around restaurant review labelled across category spent longer implementing tf idf cosine similarity classification test data get poor result f measure time side need implement something significantly effective steep learning curve considering using tf idf value conjunction naive bayes doe sound sensible know get data right format scikit learn anything else recommend consider
Is BertForSequenceClassification using the CLS vector?,"<p>In the hugging face <a href=""https://github.com/huggingface/transformers/blob/536ea2aca234fb48c5c69769431d643b0d93b233/src/transformers/models/bert/modeling_bert.py#L1552"" rel=""nofollow noreferrer"">source code</a>, <code>pooled_output = outputs[1]</code> is used.</p>
<pre class=""lang-py prettyprint-override""><code>        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        pooled_output = outputs[1]
</code></pre>
<p>Shouldn't it be <code>pooled_output = outputs[0]</code>? (This <a href=""https://stackoverflow.com/a/60883003/20898396"">answer</a> mentioning BertPooler seems to be outdated)</p>
<p>Based on <a href=""https://stackoverflow.com/a/62981360/20898396"">this</a> answer, it seems that the CLS token learns a sentence level representation. I am confused as to why/how masked language modelling would lead to the start token learning a sentence level representation. (I am thinking that <code>BertForSequenceClassification</code> freezes the Bert model and only trains the classification head, but maybe that's not the case)</p>
<p>Would a sentence embedding be equivalent or even better than the [CLS] token embedding?</p>
",Vectorization & Embeddings,bertforsequenceclassification using cl vector hugging face source code used would sentence embedding equivalent even better cl token embedding
How can I implement a multi-label sequence classification model using TF-IDF vectorization followed by LSTM algorithm?,"<p>This is the link for the dataset- <a href=""https://huggingface.co/datasets/surrey-nlp/PLOD-CW"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/surrey-nlp/PLOD-CW</a></p>
<p>You can load the dataset using this line- load_dataset(&quot;surrey-nlp/PLOD-CW&quot;)</p>
<p>I have been trying to code in which first I am vectorising the dataset using word2vec then applying LSTM algorithm to for multilabel sequence classifocation but I am facing the error.</p>
<p>This is the code that I have created-</p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import to_categorical
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

datasets = load_dataset(&quot;surrey-nlp/PLOD-CW&quot;)

# Preprocess the data for a single dictionary
def preprocess_data(data):
    processed_data = []
    for sentence in data['tokens']:
        processed_sentence = []
        for word in sentence:
            processed_sentence.append(word.lower())
        processed_data.append(processed_sentence)
    return processed_data

# Convert the data into Tfidf vectors
# Convert the data into Tfidf vectors
def tfidf_vectorization(data):
    data_strings = [' '.join(sentence) for sentence in data]
    vectorizer = TfidfVectorizer(max_features=5000)
    tfidf_matrix = vectorizer.fit_transform(data_strings)
    return tfidf_matrix, vectorizer

# Build the LSTM model
def build_lstm_model(input_shape, num_labels):
    inputs = Input(shape=input_shape)
    embedding_layer = Embedding(input_dim=5000, output_dim=100, input_length=input_shape[0])(inputs)
    lstm_layer = LSTM(units=100, return_sequences=False)(embedding_layer)
    dense_layer = Dense(units=num_labels, activation='sigmoid')(lstm_layer)
    model = Model(inputs=inputs, outputs=dense_layer)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Preprocess the data for the list of dictionaries
processed_data = [preprocess_data(dataset) for dataset in datasets.values()]

# Convert the data into Tfidf vectors
tfidf_matrix, vectorizer = tfidf_vectorization(processed_data)

# Prepare the data for LSTM input
max_sequence_length = 50
X = tfidf_matrix.toarray()
X = pad_sequences(X, maxlen=max_sequence_length, padding='post', truncating='post')

# Prepare the labels
num_labels = len(np.unique(np.concatenate([dataset['train']['ner_tags'] for dataset in datasets])))
Y = np.concatenate([np.array(dataset['train']['ner_tags']) for dataset in datasets])
Y = to_categorical(Y, num_labels)

# Split the data into training and validation sets
X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=42)

# Build the LSTM model
input_shape = (max_sequence_length,)
model = build_lstm_model(input_shape, num_labels)

# Train the model
checkpointer = ModelCheckpoint(filepath='best_model.h5', verbose=1, save_best_only=True)
model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, batch_size=32, callbacks=[checkpointer])

# Evaluate the model
model.load_weights('best_model.h5')
loss, accuracy, f1_score = model.evaluate(X_val, Y_val, verbose=1)
print('Loss:', loss)
print('Accuracy:', accuracy)
print('F1 Score:', f1_score)
</code></pre>
<p>The error that I am getting is-</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [67], in &lt;cell line: 45&gt;()
     42 processed_data = [preprocess_data(dataset) for dataset in datasets.values()]
     44 # Convert the data into Tfidf vectors
---&gt; 45 tfidf_matrix, vectorizer = tfidf_vectorization(processed_data)
     47 # Prepare the data for LSTM input
     48 max_sequence_length = 50

Input In [67], in tfidf_vectorization(data)
     25 def tfidf_vectorization(data):
---&gt; 26     data_strings = [' '.join(sentence) for sentence in data]
     27     vectorizer = TfidfVectorizer(max_features=5000)
     28     tfidf_matrix = vectorizer.fit_transform(data_strings)

Input In [67], in &lt;listcomp&gt;(.0)
     25 def tfidf_vectorization(data):
---&gt; 26     data_strings = [' '.join(sentence) for sentence in data]
     27     vectorizer = TfidfVectorizer(max_features=5000)
     28     tfidf_matrix = vectorizer.fit_transform(data_strings)

TypeError: sequence item 0: expected str instance, list found
</code></pre>
<p>I have been trying a way out to make this code run so that I can create the code for multilabel sequence classification using TFIDF vectorisation and LSTM algorithm.</p>
<p>The datsets consist of-</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['tokens', 'pos_tags', 'ner_tags'],
        num_rows: 1072
    })
    validation: Dataset({
        features: ['tokens', 'pos_tags', 'ner_tags'],
        num_rows: 126
    })
    test: Dataset({
        features: ['tokens', 'pos_tags', 'ner_tags'],
        num_rows: 153
    })
})
</code></pre>
<p>For these 2 lines-</p>
<pre><code>data=datasets['train'][0:3]
print(data)
</code></pre>
<p>The output comes-</p>
<pre><code>{'tokens': [['For', 'this', 'purpose', 'the', 'Gothenburg', 'Young', 'Persons', 'Empowerment', 'Scale', '(', 'GYPES', ')', 'was', 'developed', '.'], ['The', 'following', 'physiological', 'traits', 'were', 'measured', ':', 'stomatal', 'conductance', '(', 'gs', ',', 'mol', 'H2O', 'm-2', 's-1', ')', ',', 'transpiration', 'rate', '(', 'E', ',', 'mmol', 'H2O', 'm-2', 's-1', ')', ',', 'net', 'photosynthetic', 'rate', '(', 'PN', ',', 'μmol', 'm-2', 's-1', ')', 'and', 'intercellular', 'CO2', 'concentration', 'CO2', '(', 'Ci', ',', 'μmol', 'm-2', 's-1', ')', '.'], ['Minor', 'H', 'antigen', 'alloimmune', 'responses', 'readily', 'occur', 'in', 'the', 'setting', 'of', 'human', 'leukocyte', 'antigen', '(', 'HLA)–matched', 'allogeneic', 'solid', 'organ', 'and', 'stem', 'cell', 'transplantation', '(', 'SCT', ')', '[', '3,4', ']', '.']], 'pos_tags': [['ADP', 'DET', 'NOUN', 'DET', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'AUX', 'VERB', 'PUNCT'], ['DET', 'ADJ', 'ADJ', 'NOUN', 'AUX', 'VERB', 'PUNCT', 'ADJ', 'NOUN', 'PUNCT', 'PROPN', 'PUNCT', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'PUNCT', 'NOUN', 'NOUN', 'PUNCT', 'PROPN', 'PUNCT', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'PUNCT', 'ADJ', 'ADJ', 'NOUN', 'PUNCT', 'PROPN', 'PUNCT', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'CCONJ', 'ADJ', 'NOUN', 'NOUN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'PUNCT'], ['ADJ', 'PROPN', 'NOUN', 'ADJ', 'NOUN', 'ADV', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'NOUN', 'PUNCT', 'PROPN', 'ADJ', 'ADJ', 'NOUN', 'CCONJ', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'PROPN', 'PUNCT', 'PUNCT', 'NUM', 'PUNCT', 'PUNCT']], 'ner_tags': [['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O'], ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'B-AC', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O'], ['B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O']]}
</code></pre>
<p>Please help me out in making the changes to the code so that I can obtain the output. Please give me an alternative or make correction according to the dataset so that I can create a code for sequence classification using Tfidf vectorisation and LSTM algorithm.</p>
",Vectorization & Embeddings,implement multi label sequence classification model using tf idf vectorization followed lstm algorithm link dataset load dataset using line load dataset surrey nlp plod cw trying code first vectorising dataset using word vec applying lstm algorithm multilabel sequence classifocation facing error code created error getting trying way make code run create code multilabel sequence classification using tfidf vectorisation lstm algorithm datsets consist line output come please help making change code obtain output please give alternative make correction according dataset create code sequence classification using tfidf vectorisation lstm algorithm
Is it possible to fine-tune a pretrained word embedding model like vec2word?,"<p>I'm working on semantic matching in my search engine system. I saw that word embedding can be used for this task. However, my dataset is very limited and small, so I don't think that training a word embedding model such as word2vec from scratch will yield good results. As such, I decided to fine-tune a pre-trained model with my data.</p>
<p>However, I can't find a lot of information, such as articles or documentation, about fine-tuning. Some people even say that it's impossible to fine-tune a word embedding model.</p>
<p>This raises my question: is fine-tuning a pre-trained word embedding model possible and has anyone tried this before? Currently, I'm stuck and looking for more information. Should I try to train a word embedding model from scratch or are there other approaches?</p>
",Vectorization & Embeddings,possible fine tune pretrained word embedding model like vec word working semantic matching search engine system saw word embedding used task however dataset limited small think training word embedding model word vec scratch yield good result decided fine tune pre trained model data however find lot information article documentation fine tuning people even say impossible fine tune word embedding model raise question fine tuning pre trained word embedding model possible ha anyone tried currently stuck looking information try train word embedding model scratch approach
Keras Fit Method &#39;Unrecognized Data Type&#39; for NLP Classification Problem with TextVectorization,"<p>I have an LSTM based model.  I am using a text vectorization layer and an embedding layer prior to the LSTM layers.  I am also using two Dense layers with a softmax output.  I am trying to go for binary classification across text inputs.</p>
<p>Here is my code (the inputs are just examples and my larger data set looks just like them, but with far more entries).</p>
<p>Libraries (some not necessary for this portion of the project):</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import numpy as np
import itertools
from sklearn.metrics import confusion_matrix
from tabulate import tabulate
from sklearn import svm
from sklearn.model_selection import GridSearchCV

import tensorflow as tf
from keras.models import Sequential
from keras.layers import LSTM, Dropout, Dense, Embedding, Input
from keras.layers import TextVectorization
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing import sequence
</code></pre>
<p>Simple inputs:</p>
<pre><code>text_data = [
    &quot;The movie was fantastic! I really enjoyed it and would watch it again.&quot;,
    &quot;I didn't like the movie. The plot had too many holes.&quot;,
    &quot;One of the best movies I've seen this year. Highly recommended!&quot;,
    &quot;The acting was poor and the storyline was boring. I wouldn't recommend it.&quot;
]

labels = [1, 0, 1, 0]
</code></pre>
<p>Text Vectorization and Adapt:</p>
<pre><code>vectorize_layer = TextVectorization(max_tokens=9000,output_mode='int',output_sequence_length = 50,                 pad_to_max_tokens = True,vocabulary = None,standardize = &quot;lower_and_strip_punctuation&quot;,
split = 'whitespace',ngrams = None)

vectorize_layer.adapt(text_data)

</code></pre>
<p>Model definition:</p>
<pre><code>lstm_model = Sequential()
lstm_model.add(Input(shape = (1,), dtype = &quot;string&quot;))
lstm_model.add(vectorize_layer)
lstm_model.add(Embedding(input_dim = 9000, output_dim = 128,embeddings_initializer= 'uniform'))
lstm_model.add(LSTM(64, return_sequences = True, recurrent_dropout = 0.25, dropout = 0.25))
lstm_model.add(LSTM(64))
lstm_model.add(Dense(32, activation='relu'))
lstm_model.add(Dense(1, activation = 'softmax'))
lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
lstm_model.build()
lstm_model.summary()
</code></pre>
<p>The model seems correct:</p>
<p>┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ text_vectorization_25           │ (None, 50)             │             0 │
│ (TextVectorization)             │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ embedding_26 (Embedding)        │ (None, 50, 128)        │     1,152,000 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm_49 (LSTM)                  │ (None, 50, 64)         │        49,408 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm_50 (LSTM)                  │ (None, 64)             │        33,024 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_49 (Dense)                │ (None, 32)             │         2,080 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_50 (Dense)                │ (None, 1)              │            33 │</p>
<p>Call model fit:</p>
<pre><code>lstm_model.fit(text_data, labels, epochs=10)
</code></pre>
<p>Error:</p>
<p>ValueError: Unrecognized data type: x=['The movie was fantastic! I really enjoyed it and would watch it again.', &quot;I didn't like the movie. The plot had too many holes.&quot;, &quot;One of the best movies I've seen this year. Highly recommended!&quot;, &quot;The acting was poor and the storyline was boring. I wouldn't recommend it.&quot;] (of type &lt;class 'list'&gt;)</p>
<p>Tried converting to np arrays:</p>
<pre><code>a = np.array(text_data)
b = np.array(labels)
</code></pre>
<p>Call model fit:</p>
<pre><code>lstm_model.fit(a, b, epochs=10)
</code></pre>
<p>Error:</p>
<p>ValueError: Invalid dtype: str2368</p>
",Vectorization & Embeddings,kera fit method unrecognized data type nlp classification problem textvectorization lstm based model using text vectorization layer embedding layer prior lstm layer also using two dense layer softmax output trying go binary classification across text input code input example larger data set look like far entry library necessary portion project simple input text vectorization adapt model definition model seems correct layer type output shape param text vectorization none textvectorization embedding embedding none lstm lstm none lstm lstm none dense dense none dense dense none call model fit error valueerror unrecognized data type x movie wa fantastic really enjoyed would watch like movie plot many hole one best movie seen year highly recommended acting wa poor storyline wa boring recommend type class list tried converting np array call model fit error valueerror invalid dtype str
Gensim&#39;s Doc2Vec with documents in multiple languages,"<p>I'm building a content based recommender system using similarities on vector representations of documents.
My documents are descriptions of books. Most of them are in English, but some of them are in other languages. I used gensim's Doc2Vec for building vector representations of the documents.</p>
<p>Based on my understanding of this model, documents in different languages should have very low similarity, because the words are not even overlapping. But this is not what's happening. Documents in different languages in my dataset can have a high similarity as 0.9.</p>
<p>Why is this happening?</p>
<pre><code>import pandas as pd
from pathlib import Path
from gensim.models.doc2vec import TaggedDocument, Doc2Vec
from gensim.utils import simple_preprocess
import numpy as np
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
data_folder = Path.cwd().parent / 'data'
transformed_folder = data_folder / 'transformed'
BOOKS_PATH = Path(transformed_folder, &quot;books_final.csv&quot;)
import seaborn as sns

book_df = pd.read_csv(BOOKS_PATH)
languages=book_df.language.unique()

training_corpus = np.empty(len(book_df), dtype=object)
for i, (isbn, desc) in tqdm(enumerate(zip(book_df['isbn'], book_df['description']))):
    training_corpus[i] = TaggedDocument(simple_preprocess(desc), [str(i)])
model=Doc2Vec(vector_size=50, min_count=2, epochs=40)
model.build_vocab(training_corpus)
#train the model
model.train(training_corpus, total_examples=model.corpus_count, epochs=model.epochs)
#get the keys of the trained model
model_keys=list(model.dv.key_to_index.keys())
matrix = model.dv.vectors
similarity_matrix = cosine_similarity(matrix)

indices_for_language = {}
for language, group_df in book_df.groupby('language'):
    indices_for_language[language] = group_df.index.to_list()

sim_japanese_italian = similarity_matrix[indices_for_language['Japanese']][:, indices_for_language['Italian']]
#make a heatmap
sns.heatmap(sim_japanese_italian)

</code></pre>
<p><a href=""https://i.sstatic.net/9sjkH.png"" rel=""nofollow noreferrer"">Heatmap of similarities between Italian and Japanese documents</a></p>
",Vectorization & Embeddings,gensim doc vec document multiple language building content based recommender system using similarity vector representation document document description book english language used gensim doc vec building vector representation document based understanding model document different language low similarity word even overlapping happening document different language dataset high similarity happening heatmap similarity italian japanese document
What&#39;s inside inner vertices in Word2Vec Hierarchical Softmax?,"<p>I have a question about Hierarchical Softmax. Actually, I do not quite understand what is stored in inner vertices (which are not leaf vertices). I clearly understand the main idea of this algorithm, but each step we calculate dot product of input word embedding with the word embedding of inner vertice. So what vectors are inside these inner vertices? Is it randomly initialized vectors of size that equals to embedding_size and then their coordinates change due to backpropagation step until we stop?</p>
",Vectorization & Embeddings,inside inner vertex word vec hierarchical softmax question hierarchical softmax actually quite understand stored inner vertex leaf vertex clearly understand main idea algorithm step calculate dot product input word embedding word embedding inner vertice vector inside inner vertex randomly initialized vector size equal embedding size coordinate change due backpropagation step stop
GloVe embedding for empty string,"<p>It looks like the embedding for the empty string in the <code>glove.twitter.27B.200d.txt</code> file that's part of this zip file:</p>
<p><a href=""https://nlp.stanford.edu/data/glove.twitter.27B.zip"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/data/glove.twitter.27B.zip</a></p>
<p>is provided on line 38523, but I'm not 100% sure this is what I think it is. I haven't worked with these embeddings much and was wondering if someone could verify that this the case (or not)?</p>
",Vectorization & Embeddings,glove embedding empty string look like embedding empty string file part zip file provided line sure think worked embeddings much wa wondering someone could verify case
Output of Cosine Similarity is not as expected,"<p>I am trying to generate the Cosine similarity between two words in a sentence. The sentence is &quot;The black cat sat on the couch and the brown dog slept on the rug&quot;.</p>
<p>My Python code is below:</p>
<pre><code>from nltk.tokenize import sent_tokenize, word_tokenize
import warnings
 
warnings.filterwarnings(action = 'ignore')
 
import gensim
from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity

sentence = &quot;The black cat sat on the couch and the brown dog slept on the rug&quot;
# Replaces escape character with space
f = sentence.replace(&quot;\n&quot;, &quot; &quot;)
 
data = []

# sentence parsing
for i in sent_tokenize(f):
    temp = []
    # tokenize the sentence into words
    for j in word_tokenize(i):
        temp.append(j.lower())
    data.append(temp)
print(data)
# Creating Skip Gram model
model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 512, window = 5, sg = 1)

# Print results
print(&quot;Cosine similarity between 'black' &quot; +
          &quot;and 'brown' - Skip Gram : &quot;,
    model2.wv.similarity('black', 'brown'))

</code></pre>
<p>As &quot;black&quot; and &quot;brown&quot; are of colour type, their cosine similarity should be maximum (somewhere around 1). But my result shows following:</p>
<pre><code>[['the', 'black', 'cat', 'sat', 'on', 'the', 'couch', 'and', 'the', 'brown', 'dog', 'slept', 'on', 'the', 'rug']]
Cosine similarity between 'black' and 'brown' - Skip Gram :  0.008911405
</code></pre>
<p>Any idea what is wrong here? Is my understanding about cosine similarity correct?</p>
",Vectorization & Embeddings,output cosine similarity expected trying generate cosine similarity two word sentence sentence black cat sat couch brown dog slept rug python code black brown colour type cosine similarity maximum somewhere around result show following idea wrong understanding cosine similarity correct
Different embedding checksums after encoding with SentenceTransformers?,"<p>I am calculating some embeddings with SentenceTransformers Library. However, I get different results when encoding the sentences and calculating their embeddings when checking the sum of their values. For instance:</p>
<p>In:</p>
<pre><code>
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)


transformer_models = [
    'M-CLIP/M-BERT-Distil-40', 
                     ]

sentences = df['content'].tolist()


for transformer_model in tqdm(transformer_models, desc=&quot;Transformer Models&quot;):
    tqdm.write(f&quot;Processing with Transformer Model: {transformer_model}&quot;)
    model = SentenceTransformer(transformer_model)
    embeddings = model.encode(sentences)
    print(f&quot;Embeddings Checksum for {transformer_model}:&quot;, np.sum(embeddings))
</code></pre>
<p>Out:</p>
<pre><code>Embeddings Checksum for M-CLIP/M-BERT-Distil-40: 1105.9185
</code></pre>
<p>Or</p>
<pre><code>Embeddings Checksum for M-CLIP/M-BERT-Distil-40: 1113.5422
</code></pre>
<p>I noticed this situation happens when I restart and clear the output of the jupyter notebook, and then re-run the full notebook. Any idea of how to fix this issue?</p>
<p>Alternative I tried to set after and before the embeddings calculation the reandom seeds:</p>
<pre><code>import torch
import numpy as np
import random
import tensorflow as tf
from sentence_transformers import SentenceTransformer
from tqdm.auto import tqdm

RANDOM_SEED = 42

# Setting seeds
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

# Ensuring PyTorch determinism
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

transformer_models = ['M-CLIP/M-BERT-Distil-40']

sentences = df['content'].tolist()

for transformer_model in tqdm(transformer_models, desc=&quot;Transformer Models&quot;):
    # Set the seed again right before loading the model
    np.random.seed(RANDOM_SEED)
    random.seed(RANDOM_SEED)
    tf.random.set_seed(RANDOM_SEED)
    torch.manual_seed(RANDOM_SEED)

    tqdm.write(f&quot;Processing with Transformer Model: {transformer_model}&quot;)
    model = SentenceTransformer(transformer_model, device='cpu')  # Force to use CPU

    embeddings = model.encode(sentences, show_progress_bar=False)  # Disable progress bar and parallel tokenization
    print(f&quot;Embeddings Checksum for {transformer_model}:&quot;, np.sum(embeddings))
</code></pre>
<p>However I am getting the same inconsistent behavior.</p>
<p><strong>UPDATE</strong></p>
<p>What I tried now, and seem to work is that now I store all the calculated embeddings in files. However, I find weird that when doing this I get different results. Does anyone has experience this before?</p>
",Vectorization & Embeddings,different embedding checksum encoding sentencetransformers calculating embeddings sentencetransformers library however get different result encoding sentence calculating embeddings checking sum value instance noticed situation happens restart clear output jupyter notebook run full notebook idea fix issue alternative tried set embeddings calculation reandom seed however getting inconsistent behavior update tried seem work store calculated embeddings file however find weird get different result doe anyone ha experience
How to calculate cosine similarity with bert over 1000 random example,"<p>I'm trying to calculate over random <strong>1000</strong> quest and <strong>1000</strong> answer using <strong>cosine similarity</strong> with <strong>bert-base-uncased</strong>, and after I want to find most similar 5 asnwer, after calculate top1 and top5 real answer accuracy. But im receiving output always <strong>0.0</strong> accuracy and answers not similar.</p>
<pre><code>sample_1000_quest = train_ds['questions'].sample(1000)
sample_1000_answer = train_ds['answers'].sample(1000)

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)


tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')
model_bert = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True).eval()


selected_question = sample_1000_quest.iloc[1]

selected_question_idx = sample_1000_quest.index.get_loc(30574)


encoded_question = tokenizer_bert(selected_question, return_tensors='pt', padding=True, truncation=True)


with torch.no_grad():
    outputs = model_bert(**encoded_question)
    question_embedding = outputs.last_hidden_state.mean(dim=1)


encoded_answers = []
answer_embeddings = []
for answer in sample_1000_answer:
    encoded_answer = tokenizer_bert(answer, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        outputs = model_bert(**encoded_answer.to(device))
        answer_embedding = outputs.last_hidden_state.mean(dim=1)
        answer_embeddings.append(answer_embedding)


similarities = []
for answer_embedding in answer_embeddings:
    similarity = cosine_similarity(question_embedding, answer_embedding)
    similarities.append(similarity.item())


most_similar_indices = np.argsort(similarities)[-5:][::-1]


ground_truth_idx = train_ds['answers'].iloc[selected_question_idx]


top1_accuracies = []
top5_accuracies = []

top1_idx = most_similar_indices[0]
top1_accuracy = 1 if top1_idx == ground_truth_idx else 0
top5_accuracy = 1 if ground_truth_idx in most_similar_indices else 0

top1_accuracies.append(top1_accuracy)
top5_accuracies.append(top5_accuracy)

print(&quot;Selected Question:&quot;, selected_question)
print(&quot;Most similar 5 asnwer:&quot;)
for i, idx in enumerate(most_similar_indices):
    print(f&quot;{i+1}. {sample_1000_answer.iloc[idx]}&quot;)

print(&quot;Top-1 Accuracy:&quot;, top1_accuracy)
print(&quot;Top-5 Accuracy:&quot;, top5_accuracy)
</code></pre>
<p>Output:</p>
<pre><code>Selected Question:  bir sunum oluşturmak için beş adım yazın.
Most similar 5 asnwer:
1.  doğum günü gülüm bütün yaz aldığım en güzel hediyeydi.
2.  bu deneyin amacı ilkeleri anlamaktır.
3.  bir satış elemanı sunum yapıyor.
4.  hangi konuda yardıma ihtiyacın olduğunu söyle.
5.  konuşmanın içeriği, projede bir sonraki adım için onay almakla ilgilidir.
Top-1 Accuracy: 0
Top-5 Accuracy: 0
</code></pre>
",Vectorization & Embeddings,calculate cosine similarity bert random example trying calculate random quest answer using cosine similarity bert base uncased want find similar asnwer calculate top top real answer accuracy im receiving output always accuracy answer similar output
Subsampling when training word embeddings,"<p>NLP newbie here with a question about word embeddings. As a learning exercise, I'm trying to train my own set of word embeddings based on word2vec. I have a corpus of english sentences that I've downloaded and cleaned and I think I have a decent grasp of how the training is supposed to work, but there's something I still don't really understand.</p>
<p>As one might imagine, the corpus contains many more instances of common words like 'the', 'and', and so on. The word frequency distribution is a fairly extreme power law, which makes sense. My question is this: what are the best practices to deal with this when I'm generating samples to train the word embeddings?</p>
<p>I can see a few of options:</p>
<ol>
<li>When I'm generating training samples, do some sort of probabilistic sampling based on the frequency of the input token in the dataset. My newbie intuition is that this makes some sense, but I'm not 100% sure how the sampling should work.</li>
<li>With some probability, drop the most common words from the vocabulary altogether and don't learn embeddings for them at all. I've seen some guidance on the web (and in <a href=""https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf"" rel=""nofollow noreferrer"">the original word2vec paper</a>) that recommends doing this and just treating them as an OOV token when looking up the embedding, but it just feels ... weird. After all, I do want to have an embedding for the word 'the', even if it appears very frequently.</li>
<li>Just power through and live with the fact that I'm going to have a lot more training samples for the word 'the' than the word 'persnickety'. This will make a training epoch take a lot longer.</li>
</ol>
<p>Can anyone give me some guidance here? How do people usually deal with this kind of imbalance?</p>
",Vectorization & Embeddings,subsampling training word embeddings nlp newbie question word embeddings learning exercise trying train set word embeddings based word vec corpus english sentence downloaded cleaned think decent grasp training supposed work something still really understand one might imagine corpus contains many instance common word like word frequency distribution fairly extreme power law make sense question best practice deal generating sample train word embeddings see option generating training sample sort probabilistic sampling based frequency input token dataset newbie intuition make sense sure sampling work probability drop common word vocabulary altogether learn embeddings seen guidance web original word vec paper recommends treating oov token looking embedding feel weird want embedding word even appears frequently power live fact going lot training sample word word persnickety make training epoch take lot longer anyone give guidance people usually deal kind imbalance
"Adapters after QLoRA fine-tuning on a llama architecture model reach about 2 GB, which is very far from the general trend seen online","<p>I was Fine Tuning a Llama Architecture Model that supports multiple languages: English, Hindi as well as Roman Hindi.
So, I loaded the model in quantized form using bitsandbytes in nf4 form along with double quantization.</p>
<pre><code>bnb_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.bfloat16,
)

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config = bnb_config,
    device_map = {&quot;&quot;:0},
    trust_remote_code = True,
)
</code></pre>
<p>I was fine tuning this model for chat format. So, I added a  token to the tokenizer for this model, added the  token to the model config and resized the model embeddings according to the final length of the tokenizer.</p>
<pre><code>**# Adding a new padding token to the tokenizer even though one is already present in the tokenizer [PAD]. It would be very helpful if someone can point out the reason for this.**

if '&lt;pad&gt;' not in tokenizer.get_vocab():
    print(&quot;Token Added&quot;)
    # Add the pad token
    tokenizer.add_tokens(['&lt;pad&gt;'])

# Setting the pad token
tokenizer.pad_token = '&lt;pad&gt;'

voca = tokenizer.get_vocab()

# Resize Token Embeddings
model.resize_token_embeddings(len(tokenizer))

# Updating pad token id in model and its config
model.pad_token_id = tokenizer.pad_token_id
model.config.pad_token_id = tokenizer.pad_token_id

assert model.pad_token_id == tokenizer.pad_token_id, &quot;The model's pad token ID does not match the tokenizer' pad token ID&quot;


model = get_peft_model(model, config)
print_trainable_parameters(model)
</code></pre>
<p>Then I set the LoRA Adapters for fine tuning:</p>
<pre><code>r = 16,
lora_rank = 32,
target_modules = [&quot;q_proj&quot;, &quot;v_proj&quot;, &quot;down_proj&quot;, &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;k_proj&quot;],
lora_dropout = 0.05,
task_type = &quot;CAUSAL_LM&quot;,

model = get_peft_model(model, config)

trainable params: 35782656 || all params: 3667800064 || trainable%: 0.9755890554453079
</code></pre>
<p>Now, the vocab size of the model is 48065. After fine tuning the model with about 80 examples in Hindi, Roman Hindi and some English prompts.</p>
<p>Traing Arguments:</p>
<pre><code>args = transformers.TrainingArguments(
        num_train_epochs=1,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=2,
        max_grad_norm=1,
        warmup_ratio=0.1,
        learning_rate=1e-4,
        fp16 = True,
        logging_steps = 1,
        output_dir = &quot;outputs&quot;,
        optim = &quot;paged_adamw_8bit&quot;,
        lr_scheduler_type = 'cosine',
    ),
    data_collator=data_collator,
)

model.config.use_cache = False
</code></pre>
<p>Then after training this model I am pushing this model onto huggingface using model.push_to_hub(repo_id = rep_id) # <strong>This is the adapter and not the merged model, right?</strong></p>
<ul>
<li>This model is 1.72 GB which is not the case anywhere online, Is it ok or what might be the issue?</li>
<li>When wanting to merge this adapter with the base model I get CUDA out of Memory for Turing T4 GPU, which is understandable.</li>
<li>So, I save the adapter and then load model again and resize the embedding for this loaded model and then add the adapters to this model for inference, but still I am not able to do inference due to the huge size of the model with added adapters now because of large adapter file.</li>
<li>How do I resolve this issue of large adapter file, and combine the base model with the adapter?</li>
<li>Also, when trying to load the saved adapter from local using:</li>
</ul>
<pre><code>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available else &quot;cpu&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;path_to_adapter_file&quot;, local_files_only = True, device_map = device)
</code></pre>
<p>The CUDA Memory gets full 16GB, I dont know the reason, even though the adapter file is 1.72 GB???</p>
<p>Am I making any mistake? Is there any solution to this? Thanks</p>
",Vectorization & Embeddings,adapter qlora fine tuning llama architecture model reach gb far general trend seen online wa fine tuning llama architecture model support multiple language english hindi well roman hindi loaded model quantized form using bitsandbytes nf form along double quantization wa fine tuning model chat format added token tokenizer model added token model config resized model embeddings according final length tokenizer set lora adapter fine tuning vocab size model fine tuning model example hindi roman hindi english prompt traing argument training model pushing model onto huggingface using model push hub repo id rep id adapter merged model right model gb case anywhere online ok might issue wanting merge adapter base model get cuda memory turing gpu understandable save adapter load model resize embedding loaded model add adapter model inference still able inference due huge size model added adapter large adapter file resolve issue large adapter file combine base model adapter also trying load saved adapter local using cuda memory get full gb dont know reason even though adapter file gb making mistake solution thanks
"Tensorflow embeddings InvalidArgumentError: indices[18,16] = 11905 is not in [0, 11905) [[node sequential_1/embedding_1/embedding_lookup","<p>I am using TF 2.2.0 and trying to create a Word2Vec CNN text classification model. But however I tried there has been always an issue with the model or embedding layers. I could not found clear solutions in the internet so decided to ask it.</p>
<pre><code>import multiprocessing
modelW2V = gensim.models.Word2Vec(filtered_stopwords_list, size= 100, min_count = 5, window = 5, sg=0, iter = 10, workers= multiprocessing.cpu_count() - 1)
model_save_location = &quot;3000tweets_notbinary&quot;
modelW2V.wv.save_word2vec_format(model_save_location)

word2vec = {}
with open('3000tweets_notbinary', encoding='UTF-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        vec = np.asarray(values[1:], dtype='float32')
        word2vec[word] = vec

num_words = len(list(tokenizer.word_index))

embedding_matrix = np.random.uniform(-1, 1, (num_words, 100))
for word, i in tokenizer.word_index.items():
    if i &lt; num_words:
        embedding_vector = word2vec.get(word)
        if embedding_vector is not None:
          embedding_matrix[i] = embedding_vector
        else:
          embedding_matrix[i] = np.zeros((100,))
</code></pre>
<p>I have created my word2vec weights by the code above and then converted it to embedding_matrix as I followed on many tutorials. But since there are a lot of words seen by word2vec but not available in embeddings, if there is no embedding I assign 0 vector. And then fed data and this embedding to tf sequential model.</p>
<pre><code>seq_leng = max_tokens
vocab_size = num_words
embedding_dim = 100
filter_sizes = [3, 4, 5]
num_filters = 512
drop = 0.5
epochs = 5
batch_size = 32

model = tf.keras.models.Sequential([
                                    tf.keras.layers.Embedding(input_dim= vocab_size,
                                                              output_dim= embedding_dim,
                                                              weights = [embedding_matrix],
                                                              input_length= max_tokens,
                                                              trainable= False),
                                    tf.keras.layers.Conv1D(num_filters, 7, activation= &quot;relu&quot;, padding= &quot;same&quot;),
                                    tf.keras.layers.MaxPool1D(2),
                                    tf.keras.layers.Conv1D(num_filters, 7, activation= &quot;relu&quot;, padding= &quot;same&quot;),
                                    tf.keras.layers.MaxPool1D(),
                                    tf.keras.layers.Dropout(drop),
                                    tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(32, activation= &quot;relu&quot;, kernel_regularizer= tf.keras.regularizers.l2(1e-4)),
                                    tf.keras.layers.Dense(3, activation= &quot;softmax&quot;)
])

model.compile(loss= &quot;categorical_crossentropy&quot;, optimizer= tf.keras.optimizers.Adam(learning_rate= 0.001, epsilon= 1e-06),
              metrics= [&quot;accuracy&quot;, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])

model.summary()

history = model.fit(x_train_pad, y_train2, batch_size= 60, epochs= epochs, shuffle= True, verbose= 1)
</code></pre>
<p>But when I run this code, tensorflow gives me the following error in any random time of the training process. But I could not find any solution to it. I have tried adding + 1 to vocab_size but when I do that I get size mismatch error which does not let me even compile my model. Can anyone please help me?</p>
<pre><code>InvalidArgumentError:  indices[18,16] = 11905 is not in [0, 11905)
     [[node sequential_1/embedding_1/embedding_lookup (defined at &lt;ipython-input-26-ef1b16cf85bf&gt;:1) ]] [Op:__inference_train_function_1533]

Errors may have originated from an input operation.
Input Source operations connected to node sequential_1/embedding_1/embedding_lookup:
 sequential_1/embedding_1/embedding_lookup/991 (defined at /usr/lib/python3.6/contextlib.py:81)

Function call stack:
train_function
</code></pre>
",Vectorization & Embeddings,tensorflow embeddings invalidargumenterror index node sequential embedding embedding lookup using tf trying create word vec cnn text classification model however tried ha always issue model embedding layer could found clear solution internet decided ask created word vec weight code converted embedding matrix followed many tutorial since lot word seen word vec available embeddings embedding assign vector fed data embedding tf sequential model run code tensorflow give following error random time training process could find solution tried adding vocab size get size mismatch error doe let even compile model anyone please help
Why do I get different embeddings when I perform batch encoding in huggingface MT5 model?,"<p>I am trying to encode some text using HuggingFace's mt5-base model. I am using the model as shown below</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import MT5EncoderModel, AutoTokenizer

model = MT5EncoderModel.from_pretrained(&quot;google/mt5-base&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;google/mt5-base&quot;)

def get_t5_embeddings(texts):
    last_hidden_state = model(input_ids=tokenizer(texts, return_tensors=&quot;pt&quot;, padding=True).input_ids).last_hidden_state
    pooled_sentence = torch.max(last_hidden_state, dim=1)
    return pooled_sentence[0].detach().numpy()
</code></pre>
<p>I was doing some experiments when I noticed that the same text had a low cosine similarity score with itself. I did some digging and realized that the model was returning very different embeddings if I did the encoding in batches. To validate this, I ran a small experiment that generated embeddings for <code>Hello</code> and a list of 10 <code>Hello</code>s incrementally. and checking the embeddings of the <code>Hello</code> and the first <code>Hello</code> in the list (both of which should be same).</p>
<pre class=""lang-py prettyprint-override""><code>for i in range(1, 10):
    print(i, (get_t5_embeddings([&quot;Hello&quot;])[0] == get_t5_embeddings([&quot;Hello&quot;]*i)[0]).sum())
</code></pre>
<p>This will return the number of values in the embeddings that match each other.
This was the result:</p>
<pre><code>1 768
2 768
3 768
4 768
5 768
6 768
7 768
8 27
9 27
</code></pre>
<p>Every time I run it, I get mismatches if the batch size is more than 768.</p>
<p>Why am I getting different embeddings and how do I fix this?</p>
",Vectorization & Embeddings,get different embeddings perform batch encoding huggingface mt model trying encode text using huggingface mt base model using model shown wa experiment noticed text low cosine similarity score digging realized model wa returning different embeddings encoding batch validate ran small experiment generated embeddings list incrementally checking embeddings first list return number value embeddings match wa result every time run get mismatch batch size getting different embeddings fix
How to embed out of vocab words at the time of testing in word2vec model?,"<p>I was training my word2vec model (skip-gram) on a vocab size of 100 000. But at the time of testing I got few words which weren't in the vocab. To find their embeddings I tried 2 approaches: </p>

<ol>
<li><p>Calculate minimum edit distance word from vocab and acquire its embedding.</p></li>
<li><p>Constructed different n-grams from the word and searched them in the vocab.</p></li>
</ol>

<p>Despite of applying these methods, I am not able to get rid of out of vocab words problem completely.</p>

<p>Does word2vec take all n-grams of a word into account while training like fastText does?</p>

<p>Note - In fastText if our input word is quora then it considers all of its possible n-grams in the corpus.</p>

<p><a href=""https://i.sstatic.net/nxBCJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nxBCJ.png"" alt=""https://www.quora.com/How-does-fastText-output-a-vector-for-a-word-that-is-not-in-the-pre-trained-model""></a></p>
",Vectorization & Embeddings,embed vocab word time testing word vec model wa training word vec model skip gram vocab size time testing got word vocab find embeddings tried approach calculate minimum edit distance word vocab acquire embedding constructed different n gram word searched vocab despite applying method able get rid vocab word problem completely doe word vec take n gram word account training like fasttext doe note fasttext input word quora considers possible n gram corpus img src alt
Do I understand well the use of word embedding for instance with logistic regression?,"<p>I wonder if I understood correctly the idea of using word embedding in natural language processing. I want to show you how I perceive it and ask whether my interpretation is correct.</p>
<p>Let's assume that we want to predict whether sentence is positive or negative. We will use a pre-trained word embedding prepared on a very large text corpus with dimension equals 100. It means that for each word we have 100 values. Our file looks in this way:</p>
<pre><code>...
    new -0.68538535 -0.08992791 0.8066535 other 97 values ... 
    man -0.6401568 -0.05007627 0.65864474 ...
    many 0.18335487 -0.10728102 0.468635 ...
    doesnt 0.0694685 -0.4131108 0.0052553082 ...
...
</code></pre>
<p>Obviously we have test and train set. We will use sklearn model to fit and predict results. Our train set looks in this way:</p>
<pre><code>1 This is positive and very amazing sentence.
0 I feel very sad.
</code></pre>
<p>And test set contains sentences like:</p>
<pre><code>In my opinion people are amazing.
</code></pre>
<p>I have mainly doubts related to the preprocessing of input data. I wonder whether it should be done in this way:</p>
<p>We do for all sentences for instance tokenization, removing of stop words, lowercasing etc. So for our example we get:</p>
<pre><code>'this', 'is', 'positive', 'very', 'amazing', 'sentence'
'i', 'feel', 'very', 'sad'

'in', 'my', 'opinion', 'people', 'amazing'
</code></pre>
<p>We use <code>pad_sequences</code>:</p>
<pre><code>1,2,3,4,5,6
7,8,4,9

10,11,12,13,5
</code></pre>
<p>Furthermore we check the length of the longest sentence in both train set and test set. Let's assume that in our case the maximum length is equals to 10. We need to have all vectors of the same length so we fill the remaining fields with zeros.</p>
<pre><code>1,2,3,4,5,0,0,0,0,0
6,7,4,8,0,0,0,0,0,0

10,11,12,13,5,0,0,0,0,0
</code></pre>
<p>Now the biggest doubt - we assign values from our word embedding word2vec file to all words from the prepared vectors from the training set and the test set.</p>
<p>Our word embedding word2vec file looks like this:</p>
<pre><code>...
    in -0.039903056 0.46479827 0.2576446 ...
    ...
    opinion 0.237968 0.17199863 -0.23182874...
    ...
    people 0.2037858 -0.29881874 0.12108547 ...
    ...
    amazing 0.20736384 0.22415389 0.09953516 ...
    ...
    my 0.46468195 -0.35753986 0.6069699 ...
...
</code></pre>
<p>And for instance for <code>'in', 'my', 'opinion', 'people', 'amazing'</code> equals to <code>10,11,12,13,5,0,0,0,0,0</code> we get the table of tables like this:
<code>[-0.039903056 0.46479827 0.2576446 ...],[0.46468195 -0.35753986 0.6069699 ...],[0.237968 0.17199863 -0.23182874...],[0.2037858 -0.29881874 0.12108547 ...],[0.20736384 0.22415389 0.09953516 ...],0,0,0,0</code></p>
<p>Finally our train set looks in this way:</p>
<pre><code>x             y
1 [0.237968 0.17199863 -0.23182874...],[next 100 values],[next 100 values],[...],[...],0,0,0,0,0,
0 [...],[...],[...],[...],[...],[...],[...],0,0,0
1 [...],[...],[...],[...],[...],0,0,0,0,0
 ...
</code></pre>
<p>And test set looks in this way:</p>
<pre><code>                   y
[100 values],[...],[...],[...],0,0,0,0,0,0
 ...
</code></pre>
<p>In the last step we train our model using for example sklearn model:</p>
<pre><code> LogisticRegression().fit(values from y column of train set, values from x column of train set)
</code></pre>
<p>Then we predict data:</p>
<pre><code> LogisticRegression().predict(values from y column of test set)
</code></pre>
<p>Above I described the whole process with the specified steps that give me the most doubts. I am asking you to indicate me the mistakes I have made in my reasoning and their explanation. I want to be sure that I understood it correctly. Thank you in advance for your help.</p>
",Vectorization & Embeddings,understand well use word embedding instance logistic regression wonder understood correctly idea using word embedding natural language processing want show ask whether interpretation correct let assume want predict whether sentence positive negative use pre trained word embedding prepared large text corpus dimension equal mean word value file look way obviously test train set use sklearn model fit predict result train set look way test set contains sentence like mainly doubt related preprocessing input data wonder whether done way sentence instance tokenization removing stop word lowercasing etc example get use furthermore check length longest sentence train set test set let assume case maximum length equal need vector length fill remaining field zero biggest doubt assign value word embedding word vec file word prepared vector training set test set word embedding word vec file look like instance equal get table table like finally train set look way test set look way last step train model using example sklearn model predict data described whole process specified step give doubt asking indicate mistake made reasoning explanation want sure understood correctly thank advance help
How to calculate readibility scores easily or how can i write a function for that?,"<p>I have to calculate readability score of a text document. Is there a package or inbuilt function. Everything on internet seems too complex. Can any one help me with that or how to write my own function? </p>

<p>I have done pre processing of text, calculated the tfidf of document but I want to find the readability score or fog index of the document. I tried using code available on other platform but it didn't work</p>

<pre><code>def text_process(mess):

    nopunc = [char for char in mess if char not in string.punctuation]

    #nopunc = [char for char in mess if char not in string.punctuation]

    nopunc = ''.join(nopunc)

    text = [word for word in tokens if word not in stops]

    text = [wl.lemmatize(word) for word in mess]

    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]

from sklearn.feature_extraction.text import TfidfVectorizer

import pandas as pd

vect = TfidfVectorizer()

tfidf_matrix = vect.fit_transform(df[""comments""].head(10000))

df1 = pd.DataFrame(tfidf_matrix.toarray(),columns=vect.get_feature_names())

print(df1)      
</code></pre>

<p>I don't know how to get the desired results of readability scores. I would appreciate if someone would help me</p>
",Vectorization & Embeddings,calculate readibility score easily write function calculate readability score text document package inbuilt function everything internet seems complex one help write function done pre processing text calculated tfidf document want find readability score fog index document tried using code available platform work know get desired result readability score would appreciate someone would help
What does Spacy use to create vector representations?,"<p>What is Spacy's built in method of creating vector representations?
I performed NLP on my corpus, and then used .similarity (cosine similarity) to map out documents that were ""similar"". However, I am unsure what method spacy uses to create vector representations. To my knowledge, I am thinking that it is probably word2vec skip-gram with negative sampling, however, I would like to be sure!</p>
",Vectorization & Embeddings,doe spacy use create vector representation spacy built method creating vector representation performed nlp corpus used similarity cosine similarity map document similar however unsure method spacy us create vector representation knowledge thinking probably word vec skip gram negative sampling however would like sure
Max Sequence length in Seq2Seq - Attention is all you need,"<p>I have gone through the paper <a href=""http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"" rel=""nofollow noreferrer"">Attention is all you need</a> and though I think I understood the overall idea behind what is happening, I am pretty confused with the way the input is being processed. Here are my doubts, and for simplicity, let's assume that we are talking about a Language translation task.</p>

<p>1) The paper states that the input embedding is of dimension 512, that would be the embedding vector of each word in the input sentence right? So if the input sentence is of length 25, then the input would be a 25*512 dimension matrix at each layer?</p>

<p>2) Does this model use a fixed ""MAX_LENGTH"" across all its batches? By this, I mean identify the longest sentence in your training set and pad all the other sentences to be equal to the MAX_LENGTH?</p>

<p>3) If the 2nd question does indeed use a concept of MAX_LENGTH, how does one process a test time query of length greater than the input query?</p>

<p>I have also referred to this video to get a better understanding <a href=""https://www.youtube.com/watch?v=z1xs9jdZnuY"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=z1xs9jdZnuY</a> and one of the frames that gives an overall idea of one single layer with 3 multi head attentions is this
<a href=""https://i.sstatic.net/Mx3HW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Mx3HW.png"" alt=""enter image description here""></a></p>

<p>here you can see that the input is of dimension 4*3(for simple representation the embedding size is 3 and the final output of one layer of attention and the Feed forward network is also 4*3). </p>
",Vectorization & Embeddings,max sequence length seq seq attention need gone paper attention need though think understood overall idea behind happening pretty confused way input processed doubt simplicity let assume talking language translation task paper state input embedding dimension would embedding vector word input sentence right input sentence length input would dimension matrix layer doe model use fixed max length across batch mean identify longest sentence training set pad sentence equal max length nd question doe indeed use concept max length doe one process test time query length greater input query also referred video get better understanding one frame give overall idea one single layer multi head attention see input dimension simple representation embedding size final output one layer attention feed forward network also
How can I get the embedding of a document in langchain?,"<p>I use the langchain Python lib to create a vector store and retrieve relevant documents given a user query. How can I get the embedding of a document in the vector store?</p>
<p>E.g., in this code:</p>
<pre><code>import pprint
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document

model = &quot;sentence-transformers/multi-qa-MiniLM-L6-cos-v1&quot;
embeddings = HuggingFaceEmbeddings(model_name = model)

def main():
    doc1 = Document(page_content=&quot;The sky is blue.&quot;,    metadata={&quot;document_id&quot;: &quot;10&quot;})
    doc2 = Document(page_content=&quot;The forest is green&quot;, metadata={&quot;document_id&quot;: &quot;62&quot;})
    docs = []
    docs.append(doc1)
    docs.append(doc2)

    for doc in docs:
        doc.metadata['summary'] = 'hello'

    pprint.pprint(docs)
    db = FAISS.from_documents(docs, embeddings)
    db.save_local(&quot;faiss_index&quot;)
    new_db = FAISS.load_local(&quot;faiss_index&quot;, embeddings)

    query = &quot;Which color is the sky?&quot;
    docs = new_db.similarity_search_with_score(query)
    print('Retrieved docs:', docs)
    print('Metadata of the most relevant document:', docs[0][0].metadata)

if __name__ == '__main__':
    main()
</code></pre>
<hr />
<p>How can I get the embedding of documents <code>doc1</code> and <code>doc2</code>?</p>
<p>The code was tested with Python 3.11 with:</p>
<pre><code>pip install langchain==0.1.1 langchain_openai==0.0.2.post1 sentence-transformers==2.2.2 langchain_community==0.0.13 faiss-cpu==1.7.4
</code></pre>
",Vectorization & Embeddings,get embedding document langchain use langchain python lib create vector store retrieve relevant document given user query get embedding document vector store e g code get embedding document code wa tested python
is there any way to retrieve the embeddings store in a langchain VectorStore?,"<p>I'm using Langchain to load a document, split it into chunks, embed those chunks, embed them and then store the embedding vectors into a langchain VectorStore database. My use case requires me to run an algorithm on the embedding vectors, which i have been trying to find a way to fetch but to no avail.</p>
<p>My idea is to be able to do something like this:</p>
<pre><code>from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import SomeVectorStore
from langchain_openai import OpenAIEmbeddings

loader = TextLoader(&quot;../document.txt&quot;)
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)
embeddings = OpenAIEmbeddings()
db = SomeVectoreStore.from_documents(docs, embeddings)

# get all the embeddings and their corresponding chunks from the db
embeddings_and_thei_chunks = db.some_way_to_get_all_embeddings()
</code></pre>
",Vectorization & Embeddings,way retrieve embeddings store langchain vectorstore using langchain load document split chunk embed chunk embed store embedding vector langchain vectorstore database use case requires run algorithm embedding vector trying find way fetch avail idea able something like
Difference between Word2Vec and contextual embedding,"<p>am trying to understand the difference between word embedding and contextual embedding.</p>
<p>below is my understanding, please add if you find any corrections.</p>
<p>word embedding algorithm has a global vocabulary (dictionary) of words. when we are performing word2vec then the input corpus(unique words) maps with the global dictionary and it will return the embeddings.</p>
<p>contextual embedding is used to learn sequence-level semantics by considering the sequence of all words in the documents.</p>
<p>but I don't understand where we considered the context in a word embedding.</p>
",Vectorization & Embeddings,difference word vec contextual embedding trying understand difference word embedding contextual embedding understanding please add find correction word embedding algorithm ha global vocabulary dictionary word performing word vec input corpus unique word map global dictionary return embeddings contextual embedding used learn sequence level semantics considering sequence word document understand considered context word embedding
specify task_type for embeddings in Vertex AI,"<p>Has someone tried the last update of GCP TextEmbeddingInput that allows to specify the task_type of your application? Theoretically it should allows you to use different fine tuned models to generate embeddings but I cannot see any difference between embeddings generated for different task_type.</p>
<pre><code>from vertexai.language_models import TextEmbeddingModel
from vertexai.language_models import TextEmbeddingInput

def text_embedding():
    &quot;&quot;&quot;Text embedding with a Large Language Model.&quot;&quot;&quot;
    model = TextEmbeddingModel.from_pretrained(&quot;textembedding-gecko-multilingual@001&quot;)
    embeddings = model.get_embeddings([&quot;La vita è un viaggio, ma non è solo un percorso da seguire. È un'esperienza che ci forma e ci cambia. È un'opportunità per imparare e crescere, per affrontare le sfide e superare gli ostacoli. È un'occasione per conoscere noi stessi e gli altri, per creare legami e costruire relazioni.&quot;])
    return embeddings

def text_embedding2():
    &quot;&quot;&quot;Text embedding with a Large Language Model.&quot;&quot;&quot;
    model = TextEmbeddingModel.from_pretrained(&quot;textembedding-gecko-multilingual@001&quot;)
    text = TextEmbeddingInput(text=&quot;La vita è un viaggio, ma non è solo un percorso da seguire. È un'esperienza che ci forma e ci cambia. È un'opportunità per imparare e crescere, per affrontare le sfide e superare gli ostacoli. È un'occasione per conoscere noi stessi e gli altri, per creare legami e costruire relazioni.&quot;, task_type='CLUSTERING')
    embeddings = model.get_embeddings([text])
    return embeddings

text_embedding2()[0].values == text_embedding()[0].values
</code></pre>
",Vectorization & Embeddings,specify task type embeddings vertex ai ha someone tried last update gcp textembeddinginput allows specify task type application theoretically allows use different fine tuned model generate embeddings see difference embeddings generated different task type
How to calculate the weighted sum of last 4 hidden layers using Roberta?,"<p>The table from this <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">paper</a> that explains various approaches to obtain the embedding, I think these approaches are also applicable to Roberta too:</p>
<p><a href=""https://i.sstatic.net/C7rQY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/C7rQY.png"" alt=""enter image description here"" /></a></p>
<p>I'm trying to calculate the weighted sum of last 4 hidden layers using Roberta to obtain token embedding, but I don't know if this is the correct way to do, this is the code I have tried:</p>
<pre><code>from transformers import RobertaTokenizer, RobertaModel
import torch

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')
caption = ['this is a yellow bird', 'example caption']

tokens = tokenizer(caption, return_tensors='pt', padding=True)

input_ids = tokens['input_ids']
attention_mask = tokens['attention_mask']

output = model(input_ids, attention_mask, output_hidden_states=True)

states = output.hidden_states
token_emb = torch.stack([states[i] for i in [-4, -3, -2, -1]]).sum(0).squeeze()
</code></pre>
",Vectorization & Embeddings,calculate weighted sum last hidden layer using roberta table paper explains various approach obtain embedding think approach also applicable roberta trying calculate weighted sum last hidden layer using roberta obtain token embedding know correct way code tried
Finding embedding dimentions of the HuggingFace model,"<p>I try to figure out how to use <code>faiss</code> Vectore Store with <code>LlamaIndex</code>.</p>
<p>Instruction says, that I must indicate vector dimensions in advance. Here is the code:</p>
<pre><code>    import faiss

    # dimensions of text-ada-embedding-002
    d = 1536
    faiss_index = faiss.IndexFlatL2(d)
</code></pre>
<p>So, dimensions of <code>text-ada-embedding-002</code> model is <code>1536</code>.</p>
<p>I want to use <code>BAAI/bge-small-en-v1.5</code> model.</p>
<p>What does embedding vector dimensions it outputs?
How do I find the output vector dimensions of other transformer models? Do I need to run the models and measure the results, or is there a simpler way?</p>
",Vectorization & Embeddings,finding embedding dimentions huggingface model try figure use vectore store instruction say must indicate vector dimension advance code dimension model want use model doe embedding vector dimension output find output vector dimension transformer model need run model measure result simpler way
Reciprocal rank fusion in PySpark,"<p>I'm dealing with a large scale (10M+) retrieval problem, where I have <code>q</code> queries and <code>D</code> documents. I've computed the top <code>k</code> nearest documents for each query using 4 embedding models. Now I want to rerank these 3 sets of results using reciprocal rank fusion. All the implementations that I could find use for loop and that doesn't seem feasible since sequentially iterating across so many number of queries will take a lot of time.</p>
<p>For clarity, my similarity matrices look like below:</p>
<pre><code>Embed_1: &quot;query_1&quot;: {&quot;doc_10&quot;: 0.3, &quot;doc_11&quot;: 0.37, &quot;doc_94&quot;: 0.38, &quot;doc_1&quot;: 0.5, ...}
Embed_2: &quot;query_1&quot;: {&quot;doc_5&quot;: 0.06, &quot;doc_96&quot;: 0.09, &quot;doc_10&quot;: 0.12, &quot;doc_8&quot;: 0.3, ...}
Embed_3: &quot;query_1&quot;: {&quot;doc_11&quot;: 0.49, &quot;doc_2&quot;: 0.82, &quot;doc_37&quot;: 0.97, &quot;doc_4&quot;: 1.0, ...}
</code></pre>
<p>I want top <code>k</code> document IDs reranked using RRF for <code>query_1</code>. I tried using multiprocessing but CPU of a machine is a bottleneck, but PySpark can scale to multiple nodes and complete this sooner.</p>
<p>Let me know if this question needs more clarity.</p>
",Vectorization & Embeddings,reciprocal rank fusion pyspark dealing large scale retrieval problem query document computed top nearest document query using embedding model want rerank set result using reciprocal rank fusion implementation could find use loop seem feasible since sequentially iterating across many number query take lot time clarity similarity matrix look like want top document id reranked using rrf tried using multiprocessing cpu machine bottleneck pyspark scale multiple node complete sooner let know question need clarity
Why token embedding different from the embedding by the BartForConditionalGeneration model,"<p>Why both the embeddings are different even when i generate them using same BartForConditionalGenration model?</p>
<p>First embedding is generated by combining token embedding and positional embedding from</p>
<pre><code>embed_pos = modelBART.model.encoder.embed_positions(input_ids.input_ids)
inputs_embeds = modelBART.model.encoder.embed_tokens(input_ids.input_ids)
</code></pre>
<p>The Second embedding by the model via</p>
<pre><code>output = modelBART(input_ids.input_ids)
print(&quot;\n\n output: \n\n&quot;,output.encoder_last_hidden_state)
</code></pre>
<p>Shouldn't the embedding by first and second be same? What to do so that difference of the embedding from first and second be zero?</p>
",Vectorization & Embeddings,token embedding different embedding bartforconditionalgeneration model embeddings different even generate using bartforconditionalgenration model first embedding generated combining token embedding positional embedding second embedding model via embedding first second difference embedding first second zero
Sentence Similarity between a phrase with 2-3 words and documents with multiple sentences,"<p><strong>What I want to achieve:</strong> I have thousands of documents (descriptions of incidents) and I would like to find the documents which match a phrase or are similar to the words in the phrase. An example, for an input phrase, &quot;electric vehicle&quot;, I would like to find all the documents that has any discussion related to anything happening with any type of electric vehicle or conveyance, the documents in the corpus might not have the word &quot;vehicle&quot;, but may have the specific vehicle type mentioned, like &quot;scooter&quot;, &quot;bicycle&quot;, &quot;hoverboard&quot; etc,. and document may have the word &quot;electrical&quot; or even something like &quot;lithium battery of a &quot;. So, from an input phrase like &quot;an electric vehicle&quot; or &quot;an electric automobile&quot; or &quot;vehicle powered by a lithium-ion battery&quot;, I need to find out all the documents that has related mentions to that term. But, I don't want to capture the documents with &quot;automobile&quot;, &quot;scooter&quot; that doesn't have any mention of &quot;electric&quot; or &quot;lithium-ion&quot;. So, from a phrase with 1 to 4 words, I must find matching documents containing anywhere from 2 to 100 words used for 1 to 7 sentences in each document.</p>
<p>And the list of input phrases (that are used to find matching documents) will vary, hence something like Siamese-networks or even training a classification model can't be done I suppose.
And the count of documents will also keep increasing by day and each of the document is independent of each other.</p>
<p><strong>Here's what I have done till now:</strong>
I have used <a href=""https://www.sbert.net/"" rel=""nofollow noreferrer"">sentence-transformers</a> (tried the <a href=""https://www.sbert.net/docs/pretrained_models.html"" rel=""nofollow noreferrer"">pre-trained models</a>, <code>multi-qa-mpnet-base-dot-v1</code>, <code>all-MiniLM-L12-v2</code>, <code>all-MiniLM-L16-v2</code> and <code>all-mpnet-base-v2</code>), to get normalized embeddings for all the documents, then my input phrase. and then computed cosine-similarity between my input phrase's embeddings with all the documents, then get the top 20 sentences with highest values.</p>
<p>The matched documents were barely relevant. For ex, for input phrase &quot;an electrical vehicle&quot; matches documents, with highest cosine-similarity, containing nothing but the word &quot;electrical&quot;, followed by documents with only &quot;vehicle&quot;, then documents with only &quot;electrical vehicle&quot; or a bit more words or the same 2 words in different forms, followed by documents just a bit more words but having mentions only of &quot;vehicle&quot; without &quot;electrical&quot; and vice-versa. I presume, because of the less count of words in the input phrase.</p>
<p>How do I counter this and find documents that actually mention all the words in my input phrase instead of just using one word to find the matching documents?</p>
",Vectorization & Embeddings,sentence similarity phrase word document multiple sentence want achieve thousand document description incident would like find document match phrase similar word phrase example input phrase electric vehicle would like find document ha discussion related anything happening type electric vehicle conveyance document corpus might word vehicle may specific vehicle type mentioned like scooter bicycle hoverboard etc document may word electrical even something like lithium battery input phrase like electric vehicle electric automobile vehicle powered lithium ion battery need find document ha related mention term want capture document automobile scooter mention electric lithium ion phrase word must find matching document containing anywhere word used sentence document list input phrase used find matching document vary hence something like siamese network even training classification model done suppose count document also keep increasing day document independent done till used sentence transformer tried pre trained model get normalized embeddings document input phrase computed cosine similarity input phrase embeddings document get top sentence highest value matched document barely relevant ex input phrase electrical vehicle match document highest cosine similarity containing nothing word electrical followed document vehicle document electrical vehicle bit word word different form followed document bit word mention vehicle without electrical vice versa presume le count word input phrase counter find document actually mention word input phrase instead using one word find matching document
What is the correct method to calculate contextualized embeddings using Roberta?,"<p>I'm trying to calculate contextualized embeddings using the <code>RobertaModel</code>. However, I'm not sure about the correct approach, I have tried two methods, the first method is as follows:</p>
<pre><code>from transformers import RobertaModel, RobertaTokenizer
import torch

model = RobertaModel.from_pretrained('roberta-base')
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

captions = [&quot;example caption&quot;, &quot;colorful bird&quot;]

tokenized_captions = tokenizer(captions, return_tensors='pt', padding=True)

input_ids = tokenized_captions['input_ids']
attention_mask = tokenized_captions['attention_mask']

output = model(input_ids, attention_mask)
contextualized_embedding = output.last_hidden_state

print(contextualized_embedding)
</code></pre>
<p>The output for the first method:</p>
<pre><code>tensor([[[-0.0524,  0.0920, -0.0036,  ..., -0.0810, -0.0577, -0.0343],
         [ 0.0383, -0.1632,  0.1182,  ..., -0.3812, -0.1962, -0.0302],
         [-0.0504,  0.0927, -0.0312,  ..., -0.1355, -0.0663, -0.0727],
         ...,
         [-0.0478, -0.1817,  0.1025,  ..., -0.1187, -0.1293, -0.0288],
         [-0.0478, -0.1817,  0.1025,  ..., -0.1187, -0.1293, -0.0288],
         [-0.0478, -0.1817,  0.1025,  ..., -0.1187, -0.1293, -0.0288]],
</code></pre>
<p>The output includes non-zero weights for padding tokens. For the second method, the code is essentially the same, with the differences are the lines for <code>output</code> and <code>contextualized_embedding</code>:</p>
<pre><code>output = model(input_ids)
contextualized_embedding = output.last_hidden_state * attention_mask.unsqueeze(-1).float()
</code></pre>
<p>The output for the second method:</p>
<pre><code>tensor([[[-0.0502,  0.0605, -0.0332,  ..., -0.1521, -0.0830, -0.0196],
         [ 0.0178, -0.2104,  0.1093,  ..., -0.5506, -0.2059, -0.0825],
         [-0.0484,  0.0579, -0.0615,  ..., -0.2082, -0.0855, -0.0473],
         ...,
         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],
         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],
         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000]],
</code></pre>
<p>In this case, the output shows zeros for padding tokens, I'm not sure about the correct method, can anyone help me clarify this ?</p>
",Vectorization & Embeddings,correct method calculate contextualized embeddings using roberta trying calculate contextualized embeddings using however sure correct approach tried two method first method follows output first method output includes non zero weight padding token second method code essentially difference line output second method case output show zero padding token sure correct method anyone help clarify
Why does local inference differ from the API when computing Jina embeddings?,"<p>I am computing <a href=""https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/"" rel=""nofollow noreferrer"">Jina v2 embeddings</a> via the <code>transformers</code> Python libraries and via the API (see <a href=""https://jina.ai/embeddings/"" rel=""nofollow noreferrer"">https://jina.ai/embeddings/</a>).</p>
<p>With <code>transformers</code> I can run something like</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModel

sentences = ['How is the weather today?']

model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)
embeddings_1 = model.encode(sentences)
</code></pre>
<p>or</p>
<pre class=""lang-py prettyprint-override""><code>from sentence_transformers import SentenceTransformer

model = SentenceTransformer('jinaai/jina-embeddings-v2-base-en')
embeddings_2 = model.encode(sentences)
</code></pre>
<p>and the resulting <code>embeddings_1</code> and <code>embeddings_2</code> match.</p>
<p>However if I use the Jina API e.g. via</p>
<pre class=""lang-py prettyprint-override""><code>import requests

url = 'https://api.jina.ai/v1/embeddings'

headers = {
  'Content-Type': 'application/json',
  'Authorization': 'Bearer jina_123456...' # visit https://jina.ai/embeddings/ for an API key
}

data = {
  'input': sentences,
  'model': 'jina-embeddings-v2-base-en' # note that the model name matches
}

response = requests.post(url, headers=headers, json=data)
embeddings_3 = eval(response.content)[&quot;data&quot;][0][&quot;embedding&quot;]
</code></pre>
<p><code>embeddings_3</code> differ from the other two arrays by a small difference, around 2e-4 in absolute value on average. I see this discrepancy both with CPU and GPU runtimes. What am I doing wrong?</p>
",Vectorization & Embeddings,doe local inference differ api computing jina embeddings computing jina v embeddings via python library via api see run something like resulting match however use jina api e g via differ two array small difference around e absolute value average see discrepancy cpu gpu runtimes wrong
How to query embeddings for semantic search?,"<p>I have 1000 description for some SKU merchandise and I want to generate inverse embedding mapping to do semantic search</p>
<p>For example here is what I have</p>
<pre><code>item   description
item1  [word1, word2, word3, word4..........]
item2  [word1, word2_2, word3_3, word4_4..........]
</code></pre>
<p>As you can see <code>item1</code> and <code>item2</code> shares <code>word1</code>, but <code>item1</code> and <code>item2</code> has two different context, by generating embedding, we should be able to capture the context of each word</p>
<p>Here is how i generate embeddings</p>
<pre><code>my_description = []
with open('/content/gdrive/My Drive/my.csv', 'r') as data:
    df = pd.read_csv(data, encoding = ('utf-8'),nrows=100)
    for index, row in df.iterrows():
        my_str = row['description']
        my_description.append(my_str)



import torch
from transformers import BertTokenizer, BertModel
%matplotlib inline
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased',
output_hidden_states = True, # Whether the model returns all hidden-states.
)
model.eval()


text2 = company_description[0]

# Add the special tokens.
marked_text2 = &quot;[CLS] &quot; + text2 + &quot; [SEP]&quot;

# Split the sentence into tokens.
tokenized_text2 = tokenizer.tokenize(marked_text2)

# Map the token strings to their vocabulary indeces.
indexed_tokens2 = tokenizer.convert_tokens_to_ids(tokenized_text2)

segments_ids2 = [1] * len(tokenized_text2)
tokens_tensor2 = torch.tensor([indexed_tokens2])
segments_tensors2 = torch.tensor([segments_ids2])

with torch.no_grad():
    outputs2 = model(tokens_tensor2, segments_tensors2)
    hidden_states2 = outputs2[2]

token_embeddings2 = torch.stack(hidden_states2, dim=0)
token_embeddings2.size()
token_embeddings2 = torch.squeeze(token_embeddings2, dim=1)
token_embeddings2.size()
token_embeddings2 = token_embeddings2.permute(1,0,2)
token_embeddings2.size()

token_vecs_cat2 = [] 

for token in token_embeddings2:
     cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)
     token_vecs_cat2.append(cat_vec)
token_vecs_sum2 = []
import numpy as np
x_token = np.empty((0, 768))

for token in token_embeddings2:
    sum_vec = torch.sum(token[-4:], dim=0)
    token_vecs_sum2.append(sum_vec)
    x_token = np.concatenate((x_token, sum_vec.numpy().reshape((1,-1))), axis=0)
</code></pre>
<p><code>x_token</code> would be the embeddings for all my word/token in one description
For example say that item1 has 500 tokens and embedding is 700
the shape of <code>x_token</code> would be (500 x 700)</p>
<p>so for each item i would have something like this</p>
<pre><code>item        token          embeddings
item 1      token 1        [x1,x2,x3,.....] 
item 1      token 2        [x1,x2,x3,.....] 
....
item 2      token 1_2      [x1,x2,x3,.....] 
item 2      token 2_2      [x1,x2,x3,.....] 
....
item n      token 1_n      [x1,x2,x3,.....] 
item n      token 2_n      [x1,x2,x3,.....] 
</code></pre>
<p>Now my question is how do i perform search</p>
<p>If my search query is a sentence</p>
<p>&quot;word1 word2 word3.....wordn&quot;</p>
<p>If I generate embedding for each word in the sentence and perform ANN for top 10 nearest neighbor for each token</p>
<p>If my query has 10 tokens, I would get 100 item description back (10 for each token)
In that case, how do i shortlist to top 10 item description? Which token should i use?</p>
<pre><code>query = [token1, token2.......tokenN]

                   top 10 nearest_neighbor's item, 
query_token1 -&gt;    [itemx1_1, itemx1_2, itemx1_10]
query_token2 -&gt;    [itemx2_1, itemx2_2, itemx2_10]
</code></pre>
<p>Am i doing semantic search wrong?</p>
",Vectorization & Embeddings,query embeddings semantic search description sku merchandise want generate inverse embedding mapping semantic search example see share ha two different context generating embedding able capture context word generate embeddings would embeddings word token one description example say item ha token embedding shape would x item would something like question perform search search query sentence word word word wordn generate embedding word sentence perform ann top nearest neighbor token query ha token would get item description back token case shortlist top item description token use semantic search wrong
3 Dimensions / Repeated output in LLAMA 2 for Word embedding,"<p>I'm trying to get output[0] in LLAMA 2 with AutoModelForCausalLM, in the code:</p>
<pre><code>with torch.no_grad():
    outputs = model(features['input_ids'].to(device),features['attention_mask'].to(device),output_hidden_states=True)
cls_train = outputs[0]
aux = cls_train.to(&quot;cpu&quot;)
Y = database['label']
</code></pre>
<p>But output[0] has 3 dimensions and the chosen machine learning models (logistic regression, svm)  only use 2. Then, i did:</p>
<pre><code>new_aux = []
for x in aux:
  new_aux.append(x[0])
vec = torch.stack(new_aux, dim=0)
</code></pre>
<p>To get just the two dimensions used in the model, but the resulting tensor is coming with the repeated values. What can I do?</p>
<p>PS: I tried using the last_hidden_state, but, apparently, this model does not have. The tokenizer didn't have the pad_token, so I did tokenizer.add_special_tokens({'pad_token': '[PAD]'}). I don't know if that influences it.</p>
",Vectorization & Embeddings,dimension repeated output llama word embedding trying get output llama automodelforcausallm code output ha dimension chosen machine learning model logistic regression svm use get two dimension used model resulting tensor coming repeated value p tried using last hidden state apparently model doe tokenizer pad token tokenizer add special token pad token pad know influence
How to tie word embedding and softmax weights in keras?,"<p>Its commonplace for various neural network architectures in NLP and vision-language problems to tie the weights of an initial word embedding layer to that of an output softmax. Usually this produces a boost to sentence generation quality. (see example <a href=""https://arxiv.org/pdf/1608.05859.pdf"" rel=""noreferrer"">here</a>)</p>

<p>In Keras its typical to embed word embedding layers using the Embedding class, however there seems to be no easy way to tie the weights of this layer to the output softmax. Would anyone happen to know how this could be implemented ?</p>
",Vectorization & Embeddings,tie word embedding softmax weight kera commonplace various neural network architecture nlp vision language problem tie weight initial word embedding layer output softmax usually produce boost sentence generation quality see example kera typical embed word embedding layer using embedding class however seems easy way tie weight layer output softmax would anyone happen know could implemented
How to Find Positional embeddings from BARTTokenizer?,"<p>The objective is to add token embeddings (customized- obtained using different model) and the positional Embeddings.</p>
<p>Is there a Way I can find out positonal embedding along with the token embeddings for an article(length 500-1000 words) using BART model.</p>
<pre><code>tokenized_sequence = tokenizer(sentence, padding='max_length', truncation=True, max_length=512, return_tensors=&quot;pt&quot;)
</code></pre>
<p>the output is <code>input_ids</code> and <code>attention_mask</code> but not parameter to return <code>position_ids</code> like in BERT model.</p>
<pre><code>bert.embeddings.position_embeddings('YOUR_POSITIONS_IDS')
</code></pre>
<p>Or the only way to obtain Positional Embedding is using sinusoidal positional encoding?</p>
",Vectorization & Embeddings,find positional embeddings barttokenizer objective add token embeddings customized obtained using different model positional embeddings way find positonal embedding along token embeddings article length word using bart model output parameter return like bert model way obtain positional embedding using sinusoidal positional encoding
How to calculate word and sentence embedding using Roberta?,"<p>I'm trying to calculate word and sentence embeddings using Roberta, for word embeddings, I extract the last hidden state <code>outputs[0]</code> from the <code>RobertaModel</code> class, but I'm not sure if this is the correct way to calculate.</p>
<p>As for sentence embeddings, I don't know how to calculate them, this is the code I have tried:</p>
<pre><code>from transformers import RobertaModel, RobertaTokenizer
import torch

model = RobertaModel.from_pretrained('roberta-base')
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
captions = [&quot;example caption&quot;, &quot;lorem ipsum&quot;, &quot;this bird is yellow has red wings&quot;, &quot;hi&quot;, &quot;example&quot;]

encoded_captions = [tokenizer.encode(caption) for caption in captions]

# Pad sequences to the same length with 0s
max_len = max(len(seq) for seq in encoded_captions)
padded_captions = [seq + [0] * (max_len - len(seq)) for seq in encoded_captions]

# Convert to a PyTorch tensor with batch size 5
input_ids = torch.tensor(padded_captions)

outputs = model(input_ids)
word_embedding = outputs[0].contiguous()
sentence_embedding = ?????
</code></pre>
<p>How to calculate word and sentence embeddings using Roberta?</p>
",Vectorization & Embeddings,calculate word sentence embedding using roberta trying calculate word sentence embeddings using roberta word embeddings extract last hidden state class sure correct way calculate sentence embeddings know calculate code tried calculate word sentence embeddings using roberta
How to query questions with high similarity based on the input question content?,"<p>I have a student exam system in Java. There are more than one million questions in the mysql database. The question content consists of Chinese, English, and latex mathematical formulas.</p>
<p>Now, I want to query questions with identical or very high similarity to the content of newly input questions from teachers.</p>
<p>How to solve this problem?</p>
<p>I want to use nlp to deal with this problem, but I don't know how to start.(gensim Word2Vec Doc2Vec)</p>
<p>I previously stored the question content in postgresql and searched for similar questions through long text. This worked well for Chinese questions with more content, but it did not work well for short English sentences.</p>
",Vectorization & Embeddings,query question high similarity based input question content student exam system java one million question mysql database question content consists chinese english latex mathematical formula want query question identical high similarity content newly input question teacher solve problem want use nlp deal problem know start gensim word vec doc vec previously stored question content postgresql searched similar question long text worked well chinese question content work well short english sentence
CLIP: Cosine Similarity of Text and Image Embeddings is low,"<p>I am using the <a href=""https://huggingface.co/openai/clip-vit-base-patch32"" rel=""nofollow noreferrer"">HuggingFace CLIP Model</a> for generating text and image embeddings with <code>get_text_features</code> and <code>get_image_features</code>. When calculating the cosine similarity it is surprisingly low - especially compared to other embedding models where I am used to values above 0.65. For example, for a photo of <a href=""https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg/1920px-Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg"" rel=""nofollow noreferrer"">a cat</a> and <a href=""https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/Labrador_Retriever_portrait.jpg/580px-Labrador_Retriever_portrait.jpg"" rel=""nofollow noreferrer"">a dog</a> and the texts <code>[&quot;cat&quot;, &quot;dog&quot;]</code> I get the following similarity matrix</p>
<pre><code>urls = [
    &quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg/1920px-Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg&quot;,
    &quot;https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/Labrador_Retriever_portrait.jpg/580px-Labrador_Retriever_portrait.jpg&quot;,
]
images = [Image.open(requests.get(url, stream=True).raw) for url in urls]
texts = [&quot;cat&quot;, &quot;dog&quot;]
with torch.no_grad():
    image_inputs = processor(
        text=texts,
        images=images,
        return_tensors=&quot;pt&quot;,
        padding=True,
        truncation=True,
    )
with torch.no_grad():
    clip_text_embeddings = model.get_text_features(image_inputs[&quot;input_ids&quot;])
    clip_text_embeddings = clip_text_embeddings / clip_text_embeddings.norm(
        dim=-1, keepdim=True
    )
    clip_image_embeddings = model.get_image_features(image_inputs[&quot;pixel_values&quot;])
    clip_image_embeddings = clip_image_embeddings / clip_image_embeddings.norm(
        dim=-1, keepdim=True
    )
    clip_cos_sim = torch.mm(clip_text_embeddings, clip_image_embeddings.T)
print(clip_cos_sim)

tensor([[0.2696, 0.1795],
        [0.2288, 0.2597]])
</code></pre>
<p>Are these expected values for CLIP or am I doing something wrong here?</p>
",Vectorization & Embeddings,clip cosine similarity text image embeddings low using huggingface clip model generating text image embeddings calculating cosine similarity surprisingly low especially compared embedding model used value example photo cat dog text get following similarity matrix expected value clip something wrong
How to calculate word and sentence embedding using GPT-2?,"<p>I'm working on a program that calculates word and sentence embeddings using GPT-2, specifically the <code>GPT2Model</code> class. For word embedding, I extract the last hidden state <code>outputs[0]</code> after forwarding the <code>input_ids</code>, that has a shape of <code>batch size x seq len</code>, to the <code>GPT2Model</code> class. As for sentence embedding, I extract the hidden state of the word at the end of sequence. This is the code I have tried:</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2Model
import torch

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')
captions = [&quot;example caption&quot;, &quot;example bird&quot;, &quot;the bird is yellow has red wings&quot;, &quot;hi&quot;, &quot;very good&quot;]

encoded_captions = [tokenizer.encode(caption) for caption in captions]

# Pad sequences to the same length with 0s
max_len = max(len(seq) for seq in encoded_captions)
padded_captions = [seq + [0] * (max_len - len(seq)) for seq in encoded_captions]

# Convert to a PyTorch tensor with batch size 5
input_ids = torch.tensor(padded_captions)

outputs = model(input_ids)
word_embedding = outputs[0].contiguous()
sentence_embedding = word_embedding[ :, -1, : ].contiguous()

</code></pre>
<p>I'm not sure if my calculation for word and sentence embedding are correct, can anyone help me confirm this?</p>
",Vectorization & Embeddings,calculate word sentence embedding using gpt working program calculates word sentence embeddings using gpt specifically class word embedding extract last hidden state forwarding ha shape class sentence embedding extract hidden state word end sequence code tried sure calculation word sentence embedding correct anyone help confirm
Find vocab word from vector for flexible comparisons,"<p>Is it possible to find a vocab word from a vector so that I can do more flexible comparisons?</p>
<p>Something like this:</p>
<pre><code>queen = nlp.vocab[&quot;king&quot;].vector - nlp.vocab[&quot;man&quot;].vector + nlp.vocab[&quot;woman&quot;].vector
king = nlp.vocab[&quot;queen&quot;].vector - nlp.vocab[&quot;woman&quot;].vector + nlp.vocab[&quot;man&quot;].vector
queen.similarity(king)
</code></pre>
<p>I realize in this example I could just check the similarity of king and queen directly, but my use-case is that I'd like to compare sentence/doc similarity and I read that in order to get sentence vector I can simply add up the words in a given sentence and I'm assuming that means I can compare them as well.</p>
",Vectorization & Embeddings,find vocab word vector flexible comparison possible find vocab word vector flexible comparison something like realize example could check similarity king queen directly use case like compare sentence doc similarity read order get sentence vector simply add word given sentence assuming mean compare well
Getting sentence embedding from huggingface Feature Extraction Pipeline,"<p>How do i get an embedding for the whole sentence from huggingface's feature extraction pipeline?</p>
<p>I understand how to get the features for each token (below) but how do i get the overall features for the sentence as a whole?</p>
<pre><code>feature_extraction = pipeline('feature-extraction', model=&quot;distilroberta-base&quot;, tokenizer=&quot;distilroberta-base&quot;)
features = feature_extraction(&quot;i am sentence&quot;)
</code></pre>
",Vectorization & Embeddings,getting sentence embedding huggingface feature extraction pipeline get embedding whole sentence huggingface feature extraction pipeline understand get feature token get overall feature sentence whole
How are tokens mapped with words in embeddings vector space?,"<p>I have been trying to learn some basics of AI using spacy, and trying to understand the internals of tokenization/embeddings, before moving on to complex topics like transformers. My question is generic, but am asking it in the context of spacy.</p>
<p>Specifically, how does spacy map a token it generates for the word in the input text with its corresponding embedding? What is the common entity that associates the two- how are they &quot;joined&quot;? I have read that tokenization and embeddings can be done separately, by different algorithms. This makes it even more difficult to understand how spacy's tokens can be associated with non-spacy created embeddings!  I wonder if spacy extracts the lemma (word) from its token, and then checks if that word/lemma is in the vector space. Is token stored as some number by spacy, or as some data structure? Is there some &quot;standard&quot; to map token-to-word-in-embeddings mappings??</p>
<p>As an example, if word2vec embeddings are used, how does spacy map the tokens it generates with the data in this vector space?? There is something tying the two databases(?) that I am not able to grasp.</p>
<p>Thanks for any clarifications on some underlying mechanics!</p>
<p>Read/saw a number of articles/posts/videos, but feel that most gloss over such details. A simple end-to-end explanation of the workflow, with just enough details would be very helpful.</p>
",Vectorization & Embeddings,token mapped word embeddings vector space trying learn basic ai using spacy trying understand internals tokenization embeddings moving complex topic like transformer question generic asking context spacy specifically doe spacy map token generates word input text corresponding embedding common entity associate two joined read tokenization embeddings done separately different algorithm make even difficult understand spacy token associated non spacy created embeddings wonder spacy extract lemma word token check word lemma vector space token stored number spacy data structure standard map token word embeddings mapping example word vec embeddings used doe spacy map token generates data vector space something tying two database able grasp thanks clarification underlying mechanic read saw number article post video feel gloss detail simple end end explanation workflow enough detail would helpful
What meaning does the length of a Word2vec vector have?,"<p>I am using Word2vec through <a href=""https://radimrehurek.com/gensim/"" rel=""noreferrer""><em>gensim</em></a> with Google's pretrained vectors trained on Google News. I have noticed that the word vectors I can access by doing direct index lookups on the <code>Word2Vec</code> object are not unit vectors:</p>

<pre><code>&gt;&gt;&gt; import numpy
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; w2v = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
&gt;&gt;&gt; king_vector = w2v['king']
&gt;&gt;&gt; numpy.linalg.norm(king_vector)
2.9022589
</code></pre>

<p>However, in the <a href=""https://github.com/piskvorky/gensim/blob/0.12.4/gensim/models/word2vec.py#L1153-L1213"" rel=""noreferrer""><code>most_similar</code></a> method, these non-unit vectors are not used; instead, normalised versions are used from the undocumented <code>.syn0norm</code> property, which contains only unit vectors:</p>

<pre><code>&gt;&gt;&gt; w2v.init_sims()
&gt;&gt;&gt; unit_king_vector = w2v.syn0norm[w2v.vocab['king'].index]
&gt;&gt;&gt; numpy.linalg.norm(unit_king_vector)
0.99999994
</code></pre>

<p>The larger vector is just a scaled up version of the unit vector:</p>

<pre><code>&gt;&gt;&gt; king_vector - numpy.linalg.norm(king_vector) * unit_king_vector
array([  0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,
         0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,
        -7.45058060e-09,   0.00000000e+00,   3.72529030e-09,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
        ... (some lines omitted) ...
        -1.86264515e-09,  -3.72529030e-09,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32)
</code></pre>

<p>Given that word similarity comparisons in Word2Vec are done by <a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" rel=""noreferrer"">cosine similarity</a>, it's not obvious to me what the lengths of the non-normalised vectors mean - although I assume they mean <em>something</em>, since gensim exposes them to me rather than only exposing the unit vectors in <code>.syn0norm</code>.</p>

<p>How are the lengths of these non-normalised Word2vec vectors generated, and what is their meaning? For what calculations does it make sense to use the normalised vectors, and when should I use the non-normalised ones?</p>
",Vectorization & Embeddings,meaning doe length word vec vector using word vec gensim google pretrained vector trained google news noticed word vector access direct index lookup object unit vector however method non unit vector used instead normalised version used undocumented property contains unit vector larger vector scaled version unit vector given word similarity comparison word vec done cosine similarity obvious length non normalised vector mean although assume mean something since gensim expose rather exposing unit vector length non normalised word vec vector generated meaning calculation doe make sense use normalised vector use non normalised one
How to understand contextualized embeddings in Transformer?,"<p>As, The input to transformers is essentially a sequence of tokens, each represented as one-hot vectors. These vectors are subsequently multiplied by an embedding matrix (E) to generate the input embeddings (X). This embedding matrix is a learned parameter during the training process. In mathematical terms, this process can be represented as X = E * I, where I stands for the input one-hot vectors.</p>
<p>so if the embedding layer just acts as look-up table to grab a learned vector representation of each token then how the embedding for word <code>left</code> have two different representations in the embedding space for below sentence ?</p>
<p>&quot;I <strong>left</strong> my phone on the <strong>left</strong> side of the table.&quot;</p>
",Vectorization & Embeddings,understand contextualized embeddings transformer input transformer essentially sequence token represented one hot vector vector subsequently multiplied embedding matrix e generate input embeddings x embedding matrix learned parameter training process mathematical term process represented x e stand input one hot vector embedding layer act look table grab learned vector representation token embedding word two different representation embedding space sentence left phone left side table
How to customize the number of encoders/decoders in a pre-trained transformer,"<p>I am implementing a pretrained transformer model using Python's transformer module to perform text summarization and I would like to compare the performance of the fine-tuned BART transformer given different number of encoders. My question is, how can I customize the number of encoders? The default transformer has 12 encoders, what if say I want to keep only the first 6 encoders?
I found the following documentation for <a href=""https://huggingface.co/transformers/v3.1.0/_modules/transformers/configuration_bart.html#BartConfig"" rel=""nofollow noreferrer"">BART</a> but I have no idea how to adapt it to my code (see below).
I am new to ML and NLP so I'd be grateful if you could provide me with detailed explanation with code, Thank you!</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq

tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)
collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

# preprocessing step omitted
# tokenized_data = preprocessed data

args = transformers.Seq2SeqTrainingArguments(
    'conversation-summ',
    evaluation_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=1,
    per_device_eval_batch_size= 1,
    gradient_accumulation_steps=2,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=3,
    predict_with_generate=True,
    eval_accumulation_steps=1,
    fp16=True
    )

trainer = transformers.Seq2SeqTrainer(
    model, 
    args,
    train_dataset=tokenized_data['train'],
    eval_dataset=tokenized_data['validation'],
    data_collator=collator,
    tokenizer=tokenizer,
    compute_metrics=compute_rouge
)

trainer.train()
</code></pre>
",Vectorization & Embeddings,customize number encoders decoder pre trained transformer implementing pretrained transformer model using python transformer module perform text summarization would like compare performance fine tuned bart transformer given different number encoders question customize number encoders default transformer ha encoders say want keep first encoders found following documentation bart idea adapt code see new ml nlp grateful could provide detailed explanation code thank
NLP classification with sparse and numerical features crashes,"<p>I have a dataset of 10 million english shows, which has been cleaned and lemmatized, and their classification into different category types such as comedy, documentary, action, ... etc</p>
<p>I also have a feature called <code>duration</code>, which is the length of the tv show.</p>
<p>Data can be found <a href=""https://drive.google.com/file/d/16kRQfTo_76yfNzQ2_WHBdNg-U5QAQn6l/view?usp=share_link"" rel=""nofollow noreferrer"">here</a></p>
<p>I perform tfidf vectorization on the titles, which returns a sparse matrix and normalization on the duration column.</p>
<p>Then I want to feed the data to a logistic regression classifier.</p>
<p>side question: I want to know if theres a better way to handle combining a sparse matrix and a numerical column</p>
<p>when I try to do it using <code>todense()</code> or <code>toarray()</code>, It works</p>
<p>When i pass it to the logistic regression function, the notebook crashes. But if i dont have the duration col, which means i dont have to apply the toarray() or todense() function, it works perfectly. Is this a memory issue?</p>
<p>This is my code:</p>
<pre><code>import os

import pandas as pd

from sklearn import metrics
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression

def normalize(df, col = ''):
    mms = MinMaxScaler()
    mms_col = mms.fit_transform(df[[col]])
    return mms_col

def tfidf(X, col = ''):
    tfidf_vectorizer = TfidfVectorizer(max_df = 0.8, max_features = 10000)
    return tfidf_vectorizer.fit_transform(X[col])

def get_training_data(df):
    df = shuffle(pd.read_csv(df).dropna())
    data = df[['name_title', 'Duration']]

    X_duration = normalize(data, col = 'Duration')
    X_sparse = tfidf(data, col = 'name_title')
    X = pd.DataFrame(X_sparse.toarray())

    X['Duration'] = X_duration
    y = df['target']

    return X, y

def logistic_regression(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y)
    lr = LogisticRegression(C = 100.0, random_state = 1, solver = 'lbfgs', multi_class = 'ovr')
    lr.fit(X_train, y_train)
    y_predict = lr.predict(X_test)
    print(y_predict)
    print(&quot;Logistic Regression Accuracy %.3f&quot; %metrics.accuracy_score(y_test, y_predict))

data_path = '../data/'
X, y = get_training_data(os.path.join(data_path, 'podcasts_en_processed.csv'))
print(X.shape) # this prints (971426, 10001)
logistic_regression(X, y)
</code></pre>
",Vectorization & Embeddings,nlp classification sparse numerical feature crash dataset million english show ha cleaned lemmatized classification different category type comedy documentary action etc also feature called length tv show data found perform tfidf vectorization title return sparse matrix normalization duration column want feed data logistic regression classifier side question want know better way handle combining sparse matrix numerical column try using work pas logistic regression function notebook crash dont duration col mean dont apply toarray todense function work perfectly memory issue code
How are the TokenEmbeddings in BERT created?,"<p>In the <a href=""https://arxiv.org/abs/1810.04805"" rel=""noreferrer"">paper describing BERT</a>, there is this paragraph about WordPiece Embeddings. </p>

<blockquote>
  <p>We use WordPiece embeddings (Wu et al.,
  2016) with a 30,000 token vocabulary. The first
  token of every sequence is always a special classification
  token ([CLS]). The final hidden state
  corresponding to this token is used as the aggregate
  sequence representation for classification
  tasks. Sentence pairs are packed together into a
  single sequence. We differentiate the sentences in
  two ways. First, we separate them with a special
  token ([SEP]). Second, we add a learned embedding
  to every token indicating whether it belongs
  to sentence A or sentence B. As shown in Figure 1,
  we denote input embedding as E, the final hidden
  vector of the special [CLS] token as C 2 RH,
  and the final hidden vector for the ith input token
  as Ti 2 RH.
  For a given token, its input representation is
  constructed by summing the corresponding token,
  segment, and position embeddings. A visualization
  of this construction can be seen in Figure 2.
  <a href=""https://i.sstatic.net/QCcYF.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/QCcYF.png"" alt=""Fig 2 from the paper""></a></p>
</blockquote>

<p>As I understand, WordPiece splits Words into wordpieces like #I #like #swim #ing, but it does not generate Embeddings. But I did not find anything in the paper and on other sources how those Token Embeddings are generated. Are they pretrained before the actual Pre-training? How? Or are they randomly initialized? </p>
",Vectorization & Embeddings,tokenembeddings bert created paper describing bert paragraph wordpiece embeddings use wordpiece embeddings wu et al token vocabulary first token every sequence always special classification token cl final hidden state corresponding token used aggregate sequence representation classification task sentence pair packed together single sequence differentiate sentence two way first separate special token sep second add learned embedding every token indicating whether belongs sentence sentence b shown figure denote input embedding e final hidden vector special cl token c rh final hidden vector ith input token ti rh given token input representation constructed summing corresponding token segment position embeddings visualization construction seen figure understand wordpiece split word wordpieces like like swim ing doe generate embeddings find anything paper source token embeddings generated pretrained actual pre training randomly initialized
"Cosine Similarity Involving Embeddings, Do we have to embed the whole sentence/text?","<p>Imagine I have some code as such.  I am using the encode function to create embeddings.  Then from these I would look to calculate a cosine similarity score, after all the model I have selected is geared towards cosine similarity (as opposed to dot-product similarity).</p>
<p>My question is do you always embed the entire string as it is or would you/could you produce cleaning on the two strings before you encode them?  Stopwords out.  Maybe only keep nouns or entities.  Is this a thing, or would the discontinuity/non-grammatical possibility in the resulting strings hurt us?</p>
<pre class=""lang-py prettyprint-override""><code>from sentence_transformers import SentenceTransformer, util
model_name = 'sentence-transformers/multi-qa-mpnet-base-cos-v1'
model = SentenceTransformer(model_name)
phrase1 = 'Some arbitrarily long string from a book'
phrase2 = &quot;This is a another arbitrarily long string from the same book'    
emb1 = model.encode(phrase1)
emb2 = model.encode(phrase2)
</code></pre>
<p>I get a cosine similarity which is not spread out that well.  There isn't enough separation between good matches and bad matches.</p>
",Vectorization & Embeddings,cosine similarity involving embeddings embed whole sentence text imagine code using encode function create embeddings would look calculate cosine similarity score model selected towards cosine similarity opposed dot product similarity question always embed entire string would could produce cleaning two string encode stopwords maybe keep noun entity thing would discontinuity non grammatical possibility resulting string hurt u get cosine similarity spread well enough separation good match bad match
How to see the Embedding of the documents with Chroma (or any other DB) saved in Lang Chain?,"<p>I can see everything but the Embedding of the documents when I used <code>Chroma</code> with <code>Langchain</code> and <code>OpenAI</code> embeddings. It always show me <code>None</code> for that</p>
<p>Here is the code:</p>
<pre><code>for db_collection_name in tqdm([&quot;class1-sub2-chap3&quot;, &quot;class2-sub3-chap4&quot;]):
    documents = []
    doc_ids = []

    for doc_index in range(3):
        cl, sub, chap = db_collection_name.split(&quot;-&quot;)
        content = f&quot;This is {db_collection_name}-doc{doc_index}&quot;
        doc = Document(page_content=content, metadata={&quot;chunk_num&quot;: doc_index, &quot;chapter&quot;:chap, &quot;class&quot;:cl, &quot;subject&quot;:sub})
        documents.append(doc)
        doc_ids.append(str(doc_index))


    # # Initialize a Chroma instance with the original document
    db = Chroma.from_documents(
         collection_name=db_collection_name,
         documents=documents, ids=doc_ids,
         embedding=embeddings, 
         persist_directory=&quot;./data&quot;)
    
     db.persist()
</code></pre>
<p>when I do <code>db.get()</code>, I see everything as expected except <code>embedding</code> is <code>None</code>.</p>
<pre><code>{'ids': ['0', '1', '2'],
 'embeddings': None,
 'documents': ['This is class1-sub2-chap3-doc0',
  'This is class1-sub2-chap3-doc1',
  'This is class1-sub2-chap3-doc2'],
 'metadatas': [{'chunk_num': 0,
   'chapter': 'chap3',
   'class': 'class1',
   'subject': 'sub2'},
  {'chunk_num': 1, 'chapter': 'chap3', 'class': 'class1', 'subject': 'sub2'},
  {'chunk_num': 2, 'chapter': 'chap3', 'class': 'class1', 'subject': 'sub2'}]}
</code></pre>
<p>My <code>embeddings</code> is also working fine as it returns:</p>
<pre><code>len(embeddings.embed_documents([&quot;EMBED THIS&quot;])[0])
&gt;&gt; 1536
</code></pre>
<p>also, in my <code>./data</code> directory I have Embedding file as <code>chroma-embeddings.parquet</code></p>
<hr />
<p>I tried the example with example given in document but it shows <code>None</code> too</p>
<pre><code># Import Document class
from langchain.docstore.document import Document

# Initial document content and id
initial_content = &quot;This is an initial document content&quot;
document_id = &quot;doc1&quot;

# Create an instance of Document with initial content and metadata
original_doc = Document(page_content=initial_content, metadata={&quot;page&quot;: &quot;0&quot;})

# Initialize a Chroma instance with the original document
new_db = Chroma.from_documents(
    collection_name=&quot;test_collection&quot;,
    documents=[original_doc],
    embedding=OpenAIEmbeddings(),  # using the same embeddings as before
    ids=[document_id],
)
</code></pre>
<p>Here also <code>new_db.get()</code> gives me <code>None</code></p>
",Vectorization & Embeddings,see embedding document chroma db saved lang chain see everything embedding document used embeddings always show code see everything expected except also working fine return also directory embedding file tried example example given document show also give
Word2Vec - to be trained on train data or whole data,"<p>I wish to create a word2vec model and want to train it on my local data. so, the question is, should I train word2vec model on my whole data or should I split the data into train and test and then train my word2vec model on train data to avoid data leakage as I intend to perform classification task using ml algorithms. I don't want to use pretrained embeddings.</p>
<p>I've trained word2vec on whole dataset but feel like it will lead to data leakage during ml model building</p>
",Vectorization & Embeddings,word vec trained train data whole data wish create word vec model want train local data question train word vec model whole data split data train test train word vec model train data avoid data leakage intend perform classification task using ml algorithm want use pretrained embeddings trained word vec whole dataset feel like lead data leakage ml model building
Bert fine-tuned for semantic similarity,"<p>I would like to apply fine-tuning Bert to calculate semantic similarity between sentences.
I search a lot websites, but I almost not found downstream about this. </p>

<p>I just found <strong>STS benchmark</strong>.
I wonder if I can use STS benchmark dataset to train a fine-tuning bert model, and apply it to my task. 
Is it reasonable? </p>

<p>As I know, there are a lot method to calculate similarity including cosine similarity, pearson correlation, manhattan distance, etc.
How choose for semantic similarity?</p>
",Vectorization & Embeddings,bert fine tuned semantic similarity would like apply fine tuning bert calculate semantic similarity sentence search lot website almost found downstream found sts benchmark wonder use sts benchmark dataset train fine tuning bert model apply task reasonable know lot method calculate similarity including cosine similarity pearson correlation manhattan distance etc choose semantic similarity
Why is my keras LSTM model predicting the same NER tag for NLP multiclassification problem?,"<p>I'm using CoLL2023 dataset as input and word2vec as the embeddings</p>
<pre><code>import gensim.downloader
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.metrics import f1_score
import string

# Load the Word2Vec model
w2v = gensim.downloader.load('word2vec-google-news-300')

# Load and preprocess the data
def load_and_preprocess_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        conll_data = file.read().splitlines()
    
    # Clean rows with empty lines
    conll_data = [line for line in conll_data if line.strip() != '']
    
    # Split each line into columns
    conll_data = [line.split() for line in conll_data]
        # Remove punctuation from each word
    for i in range(len(conll_data)):
        conll_data[i] = [word.strip(string.punctuation) for word in conll_data[i]]
    
    return conll_data

train_data = load_and_preprocess_data('data/eng.train')
test_data = load_and_preprocess_data('data/eng.testb')
dev_data = load_and_preprocess_data('data/eng.testa')

# Create DataFrames
columns = ['word', 'POS', 'NP', 'label']
train_df = pd.DataFrame(train_data, columns=columns)
test_df = pd.DataFrame(test_data, columns=columns)
dev_df = pd.DataFrame(dev_data, columns=columns)

# Create labels for NER
label_to_id = {label: i for i, label in enumerate(train_df['label'].unique())}

for index, row in train_df.iterrows():
    train_df.at[index, 'label_id'] = label_to_id[row['label']]
for index, row in test_df.iterrows():
    test_df.at[index, 'label_id'] = label_to_id[row['label']]
for index, row in dev_df.iterrows():
    dev_df.at[index, 'label_id'] = label_to_id[row['label']]

texts = train_df['word'].tolist()  # list of text samples
labels_index = label_to_id  # dictionary mapping label name to numeric id
labels = train_df['label_id'].tolist()  # list of label ids
print(labels)

validation_texts = dev_df['word'].tolist()  # list of text samples
labels_index = label_to_id  # dictionary mapping label name to numeric id

# Step 1: Create a label mapping dictionary based on your training labels
label_mapping = {}

# Iterate over the unique labels in your training dataset
for label in train_df['label'].unique():
    # Map the original label to a consistent label
    label_mapping[label] = label

# Step 2: Replace validation labels with consistent labels
dev_labels = [label_mapping[label] for label in dev_df['label']]
dev_df['labels'] = dev_labels
for index, row in dev_df.iterrows():
    dev_df.at[index, 'label_id'] = label_to_id[row['label']]
validation_labels = dev_df['label_id'].tolist() 

print(len(dev_df['label'].unique()))

test_texts = test_df['word'].tolist()  # list of text samples
labels_index = label_to_id  # dictionary mapping label name to numeric id
test_labels = test_df['label_id'].tolist()  # list of label ids

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
import numpy as np

# Define MAX_NB_WORDS and MAX_SEQUENCE_LENGTH before using them
MAX_NB_WORDS = 10000  # Define your desired maximum number of words
MAX_SEQUENCE_LENGTH = 100  # Define your desired maximum sequence length

# Tokenize your text data
tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')

labels = to_categorical(np.asarray(labels))
print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)

# Assuming you have separate validation and test datasets
x_val = pad_sequences(tokenizer.texts_to_sequences(validation_texts), maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')
y_val = padded_validation_labels = to_categorical(validation_labels, num_classes=8)
print(y_val)
print('Shape of data tensor:', x_val.shape)
print('Shape of label tensor:', y_val.shape)

x_test = pad_sequences(tokenizer.texts_to_sequences(test_texts), maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')
y_test = to_categorical(np.asarray(test_labels))
print('Shape of data tensor:', x_test.shape)
print('Shape of label tensor:', y_test.shape)
to_categorical(np.asarray(test_labels))

embeddings_index = {}

# Iterate through words in the model's vocabulary and their corresponding vectors
for index, word in enumerate(w2v.index_to_key):
    embeddings_index[word] = w2v[word]

print('Found %s word vectors.' % len(embeddings_index))

# print vector for the word 'the'
print(embeddings_index['the'])

EMBEDDING_DIM = 300  # Adjust this to match the dimension of your Word2Vec model

embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

from keras.layers import Embedding

embedding_layer = Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)


from sklearn.metrics import f1_score
import string
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
from keras.metrics import Precision, Recall, F1Score

# Create an LSTM model
model = Sequential()

# Add an Embedding layer with pre-trained Word2Vec weights
model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))
model.add(Dense(8, activation='softmax'))
# Add an LSTM layer with a specified number of units (you can adjust this)
LSTM_UNITS = 64
model.add(LSTM(LSTM_UNITS, dropout=0.2, recurrent_dropout=0.2))
# 
# Add a Dense output layer with the number of labels as units and 'softmax' activation
model.add(Dense(len(labels_index), activation='softmax'))


model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy', F1Score(average='micro')]
)


# Train the model
EPOCHS = 10  # You can adjust this
BATCH_SIZE = 10  # You can adjust this
history = model.fit(data, labels, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(x_val, y_val))

# Evaluate the model on the test data
scores = model.evaluate(x_test, y_test, verbose=0)
test_f1 = scores[model.metrics_names.index('f1_score')]
print(&quot;Test F1 Score:&quot;, test_f1)


import pandas as pd
import numpy as np

# Reverse the word and label index dictionaries
index_to_word = {v: k for k, v in word_index.items()}
index_to_labels = {v: k for k, v in label_to_id.items()}  # Replace label_to_id with your mapping

# Initialize lists to store original texts and predicted labels
# original_texts = [index_to_word.get(i, '?') for i in x_test.flatten()]  # Flatten if x_test is 2D
predicted_labels_list = []

# Make predictions
predictions = model.predict(x_test)
predicted_indices = np.argmax(predictions, axis=-1).flatten()  # Flatten if predictions is 2D

# Translate numerical indices back to original labels
predicted_labels = []
for i in predicted_indices:
    label = index_to_labels.get(i, '?')
    predicted_labels.append(label)
    
# Convert lists into DataFrame
df = pd.DataFrame({
    'Original_Texts': test_texts,
    'true labels id': test_labels,
    'true labels': test_df['label'].tolist(),
    'Predicted Labels id': predicted_indices,
    'Predicted Labels': predicted_labels
})

df.head(30)
</code></pre>
<p>I've tried adding more dense layers, changing num of epochs, etc. but the output is still the same:</p>
<p>Original_Texts  true labels id  true labels Predicted Labels id Predicted Labels
0   SOCCER  1.0 O   1   O
1       1.0 O   1   O
2   JAPAN   4.0 I-LOC   1   O
3   GET 1.0 O   1   O
4   LUCKY   1.0 O   1   O
5   WIN 1.0 O   1   O
6       1.0 O   1   O
7   CHINA   3.0 I-PER   1   O
8   IN  1.0 O   1   O
9   SURPRISE    1.0 O   1   O
10  DEFEAT  1.0 O   1   O
11      1.0 O   1   O
12  Nadim   3.0 I-PER   1   O
13  Ladki   3.0 I-PER   1   O
14  AL-AIN  4.0 I-LOC   1   O
15      1.0 O   1   O</p>
",Vectorization & Embeddings,kera lstm model predicting ner tag nlp multiclassification problem using coll dataset input word vec embeddings tried adding dense layer changing num epoch etc output still original text true label id true label predicted label id predicted label soccer japan loc get lucky win china per surprise defeat nadim per ladki per al loc
Get weight matrices from gensim word2Vec,"<p>I am using gensim word2vec package in python.
I would like to retrieve the <code>W</code> and <code>W'</code> weight matrices that have been learn during the skip-gram learning.</p>
<p>It seems to me that <code>model.syn0</code> gives me the first one but I am not sure how I can get the other one. Any idea?</p>
<p>I would actually love to find any exhaustive documentation on models accessible attributes because the official one does not seem to be precise (for instance <code>syn0</code> is not described as an attribute)</p>
",Vectorization & Embeddings,get weight matrix gensim word vec using gensim word vec package python would like retrieve weight matrix learn skip gram learning seems give first one sure get one idea would actually love find exhaustive documentation model accessible attribute official one doe seem precise instance described attribute
How to get most similar words to a tagged document in gensim doc2vec,"<p>I have trained a doc2vec model.</p>
<pre><code>doc2vec = Doc2Vec(vector_size= 300,
                    window=10,
                    min_count=100, 
                    dm=1,
                    epochs=40)
doc2vec.build_vocab(corpus_file=train_data, progress_per=1000)
doc2vec.train(....)
</code></pre>
<p>The documents are tagged with incremental integer 0, 1, ...1000.</p>
<p>To get the top-n similar words to a document with tag=0, I used:</p>
<pre><code>doc_vector = doc2vec.dv[tag]
sims = doc2vec.wv.similar_by_vector(doc_vector, top_n=20)
</code></pre>
<p>The similarity makes sense, however, the similarity score are really looked &quot;weird&quot;, all of them are almost <code>1.0</code>. I checked <code>top_n=3000</code> and it is still around 1.0. Does it make sense to get all words with high similarity score.</p>
",Vectorization & Embeddings,get similar word tagged document gensim doc vec trained doc vec model document tagged incremental integer get top n similar word document tag used similarity make sense however similarity score really looked weird almost checked still around doe make sense get word high similarity score
Detecting semantic dissimilarity in sentences with same words,"<p>For e.g. :
Question : What is the capital of USA?
Expected Answer : Washington D.C. is the capital of USA.
Actual Answer : USA is the capital of Washington D.C.</p>
<p>The answers are lexically similar however they are semantically different due to the subject-object swap.</p>
<p>I'm new to NLP and I read few articles on Doc2Vec, however the examples provided are not similar enough for my doubt. Please advice methods that I should be trying and any references.</p>
",Vectorization & Embeddings,detecting semantic dissimilarity sentence word e g question capital usa expected answer washington c capital usa actual answer usa capital washington c answer lexically similar however semantically different due subject object swap new nlp read article doc vec however example provided similar enough doubt please advice method trying reference
How to get the sentence embeddings with DeBERTa.deberta.pooling?,"<p>How to get the sentence embeddings with DeBERTa.deberta.pooling?</p>
<p>Hi everyone, I applied a DeBERTa model to analyze sentences, this is what my code looks like:</p>
<pre><code>from transformers import DebertaTokenizer, DebertaModel
import torch
# downloading the models
tokenizer = DebertaTokenizer.from_pretrained(&quot;microsoft/deberta-base&quot;)
model = DebertaModel.from_pretrained(&quot;microsoft/deberta-base&quot;)
# tokenizing the input text and converting it into pytorch tensors
inputs = tokenizer([&quot;The cat cought the mouse&quot;, &quot;This is the second sentence&quot;], return_tensors=&quot;pt&quot;, padding=True)
# pass through the model 
outputs = model(**inputs)
</code></pre>
<p>I realize that one option to get the sentence embeddings is to look at the CLS hidden state using</p>
<pre><code>outputs.last_hidden_state[:,0,:]
</code></pre>
<p>However, I would prefer to get the pooled output. As I learned, <code>pooled_output</code> is not supported, but there seems to be an implementation in DeBERTa named DeBERTa.deberta.pooling (see <a href=""https://deberta.readthedocs.io/en/latest/_modules/DeBERTa/deberta/pooling.html"" rel=""nofollow noreferrer"">https://deberta.readthedocs.io/en/latest/_modules/DeBERTa/deberta/pooling.html</a>). Does anyone know how to use it?</p>
",Vectorization & Embeddings,get sentence embeddings deberta deberta pooling get sentence embeddings deberta deberta pooling hi everyone applied deberta model analyze sentence code look like realize one option get sentence embeddings look cl hidden state using however would prefer get pooled output learned supported seems implementation deberta named deberta deberta pooling see doe anyone know use
Computational Complexity of Self-Attention in the Transformer Model,"<p>I recently went through the <a href=""https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""noreferrer"">Transformer</a> paper from Google Research describing how self-attention layers could completely replace traditional RNN-based sequence encoding layers for machine translation. In Table 1 of the paper, the authors compare the computational complexities of different sequence encoding layers, and state (later on) that self-attention layers are faster than RNN layers when the sequence length <code>n</code> is smaller than the dimension of the vector representations <code>d</code>.</p>
<p>However, the self-attention layer seems to have an inferior complexity than claimed if my understanding of the computations is correct. Let <code>X</code> be the input to a self-attention layer. Then, <code>X</code> will have shape <code>(n, d)</code> since there are <code>n</code> word-vectors (corresponding to rows) each of dimension <code>d</code>. Computing the output of self-attention requires the following steps (consider single-headed self-attention for simplicity):</p>
<ol>
<li>Linearly transforming the rows of <code>X</code> to compute the query <code>Q</code>, key <code>K</code>, and value <code>V</code> matrices, each of which has shape <code>(n, d)</code>. This is accomplished by post-multiplying <code>X</code> with 3 learned matrices of shape <code>(d, d)</code>, amounting to a computational complexity of <code>O(n d^2)</code>.</li>
<li>Computing the layer output, specified in Equation 1 of the paper as <code>SoftMax(Q Kt / sqrt(d)) V</code>, where the softmax is computed over each row. Computing <code>Q Kt</code> has complexity <code>O(n^2 d)</code>, and post-multiplying the resultant with <code>V</code> has complexity <code>O(n^2 d)</code> as well.</li>
</ol>
<p>Therefore, the total complexity of the layer is <code>O(n^2 d + n d^2)</code>, which is worse than that of a traditional RNN layer. I obtained the same result for multi-headed attention too, on considering the appropriate intermediate representation dimensions (<code>dk</code>, <code>dv</code>) and finally multiplying by the number of heads <code>h</code>.</p>
<p>Why have the authors ignored the cost of computing the Query, Key, and Value matrices while reporting total computational complexity?</p>
<p>I understand that the proposed layer is fully parallelizable across the <code>n</code> positions, but I believe that Table 1 does not take this into account anyway.</p>
",Vectorization & Embeddings,computational complexity self attention transformer model recently went transformer paper google research describing self attention layer could completely replace traditional rnn based sequence encoding layer machine translation table paper author compare computational complexity different sequence encoding layer state later self attention layer faster rnn layer sequence length smaller dimension vector representation however self attention layer seems inferior complexity claimed understanding computation correct let input self attention layer shape since word vector corresponding row dimension computing output self attention requires following step consider single headed self attention simplicity linearly transforming row compute query key value matrix ha shape accomplished post multiplying learned matrix shape amounting computational complexity computing layer output specified equation paper softmax computed row computing ha complexity post multiplying resultant ha complexity well therefore total complexity layer worse traditional rnn layer obtained result multi headed attention considering appropriate intermediate representation dimension finally multiplying number head author ignored cost computing query key value matrix reporting total computational complexity understand proposed layer fully parallelizable across position believe table doe take account anyway
Sentence Embeddings with Multiprocessing,"<p>I would like to compute sentence embeddings on a very large text dataset in the fastest way possible. For this purpose I have access to a multi-core CPU (x32).</p>
<p>My current code is as follows:</p>
<pre><code>import tensorflow as tf

from transformers import AutoTokenizer, TFAutoModel 

tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')
model = TFAutoModel.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')

input_ids = tf.constant(tokenizer(sentences)['input_ids'])[None, :][0] # sentences is a list() 
sentence_embeddings = [model(input_id).last_hidden_state[0][0].numpy() for input_id in input_ids] 
</code></pre>
<p>Kindly advise how I may be able to speed up this block using parallelism. I have tried various solutions on multiprocessing available on the web, but to no avail. Any help would be much appreciated.</p>
",Vectorization & Embeddings,sentence embeddings multiprocessing would like compute sentence embeddings large text dataset fastest way possible purpose access multi core cpu x current code follows kindly advise may able speed block using parallelism tried various solution multiprocessing available web avail help would much appreciated
How to fetch vectors for a word list with Word2Vec?,"<p>I want to create a text file that is essentially a dictionary, with each word being paired with its vector representation through word2vec. I'm assuming the process would be to first train word2vec and then look-up each word from my list and find its representation (and then save it in a new text file)? </p>

<p>I'm new to word2vec and I don't know how to go about doing this. I've read from several of the main sites, and several of the questions on Stack, and haven't found a good tutorial yet.</p>
",Vectorization & Embeddings,fetch vector word list word vec want create text file essentially dictionary word paired vector representation word vec assuming process would first train word vec look word list find representation save new text file new word vec know go read several main site several question stack found good tutorial yet
Is there a way to load Word2Vec embeddings to ChromaDB?,"<p>I want to query for similar words using ChromaDB. For example, 'great' should return all the words that are similar to 'great', in most cases, it would be synonyms. For this, I would like to upload Word2Vec or Glove embeddings to ChromaDB and query.</p>
<p>Most of the examples demonstrate how one can build embeddings into ChromaDB while processing the documents. As I have very little document, I want to use embeddings provided by Word2Vec or GloVe.</p>
<p>Is it possible to load the Word2Vec/Glove embeddings directly into ChromaDB?</p>
",Vectorization & Embeddings,way load word vec embeddings chromadb want query similar word using chromadb example great return word similar great case would synonym would like upload word vec glove embeddings chromadb query example demonstrate one build embeddings chromadb processing document little document want use embeddings provided word vec glove possible load word vec glove embeddings directly chromadb
Learnable weighted mean operation in Pytorch,"<p>I was getting familiar with the notion of word embeddings and was trying to build a classifier in pytorch to do the same. While going through certain codes on GitHub, I noticed the following.</p>
<pre class=""lang-py prettyprint-override""><code>class EmbeddingCls(torch.nn.Module):
def __init__(self, vocab_size, embedding_dim, num_class):
    super().__init__()
    self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)
    self.fc = torch.nn.Linear(embed_dim, num_class)

def forward(self, x):
    # size of x: (batch_size, sequence_length)
    x = self.embedding(x)
    x = torch.mean(x,dim=1)
    out = self.fc1(x)
    return out
</code></pre>
<p>I understand why we are using <code>torch.mean(x, dim = 1)</code>. We just want to bring down the sequence length to the length of the embedding dimension. What I wanted to try was, to define a <code>max_length</code> parameter, concatenate the input x with zeros in order for x to have a dimension of <code>(batch_size, max_length)</code>. Then when it passes through the <code>self.embedding</code>, it should return a tensor of size <code>(batch_size, max_length, embedding_dim)</code>. Now instead of using <code>torch.mean(x, dim = 1)</code>, I would like to use a learnable weighted mean operation at dimension 1. Any idea how to do it in pytorch ?</p>
<p>The main reason I'm trying to do this, is because performing <code>torch.mean</code> is like giving equal importance to all the words in a sentence, I understand that it does little harm in the end, however, I would still like to know how the result would change if I used trainable weights instead of using <code>torch.mean</code>.</p>
<p>I wrote the following code with a bit of addons, however I think the problem lies in the <code>self.fc1</code> parameter.</p>
<pre class=""lang-py prettyprint-override""><code>class EmbeddingClassifier(nn.Module):
  def __init__(self, vocab_size, embedding_dim, num_classes, max_length = 20):
    super().__init__()
    self.max_length = max_length
    self.embedding = nn.Embedding(vocab_size, embedding_dim)
    self.relu = nn.ReLU()
    self.fc1 = nn.Linear(max_length, embedding_dim)
    self.fc2 = nn.Linear(embedding_dim, num_classes)
  def forward(self, x):
    &quot;&quot;&quot;
      x: pytorch LongTensor object of input_ids
      Note: x is of size --&gt; (batch_size, sequence_length)
    &quot;&quot;&quot;
    if x.shape[1] &lt; self.max_length:
      zs = self.max_length - x.shape[1] # these many zeros are needed
      z = torch.zeros(size = (x.shape[0], zs), dtype = torch.int)
      x = torch.cat([x, z], dim = 1) # size of x: (batch_size, max_length)
    x = self.embedding(x) # (batch_size, max_length, embedding_dim)
    x = self.relu(self.fc1(x)) # (batch_size, embedding_dim)
    out = self.fc2(x) # (batch_size, embedding_dim)
    return out
</code></pre>
<p>Following error popped up:</p>
<blockquote>
<p>RuntimeError: mat1 and mat2 shapes cannot be multiplied (200x5 and
20x5)</p>
</blockquote>
<p>What I expect: I want the model to learn a tensor of size <code>(max_length,)</code>, say <code>[w_1,w_2,...w_n]</code>, which when operated on x (size of x: <code>(batch_size, max_length, embedding_dim)</code>) outputs a tensor of size <code>(batch_size, embedding_dim)</code>. Basically a replacement of torch.mean with trainable parameters.</p>
",Vectorization & Embeddings,learnable weighted mean operation pytorch wa getting familiar notion word embeddings wa trying build classifier pytorch going certain code github noticed following understand using want bring sequence length length embedding dimension wanted try wa define parameter concatenate input x zero order x dimension pass return tensor size instead using would like use learnable weighted mean operation dimension idea pytorch main reason trying performing like giving equal importance word sentence understand doe little harm end however would still like know result would change used trainable weight instead using wrote following code bit addons however think problem lie parameter following error popped runtimeerror mat mat shape multiplied x x expect want model learn tensor size say operated x size x output tensor size basically replacement torch mean trainable parameter
How to implement TorchDrift&#39;s Drift Detection for Monitoring Separate Text Embedding Distributions Across Multiple Topics?,"<p>I'm working on a project involving text data with multiple topics, and I want to use the Kernel Maximum Mean Discrepancy (Kernel MMD) for drift detection on text embeddings <strong>for each topic separately</strong>.</p>
<p>I want to use the Kernel Maximum Mean Discrepancy (Kernel MMD) for drift detection on each topic's text embeddings. To do this, I'd like to use TorchDrift's Kernel MMD implementation (<code>torchdrift.detectors.KernelMMDDriftDetector()</code>) and creating separate detectors for each topic. However, I'm stuck because the documentation for the <code>torchdrift.utils.fit(...)</code> function doesn't provide clear instructions on how it works and how to train multiple KernelMMDDriftDetector instances with different datasets.</p>
<p>The code snippet provided to me has been sent for me to draw inspiration from. However, the sender designed it to operate in a text classification context, whereas my goal is to perform Drift Detection on unlabeled embeddings, for each available topic. In other words, I don't understand how to &quot;expand for 'n' topics&quot; this implementation:</p>
<pre><code># ...

my_dataset = TensorDataset(tensor_x,tensor_y)
my_dataloader = DataLoader(my_dataset)

class MyIdentityModule(torch.nn.Module):
    def __init__(self):
        super(MyIdentityModule, self).__init__()
        self.identity = torch.nn.Identity()

    def forward(self, X):
        Y = self.identity(X)
        return Y

my_feature_extractor = MyIdentityModule()

drift_detector = torchdrift.detectors.KernelMMDDriftDetector()

torchdrift.utils.fit(my_dataloader, my_feature_extractor, drift_detector, device=&quot;cpu&quot;)

deg_np_x = X_test_emb
deg_np_y = X_test_predicted

tensor_x = torch.Tensor(deg_np_x) 
tensor_y = torch.Tensor(deg_np_y)

score = drift_detector(tensor_x)
p_val = drift_detector.compute_p_value(tensor_x)
score, p_val

if p_val &lt; 0.05:
    print(&quot;1: Drift Detected&quot;)
else:
    print(&quot;0: Drift NOT Detected&quot;)

</code></pre>
<p>In this reference code, a Module that computes the identity is created because it is needed by the fit function, even if I already have the text embeddings ready.</p>
<p>Has anyone encountered a similar situation or can provide guidance on how to tackle this issue? I hope my question is clear, and I appreciate any assistance or insights you can provide.</p>
",Vectorization & Embeddings,implement torchdrift drift detection monitoring separate text embedding distribution across multiple topic working project involving text data multiple topic want use kernel maximum mean discrepancy kernel mmd drift detection text embeddings topic separately want use kernel maximum mean discrepancy kernel mmd drift detection topic text embeddings like use torchdrift kernel mmd implementation creating separate detector topic however stuck documentation function provide clear instruction work train multiple kernelmmddriftdetector instance different datasets code snippet provided ha sent draw inspiration however sender designed operate text classification context whereas goal perform drift detection unlabeled embeddings available topic word understand expand n topic implementation reference code module computes identity created needed fit function even already text embeddings ready ha anyone encountered similar situation provide guidance tackle issue hope question clear appreciate assistance insight provide
Embedding Process Stuck While Handling Large CSV,"<p>I wanted to use the script below for embedding. It worked fine on a small amount of data, but when I loaded a CSV with 300,000 records into it, the embedding has been running for 40 minutes and doesn't stop.</p>
<p>The script:</p>
<pre><code>load_dotenv('.env')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
openai.api_key = OPENAI_API_KEY

model = OpenAIEmbeddings()

dataset = pd.read_csv('keywords.csv', encoding='ISO-8859-1')


dataset['embedding'] = dataset['keyword'].apply(
    lambda x: get_embedding(x, engine='text-embedding-ada-002')
)

dataset['embedding'].apply(np.array)
     
keyword = input('Input:')

keywordVector = get_embedding(
    keyword, engine=&quot;text-embedding-ada-002&quot;
)


print(keywordVector)
</code></pre>
<p>How can I optimize this?</p>
<p>Instead of calling the API for each keyword separately, you can try batching multiple keywords together if the API supports it. Unfortunately, with the OpenAI API, it appears that the openai.Embed.create function only accepts one prompt at a time, so this might not be possible in this case.</p>
",Vectorization & Embeddings,embedding process stuck handling large csv wanted use script embedding worked fine small amount data loaded csv record embedding ha running minute stop script optimize instead calling api keyword separately try batching multiple keywords together api support unfortunately openai api appears openai embed create function accepts one prompt time might possible case
"Why do I get inconsistent results between Fasttext, Longformer, and Doc2vec?","<p>I am using a Doc2Vec model to calculate cosine similarity between observations in a dataset of website text. I want to be sure that my measure is <em>roughly</em> consistent if I instead use Fasttext (trained on my data) or Longformer (pre-trained) [I know they won't be identical]. However, the pairwise cosine similarity measure is <strong>strongly negatively correlated</strong> between Doc2Vec &amp; Longformer or Doc2Vec &amp; Fasttext. The measure is positively correlated between Longformer &amp; Fasttext. Is there a reason why one might expect this? Am I mistakenly doing something in my code that could be causing this?</p>
<pre><code># PREPARE DATA
website_df = pd.read_csv(data_path+'cleaned_docdf_may2023.csv')
website_df[['documents_cleaned','website']]=website_df[['documents_cleaned','website']].astype(str)
website_df['documents_cleaned']=website_df['documents_cleaned'].str.lower()
website_df['documents_cleaned']=website_df['documents_cleaned'].str.strip()

####################### 
# Train Doc2vec model
#######################

# Clean data for model input (trim long docs, lower case, tokenize):
counter = 0
all_docs = []
all_docs_simple = []
for train_doc in website_df.documents_cleaned:
    doc = train_doc[:150000] if len(train_doc) &gt; 150000 else train_doc
    # clean using simple_preprocess for Fasttext model input
    simple_pre = gensim.utils.simple_preprocess(train_doc)
    doc = remove_stopwords(doc)
    doc_tokens =nltk.word_tokenize(doc.lower())
    all_docs.append(doc_tokens)
    all_docs_simple.append(simple_pre)
    if (counter%100) == 0:
        print(&quot;{0} .. len: {1}&quot;.format(counter,len(doc)))
    counter += 1

# Creating all tagged documents
documents_websites = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_docs)]
documents_simplepre_websites = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_docs_simple)]
print(&quot;\t. Run model&quot;)
doc2vec_model_websites = Doc2Vec(documents = documents_websites,
                vector_size=700,
                window=7,
                min_count =3)
print(&quot;\t. Done&quot;)
doc2vec_model_websites.save(data_path + &quot;doc2vec_websites.model&quot;)

# Grab document level vectors
vectors_d2v_websites = doc2vec_model_websites.dv.get_normed_vectors()

######################### 
# FASTTEXT MODEL
#########################

# create and save Fasttext input
sent_df_websites=pd.Series(documents_simplepre_websites)
with open(data_path + 'sentences_websites', 'a') as f:
    df_string = sent_df_websites.to_string(header=False, index=False)
    f.write(df_string)

# Skipgram model (use comparable model parameters to doc2vec model) :
ft_model_sg_websites = fasttext.train_unsupervised(input=sent_df_websites, model='skipgram', ws=7, epoch=10, minCount=3)
ft_model_sg_websites.save_model(data_path + &quot;ft_websites_sg.bin&quot;)
# cbow model (use comparable model parameters to doc2vec model) :
ft_model_cbow_websites = fasttext.train_unsupervised(input=data_path + 'sentences_websites', model='cbow', ws=7, epoch=10, minCount=3)
ft_model_cbow_websites.save_model(data_path + &quot;ft_websites_cbow.bin&quot;)

def generateVector(sentence):
    return ft.get_sentence_vector(sentence)

ft = ft_model_sg_websites
website_df['embeddings_sg'] = website_df['documents_cleaned'].apply(generateVector)
embeddings_sg_website=website_df['embeddings_sg']
ft = ft_model_cbow_websites
website_df['embeddings_cbow'] = website_df['documents_cleaned'].apply(generateVector)
embeddings_cbow_website=website_df['embeddings_sg']

#########################
# LONGFORMER 
#########################

model_name = 'allenai/longformer-base-4096'
tokenizer = LongformerTokenizer.from_pretrained(model_name)
model = LongformerModel.from_pretrained(model_name)

def get_longformer_embeddings(text):
    encoded_input = tokenizer(text, return_tensors=&quot;pt&quot;, max_length=4096, truncation=True)
    output = model(**encoded_input, output_hidden_states=True)
    embeddings = output.last_hidden_state
    avg_emb = embeddings.mean(dim=1)
    return avg_emb.cpu().detach().numpy()

def get_cosine_sim(a,b):
    value = dot(a, b)/(norm(a)*norm(b))
    return value

# subset data for speed during pilot
website_subset = website_df[:100]
website_subset['embeddings'] = website_subset['documents_cleaned'].apply(get_longformer_embeddings)

#########################
# EVALUATE CONSISTENCY BETWEEN MODELS
#########################

# create dataframe of random pairwise combinations
rand = np.random.randint(1,100,size=(500,2))
df = pd.DataFrame(rand, columns=['rand1', 'rand2'])
df['sim_lf']=0
df['sim_dv']=0
df['sim_ft_sg']=0
df['sim_ft_cbow']=0

for ind in df.index:
    a_loc = df['rand1'][ind]
    b_loc = df['rand2'][ind]
    a_vec_dv = vectors_d2v_websites[a_loc]
    b_vec_dv = vectors_d2v_websites[b_loc]
    a_vec_ft_sg = embeddings_sg_website[a_loc]
    b_vec_ft_sg = embeddings_sg_website[b_loc]
    a_vec_ft_cbow = embeddings_cbow_website[a_loc]
    b_vec_ft_cbow = embeddings_cbow_website[b_loc]
    a_vec_lf = website_subset['embeddings'][a_loc]
    b_vec_lf = website_subset['embeddings'][b_loc].T
    cos_sim_lf = get_cosine_sim(a_vec_lf, b_vec_lf)
    cos_sim_dv = get_cosine_sim(a_vec_dv, b_vec_dv)
    cos_sim_ft_sg = get_cosine_sim(a_vec_ft_sg,b_vec_ft_sg)
    cos_sim_ft_cbow = get_cosine_sim(a_vec_ft_cbow,b_vec_ft_cbow)
    df['sim_lf'][ind]=cos_sim_lf
    df['sim_dv'][ind]=cos_sim_dv
    df['sim_ft_sg'][ind]=cos_sim_ft_sg
    df['sim_ft_cbow'][ind]=cos_sim_ft_cbow

print('MY WEBSITE DATA SIMILARITY')
print('corr(Longformer, Fasttext (skipgram)) = ',df['sim_lf'].corr(df['sim_ft_sg']))
print('corr(Longformer, Fasttext (cbow)) = ',df['sim_lf'].corr(df['sim_ft_cbow']))
print('corr(Longformer, d2v) = ',df['sim_lf'].corr(df['sim_dv']))
print('corr(Fasttext (skipgram), d2v) = ',df['sim_ft_sg'].corr(df['sim_dv']))
print('corr(Fasttext (cbow), d2v) = ',df['sim_ft_cbow'].corr(df['sim_dv']))
</code></pre>
",Vectorization & Embeddings,get inconsistent result fasttext longformer doc vec using doc vec model calculate cosine similarity observation dataset website text want sure measure roughly consistent instead use fasttext trained data longformer pre trained know identical however pairwise cosine similarity measure strongly negatively correlated doc vec longformer doc vec fasttext measure positively correlated longformer fasttext reason one might expect mistakenly something code could causing
why nn.Embedding layer is used for positional encoding in bert?,"<p>In the huggingface implementation of <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py#L186"" rel=""nofollow noreferrer"">bert model</a>, for positional embedding nn.Embedding is used.
Why it is used instead of traditional sin/cos positional embedding described in the transformer paper? how this two things are same?</p>
<p>Also I am confused about the nn.Embedding layer? there are many word embedding like word2vec, glove. among them which is actually nn.Embedding layer? Can you please explain the inner structure of nn.Embedding in detail?
<a href=""https://stackoverflow.com/questions/75646273/what-is-the-difference-nn-embedding-and-nn-linear"">This question</a> also comes in my mind.</p>
",Vectorization & Embeddings,nn embedding layer used positional encoding bert huggingface implementation bert model positional embedding nn embedding used used instead traditional sin co positional embedding described transformer paper two thing also confused nn embedding layer many word embedding like word vec glove among actually nn embedding layer please explain inner structure nn embedding detail href question also come mind
Right embedding for NLP Classifier negative sentence,"<p>I have a classifier which has classes - &quot;approve&quot; , &quot;reject&quot;  and many other classes.
I am using multiple language model based pre trained embedding from <code>sapcy</code>. So when I have a input sentence like <code>I approve this offer</code> , my classifier correctly predicts class <code>approve</code>.</p>
<p>However when I have input sentence such as
<code>I can not reject this offer</code> I want class to be <code>approve</code> but classifier is predicting <code>reject</code>.</p>
<p>One solution is to improve quality of my annotations which is time consuming. I want to solve by choosing right embedding I want to know if there is embedding which would show higher similarity between <code>not reject</code> and <code>approve</code> or is there any other solution.</p>
",Vectorization & Embeddings,right embedding nlp classifier negative sentence classifier ha class approve reject many class using multiple language model based pre trained embedding input sentence like classifier correctly predicts class however input sentence want class classifier predicting one solution improve quality annotation time consuming want solve choosing right embedding want know embedding would show higher similarity solution
How to achieve Text Embedding by BERT?,"<p>I am tring to build a Text Embedding function by BERT. It said that BERT can do text embedding. However, I cannot find the embedding function on BERT's tutorial. Here is the link I looked up: <a href=""https://huggingface.co/docs/transformers/model_doc/bert"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/model_doc/bert</a>
Does anyone know the resource of BERT text embedding? Or are there other names that represents it? I know for OpenAI  there is a function just call OpenAIEmbeddings(). I just want to find the similar function like that. Thank you!</p>
<p>I just want to find the similar function like OpenAIEmbeddings().</p>
",Vectorization & Embeddings,achieve text embedding bert tring build text embedding function bert said bert text embedding however find embedding function bert tutorial link looked doe anyone know resource bert text embedding name represents know openai function call openaiembeddings want find similar function like thank want find similar function like openaiembeddings
"Node: &#39;sequential/embedding/embedding_lookup&#39; indices[31,197] = 159947 is not in [0, 159943)","<p>here's my neural network:</p>
<pre class=""lang-py prettyprint-override""><code>model = Sequential()
model.add(Input(shape=(max_len, )))
model.add(Embedding(input_dim = vocab_size,  output_dim = embd_dim, input_length = max_len, weights = [final_embeddings], trainable=False))
model.add(Bidirectional(LSTM(64, return_sequences=True)))
model.add(GlobalMaxPool1D())
model.add(Dense(64, activation='relu'))
model.add(Dense(6, activation='sigmoid'))
</code></pre>
<p>here's how the word embeddings were generated:</p>
<pre class=""lang-py prettyprint-override""><code>vocab_size = 159943

all_embd = np.stack(embd_values)
embd_mean, embd_std = all_embd.mean(), all_embd.std()
final_embeddings = np.random.normal(embd_mean, embd_std, (vocab_size, embd_dim))
for word , i in tok.word_index.items():
    word_embedding = word_vec_dict.get(word)
    if word_embedding is not None:
        final_embeddings[i] = word_embedding

len(final_embeddings)--&gt; 159943
</code></pre>
<p>here's the test data size-&gt;153164</p>
<pre class=""lang-py prettyprint-override""><code>tok = Tokenizer()
tok.fit_on_texts(test['comment_text_transformed'])
encd_reviews_test = tok.texts_to_sequences(test['comment_text_transformed'])
</code></pre>
<p>here's the predict function</p>
<pre class=""lang-py prettyprint-override""><code>y_pred_test = model.predict([pad_reviews_test], 1024)
</code></pre>
<p>and now here is the error:</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Cell In[121], line 1
----&gt; 1 y_pred_test = model.predict([pad_reviews_test])

InvalidArgumentError: Graph execution error:

Detected at node 'sequential/embedding/embedding_lookup' defined at (most recent call last):

Node: 'sequential/embedding/embedding_lookup'
indices[31,197] = 159947 is not in [0, 159943)
     [[{{node sequential/embedding/embedding_lookup}}]] [Op:__inference_predict_function_26583]

</code></pre>
<p>How can I solve the problem?</p>
",Vectorization & Embeddings,node sequential embedding embedding lookup index neural network word embeddings generated test data size predict function error solve problem
Sinusoidal embedding - Attention is all you need,"<p>In <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""noreferrer"">Attention Is All You Need</a>, the authors implement a positional embedding (which adds information about where a word is in a sequence). For this, they use a sinusoidal embedding: </p>

<pre><code>PE(pos,2i) = sin(pos/10000**(2*i/hidden_units))
PE(pos,2i+1) = cos(pos/10000**(2*i/hidden_units))
</code></pre>

<p>where pos is the position and i is the dimension. It must result in an embedding matrix of shape [max_length, embedding_size], i.e., given a position in a sequence, it returns the tensor of PE[position,:].</p>

<p>I found the <a href=""https://github.com/Kyubyong/transformer/blob/master/modules.py#L143"" rel=""noreferrer"">Kyubyong's</a> implementation, but I do not fully understand it.</p>

<p>I tried to implement it in numpy the following way:</p>

<pre><code>hidden_units = 100 # Dimension of embedding
vocab_size = 10 # Maximum sentence length
# Matrix of [[1, ..., 99], [1, ..., 99], ...]
i = np.tile(np.expand_dims(range(hidden_units), 0), [vocab_size, 1])
# Matrix of [[1, ..., 1], [2, ..., 2], ...]
pos = np.tile(np.expand_dims(range(vocab_size), 1), [1, hidden_units])
# Apply the intermediate funcitons
pos = np.multiply(pos, 1/10000.0)
i = np.multiply(i, 2.0/hidden_units)
matrix = np.power(pos, i)
# Apply the sine function to the even colums
matrix[:, 1::2] = np.sin(matrix[:, 1::2]) # even
# Apply the cosine function to the odd columns
matrix[:, ::2] = np.cos(matrix[:, ::2]) # odd
# Plot
im = plt.imshow(matrix, cmap='hot', aspect='auto')
</code></pre>

<p><a href=""https://i.sstatic.net/1kexj.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/1kexj.png"" alt=""Result of matrix plot""></a></p>

<p>I don't understand how this matrix can give information on the position of inputs. Could someone first tell me if this is the right way to compute it and second what is the rationale behind it?</p>

<p>Thank you.</p>
",Vectorization & Embeddings,sinusoidal embedding attention need attention need author implement positional embedding add information word sequence use sinusoidal embedding po position dimension must result embedding matrix shape max length embedding size e given position sequence return tensor pe position found kyubyong implementation fully understand tried implement numpy following way understand matrix give information position input could someone first tell right way compute second rationale behind thank
shape of my dataframe(#rows) and that of final embeddings array doesn&#39;t match,"<p>I generated the word embeddings for my corpus(2-D List) then tried to generate the Average Word2Vec embeddings for each of the individual word list(that is for each comment which have been converted into a list though split() method) inside my corpus but the final length of my average word2vec embeddings numpy array and that of the #rows doesn't match i.e. 159571, which is the number of comments.</p>
<p>here's the code for generating the 'final_embeddings' array:</p>
<pre class=""lang-py prettyprint-override""><code>#Building vocabulary
vocabulary = set(model.wv.index_to_key)

final_embeddings = []
for i in flatten_corpus:
    avg_embeddings = None
    for j in i:
      
         if j in vocabulary:

            if avg_embeddings is None:
                avg_embeddings = model.wv[j]
            else:
                avg_embeddings = avg_embeddings + model.wv[j]
    if avg_embeddings is not None:
        avg_embeddings = avg_embeddings / len(avg_embeddings)
        final_embeddings.append(avg_embeddings)

</code></pre>
<ul>
<li>length of flatten_corpus: 159571</li>
<li>length of the above array: 159487 (doesn't match to above number)</li>
</ul>
<p>what am I doing wrong?</p>
",Vectorization & Embeddings,shape dataframe row final embeddings array match generated word embeddings corpus list tried generate average word vec embeddings individual word list comment converted list though split method inside corpus final length average word vec embeddings numpy array row match e number comment code generating final embeddings array length flatten corpus length array match number wrong
Methods for Geotagging or Geolabelling Text Content,"<p>What are some good algorithms for automatically labeling text with the city / region  or origin?  That is, if a blog is about New York, how can I tell programatically.  Are there packages / papers that claim to do this with any degree of certainty?  </p>

<p>I have looked at some tfidf based approaches, proper noun intersections, but so far, no spectacular successes, and I'd appreciate ideas!  </p>

<p>The more general question is about assigning texts to topics, given some list of topics.</p>

<p>Simple / naive approaches preferred to full on Bayesian approaches, but I'm open.</p>
",Vectorization & Embeddings,method geotagging geolabelling text content good algorithm automatically labeling text city region origin blog new york tell programatically package paper claim degree certainty looked tfidf based approach proper noun intersection far spectacular success appreciate idea general question assigning text topic given list topic simple naive approach preferred full bayesian approach open
Proper teatment of lists of word tokens in a df for clustering - advice needed,"<p>I have been provided with tokenized text that has been previously generated. I would like to keep these tokens and use them in my analysis. For each item of interest, there are multiple lists of tokens for various text attributes of the item organized in a dataframe. Here is an example of one such item taken from the dataframe:</p>
<blockquote>
<p>ingredient_tokens    [12698, 11125, 4465, 29487, 6444, 15000, 27043...<br>
steps_tokens         [40480, 40482, 8263, 280, 7420, 11125, 556, 64...<br>
techniques           [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<br>
ingredient_ids                                 [648, 3355, 7501, 4253]<br></p>
</blockquote>
<p>Each item has a different length of ingredient_tokens, steps_tokens, and ingredient_ids.</p>
<p>I am wondering how to make this data usable. I would like to cluster the data, but I cannot use a dataframe filled with lists. There is additional data cleaning required. I am thinking of encoding, for example, ingredient_ids so that the dataframe would contain new columns (648,3355,7501,4353) with 1's and 0's for ingredients used and not used. However, I am having a hard time making this transformation (see below). I would do this for the other features as well.</p>
<p>However, there are two issues:</p>
<ol>
<li>Wouldn't I lose some of the meaning preserved in the embeddings if I treat them just like generic feature names?</li>
<li>How do I technically go about doing this encoding? I am getting tripped up on this, as the list input is not standard. I've tried using MultiLabelBinarizer but it reduces the data to individual numbers and ignores the list structure.</li>
</ol>
<p>I would really appreciate advice on how to make this data usable. Any guidance would be greatly appreciated.</p>
",Vectorization & Embeddings,proper teatment list word token df clustering advice needed provided tokenized text ha previously generated would like keep token use analysis item interest multiple list token various text attribute item organized dataframe example one item taken dataframe ingredient token step token technique ingredient id item ha different length ingredient token step token ingredient id wondering make data usable would like cluster data use dataframe filled list additional data cleaning required thinking encoding example ingredient id dataframe would contain new column ingredient used used however hard time making transformation see would feature well however two issue lose meaning preserved embeddings treat like generic feature name technically go encoding getting tripped list input standard tried using multilabelbinarizer reduces data individual number ignores list structure would really appreciate advice make data usable guidance would greatly appreciated
How to find a similar substring inside a large string with a similarity score in python?,"<p>What I'm looking for is not just a plain similarity score between two texts. But a similarity score of a substring inside a string. Say:</p>

<pre><code>text1 = 'cat is sleeping on the mat'.

text2 = 'The cat is sleeping on the red mat in the living room'.
</code></pre>

<p>In the above example, all the words of <code>text1</code> are present in the <code>text2</code> completely, hence the similarity should be 100%. </p>

<p>If some words of <code>text1</code> are missing, the score shall be less.</p>

<p>I'm working with a large dataset of varying paragraph size, hence finding a smaller paragraph inside a bigger one with such similarity score is crucial. </p>

<p>I found only string similarities such as cosine similarities, difflib similarity etc. which compares two strings. But not about a score of substring inside another string. </p>
",Vectorization & Embeddings,find similar substring inside large string similarity score python looking plain similarity score two text similarity score substring inside string say example word present completely hence similarity word missing score shall le working large dataset varying paragraph size hence finding smaller paragraph inside bigger one similarity score crucial found string similarity cosine similarity difflib similarity etc compare two string score substring inside another string
Best way to compare meaning of text documents?,"<p>I'm trying to find the best way to compare two text documents using AI and machine learning methods. I've used the TF-IDF-Cosine Similarity and other similarity measures, but this compares the documents at a word (or n-gram) level.</p>

<p>I'm looking for a method that allows me to compare the <em>meaning</em> of the documents. What is the best way to do that?</p>
",Vectorization & Embeddings,best way compare meaning text document trying find best way compare two text document using ai machine learning method used tf idf cosine similarity similarity measure compare document word n gram level looking method allows compare meaning document best way
Output of cosine_similarity() not as expected (all values equal to 1.),"<p>I have been trying to find cosine similarity between the vector representation of the tag of a movie (computed using average word2vec) and all the other movies's vector representation (also using avg word2vec).</p>
<p>But the output always comes out to be <code>1.</code> somehow, even when I pass in to both parameters of the cosine_similarity function as the vector representation of movies itself.</p>
<p>Code for generation of avg word2vec</p>
<pre class=""lang-py prettyprint-override""><code>model = gensim.models.Word2Vec(window=10, min_count=1, workers=8, vector_size=300, alpha=0.03, min_alpha=0.0007)
model.build_vocab(flatten_corpus) #flatten_corpus is basically the corpus
model.train(flatten_corpus, total_examples=model.corpus_count, epochs=10)
embeddings = model.wv.get_normed_vectors()
</code></pre>
<p>Here the output of the embedding of every word-&gt;</p>
<pre class=""lang-py prettyprint-override""><code>array([[-0.01744956, -0.11224882,  0.13236344, ...,  0.0972883 ,
         0.02966034, -0.04626285],
       [ 0.05841004, -0.00941145, -0.07332838, ..., -0.07433803,
         0.02825699, -0.12789766],
       [ 0.10318288, -0.08702228, -0.14564443, ...,  0.1026295 ,
         0.01967871,  0.03213153],
       ...,
       [-0.03165161,  0.08460254, -0.05135883, ..., -0.00983593,
         0.08976564,  0.01239256],
       [-0.04694653,  0.12579133, -0.03276197, ..., -0.02105372,
         0.08431913, -0.0318971 ],
       [ 0.03841057,  0.03328352, -0.05580311, ...,  0.04187248,
        -0.03482581, -0.02766625]], dtype=float32)
</code></pre>
<p>Here''s how I computed the Avg Word2Vec-&gt;</p>
<pre class=""lang-py prettyprint-override""><code>ls = []
avg_embeddings = []
for i in range(len(flatten_corpus)):
    
    if len(flatten_corpus[i]) != 0:
    
        for j in range(len(flatten_corpus[i])):

            if len(flatten_corpus[j]) != 0:

                if len(flatten_corpus[i][j]) != 0:
                    ls.append(model.wv[flatten_corpus[i][j]].tolist())


        ls = list(chain.from_iterable(ls))

        avg_embeddings.append(sum(ls)/len(flatten_corpus[i]))
        ls = []
</code></pre>
<p>Now when I try the similarity function i get this-&gt;</p>
<p>Code</p>
<pre class=""lang-py prettyprint-override""><code>cosine_similarity(np.array(avg_embeddings[idx]).reshape(1, -1), np.array(avg_embeddings).reshape(1, -1))
</code></pre>
<p>Error-&gt;</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[702], line 1
----&gt; 1 cosine_similarity(np.array(avg_embeddings[idx]).reshape(1, -1), np.array(avg_embeddings).reshape(1, -1))

File ~/anaconda3/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:1393, in cosine_similarity(X, Y, dense_output)
   1358 &quot;&quot;&quot;Compute cosine similarity between samples in X and Y.
   1359 
   1360 Cosine similarity, or the cosine kernel, computes similarity as the
   (...)
   1389     Returns the cosine similarity between samples in X and Y.
   1390 &quot;&quot;&quot;
   1391 # to avoid recursive import
-&gt; 1393 X, Y = check_pairwise_arrays(X, Y)
   1395 X_normalized = normalize(X, copy=True)
   1396 if X is Y:

File ~/anaconda3/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:180, in check_pairwise_arrays(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)
    174         raise ValueError(
    175             &quot;Precomputed metric requires shape &quot;
    176             &quot;(n_queries, n_indexed). Got (%d, %d) &quot;
    177             &quot;for %d indexed.&quot; % (X.shape[0], X.shape[1], Y.shape[0])
    178         )
    179 elif X.shape[1] != Y.shape[1]:
--&gt; 180     raise ValueError(
    181         &quot;Incompatible dimension for X and Y matrices: &quot;
    182         &quot;X.shape[1] == %d while Y.shape[1] == %d&quot; % (X.shape[1], Y.shape[1])
    183     )
    185 return X, Y

ValueError: Incompatible dimension for X and Y matrices: X.shape[1] == 1 while Y.shape[1] == 4881
</code></pre>
<p>When i pass both parameters as avg_embeddings then i get this-&gt;</p>
<pre class=""lang-py prettyprint-override""><code>array([[1.]])  #just this and not the similarity values, it should been 4886 since these are the total no of movies present
</code></pre>
<p>What am I doing wrong?</p>
",Vectorization & Embeddings,output cosine similarity expected value equal trying find cosine similarity vector representation tag movie computed using average word vec movie vector representation also using avg word vec output always come somehow even pas parameter cosine similarity function vector representation movie code generation avg word vec output embedding every word computed avg word vec try similarity function get code error pas parameter avg embeddings get wrong
I have text data to perform sentiment analysis. With 3 classes I want to create embeddings and get centroids of the data. Any ideas?,"<p>I have text data to perform sentiment analysis. With three classes (-1,0,1) I would like to create <code>embeddings</code> and get the <em>centroids</em> of the data so new data can be assigned according to the centroids based on cosine similarity.</p>
<p>Any ideas?</p>
<p>I am trying to create the embeddings using <code>MPNET</code>.</p>
<p>This is the code I tried</p>
<pre><code>import pandas as pd
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support

# Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

# Function to classify data based on cosine similarity and threshold
def classify_data(embeddings, centroids, threshold):
    similarity_scores = torch.cosine_similarity(embeddings.unsqueeze(1), centroids.unsqueeze(0), dim=2)
    return similarity_scores.argmax(dim=1)

# Load model from HuggingFace Hub
tokenizer_mpnet = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')
model_mpnet = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')

# Example DataFrame with 'sentence' and 'label' columns
data = {
    'sentence': [
        'This is a positive sentence',
        'Each sentence is neutral',
        'Another example negative sentence',
        'More sentences to test',
    ],
    'label': [1, 0, -1, 0],  # Assuming sentiment labels: 1 for positive, 0 for neutral, -1 for negative
}
df = pd.DataFrame(data)

# Split the dataset into training and testing sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Tokenize sentences using MPNet
encoded_input_mpnet = tokenizer_mpnet(train_df['sentence'].tolist(), padding=True, truncation=True, return_tensors='pt')

# Compute token embeddings using MPNet
with torch.no_grad():
    model_output_mpnet = model_mpnet(**encoded_input_mpnet)

# Perform pooling for embeddings using MPNet
sentence_embeddings_mpnet = mean_pooling(model_output_mpnet, encoded_input_mpnet['attention_mask'])

# Normalize embeddings
sentence_embeddings_mpnet = F.normalize(sentence_embeddings_mpnet, p=2, dim=1)

# Compute the centroids of each class for MPNet
centroids_mpnet = []
for label in [-1, 0, 1]:
    centroid = sentence_embeddings_mpnet[train_df['label'] == label].mean(dim=0)
    centroids_mpnet.append(centroid)

# Tokenize testing sentences using MPNet
encoded_input_test = tokenizer_mpnet(test_df['sentence'].tolist(), padding=True, truncation=True, return_tensors='pt')

# Compute token embeddings for testing sentences using MPNet
with torch.no_grad():
    model_output_test = model_mpnet(**encoded_input_test)

# Perform pooling for testing embeddings using MPNet
sentence_embeddings_test = mean_pooling(model_output_test, encoded_input_test['attention_mask'])

# Normalize testing embeddings
sentence_embeddings_test = F.normalize(sentence_embeddings_test, p=2, dim=1)

# Classify new data based on cosine similarity and threshold for MPNet
threshold_mpnet = 0.33
predicted_labels_mpnet = classify_data(sentence_embeddings_test, torch.stack(centroids_mpnet), threshold_mpnet)

# Calculate precision, recall, and F1-score for each class for MPNet
precision_mpnet, recall_mpnet, f1_score_mpnet, _ = precision_recall_fscore_support(test_df['label'], predicted_labels_mpnet, average=None)

print(&quot;MPNet Precision:&quot;)
print(precision_mpnet)
print(&quot;MPNet Recall:&quot;)
print(recall_mpnet)
print(&quot;MPNet F1-score:&quot;)
print(f1_score_mpnet)
</code></pre>
<p>the error i was get :</p>
<pre><code>getting keyerror at line : label in [-1, 0, 1]:
    centroid = sentence_embeddings_mpnet[train_df['label'] == label].mean(dim=0)
</code></pre>
<p>Should I use labels or not?</p>
",Vectorization & Embeddings,text data perform sentiment analysis class want create embeddings get centroid data idea text data perform sentiment analysis three class would like create get centroid data new data assigned according centroid based cosine similarity idea trying create embeddings using code tried error wa get use label
Why are Neural Networks Needed with Word Embeddings?,"<p>Why do we need a neural network to do text classification when we vectorize documents with word embeddings? If word embeddings capture the meaning of words/documents, then why can't we just use cosine similarity to see which previously labeled documents a new document is closest to, and based off that, make a classification decision?</p>
",Vectorization & Embeddings,neural network needed word embeddings need neural network text classification vectorize document word embeddings word embeddings capture meaning word document use cosine similarity see previously labeled document new document closest based make classification decision
How to get embeddings from long texts without pooling?,"<p>I have a collection of relatively long texts, containing roughly 2k tokens each. I want to covert each into an embedding.</p>
<p>I found that <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">sentence-transformers</a> is quite popular, but it can only take short sequences into account. One approach is to create an embedding for each sentence and then average the results, but I don't want to do that. I'm interested in getting an embedding <strong>without any pooling operations</strong>. I also found <a href=""https://huggingface.co/tasks/feature-extraction"" rel=""nofollow noreferrer"">Huggingface's feature extraction pipeline</a>, but from <a href=""https://stackoverflow.com/questions/64685243/getting-sentence-embedding-from-huggingface-feature-extraction-pipeline"">here</a> I understand that it also contains some pooling operator over sequences (unless I'm mistaken).</p>
<p>For example, let's say I want to use a <a href=""https://huggingface.co/gpt2"" rel=""nofollow noreferrer"">GPT2 model</a> (or some other model that can take long sequences into account):</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')
text = &quot;Some very very very long text&quot;
</code></pre>
",Vectorization & Embeddings,get embeddings long text without pooling collection relatively long text containing roughly k token want covert embedding found sentence transformer quite popular take short sequence account one approach create embedding sentence average result want interested getting embedding without pooling operation also found huggingface feature extraction pipeline gpt model model take long sequence account
How can i measure semantic similarity between a sentence and its negative meaning?,"<pre><code>tecAnswer = &quot;&quot;&quot;
Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates.
&quot;&quot;&quot;
stdAnswer = &quot;&quot;&quot;
Gradient descent (GD) is not an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function. This method is commonly used in machine learning (ML) and deep learning(DL) to minimise a cost/loss function (e.g. in a linear regression). Due to its importance and ease of implementation, this algorithm is usually taught at the beginning of almost all machine learning courses.
&quot;&quot;&quot;`
</code></pre>
<pre><code>predict_similarity([tecAnswer, stdAnswer])
</code></pre>
<p>I have been developing a model for the <code>Automatic Short Answer Grading System</code>. In this model, the teacher's answer and the student's answer will be matched for similarity.</p>
<p>I calculated the semantic similarity between these texts. Here I used BERT pre-trained model called <code>bert-base-uncased</code>. But it gives the cosine similarity 0.92. Is it correct? In this situation, each text has a different meaning.</p>
<p>How can we handle this issue using any nlp model? I really appreciate any help you can provide.</p>
",Vectorization & Embeddings,measure semantic similarity sentence negative meaning developing model model teacher answer student answer matched similarity calculated semantic similarity text used bert pre trained model called give cosine similarity correct situation text ha different meaning handle issue using nlp model really appreciate help provide
Embedding Layer word embeddings in NLP,"<p>I am learning about word embeddings and using them in the LSTM model. I understand that the Embedding layer is used as the first layer to map word embeddings to the padded tokenised datasets.</p>
<p>Can we not directly feed the padded embedded dataset instead of the padded tokenised dataset to the LSTM directly, without an embedding layer?</p>
<p>If yes, then what is the significance of one over the other method?</p>
<p>I'm sorry if my question is unclear or very basic, I'm trying to understand each step in detail.</p>
",Vectorization & Embeddings,embedding layer word embeddings nlp learning word embeddings using lstm model understand embedding layer used first layer map word embeddings padded tokenised datasets directly feed padded embedded dataset instead padded tokenised dataset lstm directly without embedding layer yes significance one method sorry question unclear basic trying understand step detail
504-loadbalancer error. Website hangs when ML model is run by only one user,"<p>I think my webapp is pretty cool. It's a natural language playlist generator. It takes in a description of a playlist, like:</p>
<p><em>&quot;midwest emo songs to cry to in the shower because my girlfriend broke up with me&quot;</em></p>
<p>and converts it into an embedding generated by a NLP transformer model (specifically SentenceTransformers) and does recommender system stuff to return songs in a playlist for a user.</p>
<p>My website hangs after the user has submitted their description, and I get a 504 load balancer error after 5 minutes. After tracing where the code hangs, it seems to stop during model.encode(text), which runs the user's query through the ML model to get the embedding.</p>
<p>This code runs no problem on my local machine, and when I run it in the console it also has no problem processing the text through the ML model.</p>
<p>What should I do? Add more workers? Free up space in the program? Let me know.</p>
<p>Below are my server logs after model.encode() is run.</p>
<pre><code>2022-11-26 07:53:26 entered the get embedding function

2022-11-26 07:53:27 announcing my loyalty to the Emperor...

2022-11-26 07:54:11 Sat Nov 26 07:54:10 2022 - HARAKIRI ON WORKER 4 (pid: 18, try: 1)

2022-11-26 07:54:11 Sat Nov 26 07:54:10 2022 - HARAKIRI !!! worker 4 status !!!

2022-11-26 07:54:11 Sat Nov 26 07:54:10 2022 - HARAKIRI [core 0] 10.0.0.75 - POST / since 1669448649

2022-11-26 07:54:11 Sat Nov 26 07:54:10 2022 - HARAKIRI !!! end of worker 4 status !!!

2022-11-26 07:54:11 DAMN ! worker 4 (pid: 18) died, killed by signal 9 :( trying respawn ...

2022-11-26 07:54:11 Respawned uWSGI worker 4 (new pid: 33)

2022-11-26 07:54:11 spawned 2 offload threads for uWSGI worker 4

2022-11-26 08:03:28 Sat Nov 26 08:03:27 2022 - HARAKIRI ON WORKER 3 (pid: 15, try: 1)

2022-11-26 08:03:28 Sat Nov 26 08:03:27 2022 - HARAKIRI !!! worker 3 status !!!

2022-11-26 08:03:28 Sat Nov 26 08:03:27 2022 - HARAKIRI [core 0] 10.0.0.75 - POST / since 1669449206

2022-11-26 08:03:28 Sat Nov 26 08:03:27 2022 - HARAKIRI !!! end of worker 3 status !!!

2022-11-26 08:03:28 DAMN ! worker 3 (pid: 15) died, killed by signal 9 :( trying respawn ...

2022-11-26 08:03:28 Respawned uWSGI worker 3 (new pid: 36)

2022-11-26 08:03:28 spawned 2 offload threads for uWSGI worker 3
</code></pre>
<p>I tried running this code in the console of pythonanywhere, and it ran just fine. I'm stuck!</p>
",Vectorization & Embeddings,loadbalancer error website hang ml model run one user think webapp pretty cool natural language playlist generator take description playlist like midwest emo song cry shower girlfriend broke convert embedding generated nlp transformer model specifically sentencetransformers doe recommender system stuff return song playlist user website hang user ha submitted description get load balancer error minute tracing code hang seems stop model encode text run user query ml model get embedding code run problem local machine run console also ha problem processing text ml model add worker free space program let know server log model encode run tried running code console pythonanywhere ran fine stuck
Combining BERT word embeddings vectors to retain combined meaning,"<p>I am looking for ways to handle the context of conversation in an &quot;organic&quot; way with usage of BERT class models.</p>
<p>Is it possible to somehow combine many BERT base word embedding vectors generated out of different textual inputs to provide single word embedding retaining the combined meaning of those many separate vectors?</p>
<p>I am using BERT base model to generate word embeddings vector for the phrase &quot;buy&quot;, and I end up with certain word embeddings vector.</p>
<p>At the next step I do the same but for different phrase which is &quot;apple&quot; which gives me vector of same dimensions yet different values.</p>
<p>Is it possible to somehow process these two vectors together that the resulting, single vector would retain the meaning (in scope of model's knowledge representation) of both vectors?</p>
<p>For instance, so it would be similar (in terms of cosine similarity between 2 vectors) to the vector generated out of the phrase &quot;buy apple&quot;.</p>
<p>I've found several papers regarding the matter of conversation context handling and some of them did mention making use of global state which would modify the model's outputs.
Other papers mention applying additional layers to the output that would be capable of retaining some memory.</p>
<p>Thanks.</p>
",Vectorization & Embeddings,combining bert word embeddings vector retain combined meaning looking way handle context conversation organic way usage bert class model possible somehow combine many bert base word embedding vector generated different textual input provide single word embedding retaining combined meaning many separate vector using bert base model generate word embeddings vector phrase buy end certain word embeddings vector next step different phrase apple give vector dimension yet different value possible somehow process two vector together resulting single vector would retain meaning scope model knowledge representation vector instance would similar term cosine similarity vector vector generated phrase buy apple found several paper regarding matter conversation context handling mention making use global state would modify model output paper mention applying additional layer output would capable retaining memory thanks
How to get the embedding of any vocabulary token in GPT?,"<p>I have a GPT model</p>
<pre><code>model = BioGptForCausalLM.from_pretrained(&quot;microsoft/biogpt&quot;).to(device)
</code></pre>
<p>When I send my batch to it I can get the logits and the hidden states:</p>
<pre><code>out = model(batch[&quot;input_ids&quot;].to(device), output_hidden_states=True, return_dict=True)
print(out.keys())
&gt;&gt;&gt; odict_keys(['logits', 'past_key_values', 'hidden_states'])
</code></pre>
<p>The logits have shape of</p>
<pre><code>torch.Size([2, 1024, 42386]) # batch of size 2, sequence length = 1024, vocab size = 42386
</code></pre>
<p>Corresponding to <code>(batch, seq_length, vocab_length)</code>. If I understand correctly, for each token in the sequence, the logits is a vector of size <code>vocab_length</code> which points the model to which token from the vocabulary to use, after passing it to softmax. I believe that each of these tokens should have an embedding.</p>
<p>From my <a href=""https://stackoverflow.com/questions/76655508/how-to-get-the-vector-embedding-of-a-token-in-gpt"">previous question</a> I found how to get the embeddings of each <strong>sequence token</strong> (shape <code>[2,1024,1024]</code> in my setting). But, how can I get the embeddings of each token in <strong>the vocabulary</strong> of the model? This should be of size <code>[2, 1024, 42386, 1024]</code> (BioGPT has a hidden size of length <code>1024</code>).</p>
<p>I'm mainly interested in just a few special tokens (e.g., indices 1,2,6,112 out of the 42386).</p>
",Vectorization & Embeddings,get embedding vocabulary token gpt gpt model send batch get logits hidden state logits shape corresponding understand correctly token sequence logits vector size point model token vocabulary use passing softmax believe token embedding href question found get embeddings sequence token shape setting get embeddings token vocabulary model size biogpt ha hidden size length mainly interested special token e g index
How can i improve my recurrent neural network,"<p>I want to implement a recurrent neural network for natural language inference. I'm new in this topic and this is a task from a module from my university, so i've had some code beforehand which i tried to adopt for this task. The problem i have is that with my current implementation i get a accuracy of 0.5633 like everytime. So far i've implemented this code.</p>
<pre><code>import pandas as pd
import torch
import torch.nn as nn
from torch.optim import Adam, SGD, Adagrad
from tqdm import tqdm
from torch.utils.data import DataLoader, Dataset
import torch.nn.utils.rnn as rnn_utils
from sklearn.metrics import accuracy_score

from nltk import word_tokenize
from nltk.corpus import stopwords
import string 

s_words = stopwords.words('english')
puncts = string.punctuation

word2index_combined = {}

def data_preprocessing(fname):
    sentence_pairs = []
    data = pd.read_csv(fname, sep='\t')
    sentences_1 = data['sentence1'].tolist()
    sentences_1 = [word_tokenize(s) for s in sentences_1]
    sentences_1 = [[w.lower() for w in sentences_1[i] if w.lower() not in s_words and w.lower() not in puncts] for i in range(len(sentences_1))] 
    sentences_2 = data['sentence2'].tolist()
    sentences_2 = [word_tokenize(s) for s in sentences_2]
    sentences_2 = [[w.lower() for w in sentences_2[i] if w.lower() not in s_words and w.lower() not in puncts] for i in range(len(sentences_2))] 
    labels = data['label'].astype(int).tolist()  
    k = 1
    for i in range(len(sentences_1)):
        for w in sentences_1[i]:
            if w not in word2index:
                word2index[w] = k
                k += 1
        for w in sentences_2[i]:
            if w not in word2index:
                word2index[w] = k
                k += 1       
    return sentences_1, sentences_2, labels

class data(Dataset):
    def __init__(self, sent_1, sent_2, labels, padding):
        self.padding = padding
        self.sent_1 = self._pad(sent_1)
        self.sent_2 = self._pad(sent_2)        
        self.labels = labels
        
    def __len__(self):
        return len(self.sent_1)
    
    def __getitem__(self, idx):
        s1 = [word2index.get(word, 0) for word in self.sent_1[idx]]
        s2 = [word2index.get(word, 0) for word in self.sent_2[idx]]
        label = self.labels[idx]
        return torch.tensor(s1), torch.tensor(s2), torch.tensor(label)
    
    def _pad(self, sentences):
        sents = []
        for sent in sentences:
            if len(sent) &lt; self.padding:
                sent += [0] * (self.padding - len(sent))
            else:
                sent = sent[:self.padding]
            sents.append(sent)
        return sents
        

class RNN(nn.Module):
    def __init__(self, vocab_size, hidden_size, emb_dim, num_layers):
        super().__init__()
        self.vocab_size = vocab_size+1
        self.hidden_size = hidden_size
        self.emb_dim = emb_dim
        self.num_layers = num_layers
        
        self.emb = nn.Embedding(self.vocab_size, self.emb_dim, padding_idx=0)
        self.lstm = nn.GRU(self.emb_dim, self.hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=True)
        self.fc1 = nn.Linear(self.hidden_size*2, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, 2)
        
    def forward(self, inp_seq1, inp_seq2):
        batch_size = inp_seq1.size(0)
        inp1 = self.emb(inp_seq1)
        inp2 = self.emb(inp_seq2)
        h_0 = (torch.rand(self.num_layers*2, batch_size, self.hidden_size))
        packed_output1, last_hidden1 = self.lstm(inp1, h_0)
        packed_output2, last_hidden2 = self.lstm(inp2, h_0)
        out = torch.cat((last_hidden1[-1], last_hidden2[-1]), dim=1)
        out = self.fc1(out)
        out = self.relu(out)
        out = self.fc2(out)
        return out

def validate(model, dev_loader):
    label_pred = []
    label_original = []
    with torch.no_grad():
        for X1, X2, y in dev_loader:
            out = model(X1, X2)
            label = torch.argmax(out, dim=1)
            label_pred.extend(label.numpy().tolist())
            label_original.extend(y.numpy().tolist())
    
    return accuracy_score(label_original, label_pred)   

def train(model, train_loader, dev_loader):
    val_accuracy = 0
    criterion = nn.CrossEntropyLoss()
    optimizer = Adam(model.parameters(), lr=0.0001)
    epochs = 10
    for e in range(epochs):
        for X1, X2, y in tqdm(train_loader):
            if X1.shape[0] != y.shape[0]:  # Check if batch sizes match
                continue
            out = model(X1, X2)
            loss = criterion(out, y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        model.eval()
        acc = validate(model, dev_loader)
        print(f&quot;accuracy at the end of epoch - {e}: {acc}&quot;)
        if acc &gt; val_accuracy:
            torch.save(model.state_dict(), 'rnn_model.pt')
            val_accuracy = acc
        model.train()     

train_sentences, train_labels = data_preprocessing('WNLI/train.tsv')
test_sentences, test_labels = data_preprocessing('WNLI/test.tsv')
dev_sentences, dev_labels = data_preprocessing('WNLI/dev.tsv')

train_data = data(train_sentences, train_labels)
test_data = data(test_sentences, test_labels)
dev_data = data(dev_sentences, dev_labels)

train_loader = DataLoader(train_data, batch_size=32, collate_fn=train_data.collate_fn, drop_last=True)
test_loader = DataLoader(test_data, batch_size=32, collate_fn=test_data.collate_fn, drop_last=True)
dev_loader = DataLoader(dev_data, batch_size=32, collate_fn=dev_data.collate_fn, drop_last=True)

vocab_size = len(word2index_combined)
hidden_size = 1024
emb_dim = 250
num_layers = 3
model = RNN(vocab_size, hidden_size, emb_dim, num_layers)
train(model, train_loader, dev_loader)
</code></pre>
<p>So far i've tried it with different batch sizes, number of layers, number of epochs, hidden sizes, embedding dimensions and learning rate but i still get the same accuracy or worse. So i don't really know where the problem could be and at this point i think maybe i'm going into the wrong direction with my implementation.</p>
",Vectorization & Embeddings,improve recurrent neural network want implement recurrent neural network natural language inference new topic task module university code beforehand tried adopt task problem current implementation get accuracy like everytime far implemented code far tried different batch size number layer number epoch hidden size embedding dimension learning rate still get accuracy worse really know problem could point think maybe going wrong direction implementation
Pytorch Transformer: Embed dimension (d_model) is same dimension as src embedding but is also total dimension of model?,"<p>I have been trying to use torch.nn.Transformer. I am confused about the embed_dim (aka d_model). If I have an input of size (10, 7) where 7 is my embedding dimension and 10 is my sequence length, does d_model = 7? The MHA component says <a href=""https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html</a></p>
<blockquote>
<p>embed_dim – Total dimension of the model.
num_heads – Number of parallel attention heads. Note that embed_dim
will be split across num_heads (i.e. each head will have dimension
embed_dim // num_heads).</p>
</blockquote>
<p>How can embed_dim be both num_heads * dim_per_head and also the embedding size of each token? This doesn't make sense. In other words, why do none of these code versions work?</p>
<p><code>x = nn.Transformer(nhead=2, d_model=7)</code> leads to</p>
<blockquote>
<p>AssertionError: embed_dim must be divisible by num_heads</p>
</blockquote>
<pre><code>x = nn.Transformer(nhead=2, d_model=14)
a = torch.rand((10,7))
x(a,a)
</code></pre>
<p>leads to</p>
<blockquote>
<p>RuntimeError: the feature number of src and tgt must be equal to
d_model</p>
</blockquote>
",Vectorization & Embeddings,pytorch transformer embed dimension model dimension src embedding also total dimension model trying use torch nn transformer confused embed dim aka model input size embedding dimension sequence length doe model mha component say embed dim total dimension model num head number parallel attention head note embed dim split across num head e head dimension embed dim num head embed dim num head dim per head also embedding size token make sense word none code version work lead assertionerror embed dim must num head lead runtimeerror feature number src tgt must equal model
Calculating similarity score in contexto.me clone,"<p>I am currently trying to clone the popular browser game contexto.me and I am having trouble with as to how to calculate the similarity score between two words (the target word and the user inputted guess word). I am able to get the cosine similarity between the two words, but as to how to properly quantify the score into a clean integer like in the game, I am confused as to how it is done.</p>
<p>For example, if the target word is 'helicopter' and I guess the word plane, contexto will return something like a similarity score of 13, but if I guess a word like 'king' contexto will return a score of '2000' for instance.</p>
<pre><code>target_word = &quot;helicopter&quot;
glove = torchtext.vocab.GloVe(name=&quot;6B&quot;, dim=100)


@app.route('/', methods=[&quot;GET&quot;, &quot;POST&quot;])
def getSimScore():
    if request.method == &quot;POST&quot;:
        text = request.form.get(&quot;word&quot;)
        new_text = singularize(text)
        sim_score = ((torch.cosine_similarity(glove[target_word].unsqueeze(0), glove[new_text].unsqueeze(0))).numpy()[0])
        print(sim_score)
    return render_template('homepage.html', messageText='sample text', gameNum=1, guessNum=1, wordAccuracy=999)
</code></pre>
<p>This is my code so far with sim_score printing to be ~0.77 for the input 'truck' and ~0.29 for the input 'king' (closer to 1 the more similar the word is to the target word).</p>
",Vectorization & Embeddings,calculating similarity score contexto clone currently trying clone popular browser game contexto trouble calculate similarity score two word target word user inputted guess word able get cosine similarity two word properly quantify score clean integer like game confused done example target word helicopter guess word plane contexto return something like similarity score guess word like king contexto return score instance code far sim score printing input truck input king closer similar word target word
How to store Bag of Words or Embeddings in a Database,"<p>I would like to store vector features, like Bag-of-Words or Word-Embedding vectors of a large number of texts, in a dataset, stored in a SQL Database.
What're the data structures and the best practices to save and retrieve these features?</p>
",Vectorization & Embeddings,store bag word embeddings database would like store vector feature like bag word word embedding vector large number text dataset stored sql database data structure best practice save retrieve feature
Ways to include context in the score generated against the document,"<p>I have a usecase with me. If you have any ideas or approaches on how to solve it (partially or fully) please let me know.</p>
<p>I am using vector database <code>milvus</code> for my application. Currently, the database contains columns <code>content</code> and <code>embeddings</code>. For each chunk (a chunk generally contains 2-3 lines of text), i am generating embeddings and storing them in <code>embeddings</code> column.</p>
<p>Now, coming to the use case. I have documents that speak about onboarding, how to do discord onboarding, are stored in vector db. I will query vector db <code>find all docs that contains slack onboarding</code>.</p>
<p>Based on the cosine similarity between the query embedding and the list of docs embedding, vector db picks the docs that has minimum distance between them.</p>
<p>So, i got list of docs which has onboarding and discord onboarding and a high score along with it. I am not aware of how the score is calculated. When i say high score assigned to each document listed in the resultant set, the scores are 83.5, 85.5 approx.</p>
<p>Since my query contains the term  <code>onboarding</code>, i got all onboarding docs. The query also contains <code>slack</code>. Is it possible to penalise all docs from the resultant set that doesn't have slack along with onboarding with it.</p>
<p>If so, i wish to know how can it be done.</p>
",Vectorization & Embeddings,way include context score generated document usecase idea approach solve partially fully please let know using vector database application currently database contains column chunk chunk generally contains line text generating embeddings storing column coming use case document speak onboarding discord onboarding stored vector db query vector db based cosine similarity query embedding list doc embedding vector db pick doc ha minimum distance got list doc ha onboarding discord onboarding high score along aware score calculated say high score assigned document listed resultant set score approx since query contains term got onboarding doc query also contains possible penalise doc resultant set slack along onboarding wish know done
How to determine if two sentences talk about similar topics?,"<p>I would like to ask you a question. Is there any algorithm/tool which can allow me to do some association between words?
For example: I have the following group of sentences:</p>
<pre><code>(1)
    &quot;My phone is on the table&quot;
    &quot;I cannot find the charger&quot;. # no reference on phone
(2) 
    &quot;My phone is on the table&quot;
    &quot;I cannot find the phone's charger&quot;. 
</code></pre>
<p>What I would like to do is to find a connection, probably a semantic connection, which can allow me to say that the first two sentences are talking about a topic (phone) as two terms (phone and charger) are common within it (in general). Same for the second sentence.
I should have something that can connect phone to charger, in the first sentence.
I was thinking of using Word2vec, but I am not sure if this is something that I can do with it.
Do you have any suggestions about algorithms that I can use to determine similarity of topics (i.e. sentence which are formulated in a different way, but having same topic)?</p>
",Vectorization & Embeddings,determine two sentence talk similar topic would like ask question algorithm tool allow association word example following group sentence would like find connection probably semantic connection allow say first two sentence talking topic phone two term phone charger common within general second sentence something connect phone charger first sentence wa thinking using word vec sure something suggestion algorithm use determine similarity topic e sentence formulated different way topic
NLContextualEmbedding neighbors in Swift?,"<p>Say I have some contextual embedding in Swift is there a way to find the neighbors? This already exists as a thing for NLEmbedding but that's not contextual. I feel like I might even be not fully understanding contextual embeddings since they do slightly confuse me but is this something that is possible right now? I checked the docs and couldn't really find anything for neighbors for NLContextualEmbedding.</p>
",Vectorization & Embeddings,nlcontextualembedding neighbor swift say contextual embedding swift way find neighbor already exists thing nlembedding contextual feel like might even fully understanding contextual embeddings since slightly confuse something possible right checked doc really find anything neighbor nlcontextualembedding
NLP: any easy and good methods to find semantic similarity between words?,"<p>I don't know whether StackOverflow covers NLP, so I am gonna give this a shot.
I am interested to find the semantic relatedness of two words from a specific domain, i.e. ""image quality"" and ""noise"".  I am doing some research to determine if reviews of cameras are positive or negative for a particular attribute of the camera.  (like image quality in each one of the reviews). </p>

<p>However, not everybody uses the exact same wording ""image quality"" in the posts, so I am out to see if there is a way for me to build something like that:</p>

<p>""image quality"" which includes (""noise"", ""color"", ""sharpness"", etc etc) 
so I can wrap all everything within one big umbrella.</p>

<p>I am doing this for another language, so Wordnet is not necessarily helpful. And no, I do not work for Google or Microsoft so I do not have data from people's clicking behaviour as input data either.      </p>

<p>However, I do have a lot of text, pos-tagged, segmented etc.          </p>
",Vectorization & Embeddings,nlp easy good method find semantic similarity word know whether stackoverflow cover nlp gon na give shot interested find semantic relatedness two word specific domain e image quality noise research determine review camera positive negative particular attribute camera like image quality one review however everybody us exact wording image quality post see way build something like image quality includes noise color sharpness etc etc wrap everything within one big umbrella another language wordnet necessarily helpful work google microsoft data people clicking behaviour input data either however lot text po tagged segmented etc
How to speed up the embedding process in LangChain,"<p>I have approximately 1600 short text files to embed using Sentence Transformers and store in a chroma vector in LangChain.</p>
<p>I want to create a Retrieval Question/Answering (QA) capability to retrieve those text files. I've done the processing and started the embedding process, but it's been 3-4 hours and it's still running. Is there any way to speed up this process?</p>
",Vectorization & Embeddings,speed embedding process langchain approximately short text file embed using sentence transformer store chroma vector langchain want create retrieval question answering qa capability retrieve text file done processing started embedding process hour still running way speed process
"Understanding Cosine Similarity, PMI, and PCA for a Beginner: Need Advice","<p>What is the relationship between cosine similarity and Pointwise Mutual Information? And how does Principal Component Analysis (PCA) fit into this? I'm currently working on a Python project that aims to measure word similarity across different centuries, and I'm a bit confused. Should I calculate the cosine similarity on the PMI Matrix? Do you have any suggestions on where I can find tutorials or scripts that serve as a good baseline? Thank you all in advance</p>
<p>Currently, I am performing the following steps for my project:</p>
<p>Creating a co-occurrence matrix with a window size of 5 to capture raw co-occurrences.
Transforming the co-occurrence matrix into a PMI matrix.
Comparing the PMI scores between three specific vectors of interest.
Extracting the vectors of words that co-occur with a particular vector (referred to as VECTOR1), which corresponds to the word I am studying.
Creating a semantic space using VECTOR1 and its associated &quot;co-vectors&quot; as a subregion of the PMI matrix.
Applying PCA to the semantic space and calculating the cosine similarity between VECTOR1 and each of the other co-vectors.</p>
",Vectorization & Embeddings,understanding cosine similarity pmi pca beginner need advice relationship cosine similarity pointwise mutual information doe principal component analysis pca fit currently working python project aim measure word similarity across different century bit confused calculate cosine similarity pmi matrix suggestion find tutorial script serve good baseline thank advance currently performing following step project creating co occurrence matrix window size capture raw co occurrence transforming co occurrence matrix pmi matrix comparing pmi score three specific vector interest extracting vector word co occur particular vector referred vector corresponds word studying creating semantic space using vector associated co vector subregion pmi matrix applying pca semantic space calculating cosine similarity vector co vector
Question about how to shorten a long sentence while keeping its semantic meaning,"<p>We are building a NLP system to compare semantic similarity between two sentences. Before sentence embedding, we'd like to 'shorten/compress' the sentences a bit because it is quite long.</p>
<p>Can we know how to shorten a long sentence in NLP task while keeping the semantic meaning of the sentence as much as possible? We've known a possible and simplest solution which uses a sliding window to extract various chunks from the input sentence:</p>
<p>For example, the input sentence is <code>In June 2017 Kaggle announced that it passed 1 million registered users.</code>. Several chunks can be extracted as follows:</p>
<pre><code>In June 2017 Kaggle announced that # chunk 1
announced that it passed 1 million # chunk 2
1 million registered users # chunk 3
</code></pre>
<p>Actually, this method may lose some semantic information of the sentence. Except for this simplest way, is there any other smarter way to shorten the sentence while keep semantic meaning?</p>
<p>Thanks!</p>
",Vectorization & Embeddings,question shorten long sentence keeping semantic meaning building nlp system compare semantic similarity two sentence sentence embedding like shorten compress sentence bit quite long know shorten long sentence nlp task keeping semantic meaning sentence much possible known possible simplest solution us sliding window extract various chunk input sentence example input sentence several chunk extracted follows actually method may lose semantic information sentence except simplest way smarter way shorten sentence keep semantic meaning thanks
Queries regarding the working of &quot;Word2Vec&quot; vectorizer to convert text to numeric representation,"<p>I worked on a predictive (classification) model. I used Word2Vec to convert the data is textual columns to numeric, following which I ran the machine learning algorithms.</p>
<p>I have the following doubts regarding the working of Word2Vec:</p>
<ol>
<li><p>When I check the vector representation of each word of a sentence, I get an array of 100 numbers/vectors. What do all these numbers mean? I know that each number corresponds to a dimension, but what is a dimension in this context (with regard to the vector space)?</p>
</li>
<li><p>When training the Word2Vec model on a 'Neural Network', each word in a sentence is fed as input to the input layer &amp; the words are one-hot encoded. So the vector representation of the words being fed would be something like, [1 0 0 0 0 0 0] &amp; [0 0 1 0 0 0 0].</p>
</li>
</ol>
<p>These vectors are initialized with random weights. The weighted sum of inputs is transmitted to the next layer (Hidden Layer). My doubt is, what is the point of assigning random weights  to these word vectors when the weights that are being multiplied with the 0s will anyways remain 0?</p>
<p>How is the neural network transmitting information across with sparse data?</p>
<p>Note: I referred to many sources &amp; these questions are being asked based on my understanding. Do let me know if I have interpreted any concept wrong. Thank you.</p>
",Vectorization & Embeddings,query regarding working word vec vectorizer convert text numeric representation worked predictive classification model used word vec convert data textual column numeric following ran machine learning algorithm following doubt regarding working word vec check vector representation word sentence get array number vector number mean know number corresponds dimension dimension context regard vector space training word vec model neural network word sentence fed input input layer word one hot encoded vector representation word fed would something like vector initialized random weight weighted sum input transmitted next layer hidden layer doubt point assigning random weight word vector weight multiplied anyways remain neural network transmitting information across sparse data note referred many source question asked based understanding let know interpreted concept wrong thank
How embedding lookup works in Word2Vec,"<p>I am trying to understand Word2Vec. For word input of 5x1 (one hot encoding) and hidden layer of 3 units.I have came across following information from famous sources. The first (monochrome) image says the first column will become the embedding vector when multiplying  3x 5 to 5 x 1 matrix while in 2nd image, 4th row is taken against 1 x 5 one hot encoding. This is confusing. The embedding lookup pics row or column is not understandable. Please help.</p>
<p><a href=""https://i.sstatic.net/xiNeO.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xiNeO.jpg"" alt=""enter image description here"" /></a></p>
",Vectorization & Embeddings,embedding lookup work word vec trying understand word vec word input x one hot encoding hidden layer unit came across following information famous source first monochrome image say first column become embedding vector multiplying x x matrix nd image th row taken x one hot encoding confusing embedding lookup pic row column understandable please help
NLP Classification on a dataset,"<p>I am trying to learned NLP. I understand the basic concepts from Text Preprocessing to td-idf, and Word Embedding. How do I apply this learning? I have a Data set with two columns: Answer and Gender. I want to use NLP to transform the Answer column to vectors and then use supervised machine learning to train a model that predict where a certain type of answer was given by male or a female.
I dont know how to process after I Pre_processed the text.</p>
",Vectorization & Embeddings,nlp classification dataset trying learned nlp understand basic concept text preprocessing td idf word embedding apply learning data set two column answer gender want use nlp transform answer column vector use supervised machine learning train model predict certain type answer wa given male female dont know process pre processed text
Handling large inputs in Programming Language Models,"<p>I am using CodeBERT and CodeT5 models to generate embeddings for my Python code dataset. Specifically, I am using the <code>codebert-base</code> and <code>codet5-small</code> models from HuggingFace. However, these models have a maximum input sequence length of 512 tokens. When tokenizing the Python codes, I find that their length is approximately 10 times larger than the maximum input sequence length. My intention is not to perform any downstream tasks; rather, I want to obtain the embeddings for the purpose of clustering. I am wondering how to handle these lengthy input codes aside from removing less important parts from the inputs.</p>
",Vectorization & Embeddings,handling large input programming language model using codebert codet model generate embeddings python code dataset specifically using model huggingface however model maximum input sequence length token tokenizing python code find length approximately time larger maximum input sequence length intention perform downstream task rather want obtain embeddings purpose clustering wondering handle lengthy input code aside removing le important part input
Tf-Idf weights for an unseen word in query,"<p>Assume we have a Tf-Idf matrix and a new query came. If this query has words that haven't been seen before, how can we find its vector and use the cosine similarity?</p>
<p>I've seen <a href=""https://stackoverflow.com/questions/37106194/how-do-i-calculate-tf-idf-of-a-query"">How do I calculate tf idf of a query</a> that for each word of a query, we have to count the number of occurrences of this word in the query as tf, and for its idf, we have to use the idf for the word that has been found from the documents. But if the query has a new word, what will be this idf?</p>
<p>I'll be grateful if you could explain it to me.</p>
",Vectorization & Embeddings,tf idf weight unseen word query assume tf idf matrix new query came query ha word seen find vector use cosine similarity seen href calculate tf idf query word query count number occurrence word query tf idf use idf word ha found document query ha new word idf grateful could explain
"use BERT word to vector embedding only on word, not sentence","<p>How to use BERT word to vector embedding only on word, not sentence?</p>
<p>I have list of nouns and I need vector version of these words using BERT. I researched a lot on how to do it, but I could only achieve a result using SentenceTransformer, which is not a correct way to solve a problem.</p>
<p>Here is the example:
<code>embedder = SentenceTransformer('all-mpnet-base-v2')</code>
<code>embeddings = embedder.encode(words).tolist()</code></p>
<p>I got confused with the BertTokenizer and Vectorizer, while reseaching on the problem. They give me tensors or weird lists that i cannot further use for my problem, whereas I could easily use the output of SentenceTransformer.</p>
<p>Any help or recommendations would be very helpful!</p>
",Vectorization & Embeddings,use bert word vector embedding word sentence use bert word vector embedding word sentence list noun need vector version word using bert researched lot could achieve result using sentencetransformer correct way solve problem example got confused berttokenizer vectorizer reseaching problem give tensor weird list use problem whereas could easily use output sentencetransformer help recommendation would helpful
How to interpret word2vec train output?,"<p>Running the code snippet below report an output (3, 60). I wonder what exactly it is reporting?</p>
<p>The code is reproducible..just copy into a notebook cell and run.</p>
<pre><code>from gensim.models import Word2Vec    
sent = [['I', 'love', 'cats'], ['Dogs', 'are', 'friendly']]
w2v_model = Word2Vec(sentences=sent, vector_size=100, window=7, min_count=1,sg=1)
w2v_model.train(sent, total_examples=len(sent), epochs=10)
</code></pre>
<p>(3, 60)</p>
",Vectorization & Embeddings,interpret word vec train output running code snippet report output wonder exactly reporting code reproducible copy notebook cell run
gensim word2vec: Find number of words in vocabulary,"<p>After training a word2vec model using python <a href=""http://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">gensim</a>, how do you find the number of words in the model's vocabulary?</p>
",Vectorization & Embeddings,gensim word vec find number word vocabulary training word vec model using python gensim find number word model vocabulary
Fine-tune SentenceTransformer/SBERT for Extractive Text Summarization,"<p>Newbie here on NLP.</p>
<p>I want to build extractive text summarization, try to read this <a href=""https://huggingface.co/blog/how-to-train-sentence-transformers"" rel=""nofollow noreferrer"">https://huggingface.co/blog/how-to-train-sentence-transformers</a>, I think there is a way to fine-tune the model with my own dataset (data, and language), in case 2 like this <code>Case 2: The example is a pair of positive (similar) sentences without a label. For example, pairs of paraphrases, pairs of full texts and their summaries</code> but there is no tutorial on it. Want to try but all tutorial use labels.</p>
<p>Or I want to fine-tune the embedding on <code>Sentence-bert</code> but didn't find how to do it too. I think we can fine-tune the embedding and get the similarity of all sentences in the text.</p>
<p>Try on BERT but get stuck cause the tutorial is obsolete.</p>
<p>I was just lost and confused. Is anybody here who ever tries or knows and can give me advice or some tutorial code?</p>
",Vectorization & Embeddings,fine tune sentencetransformer sbert extractive text summarization newbie nlp want build extractive text summarization try read think way fine tune model dataset data language case like tutorial want try tutorial use label want fine tune embedding find think fine tune embedding get similarity sentence text try bert get stuck cause tutorial obsolete wa lost confused anybody ever try know give advice tutorial code
How to find most similar string values in a dataframe?,"<p>I am finding the similarity between the sentence using embedding sentence and looping through all the document's embedded sentences to find the right match relative to the search string. I also want to display the document name in the output along with the similarity match result but am not sure how I can extract that information from the dataframe respective to the sentence we get in the output result. I have tried the index method but it is not showing me the correct document name.</p>
<p>Please guide how can I get the document name in the result output along with the sentence.</p>
<p>My data frame looks like this:</p>
<pre><code>Document name        Document sentences in tokens

Doc 1                 [Sentence 1, sentence 2, sentence 3]

Doc 2                 [Sentence 1, sentence 2, sentence 3]

</code></pre>
<p>I have used the following code to find the top 10 matches with the search string.</p>
<pre><code>from itertools import chain
docs_sent_tokens=list(chain.from_iterable(main_df['Sentence_Tokenize_rules']))
docs_name=main_df['Document name']


results=[]

#set the threshold value to get the similarity result accordingly
threshold=0

#embedding all the documents and find the similarity between search text and all the tokenize sentences
for docs_sent_token in docs_sent_tokens:

#To find the document name
    for index in main_df.index:
        doc_name= main_df['Document name'][index]

    sentence_embeddings = model.encode(docs_sent_token)
    sim_score1 = cosine_sim(search_sentence_embeddings, sentence_embeddings)
    if sim_score1 &gt; threshold:
            results.append((
                docs_sent_token,
                sim_score1,
                doc_name

                ))

#printing the top 10 matching result in dataframe format
df=pd.DataFrame(results, columns=['Matching Sentence','Similarity Score','Docuemnt name'])

# sorting in descending order based on the similarity score
df.sort_values(&quot;Similarity Score&quot;, ascending = False, inplace = True)

#change the value of n to see more results
df.head(n=10)
</code></pre>
<p>Output should be like this:</p>
<pre><code>Matching sentence    similarity score    document name
Sentence 12              0.80            doc 1
sentence 15              0.69            doc 3
</code></pre>
",Vectorization & Embeddings,find similar string value dataframe finding similarity sentence using embedding sentence looping document embedded sentence find right match relative search string also want display document name output along similarity match result sure extract information dataframe respective sentence get output result tried index method showing correct document name please guide get document name result output along sentence data frame look like used following code find top match search string output like
How to cluster similar sentences using BERT,"<p>For ElMo, FastText and Word2Vec, I'm averaging the word embeddings within a sentence and using HDBSCAN/KMeans clustering to group similar sentences.</p>

<p>A good example of the implementation can be seen in this short article: <a href=""http://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-machine-learning/"" rel=""noreferrer"">http://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-machine-learning/</a></p>

<p>I would like to do the same thing using BERT (using the BERT python package from hugging face), however I am rather unfamiliar with how to extract the raw word/sentence vectors in order to input them into a clustering algorithm. I know that BERT can output sentence representations - so how would I actually extract the raw vectors from a sentence?</p>

<p>Any information would be helpful.</p>
",Vectorization & Embeddings,cluster similar sentence using bert elmo fasttext word vec averaging word embeddings within sentence using hdbscan kmeans clustering group similar sentence good example implementation seen short article would like thing using bert using bert python package hugging face however rather unfamiliar extract raw word sentence vector order input clustering algorithm know bert output sentence representation would actually extract raw vector sentence information would helpful
How can I find the cosine similarity between two song lyrics represented as strings?,"<p>My friends and I are doing an NLP project on song recommendation.</p>
<p>Context: We originally planned on giving the model a recommended song playlist that has the most similar lyrics based on the random input corpus(from the literature etc), however we didn't really have a concrete idea of its implementation.</p>
<p>Currently our task is to find similar lyrics to a random lyric fed as a string input. We are using sentence BERT model(sbert) and cosine similarity to find the similarity between the songs and it seems like the output numbers are meaningful enough to find the most similar song lyrics.</p>
<p>Is there any other way that we can improve this approach?</p>
<p>We'd like to use BERT model and are open to suggestions that can be used on top of BERT if possible, but if there is any other models that should be used instead of BERT, we'd be happy to learn. Thanks.</p>
",Vectorization & Embeddings,find cosine similarity two song lyric represented string friend nlp project song recommendation context originally planned giving model recommended song playlist ha similar lyric based random input corpus literature etc however really concrete idea implementation currently task find similar lyric random lyric fed string input using sentence bert model sbert cosine similarity find similarity song seems like output number meaningful enough find similar song lyric way improve approach like use bert model open suggestion used top bert possible model used instead bert happy learn thanks
Cosine Similarity on Text,"<p>I have a dataset with one column containing course names. I need to write a code to allow for a query search that returns the 10 most similar courses to the given query.</p>
<p>Below is the code that I tried. However, it is returning the query itself with the score of 1.0. Kindly help me out with this!</p>
<pre><code>import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# List of documents to search

documents = list_of_names

# Transform documents into vectors using the CountVectorizer
vectorizer = CountVectorizer()
vectorizer.fit_transform(documents)

# Define the search query
query = 'aws'

# Transform the query into a vector using the fitted vectorizer
query_vector = vectorizer.transform([query])

# Calculate the cosine similarity between the query vector and all document vectors
similarity_scores = cosine_similarity(query_vector, vectorizer.transform(documents))

# Find the indices of the top 10 most similar documents
most_similar_indices = np.argsort(similarity_scores)[:, :-11:-1]

# Print the top 10 most similar documents and their similarity scores
for i, indices in enumerate(most_similar_indices[0]):
    print('Rank:', i+1)
    print('Document:', documents[indices])
    print('Similarity score:', similarity_scores[0][indices])
    print('---')
</code></pre>
",Vectorization & Embeddings,cosine similarity text dataset one column containing course name need write code allow query search return similar course given query code tried however returning query score kindly help
Why does model.fit() function give error?,"<p>I generated embedding matrix from BERT embeddings as below:</p>
<pre><code># Load pre-trained model tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertModel.from_pretrained('bert-base-multilingual-cased')

# Define batch size
batch_size = 1

# Tokenize and encode input data in batches
encoded_inputs = []
for i in range(0, len(labeled_data), batch_size):
    inputs = labeled_data[i:i+batch_size]
    encoded_inputs.append(tokenizer.batch_encode_plus(inputs, padding=True, truncation=True, return_tensors=&quot;pt&quot;))

# Generate embeddings for each batch
embeddings_new = []
for encoded_input in tqdm(encoded_inputs):
    with torch.no_grad():
        model_output = model(**encoded_input)
    batch_embeddings = model_output.last_hidden_state.mean(dim=1)
    embeddings_new.append(batch_embeddings)

embeddings_new = tf.concat(embeddings_new, axis=0) 
embedding_matrix = model.embeddings.word_embeddings.weight
embedding_matrix = embedding_matrix.cpu().detach().numpy()
embed_tensor = tf.convert_to_tensor(embedding_matrix, dtype=tf.float32)
</code></pre>
<p>Then using this matrix as initial weights to an LSTM:</p>
<pre><code>lstm_out1 = 150
embed_dim = 768
   
model = Sequential()
model.add(Embedding(embedding_matrix.shape[0], embed_dim, weights=[embed_tensor], input_length=50, trainable=False))
model.add(LSTM(lstm_out1, dropout=0.2, recurrent_dropout=0.2))

model.add(Dense(64, activation='relu'))

model.add(Dense(1, activation='sigmoid'))

adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss='binary_crossentropy',
                  optimizer=adam,
                  metrics=['accuracy'])

model.summary()
</code></pre>
<p>Now when I call model.fit it produces error.</p>
<pre><code>model.fit(tokenized_sentences, labels, batch_size=5, epochs=1, shuffle=True)
</code></pre>
<p>The error is:</p>
<pre><code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).
</code></pre>
<p>The content of tokenized_sentences is a list:</p>
<pre><code>[[101, 10406, 10161, 170, 10350, 87881, 10113, 21681, 11460, 13080, 10114, 10380, 10201, 10113, 108850, 10138, 12718, 10112, 10126, 15694, 10269, 62137, 13173, 10483, 102].....]
</code></pre>
<p>The content of the labels is also a list:</p>
<pre><code>[1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0]
</code></pre>
<p>The embed_tensor is like below:</p>
<pre><code>tf.Tensor(
[[ 0.02595074 -0.00617341 -0.00409975 ...  0.02965234  0.02417551
   0.01970279]
 [ 0.01038065 -0.0136286   0.00672081 ...  0.01237162  0.0267217
   0.03370738]
 [ 0.0220679  -0.00360613  0.01932366 ...  0.0069061   0.026809
   0.00498276]
 ...
 [ 0.00684139  0.01885802  0.02666426 ...  0.02292391  0.06465269
   0.04373793]
 [ 0.0183579   0.01480132  0.02434449 ...  0.03205629  0.00708906
   0.02039703]
 [ 0.02139908  0.01879423 -0.01343376 ... -0.00597953  0.00583893
  -0.00586251]], shape=(119547, 768), dtype=float32)
</code></pre>
<p>Code for generating tokenized_sentences:</p>
<pre><code>tokenized_sentences = []
for sentence in labeled_data:
    # Apply the tokenizer to each sentence to obtain its tokens
    tokens = tokenizer.encode(sentence, add_special_tokens=True)
    # Append the tokenized sentence to the list
    tokenized_sentences.append(tokens)
</code></pre>
<p>Why does model.fit() give error?</p>
",Vectorization & Embeddings,doe model fit function give error generated embedding matrix bert embeddings using matrix initial weight lstm call model fit produce error error content tokenized sentence list content label also list embed tensor like code generating tokenized sentence doe model fit give error
How does the word2vec produce embeddings for the unseen words?,"<p>I'm using an <strong>unlabeled</strong> news corpus to fine-tune the Word2Vec model. After that I'm using those embeddings to generate embeddings for words present in a new <strong>labeled</strong> dataset. These new embeddings were fed to an RNN as initial weights. I've shared the code for generating the embedding matrix:</p>
<pre><code>embed_dim = embedding_size
words_not_found = []
nb_words = min(MAX_NB_WORDS, len(word_index))
embedding_matrix = np.random.rand(nb_words+1, embed_dim)

    for word, i in word_index.items():
        if i &gt;= nb_words:
            continue
        #print(word)
    
        if embeddings_index.wv.__contains__(word): 
    
            embedding_vector = embeddings_index.wv[word]
        
            embedding_matrix[i] = embedding_vector
        else:
            words_not_found.append(word)   
</code></pre>
<p>I've seen that the matrix contains embeddings for all of the unique words in the new <strong>labeled</strong> dataset.</p>
<p><strong>Can anyone tell me how did the word2vec model produce embeddings for the words which were not present in the initial news corpus?</strong></p>
",Vectorization & Embeddings,doe word vec produce embeddings unseen word using unlabeled news corpus fine tune word vec model using embeddings generate embeddings word present new labeled dataset new embeddings fed rnn initial weight shared code generating embedding matrix seen matrix contains embeddings unique word new labeled dataset anyone tell word vec model produce embeddings word present initial news corpus
Fine tune a custom word2vec model with gensim 4,"<p>I am new using gensim, especially with gensim 4. To be honest, I found quite hard to understand the docs how to fine-tune a pre-trained word2vec model.
I have a binary pre-trained model saved local. I would like to fine tune this model on new data.</p>
<p>My questions are;</p>
<ul>
<li>how to create the vocab merging both vocabs?</li>
<li>is that the correct approach to fine-tune a word2vec model?</li>
</ul>
<p>So far i have created the following code:</p>
<pre><code># path to pretrained model
pretrained_path = '../models/german.model'

# new data
sentences = df.stem_token_wo_sw.to_list() # Pandas column containing text data

# Create new model
w2v_de = Word2Vec(
    min_count = min_count,
    vector_size = vector_size,
    window = window,
    workers = workers,
)

# Build vocab
w2v_de.build_vocab(sentences)

# Extract number of examples
total_examples = w2v_de.corpus_count

# Load pretrained model
model = KeyedVectors.load_word2vec_format(pretrained_path, binary=True)

# Add previous words from pretrained model
w2v_de.build_vocab([list(model.key_to_index.keys())], update=True)

# Train model
w2v_de.train(sentences, total_examples=total_examples, epochs=2)

# create array of vectors
vectors = np.asarray(w2v_de.wv.vectors)
# create array of labels
labels = np.asarray(w2v_de.wv.index_to_key) 

# create dataframe of vectors for each word
w_emb = pd.DataFrame(
    index = labels,
    columns = [f'X{n}' for n in range(1, vectors.shape[1] + 1)],
    data = vectors,
)
</code></pre>
<p>After training I use PCA to reduce the dimensions from 300 to two, in order to plot the word-embedding space.</p>
<pre><code># create pipeline
pipeline = Pipeline(
    steps = [
        # ('scaler', StandardScaler()),
        ('pca', PCA(n_components=2)),
    ]
)

# fit pipeline
pipeline.fit(w_emb)

# Transform vectors
vectors_transformed = pipeline.transform(w_emb)

w_emb_transformed = (
    pd.DataFrame(
        index = labels,
        columns = ['PC1', 'PC2'],
        data = vectors_transformed,
    )
)
</code></pre>
<p>The <code>labels</code> and <code>vectors</code> do only contain the new words, and not the old + new words and so does my plot and PCA values.</p>
",Vectorization & Embeddings,fine tune custom word vec model gensim new using gensim especially gensim honest found quite hard understand doc fine tune pre trained word vec model binary pre trained model saved local would like fine tune model new data question create vocab merging vocabs correct approach fine tune word vec model far created following code training use pca reduce dimension two order plot word embedding space contain new word old new word doe plot pca value
"Read GloVe pre-trained embeddings into R, as a matrix","<p>Working in R. I know the pre-trained GloVe embeddings (e.g., ""glove.6B.50d.txt"") can be found here: <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a>. However, I've had zero luck reading this text file into R so that the product is the word embedding matrix of words by vectors. Has anyone successfully done this, either pulling from a saved .txt file or from the site itself, and if so how was that text converted to a matrix in R?</p>
",Vectorization & Embeddings,read glove pre trained embeddings r matrix working r know pre trained glove embeddings e g glove b txt found however zero luck reading text file r product word embedding matrix word vector ha anyone successfully done either pulling saved txt file site wa text converted matrix r
is there anyway to get the actual vector embedding of a word or set of characters using flair nlp? i.e flair embeddings,"<p>Basically I'm trying to use a custom flair language model to get a word or sentence's embedding in a vector. Is this possible or do flair embeddings only function when using flair NER models?</p>
<p>When using the embeddings .embed() function I receive an output like &quot;[Sentence: &quot;pain&quot;   [− Tokens: 1]]&quot;
where as I'm looking for the vector of continuous numbers.</p>
<p>Thank you.</p>
",Vectorization & Embeddings,anyway get actual vector embedding word set character using flair nlp e flair embeddings basically trying use custom flair language model get word sentence embedding vector possible flair embeddings function using flair ner model using embeddings embed function receive output like sentence pain token looking vector continuous number thank
how can I combine my own datasets and &quot;wiki_dpr&quot; provided by huggingface?,"<p>I tried to add the document I found to the &quot;wiki_dpr&quot; dataset provided by &quot;https://huggingface.co/datasets/wiki_dpr &quot;, but I couldn't find a way to get the embedding value from that dataset.</p>
<p>I am currently trying to combine wiki_dpr and my own datasets.
but I don't know how to make the embedding value the same way as wiki_dpr.</p>
<p>As an experiment, I embed the text of id=&quot;7&quot; of wiki_dpr, but this result was very different from wiki_dpr.</p>
<p>I executed the code below.</p>
<pre><code>!pip install datasets evaluate transformers\[sentencepiece\]
!apt install libomp-dev
!pip install faiss-cpu
!pip install -U sentence-transformers
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer

\#data
add_dpr_index = &quot;7&quot;
add_dpr_title = &quot;Aaron&quot;
add_dpr_text = &quot;in literature dating to the Babylonian captivity and later. The books of Judges, Samuel and Kings mention priests and Levites, but do not mention the Aaronides in particular. The Book of Ezekiel, which devotes much attention to priestly matters, calls the priestly upper class the Zadokites after one of King David's priests. It does reflect a two-tier priesthood with the Levites in subordinate position. A two-tier hierarchy of Aaronides and Levites appears in Ezra, Nehemiah and Chronicles. As a result, many historians think that Aaronide families did not control the priesthood in pre-exilic Israel. What is clear is that high&quot;

tokenizer = DPRContextEncoderTokenizer.from_pretrained(&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;)
model = DPRContextEncoder.from_pretrained(&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;)
input_ids = tokenizer(add_dprtext, return_tensors=&quot;pt&quot;)\[&quot;input_ids&quot;\]
embeddings = model(input_ids).pooler_output
print(embeddings)
</code></pre>
<p>I expected the result of the code to be a vector value of</p>
<pre><code>[0.12092622369527817, 0.4741949737071991, -0.3044947385787964, ...]
</code></pre>
<p>but the different values came out</p>
<pre><code>[5.3140e-01, 2.9851e-01, -5.3120e-01, ...]
</code></pre>
<p>and this means that the way is different.</p>
<p>Where can I get an embedding model specific to <code>wiki_dpr</code>?</p>
",Vectorization & Embeddings,combine datasets wiki dpr provided huggingface tried add document found wiki dpr dataset provided find way get embedding value dataset currently trying combine wiki dpr datasets know make embedding value way wiki dpr experiment embed text id wiki dpr result wa different wiki dpr executed code expected result code vector value different value came mean way different get embedding model specific
Does hashing in Fasttext lead to different ngrams sharing the same embedding?,"<p>As per Section 3.2 in the <a href=""https://arxiv.org/pdf/1607.04606.pdf"" rel=""nofollow noreferrer"">original paper on Fasttext</a>, the authors state:</p>
<blockquote>
<p>In order to bound the memory requirements of our model, we use a
hashing function that maps n-grams to integers in 1 to K</p>
</blockquote>
<p>Does this mean the model computes only K embeddings regardless of the number of distinct ngrams extracted from the training corpus, and if 2 different ngrams collide when hashed, they share the same embedding?</p>
<p>Thanks.</p>
",Vectorization & Embeddings,doe hashing fasttext lead different ngrams sharing embedding per section original paper fasttext author state order bound memory requirement model use hashing function map n gram integer k doe mean model computes k embeddings regardless number distinct ngrams extracted training corpus different ngrams collide hashed share embedding thanks
My text classification problem only need an embedding layer but why and how can I solve that?,"<p>I have a problem with my text classification with tensorflow.
The text was done using the layer TextVectorization and the output_mode=&quot;int&quot;.
The problem I have, the best test_accuracy I get is with an embedding layer combined with the output layer.
As soon as other layers come between the embedding layer and the output layer, be it Dense, LSTM, CNN layers, the result gets only slightly worse or stays the same.
What am I doing wrong?</p>
<p>This is my Bag of Words:</p>
<pre><code>#Tokenizer:
train_layer = tf.keras.layers.TextVectorization(
    max_tokens=15000,
    output_mode=&quot;int&quot;,
    output_sequence_length=200,
    ngrams=None

train_layer.adapt(X_train)
</code></pre>
<p>This is my best solution Model:</p>
<pre><code>model = tf.keras.models.Sequential([
        train_layer,
        tf.keras.layers.Embedding(15000, 100, input_length=200),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(8, activation=&quot;softmax&quot;)
        ])

    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)

    model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=optimizer, metrics=[&quot;accuracy&quot;])
</code></pre>
<p>Now I can insert more Dense layers or LSTM or Conv1D + GlobalMaxPool1D Layers between the embedding layer and the last dense layer. The result gets slightly worse or stays the same at 0.86 test_accuracy, 0.83 test_precision and 0.8 test_recall in average.
I'am expected that the model with cnn layer(s) or lstm layer(s) are better than the only embedding layer model?</p>
",Vectorization & Embeddings,text classification problem need embedding layer solve problem text classification tensorflow text wa done using layer textvectorization output mode int problem best test accuracy get embedding layer combined output layer soon layer come embedding layer output layer dense lstm cnn layer result get slightly worse stay wrong bag word best solution model insert dense layer lstm conv globalmaxpool layer embedding layer last dense layer result get slightly worse stay test accuracy test precision test recall average expected model cnn layer lstm layer better embedding layer model
How to apply similarity measure on csr format datatype,"<p>I have a 22 million dataframe for training, where I applied tf-idf vectoriser, The resultant vector is in csr-format. Now, I have to apply the tfidf.transform on the test data and compare each row in the test data to each row in the training data with some similarity measure. How can I do this efficiently?</p>
<p>(On applying toarray() to the tfidf of training data, my system is crashing.)</p>
",Vectorization & Embeddings,apply similarity measure csr format datatype million dataframe training applied tf idf vectoriser resultant vector csr format apply tfidf transform test data compare row test data row training data similarity measure efficiently applying toarray tfidf training data system crashing
Boosting documents with term matches in elasticsearch after cosine similarity,"<p>I am using text embeddings stored in elasticsearch to get documents similar to a query. But I noticed that in some cases, I get documents that don't have the words from the query in them with a higher score. So I want to boost the score for documents that have the words from the query. How do I do this in elasticsearch?</p>
<p>This is my index</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;mappings&quot;: {
        &quot;properties&quot;: {
            &quot;question_text&quot;: {
            &quot;type&quot;: &quot;text&quot;
            },
            &quot;question_vector&quot;: {
            &quot;type&quot;: &quot;dense_vector&quot;,
            &quot;dims&quot;: 768
            }
        }
    }
}
</code></pre>
<p>I tried doing this</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;query&quot;:{
        &quot;script_score&quot;: {
            &quot;query&quot;: {
                &quot;bool&quot;: {
                    &quot;must&quot;: [
                        {
                            &quot;more_like_this&quot;: {
                                &quot;fields&quot;: [
                                    &quot;question_text&quot;
                                ],
                                &quot;like&quot;: query_text,
                                &quot;min_term_freq&quot;: 1,
                                &quot;max_query_terms&quot;: 12,
                                &quot;minimum_should_match&quot;: &quot;3&lt;60%&quot;
                            }
                        }
                    ]
                }
            },
            &quot;script&quot;: {
                &quot;source&quot;: &quot;cosineSimilarity(params.query_vector, 'question_vector') + 1.0&quot;,
                &quot;params&quot;: {&quot;query_vector&quot;: query_vector}
            }
        }
    },
    &quot;fields&quot;: [
        &quot;question_text&quot;
    ],
    &quot;_source&quot;: false
}
</code></pre>
<p>But now I only get documents that have the words in them. Is there a way to do this, but still get matches that don't have the words in them, but with lower scores?</p>
",Vectorization & Embeddings,boosting document term match elasticsearch cosine similarity using text embeddings stored elasticsearch get document similar query noticed case get document word query higher score want boost score document word query elasticsearch index tried get document word way still get match word lower score
Why are my deep learning models giving unreasonably high accuracy on test data?,"<p>I'm trying to do sarcasm detection on Twitter data to replicate the results mentioned in this <a href=""https://aclanthology.org/2020.wnut-1.2.pdf"" rel=""nofollow noreferrer"">paper</a>. Binary classification problem. For that I used a separate set of <strong>unlabeled</strong> tweets to create the embedding matrix using Word2Vec model. Before doing that I preprocessed the <strong>unlabeled</strong> data and removed the rare words as mentioned in the paper. Code is as follows:</p>
<pre><code>model = Word2Vec(df_hing_eng['tweet_text'], vector_size=300, window=10, hs=0, negative = 1)
embedding_size = model.wv.vectors.shape[1]
</code></pre>
<p>Next I fit a tokenizer on this <strong>unlabeled</strong> data:</p>
<pre><code>tok = Tokenizer()
tok.fit_on_texts(df_hing_eng['tweet_text'])
vocab_size = len(tok.word_index) + 1
</code></pre>
<p>Next, I created the embedding matrix as follows:</p>
<pre><code>word_vec_dict={}
for word in vocab:
    word_vec_dict[word]=model.wv.get_vector(word)

embed_matrix=np.zeros(shape=(vocab_size,embedding_size))
for word,i in tok.word_index.items():
    embed_vector=word_vec_dict.get(word)
    if embed_vector is not None:  
        embed_matrix[i]=embed_vector 
</code></pre>
<p>Now, I'm using a separate set of <strong>labeled</strong> tweets to be used as training and test data (for the DL models). I used the same preprocessing steps as the <code>unlabeled</code> data and removed the same rare words we found in the <code>unlabeled</code> data. Now I find the maximum length of all tweets in the <strong>labeled</strong> data.</p>
<pre><code>maxi = -1
for row in df_labeled.loc[:,'tweet_text']:
    if len(row)&gt;maxi:
        maxi = len(row)
</code></pre>
<p>After that I used the tokenizer, that I fit on the <strong>unlabeled</strong> data, to create the word indices for the <strong>labeled</strong> data as follows:</p>
<pre><code>encoded_tweets = tok.texts_to_sequences(df_labeled['tweet_text'])
</code></pre>
<p>Now I padded the <strong>labeled</strong> data to the length of the maximum tweets among the <strong>labeled</strong> data.</p>
<pre><code>padded_tweets = pad_sequences(encoded_tweets, maxlen=maxi, padding='post')
</code></pre>
<p>Finally, I split the <strong>labeled</strong> data into training and test data as follows,</p>
<pre><code>x_train,x_test,y_train,y_test=train_test_split(padded_tweets, df_labeled['is_sarcastic'], test_size=0.10, random_state=42)
</code></pre>
<p><strong>Is there any data leakage anywhere from training to test data or any other problem?</strong> Almost all of my DL models are giving more than 90% accuracy contrary to the original paper which reported a maximum of 75% accuracy. The codes for DL models were written by the authors of the papers. I used the same parameters as they mentioned.</p>
<p>The tokenizer was actually fit on a completely different <strong>unlabeled</strong> data that is absolutely separate from (<strong>labeled</strong>) training and test data.</p>
<p><strong>Edit</strong></p>
<p>Further analyses of the datasets reveal the followings:</p>
<ol>
<li>There are huge overlaps between <strong>unlabeled</strong> embedding data and <strong>labeled</strong> data.</li>
<li>There are many duplicate rows in both <strong>labeled</strong> and <strong>unlabeled</strong> data.</li>
</ol>
<p>I shared the code of an LSTM below. These codes originally written by the authors of the paper.</p>
<pre><code>#define callbacks
early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)
callbacks_list = [early_stopping]



 #this one---&gt;LSTM

    lstm_out1 = 150
    
    model = Sequential()
    model.add(Embedding(vocab_size, embed_dim,
              weights=[embed_matrix], input_length=max_tweet_len, trainable=False))
    model.add(LSTM(lstm_out1, dropout=0.2, recurrent_dropout=0.2))
    
    model.add(Dense(64, activation='relu'))
    
    model.add(Dense(1, activation='sigmoid'))
    
    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
    
    model.compile(loss='binary_crossentropy',
                      optimizer=adam,
                      metrics=['accuracy'])
    
    model.summary()
</code></pre>
",Vectorization & Embeddings,deep learning model giving unreasonably high accuracy test data trying sarcasm detection twitter data replicate result mentioned paper binary classification problem used separate set unlabeled tweet create embedding matrix using word vec model preprocessed unlabeled data removed rare word mentioned paper code follows next fit tokenizer unlabeled data next created embedding matrix follows using separate set labeled tweet used training test data dl model used preprocessing step data removed rare word found data find maximum length tweet labeled data used tokenizer fit unlabeled data create word index labeled data follows padded labeled data length maximum tweet among labeled data finally split labeled data training test data follows data leakage anywhere training test data problem almost dl model giving accuracy contrary original paper reported maximum accuracy code dl model written author paper used parameter mentioned tokenizer wa actually fit completely different unlabeled data absolutely separate labeled training test data edit analysis datasets reveal following huge overlap unlabeled embedding data labeled data many duplicate row labeled unlabeled data shared code lstm code originally written author paper
Is it possible to freeze only certain embedding weights in the embedding layer in pytorch?,"<p>When using GloVe embedding in NLP tasks, some words from the dataset might not exist in GloVe. Therefore, we instantiate random weights for these unknown words.</p>

<p>Would it be possible to freeze weights gotten from GloVe, and train only the newly instantiated weights?</p>

<p>I am only aware that we can set:
model.embedding.weight.requires_grad = False</p>

<p>But this makes the new words untrainable..</p>

<p>Or are there better ways to extract semantics of words.. </p>
",Vectorization & Embeddings,possible freeze certain embedding weight embedding layer pytorch using glove embedding nlp task word dataset might exist glove therefore instantiate random weight unknown word would possible freeze weight gotten glove train newly instantiated weight aware set model embedding weight requires grad false make new word untrainable better way extract semantics word
How to efficiently mean-pool BERT embeddings while excluding padding?,"<p>Consider a batch of sentences with different lengths.</p>
<p>When using the BertTokenizer, I apply padding so that all the sequences have the same length and we end up with a nice tensor of shape <code>(bs, max_seq_len)</code>.</p>
<p>After applying the BertModel, I get a last hidden state of shape <code>(bs, max_seq_len, hidden_sz)</code>.</p>
<p>My goal is to get the mean-pooled sentence embedding for each sentence (resulting in something with shape <code>(bs, hidden_sz)</code>), but <em>excluding</em> the embeddings for the PAD tokens when taking the mean.</p>
<p>Is there a way to do this efficiently without looping over each sequence in the batch?</p>
<p>Thanks!</p>
",Vectorization & Embeddings,efficiently mean pool bert embeddings excluding padding consider batch sentence different length using berttokenizer apply padding sequence length end nice tensor shape applying bertmodel get last hidden state shape goal get mean pooled sentence embedding sentence resulting something shape excluding embeddings pad token taking mean way efficiently without looping sequence batch thanks
Should I remove all the empty rows in a Dataframe before generating Word2Vec embeddings?,"<p>The Pandas Dataframe column which I used to create the Word2Vec embeddings contains empty rows for some rows. It looks like this after tokenization---&gt;<code>[]</code>. should I remove all such samples?</p>
<p>I have shared the code for tokenization and Word2Vec generation below:</p>
<pre><code>nltk.download('punkt')
df['tweet_text'] = df['tweet_text'].apply(nltk.word_tokenize)
    
model = Word2Vec(df['tweet_text'], vector_size=300, window=10, hs=0, negative = 1)
</code></pre>
<p>If I don't remove such empty rows will it cause serious problems? If yes, what kinds of problems?</p>
",Vectorization & Embeddings,remove empty row dataframe generating word vec embeddings panda dataframe column used create word vec embeddings contains empty row row look like tokenization remove sample shared code tokenization word vec generation remove empty row cause serious problem yes kind problem
How can I fine tune sentence transfomer without any labels?,"<p>I only have product descriptions and nothing else. I need to match similar products using cosine similarity. I have achieved this by taking embeddings from the Sentence Transformer. However, I need to fine-tune the Sentence Transformer on my product description data to obtain better embeddings. Can someone please explain how I can perform this task?</p>
<p>I have attempted to use the Sentence Transformer, but I need to fine-tune it on unlabelled data to obtain improved embeddings.</p>
",Vectorization & Embeddings,fine tune sentence transfomer without label product description nothing else need match similar product using cosine similarity achieved taking embeddings sentence transformer however need fine tune sentence transformer product description data obtain better embeddings someone please explain perform task attempted use sentence transformer need fine tune unlabelled data obtain improved embeddings
String comparison with BERT seems to ignore &quot;not&quot; in sentence,"<p>I implemented a string comparison method using SentenceTransformers and BERT like following</p>
<pre><code>from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')

sentences = [
    &quot;I'm a good person&quot;,
    &quot;I'm not a good person&quot;
]

sentence_embeddings = model.encode(sentences)

cosine_similarity(
    [sentence_embeddings[0]],
    sentence_embeddings[1:]
)
</code></pre>
<p>Notice how my sentence examples are very similar but with the opposite meaning. The problem is the cosine similarity returns 0.9, indicating that these two strings are very similar in context when I expected it to return something closer to zero, as they have the opposite meanings.</p>
<p>How can I adapt my code to return a more accurate result?</p>
",Vectorization & Embeddings,string comparison bert seems ignore sentence implemented string comparison method using sentencetransformers bert like following notice sentence example similar opposite meaning problem cosine similarity return indicating two string similar context expected return something closer zero opposite meaning adapt code return accurate result
"Will Word2Vec be more efficient in text based Plagiarism detection than WordNet or any other word embeddings like GloVe, fastText etc?","<p>I am a beginner in learning Word2Vec and just started to do some study on Word2vec from the Internet. I have gone through almost all the questions in Quora and StackOverflow but didn't get my answer anywhere from the previous questions. So my question is-</p>
<ol>
<li>Is it possible to apply word2vec in plagiarism detection?</li>
<li>If yes, then will Word2Vec be more efficient in text-based Plagiarism detection than WordNet or any other word embeddings like GloVe, fastText, etc?</li>
</ol>
<p>Thanks in advance.</p>
",Vectorization & Embeddings,word vec efficient text based plagiarism detection wordnet word embeddings like glove fasttext etc beginner learning word vec started study word vec internet gone almost question quora stackoverflow get answer anywhere previous question question possible apply word vec plagiarism detection yes word vec efficient text based plagiarism detection wordnet word embeddings like glove fasttext etc thanks advance
How can I apply a Word2Vec model to a column of a Pandas dataframe?,"<p>I want to apply a Word2Vec model to a pandas dataframe column and stores the embeddings to a new column:</p>
<pre><code>model = Word2Vec(df['tweet_text'], sg=1, window=10, min_count=1, workers=4) 
df['embeddings'] = df['tweet_text'].apply(model.wv) #error in this line
df[['embeddings']].head(5)
</code></pre>
<p>It gives the following error:</p>
<pre><code>TypeError: 'KeyedVectors' object is not callable
</code></pre>
",Vectorization & Embeddings,apply word vec model column panda dataframe want apply word vec model panda dataframe column store embeddings new column give following error
Interpreting negative Word2Vec similarity from gensim,"<p>E.g. we train a word2vec model using <code>gensim</code>:</p>

<pre><code>from gensim import corpora, models, similarities
from gensim.models.word2vec import Word2Vec

documents = [""Human machine interface for lab abc computer applications"",
              ""A survey of user opinion of computer system response time"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey""]

texts = [[word for word in document.lower().split()] for document in documents]
w2v_model = Word2Vec(texts, size=500, window=5, min_count=1)
</code></pre>

<p>And when we query the similarity between words, we find negative similarity scores:</p>

<pre><code>&gt;&gt;&gt; w2v_model.similarity('graph', 'computer')
0.046929569156789336
&gt;&gt;&gt; w2v_model.similarity('graph', 'system')
0.063683518562347399
&gt;&gt;&gt; w2v_model.similarity('survey', 'generation')
-0.040026775040430063
&gt;&gt;&gt; w2v_model.similarity('graph', 'trees')
-0.0072684112978664561
</code></pre>

<p><strong>How do we interpret the negative scores?</strong> </p>

<p>If it's a cosine similarity shouldn't the range be <code>[0,1]</code>?</p>

<p><strong>What is the upper bound and lower bound of the <code>Word2Vec.similarity(x,y)</code> function?</strong> There isn't much written in the docs: <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.similarity"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.similarity</a> =(</p>

<p>Looking at the Python wrapper code, there isn't much too: <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L1165"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L1165</a></p>

<p>(If possible, please do point me to the <code>.pyx</code> code of where the similarity function is implemented.)</p>
",Vectorization & Embeddings,interpreting negative word vec similarity gensim e g train word vec model using query similarity word find negative similarity score interpret negative score cosine similarity range upper bound lower bound function much written doc looking python wrapper code much possible please point code similarity function implemented
Find and compare all methods recursively,"<p>This python code is working correctly and it returns the similarity score of 91%</p>
<pre><code>import textdistance

string_a = 'Hello World!'
string_b = 'Hello Word!'

algs = textdistance.algorithms
algs.levenshtein.normalized_similarity(string_a, string_b)
</code></pre>
<p>But Jaro inkler score is a lot higher at 98% in this use case.</p>
<pre><code>algs.jaro_winkler.normalized_similarity(string_a, string_b)
</code></pre>
<p>How do I find if there is any other method that will return a score of more than 98%?</p>
",Vectorization & Embeddings,find compare method recursively python code working correctly return similarity score jaro inkler score lot higher use case find method return score
How to encode the shortest dependency path between words in sentence in neural network?,"<p>I am doing a first project on relation extraction between two entities in sentences in NLP. I use an LSTM model, and in addition to inputting the word embedding, I want to input the shortest dependency path between these 2 entities</p>
<p>for example:
<em>thousands of <strong>people</strong> are flocking towards the <strong>center,</strong></em>
given the 2 entities are 'people' and 'center', the path is people -&gt; of -&gt; thousands -&gt; flocking &lt;- towards &lt;- center.</p>
<p>So how can I encode this features to make the model can learn this knowledge</p>
<p>Specially thanks!</p>
<p>i have tried creating an array to store the index of words on that shortest path, words that appear will get a None value.
For, example, in the above example, ”people”, ”of”, ”thousands”, ”flocking”, ”towards”,
”center” will hold respectively -3, -2, -1, 0, 1, 2. Using negative numbers and positive numbers to indicate 2 directions in the path. I feel this approach is not goog enough, and the model performance does not increase much</p>
",Vectorization & Embeddings,encode shortest dependency path word sentence neural network first project relation extraction two entity sentence nlp use lstm model addition inputting word embedding want input shortest dependency path entity example thousand people flocking towards center given entity people center path people thousand flocking towards center encode feature make model learn knowledge specially thanks tried creating array store index word shortest path word appear get none value example example people thousand flocking towards center hold respectively using negative number positive number indicate direction path feel approach goog enough model performance doe increase much
Measure similarity between text documents,"<p>I have 10000 questions framed in different ways, and I want to find the top 20 most frequently asked questions. What would be the best method to do so? So far I'm juggling with</p>
<ol>
<li>topic modelling like BERTopic or NMF or</li>
<li>finding text similarity with cosine similarity metrics or</li>
<li>using like k-means clustering or hierarchical clustering</li>
</ol>
<p>Since I'm new to this field, I might be missing something obvious (either a method or an existing software/package that deals with this specifically).</p>
<p>Any suggestions are highly appreciated 🙏</p>
",Vectorization & Embeddings,measure similarity text document question framed different way want find top frequently asked question would best method far juggling topic modelling like bertopic nmf finding text similarity cosine similarity metric using like k mean clustering hierarchical clustering since new field might missing something obvious either method existing software package deal specifically suggestion highly appreciated
What would be the best way to compare different parts of a document in just one doc2vec embedding?,"<p>Let's say I have many documents with a question and an answer. I want to build an embedding where I can find the most similar documents based on just a new question without an answer but still be able to find similar documents based on the whole document, meaning question and answer.</p>
<p>What would be the best way where I only need one embedding?</p>
<p>I thought of some possible approaches, but here I would need to have two different embeddings:</p>
<ol>
<li><p>Split all documents into questions and answers and build two different embeddings. One question- and one answer-embedding. Now, if I want to find the most similar doc for a question, I will just use the question-embedding. When I want to find the most similar doc based on a new doc I will split the new doc and find the most similar vectors in both embeddings and calculate something like an average(question_vec, answer_vec).</p>
</li>
<li><p>I create a question-only-embedding and a whole-doc-embedding. Here I can just use an embedding depending on the task.</p>
</li>
</ol>
",Vectorization & Embeddings,would best way compare different part document one doc vec embedding let say many document question answer want build embedding find similar document based new question without answer still able find similar document based whole document meaning question answer would best way need one embedding thought possible approach would need two different embeddings split document question answer build two different embeddings one question one answer embedding want find similar doc question use question embedding want find similar doc based new doc split new doc find similar vector embeddings calculate something like average question vec answer vec create question embedding whole doc embedding use embedding depending task
Vector based information retrieval on code resulting in high correlation values for all candidates,"<p>I am writing an algorithm that scans over a code base and uses the &quot;text-embedding-ada-002&quot; model from open ai, to turn each code line into a vector. Then I also use the same model to embed a natural language query e.g. &quot;Where is the search functionality housed ?&quot;. Then I simply run a linear search and compare the cosine similarity between the query vector and each code line vector. Then I take the topk results.</p>
<p>The issue:</p>
<p>I notice that seemingly all code lines have high cosine similarity scores (greater than .6). I tried extending the window in which i embed 1 -&gt; 10 lines at a time. But they all still seem high. Also it biases heavily towards natural language documents like the readme. Any ideas on how I can improve this ? Maybe some low hanging fruit that i'm missing ?</p>
",Vectorization & Embeddings,vector based information retrieval code resulting high correlation value candidate writing algorithm scan code base us text embedding ada model open ai turn code line vector also use model embed natural language query e g search functionality housed simply run linear search compare cosine similarity query vector code line vector take topk result issue notice seemingly code line high cosine similarity score greater tried extending window embed line time still seem high also bias heavily towards natural language document like readme idea improve maybe low hanging fruit missing
How to improve text similarity/classification performance when classes are semantically similar?,"<p>I have an NLP classification problem whereby I want to match an input string (a question) to the most suitable string from a list of reference strings (FAQs), or abstain if confidence in a classification is low.</p>
<p>I have an existing function that uses <code>distilbert-base-uncased</code> embeddings and cosine similarity, which performs OK. However, the similarity scores are typically high for all reference strings, which is a consequence of them all being semantically similar. The strings themselves are all on a particular topic (e.g., &quot;What is X?&quot;, &quot;How can I prevent X?&quot;, &quot;What are the symptoms of X?&quot;, &quot;How can I tell if X is happening?&quot;), so this isn't exactly surprising.</p>
<p>What techniques can I use to improve performance here? I do not have any training data, so fine-tuning is out. I can obviously try different language models and similarity measures, but it's difficult to determine whether this is going to have any noticeable impact.</p>
<p><strong>Are there any statistical or additional NLP techniques people can recommend for this problem?</strong></p>
<p>My existing function is as follows:</p>
<pre><code>
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity


def mapping(user_input: str, abstain_threshold: float,
            language_model='distilbert-base-uncased', faqs_file='faqs.txt'):

    # Load the pre-trained transformer model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained(language_model)
    model = AutoModel.from_pretrained(language_model)

    # Load the FAQ list
    with open(faqs_file, 'r') as f:
        faqs = [line.strip() for line in f]

    # Tokenize the user input and FAQs
    user_input_tokens = tokenizer.encode(user_input, add_special_tokens=True)
    faq_tokens = [tokenizer.encode(faq, add_special_tokens=True) for faq in faqs]

    # Pad the tokenized sequences to the same length
    max_len = max(len(tokens) for tokens in faq_tokens + [user_input_tokens])
    user_input_tokens = user_input_tokens + [0] * (max_len - len(user_input_tokens))
    faq_tokens = [tokens + [0] * (max_len - len(tokens)) for tokens in faq_tokens]

    # Convert the tokenized sequences to PyTorch tensors
    user_input_tensor = torch.tensor(user_input_tokens).unsqueeze(0)
    faq_tensors = [torch.tensor(tokens).unsqueeze(0) for tokens in faq_tokens]

    # Pass the user input and FAQs through the transformer model
    with torch.no_grad():
        user_input_embedding = model(user_input_tensor)[0][:, 0, :]
        faq_transformer_embeddings = [model(faq_tensor)[0][:, 0, :] for faq_tensor in faq_tensors]

    # use cosine similarity to get the best match
    faq_similarity_scores = []
    for faq_transformer_embedding in faq_transformer_embeddings:
        similarity = cosine_similarity(user_input_embedding, faq_transformer_embedding)
        print(similarity)
        faq_similarity_scores.append(similarity)

    # Find the most similar FAQ
    max_score_index = np.argmax(faq_similarity_scores)
    max_score = faq_similarity_scores[max_score_index]
    best_match = faqs[max_score_index]
    
    #  check if model abstains
    if max_score &gt;= abstain_threshold:
        return best_match
    else:
        return None


</code></pre>
",Vectorization & Embeddings,improve text similarity classification performance class semantically similar nlp classification problem whereby want match input string question suitable string list reference string faq abstain confidence classification low existing function us embeddings cosine similarity performs ok however similarity score typically high reference string consequence semantically similar string particular topic e g x prevent x symptom x tell x happening exactly surprising technique use improve performance training data fine tuning obviously try different language model similarity measure difficult determine whether going noticeable impact statistical additional nlp technique people recommend problem existing function follows
Semantic searching using Google flan-t5,"<p>I'm trying to use google flan t5-large to create embeddings for a simple semantic search engine. However, the generated embeddings cosine similarity with my query is very off. Is there something I'm doing wrong?</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoTokenizer, AutoModel
import torch
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import euclidean

tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-large')
model = AutoModel.from_pretrained('google/flan-t5-large')

# Set the text to encode

def emebddings_generate(text):
  all_embeddings = []
  for i in text:
    input_ids = tokenizer.encode(i, return_tensors='pt')
    with torch.no_grad():
      embeddings = model(input_ids, decoder_input_ids=input_ids).last_hidden_state.mean(dim=1)
      all_embeddings.append((embeddings,i))
  return all_embeddings

def run_query(query,corpus):
  input_ids = tokenizer.encode(query, return_tensors='pt')
  with torch.no_grad():
        quer_emebedding=model(input_ids,decoder_input_ids=input_ids).last_hidden_state.mean(dim=1)

  similairtiy = []

  for embeds in corpus:
    sim = euclidean(embeds[0].flatten(),quer_emebedding.flatten())
    similairtiy.append((embeds[1],float(sim)))
  return similairtiy


text = ['some sad song', ' a very happy song']
corpus = emebddings_generate(text)

query = &quot;I'm feeling so sad rn&quot;
similairtiy = run_query( query,corpus)
for i in similairtiy:
  print(i)
  print(i[1],i[0])

</code></pre>
<p>I've tried different pooling techniques as well as using other distance metrics.</p>
",Vectorization & Embeddings,semantic searching using google flan trying use google flan large create embeddings simple semantic search engine however generated embeddings cosine similarity query something wrong tried different pooling technique well using distance metric
Map (1:1) N input sentences to N given sentences by similarity,"<p>I want to map <code>N_a</code> input sentences to <code>N_b</code> given sentences so that the mapping is one-to-one. That is,</p>
<ul>
<li>Every <code>N_a</code> is assigned</li>
<li>No <code>N_a</code> appears more than once</li>
</ul>
<p>Unfortunately the inputs vary slightly over time. Here is a representative mapping:</p>
<pre><code>{ &quot;typeA&quot;:            &quot;step1 typeA (before typeB)&quot;
, &quot;typeD&quot;:            &quot;type D&quot;
, &quot;actionA&quot;:          &quot;actionA-suffix: typeB or Type D (type E available)&quot;
, &quot;typeE&quot;:            &quot;typeE - (not-actionA)&quot;
, &quot;actionB&quot;:          &quot;actionB some descriptive words&quot;
, &quot;typeA subtypeA&quot;:   &quot;subtypeA typeA or typeX - (not for typeB)&quot;
, &quot;actionA subtypeA&quot;: &quot;actionA-suffix: subtypeA (type E available)&quot;
, &quot;typeB subtypeA&quot;:   &quot;subtypeA typeB&quot;
, &quot;typeC subtypeA&quot;:   &quot;subtypeA typeC&quot;
, &quot;typeB&quot;:            &quot;typeB (not subtypeA)&quot;
, &quot;typeF&quot;:            &quot;get typeF or typeF-subtypeA&quot;
, &quot;typeF actionB&quot;:    &quot;actionB typeF or typeF subtypeA&quot;
}
</code></pre>
<p>Following [1], I've created this workflow using the <code>sentence_transformers</code> package[2]:</p>
<ul>
<li>BERT -&gt; mean-pool -&gt; cosine similarity</li>
</ul>
<p>Given the inputs it is clear that string-based alignment plus edit-distance (of the type featured in <code>rapidfuzz</code>[3]) won't work. But I'm not sure if BERT is the best approach, or if it is overkill for my needs. Perhaps I should use a word embedding model (word2vec, glove, etc) rather than a sentence embedding model. For this particular task I wonder if BERT could be tweaked to perform better.</p>
<p>The categories aren't well differentiated so that BERT sometimes maps an input to more than one given. An input <code>N_a</code> can be the best match for multiple givens <code>N_b0, ..., N_bi</code>. In such cases I keep the map with the best score and for the others fall upon the 2nd-best, 3rd-best, ... map. How can I improve BERT's performance to avoid these duplicates?</p>
<hr />
<p>Current implementation below:</p>
<pre><code>import pandas as pd
import sentence_transformers as st

model = st.SentenceTransformer('all-mpnet-base-v2')

sentences_input = [ ... ]
sentences_given = [ ... ]

# Create the encodings and apply mean pooling.
enc_input = model.encode(sentences_input)
enc_given = model.encode(sentences_given)

# Calculate a cosine similarity matrix
cos_similarity = st.util.cos_sim(enc_input, enc_given)

# As a pandas dataframe, label the matrix with the sentences.
df = pd.DataFrame\
  (columns=sentences_input, index=sentences_given, dtype=float)
for i, sgiven in enumerate(sentences_given):
  for j, sinput in enumerate(sentences_input):
    df.loc[sgiven, sinput] = cos_similarity[j,i].item()

# For each given sentence, extract the best-scoring input sentence. Which
# unfortunately is not a one-to-one mapping.
mapping_bad_duplicates = df.idxmax(axis=1)

# Create a one-to-one mapping by iterating over the matches in order of best
# score. For each map, blocklist the row and column sentences, preventing
# duplicates.
mapping_good = {}
by_scores = sorted(df.unstack().items(), key=lambda k: k[1], reverse=True)
sentences = set(sentences_input) | set(sentences_given)
for (sinput,sgiven), score in by_scores:
  if not sentences:
    break
  if sgiven not in sentences or sinput not in sentences:
    continue
  mapping_good[sgiven] = sinput
  sentences.remove(sgiven)
  sentences.remove(sinput)

# Convert the result to a dataframe
mapping_good_df = pd.Series(mapping_good)
</code></pre>
<ol>
<li><a href=""https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1"" rel=""nofollow noreferrer"">BERT For Measuring Text Similarity</a></li>
<li><a href=""https://www.sbert.net/docs/usage/semantic_textual_similarity.html"" rel=""nofollow noreferrer"">SBERT: Semantic Textual Similarity</a></li>
<li><a href=""https://maxbachmann.github.io/RapidFuzz/Usage/fuzz.html#rapidfuzz.fuzz.partial_ratio"" rel=""nofollow noreferrer"">rapidfuzz</a></li>
</ol>
",Vectorization & Embeddings,map n input sentence n given sentence similarity want map input sentence given sentence mapping one one every assigned appears unfortunately input vary slightly time representative mapping following created workflow using package bert mean pool cosine similarity given input clear string based alignment plus edit distance type work sure bert best approach overkill need perhaps use word embedding model word vec glove etc rather sentence embedding model particular task wonder bert could perform better category well differentiated bert sometimes map input one given input best match multiple given case keep map best score others fall upon nd best rd best map improve bert performance avoid duplicate current implementation bert measuring text similarity sbert semantic textual similarity rapidfuzz
Attention heads &quot;competence&quot; in NLP transformer networks,"<p>I am currently writing my masters' thesis on transformers in NLP. I have been reading a lot and have been wondering about one fact for a while. In transformers, we have self-attention and attention heads. Say I have word embeddings of 512 dimensions and 8 heads, then every head will deal with 64 dimensions of every input word to calculate self-attention.</p>
<p>Here is a picture from a textbook that might illustrate what I mean:</p>
<p><a href=""https://i.sstatic.net/DWPHK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DWPHK.png"" alt=""enter image description here"" /></a></p>
<p>Here is my question now. Could we say that every attention-head has a sort of &quot;competence&quot; for a part-meaning of the words? Say the first 64 dimensions of a word embeddings always deal with the words' emotionality; would the first head then be the emotionality-head? And what would that mean for interpretability and the learning in the network?</p>
<p>This is my first question here; I hope to have expressed myself clear enough.</p>
<p>Thank you for any answers!</p>
",Vectorization & Embeddings,attention head competence nlp transformer network currently writing master thesis transformer nlp reading lot wondering one fact transformer self attention attention head say word embeddings dimension head every head deal dimension every input word calculate self attention picture textbook might illustrate mean question could say every attention head ha sort competence part meaning word say first dimension word embeddings always deal word emotionality would first head emotionality head would mean interpretability learning network first question hope expressed clear enough thank answer
How to get TF-IDF scores for the words?,"<p>I have a large corpus (around 400k unique sentences). I just want to get TF-IDF score for each word. I tried to calculate the score for each word by scanning each word and calculating the frequency but it's taking too long.</p>

<p>I used :</p>

<pre><code>  X= tfidfVectorizer(corpus)
</code></pre>

<p>from sklearn but it directly gives back the vector representation of the sentence. Is there any way I can get the TF-IDF scores for each word in the corpus?</p>
",Vectorization & Embeddings,get tf idf score word large corpus around k unique sentence want get tf idf score word tried calculate score word scanning word calculating frequency taking long used sklearn directly give back vector representation sentence way get tf idf score word corpus
Layernorm in PyTorch,"<p>Consider the following example:</p>
<pre><code>    batch, sentence_length, embedding_dim = 2, 3, 4
    embedding = torch.randn(batch, sentence_length, embedding_dim)
    print(embedding)
    
# Output:
    tensor([[[-2.1918,  1.2574, -0.3838,  1.3870],
             [-0.4043,  1.2972, -1.7326,  0.4047],
             [ 0.4560,  0.6482,  1.0858,  2.2086]],
    
            [[-1.4964,  0.3722, -0.7766,  0.3062],
             [ 0.9812,  0.1709, -0.9177, -1.2558],
             [-1.1560, -0.0367,  0.5496, -1.1142]]])
</code></pre>
<p>Applying the Layernorm which normalized across the embedding dimension, I get:</p>
<pre><code>layer_norm = torch.nn.LayerNorm(embedding_dim)
layer_norm(embedding)

# Output:
tensor([[[-1.5194,  0.8530, -0.2758,  0.9422],
         [-0.2653,  1.2620, -1.4576,  0.4609],
         [-0.9470, -0.6641, -0.0204,  1.6315]],

        [[-1.4058,  0.9872, -0.4840,  0.9026],
         [ 1.3933,  0.4803, -0.7463, -1.1273],
         [-0.9869,  0.5545,  1.3619, -0.9294]]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;)
</code></pre>
<p>Now, when I normalize the first vector of above embedding tensor with a naive python implementation, I get:</p>
<pre><code>    a = [-2.1918,  1.2574, -0.3838,  1.3870]
    mean_a = statistics.mean(a)
    var_a = statistics.stdev(a)
    eps = 1e-5
    d = [ ((i-mean_a)/math.sqrt(var_a + eps)) for i in a]
    print(d)
    
    #Output:
[-1.7048934056508998,0.9571791768620398,-0.3094894774404756,1.0572037062293356]
</code></pre>
<p>The normalized values are not the same as what I get from PyTorch's Layernorm. Is there something wrong with the way I calculated Layernorm?</p>
",Vectorization & Embeddings,layernorm pytorch consider following example applying layernorm normalized across embedding dimension get normalize first vector embedding tensor naive python implementation get normalized value get pytorch layernorm something wrong way calculated layernorm
how to crawl semantically similar sentences,"<p>I want to create a corpus for a machine learning task. I have a small textual dataset and want to crawl similar sentences from web. I used sentence_transformers package with Bert pertained model, doc2vec and spacy similarity to measure similarity. I set the threshold to 85%, but the sentences with the similarity score higher than the threshold weren't really relevant. how can I crawl similar sentences from web in python?</p>
",Vectorization & Embeddings,crawl semantically similar sentence want create corpus machine learning task small textual dataset want crawl similar sentence web used sentence transformer package bert pertained model doc vec spacy similarity measure similarity set threshold sentence similarity score higher threshold really relevant crawl similar sentence web python
Build doc2vec model and find textually similar reviews using gensim,"<p>The dataset is Amazon's review dataset in gz file.</p>
<pre><code># A function to read the zipped data at a specfic path
#
# How to use:
# PATH = &quot;/path/to/file&quot;
# for line in parse(PATH):
#   do something with line
#
def parse(path):
    g = gzip.open(path, 'r')
    for l in g:
        yield eval(l)
</code></pre>
<p>The following code is given.</p>
<pre><code>import os
def read_reviewers_data(fname, min_count=0):
    '''
    Save all reviews into their own product asin files.
    Make sure you have 'product' folder when you run this answer.
    In each file, you can choose your own log structure. In this answer, log 
    strucutre is like 
        &quot;reviewText&quot;\t&quot;reviewerID&quot;\t&quot;helpful&quot;
    Args: 
        fname: dataset file path
        min_count: minimum number of reviews of a product
    Returns:
        none
    '''
    if not os.path.isdir('product'):
        os.makedirs('product')
    asin_list = []
    tmp_list = []
    last_asin = &quot;&quot;
    j = 0
    for i in parse(fname):
        if last_asin != i['asin']:
            if len(tmp_list) &gt; min_count:
                f = open(&quot;product/&quot; + last_asin+&quot;.txt&quot;, 'w')
                for one in tmp_list:
                    f.write(one)
                f.close()
            tmp_list = []
            last_asin = i['asin']
        tmp_list.append(i[&quot;reviewText&quot;] + '\t' + i[&quot;reviewerID&quot;] +
                    '\t' + handle_helpful(i[&quot;helpful&quot;]) + &quot;\n&quot;)
        j += 1
        if j &gt; 100000:
            break
            
def handle_helpful(helpful):
    '''
    Helper function for helpful_score calculate
    Args: 
        helpful: list. The first element is the number of people think this is helpful. The second element
            is the total number of people evaluate this comment
    Returns:
        String: number represent helpfulness
    '''
    if helpful[1] != 0:
        helpfulness = 1.0 * helpful[0] / helpful[1]
        return str(helpfulness)
    else:
        return str(0)
</code></pre>
<p>From my understanding, the above code creates a 'product' folder with txt files named after the asin code. For each txt it stores the review,the reviewer_ID and their helpfulness score.</p>
<pre><code>read_reviewers_data(&quot;reviews_Electronics_5.json.gz&quot;)
</code></pre>
<pre><code>class TaggedReviewDocument(object):
    '''
    This class could save all products and review information in its dictionary and generate iter for TaggedDocument
        which could used for Doc2Vec model
    '''
    def __init__(self, dirname):
        self.dirname = dirname
        self.helpfulness = {}  # key:reviewerID value:helpfulness
        self.product = {}      # key:asin value:reviewerID
        self.asin = []

    def __iter__(self):
        for filename in os.listdir(self.dirname):
            asin_code = filename[:-4] #delete &quot;.txt&quot;
            self.product[asin_code] = []
            self.asin.append(asin_code)
            for line in enumerate(open(self.dirname + &quot;/&quot; + filename)):
                line_content = line[1].split(&quot;\t&quot;)
                self.product[asin_code].append(line_content[1])
                self.helpfulness[line_content[1]] = float(line_content[2])
                yield TaggedDocument(clean_line(line_content[0]), [line_content[1], line_content[2]])
</code></pre>
<pre><code>documents = TaggedReviewDocument(&quot;product&quot;)
</code></pre>
<p>I'm a bit unsure of what does &quot;TaggedReviewDocument&quot; function do here.It seems to create a document with dictionary and key value.
The first task is to create a doc2Vec model. My code goes:</p>
<pre><code>from gensim.models.doc2vec import TaggedDocument, Doc2Vec

model_v = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)

</code></pre>
<p>The second task is to find top 5 helpful reviews of the same product with similarity score above 0.8.I'm thinking similarity score is based on similarity of reviews(text) and then rank them by helpfulness score. However,I had completely no idea on how to call the most_similar argument in gensim.model using the above document.I had no idea on how to extract the text review from documents either.</p>
<pre><code>def find_similar_reviews(asin,reviewer_id):
    '''
    If one review is similar to the specefic review and it is much helpful, save it to a list
    Args: 
        asin: product asin
        reviewer_id: the specific review
    Returns:
        list of reviewer id
        
    '''
    result = []
    #
    
    return result
</code></pre>
<p>I don't know which part of the document stores the text review.Any help will be appreciated.</p>
<p>And whatever I tried to output,it shows nothing in documents:</p>
<pre><code>documents.product
</code></pre>
<p>output: {}</p>
<pre><code>documents.helpfulness
</code></pre>
<p>output: {}</p>
<p>The dataset is taken from here:
<a href=""http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/"" rel=""nofollow noreferrer"">http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/</a></p>
<p>Named: 'reviews_Electronics_5.json.gz'</p>
",Vectorization & Embeddings,build doc vec model find textually similar review using gensim dataset amazon review dataset gz file following code given understanding code creates product folder txt file named asin code txt store review reviewer id helpfulness score bit unsure doe taggedreviewdocument function seems create document dictionary key value first task create doc vec model code go second task find top helpful review product similarity score thinking similarity score based similarity review text rank helpfulness score however completely idea call similar argument gensim model using document idea extract text review document either know part document store text review help appreciated whatever tried output show nothing document output output dataset taken named review electronics json gz
Compare each string with all other strings in a dataframe,"<p>I have this dataframe:</p>
<pre><code>mylist = [
    &quot;₹67.00 to Rupam Sweets using Bank Account XXXXXXXX5343&lt;br&gt;11 Feb 2023, 20:42:25&quot;,
    &quot;₹66.00 to Rupam Sweets using Bank Account XXXXXXXX5343&lt;br&gt;10 Feb 2023, 21:09:23&quot;,
    &quot;₹32.00 to Nagori Sajjad Mohammed Sayyed using Bank Account XXXXXXXX5343&lt;br&gt;9 Feb 2023, 07:06:52&quot;,
    &quot;₹110.00 to Vikram Manohar Jsohi using Bank Account XXXXXXXX5343&lt;br&gt;9 Feb 2023, 06:40:08&quot;,
    &quot;₹120.00 to Winner Dinesh Gupta using Bank Account XXXXXXXX5343&lt;br&gt;30 Jan 2023, 06:23:55&quot;,
]
import pandas as pd

df = pd.DataFrame(mylist)
df.columns = [&quot;full_text&quot;]
ndf = df.full_text.str.split(&quot;to&quot;, expand=True)
ndf.columns = [&quot;amt&quot;, &quot;full_text&quot;]
ndf2 = ndf.full_text.str.split(&quot;using Bank Account XXXXXXXX5343&lt;br&gt;&quot;, expand=True)
ndf2.columns = [&quot;client&quot;, &quot;date&quot;]
df = ndf.join(ndf2)[[&quot;date&quot;, &quot;client&quot;, &quot;amt&quot;]]
</code></pre>
<p>I have created embeddings for each client name:</p>
<pre><code>from openai.embeddings_utils import get_embedding, cosine_similarity
import openai

openai.api_key = 'xxx'
embedding_model = &quot;text-embedding-ada-002&quot;
embeddings = df.client.apply([lambda x: get_embedding(x, engine=embedding_model)])
df[&quot;embeddings&quot;] = embeddings
</code></pre>
<p>I can now calculate the similarity index for a given string. For e.g. &quot;Rupam Sweet&quot; using:</p>
<pre><code>query_embedding = get_embedding(&quot;Rupam Sweet&quot;, engine=&quot;text-embedding-ada-002&quot;)
df[&quot;similarity&quot;] = df.embeddings.apply(lambda x: cosine_similarity(x, query_embedding))
</code></pre>
<p>But I need the similarity score of each client across all other clients. In other words, the client names will be in rows as well as in columns and the score will be the data. How do I achieve this?</p>
",Vectorization & Embeddings,compare string string dataframe dataframe created embeddings client name calculate similarity index given string e g rupam sweet using need similarity score client across client word client name row well column score data achieve
How can I iterate through a doc2vec model?,"<p>I have built a Doc2Vec model and am trying to get the vectors of all my testing set (176 points). The code below I can only see one vector at a time. I want to be able to do &quot;clean_corpus[404:]&quot; to get the entire data set but when I try that it still outputs one vector.</p>
<pre><code>model.save(&quot;d2v.model&quot;)
print(&quot;Model Saved&quot;)


from gensim.models.doc2vec import Doc2Vec

model= Doc2Vec.load(&quot;d2v.model&quot;)
#to find the vector of a document which is not in training data
test_data = clean_corpus[404]
v1 = model.infer_vector(test_data)
print(&quot;V1_infer&quot;, v1)
</code></pre>
<p>Is there a way to easily iterate over the model to get and save all 176 vectors?</p>
",Vectorization & Embeddings,iterate doc vec model built doc vec model trying get vector testing set point code see one vector time want able clean corpus get entire data set try still output one vector way easily iterate model get save vector
Spacy Entity Linker with Transformer Listener problem,"<p>I have a pretrained pipeline composed by a transformer and NER components and I am trying to create an Entity Linker able to use embedding representation produced by the transformer rather than using a CNN.
I am adding the Linker as</p>
<pre class=""lang-py prettyprint-override""><code>    entity_linker = nlp.add_pipe(&quot;entity_linker&quot;,config = {
    &quot;labels_discard&quot;: [],
    &quot;n_sents&quot;: 0,
    &quot;incl_prior&quot;: True,
    &quot;incl_context&quot;: True,
    &quot;model&quot;: {
    &quot;@architectures&quot;: &quot;spacy.EntityLinker.v2&quot;,
    &quot;tok2vec&quot;: {
    &quot;@architectures&quot; : &quot;spacy-transformers.TransformerListener.v1&quot;,
    &quot;pooling&quot; : {&quot;@layers&quot;:&quot;reduce_mean.v1&quot;},
    &quot;upstream&quot; : &quot;*&quot;
    },
    &quot;nO&quot;: 768
    },
    &quot;entity_vector_length&quot;: 768,
    &quot;get_candidates&quot;: {'@misc': 'spacy.CandidateGenerator.v1'},
    &quot;threshold&quot;: None,
    })
</code></pre>
<p>but I receive dimension unset Error</p>
<pre class=""lang-py prettyprint-override""><code>File &quot;/home/blanco/wiki_kb/.venv/lib/python3.9/site-packages/spacy_transformers/layers/listener.py&quot;, line 64, in forward
width = model.get_dim(&quot;nO&quot;)
File &quot;/home/blanco/wiki_kb/.venv/lib/python3.9/site-packages/thinc/model.py&quot;, line 175, in get_dim
raise ValueError(err)ValueError: Cannot get dimension 'nO' for model 'transformer-listener': value unset
</code></pre>
<p>Looking online I found that this last problem typically occurs when you try to finetune a late component in the pipeline while freezing the transformer but in my case I am no quite close to that point.
My final aim would be to have a Linker that uses transformer embedding for entity and context and (maybe learning a linear layer for reprojection to a lowest dimension or in case no train at all) compares embedding coming from a knowledge base whose embedding representations have been created using the same pretrained pipeline.</p>
<p>Thanks in advance to everybody who will reply and help me</p>
<p>Current Environment</p>
<ul>
<li>Python 3.9.13</li>
<li>spacy 3.4.4</li>
<li>spacy-alignments 0.9.0</li>
<li>spacy-legacy 3.0.12</li>
<li>spacy-loggers 1.0.4</li>
</ul>
",Vectorization & Embeddings,spacy entity linker transformer listener problem pretrained pipeline composed transformer ner component trying create entity linker able use embedding representation produced transformer rather using cnn adding linker receive dimension unset error looking online found last problem typically occurs try finetune late component pipeline freezing transformer case quite close point final aim would linker us transformer embedding entity context maybe learning linear layer reprojection lowest dimension case train compare embedding coming knowledge base whose embedding representation created using pretrained pipeline thanks advance everybody reply help current environment python spacy spacy alignment spacy legacy spacy logger
How Seq2Seq Context Vector is generated?,"<p>I have studied the theory of seq2seq model but I couldn't clearly understand that what exactly is context vector and how is it generated. I know it summarizes the meaning of to-be-encoded sequence into it but how exactly?</p>
<p>In attention mechanism it was ; ci = Σ( αij hj ) [according to Dzmitry Bahdanau 2014]</p>
<p>But in normal seq2seq, I couldn't find a formula for context vector in Ilya Sutskever 2014 and on internet, There is only given formula of conditional probability as (y1,y2,...,yt|x1,x2,..,xt).</p>
<p>I am also confused that is classic seq2seq context vector of a sentence is same as average word2vec?</p>
<p>In Short, I am expecting a clear working of how context vector is created and what does it presents and how. Furthermore, how decoder extracts information from it.</p>
<p>I am expecting a clear working of how context vector is created and what does it presents and how. Furthermore, how decoder extracts information from it.</p>
",Vectorization & Embeddings,seq seq context vector generated studied theory seq seq model clearly understand exactly context vector generated know summarizes meaning encoded sequence exactly attention mechanism wa ci ij hj according dzmitry bahdanau normal seq seq find formula context vector ilya sutskever internet given formula conditional probability yt x x xt also confused classic seq seq context vector sentence average word vec short expecting clear working context vector created doe present furthermore decoder extract information expecting clear working context vector created doe present furthermore decoder extract information
Can stop phrases be removed while doing text processing in python?,"<p>On the task that I'm working on, involves finding the cosine similarity using tfidf between a base transcript and other sample transcripts.</p>
<p>I am removing stop words for this. But I would also like to remove certain stop phrases that are unique to the sample transcripts.</p>
<p>For example - I would like to retain words like 'sounds' , 'like'. But want to remove the phrase 'sounds like' when it occurs together.</p>
<p>I am using sklearn tfidfvectorizer package currently. Is there an efficient way to do the above?</p>
",Vectorization & Embeddings,stop phrase removed text processing python task working involves finding cosine similarity using tfidf base transcript sample transcript removing stop word would also like remove certain stop phrase unique sample transcript example would like retain word like sound like want remove phrase sound like occurs together using sklearn tfidfvectorizer package currently efficient way
pytorch embedding index out of range,"<p>I'm following this tutorial here <a href=""https://cs230-stanford.github.io/pytorch-nlp.html"" rel=""noreferrer"">https://cs230-stanford.github.io/pytorch-nlp.html</a>. In there a neural model is created, using <code>nn.Module</code>, with an embedding layer, which is initialized here</p>

<pre><code>self.embedding = nn.Embedding(params['vocab_size'], params['embedding_dim'])
</code></pre>

<p><code>vocab_size</code> is the total number of training samples, which is 4000.  <code>embedding_dim</code> is 50.  The relevant piece of the <code>forward</code> method is below</p>

<pre><code>def forward(self, s):
        # apply the embedding layer that maps each token to its embedding
        s = self.embedding(s)   # dim: batch_size x batch_max_len x embedding_dim
</code></pre>

<p>I get this exception when passing a batch to the model like so
<code>model(train_batch)</code>
<code>train_batch</code> is a numpy array of dimension <code>batch_size</code>x<code>batch_max_len</code>.  Each sample is a sentence, and each sentence is padded so that it has the length of the longest sentence in the batch.</p>

<blockquote>
  <p>File
  ""/Users/liam_adams/Documents/cs512/research_project/custom/model.py"",
  line 34, in forward
      s = self.embedding(s)   # dim: batch_size x batch_max_len x embedding_dim   File
  ""/Users/liam_adams/Documents/cs512/venv_research/lib/python3.7/site-packages/torch/nn/modules/module.py"",
  line 493, in <strong>call</strong>
      result = self.forward(*input, **kwargs)   File ""/Users/liam_adams/Documents/cs512/venv_research/lib/python3.7/site-packages/torch/nn/modules/sparse.py"",
  line 117, in forward
      self.norm_type, self.scale_grad_by_freq, self.sparse)   File ""/Users/liam_adams/Documents/cs512/venv_research/lib/python3.7/site-packages/torch/nn/functional.py"",
  line 1506, in embedding
      return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) RuntimeError: index out of range at
  ../aten/src/TH/generic/THTensorEvenMoreMath.cpp:193</p>
</blockquote>

<p>Is the problem here that the embedding is initialized with different dimensions than those of my batch array?  My <code>batch_size</code> will be constant but <code>batch_max_len</code> will change with every batch. This is how its done in the tutorial.</p>
",Vectorization & Embeddings,pytorch embedding index range following tutorial neural model created using embedding layer initialized total number training sample relevant piece method get exception passing batch model like numpy array dimension x sample sentence sentence padded ha length longest sentence batch file user liam adam document c research project custom model py line forward self embedding dim batch size x batch max len x embedding dim file user liam adam document c venv research lib python site package torch nn module module py line call result self forward input kwargs file user liam adam document c venv research lib python site package torch nn module sparse py line forward self norm type self scale grad freq self sparse file user liam adam document c venv research lib python site package torch nn functional py line embedding return torch embedding weight input padding idx scale grad freq sparse runtimeerror index range aten src th generic thtensorevenmoremath cpp problem embedding initialized different dimension batch array constant change every batch done tutorial
What does the embedding elements stand for in huggingFace bert model?,"<p>Prior to passing my tokens through encoder in BERT model, I would like to perform some processing on their embeddings. I extracted the embedding weight using:</p>
<pre><code>from transformers import TFBertModel

# Load a pre-trained BERT model
model = TFBertModel.from_pretrained('bert-base-uncased')

# Get the embedding layer of the model
embedding_layer = model.get_layer('bert').get_input_embeddings()

# Extract the embedding weights
embedding_weights = embedding_layer.get_weights()
</code></pre>
<p>I found it contains 5 elements as shown in Figure.
<a href=""https://i.sstatic.net/sSCfy.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>In my understanding, the first three elements are the word embedding weights, token type embedding weights, and positional embedding weights. My question is what does the last two elements stand for?</p>
<p>I dive deep into the source code of bert model. But I cannot figure out the meaning of the last two elements.</p>
",Vectorization & Embeddings,doe embedding element stand huggingface bert model prior passing token encoder bert model would like perform processing embeddings extracted embedding weight using found contains element shown figure enter image description understanding first three element word embedding weight token type embedding weight positional embedding weight question doe last two element stand dive deep source code bert model figure meaning last two element
How to find Sentence Transformer support languages?,"<p>I want to get the sentence embedding results to find the sentence similarities in my NLP project. Since I am working with a low-resource language (Sinhala), I want to know whether any sentence_transformer <a href=""https://www.sbert.net/docs/pretrained_models.html"" rel=""nofollow noreferrer"">model</a> supports my low-resource language. However, I was unable to find the pre-trained languages of those models. So How can I find that?
If those models are not trained with this language, How can I implement a sentence embedding model?</p>
",Vectorization & Embeddings,find sentence transformer support language want get sentence embedding result find sentence similarity nlp project since working low resource language sinhala want know whether sentence transformer model support low resource language however wa unable find pre trained language model find model trained language implement sentence embedding model
How to save an embedding layer which was created outside model?,"<p>I created a word embedding layer outside model and used it as input before fitting my model. Now I need to predict new sentences by this model, how can I save the pre-trained embedding layer and apply it to my new sentences?</p>
<p>Code example:</p>
<pre><code>Before input to model and fitting:
embedding_sentence = tf.keras.layers.Embedding(vocab_size, model_dimension, trainable=True)
embedded_sentence = embedding_sentence(vectorized_sentence)

Model fitting:
model = tf.keras.Sequential()
model.add(tf.keras.layers.GlobalAveragePooling1D())
...
</code></pre>
<p>Now I need to predict new sentences, how can I apply the trained embedding to them?</p>
",Vectorization & Embeddings,save embedding layer wa created outside model created word embedding layer outside model used input fitting model need predict new sentence model save pre trained embedding layer apply new sentence code example need predict new sentence apply trained embedding
Apply `torch.nn.MultiheadAttention`’s heads to same input,"<p>My question surely has a simple answer, but I couldn't find it. I wish to apply <code>MultiheadAttention</code> to the same sequence without copying the sequence. My data is temporal data with dimensions (batch, time, channels). I treat the &quot;channels&quot; dimension as the embedding, and the time dimension as the sequence dimension. For example:</p>
<pre class=""lang-py prettyprint-override""><code>N, C, T = 2, 3, 5
n_heads = 7
X = torch.rand(N, T, C)
</code></pre>
<p>Now, I want to apply 7 different heads as self-attention to the same input <code>X</code>, but as far as I understand, it attrequires me to copy the data 7 times:</p>
<pre class=""lang-py prettyprint-override""><code>attn = torch.nn.MultiheadAttention(C * n_heads, n_heads, batch_first=True)
X_ = X.repeat(1, 1, n_heads)
attn(X_, X_, X_)
</code></pre>
<p>Is there any way to do this without copying the data 7 times?
Thanks!</p>
",Vectorization & Embeddings,apply head input question surely ha simple answer find wish apply sequence without copying sequence data temporal data dimension batch time channel treat channel dimension embedding time dimension sequence dimension example want apply different head self attention input far understand attrequires copy data time way without copying data time thanks
How to compare sentence similarities using embeddings from BERT,"<p>I am using the HuggingFace Transformers package to access pretrained models. As my use case needs functionality for both English and Arabic, I am using the <a href=""https://github.com/google-research/bert/blob/master/multilingual.md"" rel=""noreferrer"">bert-base-multilingual-cased</a> pretrained model. I need to be able to compare the similarity of sentences using something such as cosine similarity. To use  this, I first need to get an embedding vector for each sentence, and can then compute the cosine similarity.</p>

<p>Firstly, what is the best way to extratc the semantic embedding from the BERT model? Would taking the last hidden state of the model after being fed the sentence suffice?</p>

<pre><code>import torch
from transformers import BertModel, BertTokenizer

model_class = BertModel
tokenizer_class = BertTokenizer
pretrained_weights = 'bert-base-multilingual-cased'

tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)

sentence = 'this is a test sentence'

input_ids = torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)])
with torch.no_grad():
    output_tuple = model(input_ids)
    last_hidden_states = output_tuple[0]

print(last_hidden_states.size(), last_hidden_states)
</code></pre>

<p>Secondly, if this is a sufficient way to get embeddings from my sentence, I now have another problem where the embedding vectors have different lengths depending on the length of the original sentence. The shapes output are <code>[1, n, vocab_size]</code>, where <code>n</code> can have any value. </p>

<p>In order to compute two vectors' cosine similarity, they need to be the same  length. How can I do this here? Could something as naive as first summing across <code>axis=1</code> still work? What other options do I have? </p>
",Vectorization & Embeddings,compare sentence similarity using embeddings bert using huggingface transformer package access pretrained model use case need functionality english arabic using bert base multilingual cased pretrained model need able compare similarity sentence using something cosine similarity use first need get embedding vector sentence compute cosine similarity firstly best way extratc semantic embedding bert model would taking last hidden state model fed sentence suffice secondly sufficient way get embeddings sentence another problem embedding vector different length depending length original sentence shape output value order compute two vector cosine similarity need length could something naive first summing across still work option
Latest Pre-trained Multilingual Word Embedding,"<p>Are there any latest <strong>pre-trained multilingual word embeddings</strong> (multiple languages are jointly mapped to a same vector space)?</p>

<p>I have looked at the following but they don't fit my needs:</p>

<ol>
<li>FastText / MUSE (<a href=""https://fasttext.cc/docs/en/aligned-vectors.html"" rel=""noreferrer"">https://fasttext.cc/docs/en/aligned-vectors.html</a>): this one seems too old, and the word vectors are not using subwords / wordpiece information.</li>
<li>LASER (<a href=""https://github.com/yannvgn/laserembeddings"" rel=""noreferrer"">https://github.com/yannvgn/laserembeddings</a>): I'm now using this one, it's using subword information (via BPE), however, it's suggested that not to use this for word embedding because it's designed to embed sentences (<a href=""https://github.com/facebookresearch/LASER/issues/69"" rel=""noreferrer"">https://github.com/facebookresearch/LASER/issues/69</a>).</li>
<li>BERT multilingual (bert-base-multilingual-uncased in <a href=""https://huggingface.co/transformers/pretrained_models.html"" rel=""noreferrer"">https://huggingface.co/transformers/pretrained_models.html</a>): it's contextualised embeddings that can be used to embed sentences, and seems not good at embedding words without contexts.</li>
</ol>

<p>Here is the problem I'm trying to solve:</p>

<p>I have a list of company names, which can be in any language (mainly English), and I have a list of keywords in English to measure how close a given company name is with regards to the keywords. Now I have a simple keyword matching solution, but I want to improve it using pretrained embeddings. As you can see in the following examples, there are several challenges:</p>

<ol>
<li>keyword and brand name is not separated by space (now I'm using package ""wordsegment"" to split words into subwords), so embedding with subword info should help a lot</li>
<li>keyword list is not extensive and company name could be in different languages (that's why I want to use embedding, because ""soccer"" is close to ""football"")</li>
</ol>

<p>Examples of company names: ""cheapfootball ltd."", ""wholesalefootball ltd."", ""footballer ltd."", ""soccershop ltd.""</p>

<p>Examples of keywords: ""football""</p>
",Vectorization & Embeddings,latest pre trained multilingual word embedding latest pre trained multilingual word embeddings multiple language jointly mapped vector space looked following fit need fasttext muse one seems old word vector using subwords wordpiece information laser using one using subword information via bpe however suggested use word embedding designed embed sentence bert multilingual bert base multilingual uncased contextualised embeddings used embed sentence seems good embedding word without context problem trying solve list company name language mainly english list keywords english measure close given company name regard keywords simple keyword matching solution want improve using pretrained embeddings see following example several challenge keyword brand name separated space using package wordsegment split word subwords embedding subword info help lot keyword list extensive company name could different language want use embedding soccer close football example company name cheapfootball ltd wholesalefootball ltd footballer ltd soccershop ltd example keywords football
"fine tuning word2vec on a specific article, using transfer learning","<p>i try to fine tune  an exicting model on specific article. I have tried transfer learning using genism build_vocab, adding gloveword2vec to a base model i trained on the article. but the build_vocab does not change the basic model- it is very small and no words are added to it's vocabulary.</p>
<p>this is the code:
#load glove model</p>
<pre><code>glove_file = datapath(&quot;/content/glove.6B.200d.txt&quot;)
tmp_file = get_tmpfile(&quot;test_word2vec.txt&quot;) 
_ = glove2word2vec(glove_file, tmp_file)
glove_vectors = KeyedVectors.load_word2vec_format(tmp_file)`
</code></pre>
<p><em>(in here - len(glove_vectors.wv.vocab) = 40000)</em></p>
<p>#create good article basic model</p>
<pre><code>base_model = Word2Vec(size=300, min_count=5) 
base_model.build_vocab([tokenizer.tokenize(data.text[0])]) 
total_examples = base_model.corpus_count`
</code></pre>
<p><em>(in here - len(base_model.wv.vocab) =   24)</em></p>
<p>#add GloVe's vocabulary &amp; weights <code>base_model.build_vocab([list(glove_vectors.vocab.keys())], update=True)</code></p>
<p><em>(in here- still - len(base_model_good_wv.vocab) =   24)</em></p>
<p>#training</p>
<pre><code>base_model.train([tokenizer.tokenize(good_trump.text[0])], total_examples=total_examples, epochs=base_model.epochs+5) 
base_model_wv = base_model.wv
</code></pre>
<p>i think that the
&quot;base_model.build_vocab([list(glove_vectors.vocab.keys())], update=True)&quot;
does nothing- so there is no transfer learning.
any recommendations?</p>
<p>i relied on this <a href=""https://towardsdatascience.com/transfer-learning-with-glove-word-vectors-7652456ae269"" rel=""nofollow noreferrer"">article</a> for the guideline...</p>
",Vectorization & Embeddings,fine tuning word vec specific article using transfer learning try fine tune exicting model specific article tried transfer learning using genism build vocab adding gloveword vec base model trained article build vocab doe change basic model small word added vocabulary code load glove model len glove vector wv vocab create good article basic model len base model wv vocab add glove vocabulary weight still len base model good wv vocab training think base model build vocab list glove vector vocab key update true doe nothing transfer learning recommendation relied article guideline
Embedding Index out of range:,"<p>I understand why this error usually occurs, which is that the input &gt;= embedding_dim.<br />
However in my case the torch.max(inputs) = embedding_dim - 1.</p>
<pre><code>print('inputs: ', src_seq)
print('input_shape: ', src_seq.shape)
print(self.src_word_emb)
inputs:  tensor([[10,  6,  2,  4,  9, 14,  6,  2,  5,  0],
        [12,  6,  3,  8, 13,  2,  0,  1,  1,  1],
        [13,  8, 12,  7,  2,  4,  0,  1,  1,  1]])
input_shape: [3, 10]
Embedding(15, 512, padding_idx=1)
emb = self.src_word_emb(src_seq)
</code></pre>
<p>I try to get a transformer model to work and for some reason the encoder embedding only accepts inputs &lt; embedding_dim_decoder, which does not make sense right?</p>
",Vectorization & Embeddings,embedding index range understand error usually occurs input embedding dim however case torch max input embedding dim try get transformer model work reason encoder embedding accepts input embedding dim decoder doe make sense right
Subset Text Similarity Score - How to detect a piece of small text that is very similar to subset of a much bigger text,"<p>There are many ways to detect similarity between 2 texts like Jaccard Index, TFIDF cosine similarity or sentence embedding. However all of them are refering to use case of 2 texts which are fully to be compared.</p>
<p>Here, I don't know how to call it but I add Subset Text Similarity Score, is to detect/calculate a score to see whether a small text is an extract from a bigger text.</p>
<p>For example, there is a big text (from a news)</p>
<pre><code>Google Stadia, the company's cloud gaming service, will shut down on January 18 after the game failed to gain the traction with users the company had hoped for.

The cloud gaming service debuted through a closed beta in October 2018 and publicly launched in November 2019.

In spite of the fact that users are about to lose access to all of their titles and save on Stadia, many publishers share ways to keep playing their games on other platforms, reports The Verge.

Moreover, Google is also refunding all Stadia hardware purchased through the Google Store as well as all the games and add-on content purchased from the Stadia store.
</code></pre>
<p>the objective of the subset text similarity is to detect whether this small text is a subset (extract) from the bigger text above. The small text can have sentences not in the same order as the bigger text to be compared.</p>
<p>Example small text</p>
<pre><code>On Stadia, users are will lose access to all of their titles and saves. all Stadia hardware purchased through the Google Store will be refunded.
</code></pre>
<p>For a small text above, the subset similarity score should be very high.</p>
<p>Is there some package or NLP method that can do this?</p>
",Vectorization & Embeddings,subset text similarity score detect piece small text similar subset much bigger text many way detect similarity text like jaccard index tfidf cosine similarity sentence embedding however refering use case text fully compared know call add subset text similarity score detect calculate score see whether small text extract bigger text example big text news objective subset text similarity detect whether small text subset extract bigger text small text sentence order bigger text compared example small text small text subset similarity score high package nlp method
Do BERT word embeddings change depending on context?,"<p>Before answering &quot;yes, of course&quot;, let me clarify what I mean:</p>
<p>After BERT has been trained, and I want to use the pretrained embeddings for some other NLP task, can I once-off extract all the word-level embeddings from BERT for all the words in my dictionary, and then have a set of static key-value word-embedding pairs, from where I retrieve the embedding for let's say &quot;bank&quot;, or will the embeddings for &quot;bank&quot; change depending on whether the sentence is &quot;Trees grow on the river bank&quot;, or &quot;I deposited money at the bank&quot; ?</p>
<p>And if the latter is the case, how do I practically use the BERT embeddings for another NLP task, do I need to run every input sentence through BERT before passing it into my own model?</p>
<p>Essentially - do embeddings stay the same for each word / token after the model has been trained, or are they dynamically adjusted by the model weights, based on the context?</p>
",Vectorization & Embeddings,bert word embeddings change depending context answering yes course let clarify mean bert ha trained want use pretrained embeddings nlp task extract word level embeddings bert word dictionary set static key value word embedding pair retrieve embedding let say bank embeddings bank change depending whether sentence tree grow river bank deposited money bank latter case practically use bert embeddings another nlp task need run every input sentence bert passing model essentially embeddings stay word token model ha trained dynamically adjusted model weight based context
Cosine similarity of two columns in a DataFrame,"<p>I've a dataframe with 2 columns and I am tring to get a cosine similarity score of each pair of sentences.</p>
<p>Dataframe (df)</p>
<pre><code>       A                   B
0    Lorem ipsum ta      lorem ipsum
1    Excepteur sint      occaecat excepteur
2    Duis aute irure     aute irure 
</code></pre>
<p>some of the code pieces that I've tried are:</p>
<pre><code>1. df[&quot;cosine_sim&quot;] = df[[&quot;A&quot;,&quot;B&quot;]].apply(lambda x1,x2:cosine_sim(x1,x2))

2. from spicy.spatial.distance import cosine
df[&quot;cosine_sim&quot;] = df.apply(lambda row: 1 - cosine(row['A'], row['B']), axis = 1)
</code></pre>
<p>The above codes didn't work, and I am still trying different approaches but in the meanwhile I would appreciate any guidance, Thank you in advance!</p>
<p>Desired output:</p>
<pre><code>       A                   B                     cosine_sim
0    Lorem ipsum ta      lorem ipsum                 0.8
1    Excepteur sint      occaecat excepteur          0.5
2    Duis aute irure     aute irure                  0.4
</code></pre>
",Vectorization & Embeddings,cosine similarity two column dataframe dataframe column tring get cosine similarity score pair sentence dataframe df code piece tried code work still trying different approach meanwhile would appreciate guidance thank advance desired output
Why loss is not decreasing in a Siamese BERT-Network training (Entity matching task),"<p>I'm trying to finetune a model for an <em>entity matching task</em> (kind of a <em>sentence similarity</em> task).</p>
<p>The idea is that if I give as input two sentences the model should output if they represent the same entity or not. I'm interested in the products' domain.</p>
<p>So for example:</p>
<pre class=""lang-py prettyprint-override""><code>sentences_left = ('logitech harmony 890 advanced universal remote control h890', 'sony silver digital voice recorder icdb600')
sentences_right = ('logitech harmony 890 advanced universal remote hdtv , tv , dvd player ( s ) , lighting , audio system 100 ft universal remote 966193-0403', 'canon black ef 70-300mm f/4 -5.6 is usm telephoto zoom lens 0345b002')
</code></pre>
<p>The output should be 1 for the first left-right pair of sentences and 0 for the second.</p>
<p>I want to test two approaches. The first is a <em>sequence classification</em> setup. So I take a pair of sentences, concat them with a [SEP] token in-between, encode it and feed it to BERT.</p>
<p><a href=""https://i.sstatic.net/k0cQg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/k0cQg.png"" alt=""This is the idea"" /></a></p>
<p>This approach kind of work, but I wanted to explore a second one that, in theory, should work too.</p>
<p>In few words, using <a href=""https://huggingface.co/sentence-transformers/all-mpnet-base-v2"" rel=""nofollow noreferrer"">mpnet</a> as pre-trained language model I'm trying to implement this setup:</p>
<p><a href=""https://i.sstatic.net/S3zVP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/S3zVP.png"" alt=""Siamese BERT from the Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper"" /></a></p>
<p>This is taken from the paper <a href=""https://arxiv.org/pdf/1908.10084.pdf"" rel=""nofollow noreferrer"">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a>. The idea is to compute not only a single embedding as before, but two separate embeddings for each of the sentences. Then concatenate the embeddings and feeds them to a <em>softmax classifier</em>.</p>
<p>After lots of struggles I'm still unable to make it work, since the loss has no intention of decreasing. It starts at 0.25 and never goes up neither down.</p>
<p>I'm using the <em>Abt-Buy</em>, <em>Amazon-Google</em> and <em>Walmart-Amazon</em> <a href=""https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution"" rel=""nofollow noreferrer"">datasets</a>.</p>
<p>This is my model:</p>
<pre class=""lang-py prettyprint-override""><code>
class FinalClassifier(nn.Module):

    def __init__(self, pos_neg=None, frozen=False):

        super(FinalClassifier, self).__init__()

        use_cuda = torch.cuda.is_available()
        self.device = torch.device(&quot;cuda&quot; if use_cuda else &quot;cpu&quot;)

        self.encoder = AutoModel.from_pretrained(
'all-mpnet-base-v2')

        if frozen:
          for param in self.encoder.parameters():
            param.requires_grad = False
        self.tokenizer = AutoTokenizer.from_pretrained(
'all-mpnet-base-v2')

        if pos_neg:
          self.criterion = BCEWithLogitsLoss(pos_weight=torch.Tensor([pos_neg]))

        self.linear = nn.Linear(3*768, 1)
        self.relu = nn.ReLu()


    def forward(self, texts_left, texts_right, labels=None):
       encoded_inputs_left = self.tokenizer(texts_left, padding='max_length',
                                        truncation=True, return_tensors='pt')
       encoded_inputs_left = encoded_inputs_left.to(self.device)
        
       output_left = self.encoder(**encoded_inputs_left)
       output_left = _mean_pooling(output_left, encoded_inputs_left['attention_mask'])
       # output_left = F.normalize(output_left, p=2, dim=1)

       encoded_inputs_right = self.tokenizer(texts_right, padding='max_length',
                                        truncation=True, return_tensors='pt')
       encoded_inputs_right = encoded_inputs_right.to(self.device)

       output_right = self.encoder(**encoded_inputs_right)
       output_right = _mean_pooling(output_right, encoded_inputs_right['attention_mask'])
       # output_right = F.normalize(output_right, p=2, dim=1)

       # Look at sBERT paper (u, v, |u-v|)
       pooled_output = torch.cat((output_left, output_right, torch.abs(output_left - output_right)), -1)


      linear_output = self.linear(pooled_output)
      relu_output = self.relu(linear_output)

      labels = labels.to(self.device)
      loss = self.criterion(linear_output.view(-1), labels.float())

      return (loss, relu_output)

</code></pre>
<p>Here's the Dataset</p>
<pre class=""lang-py prettyprint-override""><code>
class FinalDataset(torch.utils.data.Dataset):

    def __init__(self, df):

        self.labels = [int(label) for label in df['label']]
        self.examples = df

    def classes(self):
        return self.labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        examples = self.examples.iloc[idx]
        text_left = examples['text_left']
        text_right = examples['text_right']
        label = np.array(self.labels[idx])
        return text_left, text_right, label


</code></pre>
<p>and finally the training loop</p>
<pre class=""lang-py prettyprint-override""><code>
def train(model, train, val, learning_rate=1e-6, epochs=5, batch_size=8):
  train_dataloader = torch.utils.data.DataLoader(train, batch_size=8, shuffle=True)
  val_dataloader = torch.utils.data.DataLoader(val, batch_size=8)

  use_cuda = torch.cuda.is_available()
  device = torch.device(&quot;cuda&quot; if use_cuda else &quot;cpu&quot;)

  optimizer = Adam(model.parameters(), lr= learning_rate)

  if use_cuda:
    model = model.cuda()

  for epoch_num in range(epochs):
    total_loss_train = 0
    tmp_loss = 0
    step = 0

    model.train()
    for i, data in enumerate(tqdm(train_dataloader)):
      left_batch, right_batch, labels = data

      (batch_loss, _) = model(left_batch, right_batch, labels)

      total_loss_train += batch_loss
      tmp_loss += batch_loss
      
      model.zero_grad()
      batch_loss.backward()
      optimizer.step()

      # every 100 mini-batches
      if i % 100 == 99:
        print(f' Loss/train at epoch {epoch_num+1} (batch {i}): {tmp_loss/500}')
        writer.add_scalar('Loss/train',
                          tmp_loss / 100,
                          epoch_num * len(train_dataloader) + i)
        tmp_loss = 0

    total_loss_val = 0
    predictions = None
    total_labels = None
    step = 0

    model.eval()
    with torch.no_grad():

      for i, data in enumerate(val_dataloader):
        left_batch, right_batch, labels = data
        (batch_loss, linear_output) = model(left_batch, right_batch, labels)

        labels = labels.detach().cpu().numpy()
        linear_output = linear_output.detach().cpu().numpy()
        if predictions is None:
          predictions = np.where(linear_output&gt;0.5, 1, 0)
          total_labels = labels
        else:
          predictions = np.append(predictions, np.where(linear_output&gt;0.5, 1, 0), axis=0)
          total_labels = np.append(total_labels, labels, axis=0)

        total_loss_val += batch_loss.item()
        tmp_loss += batch_loss.item()

        # every 100 mini-batches
        if i % 100 == 99:
          print(f' Loss/val at epoch {epoch_num+1} (batch {i}): {tmp_loss/500}')
          writer.add_scalar('Loss/val',
                            tmp_loss / 100,
                            epoch_num * len(val_dataloader) + i)
          writer.add_scalar('F1/val',
                            f1_score(y_true=total_labels.flatten()[step:i], y_pred=predictions.flatten()[step:i]),
                            epoch_num * len(val_dataloader) + i)
          tmp_loss = 0
          step += 100
        
    f1 = f1_score(y_true=total_labels.flatten(), y_pred=predictions.flatten())
    report = classification_report(total_labels, predictions, zero_division=0)

    # plot all the pr curves
    for i in range(len([0, 1])):
      add_pr_curve_tensorboard(i, predictions.flatten(), total_labels.flatten())
    
    for name, p in model.named_parameters():
      writer.add_histogram(name, p, bins='auto')
    
    print(
        f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train): .3f} \
        | Val Loss: {total_loss_val / len(val): .3f} \
        | Val F1: {f1: .3f}')

    tqdm.write(report)


writer = SummaryWriter(log_dir=tensorboard_path)

EPOCHS = 5
  
LR = 1e-6
train_pos_neg_ratio = 9

model = FinalClassifier(train_pos_neg_ratio, frozen=False)
train_data, val_data = FinalDataset(df_train), FinalDataset(df_dev)

              
train(model, train_data, val_data, LR, EPOCHS)
writer.flush()
writer.close()


</code></pre>
<p>The issue is that the loss does NOT decrease, and the F1 accuracy as a result. I tried to normalize the outputs, add a dropout layer, analized the dataset to be sure that the problem wasn't there but now I ran out of ideas. An help would be extremely valuable.</p>
",Vectorization & Embeddings,loss decreasing siamese bert network training entity matching task trying finetune model entity matching task kind sentence similarity task idea give input two sentence model output represent entity interested product domain example output first left right pair sentence second want test two approach first sequence classification setup take pair sentence concat sep token encode feed bert approach kind work wanted explore second one theory work word using mpnet pre trained language model trying implement setup taken paper sentence bert sentence embeddings using siamese bert network idea compute single embedding two separate embeddings sentence concatenate embeddings feed softmax classifier lot struggle still unable make work since loss ha intention decreasing start never go neither using abt buy amazon google walmart amazon datasets model dataset finally training loop issue loss doe decrease f accuracy result tried normalize output add dropout layer analized dataset sure problem ran idea help would extremely valuable
"Why the ouput of nn.Embeddings(vocab_size, dim) chnages on re-running the code for same input string?","<p>I am trying to understand how word embeddings are generated, I've been reading that 1-hot encoded vector is used and it servers as a lookup table but, I want to print that and see, how can I do that.  When I am doing the following:</p>
<pre><code> self.embeddings = nn.Embedding(vocab_size, dim)
 print('embed',self.embeddings.weight)
</code></pre>
<p>I am getting different results when I re-run the code for same input string/vocab_size and same dim.</p>
<p>I want to know why this happens and how are those weight values calulated? What equation/fucntions is used to get these values? Is any function like softmax etc used to get the weights?</p>
",Vectorization & Embeddings,ouput nn embeddings vocab size dim chnages running code input string trying understand word embeddings generated reading hot encoded vector used server lookup table want print see following getting different result run code input string vocab size dim want know happens weight value calulated equation fucntions used get value function like softmax etc used get weight
Generate and Read Word Embeddings with KeyedVectors,"<p>I trained a neural network with tensorflow and extracted the weights from the embedding layer to make an array of embeddings. I generated it as a txt file and I can't read it with KeyedVectors
Generate the file.</p>
<p>Generate the file</p>
<pre><code>import io
out_vectors = io.open('vecs.txt', 'w', encoding='utf-8')
out_vectors.write(f'{vocab_size} {embedding_dim}')
for word_num in range(1,vocab_size):
  word = reversed_word_index[word_num]
  embeddings = weights[word_num]
  line_embedding ='\n' + word + '\t'+ '\t'.join([str(x) for x in embeddings]) 
  out_vectors.write(line_embedding)
  
out_vectors.close()
</code></pre>
<p>Read the file</p>
<pre><code>from gensim.models import KeyedVectors
model_embedding = KeyedVectors.load_word2vec_format('vecs.txt')
</code></pre>
",Vectorization & Embeddings,generate read word embeddings keyedvectors trained neural network tensorflow extracted weight embedding layer make array embeddings generated txt file read keyedvectors generate file generate file read file
Classic king - man + woman = queen example with pretrained word-embedding and word2vec package in R,"<p>I am really desperate, I just cannot reproduce the allegedly classic example of <code>king - man + woman = queen</code> with the <code>word2vec</code> package in R and any (!) pre-trained embedding model (as a <code>bin</code> file).</p>
<p>I would be very grateful if anybody could provide working code to reproduce this example... including a link to the necessary pre-trained model which is also downloadable (many are not!).</p>
<p>Thank you very much!</p>
",Vectorization & Embeddings,classic king man woman queen example pretrained word embedding word vec package r really desperate reproduce allegedly classic example package r pre trained embedding model file would grateful anybody could provide working code reproduce example including link necessary pre trained model also downloadable many thank much
How to convert TS-SS result to similarity measure between 0 - 1?,"<p>I'm currently developing a question plugin for some LMS that auto grade the answer based on the similarity between the answer and answer key with cosine similarity. But lately, I found that there is a better algorithm that promised to be more accurate called <a href=""https://github.com/taki0112/Vector_Similarity/blob/master/TS-SS_paper.pdf"" rel=""nofollow noreferrer"">TS-SS</a>. But, the result of the calculation 0 - infinity. Being not a machine learning guy, I was assuming that the result maybe a distance, just like Euclidean Distance, but I'm not sure. It can be a geometry or something, because the algorithm is calculating the triangle and sector, so I'm assuming that it is a geometric similarity or something, I'm not sure though.</p>
<p>So I have some example in my note, and then I tried to convert it with what people suggest, <code>S = 1 / (1 + D)</code>, but the result was not what I'm looking for. With cosine similarity I got 0.77, but with TS-SS plus equation before, I got 0.4. And then I found this <a href=""https://stackoverflow.com/a/43689075/19323874"">SO</a> answer that uses <code>S = 1 / (1.1 ** D)</code>. When I tried the equation, sure enough it gave me &quot;relevant&quot; result, 0.81. That is not far from cosine similarity, and in my opinion the result is better suited for auto grading than 0.77 one based on the answer key.</p>
<p>But unfortunately, I don't know where that equation come from, and I tried to google it but no luck, so that is why I'm asking this question.
How to convert the TS-SS result to similarity measure the right way? Is the <code>S = 1 / (1.1 ** D)</code> enough or...?</p>
<p>Edit:</p>
<p>When calculating TS-SS, it is actually using cosine similarity calculation as well. So, if the cosine similarity is 1, then the TS-SS will be 0. But, if the cosine similarity is 0, the TS-SS is not infinty. So, I think it is reasonable to compare the result between the two to know what conversion formula will be used</p>
<pre><code>TS-SS  Cosine Similarity
38.19   0
7.065   0.45
3.001   0.66
1.455   0.77
0.857   0.81
0.006   0.80
0       1

another random comparison from multiple answer key
36.89   0
9.818   0.42
7.581   0.45
3.910   0.63
2.278   0.77
2.935   0.75
1.329   0.81
0.494   0.84
0.053   0.75
0.011   0.80
0.003   0.98
0           1

comparison from the same answer key
38.11   0.71
4.293   0.33
1.448   0
1.203   0.17
0.527   0.62
</code></pre>
<p>Thank you in advance</p>
",Vectorization & Embeddings,convert result similarity measure currently developing question plugin lm auto grade answer based similarity answer answer key cosine similarity lately found better algorithm promised accurate called result calculation infinity machine learning guy wa assuming result maybe distance like euclidean distance sure geometry something algorithm calculating triangle sector assuming geometric similarity something sure though example note tried convert people suggest result wa looking cosine similarity got plus equation got found href answer us tried equation sure enough gave quot relevant quot result far cosine similarity opinion result better suited auto one based answer key p unfortunately know equation come tried google luck asking question convert result similarity measure right way enough edit calculating actually using cosine similarity calculation well cosine similarity cosine similarity infinty think reasonable compare result two know conversion formula used thank advance
Issue with Word2Vec embedding matrix,"<p>I am currenlty working on a ML project. I am stuck on the embedding matrix for my Word2Vec model.</p>
<p>The code snippet is below:;</p>
<pre><code>vocab_size = len(tokenizer.word_index)+1
embedding_matrix = np.zeros((vocab_size, embedding_vector_size))
# +1 is done because i starts from 1 instead of 0, and goes till len(vocab)
for  word, i in tokenizer.word_index.items():
    embedding_vector = model_1.wv[word] #error
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
</code></pre>
<p>The error I get is this message:</p>
<blockquote>
<p>raise KeyError(f&quot;Key '{key}' not present&quot;) KeyError: &quot;Key 'https' not present&quot;</p>
</blockquote>
<p>What would be a way to fix this issue?</p>
",Vectorization & Embeddings,issue word vec embedding matrix currenlty working ml project stuck embedding matrix word vec model code snippet error get message raise keyerror f key key present keyerror key present would way fix issue
How can I get similarity rate between columns in different dataframes and filter by some value,"<p>I need to compute differences between rows in different dataframes. On one hand I have df1 with 150 different rows with specific phrases. On the other hand I have df2 with 250k of rows with any type of phrases. I want to compare each row in df1 with each row in df2 getting or labeling all the rows in df2 that have a value higher than a threshold (to be determinated) I use SequenceMatcher in order to get the similarity between them.</p>
<p>How could i do it? Any advice in order to do it in the more efficient way? How you code that?</p>
<p>df1 (len = 150)</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Phrases</th>
</tr>
</thead>
<tbody>
<tr>
<td>My cat is black</td>
</tr>
<tr>
<td>Dog is white</td>
</tr>
<tr>
<td>Peter is waiting</td>
</tr>
<tr>
<td>Dog is white</td>
</tr>
</tbody>
</table>
</div>
<p>df2 (len = 250k)</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Phrases</th>
</tr>
</thead>
<tbody>
<tr>
<td>My cat is white</td>
</tr>
<tr>
<td>Dog is jumping</td>
</tr>
<tr>
<td>Marcos is waiting</td>
</tr>
<tr>
<td>Dog is white</td>
</tr>
</tbody>
</table>
</div>
<p>Output:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Phrases</th>
<th>Labels</th>
</tr>
</thead>
<tbody>
<tr>
<td>My cat is white</td>
<td>0.75</td>
</tr>
<tr>
<td>My cat is white</td>
<td>0</td>
</tr>
<tr>
<td>My cat is white</td>
<td>0</td>
</tr>
<tr>
<td>My cat is white</td>
<td>0.33</td>
</tr>
<tr>
<td>Dog is jumping</td>
<td>0.33</td>
</tr>
<tr>
<td>Dog is jumping</td>
<td>0.66</td>
</tr>
<tr>
<td>Dog is jumping</td>
<td>0.33</td>
</tr>
<tr>
<td>Dog is jumping</td>
<td>0.66</td>
</tr>
<tr>
<td>Marcos is waiting</td>
<td>0.33</td>
</tr>
<tr>
<td>Marcos is waiting</td>
<td>0.33</td>
</tr>
<tr>
<td>Marcos is waiting</td>
<td>0.66</td>
</tr>
<tr>
<td>Marcos is waiting</td>
<td>0.33</td>
</tr>
<tr>
<td>Dog is white</td>
<td>0.5</td>
</tr>
<tr>
<td>Dog is white</td>
<td>0.66</td>
</tr>
<tr>
<td>Dog is white</td>
<td>0.33</td>
</tr>
<tr>
<td>Dog is white</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>",Vectorization & Embeddings,get similarity rate column different dataframes filter value need compute difference row different dataframes one hand df different row specific phrase hand df k row type phrase want compare row df row df getting labeling row df value higher threshold determinated use sequencematcher order get similarity could advice order efficient way code df len phrase cat black dog white peter waiting dog white df len k phrase cat white dog jumping marcos waiting dog white output phrase label cat white cat white cat white cat white dog jumping dog jumping dog jumping dog jumping marcos waiting marcos waiting marcos waiting marcos waiting dog white dog white dog white dog white
How to know which contextual embedding to use at test time,"<p>Models like BERT generate contextual embeddings for words with different contextual meanings, like 'bank', 'left'.
I don't understand which contextual embedding the model chooses to use at test time? Given a test sentence for classification, when we load the pre-trained bert, how do we initialize the word (token) embedding to use the right contextual embedding over the other embeddings of the same word?</p>
<p>More specifically, there is a convert_to_id() function which converts a word to an id? how does one id represent the correct contextual embedding for the input sentence at test time? Thank you.</p>
<p>I searched all over online but only found explanation about the difference between static vs. contextual embedding, the high level concept is easy to get, but how is that really achieved is unclear. I also search some code example, but the convert_to_id() makes me further confused as I asked in my question.</p>
",Vectorization & Embeddings,know contextual embedding use test time model like bert generate contextual embeddings word different contextual meaning like bank left understand contextual embedding model chooses use test time given test sentence classification load pre trained bert initialize word token embedding use right contextual embedding embeddings word specifically convert id function convert word id doe one id represent correct contextual embedding input sentence test time thank searched online found explanation difference static v contextual embedding high level concept easy get really achieved unclear also search code example convert id make confused asked question
Getting similarity score with spacy and a transformer model,"<p>I've been using the spacy <code>en_core_web_lg</code> and wanted to try out <code>en_core_web_trf</code> (transformer model) but having some trouble wrapping my head around the difference in the model/pipeline usage.</p>
<p>My use case looks like the following:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy import displacy
nlp = spacy.load(&quot;en_core_web_trf&quot;)

s1 = nlp(&quot;Running for president is probably hard.&quot;)
s2 = nlp(&quot;Space aliens lurk in the night time.&quot;)
s1.similarity(s2)
</code></pre>
<p>Output:</p>
<pre><code>The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements.
(0.0, Space aliens lurk in the night time.)
</code></pre>
<p>Looking at <a href=""https://github.com/explosion/spaCy/discussions/7643"" rel=""noreferrer"">this post</a>, the transformer model does not have a word vector in the same way <code>en_core_web_lg</code> does, but you can get the embedding via <code>s1._.trf_data.tensors</code>. Which looks like:</p>
<pre class=""lang-py prettyprint-override""><code>sent1._.trf_data.tensors[0].shape
(1, 9, 768)
sent1._.trf_data.tensors[1].shape
(1, 768)
</code></pre>
<p>So I tried to manually take the cosine similarity (<a href=""https://github.com/explosion/spaCy/discussions/6511?sort=new#discussioncomment-971055"" rel=""noreferrer"">using this post as ref</a>):</p>
<pre class=""lang-py prettyprint-override""><code>def similarity(obj1, obj2):
        (v1, t1), (v2, t2) = obj1._.trf_data.tensors, obj2._.trf_data.tensors
        try:
            return ((1 - cosine(v1, v2)) + (1 - cosine(t1, t2))) / 2
        except:
            return 0.0
</code></pre>
<p>But this does not work.</p>
",Vectorization & Embeddings,getting similarity score spacy transformer model using spacy wanted try transformer model trouble wrapping head around difference model pipeline usage use case look like following output looking post transformer model doe word vector way doe get embedding via look like tried manually take cosine similarity using post ref doe work
Recommendation system for news articles,"<p>I am trying to create an recommendation system for similar articles. I do have a list of articles as reference and I want other new articles that I acquire from a certain API needs to be similar to those reference articles.</p>
<p>One way I could is just merge all of those reference article into one big article and run cosine similarity and get list of articles that are similar to to merged reference articles. Is there any other way I could implement cosine similarity?</p>
<p>Thanks</p>
",Vectorization & Embeddings,recommendation system news article trying create recommendation system similar article list article reference want new article acquire certain api need similar reference article one way could merge reference article one big article run cosine similarity get list article similar merged reference article way could implement cosine similarity thanks
Conv2d removing part of the input,"<p>I am writing a program that uses words embeddings and then train it on a simple CNN. The embedding dim is equal to 768 and the hidden dim is equal to 100. I use a PyTorch Conv2d:</p>
<p>After creating embeddings, this is the init() part:</p>
<pre><code>self.conv = nn.Conv2d(1, params.hidden_dim, kernel_size=(3, params.embedding_dim))
nn.init.xavier_uniform_(self.conv.weight)
nn.init.constant_(self.conv.bias, 0.0)
self.fc = nn.Linear(params.hidden_dim, params.num_of_tags)
</code></pre>
<p>And this is the forward() part:</p>
<pre><code>print(x.shape)   # [4, 39, 768]
x = x.unsqueeze(1)
x = self.conv(x).squeeze(3)
x = x.permute(0, 2, 1)
print(x.shape)   # [4, 37, 100]

x = x.contiguous()
x = x.view(-1, x.shape[2])
x = self.dropout(x)
x = self.fc(x)
</code></pre>
<p>I don't really understand why the shape changes (see the prints). It is the number of words in the sentenced (already padded so in all 4 sentences there is the same number of word).</p>
<p>Does someone know why the the shape changes and what to do to avoid it?</p>
<p>I tried to pad the x in the end but I guess it doesn't make sense because I may lose important input (two removed words). I want the shape to remain the same.</p>
",Vectorization & Embeddings,conv removing part input writing program us word embeddings train simple cnn embedding dim equal hidden dim equal use pytorch conv creating embeddings init part forward part really understand shape change see print number word sentenced already padded sentence number word doe someone know shape change avoid tried pad x end guess make sense may lose important input two removed word want shape remain
Converting word2vec output into dataframe for sklearn,"<p>I am attempting to use <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim's word2vec</a> to transform a column of a pandas dataframe into a vector that I can pass to a <a href=""https://scikit-learn.org/stable/supervised_learning.html"" rel=""nofollow noreferrer""><code>sklearn</code> classifier</a> to make a prediction.</p>
<p>I understand that I need to average the vectors for each row. I have tried <a href=""https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"" rel=""nofollow noreferrer"">following this guide</a> but I am stuck, as I am getting models back but I don't think I can access the underlying embeddings to find the averages.</p>
<p>Please see <a href=""https://stackoverflow.com/help/minimal-reproducible-example"">a minimal, reproducible example</a> below:</p>
<pre><code>import pandas as pd, numpy as np
from gensim.models import Word2Vec
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn.feature_extraction.text import CountVectorizer

temp_df = pd.DataFrame.from_dict({'ID': [1,2,3,4,5], 'ContData': [np.random.randint(1, 10 + 1)]*5, 
                                'Text': ['Lorem ipsum dolor sit amet', 'consectetur adipiscing elit.', 'Sed elementum ultricies varius.',
                                         'Nunc vel risus sed ligula ultrices maximus id qui', 'Pellentesque pellentesque sodales purus,'],
                                'Class': [1,0,1,0,1]})
temp_df['text_lists'] = [x.split(' ') for x in temp_df['Text']]

w2v_model = Word2Vec(temp_df['text_lists'].values, min_count=1)

cv = CountVectorizer()
count_model = pd.DataFrame(data=cv.fit_transform(temp_df['Text']).todense(), columns=list(cv.get_feature_names_out()))
</code></pre>
<p>Using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer""><code>sklearn's CountVectorizer</code></a>, I am able to get a simple frequency representation that I can pass to a classifier. How can I get that same format using Word2vec?</p>
<p>This toy example produces:</p>
<pre><code>adipiscing  amet    consectetur dolor   elementum   elit    id  ipsum   ligula  lorem   ... purus   qui risus   sed sit sodales ultrices    ultricies   varius  vel
0   0   1   0   1   0   0   0   1   0   1   ... 0   0   0   0   1   0   0   0   0   0
1   1   0   1   0   0   1   0   0   0   0   ... 0   0   0   0   0   0   0   0   0   0
2   0   0   0   0   1   0   0   0   0   0   ... 0   0   0   1   0   0   0   1   1   0
3   0   0   0   0   0   0   1   0   1   0   ... 0   1   1   1   0   0   1   0   0   1
4   0   0   0   0   0   0   0   0   0   0   ... 1   0   0   0   0   1   0   0   0   0
</code></pre>
<p>While this runs without error, I cannot access the embedding that I can pass with this current format. I would like to produce the same format, with the exception of instead of there being counts, its the <code>word2vec</code> value embeddings</p>
",Vectorization & Embeddings,converting word vec output dataframe sklearn attempting use gensim word vec transform column panda dataframe vector pas classifier make prediction understand need average vector row tried following guide stuck getting model back think access underlying embeddings find average please see able get simple frequency representation pas classifier get format using word vec toy example produce run without error access embedding pas current format would like produce format exception instead count value embeddings
PySpark CrossValidator Fails - org.apache.spark.SparkException: Failed to execute user defined function,"<p>I have created an ML Pipeline with PySpark, and I can fit and transform the pipeline with LogisticRegression successfully. I get a trained model.</p>
<p>The issue is when I try to use CrossValidate. It runs part way, and then always fails with the following error:</p>
<pre><code>Py4JJavaError: An error occurred while calling o1883.evaluate.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 135.0 failed 1 times, most recent failure: Lost task 0.0 in stage 135.0 (TID 110) (06496cbd55cf executor driver): org.apache.spark.SparkException: Failed to execute user defined function (HasSimpleAnnotate$$Lambda$3820/0x0000000841652840: (array&lt;array&lt;struct&lt;annotatorType:string,begin:int,end:int,result:string,metadata:map&lt;string,string&gt;,embeddings:array&lt;float&gt;&gt;&gt;&gt;) =&gt; array&lt;struct&lt;annotatorType:string,begin:int,end:int,result:string,metadata:map&lt;string,string&gt;,embeddings:array&lt;float&gt;&gt;&gt;)
    at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)
    at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
    at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)
    at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
    at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
    at org.apache.spark.scheduler.Task.run(Task.scala:131)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.Exception: feature rules is not set
    at com.johnsnowlabs.nlp.serialization.Feature.$anonfun$getOrDefault$1(Feature.scala:116)
    at scala.Option.getOrElse(Option.scala:189)
    at com.johnsnowlabs.nlp.serialization.Feature.getOrDefault(Feature.scala:116)
    at com.johnsnowlabs.nlp.HasFeatures.$$(HasFeatures.scala:73)
    at com.johnsnowlabs.nlp.HasFeatures.$$$(HasFeatures.scala:73)
    at com.johnsnowlabs.nlp.AnnotatorModel.$$(AnnotatorModel.scala:28)
    at com.johnsnowlabs.nlp.annotators.TokenizerModel.$anonfun$tag$5(TokenizerModel.scala:340)
    at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
    at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
    at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
    at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
    at scala.collection.AbstractIterator.to(Iterator.scala:1431)
    at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
    at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
    at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)
    at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
    at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
    at scala.collection.AbstractIterator.toArray(Iterator.scala:1431)
    at com.johnsnowlabs.nlp.annotators.TokenizerModel.$anonfun$tag$2(TokenizerModel.scala:392)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
    at scala.collection.TraversableLike.map(TraversableLike.scala:286)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
    at scala.collection.AbstractTraversable.map(Traversable.scala:108)
    at com.johnsnowlabs.nlp.annotators.TokenizerModel.tag(TokenizerModel.scala:288)
    at com.johnsnowlabs.nlp.annotators.TokenizerModel.annotate(TokenizerModel.scala:400)
    at com.johnsnowlabs.nlp.HasSimpleAnnotate.$anonfun$dfAnnotate$1(HasSimpleAnnotate.scala:46)
    ... 19 more

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
    at scala.Option.foreach(Option.scala:407)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)
    at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
    at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
    at org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
    at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)
    at org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)
    at org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)
    at org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass$lzycompute(MulticlassMetrics.scala:66)
    at org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass(MulticlassMetrics.scala:64)
    at org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure(MulticlassMetrics.scala:227)
    at org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure$lzycompute(MulticlassMetrics.scala:235)
    at org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure(MulticlassMetrics.scala:235)
    at org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:153)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
    at java.base/java.lang.Thread.run(Thread.java:829)
</code></pre>
<p>I have tried to debug this, and I am stuck. If I use just my OneHotEncoder column as the 'features' vector I have no issues. It is when I add in the TFIDF vectors too that it starts to fail.</p>
<p>Here is my code to build the pipeline:</p>
<pre><code>import sparknlp
spark = sparknlp.start()

from pyspark.ml.feature import HashingTF, IDF
from pyspark.ml.feature import CountVectorizer
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import OneHotEncoder
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline


documentAssemblerTitle = DocumentAssembler().setInputCol('title').setOutputCol('titles')
documentAssemblerBody = DocumentAssembler().setInputCol('body').setOutputCol('bodies')
tokenizer_titles = Tokenizer().setInputCols(['titles']).setOutputCol('tokenized_titles')
tokenizer_bodies = Tokenizer().setInputCols(['bodies']).setOutputCol('tokenized_bodies')
stemmer_titles = Stemmer().setInputCols(['tokenized_titles']).setOutputCol('stemmed_titles')
stemmer_bodies = Stemmer().setInputCols(['tokenized_bodies']).setOutputCol('stemmed_bodies')
ngrammer_titles = NGramGenerator().setInputCols(['stemmed_titles']).setOutputCol('ngrams_titles').setN(1).setEnableCumulative(True)
ngrammer_bodies = NGramGenerator().setInputCols(['stemmed_bodies']).setOutputCol('ngrams_bodies').setN(1).setEnableCumulative(True)
finisher = Finisher().setInputCols(['ngrams_titles','ngrams_bodies'])
# tf_titles = HashingTF(inputCol='finished_stemmed_titles',outputCol='tf_features_titles')
# tf_bodies = HashingTF(inputCol='finished_stemmed_bodies',outputCol='tf_features_bodies')
tf_titles = CountVectorizer(inputCol='finished_ngrams_titles',outputCol='tf_features_titles')
tf_bodies = CountVectorizer(inputCol='finished_ngrams_bodies',outputCol='tf_features_bodies')
idf_titles = IDF(inputCol='tf_features_titles', outputCol='idf_titles',minDocFreq=5)
idf_bodies = IDF(inputCol='tf_features_bodies', outputCol='idf_bodies',minDocFreq=5)
author_indexer = StringIndexer(inputCol=&quot;author_association&quot;, outputCol=&quot;author_index&quot;)
ohe = OneHotEncoder(inputCol=&quot;author_index&quot;, outputCol=&quot;aa&quot;)
assembler = VectorAssembler(inputCols=['aa', 'idf_titles', 'idf_bodies'],outputCol='features')

pipeline_stages = [documentAssemblerTitle,
                    documentAssemblerBody,
                    tokenizer_titles,
                    tokenizer_bodies,
                    stemmer_titles,
                    stemmer_bodies,
                    ngrammer_titles,
                    ngrammer_bodies,
                    finisher,
                    tf_titles,
                    tf_bodies,
                    idf_titles,
                    idf_bodies,
                    author_indexer,
                    ohe,
                    assembler]

from pyspark.ml.classification import LogisticRegression

label_stringIdx = StringIndexer(inputCol = &quot;Target&quot;, outputCol = &quot;label&quot;)
pipeline_stages.append(label_stringIdx)

lr = LogisticRegression(maxIter=20)  # regParam=0.3, elasticNetParam=0.8
pipeline_stages.append(lr)

pipeline = Pipeline().setStages(pipeline_stages)
pipeline_stages
</code></pre>
<p>Here is the code for the cross-validation:</p>
<pre><code>from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator
from pyspark.ml.tuning import TrainValidationSplit, CrossValidator, ParamGridBuilder
import numpy as np

# pipeline = Pipeline().setStages(pipeline_stages)
paramGrid_lr = ParamGridBuilder().addGrid(lr.maxIter, [1,2]).build()
# paramGrid_lr = ParamGridBuilder() \
#     .addGrid(lr.regParam, [0.01, 0.5, 1]) \
#     .addGrid(lr.elasticNetParam, [0.0, 0.5, 1]) \
#     .addGrid(model.maxIter, [10, 20]) \
#     .addGrid(idf.numFeatures, [10, 100, 1000]) \
#     .build()
# evaluator = BinaryClassificationEvaluator()
evaluator = MulticlassClassificationEvaluator()
cv = TrainValidationSplit(estimator=pipeline,
                    estimatorParamMaps=paramGrid_lr,
                    evaluator=evaluator)

cvModel = cv.fit(train)
</code></pre>
<p>And my preprocessing code:</p>
<pre><code>import re
from pyspark.sql.functions import udf

@udf(&quot;String&quot;)
def strip_text(text):
  if text is not None:
    stripped = text.lower()    

    # remove all headings, bold text, and HTML comments from the Markdown text.
    # These items have all been used by the React team in their issue templates on GitHub
    headings_pattern = r'(&lt;=\s|^)#{1,6}(.*?)$'
    bold_pattern = r'\*\*(.+?)\*\*(?!\*)'
    comments_pattern = r'&lt;!--((.|\n)*?)--&gt;'
    combined_pattern = r'|'.join((headings_pattern, bold_pattern, comments_pattern))

    stripped = re.sub(combined_pattern, '', stripped)

    # find all URLs in the string, and then remove the final directory from each to leave the general URL form
    # there may be useful patterns based on what URLs issues are commonly linking to
    url_pattern = re.compile(r'(https?://[^\s]+)')
    for url in re.findall(url_pattern, stripped):
        new_url = url.rsplit(&quot;/&quot;, 1)[0]
        stripped = stripped.replace(url, new_url)

    non_alpha_pattern = r'[^A-Za-z ]+'
    stripped = re.sub(non_alpha_pattern, '', stripped)    
    
    return ' '.join(stripped.split())
  else:
    return &quot; &quot;

from pyspark.sql.functions import col
train = train.withColumn(&quot;body&quot;, strip_text(col(&quot;body&quot;))).withColumn(&quot;title&quot;, strip_text(col(&quot;title&quot;)))
validation = validation.withColumn(&quot;body&quot;, strip_text(col(&quot;body&quot;))).withColumn(&quot;title&quot;, strip_text(col(&quot;title&quot;)))
</code></pre>
<p>Thanks for any help you may be able to provide. I have been working on this with no progress for a long time now.</p>
<p>I have tried removing my pre-processing UDF, but there was no difference.</p>
<p>I have tried to impute null values as I saw this could potentially be an issue, but it doesn't fix it</p>
<pre><code>train = train.fillna(0)
</code></pre>
<p>I have tried removing the IDF text features altogether just leaving the OneHotEncoded column, and this did work. So I believe it is an issue with the text column, but I am not sure what to do next.</p>
",Vectorization & Embeddings,pyspark crossvalidator fails org apache spark sparkexception failed execute user defined function created ml pipeline pyspark fit transform pipeline logisticregression successfully get trained model issue try use crossvalidate run part way always fails following error tried debug stuck use onehotencoder column feature vector issue add tfidf vector start fail code build pipeline code cross validation preprocessing code thanks help may able provide working progress long time tried removing pre processing udf wa difference tried impute null value saw could potentially issue fix tried removing idf text feature altogether leaving onehotencoded column work believe issue text column sure next
How to construct PPMI matrix from a text corpus?,"<p>I am trying to use an SVD model for word embedding on the Brown corpus. For this, I want to first generate a word-word co-occurence matrix and then convert to PPMI matrix for the SVD matrix multiplication process.</p>

<p>I have tried to create a co-occurence using SkLearn CountVectorizer </p>

<pre><code>count_model = CountVectorizer(ngram_range=(1,1))

X = count_model.fit_transform(corpus)
X[X &gt; 0] = 1
Xc = (X.T * X)
Xc.setdiag(0)
print(Xc.todense())
</code></pre>

<p>But:</p>

<p>(1) Am not sure how I can control the context window with this method? I want to experiment with various context sizes and see how the impact the process.</p>

<p>(2) How do I then compute the PPMI properly assuming that 
PMI(a, b) = log p(a, b)/p(a)p(b)</p>

<p>Any help on the thought process and implementation would be greatly appreciated!</p>

<p>Thanks (-:</p>
",Vectorization & Embeddings,construct ppmi matrix text corpus trying use svd model word embedding brown corpus want first generate word word co occurence matrix convert ppmi matrix svd matrix multiplication process tried create co occurence using sklearn countvectorizer sure control context window method want experiment various context size see impact process compute ppmi properly assuming pmi b log p b p p b help thought process implementation would greatly appreciated thanks
Calculating embedding overload problems with BERT,"<p>I'm trying to calculate the embedding of a sentence using BERT. After I input the sentence into BERT, I calculate the Mean-pooling, which is used as the embedding of the sentence.</p>
<h2>Problem</h2>
<p>My code can calculate the embedding of sentences, but the computational cost is very high. I don't know what's wrong and I hope someone can help me.</p>
<h3>Install BERT</h3>
<pre><code>import torch
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;)
</code></pre>
<h3>Get Embedding Function</h3>
<pre><code># get the word embedding from BERT
def get_word_embedding(text:str):
    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1
    outputs = model(input_ids)
    last_hidden_states = outputs[1]  
    # The last hidden-state is the first element of the output tuple
    return last_hidden_states[0]
</code></pre>
<h3>Data</h3>
<p>The maximum number of words in the text is 50. I calculate the entity+text embedding</p>
<p><a href=""https://i.sstatic.net/hvw4Y.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<h3>Run code</h3>
<p><code>entity_desc</code> is my data.
It's this step that overloads my computer every time I run it.
Please help me!!!</p>
<p>I was use RAM 80GB machine in Colab.</p>
<pre><code>entity_embedding = {}
for i in range(len(entity_desc)):
    entity = entity_desc['entity'][i]
    text = entity_desc['text'][i]
    entity += ' ' + text
    entity_embedding[entity_desc['entity_id'][i]] = get_word_embedding(entity)
</code></pre>
",Vectorization & Embeddings,calculating embedding overload problem bert trying calculate embedding sentence using bert input sentence bert calculate mean pooling used embedding sentence problem code calculate embedding sentence computational cost high know wrong hope someone help install bert get embedding function data maximum number word text calculate entity text embedding enter image description run code data step overload computer every time run please help wa use ram gb machine colab
How to obtain the [CLS] sentence embedding of multiple sentences successively without facing a RAM crash?,"<p>I would like to obtain the [CLS] token's sentence embedding (as it represents the whole sentence's meaning) using BERT. I have many sentences (about 40) that belong to a Document, and 246 such documents in the form of a dictionary shown below.</p>
<pre><code></code></pre>
<p>{'DocNo': 1,
'Text': 'Learned counsel for the Appellants-Revenue Mr.E.I.Sanmathi submits that he does not press the substantial question of law No.1.',
'Embedding': [],
'Labels': 'ARG_PETITIONER'},</p>
<pre><code>
I have a python list, named rows_list ([]) which contains multiple such dictionaries - for the sentences present in the Documents. 
I need to obtain the sentence embedding for each of these sentences and add them as the value to the 'Embedding' key. 
I am using Google Colab's free version.

</code></pre>
<pre><code>for row in rows_list:
  if row[&quot;DocNo&quot;] != 1:
    tokenizedText = tokenizer(row['Text'], padding='max_length', max_length=512, truncation=True, return_tensors=&quot;pt&quot;)
    outputs = model(**tokenizedText, output_hidden_states=True)
    last_hidden_states = outputs.hidden_states[-1]
    row[&quot;Embedding&quot;] = last_hidden_states[0,0,:]
  else:
    break;
</code></pre>
<pre><code>
This is the code I wrote for doing the same. I first tried to run the for loop as a whole, without the if-else : but came across the error - **Your session crashed after using all available RAM. **

I got the same error even when I wanted to obtain the embeddings for 1 Document. 
How do I ensure that I receive the embeddings for each sentence throughtout all the documents without facing the RAM issue? 
</code></pre>
",Vectorization & Embeddings,obtain cl sentence embedding multiple sentence successively without facing ram crash would like obtain cl token sentence embedding represents whole sentence meaning using bert many sentence belong document document form dictionary shown docno text learned counsel appellant revenue mr e sanmathi submits doe press substantial question law embedding label arg petitioner
how do you get the frequency of the terms generated by tfidf.get_feature_names_out(),"<p>After fitting with tfidf, I'm looking at the features that were generated:</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
corpus = [
    'This is the first document.',
    'This document is the second document.',
    'And this is the third one.',
    'Is this the first document?',
]
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names_out())
</code></pre>
<p>but I want to get the frequency of each term as well</p>
",Vectorization & Embeddings,get frequency term generated tfidf get feature name fitting tfidf looking feature generated want get frequency term well
Why word embedding technique works,"<p>I have look into some word embedding techniques, such as</p>
<ol>
<li>CBOW: from context to single word. Weight matrix produced used as embedding vector</li>
<li>Skip gram: from word to context (from what I see, its acutally word to word, assingle prediction is enough). Again Weight matrix produced used as embedding</li>
</ol>
<p>Introduction to these tools would always quote &quot;cosine similarity&quot;, which says words of similar meanning would convert to similar vector.</p>
<p>But these methods all based on the 'context', account only for words around a target word. I should say they are 'syntagmatic' rather than 'paradigmatic'. So why the close in distance in a sentence indicate close in meaning? I can think of many counter example that frequently occurs</p>
<ol>
<li>&quot;Have a good day&quot;. (good and day are vastly different, though close in distance).</li>
<li>&quot;toilet&quot; &quot;washroom&quot; (two words of similar meaning, but a sentence contains one would unlikely to contain another)</li>
</ol>
<p>Any possible explanation?</p>
",Vectorization & Embeddings,word embedding technique work look word embedding technique cbow context single word weight matrix produced used embedding vector skip gram word context see acutally word word assingle prediction enough weight matrix produced used embedding introduction tool would always quote cosine similarity say word similar meanning would convert similar vector method based context account word around target word say syntagmatic rather paradigmatic close distance sentence indicate close meaning think many counter example frequently occurs good day good day different though close distance toilet washroom two word similar meaning sentence contains one would unlikely contain another possible explanation
Generating embedding for long documents using pre-trained word vectors,"<p>I have a set of pre-trained word embeddings from the Wikipedia corpus. I also have 300 dimension embeddings of Wikipedia article pages. I am looking to build a similarity engine by running a simple cosine similarity algorithm for any new query (long documents) against these pre-trained embeddings. To do this, I want to represent any new input document as a 300d vector using the pre-trained word embeddings and then run cosine similarity against the corpus. How can this be achieved?</p>
",Vectorization & Embeddings,generating embedding long document using pre trained word vector set pre trained word embeddings wikipedia corpus also dimension embeddings wikipedia article page looking build similarity engine running simple cosine similarity algorithm new query long document pre trained embeddings want represent new input document vector using pre trained word embeddings run cosine similarity corpus achieved
How to detect if two news articles have the same topic? (Python semantic similarity),"<p>I'm trying to scrape headlines and body text from articles on a few specific sites, similar to what Google does with Google News.</p>
<p>The problem is that across different sites, they may have articles on the same subject worded slightly differently.</p>
<p>Can anyone tell me what I need to know in order to write a comparison algorithm to auto-detect similar articles? Or, is there any library that can be used for text comparisons and return some type of similarity rating? Solutions that use Python are desired.</p>
",Vectorization & Embeddings,detect two news article topic python semantic similarity trying scrape headline body text article specific site similar google doe google news problem across different site may article subject worded slightly differently anyone tell need know order write comparison algorithm auto detect similar article library used text comparison return type similarity rating solution use python desired
The decoder part in a transformer model,"<p>I'm fairly new to NLP and I was reading a blog explaining the transformer model. I was quite confused about the input/output for the decoder block (attached below). I get that y_true is fed into the decoder during the training step to combine with the output of the encoder block. What I don't get is, if we already know y_true, why run this step to get the output probability? I just don't quite get the relationship between the bottom right &quot;Output Embedding&quot; and the top right &quot;Output Probabilities&quot;. When we use the model, we wouldn't really have y_true, do we just use y_pred and feed them into the decoder instead? This might be a noob question. Thanks in advance.</p>
<p><a href=""https://i.sstatic.net/nQ2f5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nQ2f5.png"" alt=""The Decoder Block of the Transformer Architecture
Taken from “Attention Is All You Need“"" /></a></p>
",Vectorization & Embeddings,decoder part transformer model fairly new nlp wa reading blog explaining transformer model wa quite confused input output decoder block attached get true fed decoder training step combine output encoder block get already know true run step get output probability quite get relationship bottom right output embedding top right output probability use model really true use pred feed decoder instead might noob question thanks advance
Can I use bag of words to find cosine similarity between vectors?,"<p>I have a Bag of Words dataset like this:</p>
<pre><code>Item A B C D 
abc  1 0 0 1
pqr  0 0 1 1
xyz  0 1 0 0
</code></pre>
<p>and so on.
Is there a way I can find the pairwise cosine similarity in this dataset?
What I see on scikit is - converting it to a <code>tfidftransformer</code> version, and then finding cosine similarity.</p>
<p>The final aim is to compare the cosine similarity of my raw data vs the latent data (after modelling the data) and comparing the difference between the two.</p>
",Vectorization & Embeddings,use bag word find cosine similarity vector bag word dataset like way find pairwise cosine similarity dataset see scikit converting version finding cosine similarity final aim compare cosine similarity raw data v latent data modelling data comparing difference two
Scikit Learn TfidfVectorizer : How to get top n terms with highest tf-idf score,"<p>I am working on keyword extraction problem. Consider the very general case</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')

t = &quot;&quot;&quot;Two Travellers, walking in the noonday sun, sought the shade of a widespreading tree to rest. As they lay looking up among the pleasant leaves, they saw that it was a Plane Tree.

&quot;How useless is the Plane!&quot; said one of them. &quot;It bears no fruit whatever, and only serves to litter the ground with leaves.&quot;

&quot;Ungrateful creatures!&quot; said a voice from the Plane Tree. &quot;You lie here in my cooling shade, and yet you say I am useless! Thus ungratefully, O Jupiter, do men receive their blessings!&quot;

Our best blessings are often the least appreciated.&quot;&quot;&quot;

tfs = tfidf.fit_transform(t.split(&quot; &quot;))
str = 'tree cat travellers fruit jupiter'
response = tfidf.transform([str])
feature_names = tfidf.get_feature_names()

for col in response.nonzero()[1]:
    print(feature_names[col], ' - ', response[0, col])
</code></pre>
<p>and this gives me</p>
<pre><code>  (0, 28)   0.443509712811
  (0, 27)   0.517461475101
  (0, 8)    0.517461475101
  (0, 6)    0.517461475101
tree  -  0.443509712811
travellers  -  0.517461475101
jupiter  -  0.517461475101
fruit  -  0.517461475101
</code></pre>
<p>which is good. For any new document that comes in, is there a way to get the top n terms with the highest tfidf score?</p>
",Vectorization & Embeddings,scikit learn tfidfvectorizer get top n term highest tf idf score working keyword extraction problem consider general case give good new document come way get top n term highest tfidf score
What is “Ensemble of 5 reversed LSTMs” in seq2seq,"<p>While reading the seq2seq <a href=""https://arxiv.org/abs/1409.3215"" rel=""nofollow noreferrer"">paper</a> (Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.), the authors said they used a 4-layer LSTMs in <em>section 3.4 Training Details</em><sup>1</sup>. However, in <em>section 3.6 Experimental Results</em><sup>2</sup>, they said they used an ensemble of 5 reversed LSTMs.</p>
<p>I'm pretty consfused about this, since I think <em>an ensemble of 5 reversed LSTMs</em> menas a 5-layer LSTMs, which conflicts with the section 3.4. I don't know whether this is a typo or I misundertand the meaning of the 5 reversed LSTMs.</p>
<p>[1]:</p>
<blockquote>
<p>We used deep LSTMs with <strong>4 layers</strong>,
with 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary
of 160,000 and an output vocabulary of 80,000.</p>
</blockquote>
<p>[2]:
<a href=""https://i.sstatic.net/2s0jj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2s0jj.png"" alt=""enter image description here"" /></a></p>
<p>I have Googled：</p>
<ul>
<li>&quot;5 reversed LSTMs&quot; meaning</li>
<li>&quot;ensemble 5 reversed LSTMs&quot; meaning</li>
<li>&quot;Ensemble of 5 reversed LSTMs&quot; 4 layers</li>
</ul>
<p>and found nothing help, and the repos in Github don't have the relevant issues/questions.</p>
",Vectorization & Embeddings,ensemble reversed lstms seq seq reading seq seq paper sutskever vinyals le q v sequence sequence learning neural network advance neural information processing system author said used layer lstms section training detail however section experimental result said used ensemble reversed lstms pretty consfused since think ensemble reversed lstms menas layer lstms conflict section know whether typo misundertand meaning reversed lstms used deep lstms layer cell layer dimensional word embeddings input vocabulary output vocabulary googled reversed lstms meaning ensemble reversed lstms meaning ensemble reversed lstms layer found nothing help repos github relevant issue question
How to extend the vocabulary of a pretrained transformer model?,"<p>I would like to extend a <a href=""https://huggingface.co/cross-encoder/nli-deberta-v3-base"" rel=""nofollow noreferrer"">zero-shot text classification (NLI) model</a>'s vocabulary, to include domain-specific vocabulary or just to keep it up-to-date. For example, I would like the model to know the names of the latest COVID-19 variants are related to the topic 'Healthcare'.</p>
<p>I've added the tokens to the tokenizer and resized the token embeddings. However, I don't know how to finetune the weights in the embedding layer, as suggested <a href=""https://github.com/huggingface/transformers/issues/1413#issuecomment-608061516"" rel=""nofollow noreferrer"">here</a>.</p>
<p>To do the finetuning, can I use simply use texts containing a mixture of new vocabulary and existing vocabulary, and have the tokenizer recognise the relations between tokens through co-occurrences in an unsupervised fashion?</p>
<p>Any help is appreciated, thank you!</p>
",Vectorization & Embeddings,extend vocabulary pretrained transformer model would like extend zero shot text classification nli model vocabulary include domain specific vocabulary keep date example would like model know name latest covid variant related topic healthcare added token tokenizer resized token embeddings however know finetune weight embedding layer suggested finetuning use simply use text containing mixture new vocabulary existing vocabulary tokenizer recognise relation token co occurrence unsupervised fashion help appreciated thank
Does NLP Transformer has backpropagation and how BERT has its word embedding?,"<p>I was reading Attention all you need papers and i have not get any idea how the weights are updated in the Transformer base architecture is there any Backpropagation ? normally yes for the model to learn and update his weights but could any one confirm me that and explain it to me if possible ?</p>
<p>I know about the sum up of the 3 embeddings in the input of the Transformer (sentece embedding, postional embedding and wordpiece embedding) howerver, could any one explain to me what'is exactly a wordpiece embedding ? all i know it has 30k vocabulary tokens but i did not know how it trained, is it trained by a Transformer?
Thanks !</p>
",Vectorization & Embeddings,doe nlp transformer ha backpropagation bert ha word embedding wa reading attention need paper get idea weight updated transformer base architecture backpropagation normally yes model learn update weight could one confirm explain possible know sum embeddings input transformer sentece embedding postional embedding wordpiece embedding howerver could one explain exactly wordpiece embedding know ha k vocabulary token know trained trained transformer thanks
sentence transformer using huggingface/transformers pre-trained model vs SentenceTransformer,"<p><a href=""https://www.sbert.net/examples/training/sts/README.html"" rel=""nofollow noreferrer"">This</a> page has two scripts</p>
<p>When should one use 1st method shown below vs 2nd? As <code>nli-distilroberta-base-v2</code> trained specially for finding sentence embedding wont that always be better than the first method?</p>
<p><code>training_stsbenchmark.py1</code>  -</p>
<pre><code>from sentence_transformers import SentenceTransformer,  LoggingHandler, losses, models, util
#You can specify any huggingface/transformers pre-trained model here, for example, bert-base-uncased, roberta-base, xlm-roberta-base
model_name = sys.argv[1] if len(sys.argv) &gt; 1 else 'distilbert-base-uncased'

# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings
word_embedding_model = models.Transformer(model_name)

# Apply mean pooling to get one fixed sized sentence vector
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),
                               pooling_mode_mean_tokens=True,
                               pooling_mode_cls_token=False,
                               pooling_mode_max_tokens=False)

model = SentenceTransformer(modules=[word_embedding_model, pooling_model])
</code></pre>
<p><code>training_stsbenchmark_continue_training.py</code> -</p>
<pre><code>from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util, InputExample
model_name = 'nli-distilroberta-base-v2'
model = SentenceTransformer(model_name)
</code></pre>
",Vectorization & Embeddings,sentence transformer using huggingface transformer pre trained model v sentencetransformer page ha two script one use st method shown v nd trained specially finding sentence embedding wont always better first method
Errors working with Spacy document vectors in Pyspark dataframes,"<p>I am having an incredibly bad time working with document vectors produced by the Spacy pre-trained large model in a Pyspark environment on AWS. The problems specifically start when I put the document vectors into a dataframe.</p>
<p>For example, this code works just fine for me:</p>
<pre><code># Load infrastructure libraries
import pandas as pd
import numpy as np

# Load NLP libraries and tools
import spacy

# Prepare the Spacy NLP parsers
nlp = spacy.load('en_core_web_lg')

# Load Spark
from pyspark.sql.session import SparkSession
import pyspark.sql.functions as F
from pyspark.sql import types as T
from pyspark.ml.linalg import Vectors
from pyspark.ml.functions import vector_to_array

# Setup spark sesssion
spark = SparkSession.builder.appName(&quot;Spacy Test&quot;).getOrCreate()

# Create a test document vector
vec = nlp(&quot;I really like cheese&quot;).vector
vec = Vectors.dense(vec)
vec
</code></pre>
<p>And the output:</p>
<pre><code>DenseVector([-0.7719, -0.152, -2.5687, -2.9057, -4.3302, -0.2236, 1.4182, 4.7625, -5.4947, 4.0054, 5.4234, 0.4195, -2.1278, -0.2198, 3.733, -4.3451, 1.6354, -4.2959, -1.9893, -1.0955, 1.2923, 2.5803, 0.8446, -5.6266, -2.1173, -3.1521, -3.2044, 1.3603, -1.8462, 0.6012, -0.0218, -3.5514, 0.0648, 1.1473, 0.8563, -3.0121, -1.114, 0.7547, 3.3102, 2.2526, -1.9224, 0.9549, 0.4257, -3.7478, 3.382, 2.2748, -2.0761, -4.0411, -2.0196, 2.7873, 1.1951, 0.0311, 1.0897, -3.7514, -2.9269, -0.8296, 0.446, 3.5525, 3.3304, 1.706, 6.1961, -1.7517, 0.5205, -0.1543, -2.7567, -0.3654, -4.5379, -3.7601, 0.6534, 3.3904, 0.0462, 2.9809, 2.0856, -0.6889, 5.0641, 1.9436, -2.7128, -1.2188, -1.1388, -4.5075, -3.0456, 1.5704, 6.1288, 0.1904, 1.4656, 0.0181, 1.4148, -1.4573, -0.9094, -1.7613, -2.7317, 0.4354, 2.9197, -6.9938, -0.6905, -3.1972, 2.4815, -4.285, 1.0154, -2.5749, 0.8907, 1.9208, -0.1247, 1.3449, -4.1218, 2.466, -3.8912, 1.6637, -2.4908, -0.7045, -0.8058, -3.7729, 3.0547, -1.9908, -0.085, 1.4266, -0.9306, -2.9857, 0.2251, -0.5722, -2.8197, -0.6044, -4.2039, 2.7789, -0.9341, -0.7502, 1.7431, -3.3388, 0.3382, -0.6125, 0.0842, 1.545, 1.432, -1.2881, -0.123, 0.8724, -4.9354, -2.8043, 5.0844, -2.8918, -2.777, -0.0504, 0.5404, -0.1609, -6.5542, -0.9069, 0.1638, 0.4534, -3.2973, -3.5313, -2.8212, 0.9449, -3.5691, 0.2708, -2.7588, 2.1502, 6.3973, 1.1362, -1.0846, -1.0468, -0.6148, -3.8309, -3.2811, 0.1241, -0.4777, 0.6669, -0.9383, -0.869, -2.0688, 2.1084, 2.3942, -0.8358, -0.3179, 3.1491, 1.8569, -3.479, -2.1367, -0.7273, 1.0882, -0.9835, -1.2419, 0.5295, 5.0464, -1.2607, -2.0058, 2.4997, 0.7322, -5.4079, 2.3466, 2.298, -2.1624, 4.132, -2.6485, 2.1919, -1.6362, -2.5953, -5.571, 0.4838, -1.8915, 5.4007, -1.4195, -0.3522, -3.0002, -1.0116, -0.3323, 2.2401, 2.1777, -0.616, -0.0131, -0.2913, 1.5619, -1.0237, 1.9203, 1.3495, -3.4587, 0.6858, -1.129, 0.58, -1.5449, -0.3716, -1.0494, -1.689, -1.4535, 4.1873, 2.1134, -2.9858, -1.2115, 1.3913, 3.0331, 2.1075, -7.7124, -2.9558, 0.8318, -1.1737, -1.575, 0.6175, -2.9589, 6.8081, 0.4126, -1.2054, -5.8529, -1.4182, 1.9067, 2.4857, 1.5191, -2.5983, 0.8784, -0.2694, 3.1889, 0.6379, -3.4795, 4.4023, -7.337, 0.9995, -0.8919, -6.4528, -1.2682, 2.664, -3.828, 3.5323, 3.0402, 3.2275, 1.1329, 2.3996, 2.9425, -0.3751, 1.7857, 1.2017, -6.3812, 1.7977, 0.4254, -2.3142, -2.666, 1.8858, -0.5762, -2.9764, 2.7129, -2.1113, 0.3109, 1.2368, -4.8247, -5.0767, 1.0253, 0.2394, 3.9423, -1.2326, 0.3167, -0.0368, -3.8825, 2.8392, 2.399, -1.3454, 2.2132, -2.8337, -2.822, -0.3926, 3.3747, 5.8344, -4.1289, 2.2593])
</code></pre>
<p>However, my next step is to derive the document vectors for a target field in a dataframe. This target field contains a series of documents, with each document stored as a string, one document per row. Here's an analogous example:</p>
<pre><code>data = [
 (&quot;1&quot;, &quot;I really like cheese&quot;, 0.35),
 (&quot;1&quot;, &quot;I don't really like cheese&quot;, 0.10),
 (&quot;1&quot;, &quot;I absolutely love cheese&quot;, 0.55)
]

from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField
schema = StructType([
    StructField(&quot;id&quot;,StringType(),True), 
    StructField(&quot;target&quot;,StringType(),True),
    StructField(&quot;pct&quot;,FloatType(),True),
])

df = spark.createDataFrame(data=data,schema=schema)
df.show()


root
 |-- id: string (nullable = true)
 |-- target: string (nullable = true)
 |-- pct: float (nullable = true)

+---+--------------------+----+
| id|              target| pct|
+---+--------------------+----+
|  1|I really like cheese|0.35|
|  1|I don't really li...| 0.1|
|  1|I absolutely love...|0.55|
+---+--------------------+----+
</code></pre>
<p>The &quot;pct&quot; column in my dataframe is there because I eventually want to multiply each embedding vector by this column, but we don't need to worry about that right now. All of the code up to this point works just fine.</p>
<p>My next step is to get the embedding vectors for each document:</p>
<pre><code>embedding_udf = F.udf(lambda x: get_embeddings(x))
def get_embeddings(x):
    # Return
    return nlp(x).vector

toDense_udf = F.udf(lambda v: toDense(v))
def toDense(v):
    v = Vectors.dense(v)
    return v

# Get the embedding vectors for each document in the dataframe
df = df.withColumn(&quot;embedding&quot;, embedding_udf(F.col(&quot;target&quot;))).select(
    &quot;id&quot;,
    &quot;embedding&quot;,
    &quot;pct&quot;
)

# Convert the embeddings to dense vectors
df = df.withColumn(&quot;embedding&quot;, toDense_udf(F.col(&quot;embedding&quot;)))
</code></pre>
<p>And this is where I start to have problems. Now, this code runs just fine, but I cannot do any kind of analysis or debugging because every attempt to examine this dataframe after the vectors are generating is failing.</p>
<p>Running:</p>
<pre><code>df.show()
</code></pre>
<p>Throws this error:</p>
<pre><code>An error was encountered:
An error occurred while calling o127.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 7) (ip-10-0-0-162.ec2.internal executor 9): java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --system-site-packages virtualenv_application_1663597828866_0004_0
    at org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:120)
    at org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:78)
    at org.apache.spark.api.python.PythonWorkerFactory.&lt;init&gt;(PythonWorkerFactory.scala:94)
    at org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:125)
    at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
    at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:125)
    at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)
    at org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)
    at org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:133)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1474)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2610)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2559)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2558)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2558)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1200)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1200)
    at scala.Option.foreach(Option.scala:407)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1200)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2798)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2740)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2729)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:978)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2215)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2255)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:519)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)
    at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
    at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3779)
    at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2769)
    at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3770)
    at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
    at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
    at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
    at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
    at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3768)
    at org.apache.spark.sql.Dataset.head(Dataset.scala:2769)
    at org.apache.spark.sql.Dataset.take(Dataset.scala:2976)
    at org.apache.spark.sql.Dataset.getRows(Dataset.scala:289)
    at org.apache.spark.sql.Dataset.showString(Dataset.scala:328)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --system-site-packages virtualenv_application_1663597828866_0004_0
    at org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:120)
    at org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:78)
    at org.apache.spark.api.python.PythonWorkerFactory.&lt;init&gt;(PythonWorkerFactory.scala:94)
    at org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:125)
    at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
    at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:125)
    at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)
    at org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)
    at org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:133)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1474)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more

Traceback (most recent call last):
  File &quot;/mnt1/yarn/usercache/livy/appcache/application_1663597828866_0004/container_1663597828866_0004_01_000001/pyspark.zip/pyspark/sql/dataframe.py&quot;, line 495, in show
    print(self._jdf.showString(n, 20, vertical))
  File &quot;/mnt1/yarn/usercache/livy/appcache/application_1663597828866_0004/container_1663597828866_0004_01_000001/py4j-0.10.9.3-src.zip/py4j/java_gateway.py&quot;, line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File &quot;/mnt1/yarn/usercache/livy/appcache/application_1663597828866_0004/container_1663597828866_0004_01_000001/pyspark.zip/pyspark/sql/utils.py&quot;, line 111, in deco
    return f(*a, **kw)
  File &quot;/mnt1/yarn/usercache/livy/appcache/application_1663597828866_0004/container_1663597828866_0004_01_000001/py4j-0.10.9.3-src.zip/py4j/protocol.py&quot;, line 328, in get_return_value
    format(target_id, &quot;.&quot;, name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o127.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 7) (ip-10-0-0-162.ec2.internal executor 9): java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --system-site-packages virtualenv_application_1663597828866_0004_0
    at org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:120)
    at org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:78)
    at org.apache.spark.api.python.PythonWorkerFactory.&lt;init&gt;(PythonWorkerFactory.scala:94)
    at org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:125)
    at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
    at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:125)
    at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)
    at org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)
    at org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:133)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1474)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2610)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2559)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2558)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2558)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1200)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1200)
    at scala.Option.foreach(Option.scala:407)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1200)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2798)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2740)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2729)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:978)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2215)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2255)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:519)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)
    at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
    at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3779)
    at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2769)
    at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3770)
    at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
    at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
    at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
    at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
    at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3768)
    at org.apache.spark.sql.Dataset.head(Dataset.scala:2769)
    at org.apache.spark.sql.Dataset.take(Dataset.scala:2976)
    at org.apache.spark.sql.Dataset.getRows(Dataset.scala:289)
    at org.apache.spark.sql.Dataset.showString(Dataset.scala:328)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --system-site-packages virtualenv_application_1663597828866_0004_0
    at org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:120)
    at org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:78)
    at org.apache.spark.api.python.PythonWorkerFactory.&lt;init&gt;(PythonWorkerFactory.scala:94)
    at org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:125)
    at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
    at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:125)
    at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)
    at org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)
    at org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:133)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1474)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
</code></pre>
<p>In fact, every attempt I make to examine any of individual vectors in the dataframe fail with a very similar error. I've tried using <code>collect()</code>, I've tried converting the dataframe to a pandas dataframe, but everything fails with some kind of verbose error. So my first question is, <strong>Why does every attempt to view the embedding vectors in a dataframe fail?</strong> I can't even convert this basic 3-row dataframe to a pandas dataframe without errors. What am I doing wrong?</p>
<p>And secondly, the data type of the field containing the vectors is throwing me off because the <code>embedding</code> field is apparently being stored as a string? For example,</p>
<p>Running:</p>
<pre><code>df.dtypes
</code></pre>
<p>Yields:</p>
<pre><code>[('id', 'string'), ('embedding', 'string'), ('pct', 'float')]
</code></pre>
<p>So my second question is, <strong>why does the column containing the document vectors have a string datatype?</strong> This doesn't seem right, especially since the data type of an individual string vector is of type numpy.ndarray:</p>
<pre><code>type(nlp(&quot;I really like cheese&quot;).vector)
&lt;class 'numpy.ndarray'&gt;
</code></pre>
<p>From what I can tell, there is virtually no documentation for integrating Spacy with Pyspark in this way, even though all of these operations should be quite simple in a regular python environment. Any insights would be greatly appreciated.</p>
<p>My environment details:</p>
<pre><code>Release label:emr-6.7.0
Hadoop distribution:Amazon 3.2.1
Applications:Spark 3.2.1, Livy 0.7.1, JupyterEnterpriseGateway 2.1.0
</code></pre>
<p>Thanks!</p>
",Vectorization & Embeddings,error working spacy document vector pyspark dataframes incredibly bad time working document vector produced spacy pre trained large model pyspark environment aws problem specifically start put document vector dataframe example code work fine output however next step derive document vector target field dataframe target field contains series document document stored string one document per row analogous example pct column dataframe eventually want multiply embedding vector column need worry right code point work fine next step get embedding vector document start problem code run fine kind analysis debugging every attempt examine dataframe vector generating failing running throw error fact every attempt make examine individual vector dataframe fail similar error tried using tried converting dataframe panda dataframe everything fails kind verbose error first question doe every attempt view embedding vector dataframe fail even convert basic row dataframe panda dataframe without error wrong secondly data type field containing vector throwing field apparently stored string example running yield second question doe column containing document vector string datatype seem right especially since data type individual string vector type numpy ndarray tell virtually documentation integrating spacy pyspark way even though operation quite simple regular python environment insight would greatly appreciated environment detail thanks
How to appropiately create a matrix with intersection count between multiple list in python,"<p>Good day,</p>
<p>As trying to learn NLP, I was making an Exploratory Data Analysis.
After some data cleaning, I found myself needing to compare several list which contain the most used words in different topics. For that, I need to display a matrix showing the number of elements that every combination of 2 of those list have in common.</p>
<p>For instance, let´s say that I have the following 3 list of 4 elements, where every element of a list must be distinct:</p>
<p>a = ['rabbit', 'fox', 'cat', 'dog']</p>
<p>b = ['snake', 'tiger', 'fox', 'cat']</p>
<p>c = ['lion', 'tiger', 'fox', 'cow']</p>
<p>And I want to obtain a 3x3 matrix like this one:</p>
<p>4   2    1</p>
<p>2   4    2</p>
<p>1   2    4</p>
<p>Ie, the element (1,2) is equal to the element (2,1), being 2, because the list a and b have 2 words in common ('fox' and 'cat').
Then, the matrix will be symmetric and the elements within the diagonal must be the lenght of the vector (4).</p>
<p>My goal is trying to visualize how similar are those different topics based on the amount of top 4 most used words in common when people speak about them.</p>
<p>I think that something like len(set(a).intersection(b)) could be an option.
Since I have more topics in my actual dataset, and the matrix will be 50*50 (and with the top 10 most used words for each category), I was considering if maybe there is a more efficient way to do it than iterating through the lists and calculating the length of the set intersection.</p>
<p>I also think that there must be a better implementation of this. I know that rewritting this on numpy must be the first step for making it more optimal. But aside from that, I was supposing this kind of need will be recurrent and probably already implemented on pandas, nltk, etc. However, I am not being able to find anything related. I am also concerned about not being considering a better visualization technique for this goal.</p>
<p>Thanks in advance and wish you a great day!</p>
",Vectorization & Embeddings,appropiately create matrix intersection count multiple list python good day trying learn nlp wa making exploratory data analysis data cleaning found needing compare several list contain used word different topic need display matrix showing number element every combination list common instance let say following list element every element list must distinct rabbit fox cat dog b snake tiger fox cat c lion tiger fox cow want obtain x matrix like one ie element equal element list b word common fox cat matrix symmetric element within diagonal must lenght vector goal trying visualize similar different topic based amount top used word common people speak think something like len set intersection b could option since topic actual dataset matrix top used word category wa considering maybe efficient way iterating list calculating length set intersection also think must better implementation know rewritting numpy must first step making optimal aside wa supposing kind need recurrent probably already implemented panda nltk etc however able find anything related also concerned considering better visualization technique goal thanks advance wish great day
how to perform cosine similarity based semantic search in elastic search query on a text field?,"<p>I am performing  a match on a text field(skills). I don't want a exact match , instead i want cosine similarity based search on the field.</p>
<pre><code>GET 2/_search
{
  &quot;_source&quot;: [&quot;Skills&quot;], 
  &quot;query&quot;: {
    &quot;function_score&quot;: {
      &quot;query&quot;: {
        &quot;match&quot;: {
          &quot;Job_Group&quot;: &quot;sales&quot;
        }
      },
      &quot;functions&quot;: [
        
        {
          &quot;filter&quot;: {
            &quot;match&quot;:{
              &quot;Skills&quot;:&quot;Designation&quot;
            }
          },
          &quot;weight&quot;: 15
        }
      ]
    }
    }
}
</code></pre>
<p>The above query is for exact match. How do i include some sort of semantic search(Cosine similarity based in the query on skills field). The skills field is a free text field, so i want matching to happen based on their semantic meaning also. Example--- skills -Communication &amp; talking should reflect some sort of similarity and boost the score.</p>
",Vectorization & Embeddings,perform cosine similarity based semantic search elastic search query text field performing match text field skill want exact match instead want cosine similarity based search field query exact match include sort semantic search cosine similarity based query skill field skill field free text field want matching happen based semantic meaning also example skill communication talking reflect sort similarity boost score
Gensim Word2Vec exhausting iterable,"<p>I'm getting the following prompt when calling model.train() from gensim word2vec</p>
<pre><code>INFO : EPOCH 0: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s
</code></pre>
<p>The only solutions I found on my search for an answer point to the <em>itarable</em> vs <em>iterator</em> difference, and at this point, I tried everything I could to solve this on my own, currently, my code looks like this:</p>
<pre><code>class MyCorpus:
    def __init__(self, corpus):
        self.corpus = corpus.copy()

    def __iter__(self):
        for line in self.corpus:
            x = re.sub(&quot;(&lt;br ?/?&gt;)|([,.'])|([^ A-Za-z']+)&quot;, '', line.lower())
            yield utils.simple_preprocess(x)

sentences = MyCorpus(corpus)
w2v_model = Word2Vec(
    sentences = sentences,
    vector_size = w2v_size, 
    window = w2v_window, 
    min_count = w2v_min_freq, 
    workers = -1
    )

</code></pre>
<p>The <code>corpus</code> variable is a list containing sentences, and each sentence is a string.</p>
<p>I tried the numerous &quot;tests&quot; to see if my class is indeed iterable, like:</p>
<pre><code>    print(sum(1 for _ in sentences))
    print(sum(1 for _ in sentences))
    print(sum(1 for _ in sentences))
</code></pre>
<p>For instance, all of them suggest that my class is iterable, so at this point, I think the problem must be something else.</p>
",Vectorization & Embeddings,gensim word vec exhausting iterable getting following prompt calling model train gensim word vec solution found search answer point itarable v iterator difference point tried everything could solve currently code look like variable list containing sentence sentence string tried numerous test see class indeed iterable like instance suggest class iterable point think problem must something else
Word2Vec on sets of integers,"<p>The purpose (and power) of Word2Vec is the context of the language (i.e., given a sentence like &quot;I ate <em>blankword</em> for breakfast&quot; we can assume that only a small fraction of our corpus, like eggs, salad etc.. might fit there).</p>
<p>I wonder if this idiom still works when we remove the context of the language and consider Word2Vec on sets. For example, consider the case where each person has some vector of integers associated to it, like [301, 285, 417,..], where this vector describes properties of that person. Clearly, the order of the properties does not matter.
We can assume that each person has at most 70 such properties. And that the entire corpus has about 500 properties. The goal is given some person, detect the list of k most similar persons, where k is some integer parameter.</p>
<p>Can we use Word2Vec for this task? On one hand, given a vector of properties, it seems that we can use Word2Vec with pretty large window size. On the other hand, this kind of contradicts the assumption that <strong>only a small fraction of our corpus fits for any given word</strong>.</p>
",Vectorization & Embeddings,word vec set integer purpose power word vec context language e given sentence like ate blankword breakfast assume small fraction corpus like egg salad etc might fit wonder idiom still work remove context language consider word vec set example consider case person ha vector integer associated like vector describes property person clearly order property doe matter assume person ha property entire corpus ha property goal given person detect list k similar person k integer parameter use word vec task one hand given vector property seems use word vec pretty large window size hand kind contradicts assumption small fraction corpus fit given word
[Keras][Embedding] How do I expand the vocabulary size of a pre trained embedding,"<p>I have a pre-trained Keras model and there's a word embedding [1000 vocabulary * 200 dimensions] inside of the model.</p>

<p>Now I want to load it back to memory and continuous training it with new data. The vocabulary size increased because of the new data.</p>

<p>I am wondering if it's possible to replace this embedding with a bigger vocabulary size [2000 vocabulary * 200 dimensions] but keeping other parameter weights unchanged for the entire model.</p>
",Vectorization & Embeddings,kera embedding expand vocabulary size pre trained embedding pre trained kera model word embedding vocabulary dimension inside model want load back memory continuous training new data vocabulary size increased new data wondering possible replace embedding bigger vocabulary size vocabulary dimension keeping parameter weight unchanged entire model
Repetitive word predictions in RNN,"<p>Hello dear community,</p>
<p>I am training a Seq2Seq model to generate a question based on a graph. Both train and val loss are converging, but the generated questions (on either train or test set) are nonsense and contain mostly repetition of tokens. I tried various hyper parameters and double checked input and outputs tensors.</p>
<p>Something that I do find odd is that the output <code>out</code> (see below) starts containing some values, which I consider as unusually high. This starts happening around half way through the first epoch:</p>
<pre><code>Out:  tensor([[  0.2016, 103.7198,  90.4739,  ...,   0.9419,   0.4810,  -0.2869]]
</code></pre>
<p>My guess for that is vanishing/exploding gradients, which I thought I had handeled by gradient clipping, but now I am not sure about this:</p>
<pre><code>for p in model_params:
        p.register_hook(lambda grad: torch.clamp(
            grad, -clip_value, clip_value))
</code></pre>
<p>Below are the training curves (10K samples, batch size=128, lr=0.065, lr_decay=0.99, dropout=0.25)
<a href=""https://i.sstatic.net/pyQtN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pyQtN.png"" alt=""Loss Curves"" /></a></p>
<p><strong>Encoder</strong> (a GNN, learning node embeddings of the input graph, that consists of around 3-4 nodes and edges. A single graph embedding is obtained by pooling the node embeddings and feeding them as the initial hidden state to the Decoder):</p>
<pre><code>class QuestionGraphGNN(torch.nn.Module):
    def __init__(self,
                 in_channels,
                 hidden_channels,
                 out_channels,
                 dropout,
                 aggr='mean'):
        super(QuestionGraphGNN, self).__init__()
        nn1 = torch.nn.Sequential(
            torch.nn.Linear(in_channels, hidden_channels),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_channels, in_channels * hidden_channels))
        self.conv = NNConv(in_channels, hidden_channels, nn1, aggr=aggr)
        self.lin = nn.Linear(hidden_channels, out_channels)
        self.dropout = dropout

    def forward(self, x, edge_index, edge_attr):
        x = self.conv(x, edge_index, edge_attr)
        x = F.leaky_relu(x)
        x = F.dropout(x, p=self.dropout)
        x = self.lin(x)
        return x
</code></pre>
<p><strong>Decoder</strong> (The <code>out</code> vector from above is printed in the forward() function):</p>
<pre><code>class DecoderRNN(nn.Module):
    def __init__(self,
                 embedding_size,
                 output_size,
                 dropout):
        super(DecoderRNN, self).__init__()
        self.output_size = output_size
        self.dropout = dropout

        self.embedding = nn.Embedding(output_size, embedding_size)
        self.gru1 = nn.GRU(embedding_size, embedding_size)
        self.gru2 = nn.GRU(embedding_size, embedding_size)
        self.gru3 = nn.GRU(embedding_size, embedding_size)
        self.out = nn.Linear(embedding_size, output_size)
        self.logsoftmax = nn.LogSoftmax(dim=1)

    def forward(self, inp, hidden):
        output = self.embedding(inp).view(1, 1, -1)
        output = F.leaky_relu(output)

        output = F.dropout(output, p=self.dropout)
        output, hidden = self.gru1(output, hidden)

        output = F.dropout(output, p=self.dropout)
        output, hidden = self.gru2(output, hidden)
        output, hidden = self.gru3(output, hidden)

        out = self.out(output[0])
        print(&quot;Out: &quot;, out)
        output = self.logsoftmax(out)
        return output, hidden
</code></pre>
<p>I am using PyTorchs <code>NLLLoss()</code>.
Optimizer is SGD.
I call <code>optimizer.zero_grad()</code> right before the backward and optimizer step and I switch the training/evaluation mode for training, evaluation and testing.</p>
<p>What are your thoughts on this?</p>
<p>Thank you very much!</p>
<p><strong>EDIT</strong></p>
<p><em>Dimensions of the Encoder</em>:</p>
<p><code>in_channels</code>=301 (This is the size of the initial node embeddings)</p>
<p><code>hidden_channels</code>=256</p>
<p><code>out_channels</code>=301 (This will also be the size of the final graph embedding, after mean pooling the node embeddings)</p>
<p><em>Dimensions of the Decoder</em>:</p>
<p><code>embedding_size</code>=301 (the size of the previously pooled graph embedding)</p>
<p><code>output_size</code>=number of words in my vocabulary. In the training above around 1.2K</p>
<p>I am using top-k sampling and my train loop follows the NMT Tutorial <a href=""https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#training-the-model"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#training-the-model</a>). Similarily, my translation function, that takes the data of a single graph, decodes a question as such:</p>
<pre><code>def translate(self, data):
    # Get node embeddings of the input graph
    h = self.encoder(data.node_embeddings,
                     data.edge_index, data.edge_embeddings)

    # Pool node embeddings into single graph embedding
    graph_embedding = self.get_graph_embeddings(h, data.graph_dict)

    # Pass graph embedding through decoder
    self.encoder.eval()
    self.decoder.eval()
    with torch.no_grad():
        # Initialize first input and hidden state
        decoder_input = decoder_input = torch.tensor(
            [[self.vocab.SOS['idx']]], device=self.device)
        decoder_hidden = graph_embedding.view(1, 1, -1)

        decoder_tokens = []
        for di in range(self.dec_max_length):
            decoder_output, decoder_hidden = self.decoder(
                decoder_input, decoder_hidden)
            topv, topi = decoder_output.data.topk(1)
            if topi.item() == self.vocab.EOS['idx']:
                break
            else:
                word = self.vocab.index2word[topi.item()]
                word = word.upper(
                ) if word == self.vocab.UNK['token'].lower() else word
                decoder_tokens.append(word)
            decoder_input = topi.squeeze().detach()

        return decoder_tokens
</code></pre>
<p>Also: At times, the <code>output</code>-vector of the final gru layer (<code>self.gru3(...)</code>) inside the forward() function (5th line from the bottom) outputs a lot of values being (close to) 1 and -1. I suppose these might otherwise be a lot higher/lower without clipping. This might be alright, but seems unusual to me. An example:</p>
<pre><code>tensor([[[-0.9984, -0.9950,  1.0000, -0.9889, -1.0000, -0.9770, -0.0299,
          -0.9996,  0.9996,  1.0000, -0.0176, -0.5815, -0.9998, -0.0265,
          -0.1471,  0.9998, -1.0000, -0.2356,  0.9964,  0.9936, -0.9998,
           0.0652, -0.9999,  0.9999, -1.0000, -0.9998, -0.9999,  0.9998,
          -1.0000, -0.9997,  0.9850,  0.9994, -0.9998, -1.0000, -1.0000,
           0.9977,  0.9015, -0.9982,  1.0000,  0.9980, -1.0000,  0.9859,
           0.6670,  0.9998,  0.3827,  0.9999,  0.9953, -0.9989,  0.1287,
           1.0000,  1.0000, -1.0000,  0.9778,  1.0000,  1.0000, -0.9907, ...
</code></pre>
",Vectorization & Embeddings,repetitive word prediction rnn hello dear community training seq seq model generate question based graph train val loss converging generated question either train test set nonsense contain mostly repetition token tried various hyper parameter double checked input output tensor something find odd output see start containing value consider unusually high start happening around half way first epoch guess vanishing exploding gradient thought handeled gradient clipping sure training curve k sample batch size lr lr decay dropout encoder gnn learning node embeddings input graph consists around node edge single graph embedding obtained pooling node embeddings feeding initial hidden state decoder decoder vector printed forward function using pytorchs optimizer sgd call right backward optimizer step switch training evaluation mode training evaluation testing thought thank much edit dimension encoder size initial node embeddings also size final graph embedding mean pooling node embeddings dimension decoder size previously pooled graph embedding number word vocabulary training around k using top k sampling train loop follows nmt tutorial similarily translation function take data single graph decodes question also time vector final gru layer inside forward function th line bottom output lot value close suppose might otherwise lot higher lower without clipping might alright seems unusual example
How to create a glove embeddings?,"<p>Python 3.9.6</p>
<p>There is a GenSym to work with Word2vek, which creates the desired mapping of words into a vector space. As I understand it, Glove is an optimized version of Word2Vec, which exactly converges on a smaller corpus of words. But I didn't find a technique on how to create vector representations of words using a glove.</p>
<p>What is the technique for obtaining glove- vector representations of words for highly specialized literature?</p>
",Vectorization & Embeddings,create glove embeddings python gensym work word vek creates desired mapping word vector space understand glove optimized version word vec exactly smaller corpus word find technique create vector representation word using glove technique obtaining glove vector representation word highly specialized literature
TFIDF and Multilingual Text Classification,"<p>I have a scenario, there is a store that has video contents of different languages including English. I want to give an item to item recommendation using TFIDF, but I am confused with stop words. How it is going to perform in diversified languages? And what should be the stop_word? </p>

<pre><code>tftdf = TfidfVectorizer()
count_matrix = tftdf.fit_transform(df[""combined_features""])
cosine_sim = cosine_similarity(count_matrix)
</code></pre>
",Vectorization & Embeddings,tfidf multilingual text classification scenario store ha video content different language including english want give item item recommendation using tfidf confused stop word going perform diversified language stop word
nn.LSTM doesn&#39;t seem to learn anything or not updating properly,"<p>I was trying out a simple LSTM use case form pytorch, with the following model.</p>
<pre><code>class SimpleLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(batch_first=True, input_size=embedding_dim, num_layers=1, hidden_size=hidden_dim, bidirectional=True)
        self.linear = nn.Linear(hidden_dim*2, 1)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):   # NxD, padded to same length with 0s in N-sized batch
        x = self.embedding(x)
        output, (final_hidden_state, final_cell_state) = self.lstm(x)
        x = self.linear(output[:,-1,:])
        x=self.sigmoid(x)
        return x
</code></pre>
<p>It is a binary classification, with BCELoss (combined with the Sigmoid output layer). Unfortunately, loss is stuck at 0.6969 (i.e. it is not learning anything).</p>
<p>I've tried using <code>final_hidden_state</code>, <code>output[:,0,:]</code> feeding into the linear layer, but so far no dice.</p>
<p>Everything else (optimizer, loss criterion, train loop, val loop) already works because I tried the exact same setup with a basic NN using nn.Embedding, nn.Linear, and nn.Sigmoid only, and could get to good loss decrease and high accuracy. In the <code>SimpleLSTM</code>, the only thing I added is the nn.LSTM.</p>
",Vectorization & Embeddings,nn lstm seem learn anything updating properly wa trying simple lstm use case form pytorch following model binary classification bceloss combined sigmoid output layer unfortunately loss stuck e learning anything tried using feeding linear layer far dice everything else optimizer loss criterion train loop val loop already work tried exact setup basic nn using nn embedding nn linear nn sigmoid could get good loss decrease high accuracy thing added nn lstm
Python: Cosine similarity between sentences with synonyms,"<p>How to calculate cosine similarity, if two sentences have any common word in the form of synonyms. For example,</p>
<p>sent1 = &quot;You are a good <strong>coder</strong>.&quot;</p>
<p>sent2 = &quot;I am new <strong>programmer</strong>&quot;</p>
<p>Consider <em><strong>coder</strong></em> is synonym of <em><strong>programmer</strong></em> here. Without considering these two specific words as synonym I get a cosine score as <em>zero(0)</em>. But considering as synonyms, it should give some cosine value. Please suggest how to approach or try to modify my below sample code. Please consider a custom synonym-dictionary or list instead of any API-based dictionary.</p>
<pre><code>import math
import re
from collections import Counter

WORD = re.compile(r&quot;\w+&quot;)    
def get_cosine(vec1, vec2):
    intersection = set(vec1.keys()) &amp; set(vec2.keys())
    numerator = sum([vec1[x] * vec2[x] for x in intersection])

    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])
    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])
    denominator = math.sqrt(sum1) * math.sqrt(sum2)

    if not denominator:
        return 0.0
    else:
        return float(numerator) / denominator    
def text_to_vector(text):
    words = WORD.findall(text)
    return Counter(words)

synonyms = {&quot;India&quot;: &quot;Hindustan&quot;,
            &quot;USA&quot;: &quot;America&quot;,}    
text2 = &quot;I live in India&quot;    
sentences = [&quot;India&quot;,
            &quot;He belongs to USA&quot;, 
            &quot;Hindustan is synonym of my country name&quot;,
            &quot;USA and America is same&quot;,
            &quot;You live in a great country.&quot;,
            &quot;All countries are great to live&quot;,]    
cosinetolist = []
for i in sentences:
    vector1 = text_to_vector(i)
    vector2 = text_to_vector(text2) 
    cosine = get_cosine(vector1, vector2)
    cosinetolist.append((cosine,i,))
l = cosinetolist
print(l)
</code></pre>
",Vectorization & Embeddings,python cosine similarity sentence synonym calculate cosine similarity two sentence common word form synonym example sent good coder sent new programmer consider coder synonym programmer without considering two specific word synonym get cosine score zero considering synonym give cosine value please suggest approach try modify sample code please consider custom synonym dictionary list instead api based dictionary
How node2vec works,"<p>I have been reading about the <a href=""https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf"" rel=""noreferrer"">node2vec</a> embedding algorithm and I am a little confused how it works. </p>

<p>For reference, node2vec is parametrised by p and q and works by simulating a bunch of random walks from nodes and just running word2vec embeddings on these walks as ""sentences"". By setting p and q in different ways, you can get more BFS or more DFS type random walks in the simulataion phase, capturing different network structure in the embedding.</p>

<p>Setting q > 1 gives us more BFS behaviour in that the samples of walks comprise of nodes within a small locality.  The thing I am confused about is that the paper says this is equivalent to embedding nodes with similar structural properties close to each other.</p>

<p>I don't quite understand how that works. If I have two separate say star/hub structured nodes in my network that are far apart, why would embedding based on the random walks from those two nodes put those two nodes close together in the embedding?</p>
",Vectorization & Embeddings,node vec work reading node vec embedding algorithm little confused work reference node vec parametrised p q work simulating bunch random walk node running word vec embeddings walk sentence setting p q different way get bfs dfs type random walk simulataion phase capturing different network structure embedding setting q give u bfs behaviour sample walk comprise node within small locality thing confused paper say equivalent embedding node similar structural property close quite understand work two separate say star hub structured node network far apart would embedding based random walk two node put two node close together embedding
Bigram vector representations using word2vec,"<p>I want to construct word embeddings for documents using the word2vec tool. I know how to find a vector embedding corresponding to a single word (unigram). Now, I want to find a vector for a bigram. Is it possible to construct a bigram word embedding using word2vec? If yes, how?</p>
",Vectorization & Embeddings,bigram vector representation using word vec want construct word embeddings document using word vec tool know find vector embedding corresponding single word unigram want find vector bigram possible construct bigram word embedding using word vec yes
Combining text and tabular data in PyTorch for classification model,"<p>I have a dataset that consists of customers and their product purchases for an ecommerce company that sells clothes.  Along with this data, I have application logs that show the customer’s interactions on the site. The data looks something like this:</p>
<pre><code>import pandas as pd
data = {'customer_id':[369799, 103508, 294535, 222573, 204286, 254953, 268167, 56201, 168900, 96618],
 'application_log':['web_pdp__click_main_banner web_pdp__click_prod',
 'web_pdp__click_main_banner web_pdp__click_prod web_pdp__click_sub',
 'web_home__click_main_banner web_home__click_prod',
 'web_pdp__click_main_banner web_pdp__click_prod web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub',
 'web_pdp__click_main_banner web_pdp__click_prod web_pdp__view_hero web_pdp__hover_index web_pdp__click_sub',
 'web_pdp__click_main_banner web_pdp__click_prod web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub',
 'web_pdp__click_main_banner web_pdp__click_prod web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub',
 'web_pdp__click_main_banner web_pdp__click_prod web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub',
 'web_pdp__click_main_banner web_pdp__click_prod web_pdp__click_sub',
 'web_home__click_main_banner web_home__click_prod'],
 'var_1':[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'var_2':[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'var_3':[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
 'var_4':[0, 1, 0, 5, 1, 3, 6, 7, 1, 0],
 'var_5':[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
 'targets':[1, 1, 0, 1, 1, 1, 1, 1, 1, 1]}
data = pd.DataFrame(data)
</code></pre>
<p>out:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>customer_id</th>
<th>application_log</th>
<th>var_1</th>
<th>var_2</th>
<th>var_3</th>
<th>var_4</th>
<th>var_5</th>
<th>targets</th>
</tr>
</thead>
<tbody>
<tr>
<td>369799</td>
<td>web_pdp__click_main_banner web_pdp__click_prod</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>103508</td>
<td>web_pdp__click_main_banner web_pdp__click_prod web_pdp__click_sub</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>294535</td>
<td>web_home__click_main_banner web_home__click_prod</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>222573</td>
<td>web_pdp__click_main_banner web_pdp__click_prod web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>5</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>204286</td>
<td>web_pdp__click_main_banner web_pdp__click_prod web_pdp__view_hero web_pdp__hover_index web_pdp__click_sub</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>254953</td>
<td>web_pdp__click_main_banner web_pdp__click_prod web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>3</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>268167</td>
<td>web_pdp__click_main_banner web_pdp__click_prod web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>6</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>56201</td>
<td>web_pdp__click_main_banner web_pdp__click_prod web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub web_pdp__click_sub</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>7</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>168900</td>
<td>web_pdp__click_main_banner web_pdp__click_prod web_pdp__click_sub</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>96618</td>
<td>web_home__click_main_banner web_home__click_prod</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>I want to predict the probability of a customer making a subsequent purchase as denoted in the field “targets” above.
 
I would like to do this in PyTorch using a “customer-as-a-text” paradigm, whereby the customer’s session logs are concatenated into discrete tokens and grouped into “customer-sentences”, which are then used to learn “customer-embeddings”. Similar to what’s being explained in this diagram below:</p>
<p><a href=""https://i.sstatic.net/Wy2Q0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Wy2Q0.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://blog.griddynamics.com/customer2vec-representation-learning-and-automl-for-customer-analytics-and-personalization/"" rel=""nofollow noreferrer"">https://blog.griddynamics.com/customer2vec-representation-learning-and-automl-for-customer-analytics-and-personalization/</a></p>
<p>The diagram comes from the tutorial that I am trying to emulate, which mentions using doc2vec to generate embeddings and then concatenating those embeddings with the remaining tabular data. The problem is that I don’t quite understand how to do this in PyTorch. Specifically, I don’t know how to create the model nor the custom dataset required for PyTorch.</p>
<p>I would very much appreciate it if someone could provide me with code that takes this data and converts it into a PyTorch dataset, as well as code for a multi-modal model that uses an LSTM layer for the text features and then combines the text features with the remaining numerical features in a multi-layer perceptron to predict the probability of the target column.</p>
<p>I found one specific tutorial that does this, except it uses PyTorch Lightning, which is something I want to avoid (<a href=""https://drivendata.co/blog/hateful-memes-benchmark"" rel=""nofollow noreferrer"">https://drivendata.co/blog/hateful-memes-benchmark</a>).</p>
<p>I currently havent written any code for the actual model. But my current code for the dataset looks something like below, but I feel that I'm going about it all wrong, especially in terms of the text data:</p>
<pre><code>import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from torchtext.vocab import build_vocab_from_iterator
from torch.nn.utils.rnn import pad_sequence
    
class Vocabulary:
    
        &quot;&quot;&quot;
        __init__ method is called by default as soon as an object of this class is initiated
        we use this method to initiate our vocab dictionaries
        &quot;&quot;&quot;
    
        def __init__(self, freq_threshold, max_size):
            &quot;&quot;&quot;
            freq_threshold : the minimum times a word must occur in corpus to be treated in vocab
            max_size : max source vocab size
            &quot;&quot;&quot;
            # initiate the index to token dict
            self.itos = {0: &quot;&lt;PAD&gt;&quot;, 1: &quot;&lt;SOS&gt;&quot;, 2: &quot;&lt;EOS&gt;&quot;, 3: &quot;&lt;UNK&gt;&quot;}
            # initiate the token to index dict
            self.stoi = {k: j for j, k in self.itos.items()}
    
            self.freq_threshold = freq_threshold
            self.max_size = max_size
    
        &quot;&quot;&quot;
        __len__ is used by dataloader later to create batches
        &quot;&quot;&quot;
    
        def __len__(self):
            return len(self.itos)
    
        &quot;&quot;&quot;
        a simple tokenizer to split on space and converts the sentence to list of words
        &quot;&quot;&quot;
    
        @staticmethod
        def tokenizer(text):
            return [tok.lower().strip() for tok in text.split(&quot; &quot;)]
    
        &quot;&quot;&quot;
        build the vocab: create a dictionary mapping of index to string (itos) and string to index (stoi)
        output ex. for stoi -&gt; {'the':5, 'a':6, 'an':7}
        &quot;&quot;&quot;
    
        def build_vocabulary(self, sentence_list):
            # calculate the frequencies of each word first to remove the words with freq &lt; freq_threshold
            frequencies = {}  # init the freq dict
            idx = 4  # index from which we want our dict to start. We already used 4 indexes for pad, start, end, unk
    
            # calculate freq of words
            for sentence in sentence_list:
                for word in self.tokenizer(sentence):
                    if word not in frequencies.keys():
                        frequencies[word] = 1
                    else:
                        frequencies[word] += 1
    
            # limit vocab by removing low freq words
            frequencies = {k: v for k, v in frequencies.items() if v &gt; self.freq_threshold}
    
            # limit vocab to the max_size specified
            if len(frequencies) &gt; self.max_size - idx:
                frequencies = dict(
                    sorted(frequencies.items(), key=lambda x: -x[1])[: self.max_size - idx]
                )  # idx =4 for pad, start, end , unk
    
            # create vocab
            for word in frequencies.keys():
                self.stoi[word] = idx
                self.itos[idx] = word
                idx += 1
    
        &quot;&quot;&quot;
        convert the list of words to a list of corresponding indexes
        &quot;&quot;&quot;
    
        def numericalize(self, text):
            tokenized_text = self.tokenizer(text)
            numericalized_text = []
            for token in tokenized_text:
                if token in self.stoi.keys():
                    numericalized_text.append(self.stoi[token])
                else:  # out-of-vocab words are represented by UNK token index
                    numericalized_text.append(self.stoi[&quot;&lt;UNK&gt;&quot;])
    
            return numericalized_text
    
    class MyDataset(Dataset):
        def __init__(self, df, target, text):
            x = df.drop([target, text], axis=1).values.astype(int)
            self.x_text = df[text]
            y = df[target].values.astype(int)
    
            self.x_text_voc = Vocabulary(1, 100)
            self.x_text_voc.build_vocabulary(self.x_text.tolist())
    
            self.x_train = torch.tensor(x, dtype=torch.int64)
            self.y_train = torch.tensor(y, dtype=torch.int64)
    
        def __len__(self):
            return len(self.y_train)
    
        def __getitem__(self, idx):
            self.text_vector = self.x_text[idx]
    
            self.num_source = [self.x_text_voc.stoi[&quot;&lt;SOS&gt;&quot;]]
            self.num_source += self.x_text_voc.numericalize(self.text_vector)
            self.num_source.append(self.x_text_voc.stoi[&quot;&lt;EOS&gt;&quot;])
    
            return self.x_train[idx], torch.tensor(self.num_source), self.y_train[idx]
    
    
    class MyCollate:
        def __init__(self, pad_idx):
            self.pad_idx = pad_idx
    
        # __call__: a default method
        ##   First the obj is created using MyCollate(pad_idx) in data loader
        ##   Then if obj(batch) is called -&gt; __call__ runs by default
        def __call__(self, batch):
            # get all source indexed sentences of the batch
            source = [item[0] for item in batch]
            # pad them using pad_sequence method from pytorch.
            source = pad_sequence(source, batch_first=False, padding_value=self.pad_idx)
    
            # get all target indexed sentences of the batch
            target = [item[1] for item in batch]
            # pad them using pad_sequence method from pytorch.
            target = pad_sequence(target, batch_first=False, padding_value=self.pad_idx)
            return source, target
    
    
    def get_train_loader(
        dataset, batch_size, num_workers=0, shuffle=True, pin_memory=False
    ):
        # get pad_idx for collate fn
        pad_idx = dataset.x_text_voc.stoi[&quot;&lt;PAD&gt;&quot;]
        # define loader
        loader = DataLoader(
            dataset,
            batch_size=batch_size,
            num_workers=num_workers,
            shuffle=shuffle,
            pin_memory=pin_memory,
            collate_fn=MyCollate(pad_idx=pad_idx),
        )
        return loader
    
    
    train = MyDataset(data, &quot;targets&quot;, &quot;application_log&quot;)
</code></pre>
",Vectorization & Embeddings,combining text tabular data pytorch classification model dataset consists customer product purchase company sell clothes along data application log show customer interaction site data look something like customer id application log var var var var var target web pdp click main banner web pdp click prod web pdp click main banner web pdp click prod web pdp click sub web home click main banner web home click prod web pdp click main banner web pdp click prod web pdp click sub web pdp click sub web pdp click sub web pdp click sub web pdp click sub web pdp click main banner web pdp click prod web pdp view hero web pdp hover index web pdp click sub web pdp click main banner web pdp click prod web pdp click sub web pdp click sub web pdp click sub web pdp click main banner web pdp click prod web pdp click sub web pdp click sub web pdp click sub web pdp click sub web pdp click sub web pdp click sub web pdp click main banner web pdp click prod web pdp click sub web pdp click sub web pdp click sub web pdp click sub web pdp click sub web pdp click sub web pdp click sub web pdp click main banner web pdp click prod web pdp click sub web home click main banner web home click prod want predict probability customer making subsequent purchase denoted field target would like pytorch using customer text paradigm whereby customer session log concatenated discrete token grouped customer sentence used learn customer embeddings similar explained diagram diagram come tutorial trying emulate mention using doc vec generate embeddings concatenating embeddings remaining tabular data problem quite understand pytorch specifically know create model custom dataset required pytorch would much appreciate someone could provide code take data convert pytorch dataset well code multi modal model us lstm layer text feature combine text feature remaining numerical feature multi layer perceptron predict probability target column found one specific tutorial doe except us pytorch lightning something want avoid currently havent written code actual model current code dataset look something like feel going wrong especially term text data
Running Flair embeddings parallel,"<p>I have a list containing millions of sentences for which I need embeddings. I am using <a href=""https://github.com/flairNLP/flair/tree/master/flair"" rel=""nofollow noreferrer"">Flair</a> for this purpose. The problem seems like it should be embarrassingly parallel. But when I try to optimize, I get either no increase in performance, or it simply stalls.</p>
<p>I define my sentences as a simple list of strings:</p>
<pre><code>texts = [
    &quot;this is a test&quot;,
    &quot;to see how well&quot;,
    &quot;this system works&quot;,
    &quot;here are alot of words&quot;,
    &quot;many of them&quot;,
    &quot;they keep comming&quot;,
    &quot;many more sentences&quot;,
    &quot;so many&quot;,
    &quot;some might even say&quot;,
    &quot;there are 10 of them&quot;,
]
</code></pre>
<p>I use Flair to create the embeddings:</p>
<pre><code>from flair.embeddings import SentenceTransformerDocumentEmbeddings
from flair.data import Sentence

sentence_embedding = SentenceTransformerDocumentEmbeddings(&quot;bert-base-nli-mean-tokens&quot;)

def sentence_to_vector(sentence):
    sentence_tokens = Sentence(sentence)
    sentence_embedding.embed(sentence_tokens)
    return sentence_tokens.get_embedding().tolist()
</code></pre>
<p>I tried with both <a href=""https://joblib.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">Joblib</a> <a href=""https://docs.python.org/3/library/concurrent.futures.html"" rel=""nofollow noreferrer"">Concurrent Futures</a> to solve the problem in parallel:</p>
<pre><code>import time
from joblib import Parallel, delayed
import concurrent.futures

def parallelize(iterable, func):
    return Parallel(n_jobs=4, prefer=&quot;threads&quot;)(delayed(func)(i) for i in iterable)

print(&quot;start embedding sequentially&quot;)
tic = time.perf_counter()
embeddings = [sentence_to_vector(text) for text in texts]
toc = time.perf_counter()
print(toc - tic)

print(&quot;start embedding parallel, w. joblib&quot;)
tic = time.perf_counter()
embeddings = parallelize(texts, sentence_to_vector)
toc = time.perf_counter()
print(toc - tic)

print(&quot;start embedding parallel w. concurrent.futures&quot;)
tic = time.perf_counter()
with concurrent.futures.ProcessPoolExecutor() as executor:
    embeddings = [executor.submit(sentence_to_vector, text) for text in texts]
toc = time.perf_counter()
print(toc - tic)
</code></pre>
<p>The Joblib function is running, but it is slower than doing it sequential. The concurrent.futures function spins up a bunch of threads but hangs indefinitely.</p>
<p>Any solutions or hints in the right direction would be much appreciated.</p>
",Vectorization & Embeddings,running flair embeddings parallel list containing million sentence need embeddings using flair purpose problem seems like embarrassingly parallel try optimize get either increase performance simply stall define sentence simple list string use flair create embeddings tried joblib concurrent future solve problem parallel joblib function running slower sequential concurrent future function spin bunch thread hang indefinitely solution hint right direction would much appreciated
Reduce fastText memory usage for big models,"<p>I trained a machine learning sentence classification model that uses, among other features, also the vectors obtained from a pretrained fastText model (like <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">these</a>) which is 7Gb.  I use the pretrained fastText Italian model: I am using this word embedding only to get some semantic features to feed into the effective ML model.</p>
<p>I built a simple API based on fastText that, at prediction time, computes the vectors needed by the effective ML model. Under the hood, this API receives a string as input and calls <code>get_sentence_vector</code>. When the API starts, it loads the fastText model into memory.</p>
<p><strong>How can I reduce the memory footprint of fastText, which is loaded into RAM?</strong></p>
<p>Constraints:</p>
<ul>
<li>My model works fine, training was time-consuming and expensive, so I wouldn't want to retrain it using smaller vectors</li>
<li>I need the fastText ability to handle out-of-vocabulary words, so I can't use just vectors but I need the full model</li>
<li>I should reduce the RAM usage, even at the expense of a reduction in speed.</li>
</ul>
<p>At the moment, I'm starting to experiment with <a href=""https://github.com/avidale/compress-fasttext"" rel=""nofollow noreferrer"">compress-fasttext</a>...</p>
<p><strong>Please share your suggestions and thoughts even if they do not represent full-fledged solutions.</strong></p>
",Vectorization & Embeddings,reduce fasttext memory usage big model trained machine learning sentence classification model us among feature also vector obtained pretrained fasttext model like gb use pretrained fasttext italian model using word embedding get semantic feature feed effective ml model built simple api based fasttext prediction time computes vector needed effective ml model hood api receives string input call api start load fasttext model memory reduce memory footprint fasttext loaded ram constraint model work fine training wa time consuming expensive want retrain using smaller vector need fasttext ability handle vocabulary word use vector need full model reduce ram usage even expense reduction speed moment starting experiment compress fasttext please share suggestion thought even represent full fledged solution
How to understand the bias term in language model head (when we tie the word embeddings)?,"<p>I was learning the masked language modeling codebase in Huggingface Transformers. Just a question to understand the language model head.</p>
<p>Here at the final linear layer where we project hidden size to vocab size (<a href=""https://github.com/huggingface/transformers/blob/f2fbe4475386bfcfb3b83d0a3223ba216a3c3a91/src/transformers/models/bert/modeling_bert.py#L685-L702"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/f2fbe4475386bfcfb3b83d0a3223ba216a3c3a91/src/transformers/models/bert/modeling_bert.py#L685-L702</a>).</p>
<pre class=""lang-py prettyprint-override""><code>self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
self.bias = nn.Parameter(torch.zeros(config.vocab_size))
self.decoder.bias = self.bias
</code></pre>
<p>We set the <code>bias</code> term to zero at the moment. And later when we initialize the weight, we tie the weight of the linear layer and the word embedding.</p>
<p>But we don't do such a thing for the bias term. I wonder how we can understand that and why we want to initialize the bias term as a zero vector.</p>
<p><a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L1060-L1079"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L1060-L1079</a></p>
<pre class=""lang-py prettyprint-override""><code>    def tie_weights(self):
        &quot;&quot;&quot;
        Tie the weights between the input embeddings and the output embeddings.
        If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the
        weights instead.
        &quot;&quot;&quot;
        if getattr(self.config, &quot;tie_word_embeddings&quot;, True):
            output_embeddings = self.get_output_embeddings()
            if output_embeddings is not None:
                self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())

        if getattr(self.config, &quot;is_encoder_decoder&quot;, False) and getattr(self.config, &quot;tie_encoder_decoder&quot;, False):
            if hasattr(self, self.base_model_prefix):
                self = getattr(self, self.base_model_prefix)
            self._tie_encoder_decoder_weights(self.encoder, self.decoder, self.base_model_prefix)

        for module in self.modules():
            if hasattr(module, &quot;_tie_weights&quot;):
                module._tie_weights()
</code></pre>
<p>My understanding:</p>
<ol>
<li>Because the final linear weight accepts the hidden representations that have been transformed by several feed-forward layers. We might not be able to match them exactly, we need the bias term to somehow regularize them.</li>
</ol>
<p>As I'm not sure my understanding is accurate, I would like to seek your opinions.</p>
",Vectorization & Embeddings,understand bias term language model head tie word embeddings wa learning masked language modeling codebase huggingface transformer question understand language model head final linear layer project hidden size vocab size set term zero moment later initialize weight tie weight linear layer word embedding thing bias term wonder understand want initialize bias term zero vector understanding final linear weight accepts hidden representation transformed several feed forward layer might able match exactly need bias term somehow regularize sure understanding accurate would like seek opinion
Calculating BLEU and Rouge score as fast as possible,"<p>I have around 200 candidate sentences and for each candidate, I want to measure the bleu score by comparing each sentence with thousands of reference sentences. These references are the same for all candidates. Here is how I'm doing it right now:</p>
<pre><code>ref_for_all = [reference] *len(sents)
score = corpus_bleu(ref_for_all, [i.split() for i in sents], weights=(0, 1, 0, 0))
</code></pre>
<p>The <code>reference</code> contains the whole corpus I want to compare each sentence with, and <code>sent</code> are my sentences (candidates). Unfortunately, this takes too long and given the experimental nature of my code, I cannot wait that long to get the results. Is there any other way (for example using Regex) that I can get these scores faster? I also have this problem with Rouge, so any suggestion is highly appreciated for that too!</p>
",Vectorization & Embeddings,calculating bleu rouge score fast possible around candidate sentence candidate want measure bleu score comparing sentence thousand reference sentence reference candidate right contains whole corpus want compare sentence sentence candidate unfortunately take long given experimental nature code wait long get result way example using regex get score faster also problem rouge suggestion highly appreciated
What does tf.nn.embedding_lookup function do?,"<pre><code>tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None)
</code></pre>

<p>I cannot understand the duty of this function. Is it like a lookup table? Which means to return the parameters corresponding to each id (in ids)?</p>

<p>For instance, in the <code>skip-gram</code> model if we use <code>tf.nn.embedding_lookup(embeddings, train_inputs)</code>, then for each <code>train_input</code> it finds the correspond embedding?</p>
",Vectorization & Embeddings,doe tf nn embedding lookup function understand duty function like lookup table mean return parameter corresponding id id instance model use find correspond embedding
How to initialize word-embeddings for Out of Vocabulary Word?,"<p>I am trying to use CoNLL-2003 NER (English) Dataset and I am trying to utilize pretrained embeddings for it. I am using SENNA pretrained embeddings. Now I have around 20k words in my vocabulary and out of this I have embedding available for only 9.5k words.<br>
My current approach is to initialize an array of <code>20k X embedding_size</code> with zeros and initialize the 9.5k words whose embeddings is known to me and make all the embeddings learn-able.<br></p>

<p>My question is what is the best way to do this? Any reference to such research will be very helpful?</p>
",Vectorization & Embeddings,initialize word embeddings vocabulary word trying use conll ner english dataset trying utilize pretrained embeddings using senna pretrained embeddings around k word vocabulary embedding available k word current approach initialize array zero initialize k word whose embeddings known make embeddings learn able question best way reference research helpful
Same sentences produces a different vector in XLNet,"<p>I have computed the vectors for two same sentences using <a href=""https://github.com/amansrivastava17/embedding-as-service"" rel=""nofollow noreferrer"">XLNet embedding-as-service</a>. But the model produces different vector embeddings for both the two same sentences hence the cosine similarity is not 1 and the Euclidean distances also not 0. in case of BERT its works fine.
for example; if</p>
<pre><code>vec1 = en.encode(texts=['he is anger'],pooling='reduce_mean')
vec2 = en.encode(texts=['he is anger'],pooling='reduce_mean')
</code></pre>
<p>the model (XLNet) is saying that these two sentences are dissimilar.</p>
",Vectorization & Embeddings,sentence produce different vector xlnet computed vector two sentence using xlnet embedding service model produce different vector embeddings two sentence hence cosine similarity euclidean distance also case bert work fine example model xlnet saying two sentence dissimilar
how to get best results in Doc2vec,"<ol>
<li><p>As I came to know we need a large dataset to get results so I found a 50k plus abstracts dataset to check.</p>
</li>
<li><p>Now to test whether this is working correctly or not I wanted to match the target document into the trained document.</p>
<pre><code> doc = 'Noise pollution can cause health problems for people and wildlife both on land 
  and in the sea. From traffic noise to rock concerts loud or inescapable sounds can 
  cause hearing loss stress and high blood pressure Noise from ships and human activities 
  in the ocean is harmful to whales and dolphins that depend on echolocation to survive.'
 target_data = word_tokenize(lst_doc)

 train_data = list(read_data())
 model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=3, epochs=20)
 train_vocab = model.build_vocab(train_data)
 train = model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)


 inferred_vector = model.infer_vector(target_data)
 sims = model.dv.most_similar([inferred_vector])
 print('Document ({}): «{}»\n'.format(0,' '.join(target_data)))
 print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\n' % model)
 for  index in sims:
     print(index, ' '.join(train_data[index[0]].words))
     print('\n')
</code></pre>
</li>
</ol>
<p><strong>Current Output</strong></p>
<pre><code>(56182, 0.8743067383766174) noise pollution can cause health problems for people and wildlife both on land and in the sea from traffic noise to rock concerts loud or inescapable sounds can cause hearing loss stress and high blood pressure noise from ships and human activities in the ocean is harmful to whales and dolphins that depend on echolocation to survive


(56183, 0.5959663391113281) global warming also known as climate change is caused by blanket of pollution that traps heat around the earth this pollution comes from cars factories homes and power plants that burn fossil fuels such as oil coal natural gas and gasoline


(21926, 0.5771061182022095) collecting sufficient labelled training data for health and medical problems is difficult antropova et al also missing values are unavoidable in health and medical datasets and tackling the problem arising from the inadequate instances and missingness is not straightforward snell et al sterne et al however machine learning algorithms have achieved significant success in many real world healthcare problems such as regression and classification and these techniques could possibly be way to resolve the issues


(12118, 0.5548962950706482) the policy gradients of the expected return objective can react slowly to rare rewards yet in some cases agents may wish to emphasize the low or high returns regardless of their probability borrowing from the economics and control literature we review the risk sensitive value function that arises from an exponential utility and illustrate its effects on an example this risk sensitive value function is not always applicable to reinforcement learning problems so we introduce the particle value function defined by particle filter over the distributions of an agent experience which bounds the risk sensitive one we illustrate the benefit of the policy gradients of this objective in cliffworld


(6635, 0.5334600806236267) crowd gatherings at social and cultural events are increasing in leaps and bounds with the increase in population surveillance through computer vision and expert decision making systems can help to understand the crowd phenomena at large gatherings understanding crowd phenomena can be helpful in early identification of unwanted incidents and their prevention motion flow is one of the important crowd phenomena that can be instrumental in describing the crowd behavior flows can be useful in understanding instabilities in the crowd however extracting motion flows is challenging task due to randomness in crowd movement and limitations of the sensing device moreover low level features such as optical flow can be misleading if the randomness is high in this paper we propose new model based on langevin equation to analyze the linear dominant flows in videos of densely crowded scenarios we assume force model with three components namely external force confinement drift force and disturbance force these forces are found to be sufficient to describe the linear or near linear motion in dense crowd videos the method is significantly faster as compared to existing popular crowd segmentation methods the evaluation of the proposed model has been carried out on publicly available datasets as well as using our dataset it has been observed that the proposed method is able to estimate and segment the linear flows in the dense crowd with better accuracy as compared to state of the art techniques with substantial decrease in the computational overhead


(30405, 0.5323445200920105) wildfires have increased in frequency and severity over the past two decades especially in the western united states beyond physical infrastructure damage caused by these wildfire events researchers have increasingly identified harmful impacts of particulate matter generated by wildfire smoke on respiratory cardiovascular and cognitive health this inference is difficult due to the spatial and temporal uncertainty regarding how much particulate matter is specifically attributable to wildfire smoke one factor contributing to this challenge is the reliance on manually drawn smoke plume annotations which are often noisy representations limited to the united states this work uses deep convolutional neural networks to segment smoke plumes from geostationary satellite imagery we compare the performance of predicted plume segmentations versus the noisy annotations using causal inference methods to estimate the amount of variation each explains in environmental protection agency epa measured surface level particulate matter um in diameter textrm pm


(5699, 0.5295888781547546) crowd gatherings at social and cultural events are increasing in leaps and bounds with the increase in population surveillance through computer vision and expert decision making systems can help to understand the crowd phenomena at large gatherings understanding crowd phenomena can be helpful in early identification of unwanted incidents and their prevention motion flow is one of the important crowd phenomena that can be instrumental in describing the crowd behavior flows can be useful in understanding instabilities in the crowd however extracting motion flows is challenging task due to randomness in crowd movement and limitations of the sensing device moreover low level features such as optical flow can be misleading if the randomness is high in this paper we propose new model based on langevin equation to analyze the linear dominant flows in videos of densely crowded scenarios we assume force model with three components namely external force confinement drift force and disturbance force these forces are found to be sufficient to describe the linear or near linear motion in dense crowd videos the method is significantly faster as compared to existing popular crowd segmentation methods the evaluation of the proposed model has been carried out on publicly available datasets as well as using our dataset it has been observed that the proposed method is able to estimate and segment the linear flows in the dense crowd with better accuracy as compared to state of the art techniques with substantial decrease in the computational overhead


(11825, 0.5268296003341675) recent advances in policy gradient methods and deep learning have demonstrated their applicability for complex reinforcement learning problems however the variance of the performance gradient estimates obtained from the simulation is often excessive leading to poor sample efficiency in this paper we apply the stochastic variance reduced gradient descent svrg to model free policy gradient to significantly improve the sample efficiency the svrg estimation is incorporated into trust region newton conjugate gradient framework for the policy optimization on several mujoco tasks our method achieves significantly better performance compared to the state of the art model free policy gradient methods in robotic continuous control such as trust region policy optimization trpo


(18521, 0.5255249738693237) cnns perform remarkably well when the training and test distributions are but unseen image corruptions can cause surprisingly large drop in performance in various real scenarios unexpected distortions such as random noise compression artefacts or weather distortions are common phenomena improving performance on corrupted images must not result in degraded performance challenge faced by many state of the art robust approaches image corruption types have different characteristics in the frequency spectrum and would benefit from targeted type of data augmentation which however is often unknown during training in this paper we introduce mixture of two expert models specializing in high and low frequency robustness respectively moreover we propose new regularization scheme that minimizes the total variation tv of convolution feature maps to increase high frequency robustness the approach improves on corrupted images without degrading in distribution performance we demonstrate this on imagenet and also for real world corruptions on an automotive dataset both for object classification and object detection


(20234, 0.5212455987930298) deep learning on an edge device requires energy efficient operation due to ever diminishing power budget intentional low quality data during the data acquisition for longer battery life and natural noise from the low cost sensor degrade the quality of target output which hinders adoption of deep learning on an edge device to overcome these problems we propose simple yet efficient mixture of pre processing experts mope model to handle various image distortions including low resolution and noisy images we also propose to use adversarially trained auto encoder as pre processing expert for the noisy images we evaluate our proposed method for various machine learning tasks including object detection on ms coco dataset multiple object tracking problem on mot challenge dataset and human activity classification on ucf dataset experimental results show that the proposed method achieves better detection tracking and activity classification accuracies under noise without sacrificing accuracies for the clean images the overheads of our proposed mope are and in terms of memory and computation compared to the baseline object detection network
</code></pre>
",Vectorization & Embeddings,get best result doc vec came know need large dataset get result found k plus abstract dataset check test whether working correctly wanted match target document trained document current output
Are the document vectors used in doc2vec one-hot?,"<p>I understand conceptually how word2vec and doc2vec work, but am struggling with the nuts and bolts of how the numbers in the vectors get processed algorithmically.</p>

<p>If the vectors for three context words are: [1000], [0100], [0010]</p>

<p>and the vector for the target word is [0001], does the algorithm perform one backward pass for each input/target output pair, like this:</p>

<pre><code>[1000]--&gt;[0001]
[0100]--&gt;[0001]
[0010]--&gt;[0001]
</code></pre>

<p>or are the input (context) vectors added together, like this:</p>

<pre><code>[1110]--&gt;[0001]
</code></pre>

<p>or is some other process used?</p>

<p>Additionally, do the document vectors used in doc2vec take the one-hot form of the word vectors, or are documents tagged with individual numbers on a continuous scale, like 1, 2, 3, etc.?</p>

<p>I get that the document tags are included as input nodes during the training process, but how are they used in the test phase? When entering the context word vectors to try to predict the target word (or vice versa) during testing, shouldn't an input for some document ID be required as well?</p>
",Vectorization & Embeddings,document vector used doc vec one hot understand conceptually word vec doc vec work struggling nut bolt number vector get processed algorithmically vector three context word vector target word doe algorithm perform one backward pas input target output pair like input context vector added together like process used additionally document vector used doc vec take one hot form word vector document tagged individual number continuous scale like etc get document tag included input node training process used test phase entering context word vector try predict target word vice versa testing input document id required well
Build vocab in doc2vec,"<p>I have a list of abstracts and articles approx 500 in csv each paragraph contains approx 800 to 1000 words whenever I build vocab and print with words giving none and how I can improve results?</p>
<pre><code>    lst_doc = doc.translate(str.maketrans('', '', string.punctuation))

    target_data = word_tokenize(lst_doc)

    train_data = list(read_data())

    model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)

    train_vocab = model.build_vocab(train_data)

    print(train_vocab)

   {train = model.train(train_data, total_examples=model.corpus_count, 
   epochs=model.epochs) }
</code></pre>
<p>Output:
None</p>
",Vectorization & Embeddings,build vocab doc vec list abstract article approx csv paragraph contains approx word whenever build vocab print word giving none improve result output none
Sentence embedding using T5,"<p>I would like to use state-of-the-art LM T5 to get sentence embedding vector.
I found this repository <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""noreferrer"">https://github.com/UKPLab/sentence-transformers</a>
As I know, in BERT I should take the first token as [CLS] token, and it will be the sentence embedding.
In this repository I see the same behaviour on T5 model:</p>
<pre><code>cls_tokens = output_tokens[:, 0, :]  # CLS token is first token
</code></pre>
<p>Does this behaviour correct? I have taken encoder from T5 and encoded two phrases with it:</p>
<pre><code>&quot;I live in the kindergarden&quot;
&quot;Yes, I live in the kindergarden&quot;
</code></pre>
<p>The cosine similarity between them was only &quot;0.2420&quot;.</p>
<p>I just need to understand how sentence embedding works - should I train network to find similarity to reach correct results? Or I it is enough of base pretrained language model?</p>
",Vectorization & Embeddings,sentence embedding using would like use state art lm get sentence embedding vector found repository know bert take first token cl token sentence embedding repository see behaviour model doe behaviour correct taken encoder encoded two phrase cosine similarity wa need understand sentence embedding work train network find similarity reach correct result enough base pretrained language model
Doc2Vec (Or Word2Vec) In Catalyst C#: How Do I get it to give results? (FastText),"<p>I'm trying to replicate results from Gensim in C# to compare results and see if we need to bother trying to get Python to work within our broader C# context.  I have been programming in C# for about a week, am usually a Python coder.  I managed to get LDA to function and assign topics with C#, but there is no <a href=""https://github.com/curiosity-ai/catalyst"" rel=""nofollow noreferrer"">Catalyst</a> model (that I could find) that does Doc2Vec explicitly, but rather I need to do something with FastText as they have in their sample code:</p>
<pre><code>// Training a new FastText word2vec embedding model is as simple as this:
var nlp = await Pipeline.ForAsync(Language.English);
var ft = new FastText(Language.English, 0, &quot;wiki-word2vec&quot;);
ft.Data.Type = FastText.ModelType.CBow;
ft.Data.Loss = FastText.LossType.NegativeSampling;
ft.Train(nlp.Process(GetDocs()));
ft.StoreAsync();
</code></pre>
<p>The claim is that it is simple, and fair enough... but what do I do with this?  I am using my own data, a list of IDocuments, each with a label attached:</p>
<pre><code>using (var csv = CsvDataReader.Create(&quot;Jira_Export_Combined.csv&quot;, new CsvDataReaderOptions
            {
                BufferSize = 0x20000
            }))
            {
                while (await csv.ReadAsync())
                {
                    var a = csv.GetString(1);  // issue key
                    var b = csv.GetString(14);  // the actual bug
                    // if (jira_base.Keys.ToList().Contains(a) == false)
                    if (jira.Keys.ToList().Contains(a) == false)
                    { // not already in our dictionary... too many repeats
                        if (b.Contains(&quot;{panel&quot;))
                        {
                            // get just the details/desc/etc
                            b = b.Substring(b.IndexOf(&quot;}&quot;) + 1, b.Length - b.IndexOf(&quot;}&quot;) - 1);
                            try { b = b.Substring(0, b.IndexOf(&quot;{panel}&quot;)); }
                            catch { }
                        }
                        b = b.Replace(&quot;\r\n&quot;, &quot;&quot;);
                        jira.Add(a, nlp.ProcessSingle(new Document(b,Language.English)));
                    }  // end if
                }  // end while loop
</code></pre>
<p>From a set of Jira Tasks and then I add labels:</p>
<pre><code>foreach (KeyValuePair&lt;string, IDocument&gt; item in jira) { jira[item.Key].Labels.Add(item.Key); }
</code></pre>
<p>Then I add to a list (based on a breakdown from a topic model where I assign all docs that are at or above a threshold in that topic to the topic, jira_topics[n] where n is the topic numner, as such:</p>
<pre><code>var training_lst = new List&lt;IDocument&gt;();
foreach (var doc in jira_topics[topic_num]) { training_lst.Add(jira[doc]); }
</code></pre>
<p>When I run the following code:</p>
<pre><code> // FastText....
 var ft = new FastText(Language.English, 0, $&quot;vector-model-topic_{topic_num}&quot;);
 ft.Data.Type = FastText.ModelType.Skipgram;
 ft.Data.Loss = FastText.LossType.NegativeSampling;
ft.Train(training_lst);
var wtf = ft.PredictMax(training_lst[0]);
</code></pre>
<p>wtf is (null,NaN).  [hence the name]</p>
<p>What am I missing?  <em><strong>What else do I need to do to get <a href=""https://github.com/curiosity-ai/catalyst"" rel=""nofollow noreferrer"">Catalyst</a> to vectorize my data?</strong></em>  I want to grab the cosine similarities between the jira tasks and some other data I have, but I can't even get the Jira data into anything resembling a vectorization I can apply to something.  Help!</p>
<p><strong>Update:</strong><br />
So, Predict methods apparently only work for supervised learning in FastText (see comments below).  And the following:
<code>var wtf = ft.CompareDocuments(training_lst[0], training_lst[0]);</code>
Throws an Implementation error (and only <em>doesn't</em> work with PVDM).  <strong>How do I use PVDM, PVDCbow in Catalyst?</strong></p>
",Vectorization & Embeddings,doc vec word vec catalyst c get give result fasttext trying replicate result gensim c compare result see need bother trying get python work within broader c context programming c week usually python coder managed get lda function assign topic c catalyst model could find doe doc vec explicitly rather need something fasttext sample code claim simple fair enough using data list idocuments label attached set jira task add label add list based topic model assign doc threshold topic topic jira topic n n topic numner run following code wtf null nan hence name missing else need get catalyst vectorize data want grab cosine similarity jira task data even get jira data anything resembling vectorization apply something help update predict method apparently work supervised learning fasttext see comment following throw implementation error work pvdm use pvdm pvdcbow catalyst
How to use LDA as an input feature into another classifier,"<p>I used LDA (Latent Dirichlet Allocation) in topic modeling on text dataset. I need the LDA result (for each instance) as an input into another ML classifier (SVM)
any idea how to add it?</p>
<p>I tried embedding the LDA result with BoW features but didn't work
here's the error I get:</p>
<p><code>ValueError: Found input variables with inconsistent numbers of samples: [5, 10103]</code></p>
",Vectorization & Embeddings,use lda input feature another classifier used lda latent dirichlet allocation topic modeling text dataset need lda result instance input another ml classifier svm idea add tried embedding lda result bow feature work error get
How to convert several-hot encoding to dence vector?,"<p>Now I am doing an NLP experiment. What I am thinking of is very similar to Word2Vec. I think my way must already exist. Maybe there is out-of-the-box code. But I don't know where to find.</p>
<p>Word2Vec's input word vector is one-hot. So the size of each word vector is equal to the size of the vocab.</p>
<p>But my input word vector is a catenation of several one-hot vectors. Maybe it can be called 'several-hot'. It's much shorter than one-hot but still sparse. I still want to dencify it using Word2Vec's scheme.</p>
<p>I have used Gensim's Word2Vec model. It seems to accept only tokens as input. That means it converts tokens to one-hot vectors internally right? I would like to know if there exist any Word2Vec code that accepts custom input vectors.</p>
",Vectorization & Embeddings,convert several hot encoding dence vector nlp experiment thinking similar word vec think way must already exist maybe box code know find word vec input word vector one hot size word vector equal size vocab input word vector catenation several one hot vector maybe called several hot much shorter one hot still sparse still want dencify using word vec scheme used gensim word vec model seems accept token input mean convert token one hot vector internally right would like know exist word vec code accepts custom input vector
How do I tie embeddings between a `torch.nn.Embedding` and `torch.nn.EmbeddingBag`?,"<p>I'd like to tie the embedding layers between two parts of my neural network: one which embeds tokens where order matters (i.e., <code>nn.Embedding</code>) and one which embeds tokens where order doesn't matter (i.e., <code>nn.EmbeddingBag</code>). I ran into numerical stability issues when creating my own EmbeddingBag-like object and doing the reduction myself so I'd like to use the officially support <code>nn.EmbeddingBag</code>; however, it seems like my attempt to tie weights (below) doesn't work</p>
<pre><code>#!/usr/bin/env python3

import torch
import torch.nn as nn

if __name__ == &quot;__main__&quot;:
    V, max_seq, padding_idx, emb_dim, B = 10, 100, 1, 512, 32

    # Create an embedding layer and initialize an embeddingbag with those weights
    emb_layer = nn.Embedding(V, emb_dim, padding_idx=padding_idx)
    emb_bag = nn.EmbeddingBag.from_pretrained(emb_layer.weight, freeze=False, padding_idx=padding_idx)

    tokens = torch.randint(0, V, (B, max_seq))
    y = torch.randn((B, emb_dim))
    loss = nn.MSELoss()

    # backprop through the embedding bag
    y_ = emb_bag(tokens)
    l = loss(y_, y)
    assert emb_bag.weight.grad is None
    l.backward()
    assert emb_bag.weight.grad is not None

    # if we're tying weights, backpropping through the emb bag should
    # yield the same gradients in the embedding layer, but... the following assertion fails
    assert emb_layer.weight.grad is not None and \
        torch.allclose(emb_bag.weight.grad, emb_layer.weight.grad)
</code></pre>
<p>Is there some way to tie the weights in both or do I need to be creative with how I emulate the embeddingbag behavior?</p>
",Vectorization & Embeddings,tie embeddings like tie embedding layer two part neural network one embeds token order matter e one embeds token order matter e ran numerical stability issue creating embeddingbag like object reduction like use officially support however seems like attempt tie weight work way tie weight need creative emulate embeddingbag behavior
Extracting text from garbled PDF,"<p>I have a PDF file with valuable textual information.</p>

<p>The problem is that I cannot extract the text, all I get is a bunch of garbled symbols. The same happens if I copy and paste the text from the PDF reader to a text file. Even <em>File -> Save as text</em> in Acrobat Reader fails.</p>

<p>I have used all tools I could get my hands on and the result is the same. I believe that this has something to do with fonts embedding, but I don't know what exactly?</p>

<p>My questions:</p>

<ul>
<li><strong>What is the culprit</strong> of this weird <strong>text garbling</strong>?</li>
<li><strong>How to extract the text content</strong> from the PDF (programmatically, with a tool, manipulating the bits directly, etc.)?</li>
<li>How to fix the PDF to not garble on copy?</li>
</ul>
",Vectorization & Embeddings,extracting text garbled pdf pdf file valuable textual information problem extract text get bunch garbled symbol happens copy paste text pdf reader text file even file save text acrobat reader fails used tool could get hand result believe ha something font embedding know exactly question culprit weird text garbling extract text content pdf programmatically tool manipulating bit directly etc fix pdf garble copy
How do we predict 1 single Datapoint with LogisticRegression,"<p>I have been working on an NLP problem (text classification).</p>
<p>I first preprocessed the text, then trained a model on that data (After Tfidf)</p>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(ref_red, uniqueOutput...)

clf = LogisticRegression(C = 0.9, penalty = 'l1', solver = 'liblinear') # Grid Search    
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print(accuracy_score(y_pred, y_test))
</code></pre>
<p>Now, I want to try and test the model on a single datapoint (ML inference).
How do pass a single datapt to predict function?</p>
<p>Please let me know.</p>
",Vectorization & Embeddings,predict single datapoint logisticregression working nlp problem text classification first preprocessed text trained model data tfidf want try test model single datapoint ml inference pas single datapt predict function please let know
Is there a list of all embedding techniques used in all applications of Machine learning?,"<p>I started to learn embedding techniques used in machine learning and allied fields. There are word embeddings, graph embeddings, and network embeddings. I was overwhelmed by the various embedding techniques as I googled. Is there a good repository or book or resource that starts with the need for embedding and slowly moves onto different advanced embedding techniques in NLP, Computer Vision, Knowledge Graphs etc.</p>
<p>I need an organic development of the subject so that I can think in different directions and to b able to come up with my own techniques in the embedding space.</p>
",Vectorization & Embeddings,list embedding technique used application machine learning started learn embedding technique used machine learning allied field word embeddings graph embeddings network embeddings wa overwhelmed various embedding technique googled good repository book resource start need embedding slowly move onto different advanced embedding technique nlp computer vision knowledge graph etc need organic development subject think different direction b able come technique embedding space
Using Bert to convert Vectors to Word in pytorch,"<p>I am using Bert for sentence embedding and I would like to then take that embedding and convert back to word. How would I go about doing this? Is this even possible?</p>
",Vectorization & Embeddings,using bert convert vector word pytorch using bert sentence embedding would like take embedding convert back word would go even possible
"How to take most frequent n-grams across all dataframe rows and transpose as columns, with new entries being the tf-idf score?","<p>Apologies for the poorly worded titled; I am new to NLP using Python and am having trouble. I have a dataframe of news articles that looks like this:</p>
<pre><code>press = 

    | Article_Text         | Date   | Unigrams                 | Bigrams                                 | Trigrams                             |
    | -------------------- | ------ | ------------------------ | --------------------------------------- | ------------------------------------ |
    | This is the Article  | 5-5-21 | [this, is, the, article] | [(this, is), (is, the), (the, article)] | [(this, is, the),(is, the, article)] |   
    | A second Article     | 9-2-20 | [a, second, article]     | [(a,second), (second,article)]          | [(a, second, article)]               |
</code></pre>
<p>I want to take the top 30 most frequent uni-,bi-, and trigrams (30 total among all 3 categories, not 30 for each ngram) and then have those values become new columns in the dataframe. So a new column could be [this,is] if that was one of the most frequent occurring across all articles. The entries in each of these new fields would then be the tfidf score for that n-gram in that article (row). I'm able to get the current dataframe but cannot go further.
My code:</p>
<pre><code>def tokenize(article):
    lemmatizer = nltk.stem.WordNetLemmatizer()
    word_tokens = word_tokenize(article.lower())  
    clean_tokens = [lemmatizer.lemmatize(w) for w in word_tokens if not w in stop_words]
    return clean_tokens

press['unigrams'] = press['Article_Text'].apply(tokenize)
press['bigrams'] = press['unigrams'].apply(lambda row: list(nltk.ngrams(row, 2)))
press['trigrams'] = press['unigrams'].apply(lambda row: list(nltk.ngrams(row, 3)))
</code></pre>
<p>I've tried using texthero tfidf and term frequency with no luck. Any point in the right direction would be appreciated.</p>
",Vectorization & Embeddings,take frequent n gram across dataframe row transpose column new entry tf idf score apology poorly worded titled new nlp using python trouble dataframe news article look like want take top frequent uni bi trigram total among category ngram value become new column dataframe new column could wa one frequent occurring across article entry new field would tfidf score n gram article row able get current dataframe go code tried using texthero tfidf term frequency luck point right direction would appreciated
TFIDVectorizer for Word Embedding/Vectorization,"<p>I want to compare two bodies of text ( A and B ) and check for the similarity between them</p>
<p>Here's my current approach:</p>
<pre><code>Turn both bodies of text into vectors

Compare these vectors using a cosine similarity measure

Return the result
</code></pre>
<p>The very first step is what is giving me pause. How would I do this with TFIDVectorizer? Is it enough to put both bodies of text in a list, fit_transform them and then put their resultant matrices in my cosine similarity measure?</p>
<p>Is there some training process with TFIDVectorizer, a vocabulary matrix ( fit() )? If so, how do I turn A and B into vectors so that I could put them into a cosine similarity measure?</p>
<p>P.S I understand what other options exist, I'm curious specifically about TFIDVectorizer</p>
",Vectorization & Embeddings,tfidvectorizer word embedding vectorization want compare two body text b check similarity current approach first step giving pause would tfidvectorizer enough put body text list fit transform put resultant matrix cosine similarity measure training process tfidvectorizer vocabulary matrix fit turn b vector could put cosine similarity measure p understand option exist curious specifically tfidvectorizer
Apply tfidf to a pandas column of word tokens,"<p>I now have a df with a list of tokenized words,I want to create an additional column with tfidf scores of each token in that column(corpus being all tokens in that column, and tf calculated with the group of tokens in each row).</p>
<p>Thanks<a href=""https://i.sstatic.net/T5R6e.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/T5R6e.png"" alt=""enter image description here"" /></a></p>
",Vectorization & Embeddings,apply tfidf panda column word token df list tokenized word want create additional column tfidf score token column corpus token column tf calculated group token row thanks
i get an NameError although i defined my variable,"<p>hello my programmer friends... i'm doing my first NLP project that counts and shows 5 documents TFIDF. here's part of the code:</p>
<pre><code>def IDF(corpus , unique_words):
    idf_dict = {}
    N = len(corpus)
    for i in unique_words:
        count = 0
        for sen in corpus:
            if i in sen.split():
                count = count+1
            idf_dict[i] = (math.log((1 + N) / (count+1))) + 1
    return idf_dict

def fit(whole_data):
    unique_words = set()
    if isinstance(whole_data, (list,)):
        for x in whole_data:
            for y in x.split():
                if len(y)&lt;2:
                    continue
                unique_words.add(y)
            unique_words = sorted(list(unique_words))
            vocab = {j:i for i,j in enumerate(unique_words)}
    Idf_values_of_all_unique_words = IDF(whole_data,unique_words)
    return vocab, Idf_values_of_all_unique_words
vocabulary, idf_of_vocabulary = fit(corpus)
</code></pre>
<p>The word <strong>IDF</strong> in line 22 gives me a NameError.
is it about positioning?</p>
",Vectorization & Embeddings,get nameerror although defined variable hello programmer friend first nlp project count show document tfidf part code word idf line give nameerror positioning
Use word2vec in tokenized sentences,"<p>I am trying to create a emotion recognition model resorting to SVM. I have a big dataset of sentences each one with a labeled emotion. After text pre-processing, I have a pandas data frame containing the tokenized sentences, like it can be seen in [1.] <a href=""https://i.sstatic.net/tbRqB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tbRqB.png"" alt=""Dataframe adter pre-processing"" /></a>.</p>
<p>My objective is to turn all this tokenized sentences to word embeddings so that I can train models such as SVM. The problem is how to use this date frame as input to word2vec or any other word embedding model.</p>
",Vectorization & Embeddings,use word vec tokenized sentence trying create emotion recognition model resorting svm big dataset sentence one labeled emotion text pre processing panda data frame containing tokenized sentence like seen objective turn tokenized sentence word embeddings train model svm problem use date frame input word vec word embedding model
What do weights in the neural network Word2vec actually measure?,"<p>I've been hearing a lot about the neural network Word2vec, which is able to solve literary analogies with respect to literary context. People often describe the weights as trained bias put in place by previously labeled data, but what is often not described is what do these weights actually calculate. In the case of Word2vec, what do their 300 hidden weights calculate? Contexual position? Connotation frequencies? A diversity of numericalized grammatical features?</p>
<p>From my standpoint, I've been able to visualize neural networks up to the complexity of an algorithm trained for boolean XOR processing. In that case, I know that weights add numericalized bias to the output, which simply gives a 0 or 1 for False and True, respectively. However, I can't make that connection to Word2vec, which is in a completely different genre (literary). Can someone explain in detail?</p>
",Vectorization & Embeddings,weight neural network word vec actually measure hearing lot neural network word vec able solve literary analogy respect literary context people often describe weight trained bias put place previously labeled data often described weight actually calculate case word vec hidden weight calculate contexual position connotation frequency diversity numericalized grammatical feature standpoint able visualize neural network complexity algorithm trained boolean xor processing case know weight add numericalized bias output simply give false true respectively however make connection word vec completely different genre literary someone explain detail
Custom text pre-processing saved in Tensorflow model,"<p>How to write custom text pre-processing that could be <strong>saved</strong> as part of a model?</p>
<p>Suppose that I would like to have two features:</p>
<ol>
<li>auto-correct string input with some function. Words might change after this operation</li>
<li>do query expansion of string input, such that outcome text/tokens might contain few additional words(for which weights would be trained).</li>
</ol>
<p>Something like this:</p>
<ol>
<li><p>fl<strong>i</strong> to London -&gt; Fl<strong>y</strong> to London</p>
</li>
<li><p>fly to London -&gt; Fly to London <strong>loc_city</strong></p>
<p>-&gt; this token would need to be in vocabulary in advance, which could be done</p>
</li>
</ol>
<p>After steps 1 and/or 2, feed the result to TextVectorisation / Embedding layer ?</p>
<p>There is <code>standardize</code> callback, but I do not see obvious way of doing that with existing tf.string operations.</p>
<p>Ideally, there is a callback function / layer which accepts string(or tokens) and maps to another string(or string tokens).</p>
",Vectorization & Embeddings,custom text pre processing saved tensorflow model write custom text pre processing could saved part model suppose would like two feature auto correct string input function word might change operation query expansion string input outcome text token might contain additional word weight would trained something like fli london fly london fly london fly london loc city token would need vocabulary advance could done step feed result textvectorisation embedding layer callback see obvious way existing tf string operation ideally callback function layer accepts string token map another string string token
Using a Word2Vec Model to Extract Data,"<p>I've used gensim Word2Vec to learn the embedding of monetary amounts and other numeric data in bank transaction memos. The goal is to use this to be able to extract these amounts and currencies from future input strings.</p>
<p><strong>Design</strong>
Our input strings are something like</p>
<pre><code>&quot;AMAZON.COM TXNw98e7r3347 USD 49.00 @ 1.283&quot;
</code></pre>
<p>During preprocessing, I tokenize and also replace all tokens that have the possibility of being a monetary amount (string consisting only of digits, commas, and &lt;= 1 decimal point/period) with a special VALUE_TOKEN. And I also manually replace exchange rates with RATE_TOKEN. The result would be</p>
<pre><code>[&quot;AMAZON&quot;, &quot;.COM&quot;, &quot;TXNw&quot;, &quot;98&quot;, &quot;e&quot;, &quot;7&quot;, &quot;r&quot;, &quot;3347&quot;, &quot;USD&quot;, &quot;VALUE_TOKEN&quot;, &quot;@&quot;, &quot;RATE_TOKEN&quot;]
</code></pre>
<p>With all my preprocessed lists of strings in list <code>data</code>, I generate model</p>
<pre><code>model = Word2Vec(data, window=3, min_count=3)
</code></pre>
<p>The embeddings of model that I'm most interested in are that of VALUE_TOKEN, RATE_TOKEN, as well as any currencies (USD, EUR, CAD, etc.). Now that I generated the model, I'm not sure what to do with it.</p>
<p><strong>Problem</strong>
Say I have a new string that the model has never seen before,</p>
<pre><code>new_string = &quot;EUR 299.99 RATE 1.3289 WITH FEE 5.00&quot;
</code></pre>
<p>I would like to use <code>model</code> to identify which tokens of <code>new_string</code> is most contextually similar to VALUE_TOKEN (which should return [&quot;299.99&quot;, &quot;5.00&quot;]), which is closest to RATE_TOKEN (&quot;1.3289&quot;). It should be able to classify these based on the learned embedding. I can preprocess <code>new_string</code> the way I do with the training data, but because I don't know the exchange rate before hand, all three tokens of [&quot;299.99&quot;, &quot;5.00&quot;, &quot;1.3289&quot;] will be tagged the same (either with VALUE_TOKEN or a new UNIDENTIFIED_TOKEN).</p>
<p>I've looked into methods like <code>most_similar</code> and <code>similarity</code> but don't think they work for tokens that are not necessarily in the vocabulary. What methods should I use to do this? Is this the right approach?</p>
",Vectorization & Embeddings,using word vec model extract data used gensim word vec learn embedding monetary amount numeric data bank transaction memo goal use able extract amount currency future input string design input string something like preprocessing tokenize also replace token possibility monetary amount string consisting digit comma decimal point period special value token also manually replace exchange rate rate token result would preprocessed list string list generate model embeddings model interested value token rate token well currency usd eur cad etc generated model sure problem say new string model ha never seen would like use identify token contextually similar value token return closest rate token able classify based learned embedding preprocess way training data know exchange rate hand three token tagged either value token new unidentified token looked method like think work token necessarily vocabulary method use right approach
Dataset for Doc2vec,"<p>I have a question is there already any free dataset available to test  doc2vec and if in case I wanted to create my own dataset what could be an appropriate way to do it.</p>
",Vectorization & Embeddings,dataset doc vec question already free dataset available test doc vec case wanted create dataset could appropriate way
How to combine TFIDF features with other features,"<p>I have a classic NLP problem, I have to classify a news as fake or real.</p>

<p>I have created two sets of features:</p>

<p>A) Bigram Term Frequency-Inverse Document Frequency</p>

<p>B) Approximately 20 Features associated to each document obtained using pattern.en (<a href=""https://www.clips.uantwerpen.be/pages/pattern-en"" rel=""noreferrer"">https://www.clips.uantwerpen.be/pages/pattern-en</a>) as subjectivity of the text, polarity, #stopwords, #verbs, #subject, relations grammaticals etc ...</p>

<p>Which is the best way to combine the TFIDF features with the other features for a single prediction?
Thanks a lot to everyone.</p>
",Vectorization & Embeddings,combine tfidf feature feature classic nlp problem classify news fake real created two set feature bigram term frequency inverse document frequency b approximately feature associated document obtained using pattern en subjectivity text polarity stopwords verb subject relation grammaticals etc best way combine tfidf feature feature single prediction thanks lot everyone
what difference between nltk tokenizer and RobertaTokenizer?,"<p>why here we take <code>RobertaTokenizer</code> and why do we perform Word2Vec?</p>
<pre class=""lang-py prettyprint-override""><code> def __init__(self, modelpath, tokenizer):
        self.model = Word2Vec.load(modelpath)
        self.tokenizer = RobertaTokenizer.from_pretrained('microsoft/graphcodebert-base', cache_dir = config[&quot;cached_dir&quot;])
</code></pre>
<p>what difference between NLTK tokenizer and <code>RobertaTokenizer</code>?
why we need <code>RobertaTokenizer</code> instead NLTK?
I know the NLTK tokenizer creates a word index of every word in the set and we perform word2vec to get the semantic meaning between words, but I a can not find why <code>RobertaTokenizer</code> is proposed instead tokenizer from NLTK?</p>
",Vectorization & Embeddings,difference nltk tokenizer robertatokenizer take perform word vec difference nltk tokenizer need instead nltk know nltk tokenizer creates word index every word set perform word vec get semantic meaning word find proposed instead tokenizer nltk
How to give input to non-keras word embedding model from Keras model during training and get back output?,"<p>I am trying to create a model using Keras which gives a input value to another non-keras word embedding model and get embedding of word back into this main keras model.</p>
<p>So, the problem is keras model run on graph and while giving input to non keras model, the value of input is of the form</p>
<pre><code>Tensor(&quot;IteratorGetNext:9&quot;, shape=(None,), dtype=string)
&lt;class 'tensorflow.python.framework.ops.Tensor'&gt;
</code></pre>
<p>But the word embedding model (Fasttext) only accept input in string format. So how to get the value of string in <code>Tensor(&quot;IteratorGetNext:9&quot;, shape=(None,), dtype=string)</code> and give it as input to Fasttext word embedding model and get output embedding and convert this output numpy embeddings back into tensor to be used into Keras model?</p>
<p>One way is to turn <code>run_eagerly = True</code> in <code>model.compile()</code> as it would give all values in numpy format instead of Tensor (running in eager mode instead of graph mode).</p>
<pre><code>tf.Tensor([list of strings], shape=(batch_size,), dtype=string)
&lt;class 'tensorflow.python.framework.ops.EagerTensor'&gt;
</code></pre>
<p>But it would make model training time high.</p>
<p>Code :</p>
<pre><code>class MovieModel(tf.keras.Model):

  def __init__(self):
    super().__init__()

    max_tokens = 10_000

    self.title_embedding = tf.keras.Sequential([
         tf.keras.layers.StringLookup(
                vocabulary=unique_movie_titles,mask_token=None),
         tf.keras.layers.Embedding(len(unique_movie_titles) + 1, 32)
    ])
    

  def call(self, inputs):
     return tf.concat([
          self.title_embedding(inputs[&quot;movie_title&quot;]),
          #Word embeddings of &quot;movie_title&quot;
     ], axis=1)

  #Now I want to give &quot;movie_title&quot; as an input to FastText Model.             
   inputs[&quot;movie_title&quot;] is a batch of movie titles. So I want to 
   process each string of this batch and split them into words and   
   call FastText Model to get embedding of every word and take 
   their average. And pass this averaged embedding along with
   self.title_embedding in def.call(). But as Keras train model on 
   Graph, I am not able to acccess the values of string in batch.
</code></pre>
<p>I can access the embeddings of word using <code>fasttext_model.wv[word]</code>
I am using <a href=""https://www.tensorflow.org/recommenders/examples/deep_recommenders"" rel=""nofollow noreferrer"">this</a> as reference to build my model.</p>
",Vectorization & Embeddings,give input non kera word embedding model kera model training get back output trying create model using kera give input value another non kera word embedding model get embedding word back main kera model problem kera model run graph giving input non kera model value input form word embedding model fasttext accept input string format get value string give input fasttext word embedding model get output embedding convert output numpy embeddings back tensor used kera model one way turn would give value numpy format instead tensor running eager mode instead graph mode would make model training time high code access embeddings word using using reference build model
"Python, compare strings in Counter and assign to closest match","<p>I have a list of input text written by humans. This text is imported to python and a Counter is generated. In the Counter, all the inputs from humans are listed and counted. At the end obtain something like:</p>
<p>&quot;Input 1&quot; : 3,</p>
<p>&quot;Input 2&quot; : 1,
...</p>
<p>The problem i have is that sometimes these inputs have spelling mistakes or are missing a space between words etc. How could I go through this list and compare it to some reference Inputs and asign to each counter row the total counts of the well written Inputs + the ones coming from the most similar Inputs with spelling mistakes. I know this falls on the NLP field but i can't really find a way to do this in a counter</p>
",Vectorization & Embeddings,python compare string counter assign closest match list input text written human text imported python counter generated counter input human listed counted end obtain something like input input problem sometimes input spelling mistake missing space word etc could go list compare reference input asign counter row total count well written input one coming similar input spelling mistake know fall nlp field really find way counter
How writing a very large dictionary to a json file?,"<p>Please tell me what am I doing wrong. I want to create a dictionary from a lot of data. I have a file of 200,000 articles that contains 125,000 unique words, it looks like this:</p>
<pre><code>data = [[['article_1', 'city', 0.43], ['article_1', 'big', 0.38], ['article_1', 'beautiful', 0.25]], 
        [['article_2', 'sun', 0.65], ['article_2', 'beautiful', 0.41], ['article_2', 'shining', 0.21]],
        [['article_3', 'big', 0.72], ['article_3', 'beautiful', 0.50], ['article_3', 'butterfly', 0.25]]]
</code></pre>
<p>That is, each list is a separate article, which consists of lists: [article_number, word, word weight (tfidf)].</p>
<p>I need to get a dictionary of the form:</p>
<pre><code>{'city': [('article_1', 0.43)],
 'big': [('article_3', 'big', 0.72), ('article_1', 'big', 0.38)],
 'beautiful': [('article_3', 0.50), ('article_2', 0.41), ('article_1', 0.25)],
 'sun': [('article_2', 0.65)],
 'shining': [('article_2', 0.21)],
 'butterfly': [('article_3',0.25)]}
</code></pre>
<p>I'm trying to create a dictionary by immediately writing it to a json file (in google colab), but this uses all the RAM, and as a result, the google colab environment is disabled. I use this code:</p>
<pre><code>with open('drive/MyDrive/dictionary.json', 'w') as f:
    d = defaultdict(list) 
    [d[i[1]].append((i[0],i[2])) for j in data for i in j]
    dct = dict(d.items())
    dct = dict(sorted(d.items()))
    f.write(json.dumps(dct) + '\n')
</code></pre>
<p>Tell me, please, what am I doing wrong? Or maybe it is better to store such a large amount of data in some other way?</p>
",Vectorization & Embeddings,writing large dictionary json file please tell wrong want create dictionary lot data file article contains unique word look like list separate article consists list article number word word weight tfidf need get dictionary form trying create dictionary immediately writing json file google colab us ram result google colab environment disabled use code tell please wrong maybe better store large amount data way
Finding similarity of 1 paragraph in different documents with Doc2vec,"<p>how to find one target paragraph or document similar to other lists of documents to the target paragraph that is semantically similar.</p>
<pre><code>import os
import gensim
import smart_open
import random
from nltk.tokenize import word_tokenize
# Set file names for train and test data
test_data_dir =('C:\\Users\\hamza\\Desktop\\')
train_file = os.path.join(test_data_dir, 'read-me.txt')
target_file = os.path.join(test_data_dir, 'read-me2.txt')

def read_file(filename):
    
    try:
        with open(filename, 'r') as f:
            data = f.read()
        return data
    
    except IOError:
        print(&quot;Error opening or reading input file: &quot;, filename)
        sys.exit()
def read_corpus(fname, tokens_only=False):
    with smart_open.open(fname, encoding=&quot;iso-8859-1&quot;) as f:
        for i, line in enumerate(f):
            tokens = gensim.utils.simple_preprocess(line)
            if tokens_only:
                yield tokens
            else:
                # For training data, add tags
                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])

train_data = list(read_corpus(train_file))
target_data = word_tokenize(read_file(target_file))

# print(target_data)
# print(test_corpus)
model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)
model.build_vocab(train_data)
# print(f&quot;Word 'noise' appeared {model.wv.get_vecattr('noise', 'count')} times in the training corpus.&quot;)
model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)
inferred_vector = model.infer_vector(target_data)
sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))
print(sims)
</code></pre>
<p>Output</p>
<p>[(1, 0.20419511198997498),
(2, 0.1924923211336136),
(0, 0.10696495324373245)]</p>
<p>Now how I can match target data to train data and how I will know how much they are similar is there any way to scale the similarity into percentage?</p>
",Vectorization & Embeddings,finding similarity paragraph different document doc vec find one target paragraph document similar list document target paragraph semantically similar output match target data train data know much similar way scale similarity percentage
"Is it necessary to do stopwords removal ,Stemming/Lemmatization for text classification while using Spacy,Bert?","<p>Is stopwords removal ,Stemming and Lemmatization necessary   for text classification while using Spacy,Bert or other advanced NLP models for getting the vector embedding of the text ?</p>
<p>text=&quot;The food served in the wedding was very delicious&quot;</p>
<p>1.since Spacy,Bert were trained on huge raw datasets are there any benefits of apply stopwords removal ,Stemming and Lemmatization on these text before generating the embedding using bert/spacy for text classification task ?</p>
<p>2.I can understand stopwords removal ,Stemming and Lemmatization will be good when we use countvectorizer,tfidf vectorizer to get embedding of sentences .</p>
",Vectorization & Embeddings,necessary stopwords removal stemming lemmatization text classification using spacy bert stopwords removal stemming lemmatization necessary text classification using spacy bert advanced nlp model getting vector embedding text text food served wedding wa delicious since spacy bert trained huge raw datasets benefit apply stopwords removal stemming lemmatization text generating embedding using bert spacy text classification task understand stopwords removal stemming lemmatization good use countvectorizer tfidf vectorizer get embedding sentence
Parameters for training a sentence-similarity model using Bert?,"<p>I have a list of sentences :</p>
<pre><code>sentences = [&quot;Missing Plate&quot;, &quot;Plate not found&quot;]
</code></pre>
<p>I am trying to find the most similar sentences in the list by using Transformers model with <a href=""https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2"" rel=""nofollow noreferrer"">Huggingface embedding</a>. I am able to find the similar sentences but the model is still not able to identify the difference between :</p>
<pre><code>&quot;Message ID exists&quot;  
&quot;Message ID doesn't exist&quot;
</code></pre>
<p>[Note: I am trying to find the similarity by using the <a href=""https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.cos_sim"" rel=""nofollow noreferrer"">Cosine similarity</a> from pytorch]</p>
<p>Can you suggest me ways to hyperparameter tune my model so that the model can weigh in more on the negative words and consider them opposite?</p>
<p>I found the <a href=""https://huggingface.co/docs/transformers/v4.20.1/en/main_classes/trainer#transformers.TrainingArguments"" rel=""nofollow noreferrer"">list of parameters that can be tuned</a> but not sure what the best parameters would be</p>
<p>Thanks!</p>
",Vectorization & Embeddings,parameter training sentence similarity model using bert list sentence trying find similar sentence list using transformer model huggingface embedding able find similar sentence model still able identify difference note trying find similarity using cosine similarity pytorch suggest way hyperparameter tune model model weigh negative word consider opposite found list parameter tuned sure best parameter would thanks
Cannot download GloVe embeddings. Have they been moved or is downloads.cs.stanford.edu down temporarily?,"<p>I am attempting to download glove.840B.300d.zip. I used the link at <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a> and also ran <code>wget https://nlp.stanford.edu/data/glove.840B.300d.zip</code>. The output from wget looks as follows:</p>
<pre><code>--2022-06-23 15:50:30--  (try: 2)  https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip
Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... failed: Connection timed out.
Retrying.
</code></pre>
<p>Does anyone know if this is a temporary issue? Thank you!</p>
",Vectorization & Embeddings,download glove embeddings moved downloads c stanford edu temporarily attempting download glove b zip used link also ran output wget look follows doe anyone know temporary issue thank
How to identify the similar words using the word2vec,"<p>input: I have a set of words(N) &amp; input sentence</p>
<p>problem statement:
the sentence is dynamic, the user can give any sentence related to one business domain. we have to map the input sentence tokens to the set of words based on the closeness.</p>
<p>for example, we can use different words to ask the same meaning questions, and hard to maintain all the synonyms hence we have a mechanism to find similar words, we can map easily.</p>
<pre><code>1) A meeting scheduled by john
2) A meeting organized by john
</code></pre>
<p>user can frame a sentence in different ways, like the above example.</p>
<p>scheduled &amp; organized are very close.</p>
<p>N set has the word, scheduled. if a user gives a sentence like (2), I have to map the <strong>organized</strong> with <strong>scheduled</strong>.</p>
",Vectorization & Embeddings,identify similar word using word vec input set word n input sentence problem statement sentence dynamic user give sentence related one business domain map input sentence token set word based closeness example use different word ask meaning question hard maintain synonym hence mechanism find similar word map easily user frame sentence different way like example scheduled organized close n set ha word scheduled user give sentence like map organized scheduled
How to get three dimensional vector embedding for a list of words,"<p>I have been asked to create three dimensional vector embeddings for a series of words. Although I understand what an embedding is and that <code>word2vec</code> will be able to create the vector embeddings, I cannot find a resource that shows me how to create a <em>three</em> dimensional vector (all the resources show many more dimensions than this).</p>
<p>The format I have to create the file in is:</p>
<pre><code>house    34444     0.3232 0.123213 1.231231
dog    14444    0.76762 0.76767 1.45454
</code></pre>
<p>which is in the format <code>&lt;token&gt;\t&lt;word_count&gt;\t&lt;vector_embedding_separated_by_spaces&gt;</code></p>
<p>Can anyone point me towards a resource that will show me how to create the desired file format given some training text?</p>
",Vectorization & Embeddings,get three dimensional vector embedding list word asked create three dimensional vector embeddings series word although understand embedding able create vector embeddings find resource show create three dimensional vector resource show many dimension format create file format anyone point towards resource show create desired file format given training text
doubts regarding word2vec Continuous bag-of-words (CBOW),"<p><a href=""https://i.sstatic.net/7BXvM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7BXvM.png"" alt=""enter image description here""></a></p>

<p>For the CBOW model, the INPUT words are feed into the training model simultaneously or one by one?</p>

<p>Thanks</p>
",Vectorization & Embeddings,doubt regarding word vec continuous bag word cbow cbow model input word feed training model simultaneously one one thanks
NLP - How to get the a list of frequently asked questions on a list of questions,"<p>Everything is in the title, so basically I have a list of several questions as strings and the idea is to get another list of frequently asked questions within that first list of questions.</p>
<p>I don't know if it'll make sense but I'll try to explain the approach I tried.</p>
<p>The approach consist of calculating the cosine similarity of each element of the list with the rest of the elements not including the element being processed to prevent performing calculations with the same element.</p>
<p>That said, a dictionary will be created containing the keys as the index of each element being processed, while the values will be a list of indexes of each element which has a cosine similarity above the threshold with the index of the key.</p>
<p>Once the dictionary has been created, the keys' indexes with the highest length of list on their values will be considered as being frequent questions, after that you can pick up the top 10 or any number you'd like.</p>
<p>Firstly, a downside is that it takes a lot of time to execute knowing that I've +60k questions (14 days).
Secondly, I don't know if it's the best way to solve this problem, what do you think?
Finally, if you have a more clearer and better idea to solve the problem, I'm all ears, it can also help other people with the same problem.</p>
<pre><code>import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')

list_of_questions = ['How does umap know which high dimensional datapoint belongs to which cluster?',...]

score = dict()
threshold = 0.7
#tokenization

#sw contains the list of stopwords
sw = stopwords.words('english')

for index, main_question in enumerate(list_of_questions):
    similarities = []
    temp_list = list_of_questions.copy()
    X_list = word_tokenize(main_question)
    temp_list.pop(index)
    for question_ in temp_list:
        l1 =[];l2 =[]
        Y_list = word_tokenize(question_)
        
        if len(X_list) == 0 or len(Y_list) == 0:
            continue
        #remove stop words from the string
        X_set = {w for w in X_list if not w in sw} 
        Y_set = {w for w in Y_list if not w in sw}

        #form a set containing keywords of both strings 
        rvector = X_set.union(Y_set) 
        for w in rvector:
            if w in X_set: l1.append(1) # create a vector
            else: l1.append(0)
            if w in Y_set: l2.append(1)
            else: l2.append(0)
        c = 0

        #cosine formula 
        try:
            for i in range(len(rvector)):
                    c+= l1[i]*l2[i]
            cosine = c / float((sum(l1)*sum(l2))**0.5)
            if cosine &gt; threshold:
                similarities.append(list_of_questions.index(question_))
                print(&quot;Cosine similarity: &quot;, cosine)
        except:
            continue
    score[index] = similarities
</code></pre>
",Vectorization & Embeddings,nlp get list frequently asked question list question everything title basically list several question string idea get another list frequently asked question within first list question know make sense try explain approach tried approach consist calculating cosine similarity element list rest element including element processed prevent performing calculation element said dictionary created containing key index element processed value list index element ha cosine similarity threshold index key dictionary ha created key index highest length list value considered frequent question pick top number like firstly downside take lot time execute knowing k question day secondly know best way solve problem think finally clearer better idea solve problem ear also help people problem
Remove 1-grams if 2+ grams are present in text,"<p>I'm getting the following output from my function
<code>[(I, None), (love, 'B2'), (ice, 'A2'), (ice cream, 'A1'), (cream, 'A2'), (and, 'A1'), (the, 'A1'), (the second person, 'B2'), (second, 'B1'), (person, 'A1')]</code>. However, the text I need to return must be the same as input <code>&quot;I love ice cream and the second person&quot;</code>, so I need to remove all the 1-grams if there's any 2+ gram containing the same word in vicinity.</p>
<p>So far I've tried this solution <a href=""https://stackoverflow.com/questions/57528505/sklearn-tfidf-vectorizer-remove-n-2-and-n-1-grams-if-n-gram-exists"">sklearn tfidf vectorizer - remove n-2 and n-1 grams if n gram exists</a> but I struggle to put these 2+ grams back in the place they've been to.</p>
<p>Any help appreciated!</p>
",Vectorization & Embeddings,remove gram gram present text getting following output function however text need return must input need remove gram gram containing word vicinity far tried solution href tfidf vectorizer remove n n gram n gram exists struggle put gram back place help appreciated
Is there a way to visualize the embeddings obtained from Wav2Vec 2.0?,"<p>I'm looking to train a word2vec 2.0 model from scratch, but I am a bit new to the field. Crucially, I would like to train it using a large dataset of non-human speech (i.e. cetacean sounds) in order to capture the underlying structure.</p>
<p>Once the pre-training is performed, is it possible to visualize the embeddings the model creates, in a similar way to how latent features are visualized in image processing when using e.g. CNNs? Or are the representations too abstract to be mapped to a spectrogram?</p>
<p>What I would like to do is to see what features the network is learning as the units of speech.</p>
<p>Thanks in advance for the help!</p>
",Vectorization & Embeddings,way visualize embeddings obtained wav vec looking train word vec model scratch bit new field crucially would like train using large dataset non human speech e cetacean sound order capture underlying structure pre training performed possible visualize embeddings model creates similar way latent feature visualized image processing using e g cnns representation abstract mapped spectrogram would like see feature network learning unit speech thanks advance help
Creating word embedings from bert and feeding them to random forest for classification,"<p>I have used bert base pretrained model with 512 dimensions to generate contextual features. Feeding those vectors to random forest classifier is providing 83 percent accuracy but in various researches i have seen that bert minimal gives 90 percent.
I have some other features too like word2vec, lexicon, TFIDF and punctuation features.
Even when i merged all the features i got 83 percent accuracy. The research paper which i am using as base paper mentioned an accuracy score of 92 percent but they have used an ensemble based approach in which they classified through bert and trained random forest on weights.
But i was willing to do some innovation thus didn't followed that approach.
My dataset is biased to positive reviews so according to me the accuracy is less as model is also biased for positive labels but still I am looking for an expert advise</p>
<p>Code implementation of bert</p>
<p><code>https://github.com/Awais-mohammad/Sentiment-Analysis/blob/main/Bert_Features.ipynb</code></p>
<p>Random forest on all features independently</p>
<p><code>https://github.com/Awais-mohammad/Sentiment-Analysis/blob/main/RandomForestClassifier.ipynb</code></p>
<p>Random forest on all features jointly</p>
<p><code>https://github.com/Awais-mohammad/Sentiment-Analysis/blob/main/Merging_Feature.ipynb</code></p>
",Vectorization & Embeddings,creating word embedings bert feeding random forest classification used bert base pretrained model dimension generate contextual feature feeding vector random forest classifier providing percent accuracy various research seen bert minimal give percent feature like word vec lexicon tfidf punctuation feature even merged feature got percent accuracy research paper using base paper mentioned accuracy score percent used ensemble based approach classified bert trained random forest weight wa willing innovation thus followed approach dataset biased positive review according accuracy le model also biased positive label still looking expert advise code implementation bert random forest feature independently random forest feature jointly
How to convert small dataset into word embeddings instead of one-hot encoding?,"<p>I have a dataset of 33 words that are a mix of verbs and nouns, for eg. father, sing, etc. I have tried converting them to 1-hot encoding but for my use case, it has been suggested to look into word2vec embedding. I have looked in gensim and glove but struggling to make it work.</p>
<p><strong>How could I convert my data into an embedding?</strong> Such that two words that may be semantically closer may have a lesser distance between their respective vectors. How may this be achieved or any helpful material on the same?</p>
<p>Such as this<img src=""https://miro.medium.com/max/1400/1*sAJdxEsDjsPMioHyzlN3_A.png"" alt=""embedding"" /></p>
",Vectorization & Embeddings,convert small dataset word embeddings instead one hot encoding dataset word mix verb noun eg father sing etc tried converting hot encoding use case ha suggested look word vec embedding looked gensim glove struggling make work could convert data embedding two word may semantically closer may lesser distance respective vector may achieved helpful material
Max position embedding in BERT,"<p>I'm studying BERT right now.</p>
<p>I thought BERT limits position embedding as 512 because of the memory problem.
However, when I look up the BERT code in hugging face I found this parameter on config.</p>
<p>max_position_embeddings: The maximum sequence length that this model might
ever be used with. Typically set this to something large just in case
(e.g., 512 or 1024 or 2048).</p>
<p>As I understand this, BERT can have 1024, 2048 which are over 512.
I don't understand how this is possible.</p>
<p>Could someone explains it in more detail of it?</p>
",Vectorization & Embeddings,max position embedding bert studying bert right thought bert limit position embedding memory problem however look bert code hugging face found parameter config max position embeddings maximum sequence length model might ever used typically set something large case e g understand bert understand possible could someone explains detail
BERT with WMD distance for sentence similarity,"<p>I have tried to calculate the similarity between the two sentences using BERT and word mover distance (WMD). I am unable to find the correct formula for WMD in python. Also tried the WMD python library but it uses the word2vec model for embedding. Kindly help to solve the below problem to get the similarity score using WMD.</p>
<pre><code>sentence_obama = 'Obama speaks to the media in Illinois'
sentence_president = 'The president greets the press in Chicago'

sentence_obama = sentence_obama.lower().split()
sentence_president = sentence_president.lower().split()

#Importing bert for creating an embedding
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')


#creating an embedding of both sentences
sentence_embeddings1 = model.encode(sentence_obama)
sentence_embeddings2 = model.encode(sentence_president)

distance = WMD(sentence_embeddings1, sentence_embeddings2)
print(distance)
</code></pre>
",Vectorization & Embeddings,bert wmd distance sentence similarity tried calculate similarity two sentence using bert word mover distance wmd unable find correct formula wmd python also tried wmd python library us word vec model embedding kindly help solve problem get similarity score using wmd
Spacy &quot;en_core_web_lg&quot; model vectors giving wrong similarity output,"<p>Just starting with NLP and following along a course on Udemy. I am trying to compute word similarity using cosine similarity of vectors.</p>
<pre><code>import spacy
nlp = spacy.load('en_core_web_lg')

# Choose the words you wish to compare, and obtain their vectors
word1 = nlp.vocab['wolf'].vector
word2 = nlp.vocab['dog'].vector
word3 = nlp.vocab['cat'].vector

# Import spatial and define a cosine_similarity function
from scipy import spatial

cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)

# Write an expression for vector arithmetic
# For example: new_vector = word1 - word2 + word3
new_vector = word1 - word2 + word3

# List the top ten closest vectors in the vocabulary to the result of the expression above
computed_similarities = []

for word in nlp.vocab:
    if word.has_vector:
        if word.is_lower:
            if word.is_alpha:
                similarity = cosine_similarity(new_vector, word.vector)
                computed_similarities.append((word, similarity))

computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])

print([w[0].text for w in computed_similarities[:10]])

['wolf', 'cat', 'i', 'cuz', 'dare', 'u', 'dog', 'she', 'ai', 'ca']
</code></pre>
<p>In the above output most of the words are not even close to the new vector.
Ideal output according to the tutor should be:</p>
<pre><code>['maned', 'wolfs', 'wolf', 'lynx', 'wolve', 'yotes', 'canids', 'boars', 'foxes', 'wolfdogs']
</code></pre>
<p>Another problem is that when I am trying to compute for other words like king, man and queen the output is:</p>
<pre><code>['king', 'woman', 'she', 'who', 'wolf', 'when', 'dare', 'cat', 'was', 'not']
</code></pre>
<p>instead of</p>
<pre><code>['king','queen','commoner','highness','prince','sultan','maharajas','princes','kumbia','kings']
</code></pre>
<p>In this output words like &quot;wolf&quot; and &quot;cat&quot; are there since I ran the function on the above wolf,cat and dog example first. If I run the king,man and queen example after reloading the model, then &quot;wolf&quot; and &quot;cat&quot; are not there but some other unrelated words.</p>
<p>I have uninstalled and re-installed the model as well as recreated the environment but same result always. Am I doing something wrong? How do I fix this?</p>
",Vectorization & Embeddings,spacy en core web lg model vector giving wrong similarity output starting nlp following along course udemy trying compute word similarity using cosine similarity vector output word even close new vector ideal output according tutor another problem trying compute word like king man queen output instead output word like wolf cat since ran function wolf cat dog example first run king man queen example reloading model wolf cat unrelated word uninstalled installed model well recreated environment result always something wrong fix
How to: CNTK C# LSTM classifier of free text (NLP) using word Word2Vec embeddings,"<p>I am new to CNTK. My environment is C# (unfortunately, I am not a python or a BrainScript programmer).</p>

<p>I am trying to use CNTK to design/train/test an LSTM on <strong>free text</strong> (NLP) to select an appropriate title (from a given set of titles, about 8,000 of them in my data).</p>

<p>I've used a separate program to map each word into a 100-element vector of real numbers (the 100 is a configurable value; my non-CNTK program, GloVe, can generate any width I select).</p>

<p>My raw input looks something like:</p>

<pre><code>|label 17 |features the brown fox jumped over the ...
|label 19 |features there comes a time when all ...
...
</code></pre>

<p>Where '17' is a shorthand for the 17-th title and really is a hot-one representation: [0, 0, ..., 1, 0, 0, ...] where the '1' is in the 17-th position.</p>

<p>Each input row is a sequence of words (separated by a space) - the typical length is a few hundred words, but some data (rows) have thousands of words in it.</p>

<p>My issue is that I don't know how to insert a run-time transformation from my raw file format into something CNTK could use.</p>

<p>I can't assume in-memory data since in production we will be training on data that has millions of rows.</p>

<p>In each mini batch:</p>

<p>The '17' (in the example above) needs to be translated to [0, ..., 1, 0, ...].</p>

<p>Each word needs to be translated (via a lookup into C# Dictionary) into an array (of 100) real numbers.</p>

<p>I realize this is the Embedding layer in CNTK's LSTM but I cannot find any tutorial/example (especially in C#) of how to add a transformation layer using a non-hot-one embedding.</p>

<p>For all its worth, my template for doing this in C# is the <strong>LSTMSequenceClassifier.cs</strong> in the CNTK examples.</p>

<p>Link to CNTK example:
<a href=""https://github.com/Microsoft/CNTK/blob/master/Examples/TrainingCSharp/Common/LSTMSequenceClassifier.cs"" rel=""nofollow noreferrer"">https://github.com/Microsoft/CNTK/blob/master/Examples/TrainingCSharp/Common/LSTMSequenceClassifier.cs</a></p>

<p>Any help would be greatly appreciated. I've racked my brains on this for the past week!</p>
",Vectorization & Embeddings,cntk c lstm classifier free text nlp using word word vec embeddings new cntk environment c unfortunately python brainscript programmer trying use cntk design train test lstm free text nlp select appropriate title given set title data used separate program map word element vector real number configurable value non cntk program glove generate width select raw input look something like shorthand th title really hot one representation th position input row sequence word separated space typical length hundred word data row thousand word issue know insert run time transformation raw file format something cntk could use assume memory data since production training data ha million row mini batch example need translated word need translated via lookup c dictionary array real number realize embedding layer cntk lstm find tutorial example especially c add transformation layer using non hot one embedding worth template c lstmsequenceclassifier c cntk example link cntk example help would greatly appreciated racked brain past week
Using the embedding layer as the input for an encoder,"<p>I want to use the embedding layer as the input for encoder, however I got an error as follow.  My input <code>y</code> is a time series data with shape of <code>1*84</code>. Could you please help me with that?</p>
<pre><code>import numpy 
import torch.nn as nn

r_input    = torch.nn.Embedding(84, 10)  
activation  = nn.functional.relu
mu_r      = nn.Linear(10, 6)
log_var_r = nn.Linear(10, 6)

y = np.random.rand(1,84) 

def encode_r(y):
    y         = torch.reshape(y, (-1, 1, 84)) # torch.Size([batch_size, 1, 84])
    hidden    = torch.flatten(activation(r_input(y)), start_dim = 1)       
    z_mu      = mu_r(hidden)
    z_log_var = log_var_r(hidden)
    return z_mu, z_log_var```

Error: RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)
</code></pre>
",Vectorization & Embeddings,using embedding layer input encoder want use embedding layer input encoder however got error follow input time series data shape could please help
Understanding Dense layer after Embedding Layer in Keras,"<p>I am having some problems to understand the functioning of a Dense layer handling text sequences. Let's imagine this simple case: I have two sentences and I assign integers to the words:</p>

<pre><code>Sentence 1  I like cheese
Sentence 2  I like milk 

Sentence to sequence
Seq1    [1, 2, 3]
Seq2    [1, 2, 4]
</code></pre>

<p>Then we take the input (the integer sequences) and add them to a Embedding layer (random numbers in 2 dimensions):</p>

<pre><code>Embedding matrix        

Term    Index   Vector
I       1       [0.2 0.6]
like    2       [0.7 0.1]
cheese  3       [0.4 0.5]
milk    4       [0.1 0.9]
</code></pre>

<p>Next step is to Flatten the sequences with the embeddings in order to make it 1D:</p>

<pre><code>    Sequence to Embedding
    Seq1    [[0.2 0.6] [0.7 0.1] [0.4 0.5]]
    Seq2    [[0.2 0.6] [0.7 0.1] [0.1 0.9]]

    Flatten to 1D for Dense layer
    Seq1    [0.2 0.6 0.7 0.1 0.4 0.5]
    Seq2    [0.2 0.6 0.7 0.1 0.1 0.9]
</code></pre>

<p>Now we can use those arrays as input for the Dense layer, something that would look like this:</p>

<p>Dense layer of 3 units with input length of sequence. Dot product of the input sequence with the weights matrix of the dense layer. </p>

<p><a href=""https://i.sstatic.net/Oky8X.png"" rel=""nofollow noreferrer"">Dense layer</a></p>

<pre><code>    Dense layer                                             

    Seq1    X1  X2  X3  X4  X5  X6  ·   WH                   =     ZH   …
           0.2 0.6 0.7 0.1 0.4 0.5  ·   W1,1    W1,2    W1,3       +    
                                        W2,1    W2,2    W2,3        
                                        W3,1    W3,2    W3,3       BH   
                                        W4,1    W4,2    W4,3            
                                        W5,1    W5,2    W5,3            
                                        W6,1    W6,2    W6,3

    Seq2    X1  X2  X3  X4  X5  X6  ·   WH                   =     ZH   …
           0.2 0.6 0.7 0.1 0.1 0.9  ·   W1,1    W1,2    W1,3       +    
                                        W2,1    W2,2    W2,3        
                                        W3,1    W3,2    W3,3       BH   
                                        W4,1    W4,2    W4,3            
                                        W5,1    W5,2    W5,3            
                                        W6,1    W6,2    W6,3            
</code></pre>

<p>The problem here is that every feature corresponds to one part of the sequence, X5 and X6 belong to the vector of the last word. Normally, with Bag of Words input <strong>every X is assigned to a word</strong>, and if the word is present then it gets a weight (following TF-IDF for example). Also with numerical data, Xi represents a feature (price, temperature, GDP...) and it is always that case. <strong>Here</strong>, however, <strong>X is not assigned to a word and depends on the order</strong>. See X5 and X6 how they change because the words are different. </p>

<p><strong>The weights in the Weight Matrix of the Dense layer are assigned to a feature (Xi)</strong>, and then they are optimised.</p>

<hr>

<p><strong>My question:</strong></p>

<p><strong>How does it work if the order of words changes all the time and Xi is referring to different words?</strong></p>

<p>I understand LSTMs and other recurrent networks can handle dynamic ordering, but <strong>Dense layers seemed to me that could not work with sequential text and that the input should be fixed by One Hot vector or TF-IDF</strong> for example. Still I have seen examples of models with Sentences to Sequences of Integers, Embedding, Flatten and Dense layer architectures, plus I have tried myself and I see it does work... I would really appreciate some explanation or correction in my flow of thinking. Thanks!</p>
",Vectorization & Embeddings,understanding dense layer embedding layer kera problem understand functioning dense layer handling text sequence let imagine simple case two sentence assign integer word take input integer sequence add embedding layer random number dimension next step flatten sequence embeddings order make use array input dense layer something would look like dense layer unit input length sequence dot product input sequence weight matrix dense layer dense layer problem every feature corresponds one part sequence x x belong vector last word normally bag word input every x assigned word word present get weight following tf idf example also numerical data xi represents feature price temperature gdp always case however x assigned word depends order see x x change word different weight weight matrix dense layer assigned feature xi optimised question doe work order word change time xi referring different word understand lstms recurrent network handle dynamic ordering dense layer seemed could work sequential text input fixed one hot vector tf idf example still seen example model sentence sequence integer embedding flatten dense layer architecture plus tried see doe work would really appreciate explanation correction flow thinking thanks
Sentence Transformers Using BOW?,"<p>I have a collection of terms that appear or are somehow related to web pages (e.g. keywords from the HTML tags). These are not sentences, they are just a collection of keywords, words in a title etc. I am interested in, given such a webpage, to find those most similar. In a case where one has sentences / paragraphs I would think of using a sentence transformer or even like Doc2vec. But in this case I only have the set of words of a page and there is no real context or sentences. Am I correct this precludes me from using sentence transformer / Doc2vec ?</p>
",Vectorization & Embeddings,sentence transformer using bow collection term appear somehow related web page e g keywords html tag sentence collection keywords word title etc interested given webpage find similar case one ha sentence paragraph would think using sentence transformer even like doc vec case set word page real context sentence correct precludes using sentence transformer doc vec
Contextual word embeddings from pretrained word2vec vectors,"<p>I would like to create word embeddings that take <strong>context</strong> into account, so the vector of the word <em>Jaguar [animal]</em> would be different from the word <em>Jaguar [car brand]</em>.</p>
<p>As you know, word2vec only gives one representation for a given word, and I would like to take already pretrained embeddings and enrich them with context. So far I've tried a simple way with taking an average vector of the word and category word, for example <a href=""https://i.sstatic.net/NtMaT.png"" rel=""nofollow noreferrer"">like this</a>.</p>
<p>Now I would like to try to create and train a neural network that would take entire sentences, e.g.</p>
<ol>
<li><em>Jaguar F-PACE is a great SUV sports car.</em></li>
<li><em>Among cats, only tigers and lions are bigger than jaguars.</em></li>
</ol>
<p>And then it would undertake the task of text classification (I have a dataset with several categories like animals, cars, etc.), but the result would be new representations for the word jaguar, but in different contexts, so two different embeddings.</p>
<p>Does anyone have any idea how I could create such a network? I don't hide that I'm a beginner and have no idea how to go about it.</p>
",Vectorization & Embeddings,contextual word embeddings pretrained word vec vector would like create word embeddings take context account vector word jaguar animal would different word jaguar car brand know word vec give one representation given word would like take already pretrained embeddings enrich context far tried simple way taking average vector word category word example like would like try create train neural network would take entire sentence e g jaguar f pace great suv sport car among cat tiger lion bigger jaguar would undertake task text classification dataset several category like animal car etc result would new representation word jaguar different context two different embeddings doe anyone idea could create network hide beginner idea go
"I created a TF-IDF code to analyze an annual report, I want to know the importance of specific keywords","<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
import path
import re



with open(r'C:\Users\maxim\PycharmProjects\THESIS\data\santander2020_1.txt', 'r') as file:
    data = file.read()

dataset = [data]


tfIdfVectorizer=TfidfVectorizer(use_idf=True, stop_words=&quot;english&quot;
                                , lowercase=True,max_features=100,ngram_range=(1,3))
tfIdf = tfIdfVectorizer.fit_transform(dataset)
df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[&quot;TF-IDF&quot;])
df = df.sort_values('TF-IDF', ascending=False)




print (df.head(25))
</code></pre>
<p>The above code is what ive created to do a TF-IDF analysis on an annual report, however currently it is giving me the values of the most important words within the report. However, I only need the TFIDF values for the keywords
[&quot;digital&quot;,&quot;hardware&quot;,&quot;innovation&quot;,&quot;software&quot;,&quot;analytics&quot;,&quot;data&quot;,&quot;digitalisation&quot;,&quot;technology&quot;], is there a way I can specify to only look for the tfidf values of these terms?</p>
<p>I'm very new to programming with little experience, I'm doing this for my thesis.</p>
<p>Any help is greatly appreciated.</p>
",Vectorization & Embeddings,created tf idf code analyze annual report want know importance specific keywords code ive created tf idf analysis annual report however currently giving value important word within report however need tfidf value keywords digital hardware innovation software analytics data digitalisation technology way specify look tfidf value term new programming little experience thesis help greatly appreciated
How to Vectorize python function,"<p>I have made a resume parser but to parse my resumes, I am using a for loop to run my parse function over each resume. Is there a way to vectorize this approach?
I tried to use <code>.apply()</code> function by putting my resumes in a Series but it doesn’t help in performance. Also, I have heard of pandas UDF for vectorization can help. Is that true? Below is my code.</p>
<pre><code>start=time.time()
resume_list,resume_name=form_list()
final=[]
for resume in resume_list:
    final.append(extract_everything(resume))
final_dataframe=pd.DataFrame(final,index=resume_name)
print(f&quot;{time.time()-start} seconds&quot;)  
</code></pre>
",Vectorization & Embeddings,vectorize python function made resume parser parse resume using loop run parse function resume way vectorize approach tried use function putting resume series help performance also heard panda udf vectorization help true code
Better way to combine Word embedding to get embedding of a sentence,"<p>I have seen in many kaggle kernels and tutorials, average word embeddings is considered to get embedding of a sentence. But, i am wondering if this is a correct approach.Since it discards the positional information of the words in the sentence. is there a better way to combine embedding? maybe hierarchically combining them in a particular way?</p>
",Vectorization & Embeddings,better way combine word embedding get embedding sentence seen many kaggle kernel tutorial average word embeddings considered get embedding sentence wondering correct approach since discard positional information word sentence better way combine embedding maybe hierarchically combining particular way
R: Correct Way to Calculate Cosine Similarity?,"<p>I am working with the R programming language.</p>
<p>I have the following data:</p>
<pre><code>text = structure(list(id = 1:8, reviews = c(&quot;I guess the employee decided to buy their lunch with my card my card hoping I wouldn't notice but since it took so long to run my car I want to head and check my bank account and sure enough they had bought food on my card that I did not receive leave. Had to demand for and for a refund because they acted like it was my fault and told me the charges are still pending even though they are for 2 different amounts.&quot;, 
&quot;I went to McDonald's and they charge me 50 for Big Mac when I only came with 49. The casher told me that I can't read correctly and told me to get glasses. I am file a report on your casher and now I'm mad.&quot;, 
&quot;I really think that if you can buy breakfast anytime then I should be able to get a cheeseburger anytime especially since I really don't care for breakfast food. I really like McDonald's food but I preferred tree lunch rather than breakfast. Thank you thank you thank you.&quot;, 
&quot;I guess the employee decided to buy their lunch with my card my card hoping I wouldn't notice but since it took so long to run my car I want to head and check my bank account and sure enough they had bought food on my card that I did not receive leave. Had to demand for and for a refund because they acted like it was my fault and told me the charges are still pending even though they are for 2 different amounts.&quot;, 
&quot;Never order McDonald's from Uber or Skip or any delivery service for that matter, most particularly one on Elgin Street and Rideau Street, they never get the order right. Workers at either of these locations don't know how to follow simple instructions. Don't waste your money at these two locations.&quot;, 
&quot;Employees left me out in the snow and wouldn’t answer the drive through. They locked the doors and it was freezing. I asked the employee a simple question and they were so stupid they answered a completely different question. Dumb employees and bad food.&quot;, 
&quot;McDonalds food was always so good but ever since they add new/more crispy chicken sandwiches it has come out bad. At first I thought oh they must haven't had a good day but every time I go there now it's always soggy, and has no flavor. They need to fix this!!!&quot;, 
&quot;I just ordered the new crispy chicken sandwich and I'm very disappointed. Not only did it taste horrible, but it was more bun than chicken. Not at all like the commercial shows. I hate sweet pickles and there were two slices on my sandwich. I wish I could add a photo to show the huge bun and tiny chicken.&quot;
)), class = &quot;data.frame&quot;, row.names = c(NA, -8L))
</code></pre>
<p>I would like to calculate a matrix of cosine similarities between each pair of elements:</p>
<pre><code>library(lsa)
library(proxy)
library(tm)

text = text[,2]

corpus &lt;- VCorpus(VectorSource(text))
tdm &lt;- TermDocumentMatrix(corpus, 
    control = list(wordLengths = c(1, Inf)))
occurrence &lt;- apply(X = tdm, 
    MARGIN = 1, 
    FUN = function(x) sum(x &gt; 0) / ncol(tdm))

tdm_mat &lt;- as.matrix(tdm[names(occurrence)[occurrence &gt;= 0.5], ])

lsaSpace &lt;- lsa(tdm_mat)

# lsaMatrix now is a k x (num doc) matrix, in k-dimensional LSA space
lsaMatrix &lt;- diag(lsaSpace$sk) %*% t(lsaSpace$dk)

# Use the `cosine` function in `lsa` package to get cosine similarities matrix
# (subtract from 1 to get dissimilarity matrix)
distMatrix &lt;- 1 - cosine(lsaMatrix)
</code></pre>
<p>When looking at the resulting matrix:</p>
<pre><code> distMatrix
             1           2         3            4          5          6            7           8
1 0.000000e+00 0.006362649 0.2616818 0.000000e+00 0.06794855 0.25138506 3.107289e-05 0.003658840
2 6.362649e-03 0.000000000 0.1904180 6.362649e-03 0.11468650 0.33082042 5.505664e-03 0.019623883
3 2.616818e-01 0.190417963 0.0000000 2.616818e-01 0.55622109 0.89444938 2.563879e-01 0.322025370
4 0.000000e+00 0.006362649 0.2616818 0.000000e+00 0.06794855 0.25138506 3.107289e-05 0.003658840
5 6.794855e-02 0.114686503 0.5562211 6.794855e-02 0.00000000 0.06202843 7.083380e-02 0.040392530
6 2.513851e-01 0.330820421 0.8944494 2.513851e-01 0.06202843 0.00000000 2.566349e-01 0.197460291
7 3.107289e-05 0.005505664 0.2563879 3.107289e-05 0.07083380 0.25663492 0.000000e+00 0.004363538
8 3.658840e-03 0.019623883 0.3220254 3.658840e-03 0.04039253 0.19746029 4.363538e-03 0.000000000
</code></pre>
<p><strong>My Question:</strong> Have I calculated the cosine similarity correctly? Is there another way to do this?</p>
<p>Thank you!</p>
<p><strong>References:</strong></p>
<ul>
<li><a href=""https://stackoverflow.com/questions/15229584/compute-cosine-similarities-between-documents-in-semantic-space-using-r-lsa-pac"">Compute cosine similarities between documents in semantic space, using R-lsa package</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Cosine_similarity</a></li>
</ul>
",Vectorization & Embeddings,r correct way calculate cosine similarity working r programming language following data would like calculate matrix cosine similarity pair element looking resulting matrix question calculated cosine similarity correctly another way thank reference
&quot;`select()` doesn&#39;t handle lists&quot; when computing textSimilarity between two word embeddings in R,"<p>How many words in word embedding variables do you need to compute semantic similarity in r-package <em>text</em>? I’m trying to run:</p>
<pre><code>library(text)
WEhello&lt;-textEmbed(&quot;hello&quot;)
WEgoodbye&lt;-textEmbed(&quot;goodbye&quot;)
textSimilarity(WEhello, WEgoodbye)
</code></pre>
<p>But I get this error:</p>
<pre><code>Error in `dplyr::select()`:
! `select()` doesn't handle lists.
</code></pre>
",Vectorization & Embeddings,handle list computing textsimilarity two word embeddings r many word word embedding variable need compute semantic similarity r package text trying run get error
Find the position of the most similar word sequence within larger text,"<p>I have some sequence of words as input (e.g. a sentence or part of a sentence). I'm looking for a way to find the position and contents of the most similar substring within a larger text, as well as some sort of similarity score.</p>
<p>The matching substring can contain typos, similarly sounding words, or fewer/extra words, compared to the input string.</p>
<p>Is there an established way to do this? I could only find answers for how to directly compare two strings for similarity, but nothing about finding the position and similarity score of the closest match within a text.</p>
<p>I'm more interested in matches in terms of characters and/or how stuff sounds, rather than meaning.</p>
<p>Ideally looking for a way to achieve this in JavaScript, but language-agnostic suggestions are also welcome.</p>
",Vectorization & Embeddings,find position similar word sequence within larger text sequence word input e g sentence part sentence looking way find position content similar substring within larger text well sort similarity score matching substring contain typo similarly sounding word fewer extra word compared input string established way could find answer directly compare two string similarity nothing finding position similarity score closest match within text interested match term character stuff sound rather meaning ideally looking way achieve javascript language agnostic suggestion also welcome
Custom word-embeddings in gensim,"<p>I have a word embedding matrix (say M) obtained of order V x N where V is the size of the vocabulary and N is the size of each word vector. I want the word2vec model of gensim to initialise its word embedding matrix with M, during training. I am able to load M in the word2vec format using
gensim.models.keyedvectors.Word2VecKeyedVectors.load_word2vec_format(model_file)
but I don't know how to feed M into the gensim word2vec model.</p>
",Vectorization & Embeddings,custom word embeddings gensim word embedding matrix say obtained order v x n v size vocabulary n size word vector want word vec model gensim initialise word embedding matrix training able load word vec format using gensim model keyedvectors word veckeyedvectors load word vec format model file know feed gensim word vec model
Word2vec word embeddings: how to have different embeddings to different words coming in same context?,"<p>Suppose I have two documents:</p>
<p>document 1 : Where can I buy this product1 in paris.
document 2 : Where can I buy this product2 in paris.</p>
<p>Assume product1 and product2 are not in word2vec and I need to train my own word2vec model.
Since the context is same, will word2vec consider product1 and product2 as synonyms?</p>
<p>Will they have similar word embeddings?
If yes, how to make them non related to each other? Should I go for doc2vec model in this case?</p>
",Vectorization & Embeddings,word vec word embeddings different embeddings different word coming context suppose two document document buy product paris document buy product paris assume product product word vec need train word vec model since context word vec consider product product synonym similar word embeddings yes make non related go doc vec model case
Predict numeric variable from a text variable using word embeddings in R,"<p>I have a text variable with reviews of movies and another variables with ratings – I want to try to use the text reviews to predict the ratings.</p>
<p>Here are some example data:</p>
<pre><code>movie_reviews &lt;- c(&quot;I really loved the movie plot&quot;, &quot;This movie really sucked&quot;, &quot;I really found this movie thought provoking&quot;, &quot;ahh what a boring movie&quot;, &quot;A wonderful movie, with a wonderful end&quot;, &quot;Great action movie: Very thrilling&quot;, &quot;Worst movie ever, it never stopped being cheesy&quot;, &quot;Enjoying, feelgood movie for the entire family&quot;, &quot;I will definitely watch this movie again&quot;)

movie_ratings &lt;- c(8, 2, 6, 3, 9, 8.5, 3.5, 9.5, 7.5)  
  
movie_df &lt;- tibble(movie_reviews, movie_ratings) 

</code></pre>
<p>Thank you.</p>
",Vectorization & Embeddings,predict numeric variable text variable using word embeddings r text variable review movie another variable rating want try use text review predict rating example data thank
Follow-up question regarding a Keras model issue,"<p>So about a week ago I posted this question: <a href=""https://stackoverflow.com/questions/67409803/issues-running-a-keras-model-with-custom-layers"">Issues running a Keras model with custom layers</a>.
The suggestion there was to try to make this question smaller and try to debug it myself. I believe I've managed to done something like that, but I still have some problems with it. Because of how long the original post is, I'm making a new question.</p>
<p>Here's the current, much simplified code I've been trying to debug (you can probably ignore the custom callbacks):</p>
<pre class=""lang-py prettyprint-override""><code>embedding_layer = Embedding(len(word_index) + 1,
                                    embedding_dim,
                                    weights=[embedding_matrix],
                                    input_length=self.MAX_SENTENCE_LENGTH,
                                    trainable=True,
                                    mask_zero=True,
                                    name='sent_embed')

sentence_input = Input(shape=(self.MAX_SENTENCE_LENGTH,), dtype='int32', name='sent_input')
embedded_sequences = embedding_layer(sentence_input)
l_lstm = Bidirectional(GRU(EMBEDDING_DIM, return_sequences=True), name='word_lstm')(embedded_sequences)
l_att = AttLayer(name='word_attention')(l_lstm)
dense = Dense(EMBEDDING_DIM, activation='relu', name='dense_relu')(l_att)
preds = Dense(2, activation='softmax', name='dense_final')(dense)
model = Model(inputs=sentence_input, outputs=preds)
optimize = RMSprop(lr=0.001)
model.compile(loss='categorical_crossentropy', optimizer=optimize)

model.fit(encoded_train_x[:, 0], y=train_y[:],
          validation_data=(encoded_val_x[:, 0], val_y[:]),
          batch_size=batch_size, epochs=epochs, verbose=1,
          callbacks=callbacks)
</code></pre>
<p>I've been trying to run this code on Keras 2.2 with TF 1.13, and Keras 2.4 with TF 2.4, with different issues on either version.</p>
<p>The input shape errors I'm getting on the former version occur when my batch_size =/= MAX_SENTENCE_LENGTH, which, firstly, doesn't make any sense to me, and secondly, is a problem since I'm trying to train this model on a data set that has 1359 samples, which I need to split between the train and val sets - this makes it kinda difficult to set a constant batch_size in the model input. I could exclude some of the samples in order to split it into batches of equal size, but that seems like a bad workaround I'd rather avoid.</p>
<p>On the latter combination of Keras and TF versions, I'm getting a different error, this time with the crossentropy:</p>
<pre class=""lang-py prettyprint-override""><code>ValueError: Can not squeeze dim[1], expected a dimension of 1, got 20 for '{{node categorical_crossentropy/weighted_loss/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](Cast)' with input shapes: [?,20].
</code></pre>
<p>Could someone explain why these issues occur, and how to fix them?
I'd like to keep the question theoretical, but if you want to try debugging this I can provide the code for generating the embedding_matrix along with sample inputs.</p>
<p>EDIT:</p>
<pre class=""lang-py prettyprint-override""><code>embedding_dim = 100   # always, because I'm using 100d glove embeddings
MAX_SENTENCE_LENGTH = 20  # I need to be able to increase this
batch_size = 20  # only works if this is equal to MAX_SENTENCE_LENGTH
</code></pre>
",Vectorization & Embeddings,follow question regarding kera model issue week ago posted question trying run code kera tf kera tf different issue either version input shape error getting former version occur batch size max sentence length firstly make sense secondly problem since trying train model data set ha sample need split train val set make kinda difficult set constant batch size model input could exclude sample order split batch equal size seems like bad workaround rather avoid latter combination kera tf version getting different error time crossentropy could someone explain issue occur fix like keep question theoretical want try debugging provide code generating embedding matrix along sample input edit
Imbalanced multiclass classification using company names,"<p>I have this classification scenario below in which Im getting a very low F1, precision, recall and other metrics.</p>
<ol>
<li>Target is multiclass (about ~200 classes) which is highly imbalanced</li>
<li>I only use company names as classifier (mostly 1-2 words which have max of 8 words), no other fields (like description, etc.)</li>
<li>Training data ~ 100k+ records</li>
<li>Preprocessing: numeric and special characters and stopwords removal</li>
<li>I have very low resources for processing (thats why when I try to use oversampling techniques like smote, distance_smote for multiclass, etc., I always get memory error)</li>
<li>Tried using different vectorization/embedding/tokenizer like word2vec, tfidf, fasttext, bert, roberta, etc. but to no avail</li>
<li>Tried using (and fine-tuning) different algorithms (networks, svm, trees, boosting, etc.) but also getting low scores.</li>
<li>I also did cost-sensitive learning (using class weights) but it only decreased my scores.</li>
</ol>
<p>Tried all options that I know but scores are not increasing. Can you recommend other options here or do you think any part of the process that may be wrong/discarded? Thank you!</p>
<p>Distribution of target labels:
<a href=""https://i.sstatic.net/2N9RO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2N9RO.png"" alt=""Distribution of target labels"" /></a></p>
<p>Sample observations
<a href=""https://i.sstatic.net/fx7YC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fx7YC.png"" alt=""Sample observations"" /></a></p>
",Vectorization & Embeddings,imbalanced multiclass classification using company name classification scenario im getting low f precision recall metric target multiclass class highly imbalanced use company name classifier mostly word max word field like description etc training data k record preprocessing numeric special character stopwords removal low resource processing thats try use oversampling technique like smote distance smote multiclass etc always get memory error tried using different vectorization embedding tokenizer like word vec tfidf fasttext bert roberta etc avail tried using fine tuning different algorithm network svm tree boosting etc also getting low score also cost sensitive learning using class weight decreased score tried option know score increasing recommend option think part process may wrong discarded thank distribution target label sample observation
Filtering out irrelevant data from a broad crawl,"<p>I am using scrapy to crawl data from Google. I would like to get the text from the first 20 hits, and then filter out any &quot;outlier&quot; from the scraped text. Basically, I would like to identify how similar the results are by first transforming the text into a vector, comparing this similarity using some metric, and then drop any results that gets a similarity &quot;score&quot; lower than some pre-defined threshold, as this would mean that that specific text is not related to the rest of the results.</p>
<p>I'm thinking of using something like <a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow noreferrer"">cosine similarity</a> but from what I see, this works best when comparing one text block to another. However, what I need is to compare one text block with the rest of the scraped text.</p>
<p>Suppose, for example, that my crawl returned these results:</p>
<pre><code>[1] XYZ is a fintech startup accelerating access to credit in emerging markets. We enable consumers to build their credit history while making easier payments .
[2] Denver-based XYZ Co., a buy now, pay later startup focused on emerging markets, announced today it has raised $2.2 million..
[3] XYZ builds an alternative credit score and app that provides store credit financing to buy essentials like food and medicine.
[4] Want to know more about XYZ? Terms of UsePrivacy Policy. All rights reserved. Copyright © 2022
</code></pre>
<p>I would like to be able to calculate a similarity score that will help me identify result number 4 as an &quot;outlier&quot; or as irrelevant and thus drop it from further processing.</p>
<p>What would be the best way to implement this?</p>
<p>EDIT: Added examples.</p>
",Vectorization & Embeddings,filtering irrelevant data broad crawl using scrapy crawl data google would like get text first hit filter outlier scraped text basically would like identify similar result first transforming text vector comparing similarity using metric drop result get similarity score lower pre defined threshold would mean specific text related rest result thinking using something like cosine similarity see work best comparing one text block another however need compare one text block rest scraped text suppose example crawl returned result would like able calculate similarity score help identify result number outlier irrelevant thus drop processing would best way implement edit added example
Are word2vec embeddings the same if i re-train on the same sentences?,"<p>If I give the same sentences to a word2vec model and train it 2 different times (of course with the same vector size), do I obtain the same embeddings for words?</p>
",Vectorization & Embeddings,word vec embeddings train sentence give sentence word vec model train different time course vector size obtain embeddings word
"Error in w2v_train(trainFile = file_train, modelFile = model, stopWordsFile = file_stopwords (full error text below)","<p>Full error text: <strong>Error in w2v_train(trainFile = file_train, modelFile = model, stopWordsFile = file_stopwords, : Expecting a single string value: [type=closure; extent=1].</strong></p>
<p>I am trying to run a word embedding analysis using this data <a href=""https://www.kaggle.com/datasets/therohk/million-headlines?resource=download"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/therohk/million-headlines?resource=download</a> to obtain:</p>
<ol>
<li>top 25 closest words to focus word</li>
<li>plot these 25 words</li>
<li>compare same analysis with different data (JSTOR data on articles with &quot;populism&quot;  <a href=""https://constellate.org/dataset/f53e497b-844e-2b60-ec2f-b9c54d2e334e?unigrams=political,%20social"" rel=""nofollow noreferrer"">https://constellate.org/dataset/f53e497b-844e-2b60-ec2f-b9c54d2e334e?unigrams=political,%20social</a>)</li>
</ol>
<p>I loaded all the data and necessary packages, as well as pre-processing the ABCNews data for the analysis. (See code)</p>
<pre><code>#Loading necessary packages
install.packages(c(&quot;tidyverse&quot;, &quot;tidytext&quot;, &quot;word2vec&quot;, &quot;Rtsne&quot;, &quot;future&quot;, &quot;jstor&quot;, &quot;magritrr&quot;, &quot;ggplot2&quot;, &quot;dplyr&quot;))
library(&quot;tidyverse&quot;)
library(&quot;tidytext&quot;)
library(&quot;word2vec&quot;)
library(&quot;Rtsne&quot;)
library(&quot;future&quot;)
library(&quot;jstor&quot;)
library(magrittr)
library(&quot;ggplot2&quot;)
library(&quot;dplyr&quot;)

#Preprocessing abcnews data
##Select text data from csv file ABC NEWS FILE
head(abcnews_pop)
abc_pop_text &lt;- abcnews_pop %&gt;%
  select(&quot;headline_text&quot;)
head(abc_pop_text)
</code></pre>
<p>I then used the following code to process the embedding:</p>
<pre><code>#ABCNews data
text_news&lt;-abc_pop_text%&gt;%
  txt_clean_word2vec(.,ascii = TRUE, alpha = TRUE, tolower = TRUE, trim = TRUE)

set.seed(123456789)
news_model&lt;-word2vec(x=text, type = &quot;cbow&quot;, dim = 500, iter = 50)
embedding_news&lt;-as.matrix(news_model)
</code></pre>
<p>The first function (text_news&lt;-abc_pop...) ran smoothly. However, the second one (set.seed(123456789) news_model...) puts out this mistake:</p>
<p><strong>Error in w2v_train(trainFile = file_train, modelFile = model, stopWordsFile = file_stopwords, : Expecting a single string value: [type=closure; extent=1].</strong></p>
<p>Does anyone know how to address this?</p>
",Vectorization & Embeddings,error w v train trainfile file train modelfile model stopwordsfile file stopwords full error text full error text error w v train trainfile file train modelfile model stopwordsfile file stopwords expecting single string value type closure extent trying run word embedding analysis using data obtain top closest word focus word plot word compare analysis different data jstor data article populism loaded data necessary package well pre processing abcnews data analysis see code used following code process embedding first function text news abc pop ran smoothly however second one set seed news model put mistake error w v train trainfile file train modelfile model stopwordsfile file stopwords expecting single string value type closure extent doe anyone know address
highlight similar sentences in two documents and not just display similarity score,"<p>I am working on a problem where I need to find exact or similar sentences in two or more documents. I read a lot about cosine similarity and how it can be used to detect similar text.</p>
<p>Here is the code that I tried:</p>
<pre><code>my_file = open(&quot;test.txt&quot;, &quot;r&quot;)
content = my_file.read()

content_list = content.split(&quot;.&quot;)
my_file.close()
print(&quot;test:&quot;content_list)
my_file = open(&quot;original.txt&quot;, &quot;r&quot;)
og = my_file.read()
print(&quot;og:&quot;og)
</code></pre>
<p><strong>Output</strong></p>
<blockquote>
<p>test:['As machines become increasingly capable', ' tasks considered to require &quot;intelligence&quot; are often removed from the definition of AI,']</p>
</blockquote>
<blockquote>
<p>og:AI applications include advanced web search engines (e.g., Google), recommendation
systems (used by YouTube, Amazon and Netflix), understanding human
speech (such as Siri and Alexa), self-driving cars (e.g., Tesla),
automated decision-making and competing at the highest level in
strategic game systems (such as chess and Go).[2][citation needed] As
machines become increasingly capable, tasks considered to require
&quot;intelligence&quot; are often removed from the definition of AI, a
phenomenon known as the AI effect.[3] For instance, optical character
recognition is frequently excluded from things considered to be AI,[4]
having become a routine technology.</p>
</blockquote>
<p>but when I am using Cosine similarity, using the code:</p>
<pre><code>import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity



def compute_cosine_similarity(text1, text2):
    
    # stores text in a list
    list_text = [text1, text2]
    
    # converts text into vectors with the TF-IDF 
    vectorizer = TfidfVectorizer(stop_words='english')
    vectorizer.fit_transform(list_text)
    tfidf_text1, tfidf_text2 = 
    vectorizer.transform([list_text[0]]), 
    vectorizer.transform([list_text[1]])
    
    # computes the cosine similarity
    cs_score = cosine_similarity(tfidf_text1, tfidf_text2)
    
    return np.round(cs_score[0][0],2)



for i in content_list:
     cosine_similarity12 = compute_cosine_similarity(i,og)
     print('The cosine similarity of sentence 1 and 2 is 
     {}.'.format(cosine_similarity12))
</code></pre>
<p>the output I am getting is:</p>
<blockquote>
<pre><code>The cosine similarity of sentence and og is 0.14.
The cosine similarity of sentence and og is 0.4.
</code></pre>
</blockquote>
<p>I tried splitting the test sentence by '.' and then tried to compare each sentence with the original document. But the cosine similarity results are not what I expected. I need to know what I am doing wrong and how I can get similar sentences from the original document for plagiarism checking. The condition being I want to point out similar sentences(or exact sentences) from the original document.
I even thought of comparing each line of two documents (test, og), but that would really increase the complexity.
I am worried because cosine similarity isn't giving a good score even when I just used the exact same sentences from a big paragraph. I really need help in this and would like to know what am doing wrong.</p>
",Vectorization & Embeddings,highlight similar sentence two document display similarity score working problem need find exact similar sentence two document read lot cosine similarity used detect similar text code tried output test machine become increasingly capable task considered require intelligence often removed definition ai og ai application include advanced web search engine e g google recommendation system used youtube amazon netflix understanding human speech siri alexa self driving car e g tesla automated decision making competing highest level strategic game system chess go citation needed machine become increasingly capable task considered require intelligence often removed definition ai known ai effect instance optical character recognition frequently excluded thing considered ai become routine technology using cosine similarity using code output getting tried splitting test sentence tried compare sentence original document cosine similarity result expected need know wrong get similar sentence original document plagiarism checking condition want point similar sentence exact sentence original document even thought comparing line two document test og would really increase complexity worried cosine similarity giving good score even used exact sentence big paragraph really need help would like know wrong
How to get the dimensions of a word2vec vector?,"<p>I have run a word2vec model on my data <code>list_of_sentence</code>:</p>
<pre><code>from gensim.models import Word2Vec

w2v_model=Word2Vec(list_of_sentence,min_count=5, workers=4)

print(type(w2v_model))

&lt;class 'gensim.models.word2vec.Word2Vec'&gt;
</code></pre>
<p>I would like to know the dimensionality of <code>w2v_model</code> vectors. How can I check it?</p>
",Vectorization & Embeddings,get dimension word vec vector run word vec model data would like know dimensionality vector check
Embedding 3D data in Pytorch,"<p>I want to implement character-level embedding. </p>

<p>This is usual word embedding.</p>

<p><strong>Word Embedding</strong></p>

<pre><code>Input: [ [‘who’, ‘is’, ‘this’] ] 
-&gt; [ [3, 8, 2] ]     # (batch_size, sentence_len)
-&gt; // Embedding(Input)
 # (batch_size, seq_len, embedding_dim)
</code></pre>

<p>This is what i want to do.</p>

<p><strong>Character Embedding</strong></p>

<pre><code>Input: [ [ [‘w’, ‘h’, ‘o’, 0], [‘i’, ‘s’, 0, 0], [‘t’, ‘h’, ‘i’, ‘s’] ] ]
-&gt; [ [ [2, 3, 9, 0], [ 11, 4, 0, 0], [21, 10, 8, 9] ] ]      # (batch_size, sentence_len, word_len)
-&gt; // Embedding(Input) # (batch_size, sentence_len, word_len, embedding_dim)
-&gt; // sum each character embeddings  # (batch_size, sentence_len, embedding_dim)
The final output shape is same as Word embedding. Because I want to concat them later.
</code></pre>

<p>Although I tried it, I am not sure how to implement 3-D embedding. Do you know how to implement such a data?</p>

<pre><code>def forward(self, x):
    print('x', x.size()) # (N, seq_len, word_len)
    bs = x.size(0)
    seq_len = x.size(1)
    word_len = x.size(2)
    embd_list = []
    for i, elm in enumerate(x):
        tmp = torch.zeros(1, word_len, self.embd_size)
        for chars in elm:
            tmp = torch.add(tmp, 1.0, self.embedding(chars.unsqueeze(0)))
</code></pre>

<p>Above code got an error because output of <code>self.embedding</code> is <code>Variable</code>.</p>

<pre><code>TypeError: torch.add received an invalid combination of arguments - got (torch.FloatTensor, float, Variable), but expected one of:
 * (torch.FloatTensor source, float value)
 * (torch.FloatTensor source, torch.FloatTensor other)
 * (torch.FloatTensor source, torch.SparseFloatTensor other)
 * (torch.FloatTensor source, float value, torch.FloatTensor other)
      didn't match because some of the arguments have invalid types: (torch.FloatTensor, float, Variable)
 * (torch.FloatTensor source, float value, torch.SparseFloatTensor other)
      didn't match because some of the arguments have invalid types: (torch.FloatTensor, float, Variable)
</code></pre>

<h2>Update</h2>

<p>I could do this. But <code>for</code> is not effective for batch. Do you guys know more efficient way?</p>

<pre><code>def forward(self, x):
    print('x', x.size()) # (N, seq_len, word_len)
    bs = x.size(0)
    seq_len = x.size(1)
    word_len = x.size(2)
    embd = Variable(torch.zeros(bs, seq_len, self.embd_size))
    for i, elm in enumerate(x): # every sample
        for j, chars in enumerate(elm): # every sentence. [ [‘w’, ‘h’, ‘o’, 0], [‘i’, ‘s’, 0, 0], [‘t’, ‘h’, ‘i’, ‘s’] ]
            chars_embd = self.embedding(chars.unsqueeze(0)) # (N, word_len, embd_size) [‘w’,‘h’,‘o’,0]
            chars_embd = torch.sum(chars_embd, 1) # (N, embd_size). sum each char's embedding
            embd[i,j] = chars_embd[0] # set char_embd as word-like embedding

    x = embd # (N, seq_len, embd_dim)
</code></pre>

<h2>Update2</h2>

<p>This is my final code. Thank you, Wasi Ahmad!</p>

<pre><code>def forward(self, x):
    # x: (N, seq_len, word_len)
    input_shape = x.size()
    bs = x.size(0)
    seq_len = x.size(1)
    word_len = x.size(2)
    x = x.view(-1, word_len) # (N*seq_len, word_len)
    x = self.embedding(x) # (N*seq_len, word_len, embd_size)
    x = x.view(*input_shape, -1) # (N, seq_len, word_len, embd_size)
    x = x.sum(2) # (N, seq_len, embd_size)

    return x
</code></pre>
",Vectorization & Embeddings,embedding data pytorch want implement character level embedding usual word embedding word embedding want character embedding although tried sure implement embedding know implement data code got error output update could effective batch guy know efficient way update final code thank wasi ahmad
How to get negative word samples in Gensim Word2Vec Model?,"<p>I am using gensim Word2Vec model to train word embeddings. My code is:</p>
<pre><code>w2v_model = Word2Vec(min_count=20,
                     window=2,
                     vector_size=50,
                     sample=6e-5,
                     alpha=0.03,
                     min_alpha=0.0007,
                     negative=20,
                     workers=cores-1)

w2v_model.build_vocab(sentences, progress_per=10000)

w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=50, report_delay=1)
</code></pre>
<p>I wonder whether I can access the negative and positive word samples during the process?</p>
<p>Thanks in advance.</p>
",Vectorization & Embeddings,get negative word sample gensim word vec model using gensim word vec model train word embeddings code wonder whether access negative positive word sample process thanks advance
How to convert the text into vector using word2vec embedding?,"<p>Suppose I have a dataframe shown below:</p>

<p>|Text</p>

<p>|Storm in RI worse than last hurricane</p>

<p>|Green Line derailment in Chicago</p>

<p>|MEG issues Hazardous Weather Outlook </p>

<p>I created word2vec model using below code:</p>

<pre><code>def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))

text_data = sent_to_words(df['Text'])
w2v_model = gensim.models.Word2Vec(text_data, size=100, min_count=1, window=5, iter=50)
</code></pre>

<p>now how I will convert the text present in the 'Text' column to vectors using this word2vec model?</p>
",Vectorization & Embeddings,convert text vector using word vec embedding suppose dataframe shown text storm ri worse last hurricane green line derailment chicago meg issue hazardous weather outlook created word vec model using code convert text present text column vector using word vec model
Should I leave periods in text when training fasttext model?,"<p>I have a dataset:</p>
<pre><code>   text
Market regularisation. Researching the marketplace and recommending the most appropriate products
Advising clients on investments, taxes, estate planning. Meeting with clients to establish their needs
...
</code></pre>
<p>I want to get embeddings of texts in each row using fasttext. Before doing that, I do some preprocessing (lemmatisation, lower,...) and than I join sentences in each row together with space. However, Im not sure maybe the model will train better, if I leave periods between sentences in each row?</p>
",Vectorization & Embeddings,leave period text training fasttext model dataset want get embeddings text row using fasttext preprocessing lemmatisation lower join sentence row together space however im sure maybe model train better leave period sentence row
How does bert produce variable output shape?,"<p>Suppose if I provide a list of sentences:</p>
<pre><code>['I like python',
 'I am learning python', # longest sentence of length 4 tokens
 'Python is simple']
</code></pre>
<p>Bert will produce an output of (3 * 4+2 * 768).<br />
Because there were 3 sentences, 4 max tokens, 768 hidden states.</p>
<p>Suppose if I provide another list of sentences:</p>
<pre><code>['I like python',
 'I am learning python',
 'Python is simple',
 'Python is fun to learn' # 5 tokens
]
</code></pre>
<p>The new embedding output would be (4 * 5+2 * 768).</p>
<p>I understand that dim[0] becomes 4 because there is now 4 sentences instead. This is achieved by increasing the rows of the tensor(batch size) during tensor computation.</p>
<p>I also understand that dim[1] becomes 5+2 because the max number of token is number 5 and there is [CLS] and [SEP] tokens at the start and end.<br />
I also understand that there is a <strong>padding mechanism</strong> that accepts up to a <strong>max_position_embeddings=512</strong> for bert model.</p>
<hr>
<p>What I want to ask is:</p>
<ul>
<li>during computation, does bert pad all the values after 5th element with zeros and process with computation using a input of (4 * 512) (4 sentences, 512 max tokens).</li>
<li>then after computation from the output of (4 * 512 * 768), the tensor is trimmed to output: (4 * 5+2 * 768).</li>
<li>if the above assumptions is true, isn't it a huge waste of resources, since majority of the 512 tokens are not attention-required.</li>
<li>I read about the <strong>attention_mask</strong> matrix that tells the model which are the tokens needed for computation, but I don't understand how does <strong>attention_mask</strong> achieve this; when the architecture of the model is initialised with N dimensional inputs, how does attention_mask help during computation to ignore/avoid the computation of the attention-masked elements?</li>
<li>which part of the bert model explicitly restrict the output to (4 * 5+2 * 768)?</li>
</ul>
",Vectorization & Embeddings,doe bert produce variable output shape suppose provide list sentence bert produce output sentence max token hidden state suppose provide another list sentence new embedding output would understand dim becomes sentence instead achieved increasing row tensor batch size tensor computation also understand dim becomes max number token number cl sep token start end also understand padding mechanism accepts max position embeddings bert model want ask computation doe bert pad value th element zero process computation using input sentence max token computation output tensor trimmed output assumption true huge waste resource since majority token attention required read attention mask matrix tell model token needed computation understand doe attention mask achieve architecture model initialised n dimensional input doe attention mask help computation ignore avoid computation attention masked element part bert model explicitly restrict output
Using pre-trained word embeddings - how to create vector for unknown / OOV Token?,"<p>I wan't to add <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">pre-trained embeddings</a> to a model. But as it seems there is no <em>out-of-vocabulary (OOV)</em> token resp. no vector for unseen words existent. </p>

<p>So what can I do to handle <em>OOV-tokens</em> I come across? I have some ideas, but none of them seem to be very good:</p>

<ul>
<li><p>I could just create a random vector for this token, but ideally I'd like the vector to within the <em>logic</em> of the existing model. If I just create it randomly I'm afraid the vector accidentally could be very similar to a very frequent word like <em>'the', 'for', 'that'</em> etc. which is not my intention.</p></li>
<li><p>Or should I just initialize the vector with plain zeros instead?</p></li>
<li><p>Another idea would be averaging the token over other existing vectors. But averaging on what vectors then? On all? This doesn't seem to be very conclusive either.</p></li>
<li><p>I also thought about trying to train this vector. However this doesn't come very handy if I want to freeze the rest of the embedding during training. </p></li>
</ul>

<p><em>(A general solution is appreciated, but I wanted to add that I'm using PyTorch - just in case PyTorch already comes with a handy solution to this problem.)</em></p>

<p><strong>So what would be a good and <em>easy</em> strategy to create such a vector?</strong></p>
",Vectorization & Embeddings,using pre trained word embeddings create vector unknown oov token wan add pre trained embeddings model seems vocabulary oov token resp vector unseen word existent handle oov token come across idea none seem good could create random vector token ideally like vector within logic existing model create randomly afraid vector accidentally could similar frequent word like etc intention initialize vector plain zero instead another idea would averaging token existing vector averaging vector seem conclusive either also thought trying train vector however come handy want freeze rest embedding training general solution appreciated wanted add using pytorch case pytorch already come handy solution problem would good easy strategy create vector
Combine similar elements in N*N matrix without duplicates,"<p>I have a list of sentences, and I want to find all the sentences similar to it and put them together in a list/tuple.</p>
<p>I formed sentence embeddings for them, then computed an N*N cosine similarity matrix for N sentences. I then iterated through the elements, and picked the ones higher than a threshold.</p>
<p>If <code>sentences[x]</code> is similar to <code>sentences[y]</code> and <code>sentences[z]</code>, if I combine <code>sentences[x]</code> and <code>sentences[y]</code>, <code>sentences[x]</code> should not combine with <code>sentences[z]</code> as the loop iterates further</p>
<p>I went with the intuition that since we are comparing cosine similarities, if X is similar to Y, and Y is similar to Z, X will be similar to Z as well, so I should not have to worry about it. My goal is to not have duplicates, but I am stuck.</p>
<p>Is there a better way / what's the best way to do this?</p>
<p>Here is my code:</p>
<pre><code>import pandas as pd
from sentence_transformers import SentenceTransformer
from sentence_transformers.util import cos_sim, pytorch_cos_sim

embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
threshold = 0.75

def form_embeddings(sentences, embedding_model=embedding_model):

    if isinstance(sentences, str):
        sentences = [sentences]

    return embedding_model.encode(sentences, convert_to_tensor=True)

df = pd.read_csv('sample_file.csv')
sentences = df['sentences'].tolist()

#form embeddings
sentence_embeddings = form_embeddings(sentences=sentences)

#form similarity matrix
sim_matrix = pytorch_cos_sim(sentence_embeddings, sentence_embeddings)

#set similarity with itself as zero
sim_matrix.fill_diagonal_(0)

#iterate through and find pairs of similarity
pairs = []
for i in range(len(sentences)):
    for j in range(i, len(sentences)):
        if sim_matrix[i,j] &gt;= threshold:
            pairs.append({'index':[i,j], 'score': sim_matrix[i,j], 'original_sentence':sentences[i], 'similar_sentence':sentences[j]})
</code></pre>
",Vectorization & Embeddings,combine similar element n n matrix without duplicate list sentence want find sentence similar put together list tuple formed sentence embeddings computed n n cosine similarity matrix n sentence iterated element picked one higher threshold similar combine combine loop iterates went intuition since comparing cosine similarity x similar similar z x similar z well worry goal duplicate stuck better way best way code
Where do dimensions in Word2Vec come from?,"<p>I am using word2vec model for training a neural network and building a neural embedding for finding the similar words on the vector space. But my question is about dimensions in the word and context embeddings (matrices), which we initialise them by random numbers(vectors) at the beginning of the training, like this <a href=""https://iksinc.wordpress.com/2015/04/13/words-as-vectors/"" rel=""nofollow"">https://iksinc.wordpress.com/2015/04/13/words-as-vectors/</a></p>

<p>Lets say we want to display {book,paper,notebook,novel} words on a graph, first of all we should build a matrix with this dimensions 4x2 or 4x3 or 4x4 etc, I know the first dimension of the matrix its the size of our vocabulary |v|. But the second dimension of the matrix (number of vector's dimensions), for example this is a vector for word “book""  [0.3,0.01,0.04], what are these numbers? do they have any meaning? for example the 0.3 number related to the relation between word “book"" and “paper” in the vocabulary, the 0.01 is the relation between book and notebook, etc.
Just like TF-IDF, or Co-Occurence matrices that each dimension (column) Y has a meaning - its a word or document related to the word in row X.</p>
",Vectorization & Embeddings,dimension word vec come using word vec model training neural network building neural embedding finding similar word vector space question dimension word context embeddings matrix initialise random number vector beginning training like let say want display book paper notebook novel word graph first build matrix dimension x x x etc know first dimension matrix size vocabulary v second dimension matrix number vector dimension example vector word book number meaning example number related relation word book paper vocabulary relation book notebook etc like tf idf co occurence matrix dimension column ha meaning word document related word row x
How to find similar words with FastText?,"<p>I am playing around with <code>FastText</code>, <a href=""https://pypi.python.org/pypi/fasttext"" rel=""noreferrer"">https://pypi.python.org/pypi/fasttext</a>,which is quite similar to <code>Word2Vec</code>. Since it seems to be a pretty new library with not to many built in functions yet, I was wondering how to extract morphological similar words. </p>

<p>For eg: <code>model.similar_word(""dog"")</code> -> dogs. But there is no function built-in.</p>

<p>If I type 
<code>model[""dog""]</code> </p>

<p>I only get the vector, that might be used to compare cosine similarity.
<code>model.cosine_similarity(model[""dog""], model[""dogs""]])</code>. </p>

<p>Do I have to make some sort of loop and do <code>cosine_similarity</code> on all possible pairs in a text? That would take time ...!!!</p>
",Vectorization & Embeddings,find similar word fasttext playing around quite similar since seems pretty new library many built function yet wa wondering extract morphological similar word eg dog function built type get vector might used compare cosine similarity make sort loop possible pair text would take time
How to map input sentence to output sentence in NLP,"<p><strong>How can I do sentence to sentence mapping?</strong></p>
<p>example: If you have an input text &quot;The price of orange has increased&quot; and output text &quot;Increase the production of orange&quot;</p>
<p>So should I convert into vector then use any algorithm or cosine similarity</p>
",Vectorization & Embeddings,map input sentence output sentence nlp sentence sentence mapping example input text price orange ha increased output text increase production orange convert vector use algorithm cosine similarity
How to find pre known bigrams or any ngrams from a document,"<p>I have a list of known bi-grams and tri-grams which are known to me , I want to find all these n-grams that exist in my document , I have tried gensim library phrases library but it will give the bigrams based on some score crossing a threshold but I want detect those n-grams which I already have. How can I do it.eg words like 'dark' and 'chocolate' should be combined into one word &quot;dark-chocolate&quot; and so on.</p>
<p>I want to find the words that form aroma and taste of a product like &quot;shoe_polish&quot; or &quot;baked_toast&quot; so on such words which combine two words and then i will calculate embeddings of these phrased words pairs using word2vec. I already have list of such aromas words unigrams, bigrams and trigram and i will extract more from sentence but i want atleast does phrases which  already i know to be detected atlest</p>
",Vectorization & Embeddings,find pre known bigram ngrams document list known bi gram tri gram known want find n gram exist document tried gensim library phrase library give bigram based score crossing threshold want detect n gram already eg word like dark chocolate combined one word dark chocolate want find word form aroma taste product like shoe polish baked toast word combine two word calculate embeddings phrased word pair using word vec already list aroma word unigrams bigram trigram extract sentence want atleast doe phrase already know detected atlest
Word2Vec clustering: embed with low dimensionality or with high dimensionality and then reduce?,"<p>I am using K-means for topic modelling using Word2Vec and would like to understand the implications of vectorizing up to, let's say, 10 dimensions, against embedding it with 200 dimensions and then using PCA to get down to 10. Does the second approach make sense at all?</p>
",Vectorization & Embeddings,word vec clustering embed low dimensionality high dimensionality reduce using k mean topic modelling using word vec would like understand implication vectorizing let say dimension embedding dimension using pca get doe second approach make sense
Is it possible to access hugging face transformer embedding layer?,"<p>I want to use a pretrained hugging face transformer language model as an encoder in a sequence to sequence model.</p>
<p>The task is grammatical error correction, so both input and output come from the same language.</p>
<p>Therefore I was wondering if it is possible to access the embedding layer from the hugging face transformer encoder, and use it as the embedding layer for the decoder?</p>
<p>Or maybe there is some other approach that you'd recommend?</p>
",Vectorization & Embeddings,possible access hugging face transformer embedding layer want use pretrained hugging face transformer language model encoder sequence sequence model task grammatical error correction input output come language therefore wa wondering possible access embedding layer hugging face transformer encoder use embedding layer decoder maybe approach recommend
How to convert a list of high-dim tuples to a dataframe?,"<p>I have a table from where I am getting my embedding data, it's contained in the <code>Embedding</code> column in the form of tuples having 3000 numbers. Now, I have to map the index of these embeddings to my dataset. I am using this code for the above:</p>
<pre><code>p_x = [p_embedding[p_embedding['p_id'] == int(pid)]['Embedding'] for pid in p_mapping]
</code></pre>
<p>Edit: Adding sample data
Edit 2: Size is 300</p>
<p><code>p_embedding</code> with tuple of size 300:</p>
<pre><code>p_id   embedding
100    (0.11757241, -0.23792185, 0.30370793...)
101    (-0.1045902, 0.27551234, -0.15883833...)
102    (-0.0038427562, 0.091357835, -0.029324641...)
</code></pre>
<p><code>p_mapping</code> with index mappings:</p>
<pre><code>{'100': 0,
 '101': 1,
 '102': 2}
</code></pre>
<p>This gives me the list that I want containing the embeddings in the correct order, but it's still in the form of a tuple in one column. The first three rows are like this:</p>
<pre><code>[Series([], Name: Embedding, dtype: object),
 2463    (-0.080065295, 0.085681394, 0.044956923, 0.078...
 Name: Embedding, dtype: object,
 2510    (0.19006088, 0.1552349, -0.028743511, -0.25197...
 Name: Embedding, dtype: object,
</code></pre>
<p>I want to split this tuple into separate columns of a dataframe, but when I do <code>pd.DataFrame</code> I just get a DF of 3000+ columns with all NAN values. Is there any reason behind this, do I have to change the index of the list?</p>
",Vectorization & Embeddings,convert list high dim tuples dataframe table getting embedding data contained column form tuples number map index embeddings dataset using code edit adding sample data edit size tuple size index mapping give list want containing embeddings correct order still form tuple one column first three row like want split tuple separate column dataframe get df column nan value reason behind change index list
How to deal with system is computationally singular error when running model in R?,"<p>I am using the new <code>conText</code> package in <code>R</code> to run a context embedding regression model. Below I provide the code that I have written thus far:</p>
<pre><code># load speeches
speeches &lt;- read.csv(&quot;convocation6.csv&quot;)
speeches$crimea &lt;- ifelse(speeches$Date &gt;= &quot;2014-02-18&quot;, 1, 0)
speeches$ruling &lt;- ifelse(speeches$party_id == &quot;UR&quot;, 1, 0)
speeches$party &lt;- ifelse(speeches$party_id == &quot;UR&quot;, &quot;Ruling&quot;, &quot;Opposition&quot;)

# create corpus
# preparing speeches
speeches$text &lt;- as.character(speeches$text)
speeches$docnames &lt;- speeches$name
speeches_corpus &lt;- quanteda::corpus(speeches, docid_field =&quot;X&quot;, docnames = 
                                      docnames, text_field =&quot;text&quot;)

# tokenize corpus removing unnecessary (i.e. semantically uninformative) elements
toks &lt;- tokens(speeches_corpus, remove_punct = T, remove_symbols = T, remove_numbers = T, 
               remove_separators = T)

# clean out stopwords and words with 2 or fewer characters
toks_nostop &lt;- tokens_select(toks, pattern = stopwords(&quot;ru&quot;, source = &quot;snowball&quot;), selection = &quot;remove&quot;,
                             min_nchar = 3
)

# only use features that appear at least 5 times in the corpus
feats &lt;- dfm(toks_nostop, tolower = T, verbose = TRUE) %&gt;% dfm_trim(min_termfreq = 5) %&gt;% 
  featnames()

# leave the pads so that non-adjacent words will not become adjacent
toks &lt;- tokens_select(toks_nostop, feats, padding = TRUE)

# build a tokenized corpus of contexts sorrounding the target term 'economy'
economy_toks &lt;- tokens_context(x = toks, pattern = &quot;экономик*&quot;, window = 6L)

# build document-feature matrix
economy_dfm &lt;- dfm(economy_toks)
economy_dfm[1:3, 1:3]

# construct the feature co-occurrence matrix for our toks object (see above)
toks_fcm &lt;- fcm(toks, context = &quot;window&quot;, window = 6, count = &quot;frequency&quot;, tri = FALSE)

# estimate glove model using text2vec
glove &lt;- GlobalVectors$new(rank = 300, x_max = 10, learning_rate = 0.05)
wv_main &lt;- glove$fit_transform(toks_fcm, n_iter = 10, convergence_tol = 0.001, n_threads = parallel::detectCores())  # set to 'parallel::detectCores()' to use all available cores

wv_context &lt;- glove$components
local_glove &lt;- wv_main + t(wv_context)  # word vectors

local_transform &lt;- compute_transform(x = toks_fcm, pre_trained = local_glove, weighting = &quot;log&quot;)

economy_dem_local &lt;- dem(x = economy_dfm, pre_trained = local_glove, transform = TRUE, 
                         transform_matrix = local_transform, verbose = TRUE)

# take the column average to get a single &quot;corpus-wide&quot; embedding
economy_wv_local &lt;- colMeans(economy_dem_local)

# find nearest neighbors for overall immigraiton embedding
find_nns(economy_wv_local, pre_trained = local_glove, N = 10, candidates = economy_dem_local@features)

# we limit candidates to features in our corpus
feats &lt;- featnames(dfm(economy_toks))

#### NEAREST NEIGHBORS ECONOMY 

# compare nearest neighbors between groups
set.seed(2021L)
economy_party_ncs &lt;- get_ncs(x = economy_toks,
                           N = 10,
                           groups = docvars(economy_toks, 'party'),
                           pre_trained = local_glove,
                           transform = TRUE,
                           transform_matrix = local_transform,
                           bootstrap = TRUE,
                           num_bootstraps = 10,
                           as_list = TRUE)

# nearest neighbors of &quot;economy&quot; for ruling party
economy_party_ncs[[&quot;Ruling&quot;]]

# nearest neighbors of &quot;economy&quot; for opposition party
economy_party_ncs[[&quot;Opposition&quot;]]

#### CONTEXT EMBEDDING REGRESSION ECONOMY 
docvars(toks)
# two factor covariates
set.seed(123)
model1 &lt;- conText(formula = &quot;экономик*&quot; ~ crimea + ruling,
                  data = toks,
                  pre_trained = local_glove,
                  transform = TRUE, transform_matrix = local_transform,
                  bootstrap = TRUE, num_bootstraps = 10,
                  permute = TRUE, num_permutations = 100,
                  window = 6, case_insensitive = TRUE,
                  verbose = FALSE)
</code></pre>
<p>Whenever I attempt to run <code>model1</code> I receive the following error:</p>
<pre><code>Error in solve.default(t(X_mat) %*% X_mat) : 
  system is computationally singular: reciprocal condition number = 0
</code></pre>
<p>This error goes away when I run the model with just the <code>crimea</code> variable. I'm not sure what is going on with the <code>Ruling</code> variable. Does anyone know what I am doing wrong here? This is a simple dummy variable. Any feedback would be appreciated.</p>
",Vectorization & Embeddings,deal system computationally singular error running model r using new package run context embedding regression model provide code written thus far whenever attempt run receive following error error go away run model variable sure going variable doe anyone know wrong simple dummy variable feedback would appreciated
How to get average pairwise cosine similarity per group in Pandas,"<p>I have a sample dataframe as below</p>
<pre><code>df=pd.DataFrame(np.array([['facebook', &quot;women tennis&quot;], ['facebook', &quot;men basketball&quot;], ['facebook', 'club'],['apple', &quot;vice president&quot;], ['apple', 'swimming contest']]),columns=['firm','text'])
</code></pre>
<p><a href=""https://i.sstatic.net/2hk08.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2hk08.png"" alt=""enter image description here"" /></a></p>
<p>Now I'd like to calculate the degree of text similarity within each firm using word embedding. For example, the average cosine similarity for facebook would be the cosine similarity between row 0, 1, and 2. The final dataframe should have a column <code>['mean_cos_between_items']</code> next to each row for each firm. The value will be the same for each company, since it is a within-firm pairwise comparison.</p>
<p>I wrote below code:</p>
<pre><code>import gensim
from gensim import utils
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
from sklearn.metrics.pairwise import cosine_similarity

 # map each word to vector space
    def represent(sentence):
        vectors = []
        for word in sentence:
            try:
                vector = model.wv[word]
                vectors.append(vector)
            except KeyError:
                pass
        return np.array(vectors).mean(axis=0)
    
    # get average if more than 1 word is included in the &quot;text&quot; column
    def document_vector(items):
        # remove out-of-vocabulary words
        doc = [word for word in items if word in model_glove.vocab]
        if doc:
            doc_vector = model_glove[doc]
            mean_vec=np.mean(doc_vector, axis=0)
        else:
            mean_vec = None
        return mean_vec
    
# get average pairwise cosine distance score 
def mean_cos_sim(grp):
   output = []
   for i,j in combinations(grp.index.tolist(),2 ): 
       doc_vec=document_vector(grp.iloc[i]['text'])
       if doc_vec is not None and len(doc_vec) &gt; 0:      
           sim = cosine_similarity(document_vector(grp.iloc[i]['text']).reshape(1,-1),document_vector(grp.iloc[j]['text']).reshape(1,-1))
           output.append([i, j, sim])
       return np.mean(np.array(output), axis=0)

# save the result to a new column    
df['mean_cos_between_items']=df.groupby(['firm']).apply(mean_cos_sim)
</code></pre>
<p>However, I got below error:</p>
<p><a href=""https://i.sstatic.net/Hecrd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Hecrd.png"" alt=""enter image description here"" /></a></p>
<p>Could you kindly help? Thanks!</p>
",Vectorization & Embeddings,get average pairwise cosine similarity per group panda sample dataframe like calculate degree text similarity within firm using word embedding example average cosine similarity facebook would cosine similarity row final dataframe column next row firm value company since within firm pairwise comparison wrote code however got error could kindly help thanks
"Working with multiple input (image,text) data in ResNet50 or any Deep Learning Models","<p>This is an abstract idea, I dont know the correct pipeline for implementing; I have used a RestNet50 architecture for training a model to classify image into 3 categories; one of the ways i was thinking of exploring was using the textual data of the image;</p>

<pre><code>train_gen = image.ImageDataGenerator().flow_from_directory(dataset_path_train, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical', shuffle=True, seed=seed)
test_gen = image.ImageDataGenerator().flow_from_directory(dataset_path_valid, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical', shuffle=True, seed=seed)
</code></pre>

<p>Data prep for model; 
now for each image i also have {text},{label} as key value pair for individual image;
if i have to pass 
1. WordtoVec
2. TFIDF </p>

<p>I have read about embedding layer in Keras; I am not sure how to embed the text-data along with test_gen and train_gen in the model( in any intermediate layer or after Flatten().</p>

<pre><code>base_model = ResNet50(weights='imagenet', include_top=False, 

input_shape=input_shape) 
from keras.models import Model, load_model
x = base_model.output
x = Flatten(name='flatten')(x)
predictions = Dense(3, activation='softmax', name='predictions')(x)
model = Model(inputs=base_model.input, outputs=predictions)

for layer in model.layers[0:141]:
    layer.trainable = True
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit_generator(train_gen,steps_per_epoch=1000 , epochs=2,validation_steps=100, validation_data=test_gen,verbose=1)
</code></pre>
",Vectorization & Embeddings,working multiple input image text data resnet deep learning model abstract idea dont know correct pipeline implementing used restnet architecture training model classify image category one way wa thinking exploring wa using textual data image data prep model image also text label key value pair individual image pas wordtovec tfidf read embedding layer kera sure embed text data along test gen train gen model intermediate layer flatten
Using BERT to generate similar word or synonyms through word embeddings,"<p>As we all know the capability of <code>BERT</code> model for word embedding, it is probably better than the <code>word2vec</code> and any other models.</p>
<p>I want to create a model on <code>BERT</code> word embedding to generate synonyms or similar words. The same like we do in the <code>Gensim</code> <code>Word2Vec</code>. I want to create method of Gensim <code>model.most_similar()</code> into BERT word embedding.</p>
<p>I researched a lot about it, seems that it is possible to do that, but the problem is it is only showing the embeddings in the form of number, there is no way to get the actual word from it. Can anybody help me regarding this?</p>
",Vectorization & Embeddings,using bert generate similar word synonym word embeddings know capability model word embedding probably better model want create model word embedding generate synonym similar word like want create method gensim bert word embedding researched lot seems possible problem showing embeddings form number way get actual word anybody help regarding
Using static embeddings compatible with BERT,"<p>I have a dataset of utterances and corresponding sentiment label. I want to use an embedding of the sentiment label as an additional input to BERT (To simplify things, you can say that I want to initialize the embeddings for some tokens in my BERT model). There are 6-7 unique labels. I planned to use static embeddings like GloVe to map the label to an embedding, but this will not be compatible with BERT, which expects the input embedding to be of size 768. How can I generate static embeddings of my labels?</p>
",Vectorization & Embeddings,using static embeddings compatible bert dataset utterance corresponding sentiment label want use embedding sentiment label additional input bert simplify thing say want initialize embeddings token bert model unique label planned use static embeddings like glove map label embedding compatible bert expects input embedding size generate static embeddings label
Using word2vec vectors to train random forest,"<p>I am working on sentiment analysis and one of my feature is to generate word embeddings using word2vec.</p>
<p>The dimensions i am using are 350 so i am getting an array of 350 values for each word.</p>
<ol>
<li><p>I am planning to take the average value and use 1 value as a vector</p>
</li>
<li><p>Storing the values as plain values for example :</p>
</li>
</ol>
<p>Review : i am a good boy</p>
<p>Vectors  for i 566 6 7 7  for am 66 7 7  7u for a  77777766 for good 6666 566 6 etc</p>
<p>Any help would be greatly appreciated</p>
<p>code</p>
<pre><code>Here is how i solved this

cleanWords=[]
for i in range(0,len(words)):
    cleanWords.append(words[i].strip())

vectorsDict={}
for i in range(0,len(cleanWords)):
    vectorsDict[cleanWords[i]]=model.wv[cleanWords[i]]

vectorized=[]
for i in range(0,len(rTxt)):
    tokens=word_tokenize(rTxt[i])
    for word in tokens:
        for key in vectorsDict:
            if word == key:
                word=vectorsDict[key]
        a = ','.join(str(v) for v in word)
    vectorized.append(a)
</code></pre>
",Vectorization & Embeddings,using word vec vector train random forest working sentiment analysis one feature generate word embeddings using word vec dimension using getting array value word planning take average value use value vector storing value plain value example review good boy vector u good etc help would greatly appreciated code
What does convolution do on embedding axis in NLP?,"<p>I'm try to understanding what convolution neural network does in NLP.</p>

<p>For example, my input sentence matrix has dimension (100,200). Here 100 is the length of my sentence, 200 is the dimension of word embedding. </p>

<p>Then I used convolution layer to extract feature. In Keras, something like <code>Conv1D(filters=128, kernel_size=3, padding='same', activation='tanh', strides=1)</code>.</p>

<p><strong>But why the output dimension is (100,128)?</strong> I can understand the first number, because I use padding same, and stride 1, so the dimension should be the same. <strong>But why the second dimension is 128, shouldn't it be 200*128?</strong> What does the kernel actually look like? I'm assuming it only scan along the sentence, but why the embedding dimension get lost, the kernel just summed it up?</p>

<p>I add a picture to illustrate it better. If it is a 1D kernel, and do convolution over the word sequence, why after convolution the word embedding dimension becomes 1(shown in picture)? That doesn't make sense to me.</p>

<p><a href=""https://i.sstatic.net/KDyWH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KDyWH.png"" alt=""enter image description here""></a></p>
",Vectorization & Embeddings,doe convolution embedding axis nlp try understanding convolution neural network doe nlp example input sentence matrix ha dimension length sentence dimension word embedding used convolution layer extract feature kera something like output dimension understand first number use padding stride dimension second dimension doe kernel actually look like assuming scan along sentence embedding dimension get lost kernel summed add picture illustrate better kernel convolution word sequence convolution word embedding dimension becomes shown picture make sense
Why does post-padding train faster than pre-padding?,"<p>I have been doing some NLP categorisation tasks and noticed that my models train much faster if I use post-padding instead of pre-padding, and was wondering why that is the case.</p>
<p>I am using Google Colab to train these model with the GPU runtime. Here is my preprocessing code:</p>
<pre><code>PADDING = 'post'

# Tokenising the input strings and padding

tokenizer = Tokenizer(char_level=True)
tokenizer.fit_on_texts(X)
X_tokenized = tokenizer.texts_to_sequences(X)
X_padded = pad_sequences(X_tokenized, maxlen=80, truncating='post', padding=PADDING)
X_train = np.array(X_padded)

# Encoding output one

y1 = y1.to_numpy().reshape(-1, 1)   # Reshape to an array of features
encoder_1 = OneHotEncoder()         # Instantiate encoder
y1 = encoder_1.fit_transform(y1)    # Fit encoder to output 
y1 = y1.toarray()                   # Make output a numpy array

# Encoding output two
    
y2 = y2.to_numpy().reshape(-1, 1)
encoder_2 = OneHotEncoder()
y2 = form_encoder.fit_transform(y2)
y2 = y2.toarray()
</code></pre>
<p>Now to create my model:</p>
<pre><code># --- MODEL PARAMETERS ---

vocab_size = len(tokenizer.index_word) + 1
y1_size = len(encoder_1.categories_[0])
y2_size = len(encoder_2.categories_[0])

embedding_size = 175
units = 96

# --- MODEL ARCHITECTURE ---

inputs = Input(shape=(None,))
input_embeddings = Embedding(vocab_size, embedding_size, mask_zero=True)(inputs)

shared_lstm = Bidirectional(LSTM(units, return_sequences=True, 
                                 dropout=0.3))(input_embeddings)

y1_lstm = Bidirectional(LSTM(units, dropout=0.3))(shared_lstm)
y1_dense = Dense(y1_size, activation='softmax', name='y1')(y1_lstm)

y2_lstm = Bidirectional(LSTM(units, dropout=0.3))(shared_lstm)
y2_dense = Dense(y2_size, activation='softmax', name='y2')(y2_lstm)

split_shared_model = Model(inputs=inputs, outputs=[y1_dense, y2_dense])
</code></pre>
<p>Which is then compiled as:</p>
<pre><code>split_shared_model.compile(
    optimizer='adam', 
    loss=CategoricalCrossentropy(), 
    metrics=['accuracy']
    )
</code></pre>
<p>The summary of the model is as follows:</p>
<pre><code>__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_4 (InputLayer)           [(None, None)]       0           []                               
                                                                                                  
 embedding_3 (Embedding)        (None, None, 175)    19075       ['input_4[0][0]']                
                                                                                                  
 bidirectional_8 (Bidirectional  (None, None, 192)   208896      ['embedding_3[0][0]']            
 )                                                                                                
                                                                                                  
 bidirectional_9 (Bidirectional  (None, 192)         221952      ['bidirectional_8[0][0]']        
 )                                                                                                
                                                                                                  
 bidirectional_10 (Bidirectiona  (None, 192)         221952      ['bidirectional_8[0][0]']        
 l)                                                                                               
                                                                                                  
 y1 (Dense)                     (None, 912)          176016      ['bidirectional_9[0][0]']        
                                                                                                  
 y2 (Dense)                     (None, 617)          119081      ['bidirectional_10[0][0]']       
                                                                                                  
==================================================================================================
Total params: 966,972
Trainable params: 966,972
Non-trainable params: 0
__________________________________________________________________________________________________
</code></pre>
<p>After calling the <code>fit()</code> method the model starts training. Below is an intermediary result with the above settings:</p>
<pre><code>Epoch 1/50
 398/2647 [===&gt;..........................] - ETA: 1:28 - loss: 8.7918 - y1_loss: 4.9236 - y2_loss: 3.8682 - y1_accuracy: 0.1495 - y2_accuracy: 0.3204
---------------------------------------------------------------------------
</code></pre>
<p>However, if I change <code>PADDING</code> to <code>'pre'</code> I find that training is much slower!</p>
<pre><code>Epoch 1/50
  90/2647 [&gt;.............................] - ETA: 45:52 - loss: 9.8153 - y1_loss: 5.3961 - y2_loss: 4.4192 - y1_accuracy: 0.1243 - y2_accuracy: 0.2788
</code></pre>
<p>Can anyone explain why this is? I think it might have something to do with the Embedding layer and it's masking but I am not sure.</p>
",Vectorization & Embeddings,doe post padding train faster pre padding nlp categorisation task noticed model train much faster use post padding instead pre padding wa wondering case using google colab train model gpu runtime preprocessing code create model compiled summary model follows calling method model start training intermediary result setting however change find training much slower anyone explain think might something embedding layer masking sure
How to find all longest common substrings that exist in multiple documents?,"<p>I have many text documents that I want to compare to one another and remove all text that is exactly the same between them. This is to remove find boiler plate text that is consistent so it can be removed for NLP.</p>

<p>The best way I figured to do this is to find Longest Common Sub-strings that exist or are mostly present in all the documents. However, doing this has been incredibly slow.</p>

<p>Here is an example of what I am trying to accomplish:</p>

<p>DocA:</p>

<pre><code>Title: To Kill a Mocking Bird
Author: Harper Lee
Published: July 11, 1960
</code></pre>

<p>DocB:</p>

<pre><code>Title: 1984
Author: George Orwell
Published: June 1949
</code></pre>

<p>DocC:</p>

<pre><code>Title: The Great Gatsby
Author: F. Scott Fitzgerald
</code></pre>

<p>The output would show something like:</p>

<pre><code>{
    'Title': 3,
    'Author': 3,
    'Published': 2,
}
</code></pre>

<p>The results would then be used to strip out the commonalities between documents.</p>

<p>Here is some code I have tested in python. It's incredibly with any significant amount of permutations:</p>

<pre><code>file_perms = list(itertools.permutations(files, 2))
results = {}
for p in file_perms:
    doc_a = p[0]
    doc_b = p[1]

    while True:
        seq_match = SequenceMatcher(a=doc_a, b=doc_b)
        match = seq_match.find_longest_match(0, len(doc_a), 0, len(doc_b)) 

        if (match.size &gt;= 5): 
            doc_a_start, doc_a_stop = match.a, match.a + match.size
            doc_b_start, doc_b_stop = match.b, match.b + match.size 
            match_word = doc_a[doc_a_start:doc_a_stop]

            if match_word in results:
                results[match_word] += 1
            else:
                results[match_word] = 1

            doc_a = doc_a[:doc_a_start] + doc_a[doc_a_stop:]
            doc_b = doc_b[:doc_b_start] + doc_b[doc_b_stop:]
        else: 
            break 

df = pd.DataFrame(
    {
        'Value': [x for x in results.keys()],
        'Count': [x for x in results.values()]
    }
)
print(df)
</code></pre>
",Vectorization & Embeddings,find longest common substring exist multiple document many text document want compare one another remove text exactly remove find boiler plate text consistent removed nlp best way figured find longest common sub string exist mostly present document however ha incredibly slow example trying accomplish doca docb docc output would show something like result would used strip commonality document code tested python incredibly significant amount permutation
"Word2Vec: change of parameter, same results","<p>I'm trying to train Word2Vec models and I would like to create an embedding to average the results over different models. The problem is that I am not getting the results I expected.  In fact, even if I change the parameters I end up  I am getting odd results.</p>
<p>The corpus is developed in this way, where 'documents' is a list of tweets.</p>
<pre><code>word_corpus = [[str(token).lower() +'_' + str(token.pos_) for token in nlp(sentence) if token.pos_ in ('NOUN', 'VERB', 'ADJ') and len(str(token))&gt;1] for sentence in documents]
</code></pre>
<p>Here the two models:</p>
<pre><code># initialize model
w2v_model1 = Word2Vec(vector_size=100, # vector size 
                     window=3, # window for sampling
                     sample=0.01, # subsampling rate
                     epochs=10, # iterations
                     negative=10, # negative samples
                     min_count=11, # minimum threshold
                     workers=-1, # parallelize to all cores
                     hs=0 # no hierarchical softmax
)

# build the vocabulary
w2v_model1.build_vocab(corpus)

# train the model
w2v_model1.train(corpus, total_examples=w2v_model1.corpus_count, epochs=w2v_model1.epochs)


#####


w2v_model2 = Word2Vec(vector_size=100, # vector size 
                     window=7, # window for sampling
                     sample=0.01, # subsampling rate
                     epochs=120, # iterations
                     negative=3, # negative samples
                     min_count=100, # minimum threshold
                     workers=-1, # parallelize to all cores
                     hs=0 # no hierarchical softmax
)

# build the vocabulary
w2v_model2.build_vocab(corpus)

# train the model
w2v_model2.train(corpus, total_examples=w2v_model2.corpus_count, epochs=w2v_model2.epochs)
</code></pre>
<p>and to evaluate them i am using the following syntax:</p>
<pre><code>emb_df1 = (pd.DataFrame([w2v_model1.wv.get_vector(str(n)) for n in w2v_model1.wv.key_to_index],index = w2v_model1.wv.key_to_index)).T
emb_df2 = (pd.DataFrame([w2v_model2.wv.get_vector(str(n)) for n in w2v_model2.wv.key_to_index],index = w2v_model2.wv.key_to_index)).T
</code></pre>
<p>And I get this results.
<a href=""https://i.sstatic.net/RCTL9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RCTL9.png"" alt=""Results from the two different Word2Vec"" /></a></p>
<p>As you can see the number of words gets different but what seems odd to me is that, for words that are analyzed in both models, I get exactly the same coordinates and I can't understand why. If i well understood its working mechanism the results are provided after some randomization steps and so it should be basically impossible to get the same results everytime, so I am not able to understand to what could be due this error.</p>
",Vectorization & Embeddings,word vec change parameter result trying train word vec model would like create embedding average result different model problem getting result expected fact even change parameter end getting odd result corpus developed way document list tweet two model evaluate using following syntax get result see number word get different seems odd word analyzed model get exactly coordinate understand well understood working mechanism result provided randomization step basically impossible get result everytime able understand could due error
Load word2vec dictionary into gensim,"<p>I've loaded pretrained word2vec embeddings into a python dictionary of the form</p>

<p><code>{word: vector}</code></p>

<p>As an example, an element of this dictionary is</p>

<p><code>w2v_dict[""house""] = [1.1,2.0, ... , 0.2]</code></p>

<p>I would like to load this model into Gensim (or a similar library) so that I can find euclidean distances between embeddings.</p>

<p>I understand that pretrained embeddings typically come in a .bin file which can be loaded into Gensim. But if I only have a dictionary of this form, how would I load the vectors into a model?</p>
",Vectorization & Embeddings,load word vec dictionary gensim loaded pretrained word vec embeddings python dictionary form example element dictionary would like load model gensim similar library find euclidean distance embeddings understand pretrained embeddings typically come bin file loaded gensim dictionary form would load vector model
load pre-trained word2vec model for doc2vec,"<p>I'm using gensim to extract feature vector from a document.
I've downloaded the pre-trained model from Google named <code>GoogleNews-vectors-negative300.bin</code> and I loaded that model using the following command:</p>

<pre><code>model = models.Doc2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>My purpose is to get a feature vector from a document. For a word, it's very easy to get the corresponding vector:</p>

<pre><code>vector = model[word]
</code></pre>

<p>However, I don't know how to do it for a document. Could you please help?</p>
",Vectorization & Embeddings,load pre trained word vec model doc vec using gensim extract feature vector document downloaded pre trained model google named loaded model using following command purpose get feature vector document word easy get corresponding vector however know document could please help
Using BERT Embeddings in Keras Embedding layer,"<p>I want to use the BERT Word Vector Embeddings in the Embeddings layer of LSTM instead of the usual default embedding layer. Is there any way I can do it?</p>
",Vectorization & Embeddings,using bert embeddings kera embedding layer want use bert word vector embeddings embeddings layer lstm instead usual default embedding layer way
Can I use a different corpus for fasttext build_vocab than train in Gensim Fasttext?,"<p>I am curious to know if there are any implications of using a different source while calling the <code>build_vocab</code> and <code>train</code> of Gensim <code>FastText</code> model. Will this impact the contextual representation of the word embedding?</p>
<p>My intention for doing this is that there is a specific set of words I am interested to get the vector representation for and when calling <code>model.wv.most_similar</code>. I only want words defined in this vocab list to get returned rather than all possible words in the training corpus. I would use the result of this to decide if I want to group those words to be relevant to each other based on similarity threshold.</p>
<p>Following is the code snippet that I am using, appreciate your thoughts if there are any concerns or implication with this approach.</p>
<ul>
<li>vocab.txt contains a list of unique words of interest</li>
<li>corpus.txt contains full conversation text (i.e. chat messages) where each line represents a paragraph/sentence per chat</li>
</ul>
<p>A follow up question to this is what values should I set for <code>total_examples</code> &amp; <code>total_words</code> during training in this case?</p>
<pre><code>from gensim.models.fasttext import FastText

model = FastText(min_count=1, vector_size=300,)

corpus_path = f'data/{client}-corpus.txt'
vocab_path = f'data/{client}-vocab.txt'
# Unsure if below counts should be based on the training corpus or vocab
corpus_count = get_lines_count(corpus_path)
total_words = get_words_count(corpus_path)

# build the vocabulary
model.build_vocab(corpus_file=vocab_path)

# train the model
model.train(corpus_file=corpus.corpus_path, epochs=100, 
    total_examples=corpus_count, total_words=total_words,
)

# save the model
model.save(f'models/gensim-fastext-model-{client}')
</code></pre>
",Vectorization & Embeddings,use different corpus fasttext build vocab train gensim fasttext curious know implication using different source calling gensim model impact contextual representation word embedding intention specific set word interested get vector representation calling want word defined vocab list get returned rather possible word training corpus would use result decide want group word relevant based similarity threshold following code snippet using appreciate thought concern implication approach vocab txt contains list unique word interest corpus txt contains full conversation text e chat message line represents paragraph sentence per chat follow question value set training case
"why does &#39;Doc2Vec&#39; object has no attribute &#39;n_similarity&#39; ,most_similar, relative cosine_similarity from?? this is while calculating the scores","<pre><code>#training a gensim model &amp; finding the cosine similarity

model = Doc2Vec(dm = 1, min_count=1, window=10, sample=1e-4, negative=10,epochs=20)
model.build_vocab(questions_labeled)
model.train(questions_labeled, total_examples=model.corpus_count, epochs=model.epochs)

model.most_similar('good')

doc2vec_scores = []
for i in range(len(questions1_split)):
    score = model.n_similarity(questions1_split[i],questions2_split[i])
    doc2vec_scores.append(score)
</code></pre>
",Vectorization & Embeddings,doe doc vec object ha attribute n similarity similar relative cosine similarity calculating score 
BERT details + how do you use BERT&#39;s sequence output with a Keras biLSTM model?,"<p>I am experimenting with a biLSTM model and 2 different embedding techniques (FastText, BERT) applied at 2 different levels (word, sentence) all for a binary text classification task. I'm new to the BERT ecosystem and the nuances of complex deep learning in general and wanted some advice.</p>
<p>My biLSTM model is in Keras:</p>
<ol>
<li>How exactly do I use BERT's sequence output here? (Similar to passing weights via an embedding matrix in an embedding layer.)</li>
<li>Is it recommend to use a biLSTM model on top of the sequence output(or BERT) in the first place? Examples that I have seen show an average pooling and couple of dense layers. Is this what is meant by &quot;fine tuning&quot;?</li>
<li>If the biLSTM model is necessary, would it then make sense to work <em>only</em> with BERT's 768 dimension embeddings or is the regular sequence output possible?</li>
<li>When should I set trainable as False when using BERT?</li>
</ol>
<p>Will appreciate your guidance. Happy to share more details if needed. Thank you.</p>
",Vectorization & Embeddings,bert detail use bert sequence output kera bilstm model experimenting bilstm model different embedding technique fasttext bert applied different level word sentence binary text classification task new bert ecosystem nuance complex deep learning general wanted advice bilstm model kera exactly use bert sequence output similar passing weight via embedding matrix embedding layer recommend use bilstm model top sequence output bert first place example seen show average pooling couple dense layer meant fine tuning bilstm model necessary would make sense work bert dimension embeddings regular sequence output possible set trainable false using bert appreciate guidance happy share detail needed thank
Speed up embedding of 2M sentences with RoBERTa,"<p>I have roughly 2 million sentences that I want to turn into vectors using Facebook AI's RoBERTa-large,fine-tuned on NLI and STSB for sentence similarity (using the awesome <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""noreferrer"">sentence-transformers</a> package).</p>

<p>I already have a dataframe with two columns: ""utterance"" containing each sentence from the corpus, and ""report"" containing, for each sentence, the title of the document from which it is from.</p>

<p>From there, my code is the following:</p>

<pre class=""lang-py prettyprint-override""><code>from sentence_transformers import SentenceTransformer
from tqdm import tqdm

model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')

print(""Embedding sentences"")

data = pd.read_csv(""data/sentences.csv"")

sentences = data['utterance'].tolist()

sentence_embeddings = []

for sent in tqdm(sentences):
    embedding = model.encode([sent])
    sentence_embeddings.append(embedding[0])

data['vector'] = sentence_embeddings
</code></pre>

<p>Right now, tqdm estimates that the whole process will take around 160 hours on my computer, which is more than I can spare.</p>

<p>Is there any way I could speed this up by changing my code? Is creating a huge list in memory then appending it to the dataframe the best way to proceed here? (I suspect not).</p>

<p>Many thanks in advance!</p>
",Vectorization & Embeddings,speed embedding sentence roberta roughly million sentence want turn vector using facebook ai roberta large fine tuned nli stsb sentence similarity using awesome sentence transformer package already dataframe two column utterance containing sentence corpus report containing sentence title document code following right tqdm estimate whole process take around hour computer spare way could speed changing code creating huge list memory appending dataframe best way proceed suspect many thanks advance
Domain-specific word similarity,"<p>Does anyone know how of an accurate tool or method that can be used to compute word embeddings or find similarity among domain-specific words? I'm working on an NLP project that involves computing cosine similarity between technical terms, such as &quot;address&quot; and &quot;socket&quot;, but pre-trained models like word2vec aren't giving useful embeddings or accurate cosine similarities because they aren't specific to technical terms. Since the more general-nontechnical meanings of &quot;address&quot; and &quot;socket&quot; aren't similar to one another, these pretrained models aren't giving them sufficiently high similarity scores for the purposes of my project. Would appreciate any advice people would be able to offer. Thank you!</p>
",Vectorization & Embeddings,domain specific word similarity doe anyone know accurate tool method used compute word embeddings find similarity among domain specific word working nlp project involves computing cosine similarity technical term address socket pre trained model like word vec giving useful embeddings accurate cosine similarity specific technical term since general nontechnical meaning address socket similar one another pretrained model giving sufficiently high similarity score purpose project would appreciate advice people would able offer thank
Identify missing words between two sentences using similarity algorithms python,"<p>I am new when it comes to NLP.
Overall what I am trying to do is: given two sentences A and B, I want to figure out which words from B are completely semantically different from words in A. Essentially I need to calculate the similarity between two sentences and figure out which words (from B) have low similarity and print them.
I computed the cosine similarity and it doesn't give much info about the matrix similarity.</p>
<p>lets say A=&quot;Lung cancer is a malignant lung tumour &quot;
and B = &quot;Lung cancer is a lung disease&quot;,</p>
<p>since disease and tumour are semantically similar, the word(s) with small similarity score in A would be 'malignant' as it doesn't match with any word in B</p>
<p>How can I do that?
Maybe I am looking at this completely wrong. But I need to find the words in A that are not in B and take into consideration semantically similar words.</p>
",Vectorization & Embeddings,identify missing word two sentence using similarity algorithm python new come nlp overall trying given two sentence b want figure word b completely semantically different word essentially need calculate similarity two sentence figure word b low similarity print computed cosine similarity give much info matrix similarity let say lung cancer malignant lung tumour b lung cancer lung disease since disease tumour semantically similar word small similarity score would malignant match word b maybe looking completely wrong need find word b take consideration semantically similar word
fine tune universal-sentence-encoder embeddings,"<p>I am new to NLP and Neural Networks. I want to do topic analysis for a dataset of reviews of our product. I tried to use the <code>universal-sentence-encoder</code> along with <code>top2vec</code> and they do a good job. However, the similarity score is low most of the time because the combination of words is unique. I want to retrain the model to capture the similarity in my dataset (which is about 40k reviews).</p>
<p>Is there a way to do so? I think this is called <em>unsupervised fine-tuning</em></p>
<p>I am aware of Keras' API to import the transformer as a layer, but I do not how to continue from there.</p>
<p><strong>Question 1:</strong> What layers or loses could I add to capture similarity between my reviews and outputs vector embedding of the text?</p>
<p>I also read about siamese networks. If I understood correctly, I can add the universal encoder twice as the common network, then add a layer for similarity like <a href=""https://keras.io/api/layers/merging_layers/dot/"" rel=""nofollow noreferrer""><code>keras.layers.Dot</code></a>.</p>
<p><strong>Question 2:</strong> If I trained the model on every possible combination of me reviews, can I then use the output of the first embedding layers (with the new weights) as the embedding of text?</p>
<p><strong>Question 3:</strong> If the answer for question is yes, both embeddings in the siamese networks should be almost identical for the same text. Right?</p>
",Vectorization & Embeddings,fine tune universal sentence encoder embeddings new nlp neural network want topic analysis dataset review product tried use along good job however similarity score low time combination word unique want retrain model capture similarity dataset k review way think called unsupervised fine tuning aware kera api import transformer layer continue question layer loses could add capture similarity review output vector embedding text also read siamese network understood correctly add universal encoder twice common network add layer similarity like question trained model every possible combination review use output first embedding layer new weight embedding text question answer question yes embeddings siamese network almost identical text right
Compare two specifically formatted strings in .txt file in Python,"<p>I have a group of targeted data, which is well-formatted and stored in the &quot;target.txt&quot; file.</p>
<p>I want to compare whether or not the output from my code is the same as the result in the &quot;target.txt&quot; file. For example, the content in the &quot;target.txt&quot; file is:</p>
<pre><code>#Target result
Fruit 1:
        Name: apple
        Color: green
       
Fruit 2:
        Name: strawberry
        Color: red
</code></pre>
<p>If the output from my code is:</p>
<pre><code># Current result
Fruit 1:
   Name: apple
        Color: green

Fruit 2:
        Name: strawberry
        Color: red
</code></pre>
<p>Then, I hope I can get the compared result, which is:</p>
<pre><code>Fruit 1:
Compare result: wrong

Fruit 2:
Compare result: correct
</code></pre>
<p>My question is:</p>
<p>how can I compare these strings, they are well-formatted with specific indentations (it is very important to make sure the indentations are the same).</p>
<p>If I use <code>==</code>, I can only compare two strings' contents but not their format.</p>
<p>Does anyone have a hint on how can I compare these kinds of strings?</p>
<p>Thanks!</p>
<p><strong>Update</strong>:</p>
<p>What I want to compare is whether or not each &quot;Fruit&quot; item in both files is the same, not to compare whether or not the two overall files. I would like to compare &quot;Fruit by Fruit&quot;, not line by line (maybe line by line is also OK initially).</p>
",Vectorization & Embeddings,compare two specifically formatted string txt file python group targeted data well formatted stored target txt file want compare whether output code result target txt file example content target txt file output code hope get compared result question compare string well formatted specific indentation important make sure indentation use compare two string content format doe anyone hint compare kind string thanks update want compare whether fruit item file compare whether two overall file would like compare fruit fruit line line maybe line line also ok initially
Combine syntax with semantic search in Elastic,"<p>I'm trying to make a simple searcher with Elastic using <code>dense-vectors</code> with trained embeddings to make better suggestions when I don't have in my database the exact terms but I do have something semantically similar.
For example: Lets assume I have in my database the following products:</p>
<p><code>['red apple', 'yellow banana', 'green apple', 'car']</code></p>
<p>If someone queries <code>'apple'</code> I would like to see the following ordered results (assuming the order is given by the cosineSimilarity between the embedded query and each product):</p>
<p><code>['red apple', 'green apple', 'yellow banana' (assuming it's closer to apple because they are fruits), 'car']</code>.</p>
<p><strong>So far I could make the following elastic queries (previously preprocessing and calculating the query embedding) having the above results</strong>:</p>
<ol>
<li>First option:</li>
</ol>
<pre><code>    {
        &quot;size&quot;: 20,
        &quot;query&quot;: {
            &quot;script_score&quot;: {
                &quot;query&quot; : {
                        &quot;match&quot;: { &quot;product_name&quot;: 'apple' }
                    },
                &quot;script&quot;: {
                    &quot;source&quot;: &quot;cosineSimilarity(params.queryVector, doc['Embedding']) + 1.0&quot;,
                    &quot;params&quot;: {
                        &quot;queryVector&quot;: query_vector
                    }
                }
            }
        }
    }
</code></pre>
<p>Gives me the result: <code>['red apple', 'green apple']</code> (the results not containing apple are not being displayed)</p>
<ol start=""2"">
<li>Second option:</li>
</ol>
<pre><code>    {
        &quot;size&quot;: 20,
        &quot;query&quot;: {
            &quot;script_score&quot;: {
                &quot;query&quot; : {
                        &quot;match_all&quot;: {}
                    },
                &quot;script&quot;: {
                    &quot;source&quot;: &quot;cosineSimilarity(params.queryVector, doc['Embedding']) + 1.0&quot;,
                    &quot;params&quot;: {
                        &quot;queryVector&quot;: query_vector
                    }
                }
            }
        }
    }
</code></pre>
<p>It's giving me the following results: <code>['red apple', 'banana', 'green apple', 'car']</code> (banana appears before the second apple). I think this is the proper way but I can't make it to show first the things that match syntactically first.</p>
<p>I would appreciate any help.</p>
",Vectorization & Embeddings,combine syntax semantic search elastic trying make simple searcher elastic using trained embeddings make better suggestion database exact term something semantically similar example let assume database following product someone query would like see following ordered result assuming order given cosinesimilarity embedded query product far could make following elastic query previously preprocessing calculating query embedding result first option give result result containing apple displayed second option giving following result banana appears second apple think proper way make show first thing match syntactically first would appreciate help
Normalizing Topic Vectors in Top2vec,"<p>I am trying to understand how <a href=""https://github.com/ddangelov/Top2Vec"" rel=""nofollow noreferrer"">Top2Vec</a> works. I have some questions about the code that I could not find an answer for in the <a href=""https://arxiv.org/abs/2008.09470"" rel=""nofollow noreferrer"">paper</a>. A summary of what the algorithm does is that it:</p>
<ul>
<li>embeds words and vectors in the same <em>semantic space</em> and normalizes them. This usually has more than 300 dimensions.</li>
<li>projects them into 5-dimensional space using UMAP and cosine similarity.</li>
<li>creates topics as centroids of clusters using HDBSCAN with Euclidean metric on the projected data.</li>
</ul>
<p>what troubles me is that <strong>they normalize the topic vectors</strong>. However, the output from UMAP is not normalized, and normalizing the topic vectors will probably move them out of their clusters. This is inconsistent with what they described in their paper as the topic vectors are the arithmetic mean of all documents vectors that belong to the same topic.</p>
<p>This leads to two questions:</p>
<p>How are they going to calculate the nearest words to find the keywords of each topic given that they altered the topic vector by normalization?</p>
<p>After creating the topics as clusters, they try to <a href=""https://github.com/ddangelov/Top2Vec/blob/be2fc43849dd29205154646dd9f92db630ed8145/top2vec/Top2Vec.py#L801"" rel=""nofollow noreferrer"">deduplicate</a> the very similar topics. To do so, they use cosine similarity. This makes sense with the normalized topic vectors. In the same time, it is an extension of the inconsistency that normalizing topic vectors introduced. Am I missing something here?</p>
",Vectorization & Embeddings,normalizing topic vector top vec trying understand top vec work question code could find answer paper summary algorithm doe embeds word vector semantic space normalizes usually ha dimension project dimensional space using umap cosine similarity creates topic centroid cluster using hdbscan euclidean metric projected data trouble normalize topic vector however output umap normalized normalizing topic vector probably move cluster inconsistent described paper topic vector arithmetic mean document vector belong topic lead two question going calculate nearest word find keywords topic given altered topic vector normalization creating topic cluster try similar topic use cosine similarity make sense normalized topic vector time extension inconsistency normalizing topic vector introduced missing something
Is it possible to get the embedding table in tf_hub models?,"<p>I'm having problems with finetuning a BERT model. I was previously using <code>get_transformer_encoder()</code> in <code>official.nlp</code> and <code>MLM</code> task in <code>official.nlp</code> to train a BERT. But this seems like tough, so I changed to finetuning a pretrained BERT model. But to also complete the <code>MLM</code> task, it seems like I need to get the embedding table from the <code>tf_hub</code> models. I think it's possible to extract the embedding table from the hub models, but I don't know whether there are some functions I can use to get them more quickly, Thanks.</p>
",Vectorization & Embeddings,possible get embedding table tf hub model problem finetuning bert model wa previously using task train bert seems like tough changed finetuning pretrained bert model also complete task seems like need get embedding table model think possible extract embedding table hub model know whether function use get quickly thanks
What are the state of the art models for long text similarities?,"<p>I've recently started working on news article similarity and I have tried some models but haven't been able to get vey high scores. So my task is like this:</p>
<p>I participated in the SemEVAL 2022 task 8, where they gave us a multilingual data set of news articles. I am only using the English news articles for now so I am not dealing with the multilingual part. The news articles have been annotated by humans in the range of 1-4 where 1 means very similar and 4 means dissimilar news. So I have two news articles and an overall similarity score from 1 to 4.</p>
<p>Here are some approaches I tried:</p>
<p>I tried a Siamese network where I built the embedding matrix by training a Word2Vec model on my data set. My Siamese network always predicted that the news articles were dissimilar by always predicting 4. This was because I was using the whole news article, so instead of doing this I trimmed the texts and only used the first 200 characters. Even though this time it wasn't predicting all 4, it was not giving very good results either.</p>
<p>Another approach that I used was using sentence BERT. First I sentence tokenized the news articles, encoded them using sentence BERT, then took the mean vector and calculated similarity using cosine similarity.</p>
<p>Another approach is tf-idf. I calculated the tf-idf vector for the news articles and calculated the cosine similarity from the resulting vectors.</p>
<p>I tried several more approaches but they weren't giving any better results. So far tf-idf was giving the better results among the models I've tried but this wasn't a good score.</p>
<p>So my question is, what are the state of the art models that I could try for this problem? or are there any steps that you suggest I could use which would give me higher results?</p>
<p>Note: Pearson correlation is being used for measuring the model performances.</p>
",Vectorization & Embeddings,state art model long text similarity recently started working news article similarity tried model able get vey high score task like participated semeval task gave u multilingual data set news article using english news article dealing multilingual part news article annotated human range mean similar mean dissimilar news two news article overall similarity score approach tried tried siamese network built embedding matrix training word vec model data set siamese network always predicted news article dissimilar always predicting wa wa using whole news article instead trimmed text used first character even though time predicting wa giving good result either another approach used wa using sentence bert first sentence tokenized news article encoded using sentence bert took mean vector calculated similarity using cosine similarity another approach tf idf calculated tf idf vector news article calculated cosine similarity resulting vector tried several approach giving better result far tf idf wa giving better result among model tried good score question state art model could try problem step suggest could use would give higher result note pearson correlation used measuring model performance
Inconsistent result output with gensim index_to_key,"<p>Good afternoon,</p>
<p>first of all thanks to all who take the time to read this.
My problem is this, I would like to have a word2vec output the most common words.</p>
<p>I do this with the following command:</p>
<p><code>#how many words to print out ( sort at frequency)</code></p>
<p><code>x = list(model.wv.index_to_key[:2500])</code></p>
<p>Basically it works, but sometimes I get only 1948 or 2290 words printed out. I can't find any connection with the size of the original corpus (tokens, lines etc.) or deviation from the target value (if I increase the output value to e.g. 3500 it outputs 3207 words).</p>
<p>I would like to understand why this is the case, unfortunately I can't find anything on Google and therefore I don't know how to solve the problem. maybe by increasing the value and later deleting all rows after 2501 by using pandas</p>
",Vectorization & Embeddings,inconsistent result output gensim index key good afternoon first thanks take time read problem would like word vec output common word following command basically work sometimes get word printed find connection size original corpus token line etc deviation target value increase output value e g output word would like understand case unfortunately find anything google therefore know solve problem maybe increasing value later deleting row using panda
My doc2vec library cannot load DocvecsArray.is there a solution.python code,"<p>it shows  Can't get attribute 'DocvecsArray' on &lt;module 'gensim.models.doc2vec' from 'C:\Users\aysha\anaconda3\lib\site-packages\gensim\models\doc2vec.py'&gt; in anaconda prompt while compiling my code.What should i do to solve this?</p>
",Vectorization & Embeddings,doc vec library load docvecsarray solution python code show get attribute docvecsarray module gensim model doc vec c user aysha anaconda lib site package gensim model doc vec py anaconda prompt compiling code solve
NLP Keras - Dimension of Embedding and Global Average Pooling Layers,"<p>I am trying to trace the calculation of Tensorflow’s NLP neural network pipeline below.</p>
<pre><code>embedding_dim=16

model = Sequential([
  vectorize_layer,
  Embedding(vocab_size, embedding_dim, name=&quot;embedding&quot;),
  GlobalAveragePooling1D(),
  Dense(16, activation='relu'),
  Dense(1)
])
</code></pre>
<p>I am a little bit confused on the dimensions of the embedding and global average pooling layers.</p>
<p>For example, if I have a sample sentence vector <code>[‘I’, ‘like’, ‘Chinese’, ‘food’]</code>. The embedding layer will expand this vector from 4x1 dimensions to a matrix of 4x16 dimensions right? Then the global average pooling 1D will take an average of each <strong>feature</strong>, and return a vector of 16x1, where each value represents an average feature value of my sentence, am I correct?</p>
",Vectorization & Embeddings,nlp kera dimension embedding global average pooling layer trying trace calculation tensorflow nlp neural network pipeline little bit confused dimension embedding global average pooling layer example sample sentence vector embedding layer expand vector x dimension matrix x dimension right global average pooling take average feature return vector x value represents average feature value sentence correct
How to get Pre-trained XLNET Sentence embeddings?,"<p>I want to get XLNET Pre-trained Sentence Embeddings of any given sentence. Please provide a code snippet to get the embeddings</p>
",Vectorization & Embeddings,get pre trained xlnet sentence embeddings want get xlnet pre trained sentence embeddings given sentence please provide code snippet get embeddings
Parallel Convolutions using Keras,"<p>What i am trying to do is create a text classification model which combines CNNS and word embeddings.The basic idea is that we have an Embedding layer at the start of the network and then 2 parallel convolutional networks for find 2,3 - grams.<br />
Each of these convolution layers takes the output of the embedding layer as input.<br />
After the outputs of the two cnn layers are concatenated,flattend and feeded to a Dense.<br />
My input is tokenized,numerical sentences of length 27(shape = (None,27)) and i have 1244 of these sentences.<br />
I've managed to create a sequential model wit ha single cnn layer but struggle wit hthe above
My code so far :</p>
<pre><code>input_shape = Embedding(voc, 100,weights=[embedding_matrix], input_length=X.shape[1])

tower_1 = Conv1D(filters=100, kernel_size=2, activation='relu')(input_shape)
tower_1 = MaxPooling1D(pool_size=2)(tower_1)

tower_2 =  Conv1D(filters=100, kernel_size=3, activation='relu')(input_shape)
tower_2 = MaxPooling1D(pool_size=2)(tower_2)


merged = keras.layers.concatenate([tower_1, tower_2,], axis=1)
merged = Flatten()(merged)

out = Dense(3, activation='softmax')(merged)
model = Model(input_shape, out)
</code></pre>
<p>This produces this error:</p>
<pre><code>TypeError: Inputs to a layer should be tensors. Got: &lt;keras.layers.embeddings.Embedding object at 0x7fadca016dd0&gt;
</code></pre>
<p>i have also trid replacing</p>
<pre><code>input_shape = Embedding(voc, 100,weights=[embedding_matrix], input_length=X.shape[1])
</code></pre>
<p>with:</p>
<pre><code>input_tensor = Input(shape=(1244,27))
input_shape = Embedding(voc, 100,weights=[embedding_matrix], input_length=X.shape[1])(input_tensor)
</code></pre>
<p>which gives me this error:</p>
<pre><code>ValueError: Input 0 of layer &quot;max_pooling1d_23&quot; is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 1244, 26, 100)
</code></pre>
",Vectorization & Embeddings,parallel convolution using kera trying create text classification model combine cnns word embeddings basic idea embedding layer start network parallel convolutional network find gram convolution layer take output embedding layer input output two cnn layer concatenated flattend feeded dense input tokenized numerical sentence length shape none sentence managed create sequential model wit ha single cnn layer struggle wit hthe code far produce error also trid replacing give error
Is there a faster way to convert sentences to TFHUB embeddings?,"<p>So I am involved in a project that involves feeding a combination of text embeddings and image vectors into a DNN to arrive at the result. Now for the word embedding part, I am using TFHUB's Electra while for the image part I am using a NASNet Mobile network.</p>
<p>However, the issue I am facing is that while running the word embedding part, using the code shown below, the code just keeps running nonstop. It has been over 2 hours now and my training dataset has just 14900 rows of tweets.</p>
<p>Note - The input to the function is just a list of 14900 tweets.</p>
<pre><code>tfhub_handle_encoder=&quot;https://tfhub.dev/google/electra_small/2&quot; 
tfhub_handle_preprocess=&quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3

# Load Models 
bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess) 
bert_model = hub.KerasLayer(tfhub_handle_encoder)

def get_text_embedding(text):

  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='Preprocessor')   
  encoder_inputs = preprocessing_layer(text)   encoder = 
  hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='Embeddings')   outputs = 
  encoder(encoder_inputs)   text_repr = outputs['pooled_output']   text_repr = 
  tf.keras.layers.Dense(128, activation='relu')(text_repr)

  return text_repr

text_repr = get_text_embedding(train_text)
</code></pre>
<p>Is there a faster way to get text representation using these models?</p>
<p>Thanks for the help!</p>
",Vectorization & Embeddings,faster way convert sentence tfhub embeddings involved project involves feeding combination text embeddings image vector dnn arrive result word embedding part using tfhub electra image part using nasnet mobile network however issue facing running word embedding part using code shown code keep running nonstop ha hour training dataset ha row tweet note input function list tweet faster way get text representation using model thanks help
Problem while creating the NLP model using tensorflow?,"<p>I have created a spam classifier model using tensorflow.. ( tokenization , embedding layer and lstm ).</p>
<pre><code>model=Sequential()
model.add(Embedding(voc_size,32,input_length=sent_len))
model.add(LSTM(units=128))
model.add(Dropout(0.5))
model.add(BatchNormalization())
model.add(Dense(units=1,activation='sigmoid'))
</code></pre>
<p>I have trained the model with my data and got an accuracy of about 97-98%.</p>
<p>I processed my data using the below function.</p>
<pre><code>def process(data):
    data=re.sub('[^a-zA-Z]',&quot; &quot;,data)
    data=data.lower()
    data=data.split()
    data=[lemmatizer.lemmatize(word) for word in data if word not in set(stopword)]
    data=&quot; &quot;.join(data)
    return data
</code></pre>
<p>I have predicted my new custom input as :</p>
<pre><code>def predict(data):
    data=[process(sent) for sent in data]
    data=tokenizer.texts_to_sequences(data)
    data=pad_sequences(data,maxlen=sent_len,padding='pre')
    pred=(mymodel.predict(data)&gt;0.5).astype('int32').tolist()
    return pred

data=input()
predict(data)
</code></pre>
<p>But I am receiving the msg as non-spam for every msg I input..no matter what I enter.<a href=""https://i.sstatic.net/sk1b9.png"" rel=""nofollow noreferrer"">Look here</a></p>
<p>I don't know what is happening. I am not able to deploy such a model.</p>
",Vectorization & Embeddings,problem creating nlp model using tensorflow created spam classifier model using tensorflow tokenization embedding layer lstm trained model data got accuracy processed data using function predicted new custom input receiving msg non spam every msg input matter enter look know happening able deploy model
How to get vocabulary size of word2vec?,"<p>I have a pretrained word2vec model in pyspark and I would like to know how big is its vocabulary (and perhaps get a list of words in the vocabulary).
Is this possible? I would guess it has to be stored somewhere since it can predict for new data, but I couldn't find a clear answer in the <a href=""https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.Word2Vec.html"" rel=""nofollow noreferrer"">documentation</a>.</p>
<p>I tried <code>w2v_model.getVectors().count()</code> but the result (<em>970</em>) seem too small for my use case. In case it may be relevant, I'm using short-text data and my dataset has tens of millions of messages each having from 10 to 30/40 words. I am using <code>min_count=50</code>.</p>
",Vectorization & Embeddings,get vocabulary size word vec pretrained word vec model pyspark would like know big vocabulary perhaps get list word vocabulary possible would guess ha stored somewhere since predict new data find clear answer documentation tried result seem small use case case may relevant using short text data dataset ha ten million message word using
Is it possible to apply some transformation ( augmentation) of Text data Dynamically during training in Tensorflow?,"<p>Let us suppose I have this very simple pipeline <strong>which already works</strong>:</p>
<pre><code>X = [['she let the balloon float up into the air with her hopes and dreams'],
        ['the old rusted farm equipment surrounded the house predicting its demise'],
        ['he was so preoccupied with whether or not he could that he failed to stop to consider if he should']] # three samples

Y = [0,1,1] # Three samples

train_X, val_X, train_y, val_y = train_test_split(X,Y, test_size=0.1, stratify = Y, random_state = SEED) # Scikit learn's

vocab_size = 10 
tokenizer = Tokenizer(num_words=vocab_size, filters = ' ') # Keras Tokenizer
tokenizer.fit_on_texts(list(train_X))
train_X = tokenizer.texts_to_sequences(train_X)
val_X = tokenizer.texts_to_sequences(val_X)

## Pad the sentences 
train_X = pad_sequences(train_X, maxlen=maxlen)
val_X = pad_sequences(val_X, maxlen=maxlen)


history = model.fit(
    train_X, train_y, batch_size=8, epochs = 2, validation_split = 0.1) # compiled model
</code></pre>
<p>And I have a function (<strong>I already have some function which does the transformation</strong>):</p>
<pre><code>def augment_text_data(string):
  temp = string.split(' ')
  temp[np.random.randint(0,len(temp)] = 'some_word'
  return ' '.join(temp)
</code></pre>
<p>Now if I want to apply this transformation of data during runtime, what will I have to do?</p>
<p>One option is that I transform my whole data first during training but I want to do something what <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"" rel=""nofollow noreferrer""><code>ImageDataGenerator</code></a> does for images where you can just pass the <code>preprocessing_function</code>.</p>
<p>Thing is that</p>
<ol>
<li>I am using Embedding (Model of Custom trained with Gensim)</li>
<li>On top of that, <a href=""https://stackoverflow.com/questions/70563462/is-there-a-way-to-use-pre-trained-embedding-with-tf-idf-in-tensorflow/"">I am experimenting with <code>tf-idf</code> weighted Embedding</a> (<strong>But ignore that for now</strong>)</li>
<li>We need to Tokenize texts first which makes the process I think, impossible</li>
</ol>
<p>Can someone suggests me what is the right approach to do given my data and so?</p>
",Vectorization & Embeddings,possible apply transformation augmentation text data dynamically training tensorflow let u suppose simple pipeline already work function already function doe transformation want apply transformation data runtime one option transform whole data first training want something doe image pas thing using embedding model custom trained gensim top href experimenting weighted embedding ignore need tokenize text first make process think impossible someone suggests right approach given data
Saving BERT Sentence Embedding,"<p>I'm currently working on an information retrieval task. I'm using SBERT to perform a semantic search. I already follows the documentation <a href=""https://www.sbert.net/examples/applications/semantic-search/README.html"" rel=""nofollow noreferrer"">here</a></p>
<p>The model i use</p>
<pre><code>model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')

</code></pre>
<p>The outline is</p>
<ol>
<li>You have a list of corpus like this:</li>
</ol>
<pre><code>    data = ['A man is eating food.',
          'A man is eating a piece of bread.',
          'The girl is carrying a baby.',
          'A man is riding a horse.',
          'A woman is playing violin.',
          'Two men pushed carts through the woods.',
          'A man is riding a white horse on an enclosed ground.',
          'A monkey is playing drums.',
          'A cheetah is running behind its prey.'
          ]
</code></pre>
<ol start=""2"">
<li>You have a query like this:</li>
</ol>
<pre><code>queries = ['A man is eating pasta.']
</code></pre>
<ol start=""3"">
<li>Perform encoding with both query and corpus</li>
</ol>
<pre><code>query_embedding = model.encode(query)
doc_embedding = model.encode(data)
</code></pre>
<p>the encode function outputs a numpy.ndarray like this
<a href=""https://i.sstatic.net/d5Bjs.png"" rel=""nofollow noreferrer"">outputs of model.encode(data)</a></p>
<ol start=""4"">
<li>And calculates the similarity using cosine similarity like this</li>
</ol>
<pre><code>similarity = util.cos_sim(query_embedding, doc_embedding)
</code></pre>
<ol start=""5"">
<li>And if you print the similarity, you'll get the torch.Tensor containing score of similarity like this</li>
</ol>
<pre><code>tensor([[0.4389, 0.4288, 0.6079, 0.5571, 0.4063, 0.4432, 0.5467, 0.3392, 0.4293]])
</code></pre>
<p>And it works fine and fast. But ofcourse it is only using a small amount of corpus. When using a large amount of corpus it will take time for the encoding to work.</p>
<p>note: The encoding of query takes no time because it is only one sentence, but the encoding of the corpus will take some time</p>
<p>So, the question is can we save the doc_embedding locally, and use it again? especially when using a large corpus</p>
<p>is there any built-in class/function to do it from the transformers?</p>
",Vectorization & Embeddings,saving bert sentence embedding currently working information retrieval task using sbert perform semantic search already follows documentation model use outline list corpus like query like perform encoding query corpus encode function output numpy ndarray like output model encode data calculates similarity using cosine similarity like print similarity get torch tensor containing score similarity like work fine fast ofcourse using small amount corpus using large amount corpus take time encoding work note encoding query take time one sentence encoding corpus take time question save doc embedding locally use especially using large corpus built class function transformer
Building the output layer of NLP model (is the &quot;embedding&quot; layer),"<p>I was looking through some notebooks in Kaggle just to get a deeper understanding of how NLP works. I came across a notebook for the natural language inference task of predicting the relationship between a given premise and hypothesis. It uses the pretrained BERT model for this task</p>
<p>I had a question about the <code>build_model()</code> function:</p>
<pre><code>max_len = 50

def build_model():
   bert_encoder = TFBertModel.from_pretrained(&quot;bert-base-multilingual-cased&quot;)
   input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_word_ids&quot;)
   input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_mask&quot;)
   input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_type_ids&quot;)
   
   embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0] # confused about this line
   output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])
   
   model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)
   model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
   
   return model 
</code></pre>
<p>I am confused about this line: <code>embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]</code></p>
<p>What does this &quot;embedding&quot; represent and why is there a [0] infront of the function call? Why is the bert_encoder used to instantiate this &quot;embedding&quot;?</p>
<p>Thanks in advance!</p>
",Vectorization & Embeddings,building output layer nlp model embedding layer wa looking notebook kaggle get deeper understanding nlp work came across notebook natural language inference task predicting relationship given premise hypothesis us pretrained bert model task question function confused line doe embedding represent infront function call bert encoder used instantiate embedding thanks advance
reformat text documents for easier comparison,"<p>For those who want to spare the reasoning behind the question jump to the <strong>TL;DR</strong></p>
<p>Hi I'm currently reading a lot of financial annual reports of companies. While the first one is the most interesting, the documents that come after it often are the same in a lot of regards. So obviously I'm more interested in the differences between them. The documents come in pdfs which are hard to compare. So I thought it would be nice to get them as pure text and compare them with a compare tool. So thats what I did. I piped the following two pdfs through pdftotext with the below params:</p>
<p><a href=""https://cdn.sea.com/webmain/static/resource/seagroup/press/2019-03-01%20-%20Form%2020-F.pdf"" rel=""nofollow noreferrer"">annual report for 2018</a></p>
<p><a href=""https://cdn.sea.com/webmain/static/resource/seagroup/press/2020-04-14%20-%20Form%2020-F.PDF"" rel=""nofollow noreferrer"">annual report for 2019</a></p>
<pre><code>pdftotext -enc UTF-8 -nopgbrk -eol mac
</code></pre>
<p>I then realized that compare tools seem to have problems with line breaks. So if I have the exact same sentences, but with different line breaks in both documents, it is shown as a difference. Bullet points in pdfs are transformed to different symbols in the text file which leads to differences as well. So I went into nlp and thought I might get some help there.</p>
<p><strong>TL;DR</strong></p>
<p>I just want to reformat the two snippets below in a defined way that I don't get diffs in a difftool anymore. Like lines are only 80 characters long at most and I want to have some normalized/canonical way for printing bullet points and stuff like that.</p>
<p>I'm currently using spacy and here is an example of two text snippets that are essentially the same but lead to a lot of diffs in difftools. So how can I reprint both snippets to a text document so that the line breaks are the same? Is there even a method to find things like two sentences are exactly the same but in one sentence there is one additional word. I would like reformat that as well without shifting the line break by one word.</p>
<pre><code>import spacy

nlp = spacy.load(&quot;en_core_web_sm&quot;)
SE_2018_10k_string = '''x

“paying users” refers to the number of unique accounts through which a payment is made in our online games in a particular period. A unique
account through which payments are made in more than one online game or in more than one market is counted as more than one paying user.
“QPUs” refers to the aggregate number of paying users during the quarterly period;

x'''
doc1 = nlp(SE_2018_10k_string)
print('SE_2018_10k_string')
for token in doc1:
    print(token.text)


SE_2019_10k_string = '''●

“paying users” refers to the number of unique accounts through which a payment is made in our online games in a particular period. A unique account
through which payments are made in more than one online game or in more than one market is counted as more than one paying user. “QPUs” refers to
the aggregate number of paying users during the quarterly period;

●'''

doc2 = nlp(SE_2019_10k_string)
print('SE_2019_10k_string')
for token in doc2:
    print(token.text)

print(doc1.similarity(doc2))
</code></pre>
",Vectorization & Embeddings,reformat text document easier comparison want spare reasoning behind question jump tl dr hi currently reading lot financial annual report company first one interesting document come often lot regard obviously interested difference document come pdfs hard compare thought would nice get pure text compare compare tool thats piped following two pdfs pdftotext params annual report annual report realized compare tool seem problem line break exact sentence different line break document shown difference bullet point pdfs transformed different symbol text file lead difference well went nlp thought might get help tl dr want reformat two snippet defined way get diffs difftool anymore like line character long want normalized canonical way printing bullet point stuff like currently using spacy example two text snippet essentially lead lot diffs difftools reprint snippet text document line break even method find thing like two sentence exactly one sentence one additional word would like reformat well without shifting line break one word
sentence transformer how to predict new example,"<p>I am exploring sentence transformers and came across this <a href=""https://www.sbert.net/docs/training/overview.html"" rel=""noreferrer"">page</a>.
It shows how to train on our custom data. But I am not sure how to predict. If there are two new sentences such as 1) this is the third example, 2) this is the example number three. How could I get a prediction about how similar those sentences are?</p>
<pre><code>from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader

#Define the model. Either from scratch of by loading a pre-trained model
model = SentenceTransformer('distilbert-base-nli-mean-tokens')

#Define your train examples. You need more than just two examples...
train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),
    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]

#Define your train dataset, the dataloader and the train loss
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
train_loss = losses.CosineSimilarityLoss(model)

#Tune the model
model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)
</code></pre>
<p>----------------------------update 1</p>
<p>I updated the code as below</p>
<pre><code>from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader

#Define the model. Either from scratch of by loading a pre-trained model
model = SentenceTransformer('distilbert-base-nli-mean-tokens')

#Define your train examples. You need more than just two examples...
train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),
    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]

#Define your train dataset, the dataloader and the train loss
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
train_loss = losses.CosineSimilarityLoss(model)
</code></pre>
<p>Saved the model...main change as compared to the old code</p>
<pre><code>model_save_path2 = '/content/gdrive/MyDrive/folderName1/folderName2/model_try-'+datetime.now().strftime(&quot;%Y-%m-%d_%H-%M-%S&quot;)

#Tune the model and save it too
model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100,output_path=model_save_path2)
</code></pre>
<p>Not sure about the below steps</p>
<pre><code>#loading the new model
model_new = SentenceTransformer(model_save_path)

#predicting
sentences = [&quot;This is an example sentence&quot;, &quot;Each sentence is converted&quot;]
model_new.encode(sentences)
</code></pre>
<p>question 1)</p>
<p>is this a correct approach to get sentence embedding after training old model and creating a new model? I am confused because during fitting process we fed two sentences along with similarity measure. While for output we are inputting one sentence at a time and getting a sentence embedding for each sentence.</p>
<p>question 2)</p>
<p>If I would like to get similarity scores for two sentences, is the only option is to take sentence embeddings from output of this model and then use cosine similarity?</p>
",Vectorization & Embeddings,sentence transformer predict new example exploring sentence transformer came across page show train custom data sure predict two new sentence third example example number three could get prediction similar sentence update updated code saved model main change compared old code sure step question correct approach get sentence embedding training old model creating new model confused fitting process fed two sentence along similarity measure output inputting one sentence time getting sentence embedding sentence question would like get similarity score two sentence option take sentence embeddings output model use cosine similarity
"How to use word embeddings (i.e., Word2vec, GloVe or BERT) to calculate the most word similarity in a set of N words?","<p>I am trying to calculate the semantic similarity by inputting the word list and output a word, which is the most word similarity in the list.</p>
<p>E.g.</p>
<p>If I pass in a list of words</p>
<pre class=""lang-py prettyprint-override""><code>words = ['portugal', 'spain', 'belgium', 'country', 'netherlands', 'italy']
</code></pre>
<p>It should output me something like this-</p>
<pre class=""lang-py prettyprint-override""><code>['country']
</code></pre>
",Vectorization & Embeddings,use word embeddings e word vec glove bert calculate word similarity set n word trying calculate semantic similarity inputting word list output word word similarity list e g pas list word output something like
How to compare the &quot;Betterness&quot; of Embedding based on changed parameters in Gensim Word2Vec,"<p><strong>TL;DR</strong>: How to compare the <strong>betterness</strong> of two <code>Word2Vec</code> Embedding which have different set of parameters?</p>
<p>I am using <code>Gensim</code> to train my data on a Vocab of around 2 Million sentences and a vocab of 200K words from scratch. Since the loss is given at the end of training, I found some answers which tell you to get the difference between the last time's loss vs the current loss. So I adapted the code and used it for me. I am trying to get the best model based on the Minimum loss. I am using <a href=""https://github.com/optuna/optuna"" rel=""nofollow noreferrer"">Optuna for Parameter Tuning</a> or say <code>GridSearch</code>. For any tweaked parameters, I am saving my model where it gives me the least loss up until now using a <code>Callback</code>.</p>
<p>I plan on using these Embedding with my Classification model. But training the Embedding along with the model will we hell of a task because:</p>
<ol>
<li>Both of them are time consuming</li>
<li><code>Gensim</code> works faster with Multiprocessors ( I have 8) than on GPUs ( It does not have that kind of support) but mu Model works on GPU</li>
</ol>
<p>How can I get something that tells me the <strong>betterness</strong> of Embedding on my data? Below is the code I am using</p>
<pre><code>from gensim.models import Word2Vec, FastText
from gensim.models.callbacks import CallbackAny2Vec


class LossCallback(CallbackAny2Vec):
    &quot;&quot;&quot;
    Callback to print loss after each epoch
    &quot;&quot;&quot;
    def __init__(self):
        self.epoch = 0
        self.min_loss = np.inf
        self.loss_previous_step = np.inf

    def on_epoch_end(self, model):
        total_loss = model.get_latest_training_loss()
        current_loss = (total_loss - self.loss_previous_step) if self.epoch &gt; 0 else total_loss
        
        if current_loss &lt; self.min_loss:
            self.min_loss = current_loss
            print('Minimum Loss. Saving Model')
            model.save(f'word2vec_{round(current_loss,2)}_loss.model')
            
        if current_loss &lt; self.loss_previous_step :
            print(f&quot;Loss Decreased from {self.loss_previous_step} to {current_loss} in Epoch {self.epoch}&quot;)
            
        self.epoch += 1
        self.loss_previous_step = current_loss
              
# Below code is in a Gris Search such as eopchs, window, loss etc etc

def tune(trial):
    
    vector_size = trial.suggest_int('vector_size', 32, 128)
    alpha = trial.suggest_uniform('alpha', 1e-3, 1e-1)
    window = trial.suggest_int('window', 3,9)
    min_count = trial.suggest_int('min_count', 2,10)
    sg = trial.suggest_categorical('hs', [0,1],)
    hs = trial.suggest_categorical('sg', [0,1],)
    negative = trial.suggest_int('negative', 0,20)
    cbow_mean = trial.suggest_categorical('cbow_mean',[0,1],)
    ns_exponent = trial.suggest_float('ns_exponent', 0.4,0.9)
    sample = trial.suggest_uniform('sample',1e-5, 1e-1)
    epochs = trial.suggest_int('epochs', 5, 100)

                                        
    w2v = Word2Vec(sentences = data, workers = 6, epochs = epochs, seed = SEED,
                  vector_size = vector_size, alpha = alpha, window = window, min_count = min_count, sg = sg, hs = hs, negative = negative, cbow_mean = cbow_mean, 
                  sample = sample, ns_exponent = ns_exponent)
    
    w2v.build_vocab(corpus_iterable = data)
    w2v.train(data,epochs = w2v.epochs,total_examples = w2v.corpus_count, total_words = w2v.corpus_total_words, compute_loss = True, callbacks=[LossCallback()])
    
    return w2v.get_latest_training_loss()


direction = 'minimize'
name = 'Word2Vec'
study = optuna.create_study(direction=direction,study_name=name, storage = f&quot;sqlite:///fine_tune_emb/{name}.db&quot;, load_if_exists=True)
study.optimize(tune, n_trials=50)
</code></pre>
",Vectorization & Embeddings,compare betterness embedding based changed parameter gensim word vec tl dr compare betterness two embedding different set parameter using train data vocab around million sentence vocab k word scratch since loss given end training found answer tell get difference last time loss v current loss adapted code used trying get best model based minimum loss using optuna parameter tuning say parameter saving model give least loss using plan using embedding classification model training embedding along model hell task time consuming work faster multiprocessor gpus doe kind support mu model work gpu get something tell betterness embedding data code using
How to load pre trained FastText Word Embeddings using Gensim?,"<p>I downloaded word embedding <a href=""https://fasttext.cc/docs/en/english-vectors.html"" rel=""nofollow noreferrer"">from this link</a>. I want to load it in <code>Gensim</code> to do some work but I am not able to load it. I have found many resources and none of it is working. I am using <code>Gensim</code> version <code>4.1</code>.</p>
<p>I have tried</p>
<pre><code>gensim.models.fasttext.load_facebook_model('/home/admin1/embeddings/crawl-300d-2M.vec')
gensim.models.fasttext.load_facebook_vectors('/home/admin1/embeddings/crawl-300d-2M.vec')
</code></pre>
<p>and it is showing me</p>
<pre><code>NotImplementedError: Supervised fastText models are not supported
</code></pre>
<p>I went to try to load it using using <code>FastText.load('/home/admin1/embeddings/crawl-300d-2M.vec',)</code> but then it showed <code>UnpicklingError: could not find MARK</code>.</p>
<p>Also, using</p>
",Vectorization & Embeddings,load pre trained fasttext word embeddings using gensim downloaded word embedding link want load work able load found many resource none working using version tried showing went try load using using showed also using
Categorical_crossentropy loss function has value of 0.0000e +00 for a BiLSTM sentiment analysis model,"<p>This is the graph of my model</p>
<h3>Model</h3>
<p><a href=""https://i.sstatic.net/lQWpd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lQWpd.png"" alt=""enter image description here"" /></a></p>
<p>Code format:</p>
<pre class=""lang-py prettyprint-override""><code>def model_creation(vocab_size, embedding_dim, embedding_matrix,
                   rnn_units, batch_size,
                   train_embed=False):
    model = Sequential(
        [
            Embedding(vocab_size, embedding_dim,
                      weights=[embedding_matrix], trainable=train_embed, mask_zero=True),
            Bidirectional(LSTM(rnn_units, return_sequences=True, dropout=0.5)),
            Bidirectional(LSTM(rnn_units, dropout=0.25)),
            Dense(1, activation=&quot;softmax&quot;)
        ])


         return model
</code></pre>
<ul>
<li>The embedding layer receive an embedding matrix with value from Word2Vec
This is the code for the embedding matrix:</li>
</ul>
<h3>Embedding Matrix</h3>
<pre class=""lang-py prettyprint-override""><code>def create_embedding_matrix(encoder,dict_w2v):
    embedding_dim = 50
    embedding_matrix = np.zeros((encoder.vocab_size, embedding_dim))

    for word in encoder.tokens:
        embedding_vector =  dict_w2v.get(word)

        if embedding_vector is not None: # dictionary contains word
            test = encoder.encode(word)
            token_id = encoder.encode(word)[0]
            embedding_matrix[token_id] = embedding_vector

    return embedding_matrix
</code></pre>
<h3>Dataset</h3>
<p>I'm using the amazon product dataset <a href=""https://jmcauley.ucsd.edu/data/amazon/"" rel=""nofollow noreferrer"">https://jmcauley.ucsd.edu/data/amazon/</a>
This is what the dataframe look like</p>
<p><a href=""https://i.sstatic.net/80xNp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/80xNp.png"" alt=""enter image description here"" /></a></p>
<p>I'm only interested in overall and reviewText</p>
<ul>
<li>overall is my Label and reviewText is my Feature</li>
<li>overall has a range of [1,5]</li>
</ul>
<p><a href=""https://i.sstatic.net/bNUaN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bNUaN.png"" alt=""enter image description here"" /></a></p>
<h3>Problem</h3>
<p>During training with categorical_crossentropy loss the is at 0.0000e +00, I don't think loss can be minimized well so accuracy is always at 0.1172</p>
<p><a href=""https://i.sstatic.net/Eb5qR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Eb5qR.png"" alt=""enter image description here"" /></a></p>
<p>Did I configure my model wrong or is there any problem? How do I fix my loss function issue ? Please tell me if it's not clear enough I'll provide more information. I'm not sure what the problem is</p>
",Vectorization & Embeddings,categorical crossentropy loss function ha value e bilstm sentiment analysis model graph model model code format embedding layer receive embedding matrix value word vec code embedding matrix embedding matrix dataset using amazon product dataset dataframe look like interested overall reviewtext overall label reviewtext feature overall ha range problem training categorical crossentropy loss e think loss minimized well accuracy always configure model wrong problem fix loss function issue please tell clear enough provide information sure problem
"expected str, bytes or os.PathLike object, not DataFrame","<p>I tried to load embedding file for Parts-of-speech analysis with NLP . But It shows</p>
<pre class=""lang-none prettyprint-override""><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-33-94170a7f0621&gt; in &lt;module&gt;()
      2 
      3 def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')
----&gt; 4 embeddings_index = dict(get_coefs(*o.split(&quot; &quot;)) for o in open(EMBEDDING_FILE))

TypeError: expected str, bytes or os.PathLike object, not DataFrame
</code></pre>
<p>What's should I do?</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from google.colab import drive
    
drive.mount('/content/drive/')
    
EMBEDDING_FILE = pd.read_csv('/content/drive/MyDrive/ML/paragram_300_sl999-2.txt', encoding= 'unicode_escape', sep=&quot; &quot;, header=None)
    
def get_coefs(word,*arr): 
    return word, np.asarray(arr, dtype='float32')

embeddings_index = dict(get_coefs(*o.split(&quot; &quot;)) for o in open(EMBEDDING_FILE))
</code></pre>
",Vectorization & Embeddings,expected str byte pathlike object dataframe tried load embedding file part speech analysis nlp show
Is it possible that python takes very long to assign a name to a huge list (~10 gb)?,"<p>I am working with a very long list of lists (I'm talking 10 gb if you put it in a file), a cleaned corpus. In my script, I assign it a name, and then use it in another function that has to do with word2vec/spacy and semantic similarity (calculate for each word in a list of words what is their semantic similarity, i.e. how similar are the contexts in which these words appear). I have many steps in my script and I ask it to print something after some of the steps, all to an output file. I am using bash to execute the script. It's been 3 hours, and nothing is in my output file, which I assume means that the list has not been assigned the name yet. However, when I run a .py script with only the list in it (also assigned to a name), it takes very short. Also, the model usually loads very quickly, so that shouldn't be the problem, either. So... am I doing something wrong here? This is how I made the list (that process worked, I already have the list!) and the actual list is just a huge list of lists:</p>
<pre><code>from tqdm import tqdm
import re
import nltk
import string
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
punct = string.punctuation + '«»' + '``'

def read_lines_from_big_file(path):
    with open(path, 'r', encoding='latin-1') as fp:
        for line in fp:
            if len(line) &gt; 1:
                parts = word_tokenize(line) 
                yield parts

contexts_big = []    

            
for split_line in tqdm(read_lines_from_big_file('.../corpus.txt')):
    if 'CURRENT' not in split_line:
        clean_2 = [re.sub('\x93|\x94|\x92|l\'|un\'','',x.strip(punct).lower()) for x in split_line if re.sub('\x93|\x94|\x92|l\'|un\'','',x.strip(punct).lower()) not in stopwords.words('italian') #don't include if the word is a stopword
        and re.sub('\x93|\x94\x92|l\'|un\'','',x.strip(punct).lower()) != &quot; &quot; #don't include extra empty spaces
        and re.sub('\x93|\x94\x92|l\'|un\'','',x.strip(punct).lower()) not in punct #double check that all punct is removed
        and len(re.sub('\x93|\x94\x92|l\'|un\'','',x.strip(punct).lower())) &gt; 1
        and not re.match(r'http\S+|\d+|\n|www\S+', re.sub('\x93|\x94\x92|l\'|un\'','',x.strip(punct).lower()))] #to remove any remaining stopwords or just random letters
        contexts_big.append(clean_2)
    else:
        continue

contexts_big = [[...],[...],[...],...]
</code></pre>
<p>Thanks for your help!</p>
",Vectorization & Embeddings,possible python take long assign name huge list gb working long list list talking gb put file cleaned corpus script assign name use another function ha word vec spacy semantic similarity calculate word list word semantic similarity e similar context word appear many step script ask print something step output file using bash execute script hour nothing output file assume mean list ha assigned name yet however run py script list also assigned name take short also model usually load quickly problem either something wrong made list process worked already list actual list huge list list thanks help
Write a fasttext customised transformer,"<p>I have a trained customised fasttext model (<a href=""https://fasttext.cc/"" rel=""nofollow noreferrer"">fasttext</a> is a word embedding algorithm developed by Facebook). I managed to get the expected result in a function but now I want to rewrite it into a customised transformer so I can add it into my sklearn pipeline as it only accepts transformer.</p>
<p>The function takes a word and returns vectors of the word:</p>
<pre><code>def name2vector(name=None):
    vec = [np.array(model.get_word_vector(w)) for w in name.lower().split(' ')]
    name_vec = np.sum(vec, axis=0) # If &quot;name&quot; is multiple words, sum the vectors
    return (name_vec)
</code></pre>
<p>returned value:</p>
<pre><code>array([-0.01087821,  0.01030535, -0.01402427,  0.0310982 ,  0.08786983,
        -0.00404521, -0.03286128, -0.00842709,  0.03934859, -0.02717219,
         0.01151722, -0.03253938, -0.02435859,  0.03330994, -0.03696496], dtype=float32))
</code></pre>
<p>I want the tranformer does the same thing as the function.
I know I can use <code>BaseEstimator</code> and <code>TransformerMixin</code> to rewrite it into a transformer by reading the <a href=""https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65"" rel=""nofollow noreferrer"">tutorial</a> but I still stuck on this. Some suggestions will be great, thanks.</p>
",Vectorization & Embeddings,write fasttext customised transformer trained customised fasttext model fasttext word embedding algorithm developed facebook managed get expected result function want rewrite customised transformer add sklearn pipeline accepts transformer function take word return vector word returned value want tranformer doe thing function know use rewrite transformer reading tutorial still stuck suggestion great thanks
same words in different contexts with Word2Vec,"<p>I want to use Word2Vec to represent words by vectors.</p>
<p>If there are 2 identical words in the Word2Vec's input,
is it possible to get a different representation for them?</p>
<p>Are there different methods to solve this issue?</p>
",Vectorization & Embeddings,word different context word vec want use word vec represent word vector identical word word vec input possible get different representation different method solve issue
"In text classification, how to find the part of sentence that is important for the classification?","<p>I have trained a text classification model that works well. I wanted to get deeper and understand what words/phrases from a sentence were most impactful in the classification outcome. I want to understand what words are most important for each classification outcome</p>
<p>I am using Keras for the classification and below is the code I am using to train the model. It's a simple embedding plus max-pooling text classification model that I am using.</p>
<pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
import tensorflow as tf 
from tensorflow.keras.callbacks import EarlyStopping 

# early stopping
callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, 
patience=5, verbose=2,  mode='auto', restore_best_weights=True)

# select optimizer
opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999,
epsilon=1e-07, amsgrad=False, name=&quot;Adam&quot;)
embedding_dim = 50

# declare model
model = Sequential()
model.add(layers.Embedding(input_dim=vocab_size, 
                           output_dim=embedding_dim, 
                           input_length=maxlen))
model.add(layers.GlobalMaxPool1D())
model.add(layers.Dense(10, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer=opt,
              loss='binary_crossentropy',
              metrics=['accuracy'])
model.summary()

# fit model
history = model.fit(X_tr, y_tr,
                    epochs=20,
                    verbose=True,
                    validation_data=(X_te, y_te),
                    batch_size=10, callbacks=[callbacks])
loss, accuracy = model.evaluate(X_tr, y_tr, verbose=False)
</code></pre>
<p>How do I extract the phrases/words that have the maximum impact on the classification outcome?</p>
",Vectorization & Embeddings,text classification find part sentence important classification trained text classification model work well wanted get deeper understand word phrase sentence impactful classification outcome want understand word important classification outcome using kera classification code using train model simple embedding plus max pooling text classification model using extract phrase word maximum impact classification outcome
FastText: TypeError: loadModel(): incompatible function arguments,"<p>Edit: Working on Windows</p>
<p>I just want do load an already downloaded fasttext embedding model but got an error (see at the bottom) I can't find a solution to. This is the code:</p>
<pre><code>import fasttext
from pathlib import Path

base_path = Path(&quot;..&quot;)
fasttext_model = base_path / &quot;models&quot; / &quot;cc.de.300.bin&quot;

class EmbeddingVectorizer:
    def __init__(self):

        self.embedding_model = fasttext.load_model(fasttext_model)

    def __call__(self, doc):
        &quot;&quot;&quot;
        Convert address to embedding vectors
        :param address: The address to convert
        :return: The embeddings vectors
        &quot;&quot;&quot;
        embeddings = []
        for word in doc:
            embeddings.append(self.embedding_model[word])
        return embeddings

embedding_model = EmbeddingVectorizer()
</code></pre>
<p>This is the error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_2152/702628572.py in &lt;module&gt;
     15         return embeddings
     16 
---&gt; 17 embedding_model = EmbeddingVectorizer()

~\AppData\Local\Temp/ipykernel_2152/702628572.py in __init__(self)
      2     def __init__(self):
      3 
----&gt; 4         self.embedding_model = fasttext.load_model(fasttext_model)
      5 
      6     def __call__(self, doc):

~\Anaconda3\envs\project-relation-skill-extraction-master-thesis\lib\site-packages\fasttext\FastText.py in load_model(path)
    439     &quot;&quot;&quot;Load a model given a filepath and return a model object.&quot;&quot;&quot;
    440     eprint(&quot;Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.&quot;)
--&gt; 441     return _FastText(model_path=path)
    442 
    443 

~\Anaconda3\envs\project-relation-skill-extraction-master-thesis\lib\site-packages\fasttext\FastText.py in __init__(self, model_path, args)
     96         self.f = fasttext.fasttext()
     97         if model_path is not None:
---&gt; 98             self.f.loadModel(model_path)
     99         self._words = None
    100         self._labels = None

TypeError: loadModel(): incompatible function arguments. The following argument types are supported:
    1. (self: fasttext_pybind.fasttext, arg0: str) -&gt; None

Invoked with: &lt;fasttext_pybind.fasttext object at 0x0000016C630257B0&gt;, WindowsPath('../models/cc.de.300.bin')
</code></pre>
<p>The documentation of fasttext doesn't give me a clue what might could be wrong. Any guesses? Thank you!</p>
",Vectorization & Embeddings,fasttext typeerror loadmodel incompatible function argument edit working window want load already downloaded fasttext embedding model got error see bottom find solution code error documentation fasttext give clue might could wrong guess thank
Calculating TFIDF score for information retrieval system,"<p>I need to build a information retrieval system and I was given a list of queries + a list of abstracts. For each query I need to rank the abstracts based on their relevancy to the words in the query by making two tfidf vectors of equal size, one for the query and one for each abstract in the abstract list.</p>
<p>The only way that I come up with is to first store the query words and their corresponding tfidf score in a dictionary and append the dictionaries in a list. For each word of the dictionaries, I iterate through the list of abstract to check if the word is in the abstract, and if so calculate the tfidf score (it is zero if it does not exist). This way I get the vector of the same size as the query.</p>
<p>However, this algorithm is too slow as it needs to run through the list of abstracts for every word of every query - I don't know if there is a way to improve this?</p>
<p><strong>Edit:
So I am able to improve the runtime by calculating and storing term frequency and the inverse score beforehand, however, when I try to calculate the cosine similarity, the ranking I get seems to be wrong(most abstracts are still in numerical order) and I am getting an error saying that the length of query array does not match the abstract array at query number 6. I can't seem to find the place where it went wrong?</strong></p>
<pre><code>def inverse_document_frequency(word, abstract_word_list):
    all_num = len(abstract_word_list)
    word_count = 0
    for abstract in abstract_word_list:
        if word in abstract:
            word_count += 1
    return log(all_num / word_count)


def term_frequency(word, doc):
    all_num = sum([doc[key] for key in doc])
    return float(doc[word]) / all_num



query_vectors = []
query_dict = []
for query in query_array:
    query_words = query.split(&quot; &quot;)
    new_list = []
    vector = []
    vector_dict = {}
    for word in query_words:
        if word not in closed_class_stop_words and word.isalpha():
            new_list.append(word)

    for word in new_list:
        tf = term_frequency(word, Counter(new_list))
        idf = inverse_document_frequency(word, new_list)
        tfidf = tf * idf
        vector.append(tfidf)
        vector_dict[word] = tfidf


    query_vectors.append(vector)
    query_dict.append(vector_dict)

abstract_idf_dict = {}
abstract_tf_dict = []

for word in abstract_extend:
    if word not in abstract_idf_dict:
        abstract_idf_dict[word] = inverse_document_frequency(word, abstract_word_list)

for abstract in abstract_word_list:
    tf_dict = {}
    for word in abstract:
        if word not in tf_dict:
            valid = [x for x in abstract if x not in closed_class_stop_words]
            tf_dict[word] = Counter(valid)[word] / len(valid)

    abstract_tf_dict.append(tf_dict)

print(abstract_tf_dict)
all_vector_list = []
for i in range(len(query_dict)):
    vector_list = []
    for k in range(len(abstract_word_list)):
        vector = np.zeros(len(query_dict[i]))
        for j in range(len(query_dict[i])):
            key_list = list(query_dict[i].keys())
            word = key_list[j]
            if word in abstract_word_list[k]:
                tfidf = abstract_tf_dict[k][word] * abstract_idf_dict[word]
                vector[j] = tfidf
        print(vector)
        vector_list.append(vector)

    all_vector_list.append(vector_list)


for i in range(len(query_vectors)):
    rank = []
    query_vector = query_vectors[i]
    for j in range(len(all_vector_list[i])):
        abstract_vector = all_vector_list[i][j]
        cos = dot(query_vector, abstract_vector) / (np.sqrt(dot(query_vector, abstract_vector)) * np.sqrt(dot(query_vector, abstract_vector)))
        rank.append((cos, j))
    rank.sort()
    for k in range(len(rank)):
        print(i, rank[k][1])
</code></pre>
",Vectorization & Embeddings,calculating tfidf score information retrieval system need build information retrieval system wa given list query list abstract query need rank abstract based relevancy word query making two tfidf vector equal size one query one abstract abstract list way come first store query word corresponding tfidf score dictionary append dictionary list word dictionary iterate list abstract check word abstract calculate tfidf score zero doe exist way get vector size query however algorithm slow need run list abstract every word every query know way improve edit able improve runtime calculating storing term frequency inverse score beforehand however try calculate cosine similarity ranking get seems wrong abstract still numerical order getting error saying length query array doe match abstract array query number seem find place went wrong
Quickly performing cosine similarity with list of embeddings,"<p>I have a list <code>phrases</code> for each of which I want to get the top most match from a set of 25k embedding vectors (<code>emb2_list</code>). I am using cosine similarity for this purpose. Following is the code:</p>
<pre><code>from sentence_transformers import SentenceTransformer, util
import numpy as np
import torch

model = SentenceTransformer('bert-base-nli-stsb-mean-tokens')

emb2_list = np.load(&quot;emb2_list.npy&quot;) #already encoded, len = 25K

phrases = ['phrase 1','phrase 2','phrase 3','phrase 4',]

for phrase in phrases:
    
    emb1 = model.encode(phrase)

    cos_sim = []

    for emb2 in emb2_list:
        cos_sim.append(util.pytorch_cos_sim(emb1, emb2)[0][0].item())


    v, i = torch.Tensor(cos_sim).topk(1)

    print(f'phrase:{phrase} match index:{i}')
</code></pre>
<p>The issue is that each iteration takes ~1 sec (total ~4 sec in this example). It really becomes problematic once the size of <code>phrases</code> increases (as this is part of an online API).</p>
<p>Is there a better way to find cosine similarity in terms of data structure, batching technique or some kind of approximation/Nearest Neighbour algorithm which might speed up this process?</p>
",Vectorization & Embeddings,quickly performing cosine similarity list embeddings list want get top match set k embedding vector using cosine similarity purpose following code issue iteration take sec total sec example really becomes problematic size increase part online api better way find cosine similarity term data structure batching technique kind approximation nearest neighbour algorithm might speed process
How to obtain contextual embedding for a phrase in a sentence using BERT?,"<p>I use <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a> to obtain sentence embedding from BERT. Using this I am able to obtain embedding for sentences or phrases. For example: I can get embedding of a sentence like <strong>&quot;system not working given to service center but no response on replacement&quot;</strong>. I can also get embedding of a phrase like <strong>&quot;no response&quot;</strong>.</p>
<p>However I want to get embedding of <strong>&quot;no response&quot;</strong> in the context of <strong>&quot;system not working given to service center but no response on replacement&quot;</strong>. Any pointers on how to obtain this will be helpful. Thanks in advance.</p>
<p>I am trying to do this because the phrase <strong>&quot;no response&quot;</strong> has different contexts in different sentences. For example the context of &quot;no response&quot; is different in the following two sentences:
<strong>&quot;system not working given to service center but no response on replacement&quot;
&quot;we tried recovery procedure on the patient but there was no response&quot;</strong></p>
",Vectorization & Embeddings,obtain contextual embedding phrase sentence using bert use obtain sentence embedding bert using able obtain embedding sentence phrase example get embedding sentence like system working given service center response replacement also get embedding phrase like response however want get embedding response context system working given service center response replacement pointer obtain helpful thanks advance trying phrase response ha different context different sentence example context response different following two sentence system working given service center response replacement tried recovery procedure patient wa response
How combine word embedded vectors to one vector?,"<p>I know the meaning and methods of word embedding(skip-gram, CBOW) completely. And I know, that Google has a word2vector API that by getting the word can produce the vector. 
but my problem is this: we have a clause that includes the subject, object, verb... that each word is previously embedded by the Google API, now ""How we can combine these vectors together to create a vector that is equal to the clause?"" 
Example: 
Clause: V= ""dog bites man""
after word embedding by the Google, we have V1, V2, V3 that each of them maps to the dog, bites, man. and we know that:
V = V1+ V2 +V3
How can we provide V?
I will appreciate if you explain it by taking an example of real vectors.</p>
",Vectorization & Embeddings,combine word embedded vector one vector know meaning method word embedding skip gram cbow completely know google ha word vector api getting word produce vector problem clause includes subject object verb word previously embedded google api combine vector together create vector equal clause example clause v dog bite man word embedding google v v v map dog bite man know v v v v provide v appreciate explain taking example real vector
BERT: Unable to reproduce sentence-to-embedding operation,"<p>I am trying to convert sentence to embedding, with the following code.</p>
<pre><code>import torch
from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM

model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = &quot;[CLS] This is a sentence. [SEP]&quot;
tokens = tokenizer.tokenize(text)
input_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))])
encoded_layers, pooled_output = model(input_ids, output_all_encoded_layers=False)
</code></pre>
<p>The code worked. However, each time I run this code, it gives a different result. <code>encoded_layers</code> and <code>pooled_output</code> changes every time, for the same input.</p>
<p>Thank you for your help!</p>
",Vectorization & Embeddings,bert unable reproduce sentence embedding operation trying convert sentence embedding following code code worked however time run code give different result change every time input thank help
How to use ELMO Embeddings as the First Embedding Layer in tf 2.0 Keras using tf-hub?,"<p>I am trying to build a <code>NER</code> model in <code>Keras</code> using <code>ELMO</code> Embeddings. SO I stumped across <a href=""https://www.depends-on-the-definition.com/named-entity-recognition-with-residual-lstm-and-elmo/"" rel=""nofollow noreferrer"">this tutorial</a> and started implementing. I got lots of errors and some of them are as:</p>
<pre><code>import tensorflow as tf
import tensorflow_hub as hub
from keras import backend as K


sess = tf.Session()
K.set_session(sess)

elmo_model = hub.Module(&quot;https://tfhub.dev/google/elmo/2&quot;, trainable=True)
sess.run(tf.global_variables_initializer())
sess.run(tf.tables_initializer())

def ElmoEmbedding(x):
    return elmo_model(inputs={&quot;tokens&quot;: tf.squeeze(tf.cast(x, tf.string)),
                            &quot;sequence_len&quot;: tf.constant(batch_size*[max_len])},signature=&quot;tokens&quot;,as_dict=True)[&quot;elmo&quot;]

input_text = Input(shape=(max_len,), dtype=tf.string)
embedding = Lambda(ElmoEmbedding, output_shape=(None, 1024))(input_text)
</code></pre>
<p>It gives me  <code>AttributeError: module 'tensorflow' has no attribute 'Session'</code> . So if I comment out <code>sess=</code> code and run, it gives me <code>AttributeError: module 'keras.backend' has no attribute 'set_session'</code>.</p>
<p>Then again, <code>Elmo</code> code line is giving me <code>RuntimeError: Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled.</code>.</p>
<p>I have the following configurations:</p>
<pre><code>tf.__version__
'2.3.1'

keras.__version__
'2.4.3'

import sys
sys.version
'3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]'
</code></pre>
<p>How can I use <code>ELMO</code> Embeddings in Keras Model?</p>
",Vectorization & Embeddings,use elmo embeddings first embedding layer tf kera using tf hub trying build model using embeddings stumped across tutorial started implementing got lot error give comment code run give code line giving following configuration use embeddings kera model
Building a voice assistant into a software,"<p>I am new to natural language processing. I am looking into embedding a voice assistant into a software that will help perform tasks (logging in, running a report etc.).</p>

<p>The software already uses AWS and I was thinking if amazon Lex (or any other service/tool) has the capabilities I am looking for. </p>

<p>Is there any ready to use platforms that I can test? </p>
",Vectorization & Embeddings,building voice assistant software new natural language processing looking embedding voice assistant software help perform task logging running report etc software already us aws wa thinking amazon lex service tool ha capability looking ready use platform test
Ram not sufficient when implementing Word embeddings using Gensim on my own corpus,"<p>I'm trying to use gensim library for word embeddings
I have a data frame with each row corresponding to the text data(like a review)
I first converted each row into a list of words using</p>
<pre><code>X_train_list = []
for document in X_train:
    sentences = document.split()
    X_train_list.append(sentences)
</code></pre>
<p>eg: ['This is a sample sentence'] =&gt; ['This', 'is', 'a', 'sample', 'sentence']</p>
<p>Then i trained word2vec model using gensim on that data</p>
<pre><code>from gensim.models import Word2Vec
model = Word2Vec(total_data, size= 100, min_count=1)
</code></pre>
<p>Now when i try to embedd each word into a vector, my 12GB RAM is not sufficient and system gets crashing all the time.</p>
<pre><code>train_documents = []
count = 0
for document in X_train_list:
    word_vectors = []
    for word in document: 
        word_vectors.append(model.wv[word])
    train_documents.append(np.concatenate(word_vectors))
    count += 1
</code></pre>
<p>An help to overcome this issue will be appreciated!</p>
",Vectorization & Embeddings,ram sufficient implementing word embeddings using gensim corpus trying use gensim library word embeddings data frame row corresponding text data like review first converted row list word using eg sample sentence sample sentence trained word vec model using gensim data try embedd word vector gb ram sufficient system get crashing time help overcome issue appreciated
does WikiCorpus remove stop_words in gensim?,"<p>I built a box-embedding model on the latest wikipedia articles dump and i need to compare it with the word2vec model in gensim. I saw that if i generate the corpus data as a txt file using get_texts() method in class WikiCorpus there are a lot of stop words, so this make me think that WikiCorpus doesn't delete stop words isn't it?. Now once trained my box model on the wiki corpus txt i notice that calling the &quot;most similar&quot; function that i create appositely for box embedding prints very often stop words, instead the same word passed to the most similar function of word2vec model trained on the same corpus txt produce best results. Can someone suggest me why Word2vec model fit so well despite the corpus txt have a lot of stop words instead my box model on the same corpus not?</p>
",Vectorization & Embeddings,doe wikicorpus remove stop word gensim built box embedding model latest wikipedia article dump need compare word vec model gensim saw generate corpus data txt file using get text method class wikicorpus lot stop word make think wikicorpus delete stop word trained box model wiki corpus txt notice calling similar function create appositely box embedding print often stop word instead word passed similar function word vec model trained corpus txt produce best result someone suggest word vec model fit well despite corpus txt lot stop word instead box model corpus
BERT Word Embedding for column of pandas data frame,"<p>I m working on a NLP project using Tamil Universal Dependency dataset. I have preprocessed the data into a data frame, of which columns are tokens and its dependency tags. I would like to perform word embedding using mBERT model. Since the dataset is a pretrained model, it is already tokenized as seen in the attached Data frame. I m not sure how to proceed because, when tokens are converted to token id's are wrongly marked by the tokenizer.</p>
<pre><code>b #List of tokens
</code></pre>
<p>Data Frame</p>
<p><a href=""https://i.sstatic.net/mIfhi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mIfhi.png"" alt=""enter image description here"" /></a></p>
<p>Token ID Error</p>
<p><a href=""https://i.sstatic.net/3qVIU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3qVIU.png"" alt=""enter image description here"" /></a></p>
",Vectorization & Embeddings,bert word embedding column panda data frame working nlp project using tamil universal dependency dataset preprocessed data data frame column token dependency tag would like perform word embedding using mbert model since dataset pretrained model already tokenized seen attached data frame sure proceed token converted token id wrongly marked tokenizer data frame token id error
Compare cosine similarity of word with BERT model,"<p>Hi I am looking to generate similar words for a word using BERT model, the same approach we use in gensim to generate most_similar word, I found the approach as:</p>
<pre><code>from transformers import BertTokenizer, BertModel

import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

model = BertModel.from_pretrained('bert-base-uncased')

word = &quot;Hello&quot;

inputs = tokenizer(word, return_tensors=&quot;pt&quot;)

outputs = model(**inputs)

word_vect = outputs.pooler_output.detach().numpy()
</code></pre>
<p>Okay, now this gives me the embedding for input word given by user, so can we compare this embedding with complete BERT model for cosine similarity to find top N embeddings that are closest match with that word, and then convert the embeddings to word using the vocab.txt file in the model? is it possible?</p>
",Vectorization & Embeddings,compare cosine similarity word bert model hi looking generate similar word word using bert model approach use gensim generate similar word found approach okay give embedding input word given user compare embedding complete bert model cosine similarity find top n embeddings closest match word convert embeddings word using vocab txt file model possible
Python top 100 ngrams,"<p>Does anyone know the best way to get the top 100 ngram, bigram, and trigram from a corpus?  For example, I have product reviews from 1500 products. I am looking to extract the top 100 terms. The terms are sometimes 1 word, 2 words, 3 words, etc.</p>
<p>I tried TFIDF vectorizer and NLTK ngrams however, the problem I'm having is that they both give all the combinations of all words instead of the most frequent terms or are limited to only 1gram, bigram or trigram.</p>
",Vectorization & Embeddings,python top ngrams doe anyone know best way get top ngram bigram trigram corpus example product review product looking extract top term term sometimes word word word etc tried tfidf vectorizer nltk ngrams however problem give combination word instead frequent term limited gram bigram trigram
Gensim 4.0.0 get the word having key in model.wv,"<pre><code>with open(args.output, 'w+', encoding='utf-8') as f:
    f.write('%d %d\n' % (len(model.wv), args.size))
    for key in tqdm(model.wv): 
        // code //
        f.write(WORD, VECTOR)
</code></pre>
<p>I want to create a txt file that contains the Word and the embedding vector.</p>
<p>Anyone can help me?</p>
",Vectorization & Embeddings,gensim get word key model wv want create txt file contains word embedding vector anyone help
Convert sparse matrix (csc_matrix) to pandas dataframe,"<p>I want to convert this matrix into a pandas dataframe.
<a href=""https://i.sstatic.net/qYkPp.png"" rel=""noreferrer"">csc_matrix</a></p>

<p>The <strong>first</strong> number in the bracket should be the <strong>index</strong>, the <strong>second</strong> number being <strong>columns</strong> and the <strong>number in the end</strong> being the <strong>data</strong>.</p>

<p>I want to do this to do feature selection in text analysis, the first number represents the document, the second being the feature of word and the last number being the TFIDF score.</p>

<p>Getting a dataframe helps me to transform the text analysis problem into data analysis.</p>
",Vectorization & Embeddings,convert sparse matrix csc matrix panda dataframe want convert matrix panda dataframe csc matrix first number bracket index second number column number end data want feature selection text analysis first number represents document second feature word last number tfidf score getting dataframe help transform text analysis problem data analysis
Python compute cosine similarity on two directories of files,"<p>I have two directories of files. One contains human-transcribed files and the other contains IBM Watson transcribed files. Both directories have the same number of files, and both were transcribed from the same telephony recordings.</p>
<p>I'm computing cosine similarity using SpaCy's .similarity between the matching files and print or store the result along with the compared file names. I have attempted using a function to iterate through in addition to for loops but cannot find a way to iterate between both directories, compare the two files with a matching index, and print the result.</p>
<p>Here's my current code:</p>
<pre><code># iterate through files in both directories
for human_file, api_file in os.listdir(human_directory), os.listdir(api_directory):
    # set the documents to be compared and parse them through the small spacy nlp model
    human_model = nlp_small(open(human_file).read())
    api_model = nlp_small(open(api_file).read())
    
    # print similarity score with the names of the compared files
    print(&quot;Similarity using small model:&quot;, human_file, api_file, human_model.similarity(api_model))
</code></pre>
<p>I've gotten it to work with iterating through just one directory and checked that it has the expected output by printing the file name, but it doesn't work when using both directories. I've also tried something like this:</p>
<pre><code># define directories
human_directory = os.listdir(&quot;./00_data/Human Transcripts&quot;)
api_directory = os.listdir(&quot;./00_data/Watson Scripts&quot;)

# function for cosine similarity of files in two directories using small model
def nlp_small(human_directory, api_directory):
    for i in (0, (len(human_directory) - 1)):
        print(human_directory[i], api_directory[i])

nlp_small(human_directory, api_directory)
</code></pre>
<p>Which returns:</p>
<pre><code>human_10.txt watson_10.csv
human_9.txt watson_9.csv
</code></pre>
<p>But that's only two of the files, not all 17 of them.</p>
<p>Any pointers on iterating through a matching index on both directories would be much appreciated.</p>
<p>EDIT:
Thanks to @kevinjiang, here's the working code block:</p>
<pre><code># set the directories containing transcripts
human_directory = os.path.join(os.getcwd(), &quot;00_data\Human Transcripts&quot;)
api_directory = os.path.join(os.getcwd(), &quot;00_data\Watson Scripts&quot;)

# iterate through files in both directories
for human_file, api_file in zip(os.listdir(human_directory), os.listdir(api_directory)):
    # set the documents to be compared and parse them through the small spacy nlp model
    human_model = nlp_small(open(os.path.join(os.getcwd(), &quot;00_data\Human Transcripts&quot;, human_file)).read())
    api_model = nlp_small(open(os.path.join(os.getcwd(), &quot;00_data\Watson Scripts&quot;, api_file)).read())
    
    # print similarity score with the names of the compared files
    print(&quot;Similarity using small model:&quot;, human_file, api_file, human_model.similarity(api_model))
</code></pre>
<p>And here's most of the output (need to fix a UTF-16 character in one of the files that halts the loop):</p>
<pre><code>nlp_small = spacy.load('en_core_web_sm')
Similarity using small model: human_10.txt watson_10.csv 0.9274665883462793
Similarity using small model: human_11.txt watson_11.csv 0.9348740684005554
Similarity using small model: human_12.txt watson_12.csv 0.9362025469343344
Similarity using small model: human_13.txt watson_13.csv 0.9557355330988958
Similarity using small model: human_14.txt watson_14.csv 0.9088701120190216
Similarity using small model: human_15.txt watson_15.csv 0.9479464053189846
Similarity using small model: human_16.txt watson_16.csv 0.9599724037676819
Similarity using small model: human_17.txt watson_17.csv 0.9367605599306302
Similarity using small model: human_18.txt watson_18.csv 0.8760760037870665
Similarity using small model: human_2.txt watson_2.csv 0.9184563762823503
Similarity using small model: human_3.txt watson_3.csv 0.9287452822270265
Similarity using small model: human_4.txt watson_4.csv 0.9415664367046419
Similarity using small model: human_5.txt watson_5.csv 0.9158895909429551
Similarity using small model: human_6.txt watson_6.csv 0.935313240861153
</code></pre>
<p>After I've fixed the character encoding bug I'll be wrapping it in a function so that I can call the large or small model on two directories for the remaining APIs I have to test.</p>
",Vectorization & Embeddings,python compute cosine similarity two directory file two directory file one contains human transcribed file contains ibm watson transcribed file directory number file transcribed telephony recording computing cosine similarity using spacy similarity matching file print store result along compared file name attempted using function iterate addition loop find way iterate directory compare two file matching index print result current code gotten work iterating one directory checked ha expected output printing file name work using directory also tried something like return two file pointer iterating matching index directory would much appreciated edit thanks kevinjiang working code block output need fix utf character one file halt loop fixed character encoding bug wrapping function call large small model two directory remaining apis test
Simple NLP: How to use ngram to do word similarity?,"<p>I Hear that google uses up to 7-grams for their semantic-similarity comparison. I am interested in finding words that are similar in context (i.e. cat and dog) and I was wondering how do I compute the similarity of two words on a n-gram model given that n > 2.</p>

<p>So basically given a text, like ""hello my name is blah blah. I love cats"", and I generate a 3-gram set of the above: </p>

<p>[('hello', 'my', 'name'),
 ('my', 'name', 'is'),
 ('name', 'is', 'blah'),
 ('is', 'blah', 'blah'),
 ('blah', 'blah', 'I'),
 ('blah', 'I', 'love'),
 ('I', 'love', 'cats')]</p>

<p>PLEASE DO NOT RESPOND IF YOU ARE NOT GIVING SUGGESTIONS ON HOW TO DO THIS SPECIFIC NGRAM PROBLEM</p>

<p>What kind of calculations could I use to find the similarity between 'cats' and 'name'? (which should be 0.5) I know how to do this with bigram, simply by dividing freq(cats,name)/ ( freq(cats,<em>) + freq(name,</em>) ). But what about for n > 2?</p>
",Vectorization & Embeddings,simple nlp use ngram word similarity hear google us gram semantic similarity comparison interested finding word similar context e cat dog wa wondering compute similarity two word n gram model given n basically given text like hello name blah blah love cat generate gram set hello name name name blah blah blah blah blah blah love love cat please respond giving suggestion specific ngram problem kind calculation could use find similarity cat name know bigram simply dividing freq cat name freq cat freq name n
BERT get sentence embedding,"<p>I am replicating code from <a href=""https://colab.research.google.com/drive/1yFphU6PW9Uo6lmDly_ud9a6c4RCYlwdX"" rel=""noreferrer"">this page</a>. I have downloaded the BERT model to my local system and getting sentence embedding.</p>
<p>I have around 500,000 sentences for which I need sentence embedding and it is taking a lot of time.</p>
<ol>
<li>Is there a way to expedite the process?</li>
<li>Would sending batches of sentences rather than one sentence at a time help?</li>
</ol>
<p>.</p>
<pre><code>#!pip install transformers
import torch
import transformers
from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased',
                                  output_hidden_states = True, # Whether the model returns all hidden-states.
                                  )

# Put the model in &quot;evaluation&quot; mode, meaning feed-forward operation.
model.eval()

corpa=[&quot;i am a boy&quot;,&quot;i live in a city&quot;]



storage=[]#list to store all embeddings

for text in corpa:
    # Add the special tokens.
    marked_text = &quot;[CLS] &quot; + text + &quot; [SEP]&quot;

    # Split the sentence into tokens.
    tokenized_text = tokenizer.tokenize(marked_text)

    # Map the token strings to their vocabulary indeces.
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)

    segments_ids = [1] * len(tokenized_text)

    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])

    # Run the text through BERT, and collect all of the hidden states produced
    # from all 12 layers. 
    with torch.no_grad():

        outputs = model(tokens_tensor, segments_tensors)

        # Evaluating the model will return a different number of objects based on 
        # how it's  configured in the `from_pretrained` call earlier. In this case, 
        # becase we set `output_hidden_states = True`, the third item will be the 
        # hidden states from all layers. See the documentation for more details:
        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel
        hidden_states = outputs[2]


    # `hidden_states` has shape [13 x 1 x 22 x 768]

    # `token_vecs` is a tensor with shape [22 x 768]
    token_vecs = hidden_states[-2][0]

    # Calculate the average of all 22 token vectors.
    sentence_embedding = torch.mean(token_vecs, dim=0)

    storage.append((text,sentence_embedding))
</code></pre>
<p>######update 1</p>
<p>I modified my code based upon the answer provided. It is not doing full batch processing</p>
<pre><code>#!pip install transformers
import torch
import transformers
from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased',
                                  output_hidden_states = True, # Whether the model returns all hidden-states.
                                  )

# Put the model in &quot;evaluation&quot; mode, meaning feed-forward operation.
model.eval()

batch_sentences = [&quot;Hello I'm a single sentence&quot;,
                    &quot;And another sentence&quot;,
                    &quot;And the very very last one&quot;]
encoded_inputs = tokenizer(batch_sentences)


storage=[]#list to store all embeddings
for i,text in enumerate(encoded_inputs['input_ids']):
    
    tokens_tensor = torch.tensor([encoded_inputs['input_ids'][i]])
    segments_tensors = torch.tensor([encoded_inputs['attention_mask'][i]])
    print (tokens_tensor)
    print (segments_tensors)

    # Run the text through BERT, and collect all of the hidden states produced
    # from all 12 layers. 
    with torch.no_grad():

        outputs = model(tokens_tensor, segments_tensors)

        # Evaluating the model will return a different number of objects based on 
        # how it's  configured in the `from_pretrained` call earlier. In this case, 
        # becase we set `output_hidden_states = True`, the third item will be the 
        # hidden states from all layers. See the documentation for more details:
        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel
        hidden_states = outputs[2]


    # `hidden_states` has shape [13 x 1 x 22 x 768]

    # `token_vecs` is a tensor with shape [22 x 768]
    token_vecs = hidden_states[-2][0]

    # Calculate the average of all 22 token vectors.
    sentence_embedding = torch.mean(token_vecs, dim=0)
    print (sentence_embedding[:10])
    storage.append((text,sentence_embedding))
</code></pre>
<p>I could update first 2 lines from the for loop to below. But they work only if all sentences have same length after tokenization</p>
<pre><code>tokens_tensor = torch.tensor([encoded_inputs['input_ids']])
segments_tensors = torch.tensor([encoded_inputs['attention_mask']])
</code></pre>
<p>moreover in that case <code>outputs = model(tokens_tensor, segments_tensors) </code> fails.</p>
<p>How could I fully perform batch processing in such case?</p>
",Vectorization & Embeddings,bert get sentence embedding replicating code page downloaded bert model local system getting sentence embedding around sentence need sentence embedding taking lot time way expedite process would sending batch sentence rather one sentence time help update modified code based upon answer provided full batch processing could update first line loop work sentence length tokenization moreover case fails could fully perform batch processing case
increasing efficiency of cosine simarlity,"<p>So I'm trying to find similar sentences in a moderately large file with 60000 rows. Now to accomplish this, I first created sentence encodings of each row using google universal sentence encoder. Then I use this to compare cosine similarity and find similar sentences</p>
<pre><code>module_url = &quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot; 
model = hub.load(module_url)

sentence_embeddings = model(sentences)


def cosine(u, v):
    return numpy.dot(u, v) / (numpy.linalg.norm(u) * numpy.linalg.norm(v))

for idx,query in list(enumerate(sentences)):
    for idx2,sente in enumerate(sentences):
        if idx1 == idx2:
            continu
        sim = cosine(sentence_embeddings[idx], sentence_embeddings[idx2])
        if sim &gt;= .80:
            # store in output

</code></pre>
<p>So with <code>60000^2</code> operations of cosine similarity, it takes days on my device to execute this code. Is there a way I can do this faster. I understand that this is probably as fast as I can get with python so if the solution involves using some other language, im open to it as well</p>
<p>Thanks a ton</p>
",Vectorization & Embeddings,increasing efficiency cosine simarlity trying find similar sentence moderately large file row accomplish first created sentence encoding row using google universal sentence encoder use compare cosine similarity find similar sentence operation cosine similarity take day device execute code way faster understand probably fast get python solution involves using language im open well thanks ton
Text augmentation effective when using Doc2Vec,"<p>There are many ways to augment text data, and many articles about this area. I would like to understand if there is a text augmentation technique which is likely to work well, when the text input is vectorized using a Doc2Vec model.</p>
<p>My intuition here would be that using synonym replacement may work well, to create a smoother decision boundary in downstream classifiers.</p>
<p>The pipeline is as follows: <code>text</code> -&gt; <code>d2v</code> -&gt; <code>clf</code></p>
<p>EDIT: based on comments.
I am not looking to augment prior to d2v training. I am using my trained model for a downstream task which looks like this:</p>
<p>text -&gt; d2v -&gt; binary classifier</p>
<p>Lets say I am predicting sentiment, but I have only a few positive samples:</p>
<p>&quot;the dog is happy&quot;</p>
<p>So I am making more samples by augmenting the positive:</p>
<p>&quot;the dog is ecstatic&quot;</p>
<p>Now I have two samples which I pass to d2v to vectorize and use downstream rather than one.</p>
<p>My question is therefore what sort of augmentation works, because for example since d2v in some modes does not care much about word order, doing things like &quot;sentence swap&quot; could be useless.</p>
",Vectorization & Embeddings,text augmentation effective using doc vec many way augment text data many article area would like understand text augmentation technique likely work well text input vectorized using doc vec model intuition would using synonym replacement may work well create smoother decision boundary downstream classifier pipeline follows edit based comment looking augment prior v training using trained model downstream task look like text v binary classifier let say predicting sentiment positive sample dog happy making sample augmenting positive dog ecstatic two sample pas v vectorize use downstream rather one question therefore sort augmentation work example since v mode doe care much word order thing like sentence swap could useless
Creating new word-embeddings out of desired words from the vocab,"<p>I have created word2vec for some text data using count vectorizer. Now I want to group certain words from the generated vocab (that denote common meaning/aspect) into new single word, and thus find the new word2vec representation.
How should I solve this problem ?</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
import numpy as np

A = {'some_text': ('cat is red and fat', 'dog is blue hairy thin','horse is brown and tall')}
data = pd.DataFrame(A)

#       some_text
#    0  cat is red and fat
#    1  dog is blue hairy thin
#    2  horse is brown and tall

cv = CountVectorizer()
vec = cv.fit_transform(data.some_text)
df = pd.DataFrame(vec.toarray(), columns=cv.get_feature_names())

#       and  blue  brown  cat  dog  fat  hairy  horse  is  red  tall  thin
#    0    1     0      0    1    0    1      0      0   1    1     0     0
#    1    0     1      0    0    1    0      1      0   1    0     0     1
#    2    1     0      1    0    0    0      0      1   1    0     1     0
</code></pre>
<p>I want to group certain words from the bag-of-words into another word like this :</p>
<pre><code>color = {'blue',' brown', 'red'}
body = {'fat', 'thin', 'tall'}
animal = {'cat',' dog', 'horse'}
</code></pre>
<p>I want the vocab to have the above words ( for example,  word 'animal' instead of 'cat' , 'dog'  or ' horse') and then get the word-counts. How should I proceed to get new desired word-embeddings?</p>
",Vectorization & Embeddings,creating new word embeddings desired word vocab created word vec text data using count vectorizer want group certain word generated vocab denote common meaning aspect new single word thus find new word vec representation solve problem want group certain word bag word another word like want vocab word example word animal instead cat dog horse get word count proceed get new desired word embeddings
NLP: Find the most similar Corpus (not Document),"<p>I'm looking for suggestions on how to approach a document classification problem.  I will explain by means of example:</p>
<p><strong>Problem statement</strong></p>
<p>I have a collection of papers published by a university.  I have another collection published by another university.  And so on, for many universities.</p>
<p>When a new paper comes in, I'd like to determine which university probably published it.</p>
<p><strong>My current approach</strong></p>
<ol>
<li>For each university, build a dictionary of all terms from all papers with frequencies.  Preprocess into terms and build a gensim <code>Dictionary</code> per university.</li>
<li>Build a &quot;master&quot; dictionary by merging all of the university-specific dictionaries.  Iterate and perform <code>master_dictionary.merge_with(university_dictionary)</code></li>
<li>Treat each university dictionary as a document in a new corpus. Turn it into a BoW representation, and build a model from that.  TfIdf/LSI/LDA.</li>
<li>Perform a similarity match of the new paper (as BoW) against the model.  Find the university dictionary document that matches closest.</li>
</ol>
<p><strong>My question</strong></p>
<p>How is a problem like this tackled? (And is there a name for this?)</p>
<ul>
<li>I'm currently comparing a new document with a summarized document, one per university.</li>
<li>I could compare a new document with every document across all universities.  Then get the universities for the top matching documents using metadata.</li>
<li>I've run across the Author-Topic Model but haven't looked into it, but that seems like this may be a good fit.</li>
</ul>
<p>Any other ideas?</p>
",Vectorization & Embeddings,nlp find similar corpus document looking suggestion approach document classification problem explain mean example problem statement collection paper published university another collection published another university many university new paper come like determine university probably published current approach university build dictionary term paper frequency preprocess term build gensim per university build master dictionary merging university specific dictionary iterate perform treat university dictionary document new corpus turn bow representation build model tfidf lsi lda perform similarity match new paper bow model find university dictionary document match closest question problem like tackled name currently comparing new document summarized document one per university could compare new document every document across university get university top matching document using metadata run across author topic model looked seems like may good fit idea
Universal sentence encoder for big document similarity,"<p>I need to create a 'search engine' experience : from a short query (few words), I need to find the relevant documents in a corpus of thousands documents.</p>

<p>After analyzing few approaches, I got very good results with the Universal Sentence Encoder from Google.
The problem is that my documents can be very long. For these very long texts it looks like the performance are decreasing so my idea was to cut the text in sentences/paragraph.</p>

<p>So I ended up with getting a list of vectors for each document (representing each part of the document).</p>

<p>My question is : is there a state-of-the-art algorithm/methodology to compute a scoring from a list of vector ? I don't really want to merge them into one as it would create the same effect than before (the relevant part would be diluted in the document). Any scoring algorithms to sum up the multiple cosine similarities between the query and the different parts of the text ?</p>

<p>important information : I can have short and long text. So I can have 1 up to 10 vectors for a document.</p>
",Vectorization & Embeddings,universal sentence encoder big document similarity need create search engine experience short query word need find relevant document corpus thousand document analyzing approach got good result universal sentence encoder google problem document long long text look like performance decreasing idea wa cut text sentence paragraph ended getting list vector document representing part document question state art algorithm methodology compute scoring list vector really want merge one would create effect relevant part would diluted document scoring algorithm sum multiple cosine similarity query different part text important information short long text vector document
How do I classify text using cosine similarity?,"<p>I have got a typical sentiment analysis task, my dataset consists of text and 3 classes (negative, neutral, positive). I have vectorized text using Bert sentence transformers and calculated the cosine similarity metric of my test_embeddings: <a href=""https://i.sstatic.net/2PUqx.png"" rel=""nofollow noreferrer"">output image</a>. Now, how do I classify each test sentence and calculate accuracy?</p>
",Vectorization & Embeddings,classify text using cosine similarity got typical sentiment analysis task dataset consists text class negative neutral positive vectorized text using bert sentence transformer calculated cosine similarity metric test embeddings output image classify test sentence calculate accuracy
Do documents without labels add information to Facebook&#39;s FastText supervised classifier?,"<p>I hope you guys are doing great.</p>
<p>I'm training a classifier with Facebook's FastText to determine if a piece of text (tweet) is talking about economy or not.
For doing this task, I have about 2200 tagged tweets as &quot;economy&quot; or &quot;not_economy&quot;, <strong>but I also have almost a million unlabeled tweets</strong>.</p>
<p>Reading FastText's documentation I know the supervised input file should be a document with a tweet per line with a prefix of the shape <code>__label__economy</code> or <code>__label__not_economy</code>.</p>
<p>The documentation doesn't talk about adding unlabeled documents to the unsupervised input file, but since it's a word embedding model, it's supposed to take context information from the word's text distribution, so I think giving the model all this extra information should help getting a better embedding representation of my vocabulary. For this reason I'm training the model (with <code>fasttext supervised -input tweets_input -output tweets_model</code>) but I'm also adding untagged documents at the end. The things is that all these almost 1M tweets doesn't seem to be enhancing the model at all.</p>
<p>The other way I know I can take advantage of this data is training a unsupervised model and start using the sentence embedings to train a classifier.</p>
<p>The question is the one in the title:</p>
<p>Do documents without labels add information to Facebook's FastText supervised classifier? Is it better to get the document embeddings and train my own classifier with other library?</p>
<p>Thanks for any information that helps me understand better.</p>
",Vectorization & Embeddings,document without label add information facebook fasttext supervised classifier hope guy great training classifier facebook fasttext determine piece text tweet talking economy task tagged tweet economy economy also almost million unlabeled tweet reading fasttext documentation know supervised input file document tweet per line prefix shape documentation talk adding unlabeled document unsupervised input file since word embedding model supposed take context information word text distribution think giving model extra information help getting better embedding representation vocabulary reason training model also adding untagged document end thing almost tweet seem enhancing model way know take advantage data training unsupervised model start using sentence embedings train classifier question one title document without label add information facebook fasttext supervised classifier better get document embeddings train classifier library thanks information help understand better
Default estimation method of Gensim&#39;s Word2vec Skip-gram?,"<p>I am now trying to use word2vec by estimating skipgram embeddings via NCE (noise contrastive estimation) rather than conventional negative sampling method, as a recent paper did (<a href=""https://asistdl.onlinelibrary.wiley.com/doi/full/10.1002/asi.24421?casa_token=uCHp2XQZVV8AAAAA%3Ac7ETNVxnpqe7u9nhLzX7pIDjw5Fuq560ihU3K5tYVDcgQEOJGgXEakRudGwEQaomXnQPVRulw8gF9XeO"" rel=""nofollow noreferrer"">https://asistdl.onlinelibrary.wiley.com/doi/full/10.1002/asi.24421?casa_token=uCHp2XQZVV8AAAAA%3Ac7ETNVxnpqe7u9nhLzX7pIDjw5Fuq560ihU3K5tYVDcgQEOJGgXEakRudGwEQaomXnQPVRulw8gF9XeO</a>). The paper has a replication GitHub repository (<a href=""https://github.com/sandeepsoni/semantic-progressiveness"" rel=""nofollow noreferrer"">https://github.com/sandeepsoni/semantic-progressiveness</a>), and it mainly relied on gensim for implementing word2vec, but the repository is not well organized and in a mess, so I have no clue about how the authors implemented NCE estimation via gensim's word2vec.</p>
<p>The authors just used gensim's word2vec as a default status without including any options, so my question is what is the default estimation method for gensim's word2vec under Skip-gram embeddings. NCE? According to your manual,  it just says there is an option for negative sampling, and if set to 0, then no negative sampling is used. But then what estimation method is used?
negative (int, optional) – If &gt; 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.</p>
<p>Thanks you in advance, and look forward to hearing from you soon!</p>
",Vectorization & Embeddings,default estimation method gensim word vec skip gram trying use word vec estimating skipgram embeddings via nce noise contrastive estimation rather conventional negative sampling method recent paper paper ha replication github repository mainly relied gensim implementing word vec repository well organized mess clue author implemented nce estimation via gensim word vec author used gensim word vec default status without including option question default estimation method gensim word vec skip gram embeddings nce according manual say option negative sampling set negative sampling used estimation method used negative int optional negative sampling used int negative specifies many noise word drawn usually set negative sampling used thanks advance look forward hearing soon
How does BERT word embedding preprocess work,"<p>I'm trying to figure out what <code>BERT</code> preprocess does. I mean, how it is done. But I can't find a good explanation. I would appreciate, if somebody know, a link to a better and deeply explained solution.
If someone, by the other hand, wants to solve it here, I would be also extremly thankful!</p>
<p>My question is, how does <code>BERT</code> mathematically convert a string input into a vector of numbers with fixed size? Which are the logical steps that follows?</p>
",Vectorization & Embeddings,doe bert word embedding preprocess work trying figure preprocess doe mean done find good explanation would appreciate somebody know link better deeply explained solution someone hand want solve would also extremly thankful question doe mathematically convert string input vector number fixed size logical step follows
NLP: Finding which sentence is closest in meaning to a list of other sentences,"<p>I have two lists of sentences (<strong>list A</strong> and <strong>list B</strong>). I want to find which sentence in <strong>A</strong> is closest in meaning to the entirety of <strong>B</strong>.</p>
<p>This is not the same as the standard cosine similarity check you can do when comparing (in spacy for example) two doc objects: even if i iterate through <strong>A</strong> and compare each element of <strong>A</strong> to all elements of <strong>B</strong>, it leaves me with a number of cosine similarity scores, while i want just one number to represent the closeness of each element of <strong>A</strong> to <em>all</em> of <strong>B</strong>.</p>
<p>So far I have tried the folowing:
for every element in <strong>A</strong>, perform cosine similarity check with every element in <strong>B</strong>, leaving me with a list of values equal in length to <strong>B</strong>. Then I calculate the average of this list, leaving me with a single value which would ideally represent how close that element of <strong>A</strong> was to <em>all</em> of <strong>B</strong>.</p>
<p>The issue with that approach is that the averaging results in too much information loss and by the time ive done this for all elements of <strong>A</strong>, there isnt much difference in these condensed averages and therefore hard to conclude which element of <strong>A</strong> is closest to <em>all</em> of <strong>B</strong>.</p>
<p>P.S.
I can show code if asked but feel it's irrelevant because the issue is with the approach itself, not broken code.</p>
",Vectorization & Embeddings,nlp finding sentence closest meaning list sentence two list sentence list list b want find sentence closest meaning entirety b standard cosine similarity check comparing spacy example two doc object even iterate compare element element b leaf number cosine similarity score want one number represent closeness element b far tried folowing every element perform cosine similarity check every element b leaving list value equal length b calculate average list leaving single value would ideally represent close element wa b issue approach averaging result much information loss time ive done element isnt much difference condensed average therefore hard conclude element closest b p show code asked feel irrelevant issue approach broken code
Seq2Seq model return same vector for all sentences,"<p>I'm trying to generate an abstractive text summarization. I'm using word2vec for embedding and bi-lstm with 2 layer in encoder and bi-lstm with 1 layer in decoder, and also I'm using Attention. I trained the model and it always return same vector for all sentences of input. How can I fix this problem?</p>
<p>Training Code</p>
<pre><code>latent_dim = 185
embedding_dim=128

encoder_inputs = Input(shape=(int(art_max_length),))

#embedding layer
enc_emb=Embedding(input_vocab_size+1,embedding_dim, weights=[x_emb_matrix_reduce],trainable=False)(encoder_inputs)

#encoder lstm 1
encoder_bi_lstm1 = Bidirectional(LSTM(latent_dim,
                                   return_sequences=True,
                                   return_state=True,
                                   dropout=0.4,
                                   recurrent_dropout=0.4), 
                                 merge_mode=&quot;concat&quot;)
encoder_output1, forward_state_h1, forward_state_c1, backward_state_h1, backward_state_c1 = encoder_bi_lstm1(enc_emb)
encoder_states1 = [forward_state_h1, forward_state_c1, backward_state_h1, backward_state_c1]

#encoder lstm 2
encoder_bi_lstm2 = Bidirectional(LSTM(latent_dim,
                                   return_sequences=True,
                                   return_state=True,
                                   dropout=0.4,
                                   recurrent_dropout=0.4), 
                                 merge_mode=&quot;concat&quot;)
encoder_output2, forward_state_h2, forward_state_c2, backward_state_h2, backward_state_c2 = encoder_bi_lstm2(encoder_output1)
encoder_states2 = [forward_state_h2, forward_state_c2, backward_state_h2, backward_state_c2]

# Set up the decoder, using `encoder_states` as initial state.
decoder_inputs = Input(shape=(None,))

#embedding layer
dec_emb_layer = Embedding(output_vocab_size+1, embedding_dim, weights=[y_emb_matrix_reduce], trainable=False)
dec_emb = dec_emb_layer(decoder_inputs)

decoder_bi_lstm = Bidirectional(LSTM(latent_dim, 
                                  return_sequences=True, 
                                  return_state=True,
                                  dropout=0.4,
                                  recurrent_dropout=0.2),
                                 merge_mode=&quot;concat&quot;)
decoder_outputs, decoder_fwd_state_h1, decoder_fwd_state_c1, decoder_back_state_h1, decoder_back_state_c1 = decoder_bi_lstm(dec_emb,initial_state=encoder_states2)
decoder_states = [decoder_fwd_state_h1, decoder_fwd_state_c1, decoder_back_state_h1, decoder_back_state_c1]

# Attention layer
attn_layer = AttentionLayer(name='attention_layer')
attn_out, attn_states = attn_layer([encoder_output2, decoder_outputs])

# Concat attention input and decoder LSTM output
decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])

model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

epochs = 75
batch_size = 3
learning_rate = 0.001
initial_accumulator_value = 0.1
name = 'Adagrad'
clipnorm = 1.0

opt = Adagrad(learning_rate=learning_rate, initial_accumulator_value=initial_accumulator_value, name=name, clipnorm=clipnorm)
model.compile(optimizer=opt, loss='sparse_categorical_crossentropy')
es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1,patience=10)
history=model.fit(x_tr, y_tr, epochs=epochs, callbacks=[es], steps_per_epoch=250, validation_steps=10, batch_size=batch_size, validation_data=(x_val,y_val))
</code></pre>
<p>Inference Code</p>
<pre><code>reverse_target_word_index = y_tokenizer.index_word
reverse_source_word_index = x_tokenizer.index_word
target_word_index = y_tokenizer.word_index

# Encode the input sequence to get the feature vector
encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_output2, forward_state_h2, forward_state_c2, backward_state_h2, backward_state_c2])

# Decoder setup
# Below tensors will hold the states of the previous time step
decoder_state_input_h_fwd = Input(shape=(latent_dim,))
decoder_state_input_h_bwd = Input(shape=(latent_dim,))

decoder_state_input_c_fwd = Input(shape=(latent_dim,))
decoder_state_input_c_bwd = Input(shape=(latent_dim,))

decoder_hidden_state_input = Input(shape=(art_max_length,latent_dim*2))

# Get the embeddings of the decoder sequence
dec_emb2= dec_emb_layer(decoder_inputs)

# To predict the next word in the sequence, set the initial states to the states from the previous time step
decoder_outputs2, decoder_fwd_state_h2, decoder_fwd_state_c2, decoder_back_state_h2, decoder_back_state_c2 = decoder_bi_lstm(dec_emb2, initial_state=[decoder_state_input_h_fwd, decoder_state_input_h_bwd, decoder_state_input_c_fwd, decoder_state_input_c_bwd])
decoder_states2 = [decoder_fwd_state_h2, decoder_fwd_state_c2, decoder_back_state_h2, decoder_back_state_c2]

#attention inference
attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])
decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])

# A dense softmax layer to generate prob dist. over the target vocabulary
decoder_outputs2 = decoder_dense(decoder_inf_concat)

# Final decoder model
decoder_model = Model(
    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h_fwd, decoder_state_input_h_bwd, decoder_state_input_c_fwd, decoder_state_input_c_bwd],
    [decoder_outputs2] + decoder_states2)
</code></pre>
<p>Code to generate summary</p>
<pre><code>def seq2summary(input_seq):
    newString=''
    for i in input_seq:
            if((i[0]!=0) and (i[0]!=target_word_index['sostok']) and (i[0]!=target_word_index['eostok'])):
                newString=newString+reverse_target_word_index[i[0]]+' '
    return newString

def seq2text(input_seq):
    newString=''
    for i in input_seq:
        if(i!=0):
            newString=newString+reverse_source_word_index[i]+' '
    return newString

def decode_sequence(input_seq):
    e_out, e_h_fwd, e_c_fwd, e_h_bwd, e_c_bwd = encoder_model.predict(input_seq)
    
    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1,1))

    # Populate the first word of target sequence with the start word.
    target_seq[0, 0] = target_word_index['sostok']

    stop_condition = False
    decoded_sentence = ''
    
    while not stop_condition:
        output_tokens, h_fwd, c_fwd, h_bwd, c_bwd = decoder_model.predict([target_seq] + [e_out, e_h_fwd, e_c_fwd, e_h_bwd, e_c_bwd])

        return output_tokens[0, -1, :]
        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        
        sampled_token = reverse_target_word_index[sampled_token_index]

        if(sampled_token!='eostok'):
            decoded_sentence += ' '+sampled_token

        # Exit condition: either hit max length or find stop word.
        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) &gt;= (high_max_length-1)):
            stop_condition = True

        # Update the target sequence (of length 1).
        target_seq = np.zeros((1,1))
        target_seq[0, 0] = sampled_token_index

        # Update internal states 
        e_h_fwd, e_c_fwd, e_h_bwd, e_c_bwd = h_fwd, c_fwd, h_bwd, c_bwd
    
    return decoded_sentence
</code></pre>
<p>Results of vectors:</p>
<pre><code>e_h_fwd
Result:
array([[-0.00384058, -0.0084216 ,  0.00099799, -0.00328317, -0.00355412,
         0.01573788, -0.00565114,  0.0002754 , -0.01011071,  0.03385576,
         0.01035002,  0.0010401 ,  0.01606524,  0.00338535, -0.0208919 ,
         0.002799  , -0.00558226, -0.00252697, -0.00916545,  0.00482792,
         0.00838646, -0.00736981, -0.00089604, -0.00780456,  0.00439578,
         0.02386101, -0.01245494,  0.0068648 , -0.01109423, -0.00279979,
        -0.0048555 ,  0.00291485, -0.00111228,  0.0121593 ,  0.00718876,
        -0.00367533,  0.00612858,  0.0026198 , -0.00990033, -0.00372838,
         0.01660432,  0.01064453,  0.01216934, -0.01671972, -0.021307  ,
         0.00358878, -0.00851676,  0.00872963, -0.00098289, -0.00512723,
         0.00447382, -0.00086343,  0.00142587, -0.01713295, -0.01154616,
        -0.00318079, -0.0213894 ,  0.01909565,  0.00537347,  0.00287433,
         0.00013318,  0.01882311, -0.00919805, -0.01009239, -0.01000161,
         0.00729822, -0.00228036,  0.01970326, -0.00668583,  0.01141307,
        -0.00155173, -0.00519767, -0.005886  ,  0.00621226,  0.0005807 ,
        -0.00401507, -0.02050336, -0.0063515 , -0.0088415 ,  0.01226105,
         0.00378229,  0.00897009, -0.00173353, -0.00694196,  0.00197844,
        -0.0178321 ,  0.00554329,  0.01416476, -0.01519079,  0.00422954,
        -0.00771015,  0.00344123, -0.01047825, -0.00756182, -0.00108388,
        -0.01648704,  0.00209498,  0.0071196 , -0.01291664, -0.00549853,
        -0.01216177,  0.0046125 ,  0.00120374, -0.00372009,  0.01676877,
        -0.00930131, -0.00677394, -0.0162948 , -0.00530502, -0.01685343,
         0.01167075,  0.0062821 , -0.01340364,  0.00760005, -0.0337769 ,
         0.00708523, -0.00263025,  0.00446939,  0.02564106, -0.00254333,
        -0.00707568,  0.01608927,  0.00716687, -0.00965973, -0.00327503,
        -0.00604013,  0.0175317 ,  0.01505202, -0.00426429, -0.00377769,
        -0.00929095, -0.01969613,  0.00719869, -0.01020684,  0.01040385,
         0.01139158, -0.0043503 , -0.00274339, -0.00616975, -0.01331878,
         0.00295496, -0.01160615, -0.00336138,  0.00886331, -0.02004485,
         0.01137386,  0.00428817, -0.00449507, -0.00655314, -0.01015342,
         0.02188095,  0.00309571,  0.00742747,  0.02219234,  0.00236926,
        -0.00491316,  0.01939732,  0.01722919,  0.00388572,  0.02340838,
        -0.01717703, -0.00525931,  0.01344595, -0.00262558,  0.01469047,
         0.0196475 ,  0.01402889, -0.0011783 , -0.01755165, -0.01247887,
         0.01138979,  0.00034305,  0.00225358, -0.01848649, -0.01921862,
         0.0028248 , -0.01087625,  0.00121242, -0.02166731,  0.01230442,
         0.01093107,  0.01236717, -0.01110782, -0.00536899, -0.01232667]],
      dtype=float32)

output_tokens 
Result:
array([[[6.1362894e-06, 5.7854427e-06, 5.8488249e-06, ...,
         5.7374464e-06, 5.7320071e-06, 5.7324951e-06]]], dtype=float32)

np.argmax(output_tokens[0, -1, :])
Result:
0

</code></pre>
",Vectorization & Embeddings,seq seq model return vector sentence trying generate abstractive text summarization using word vec embedding bi lstm layer encoder bi lstm layer decoder also using attention trained model always return vector sentence input fix problem training code inference code code generate summary result vector
I want to find the word embedding using BERT for a single word in a sentence. Anyone has any idea how to do it?,"<p>For example in the sentence: &quot;Earth revolves around the sun and rotate around its own axis&quot;, I want to embed it with BERT word embeddings and find the embedding for &quot;rotate&quot; in this particular sentence. How to do it?</p>
",Vectorization & Embeddings,want find word embedding using bert single word sentence anyone ha idea example sentence earth revolves around sun rotate around axis want embed bert word embeddings find embedding rotate particular sentence
How to combine both word embeddings and pos embedding together to build the classifier,"<p>You known POS is like 'NP', 'VERB'. How can I combine these features to word2vec?</p>

<p>Just like the follow vectors?</p>

<pre><code>keyword    V1         V2          V3         V4            V5         V6   
corruption 0.07397  0.290874    -0.170812   0.085428     'VERB'    'NP' 
people      ..............................................................
budget      ...........................................................
</code></pre>
",Vectorization & Embeddings,combine word embeddings po embedding together build classifier known po like np verb combine feature word vec like follow vector
How to use model with TFIdVectorizer in react webapp,"<p>I created this book recommendation model by watching tutorials on youtube and searching articles online,now I want to use it for my react webapp. I have used TfidVectorizer to create a cosine similarity matrix to recommend books but I have no idea how to take that model and use it on my react webapp which is made using node.js and react.js.</p>
<pre><code>def get_recommendations(book_title, cosine_sim=cosine_sim):
    # Get the index of the movie that matches the title
    idx = indices[book_title]
    
    # Similarity scores
    similarity_scores = list(enumerate(cosine_sim[idx]))

    # Sort the books based on the similarity scores
    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)

    # Get the scores of the six most similar books
    similarity_scores =  similarity_scores[1:6]

    # Get the book indices
    book_indices = [i[0] for i in similarity_scores]

    # Return the top 5 most similar books
    return book_summary['book_title'].iloc[book_indices]
</code></pre>
<p>This is the code for the recommendation function and I want to use this in my webapp</p>
",Vectorization & Embeddings,use model tfidvectorizer react webapp created book recommendation model watching tutorial youtube searching article online want use react webapp used tfidvectorizer create cosine similarity matrix recommend book idea take model use react webapp made using node j react j code recommendation function want use webapp
How to visualize the SpaCy word embedding as scatter plot?,"<p>Each word in SpaCy is represented by a vector of length 300. How can I plot these words on a scatter plot to get a visual perspective on how close any 2 words are?</p>
",Vectorization & Embeddings,visualize spacy word embedding scatter plot word spacy represented vector length plot word scatter plot get visual perspective close word
Find most similar sentence in a large dataset of sentences,"<p>I currently have a text file with around a million sentences, each on a new line.
I am trying to build a solution where I can take a new sentence outside of this text file and have the program return the most similar sentence present in the file.</p>
<p>I have found some solutions which return the pair of sentences with the highest similarity INSIDE the existing dataset.For example <a href=""https://stackoverflow.com/questions/63718559/finding-most-similar-sentences-among-all-in-python"">this</a> one. But that is not what I am going for. I want to be able to compare a new sentence with all of those in the text file.</p>
<p>Also, I am not sure if I should be focusing on semantic similarity or cosine similarity.</p>
",Vectorization & Embeddings,find similar sentence large dataset sentence currently text file around million sentence new line trying build solution take new sentence outside text file program return similar sentence present file found solution return pair sentence highest similarity inside existing dataset example href one going want able compare new sentence text file p also sure focusing semantic similarity cosine similarity
gensim word2vec vocabulary size fluctuates up &amp; down as corpus grows despite `max_vocab_size` setting,"<p>I am training word embeddings using <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim Word2Vec</a> model with a multi-million sentence corpus that is made of 3 million unique tokens with <code>max_vocab_size = 32_000</code>.</p>
<p>Even though I set <code>min_count = 1</code>, model creates a vocabulary of far less than 32_000. When I use a subset of the corpus, vocabulary size increases!</p>
<p>In order to troubleshoot, I set up an experiment where I control the size of vocabulary with different sized subcorpus. The size of the vocabulary flactuates!</p>
<p>You can re-produce with the code below:</p>
<pre><code>import string
import numpy as np
from gensim.models import Word2Vec

letters = list(string.ascii_lowercase)

# creating toy sentences
sentences = []
number_of_sentences = 100_000

for _ in range(number_of_sentences):
    number_of_tokens = np.random.randint(1, 15, 1)[0]
    sentence = []
    for i in range(number_of_tokens):
        token = &quot;&quot;
        len_of_token = np.random.randint(1, 5, 1)[0]
        for j in range(len_of_token):
            token += np.random.choice(letters)
        sentence.append(token)
    sentences.append(sentence)

# Sanity check to ensure that input data is a list of list of strings(tokens)
for _ in range(4):
    print(np.random.choice(sentences))

# collecting some statistics about tokens
flattened = []
for sublist in sentences:
    for item in sublist:
        flattened.append(item)
        
unique_tokens = {}
for token in flattened:
    if token not in unique_tokens:
        unique_tokens[token] = len(unique_tokens)

print('Number of tokens:', f'{len(flattened):,}')
print('Number of unique tokens:', f'{len(unique_tokens):,}')


# gensim model
vocab_size = 32_000
min_count = 1
collected_data = []
for num_sentence in range(5_000, number_of_sentences + 5_000, 5_000):
    model = Word2Vec(min_count=min_count, max_vocab_size= vocab_size)
    model.build_vocab(sentences[:num_sentence])

    collected_data.append((num_sentence, len(model.wv.key_to_index)))

for duo in collected_data:
    print('Vocab size of', duo[1], 'for', duo[0], 'number of sentences!')
</code></pre>
<p>Output:</p>
<pre><code>['cpi', 'bog', 'df', 'tgi', 'xck', 'kkh', 'ktw', 'ay']
['z', 'h', 'w', 'jek', 'w', 'dqm', 'wfb', 'agq', 'egrg']
['kgwb', 'lahf', 'kzx', 'd', 'qdok', 'xka', 'hbiz', 'bjo', 'fvk', 'j', 'hx']
['old', 'c', 'ik', 'n', 'e', 'n', 'o', 'r', 'ehx', 'dlud', 'd']

Number of tokens: 748,383
Number of unique tokens: 171,485

Vocab size of 16929 for 5000 number of sentences!
Vocab size of 30314 for 10000 number of sentences!
Vocab size of 19017 for 15000 number of sentences!
Vocab size of 31394 for 20000 number of sentences!
Vocab size of 19564 for 25000 number of sentences!
Vocab size of 31831 for 30000 number of sentences!
Vocab size of 19543 for 35000 number of sentences!
Vocab size of 31744 for 40000 number of sentences!
Vocab size of 19536 for 45000 number of sentences!
Vocab size of 31642 for 50000 number of sentences!
Vocab size of 18806 for 55000 number of sentences!
Vocab size of 31255 for 60000 number of sentences!
Vocab size of 18497 for 65000 number of sentences!
Vocab size of 31166 for 70000 number of sentences!
Vocab size of 18142 for 75000 number of sentences!
Vocab size of 30886 for 80000 number of sentences!
Vocab size of 17693 for 85000 number of sentences!
Vocab size of 30390 for 90000 number of sentences!
Vocab size of 17007 for 95000 number of sentences!
Vocab size of 30196 for 100000 number of sentences!
</code></pre>
<p>I tried increasing <code>min_count</code> but it did not help this flactuation of vocabulary size. What am I missing?</p>
",Vectorization & Embeddings,gensim word vec vocabulary size fluctuates corpus grows despite setting training word embeddings using gensim word vec model multi million sentence corpus made million unique token even though set model creates vocabulary far le use subset corpus vocabulary size increase order troubleshoot set experiment control size vocabulary different sized subcorpus size vocabulary flactuates produce code output tried increasing help flactuation vocabulary size missing
Pyspark - Display Top 10 words of document,"<p>I'm quite new to Pyspark and did a tfidf processing on a dataframe with the following code</p>
<pre><code>from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer
from pyspark.sql.types import *
from pyspark.sql.functions import udf
wordsData = spark.createDataFrame([(0,&quot;Hello&quot;,&quot;World&quot;,&quot;Spark&quot;,&quot;Is&quot;,&quot;Awesome&quot;,&quot;Hello&quot;,&quot;World&quot;]),(1,[&quot;Hello&quot;,&quot;World&quot;,&quot;Spark&quot;,&quot;Is&quot;,&quot;Awesome&quot;,&quot;Hello&quot;,&quot;World&quot;]),(2,[&quot;Hello&quot;,&quot;World&quot;]),(3,[&quot;PYTHON&quot;, &quot;Is&quot;, &quot;Pretty&quot;, &quot;Awesome&quot;])],[&quot;label&quot;,&quot;words&quot;])

#hashingTF way
hashingTF = HashingTF(inputCol=&quot;words&quot;, outputCol=&quot;rawFeatures&quot;)
featurizedData = hashingTF.transform(wordsData)
idf = IDF(inputCol=&quot;rawFeatures&quot;, outputCol=&quot;features&quot;)
idfModel = idf.fit(featurizedData)
rescaledData = idfModel.transform(featurizedData)
rescaledData.show(truncate=False)
print(hashingTF.indexOf(&quot;PYTHON&quot;))
</code></pre>
<p>Now I want to store the Top 10 words with their tfidf value in a separate column. But since I'm not really used to working with vectors I'm a little stuck on how to achieve this. I know I somehow need to apply the indexOf function to every token of a document to find a mapping to its value, but I don't know how to do it. As far as I understood each vector is built up like this: (Size,[Key],[Value])</p>
<p>I was also thinking of using the CounteVectorizer way (and for that using its vocabulary) but I run in the same problem there.</p>
<p>Anyone who can help?
This is the output so far:</p>
<pre><code>+-----+------------------------------------------------+----------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+
|label|words                                           |rawFeatures                                                     |features                                                                                                                                       |
+-----+------------------------------------------------+----------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+
|0    |[Hello, World, Spark, Is, Awesome, Hello, World]|(262144,[32755,44691,64441,179674,262052],[2.0,2.0,1.0,1.0,1.0])|(262144,[32755,44691,64441,179674,262052],[0.44628710262841953,0.44628710262841953,0.22314355131420976,0.5108256237659907,0.22314355131420976])|
|1    |[Hello, World, Spark, Is, Awesome, Hello, World]|(262144,[32755,44691,64441,179674,262052],[2.0,2.0,1.0,1.0,1.0])|(262144,[32755,44691,64441,179674,262052],[0.44628710262841953,0.44628710262841953,0.22314355131420976,0.5108256237659907,0.22314355131420976])|
|2    |[Hello, World]                                  |(262144,[32755,44691],[1.0,1.0])                                |(262144,[32755,44691],[0.22314355131420976,0.22314355131420976])                                                                               |
|3    |[PYTHON, Is, Pretty, Awesome]                   |(262144,[61511,64441,191247,262052],[1.0,1.0,1.0,1.0])          |(262144,[61511,64441,191247,262052],[0.9162907318741551,0.22314355131420976,0.9162907318741551,0.22314355131420976])                           |
+-----+------------------------------------------------+----------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+
</code></pre>
",Vectorization & Embeddings,pyspark display top word document quite new pyspark tfidf processing dataframe following code want store top word tfidf value separate column since really used working vector little stuck achieve know somehow need apply indexof function every token document find mapping value know far understood vector built like size key value wa also thinking using countevectorizer way using vocabulary run problem anyone help output far
an error in Recurrent Neural Network(LSTM) for tweet classification in Python,"<p>I am trying to improve the result by the LSTM. in part of my project I did the following for RNN:</p>
<p>Below is a quick method used to train the models:</p>
<pre><code>    def threshold_search(y_true, y_proba, average = None):
        best_threshold = 0
        best_score = 0
        for threshold in [i * 0.01 for i in range(100)]:
            score = f1_score(y_true=y_true, y_pred=y_proba &gt; threshold, average=average)
            if score &gt; best_score:
                best_threshold = threshold
                best_score = score
        search_result = {'threshold': best_threshold, 'f1': best_score}
        return search_result
    def train(model, 
              X_train, y_train, X_test, y_test, 
              checkpoint_path='model.hdf5', 
              epcohs = 25, 
              batch_size = DEFAULT_BATCH_SIZE, 
              class_weights = None, 
              fit_verbose=2,
              print_summary = True
             ):
        m = model()
        if print_summary:
            print(m.summary())
        m.fit(
            X_train, 
            y_train, 
            #this is bad practice using test data for validation, in a real case would use a seperate validation set
            validation_data=(X_test, y_test),
            epochs=epcohs, 
            batch_size=batch_size,
            class_weight=class_weights,
             #saves the most accurate model, usually you would save the one with the lowest loss
            callbacks= [
                ModelCheckpoint(checkpoint_path, monitor='val_acc', verbose=1, save_best_only=True),
                EarlyStopping(patience = 2)
            ],
            verbose=fit_verbose
        ) 
        print(&quot;\n\n****************************\n\n&quot;)
        print('Loading Best Model...')
        m.load_weights(checkpoint_path)
        predictions = m.predict(X_test, verbose=1)
        print('Validation Loss:', log_loss(y_test, predictions))
        print('Test Accuracy', (predictions.argmax(axis = 1) == y_test.argmax(axis = 1)).mean())
        print('F1 Score:', f1_score(y_test.argmax(axis = 1), predictions.argmax(axis = 1), average='weighted'))
        plot_confusion_matrix(y_test.argmax(axis = 1), predictions.argmax(axis = 1), classes=encoder.classes_)
        plt.show()    
        return m #returns best performing model
</code></pre>
<p>and then I used the simple implementation of an LSTM. Where The layers are as follows:</p>
<ul>
<li>Embedding: Matrix of Word Vectors, where each vector store the
&quot;meaning&quot; of the word. These can be trained on the fly or by existing
pre-trained vector.</li>
<li>LSTM: RNN that allows for the &quot;building&quot; of
state over time</li>
<li>Dense(64): Feed Forward Neural Network used to
interpret the LSTM Output</li>
<li>Dense(3): This it the output of the model,
3 nodes corresponding to each class. The softmax output will ensure
that the sum of values = 1.0 for each output.</li>
</ul>
<pre><code>def model_1():
    model = Sequential()
    model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))
    model.add(LSTM(128))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(3, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

m1 = train(model_1, 
           train_text_vec,
           y_train,
           test_text_vec,
           y_test,
           checkpoint_path='model_1.h5',
           class_weights= model.any(cws))
</code></pre>
<p>But I got the following output and error:</p>
<p><a href=""https://i.sstatic.net/2GFli.png"" rel=""nofollow noreferrer"">Screenshot of the error</a></p>
<p>As you can see in the screenshot, the error is:</p>
<blockquote>
<p>ValueError: The truth value of an array with more than one element is
ambiguous. Use a.any() or a.all()</p>
</blockquote>
<p>Would you please help me to solve this error?</p>
",Vectorization & Embeddings,error recurrent neural network lstm tweet classification python trying improve result lstm part project following rnn quick method used train model used simple implementation lstm layer follows embedding matrix word vector vector store meaning word trained fly existing pre trained vector lstm rnn allows building state time dense feed forward neural network used interpret lstm output dense output model node corresponding class softmax output ensure sum value output got following output error screenshot error see screenshot error valueerror truth value array one element ambiguous use would please help solve error
Dealing with NaN in BERT for Multi-class Text Classification of unlabelled text,"<p>I am just starting to explore Bert in a multiclass text classification task. In doing that, I am using this <a href=""https://drive.google.com/file/d/1gulUflDeftjTB0oFV_A4_ahAjwcg7nQi/view?usp=sharing"" rel=""nofollow noreferrer"">data</a>. This is my <a href=""https://drive.google.com/file/d/1uhwL-qjFpFBPsCGWTM3C7XwYok7Bw70c/view?usp=sharing"" rel=""nofollow noreferrer"">code</a>. During model testing, attempt to compute cosine similarities:</p>
<pre><code>#--- Model Algorithm ---#
## compute cosine similarities
similarities = np.array(
            [metrics.pairwise.cosine_similarity(X, y).T.tolist()[0] 
             for y in dic_y.values()]
            ).T
</code></pre>
<p>gives this error:</p>
<pre><code>ValueError: Input contains NaN, infinity or a value too large for dtype('float32').
</code></pre>
<p>From many stackoverflow posts, there were suggestions to eliminate <code>NaN</code> and null cells which I did at the beginning (see my <a href=""https://drive.google.com/file/d/1uhwL-qjFpFBPsCGWTM3C7XwYok7Bw70c/view?usp=sharing"" rel=""nofollow noreferrer"">code</a>). There were also suggestions to replace <code>NaN</code> but I when I did as thus:</p>
<pre><code>X = np.nan_to_num(X.astype(np.float32))
similarities = np.array(
                [metrics.pairwise.cosine_similarity(X, y).T.tolist()[0] 
                 for y in dic_y.values()]
                ).T
</code></pre>
<p>I got another error:</p>
<pre><code>ValueError: Expected 2D array, got 1D array instead:
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
</code></pre>
<p>Applying <code>.reshape(-1, 1)</code> or <code>.reshape(1, -1)</code> to <code>X</code>:</p>
<pre><code>X = np.reshape(X, (-1, 1))
    similarities = np.array(
                    [metrics.pairwise.cosine_similarity(X, y).T.tolist()[0] 
                     for y in dic_y.values()]
                    ).T
</code></pre>
<p>generates the same error:</p>
<pre><code>ValueError: Expected 2D array, got 1D array instead:
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
</code></pre>
<p>I am sure that the input <code>X</code> is a 2D array and not 1D, because <code>X.ndim</code> is <code>2</code>. Any help will be appreciated.</p>
",Vectorization & Embeddings,dealing nan bert multi class text classification unlabelled text starting explore bert multiclass text classification task using data code model testing attempt compute cosine similarity give error many stackoverflow post suggestion eliminate null cell beginning see code also suggestion replace thus got another error applying generates error sure input array help appreciated
Semantic Search fine-tune,"<p>eg. Pre-Trained BERT Result for sentence cosine similarity</p>
<pre><code>======================

Query: milk with chocolate flavor

Top 10 most similar sentences in corpus:
Milka milk chocolate 100 g (Score: 0.8672)
Alpro, Chocolate soy drink 1 ltr (Score: 0.6821)
Danone, HiPRO 25g Protein chocolate flavor 330 ml (Score: 0.6692)
</code></pre>
<p>in the above example, I am searching for milk the result should be milk-related first but here it returns chocolate in the first place. how do I fine-tune similarity for the result?</p>
<p>I googled it but do not found any proper solution, please help me.</p>
<p><strong>Code:</strong></p>
<pre><code>import scipy
import numpy as np
from sentence_transformers import models, SentenceTransformer
model = SentenceTransformer('distilbert-base-multilingual-cased')

corpus = [
          &quot;Alpro, Chocolate soy drink 1 ltr&quot;,
          &quot;Milka milk chocolate 100 g&quot;,
          &quot;Danone, HiPRO 25g Protein chocolate flavor 330 ml&quot;
         ]
corpus_embeddings = model.encode(corpus)

queries = [
            'milk with chocolate flavor',
          ]
query_embeddings = model.encode(queries)

# Calculate Cosine similarity of query against each sentence i
closest_n = 10
for query, query_embedding in zip(queries, query_embeddings):
    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, &quot;cosine&quot;)[0]

    results = zip(range(len(distances)), distances)
    results = sorted(results, key=lambda x: x[1])

    print(&quot;\n======================\n&quot;)
    print(&quot;Query:&quot;, query)
    print(&quot;\nTop 10 most similar sentences in corpus:&quot;)

    for idx, distance in results[0:closest_n]:
        print(corpus[idx].strip(), &quot;(Score: %.4f)&quot; % (1-distance))
</code></pre>
",Vectorization & Embeddings,semantic search fine tune eg pre trained bert result sentence cosine similarity example searching milk result milk related first return chocolate first place fine tune similarity result googled found proper solution please help code
ValueError: Input has n_features=12 while the model has been trained with n_features=2494,"<p>I have trained a model using count_vectorizer, Tfidf_transformer and sgd classifier.</p>
<p>This is the tokenizer part</p>
<pre><code>from keras.preprocessing.text import Tokenizer
# The maximum number of words to be used. (most frequent)
MAX_NB_WORDS = 50000
# Max number of words in each complaint.
MAX_SEQUENCE_LENGTH = 250
# This is fixed.
EMBEDDING_DIM = 100
tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(master_df['Observation'].values)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))
</code></pre>
<p>I trained the model</p>
<pre><code>from sklearn.linear_model import SGDClassifier
cv=CountVectorizer(max_df=1.0,min_df=1, stop_words=stop_words, max_features=10000, ngram_range=(1,3))
X=cv.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42, stratify=y)
sgd = Pipeline([('tfidf', TfidfTransformer()),
                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),
               ])
sgd.fit(X_train, y_train)


y_pred = sgd.predict(X_test)

print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred,target_names=my_tags))
</code></pre>
<p>This part works fine
When I try to use this model to predict using this code</p>
<pre><code>sentence=&quot;Drill was not in operation in the mine at the time of visit.&quot;
test=preprocess_text(sentence)
test=test.lower()
print(test)
test=[test] 
tokenizer.fit_on_texts(test)
word_index = tokenizer.word_index
#print(word_index)
test1=cv.transform(test)
print(test1)
output=sgd.predict(test1)
output
</code></pre>
<p>It gives me this error.</p>
<pre><code>ValueError: Input has n_features=12 while the model has been trained with n_features=2494
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_18044/596445027.py in &lt;module&gt;
      9 test1=cv.fit_transform(test)
     10 print(test1)
---&gt; 11 output=sgd.predict(test1)
     12 output

~\AppData\Local\Programs\Python\Python38\lib\site-packages\sklearn\utils\metaestimators.py in &lt;lambda&gt;(*args, **kwargs)
    118 
    119         # lambda, but not partial, allows help() to work with update_wrapper
--&gt; 120         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
    121         # update the docstring of the returned function
    122         update_wrapper(out, self.fn)

~\AppData\Local\Programs\Python\Python38\lib\site-packages\sklearn\pipeline.py in predict(self, X, **predict_params)
    416         Xt = X
    417         for _, name, transform in self._iter(with_final=False):
--&gt; 418             Xt = transform.transform(Xt)
    419         return self.steps[-1][-1].predict(Xt, **predict_params)
    420 

~\AppData\Local\Programs\Python\Python38\lib\site-packages\sklearn\feature_extraction\text.py in transform(self, X, copy)
   1491             expected_n_features = self._idf_diag.shape[0]
   1492             if n_features != expected_n_features:
-&gt; 1493                 raise ValueError(&quot;Input has n_features=%d while the model&quot;
   1494                                  &quot; has been trained with n_features=%d&quot; % (
   1495                                      n_features, expected_n_features))

ValueError: Input has n_features=12 while the model has been trained with n_features=2494
</code></pre>
<p>I think the problem lies in <code>word_index=tokenizer</code> line but I dont know how to rectify it.</p>
",Vectorization & Embeddings,valueerror input ha n feature model ha trained n feature trained model using count vectorizer tfidf transformer sgd classifier tokenizer part trained model part work fine try use model predict using code give error think problem lie line dont know rectify
Inputs of a transformer model,"<p>I am trying to understand the transformer model. Please consider my below example and help me to understand the concept.</p>
<p>Example: English to french conversion</p>
<p>My questions:</p>
<ol>
<li><p>Is the input word embedding is an English- french pretrained embedding?</p>
</li>
<li><p>In which step of the decoder the prediction of a french word is happening?</p>
</li>
<li><p>Is output embedding in decoder is just decoder's output till predicted, if so why should I mask the next word since that is unknown to me as I still yet not passed as output</p>
</li>
</ol>
<p>Please clarify me this doubt</p>
<p>I also referred to these links:</p>
<ul>
<li><a href=""https://datascience.stackexchange.com/questions/81727/what-would-be-the-target-input-for-transformer-decoder-during-test-phase"">https://datascience.stackexchange.com/questions/81727/what-would-be-the-target-input-for-transformer-decoder-during-test-phase</a></li>
<li><a href=""https://datascience.stackexchange.com/questions/51785/what-is-the-first-input-to-the-decoder-in-a-transformer-model"">https://datascience.stackexchange.com/questions/51785/what-is-the-first-input-to-the-decoder-in-a-transformer-model</a></li>
</ul>
",Vectorization & Embeddings,input transformer model trying understand transformer model please consider example help understand concept example english french conversion question input word embedding english french pretrained embedding step decoder prediction french word happening output embedding decoder decoder output till predicted mask next word since unknown still yet passed output please clarify doubt also referred link
Why is my LSTM giving the same prediction for similar words in different sentences?,"<p>I have a corpus of sentences, where each sentence is 10 words long.
I want to build a neural net that will produce a binary classification for each word.</p>
<p>My approach has two phases. In the first phase I create character level embeddings and pass them in to an LSTM, to build a representation for each word.</p>
<p>In the second phase I pass the word representations for each sentence in to a second LSTM.</p>
<p>My model compiles and works but I am not sure it is doing what it is intended to do.</p>
<p>My main concern is that it gives the same prediction for similar words in different sentences. For instance for the following sentences: “cat sits on a mat”, “cat plays with a ball”, &quot;cat is a cute animal&quot; the word cat gets the same prediction, even though the sentences are different.</p>
<p>What am I doing wrong here? Your help and input is highly appreciated.</p>
<pre class=""lang-py prettyprint-override""><code>model = Sequential()

model.add(Embedding(
    input_dim=vocab_size,
    output_dim=char_embedding_size,
    input_length=input_length))

model.add(Bidirectional(LSTM(24, return_sequences=True)))
model.add(Flatten())
model.add(Dense(50, activation='tanh'))
model.add(Reshape((1, 50)))
model.add(Bidirectional(LSTM(24, return_sequences=True)))
model.add(Dense(1, activation='sigmoid'))
optimizer = Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
</code></pre>
<p>Please see the model architecture image:
<a href=""https://i.sstatic.net/wiA4I.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wiA4I.png"" alt=""enter image description here"" /></a></p>
",Vectorization & Embeddings,lstm giving prediction similar word different sentence corpus sentence sentence word long want build neural net produce binary classification word approach ha two phase first phase create character level embeddings pas lstm build representation word second phase pas word representation sentence second lstm model compiles work sure intended main concern give prediction similar word different sentence instance following sentence cat sits mat cat play ball cat cute animal word cat get prediction even though sentence different wrong help input highly appreciated please see model architecture image
Is BertTokenizer similar to word embedding?,"<p>The idea of using BertTokenizer from huggingface really confuses me.</p>
<ol>
<li><p>When I use</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
tokenizer.encode_plus(&quot;Hello&quot;)
</code></pre>
</li>
</ol>
<p>Does the result is somewhat similar to when I pass
a one-hot vector representing &quot;Hello&quot; to a learning embedding matrix?</p>
<ol start=""2"">
<li><p>How is</p>
<pre><code>BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;) 
</code></pre>
</li>
</ol>
<p>different from</p>
<pre><code>BertTokenizer.from_pretrained(&quot;bert-**large**-uncased&quot;) 
</code></pre>
<p>and other pretrained?</p>
",Vectorization & Embeddings,berttokenizer similar word embedding idea using berttokenizer huggingface really confuses use doe result somewhat similar pas one hot vector representing hello learning embedding matrix different pretrained
Fine-tuning Glove Embeddings,"<p>Has anyone tried to fine-tune <strong>Glove embeddings</strong> on a domain-specific corpus?<br>
<strong>Fine-tuning word2vec</strong> embeddings has proven very efficient for me in a various NLP tasks, but I am wondering whether generating a cooccurrence matrix on my domain-specific corpus, and training glove embeddings (initialized with pre-trained embeddings) on that corpus would generate similar improvements.</p>
",Vectorization & Embeddings,fine tuning glove embeddings ha anyone tried fine tune glove embeddings domain specific corpus fine tuning word vec embeddings ha proven efficient various nlp task wondering whether generating cooccurrence matrix domain specific corpus training glove embeddings initialized pre trained embeddings corpus would generate similar improvement
Finding cosine Similarity between 2 csr_matrices,"<p>I have 2 <code>csr_matrix</code>. I want to compute cosine similarity between each vector of first matrix with each vector of 2nd matrix. For this purpose I am firstly building full vector <strong>including zeros</strong> from 1st matrix and then finding dot product product with every <strong>full vector</strong> of 2nd matrix. Is there any other way of doing this ? If the question is not clear , please let me know, however, I tried my best to explain.</p>
",Vectorization & Embeddings,finding cosine similarity csr matrix want compute cosine similarity vector first matrix vector nd matrix purpose firstly building full vector including zero st matrix finding dot product product every full vector nd matrix way question clear please let know however tried best explain
calculate similarity between two lists of tags,"<p>How can I calculate the semantic similarity between two lists of tags?
For example:</p>
<p><strong>Input</strong></p>
<pre><code>list1 = ['marketing', 'social medial', 'operations', 'management']
list2 = ['software development', 'system network', 'system design']
</code></pre>
<p><strong>Ouput</strong></p>
<pre><code>5%
</code></pre>
<p>Are there any python packages/libraries I can use to do this?</p>
",Vectorization & Embeddings,calculate similarity two list tag calculate semantic similarity two list tag example input ouput python package library use
Numpy Array not Getting Converted to a Tensor - Tensorflow2,"<p>I'm trying to train a Neural Network for an NLP application in which I'm using a training set of 25000 examples. I pre-processed these examples into a feature vector of 25000 examples. I have only 3 features. I converted this feature vector into a numpy array using the general method</p>
<p>X = np.array(X)
y = np.array(y)</p>
<p>I now have a data set of shape (25000, 3). I compiled my model with input shape = (3, ) with my first layer being Embedding()</p>
<p>I passed a model fit query</p>
<pre><code>history = model.fit(X, y, validation_split= 0.2, epochs = 30, verbose = 1, callbacks = [earlystop, modelcheckpoint, lr_red], 
                    batch_size = 256, shuffle = True
                    )
</code></pre>
<p>However, it gives me the error that</p>
<pre><code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type int).
</code></pre>
<p>Even when I typed</p>
<pre><code>print(X[0].shape)
</code></pre>
<p>It gave me (3, ), which is what the model should expect right?</p>
<p>Please help me to solve this issue. I really don't know where to even start fixing this.</p>
",Vectorization & Embeddings,numpy array getting converted tensor tensorflow trying train neural network nlp application using training set example pre processed example feature vector example feature converted feature vector numpy array using general method x np array x np array data set shape compiled model input shape first layer embedding passed model fit query however give error even typed gave model expect right please help solve issue really know even start fixing
Workflow of NLP,"<p>When should I perform preprocessing and matrix creation of text data in NLP, before or after <code>train_test_split</code>? Below is my sample code where I have done preprocessing and matrix creation (tfidf) before <code>train_test_split</code>. I want to know will there be data leakage?</p>
<pre><code>corpus = []

for i in range(0 ,len(data1)):
    review = re.sub('[^a-zA-Z]', ' ', data1['features'][i])
    review = review.lower()
    review = review.split()
    review = [stemmer.stem(j) for j in review if not j in set(stopwords.words('english'))]
    review = ' '.join(review)
    corpus.append(review)

from sklearn.feature_extraction.text import TfidfVectorizer
cv = TfidfVectorizer(max_features = 6000)
x = cv.fit_transform(corpus).toarray()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(data1['label'])

from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = 69, 
                                                                                stratify = y)

spam_model = MultinomialNB().fit(train_x, train_y)
pred = spam_model.predict(test_x)
c_matrix = confusion_matrix(test_y, pred)
acc_score = accuracy_score(test_y, pred)
</code></pre>
",Vectorization & Embeddings,workflow nlp perform preprocessing matrix creation text data nlp sample code done preprocessing matrix creation tfidf want know data leakage
Python: Variable size changed after the return statement,"<p>I am trying to use word2vec embedding for a text classification task. However, it is strange that the value returned from the <code>preprocess()</code> function is different from the moment just before it was returned. Does anyone know what is wrong with my code?</p>
<pre><code>train_data = [   {'corrected': 'have a good day', 'father': 1},
         {'corrected': 'i suggest you see this movie', 'father': 1},
         {'corrected': 'The afternoon grew so glowering that in the sixth inning the arc lights were turned on--always a wan sight in the daytime, like the burning headlights of a funeral procession. Aided by the gloom, Fisher was slicing through the Sox rookies, and Williams did not come to bat in the seventh. He was second up in the eighth. This was almost certainly his last time to come to the plate in Fenway Park, and instead of merely cheering, as we had at his three previous appearances, we stood, all of us, and applauded.', 'father': 2},
         {'corrected': 'worse than any show', 'father': 1},
         {'corrected': 'nice movie, so love it', 'father': 2},
         {'corrected': &quot;The day I picked my dog up from the pound was one of the happiest days of both of our lives. I had gone to the pound just a week earlier with the idea that I would just 'look' at a puppy. Of course, you can no more just look at those squiggling little faces so filled with hope and joy than you can stop the sun from setting in the evening. I knew within minutes of walking in the door that I would get a puppy… but it wasn't until I saw him that I knew I had found my puppy&quot;, 'father': 2}
    ]

train_data= pd.DataFrame(train_data)
</code></pre>
<pre><code># Load Pretrained Word2Vec
embed = hub.load(&quot;https://tfhub.dev/google/Wiki-words-250/2&quot;)

def get_word_count(essay):
    &quot;&quot;&quot;
    get the number of vocab in the essay
    &quot;&quot;&quot;
    return len(essay)

def get_word2vec_enc(essays):
    &quot;&quot;&quot;
    get word2vec value for each word in sentence.
    concatenate word in numpy array, so we can use it as RNN input
    &quot;&quot;&quot;
    encoded = []
    for essay in essays:
        tokens = essay.split(&quot; &quot;)
        word2vec_embedding = embed(tokens)
        encoded.append(word2vec_embedding)
    return encoded

def get_padded_encoded_essays(encoded_essays):
    &quot;&quot;&quot;
    for short essays, we prepend zero padding so all input to RNN has same length,
    for long essays, we truncate it to the first 250 words
    &quot;&quot;&quot;
    padded_essays_encoding = []
    for enc_essay in encoded_essays:
        if get_word_count(enc_essay)&gt; 250:
            enc_essay[:249]
        
        else:
            zero_padding_cnt = 250 - enc_essay.shape[0]
            pad = np.zeros((1, 250))
            for i in range(zero_padding_cnt):
                enc_essay = np.concatenate((pad, enc_essay), axis=0)
        padded_essays_encoding.append(enc_essay)
    return padded_essays_encoding

def ses_encode(ses):
    &quot;&quot;&quot;
    return one hot encoding for Y value
    &quot;&quot;&quot;
    if ses == 1: 
        return [1,0]  # for high ses
    else: 
        return [0,1]  # for low ses
    
def preprocess(df):
    &quot;&quot;&quot;
    encode text value to numeric value
    &quot;&quot;&quot;
 
    # encode words into word2vec
    essays = df['corrected'].tolist()
    print(&quot;essay length:&quot; + str(len(essays)))

    
    encoded_essays = get_word2vec_enc(essays)
    padded_encoded_essays = get_padded_encoded_essays(encoded_essays)
    print(&quot;padded_encoded_essays length:&quot; + str(len(padded_encoded_essays)))
    
    # encoded ses
    sess = df['father'].tolist()

    encoded_ses = [ses_encode(ses) for ses in sess]
    X = np.vstack(padded_encoded_essays)
    print(&quot;X length:&quot; + str(len(X)))
    Y = np.vstack(encoded_ses)

    return X, Y
</code></pre>
<pre><code>train_X, train_Y = preprocess(train_data)

len(train_X) # it returns 1500

len(train_Y) # it returns 6

</code></pre>
<p>When I call <code>train_X, train_Y = preprocess(train_data)</code>, three print statements are &quot;essay length:6;
padded_encoded_essays length:6;
X length:1500&quot;. I don't know why <code>np.vstack()</code> changes the size. Is there a way keep the size same while let the code working without warnings (when I did not include <code>np.vstack()</code> , my code had <a href=""https://stackoverflow.com/questions/68815664/python-failed-to-convert-a-numpy-array-to-a-tensor"">another problem</a>)?</p>
<p>Thank you in advance</p>
",Vectorization & Embeddings,python variable size changed return statement trying use word vec embedding text classification task however strange value returned function different moment wa returned doe anyone know wrong code call three print statement essay length padded encoded essay length x length know change size way keep size let code working without warning include code href problem thank advance
R: how to combine Word2Vec Embedding and LSTM Network,"<p>I plan to use Word2Vec (Skip-gram) and LSTM for text classification. For the code, I referred to <a href=""https://rpubs.com/nabiilahardini/word2vec"" rel=""nofollow noreferrer"">Word Embeddings with Keras
</a> for and <a href=""https://shirinsplayground.netlify.app/2019/01/text_classification_keras_data_prep/"" rel=""nofollow noreferrer"">How to prepare data for NLP (text classification) with Keras and TensorFlow</a>. However, I am not sure how to combine these two steps.</p>
<p>Currently, I have the following code. Code in the first chunk, I assume, would produce an embedding matrix that I can use for the text classification later.</p>
<pre><code>#clean textual data 
essay &lt;- tolower(data$corrected) %&gt;%
  text_clean() # removing punctionations, stop words, spaces etc. 

tokenizer &lt;- text_tokenizer(num_words = max_features)

tokenizer%&gt;%
  fit_text_tokenizer(essay)

skipgrams_generator &lt;- function(text, tokenizer, window_size, negative_samples) {
  
  gen &lt;- texts_to_sequences_generator(tokenizer, sample(text))
  
  function() {
    skip &lt;- generator_next(gen) %&gt;%
      skipgrams(
        vocabulary_size = tokenizer$num_words, 
        window_size = window_size, 
        negative_samples = 1
      )
    
    x &lt;- transpose(skip$couples) %&gt;% map(. %&gt;% unlist %&gt;% as.matrix(ncol = 1))
    y &lt;- skip$labels %&gt;% as.matrix(ncol = 1)
    
    list(x, y)
  }
}

# determine model tuning inputs
embedding_size &lt;- 256  # dimension of embedding vector (explianation of how to decide the embedding size https://stackoverflow.com/questions/48479915/what-is-the-preferred-ratio-between-the-vocabulary-size-and-embedding-dimension)
skip_window &lt;- 5       # number of skip-gram
num_sampled &lt;- 2       # number of negative sample for each word (https://stats.stackexchange.com/questions/244616/how-does-negative-sampling-work-in-word2vec)

input_target &lt;- layer_input(shape = 1)
input_context &lt;- layer_input(shape = 1)

embedding &lt;- layer_embedding(
  input_dim = tokenizer$num_words + 1, 
  output_dim = embedding_size, 
  input_length = 1, 
  name = &quot;embedding&quot;
)


target_vector &lt;- input_target %&gt;% 
  embedding() %&gt;% 
  layer_flatten() # to return the dimension of the input

context_vector &lt;- input_context %&gt;%
  embedding() %&gt;%
  layer_flatten()

dot_product &lt;- layer_dot(list(target_vector, context_vector), axes = 1)

output &lt;- layer_dense(dot_product, units = 1, activation = &quot;sigmoid&quot;)

model &lt;- keras_model(list(input_target, input_context), output)
model %&gt;% compile(loss = &quot;binary_crossentropy&quot;, optimizer = &quot;adam&quot;)

#Model Training 
model %&gt;%
  fit_generator(
    skipgrams_generator(essay, tokenizer, skip_window, negative_samples),
    steps_per_epoch = 100, epochs = 30
    )

#Obtaining Weights for Word Embeddings
embedding_matrix &lt;- get_weights(model)[[1]]

words &lt;-data_frame(
  word = names(tokenizer$word_index), 
  id = as.integer(unlist(tokenizer$word_index))
)

words &lt;- words %&gt;%
  filter(id &lt;= tokenizer$num_words) %&gt;%
  arrange(id)

row.names(embedding_matrix) &lt;- c(&quot;UNK&quot;, words$word)

dim(embedding_matrix)
</code></pre>
<p>Then, I hope to use this embedding matrix in the LSTM model.</p>
<pre><code>text_seqs &lt;- texts_to_sequences(tokenizer, essay)
text_seqs &lt;- pad_sequences(text_seqs, maxlen = 400)

embedding_dims &lt;- 300
filters &lt;- 64 
kernel_size &lt;- 3 
hidden_dims &lt;- 50
epochs &lt;- 10
maxlen &lt;- 400
batch_size &lt;- 500

model &lt;- keras_model_sequential()%&gt;%
  layer_embedding(input_dim = max_features, output_dim = 128, weights = embedding_matrix) %&gt;%  # I attempted to add weights here
  layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %&gt;% 
  layer_dense(units = 1, activation = 'sigmoid')%&gt;% 
  
  compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)
</code></pre>
<p>But the way I combined them is wrong as it shows an error message:</p>
<blockquote>
<p>Error in py_call_impl(callable, dots$args, dots$keywords) :
ValueError: You called <code>set_weights(weights)</code> on layer &quot;embedding_1&quot; with a weight list of length 1001, but the layer was expecting 1 weights. Provided weights: [[ 0.01752407 -0.03668756  0.00466535 ...  0.03698...</p>
</blockquote>
<p>Does anyone know how to properly use the embedding matrix? Thank you in advance for your help.</p>
",Vectorization & Embeddings,r combine word vec embedding lstm network plan use word vec skip gram lstm text classification code referred word embeddings kera prepare data nlp text classification kera tensorflow however sure combine two step currently following code code first chunk assume would produce embedding matrix use text classification later hope use embedding matrix lstm model way combined wrong show error message error py call impl callable dot args dot keywords valueerror called layer embedding weight list length layer wa expecting weight provided weight doe anyone know properly use embedding matrix thank advance help
How to use multiple text features for NLP classifier?,"<p>I am trying to build text classifier, Usually, we have one text column and ground truth. But I am working on a problem where dataset contains many text features. I am exploring different ways how to make use of different text features.</p>

<p>For example, my dataset looks like this </p>

<pre><code>Index_no                   domain  comment_by   comment       research_paper      books_name

01                         Science  Professor   Thesis needs  Evolution of         MOIRCS 
                                                more work     Quiescent            Deep 
                                                              Galaxies as a        Survey
                                                              Function of
                                                              Stellar Mass       



02                         Math    Professor   Doesn't follow  Evolution of   
                                               Latex format   Quiescent           nonlinear 
                                                              Galaxies as a       dispersive
                                                              Function of         equations
                                                              Stellar Mass             
</code></pre>

<p>This is just a dummy dataset, Here my ground truth (Y) is domain and features are <code>comment_by</code>, <code>comment</code>, <code>research_paper</code>, <code>books_name</code></p>

<p>If I am using any NLP model (RNN-LSTM, Transformers etc), those models usually take one 3 dim vectors, for that if I am using one text column that works but How to many text features for text classifier? </p>

<p>What I've tried :</p>

<blockquote>
  <p>1) <strong>Joining all column and making a long string</strong></p>
</blockquote>

<p>Professor Thesis needs more work Evolution of Quiescent Galaxies as a Function of Stellar Mass MOIRCS Deep Survey  </p>

<blockquote>
  <p>2) <strong>Using a token between columns</strong></p>
</blockquote>

<pre><code>&lt;CB&gt; Professor &lt;C&gt; Thesis needs more work &lt;R&gt; Evolution of Quiescent Galaxies as a Function of Stellar Mass &lt;B&gt; MOIRCS Deep Survey 
</code></pre>

<p>where <code>&lt;CB&gt;</code> comment_by , <code>&lt;C&gt;</code> comment, <code>&lt;R&gt;</code> research_paper, <code>&lt;B&gt;</code> books_name</p>

<p>Should I use <code>&lt;CB&gt;</code> at the beginning or use like this?</p>

<pre><code>Professor &lt;1&gt; Thesis needs more work &lt;2&gt; Evolution of Quiescent Galaxies as a Function of Stellar Mass &lt;3&gt; MOIRCS Deep Survey
</code></pre>

<blockquote>
  <p>3) <strong>Using different dense layers (or embedding) for each column and
  concatenate them.</strong></p>
</blockquote>

<p>I've tried all three approaches, Is there any other approach I can try to improve the model accuracy? or extract, combine, join the better features?</p>

<p>Thanks in advance!</p>
",Vectorization & Embeddings,use multiple text feature nlp classifier trying build text classifier usually one text column ground truth working problem dataset contains many text feature exploring different way make use different text feature example dataset look like dummy dataset ground truth domain feature using nlp model rnn lstm transformer etc model usually take one dim vector using one text column work many text feature text classifier tried joining column making long string professor thesis need work evolution quiescent galaxy function stellar mass moircs deep survey using token column comment comment research paper book name use beginning use like using different dense layer embedding column concatenate tried three approach approach try improve model accuracy extract combine join better feature thanks advance
Extracting embedding values of NLP pertained models from tokenized strings,"<p>I am using <a href=""https://huggingface.co/transformers/pretrained_models.html"" rel=""nofollow noreferrer"">huggingface</a> pipeline to extract embeddings of words in a sentence. As far as I know, first a sentence will be turned into a tokenized strings. I think the length of the tokenized string might not be equal to the number of words in the original sentence. I need to retrieve word embedding of a particular sentence.</p>
<p>For example, here is my code:</p>
<pre><code>#https://discuss.huggingface.co/t/extracting-token-embeddings-from-pretrained-language-models/6834/6

from transformers import pipeline, AutoTokenizer, AutoModel
import numpy as np
import re

model_name = &quot;xlnet-base-cased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

tokenizer.add_special_tokens({'pad_token': '[PAD]'})
model.resize_token_embeddings(len(tokenizer))

model_pipeline = pipeline('feature-extraction', model=model_name, tokenizer=tokenizer)

def find_wordNo_sentence(word, sentence):
    
    print(sentence)
    splitted_sen = sentence.split(&quot; &quot;)
    print(splitted_sen)
    index = splitted_sen.index(word)


    for i,w in enumerate(splitted_sen):
        if(word == w):
            return i

    print(&quot;not found&quot;) #0 base




  
def return_xlnet_embedding(word, sentence):
        
    word = re.sub(r'[^\w]', &quot; &quot;, word)
    word = &quot; &quot;.join(word.split())
    
    sentence = re.sub(r'[^\w]', ' ', sentence)
    sentence = &quot; &quot;.join(sentence.split())
    
    id_word = find_wordNo_sentence(word, sentence)
    
   
        
    try:
        data = model_pipeline(sentence)
        
        n_words = len(sentence.split(&quot; &quot;))
        #print(sentence_emb.shape)
        n_embs  = len(data[0])
        print(n_embs, n_words)
        print(len(data[0]))
    
        if (n_words != n_embs):
            &quot;There is extra tokenized word&quot;
            
            
        results = data[0][id_word]  
        return np.array(results)
    
    except:
        return &quot;word not found&quot;

return_xlnet_embedding('your', &quot;what is your name?&quot;)
</code></pre>
<p>Then the output is:</p>
<blockquote>
<p>what is your name ['what', 'is', 'your', 'name'] 6 4 6</p>
</blockquote>
<p>So the length of tokenized string that is fed to the pipeline is two more than number of my words.
How can I find which one (among these 6 values) are the embedding of my word?</p>
",Vectorization & Embeddings,extracting embedding value nlp pertained model tokenized string using huggingface pipeline extract embeddings word sentence far know first sentence turned tokenized string think length tokenized string might equal number word original sentence need retrieve word embedding particular sentence example code output name name length tokenized string fed pipeline two number word find one among value embedding word
Looping cosine similarity formula from one dataframe to another dataframe using pandas &amp; BERT,"<p>I am building a NLP project which compares sentence similarities between two different dataframes. Here is an example of the dataframes:</p>
<pre><code>df = pd.DataFrame({'Element Detail':['Too many competitors in market', 'Highly skilled employees']})
df1 = pd.DataFrame({'Element Details':['Our workers have a lot of talent', 
                                      'this too is a sentence',
                                      'this is very different',
                                      'another sentence is this',
                                      'not much of anything']
                    })
</code></pre>
<p>I currently have the code set up in a way that it compares the first cell in df with all the cells in df1. It then picks the highest cosine similarity score and puts that in a separate dataframe with the following code:</p>
<pre><code>import pandas as pd
import numpy as np

model_name = 'bert-base-nli-mean-tokens'
from sentence_transformers import SentenceTransformer
model = SentenceTransformer(model_name)
sentence_vecs = model.encode(df['Element Detail'])
sentence_vecs1 = model.encode(df1['Element Details'])

from sklearn.metrics.pairwise import cosine_similarity

new = cosine_similarity(
    [sentence_vecs[0]],
    sentence_vecs1[0:]
)

d = pd.DataFrame(new)
T =pd.DataFrame.transpose(d)
df_new = T.insert(0, 'New_ID', range(1, 1 + len(T)))
Tnew = (T.add_prefix('X'))
Final = (Tnew[Tnew.X0 == Tnew.X0.max()])
</code></pre>
<p>The end product is this dataframe:</p>
<pre><code>    XNew_ID     X0  
0   1           0.615005 
</code></pre>
<p>How can I write a piece of code so it will loop through the rest of the elements in df and write the to the 'Final' dataframe in the same manner?</p>
",Vectorization & Embeddings,looping cosine similarity formula one dataframe another dataframe using panda bert building nlp project compare sentence similarity two different dataframes example dataframes currently code set way compare first cell df cell df pick highest cosine similarity score put separate dataframe following code end product dataframe write piece code loop rest element df write final dataframe manner
How to increase dimension-vector size of BERT sentence-transformers embedding,"<p>I am using sentence-transformers for semantic search but sometimes it does not understand the contextual meaning and returns wrong result
eg. <a href=""https://stackoverflow.com/questions/68627093/bert-problem-with-context-semantic-search-in-italian-language"">BERT problem with context/semantic search in italian language</a></p>
<p>by default the vector side of embedding of the sentence is 78 columns, so how do I increase that dimension so that it can understand the contextual meaning in deep.</p>
<p><strong>code:</strong></p>
<pre><code># Load the BERT Model
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('bert-base-nli-mean-tokens')

# Setup a Corpus
# A corpus is a list with documents split by sentences.

sentences = ['Absence of sanity', 
             'Lack of saneness',
             'A man is eating food.',
             'A man is eating a piece of bread.',
             'The girl is carrying a baby.',
             'A man is riding a horse.',
             'A woman is playing violin.',
             'Two men pushed carts through the woods.',
             'A man is riding a white horse on an enclosed ground.',
             'A monkey is playing drums.',
             'A cheetah is running behind its prey.']

# Each sentence is encoded as a 1-D vector with 78 columns 
sentence_embeddings = model.encode(sentences) ### how to increase vector dimention 

print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))

print('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])
</code></pre>
",Vectorization & Embeddings,increase dimension vector size bert sentence transformer embedding using sentence transformer semantic search sometimes doe understand contextual meaning return wrong result eg href problem context semantic search italian language default vector side embedding sentence column increase dimension understand contextual meaning deep code
How to fine tune sentence-transformers to understand the semantic similarity,"<p>I am using BERT model for context search in the Italian language but it does not understand the contextual meaning of the sentence and returns the wrong result.</p>
<p>in the below example code when I compare &quot;milk with chocolate flavor&quot; with two other types of milk and one chocolate so it returns high similarity with chocolate. it should return high similarity with other kinds of milk.</p>
<p>can anyone suggest to me how to fine tune sentence-transformers so that it can understand the semantic meaning of the text and return similarity according to it?</p>
<p><strong>Code :</strong></p>
<pre><code>!python -m spacy download it_core_news_lg
!pip install sentence-transformers


import scipy
import numpy as np
from sentence_transformers import models, SentenceTransformer
model = SentenceTransformer('distiluse-base-multilingual-cased') # workes with Arabic, Chinese, Dutch, English, French, German, Italian, Korean, Polish, Portuguese, Russian, Spanish, Turkish

corpus = [
          &quot;Alpro, Cioccolato bevanda a base di soia 1 ltr&quot;, #Alpro, Chocolate soy drink 1 ltr(soya milk)
          &quot;Milka  cioccolato al latte 100 g&quot;, #Milka milk chocolate 100 g
          &quot;Danone, HiPRO 25g Proteine gusto cioccolato 330 ml&quot;, #Danone, HiPRO 25g Protein chocolate flavor 330 ml(milk with chocolate flabor)
         ]
corpus_embeddings = model.encode(corpus)


queries = [
            'latte al cioccolato', #milk with chocolate flavor,
          ]
query_embeddings = model.encode(queries)


# Calculate Cosine similarity of query against each sentence i
closest_n = 10
for query, query_embedding in zip(queries, query_embeddings):
    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, &quot;cosine&quot;)[0]

    results = zip(range(len(distances)), distances)
    results = sorted(results, key=lambda x: x[1])

    print(&quot;\n======================\n&quot;)
    print(&quot;Query:&quot;, query)
    print(&quot;\nTop 10 most similar sentences in corpus:&quot;)

    for idx, distance in results[0:closest_n]:
        print(corpus[idx].strip(), &quot;(Score: %.4f)&quot; % (1-distance))
</code></pre>
<p><strong>Output :</strong></p>
<pre><code>======================

Query: latte al cioccolato

Top 10 most similar sentences in corpus:
Milka  cioccolato al latte 100 g (Score: 0.7714)
Alpro, Cioccolato bevanda a base di soia 1 ltr (Score: 0.5586)
Danone, HiPRO 25g Proteine gusto cioccolato 330 ml (Score: 0.4569)
</code></pre>
",Vectorization & Embeddings,fine tune sentence transformer understand semantic similarity using bert model context search italian language doe understand contextual meaning sentence return wrong result example code compare milk chocolate flavor two type milk one chocolate return high similarity chocolate return high similarity kind milk anyone suggest fine tune sentence transformer understand semantic meaning text return similarity according code output
Are names of intent important for the DIET Classifier in RASA NLU?,"<p>I am trying to implement a multilingual chatbot using RASA NLU. Concretly, I'm designing a separate chatbot for each language (with its own translated and adapted training data). However, it would convenient for the application side if the names of the intents could stay the same between all the chatbots. In that sense, my question is:</p>
<p>Is the name of the intent important for the intent classification when using DIET Classifier ? In other words, is the name of the intent used though the word embedding network or is it just a &quot;label&quot; that has no signification ?</p>
<p>Thank you for your help</p>
",Vectorization & Embeddings,name intent important diet classifier rasa nlu trying implement multilingual chatbot using rasa nlu concretly designing separate chatbot language translated adapted training data however would convenient application side name intent could stay chatbots sense question name intent important intent classification using diet classifier word name intent used though word embedding network label ha signification thank help
How to compare similarities between paragraphs NLP,"<p>I've been experimenting with NLP, and use the Doc2Vec model.</p>
<p>The aim of my objective, is a forum suggested question feature. For example, If a user types a question it will compare the vector to other questions already asked. So far this has worked ok in the sense of comparing a question to another asked question.</p>
<p>However, I would like to extend this to comparing the body of the question. For example, just like stackoverflow, I'm writing a the description to my question.</p>
<p>I understand that doc2vec represents sentences through paragraph ids. So for my question example I spoke about first, each sentence will be a unique paragraph id. However, with paraphs i.e the body to the question, sentences will have the same id as other sentences apart of the same paragraph.</p>
<pre class=""lang-py prettyprint-override""><code>para = 'This is a sentence. This is another sentence'
[['This','is','a','sentence',tag=[1]], ['This','is','another','sentence',tag=[1]]
</code></pre>
<p>I'm wondering how to go about doing this. How can i input a corpus like so:</p>
<pre class=""lang-py prettyprint-override""><code>['It is a nice day today. I wish I was outside in the sun. But I need to work.']
</code></pre>
<p>and compare that to another paragraph like this:</p>
<pre class=""lang-py prettyprint-override""><code>['It is a lovely day today. The sun is shining outside today. However, I am working.']
</code></pre>
<p>In which I would expect a very close similarity  between the two. Does similarity get calculated by sentence to sentence, rather then paragraph to paragraph? i.e.</p>
<pre class=""lang-py prettyprint-override""><code>cosine_sim(['It is a nice day today'],['It is a lovely day today.]
</code></pre>
<p>and do this for the other sentences and average out the similarity scores?</p>
<p>Thanks.</p>
<p><strong>EDIT</strong>
What I am confused about is using the above sentences, say the vectors are like so</p>
<pre class=""lang-py prettyprint-override""><code>sent1 = [0.23,0.1,0.33...n]
sent2 = [0.78,0.2,-0.6...n]
sent3 = [0.55,-0.5,0.9...n]

#Avergae out these vectors

para = [0.5,0.2,0.3...n]
</code></pre>
<p>and using this vector compare to another paragraph using the same process.</p>
",Vectorization & Embeddings,compare similarity paragraph nlp experimenting nlp use doc vec model aim objective forum suggested question feature example user type question compare vector question already asked far ha worked ok sense comparing question another asked question however would like extend comparing body question example like stackoverflow writing description question understand doc vec represents sentence paragraph id question example spoke first sentence unique paragraph id however paraph e body question sentence id sentence apart paragraph wondering go input corpus like compare another paragraph like would expect close similarity two doe similarity get calculated sentence sentence rather paragraph paragraph e sentence average similarity score thanks edit confused using sentence say vector like using vector compare another paragraph using process
BERT problem with context/semantic search in italian language,"<p>I am using BERT model for context search in Italian language but it does not understand the contextual meaning of the sentence and returns wrong result.</p>
<p>in below example code when I compare &quot;milk with chocolate flavour&quot; with two other type of milk and one chocolate so it returns high similarity with chocolate. it should return high similarity with other milks.</p>
<p>can anyone suggest me any improvement on the below code so that it can return semantic results?</p>
<p><strong>Code :</strong></p>
<pre><code>!python -m spacy download it_core_news_lg
!pip install sentence-transformers


import scipy
import numpy as np
from sentence_transformers import models, SentenceTransformer
model = SentenceTransformer('distiluse-base-multilingual-cased') # workes with Arabic, Chinese, Dutch, English, French, German, Italian, Korean, Polish, Portuguese, Russian, Spanish, Turkish

corpus = [
          &quot;Alpro, Cioccolato bevanda a base di soia 1 ltr&quot;, #Alpro, Chocolate soy drink 1 ltr(soya milk)
          &quot;Milka  cioccolato al latte 100 g&quot;, #Milka milk chocolate 100 g
          &quot;Danone, HiPRO 25g Proteine gusto cioccolato 330 ml&quot;, #Danone, HiPRO 25g Protein chocolate flavor 330 ml(milk with chocolate flabor)
         ]
corpus_embeddings = model.encode(corpus)


queries = [
            'latte al cioccolato', #milk with chocolate flavor,
          ]
query_embeddings = model.encode(queries)


# Calculate Cosine similarity of query against each sentence i
closest_n = 10
for query, query_embedding in zip(queries, query_embeddings):
    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, &quot;cosine&quot;)[0]

    results = zip(range(len(distances)), distances)
    results = sorted(results, key=lambda x: x[1])

    print(&quot;\n======================\n&quot;)
    print(&quot;Query:&quot;, query)
    print(&quot;\nTop 10 most similar sentences in corpus:&quot;)

    for idx, distance in results[0:closest_n]:
        print(corpus[idx].strip(), &quot;(Score: %.4f)&quot; % (1-distance))
</code></pre>
<p><strong>Output :</strong></p>
<pre><code>======================

Query: latte al cioccolato

Top 10 most similar sentences in corpus:
Milka  cioccolato al latte 100 g (Score: 0.7714)
Alpro, Cioccolato bevanda a base di soia 1 ltr (Score: 0.5586)
Danone, HiPRO 25g Proteine gusto cioccolato 330 ml (Score: 0.4569)
</code></pre>
",Vectorization & Embeddings,bert problem context semantic search italian language using bert model context search italian language doe understand contextual meaning sentence return wrong result example code compare milk chocolate flavour two type milk one chocolate return high similarity chocolate return high similarity milk anyone suggest improvement code return semantic result code output
How to get word2vec from google&#39;s pre-trained model,"<p>I want to fetch vector representation of words.
I tried to use GENSIM api but got the same error as here (for Python 3.6):
<a href=""https://stackoverflow.com/questions/64425652/valueerror-when-downloading-gensim-data-set"">ValueError when downloading gensim data set</a></p>
<p>What is the best way to get the vector out of the pre-trained model?</p>
",Vectorization & Embeddings,get word vec google pre trained model want fetch vector representation word tried use gensim api got error python href downloading gensim data set best way get vector pre trained model
Doc2Vec model not producing expected similarity scores,"<p>I'm trying to compare two sentences and get the cosine similarity between them.</p>
<p>I have about 50 sentences, and I used genism's pre-trained doc2vec and trained the model on these 50 sentences to just tweak the weights a little bit. However, the cosine similarity between two sentences is not truly reflecting the similarity. For example, sentence1 is not in English close to sentence2 but their embeddings are very similar.</p>
<p>My question is, how do I go about generally comparing 2 sentences for similarities (as doc2vec is not working for me). It seems to be due to the low amount of training inputs to tweak the weights, but I wonder if there is another technique to achieve this task.</p>
<p>e.g. rough implementation so far</p>
<pre class=""lang-py prettyprint-override""><code>s1 = &quot;This is a sentence&quot;
s2 = &quot;This is also a sentence&quot;
...
s50 =&quot;This is the last sentence

list = [s1,s2..s50]

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[
                                      str(i)]) for i, _d in enumerate(list)]
model = Doc2Vec(vector_size=vec_size,
                        alpha=alpha,
                        min_alpha=0.00025,
                        min_count=1,
                        dm=1)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
   print('iteration {0}'.format(epoch))
   model.train(tagged_data,
   total_examples=model.corpus_count,
   epochs=100)
   # decrease the learning rate
   model.alpha -= 0.0002
   # fix the learning rate, no decay
   model.min_alpha = model.alpha

</code></pre>
<p>I then loop through each sentence and perform <code>model.infer_vector(sent_tokens)</code> to get the embeddings. But as I said, they are not even close to being correct when using similarities.</p>
<p>If I am doing something wrong please let me know.</p>
",Vectorization & Embeddings,doc vec model producing expected similarity score trying compare two sentence get cosine similarity sentence used genism pre trained doc vec trained model sentence tweak weight little bit however cosine similarity two sentence truly similarity example sentence english close sentence embeddings similar question go generally comparing sentence similarity doc vec working seems due low amount training input tweak weight wonder another technique achieve task e g rough implementation far loop sentence perform get embeddings said even close correct using similarity something wrong please let know
Text summarization in a &quot;document&quot; without sentences,"<p>I have a non-linguistic corpus of ~100 &quot;documents&quot;, each comprising a sequence of ~10k &quot;words&quot; (i.e. I have a set of ~100 integer sequences). I can learn good doc2vec embeddings that respect known classes in the corpus. I'm now interested in summarizing these documents to help explain which motifs are not only representative of each document but also discriminative between classes.</p>
<p>I am primarily familiar with TextRank as an extractive summarization method, but this typically relies on sentences (i.e. subsequences that end with a period) as a sensible atom for the underlying node ranking algorithm. In my case, the sequence tokens are not known in advance as there are no sentences, per se.</p>
<p>Are there any summarization methods that take this into account? So far, I have tried using TextRank on all n-grams for a fixed n, but this precludes summaries involving tokens of different lengths, which happens to be crucial in my setting. Are there any multi-scale summarization methods, for instance?</p>
",Vectorization & Embeddings,text summarization document without sentence non linguistic corpus document comprising sequence k word e set integer sequence learn good doc vec embeddings respect known class corpus interested summarizing document help explain motif representative document also discriminative class primarily familiar textrank extractive summarization method typically relies sentence e subsequence end period sensible atom underlying node ranking algorithm case sequence token known advance sentence per se summarization method take account far tried using textrank n gram fixed n precludes summary involving token different length happens crucial setting multi scale summarization method instance
How do I create a Keywords column using extracted keywords in pandas?,"<p>I have a dataframe which has 3columns and more than 10k records. The Dataframe looks like this:</p>
<pre><code> _id  page_number                        pdf_name text_data
0  60f1530e3249c01351086b2f          1.0  a.pdf        This is my timee,eerwff..
1  60f1530e3249c01351086b30          2.0  a.pdf        how is it going, tbh you nee..
2  60f1530e3249c01351086b31          3.0  a.pdf        mark marquez is, going to..
3  60f1530e3249c01351086b32          4.0  b.pdf        coronavirus seems to be...
4  60f1530e3249c01351086b33          5.0  b.pdf        sports all over,has,lost..
</code></pre>
<p>I have used the Countvectorizer from sklearn alongwith tfidf_transformers to get the keywords. at present I am getting the keywords extracted but they're only coming out for the last sentence.
Here is my code which extracts and then gives me the keywords.</p>
<pre><code>for i in range(0, 10124):
    # Remove punctuations
    text = re.sub('[^a-zA-Z]', ' ', df['text_data'][i])

    # Convert to lowercase
    text = text.lower()

    # remove tags
    text = re.sub(&quot;&amp;lt;/?.*?&amp;gt;&quot;, &quot; &amp;lt;&amp;gt; &quot;, text)

    ##Convert to list from string
    text = text.split()

    ##Stemming
    ps = PorterStemmer()
    # Lemmatisation
    lem = WordNetLemmatizer()
    text = [lem.lemmatize(word) for word in text if not word in stop_words]
    text = &quot; &quot;.join(text)
    corpus.append(text)

# print(corpus)
cv = CountVectorizer(max_df=0.8, stop_words=stop_words, max_features=10000,
                     ngram_range=(1, 3))  # max_df= ignore words that appear in 80% of documents
# max features= the size of the vocabulary, ngram is vocabulary contains single words, bigrams, trigrams
X = cv.fit_transform(corpus)

tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)
tfidf_transformer.fit(X)
# get feature names
feature_names = cv.get_feature_names()

# fetch document for which keywords needs to be extracted
doc = corpus[-1]

# generate tf-idf for the given document
tf_idf_vector = tfidf_transformer.transform(cv.transform([doc]))


# Function for sorting tf_idf in descending order


def sort_coo(coo_matrix):
    tuples = zip(coo_matrix.col, coo_matrix.data)
    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)


def extract_topn_from_vector(feature_names, sorted_items):
    &quot;&quot;&quot;get the feature names and tf-idf score of top n items&quot;&quot;&quot;

    # use only topn items from vector
    # sorted_items = sorted_items[:topn]

    score_vals = []
    feature_vals = []

    # word index and corresponding tf-idf score
    for idx, score in sorted_items:
        # keep track of feature name and its corresponding score
        score_vals.append(round(score, 3))
        feature_vals.append(feature_names[idx])

    # create a tuples of feature,score
    # results = zip(feature_vals,score_vals)
    results = {}
    for idx in range(len(feature_vals)):
        results[feature_vals[idx]] = score_vals[idx]

    return results


# sort the tf-idf vectors by descending order of scores
sorted_items = sort_coo(tf_idf_vector.tocoo())
# extract only the top n; n here is 10
keywords = extract_topn_from_vector(feature_names, sorted_items)

# print the results
print(&quot;\nAbstract:&quot;)
print(doc)
print(&quot;\nKeywords:&quot;)
for k in keywords:
    print(k, keywords[k])
</code></pre>
<p>which gives me an output like</p>
<pre><code>Keywords:
market trend 0.613
trends 0.482
customers 0.366
best product 0.353
cameras 0.348
cameras on sale 0.082
discount 0.081
</code></pre>
<p>How do I ensure that I get keywords for all my records in the dataframe?
The end result that I want is a dataframe with 4columns having values:</p>
<pre><code>_id  page_number                        pdf_name text_data                           Keywords
0  60f1530e3249c01351086b2f          1.0  a.pdf        This is my timee,eerwff.. time,
1  60f1530e3249c01351086b30          2.0  a.pdf        how is it going, tbh you nee.. going,
2  60f1530e3249c01351086b31          3.0  a.pdf        mark marquez is, going to.. mark,
3  60f1530e3249c01351086b32          4.0  b.pdf        coronavirus seems to be... corona,
4  60f1530e3249c01351086b33          5.0  b.pdf        sports all over,has,lost.. sports,
</code></pre>
<p>Please help. Thanks</p>
",Vectorization & Embeddings,create keywords column using extracted keywords panda dataframe ha column k record dataframe look like used countvectorizer sklearn alongwith tfidf transformer get keywords present getting keywords extracted coming last sentence code extract give keywords give output like ensure get keywords record dataframe end result want dataframe column value please help thanks
PyTorch How to code Multi Head Self Attention in parallel?,"<p>I want to encode the word (embedding) sequence with 16-Head Self-Attention.</p>
<p>Currently I use a <code>nn.ModuleList</code> together with a <code>for</code> loop to generate the output of each head then concatenate all of them. This approach is extremely slow and I wonder if there's way to code MHA in parallel?</p>
<p>To generalize, I would like to know if I can 'stack' multiple <code>nn.Linear</code>, as I feed the input vector, the multiple outputs will be computed <strong>in parallel</strong>.</p>
",Vectorization & Embeddings,pytorch code multi head self attention parallel want encode word embedding sequence head self attention currently use together loop generate output head concatenate approach extremely slow wonder way code mha parallel generalize would like know stack multiple feed input vector multiple output computed parallel
NLP-How can we feed the output of FastText in Neural Network,"<p>I am working on Text classification where I want to classify movie genres. I want to give input as movie summary/plot and want output as Movie Genres. I have used FastText using Gensim library for obtaining vector representations for words and I want to feed the output of FastText in Neural Network for training so that I can give movie summary/plot as an input to Neural Network and get the output of Movie Genre such as Drama, Horror, etc. I have read many blogs and all are feeding TFIDF in Neural Network but no one is feeding the output of FastText in Neural Network. Can someone please explain to me if it is possible or you think otherwise.</p>
<p>Thank you for your cooperation and understanding in this regard.</p>
<pre><code>import time
from gensim.models import FastText
start = time.time()
model_ted = FastText(sentences=movies_new['genre_new'], size=100, window=5, min_count=5, workers=4,sg=1)
print(model_ted)
end = time.time()
print('Time to train fasttext from generator: %0.2fs' % (end - start))

model_ted.wv.most_similar(&quot;The Lemon Drop Kid , a New York City swindler, is illegally touting horses at a Florida racetrack. After several successful hustles, the Kid comes across a beautiful, but gullible, woman intending to bet a lot of money. The Kid convinces her to switch her bet, employing a prefabricated con. Unfortunately for the Kid, the woman belongs to notorious gangster Moose Moran , as does the money.&quot;)
</code></pre>
<p><strong>Output</strong></p>
<pre><code>[('Foreign legion', 0.9828806519508362),
 ('Space opera', 0.9763268828392029),
 ('Cyberpunk', 0.9738191366195679),
 ('Reboot', 0.9718296527862549),
 ('Kafkaesque', 0.9635183215141296),
 ('Libraries and librarians', 0.9622164368629456),
 ('Parkour in popular culture', 0.961660623550415),
 ('Movies About Gladiators', 0.9592210650444031),
 ('Women in prison films', 0.9587017297744751),
 ('Outlaw', 0.9548137784004211)]
</code></pre>
",Vectorization & Embeddings,nlp feed output fasttext neural network working text classification want classify movie genre want give input movie summary plot want output movie genre used fasttext using gensim library obtaining vector representation word want feed output fasttext neural network training give movie summary plot input neural network get output movie genre drama horror etc read many blog feeding tfidf neural network one feeding output fasttext neural network someone please explain possible think otherwise thank cooperation understanding regard output
I got ValueError: X has 5851 features per sample; expecting 2754 when applying Linear SVC model to test set,"<p>I'm trying to classify texts using Linear SVC, but I got an error.</p>
<p>I applied a model to the test set as below. In this code, I made Tfidf, and did oversampling of training set.</p>
<pre><code>#Import datasets
train = pd.read_csv('train_labeled.csv')
test = pd.read_csv('test.csv')

#Clean datasets
custom_pipeline = [preprocessing.fillna,
                   preprocessing.lowercase,
                   preprocessing.remove_whitespace,
                   preprocessing.remove_punctuation,
                   preprocessing.remove_urls,
                   preprocessing.remove_digits,
                   preprocessing.stem  
                   ]



train[&quot;clean_text&quot;] = train[&quot;text&quot;].pipe(hero.clean, custom_pipeline)
test[&quot;clean_text&quot;] = test[&quot;text&quot;].pipe(hero.clean, custom_pipeline)

#Create Tfidf

count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(train[&quot;clean_text&quot;])
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

X_test_counts = count_vect.fit_transform(test[&quot;clean_text&quot;])
X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)

#Oversampling of trainig set
over = RandomOverSampler(sampling_strategy='minority')

X_os, y_os = over.fit_resample(X_train_tfidf, train[&quot;label&quot;])

#Model
clf = svm.LinearSVC(C=1.0, penalty='l2', loss='squared_hinge', dual=True, tol=1e-3)
clf.fit(X_os, y_os)

pred = clf.predict(X_test_tfidf)

</code></pre>
<p>and I got an error like this. I think it's because the test set has 5851 samples, but the training set has 2754 samples.</p>
<pre><code>ValueError: X has 5851 features per sample; expecting 2754

</code></pre>
<p>In this case, what am I supposed to do?</p>
",Vectorization & Embeddings,got valueerror x ha feature per sample expecting applying linear svc model test set trying classify text using linear svc got error applied model test set code made tfidf oversampling training set got error like think test set ha sample training set ha sample case supposed
"NLP CNN with Embedding, predicting 5 values from Twitter text","<p>I have a bunch of twitter texts (around 70K) that are around 10K words. Some are less and some are more. I have created a Keras architecture to predict 5 values for each Twitter texts and I have trained on those 70K. However, the accuracy (which is defined as follows: a match of pred1 and pred2 happens when all respective 5 values are with no more than 10 difference) is 21% (21% of the test data comply to the mentioned condition). I am not sure that the architecture, the tokenizer and the parameters are appropriate for this problem, but I will provide the code and ask for help. I would appreciate if someone could help me figure out why the accuracy is so low. Here is the model:</p>
<pre><code>class NeuralNetMulti(Regressor):
    def __init__(self):
        self.name = 'keras-sequential'
        self.model = Sequential()
        self.num_words = 35000
        self.tokenizer = Tokenizer(num_words=self.num_words, lower=True)
        # self.earlystopping = callbacks.EarlyStopping(monitor=&quot;mae&quot;,
        #                                              mode=&quot;min&quot;, patience=5,
        #                                              restore_best_weights=True)

    def fit(self, X, y):
        print('Fitting into the neural net...')
        #n_inputs = X.shape[1]
        n_outputs = y.shape[1]
        self.tokenizer.fit_on_texts(X)
        encoded_docs = self.tokenizer.texts_to_sequences(X)
        max_length = max([len(s.split()) for s in X])
        self.max_length = max_length
        X_train = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
        vocab_size = len(self.tokenizer.word_index) + 1
        print(max_length)
        self.model.add(Embedding(self.num_words, 512, input_length=max_length))
        self.model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))
        self.model.add(MaxPooling1D(pool_size=2))
        self.model.add(Conv1D(filters=16, kernel_size=4, activation='relu'))
        self.model.add(MaxPooling1D(pool_size=2))
        self.model.add(Conv1D(filters=8, kernel_size=4, activation='relu'))
        self.model.add(MaxPooling1D(pool_size=2))
        self.model.add(Flatten())
        self.model.add(Dense(1024, activation='relu'))
        self.model.add(Dense(512, activation='relu'))
        self.model.add(Dense(256, activation='relu'))
        self.model.add(Dense(n_outputs))
        self.model.summary()
        self.model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])
        history = self.model.fit(X_train, y, verbose=1, epochs=5, validation_split=0.1, batch_size=16)
def predict(self, X):
    print('Predicting...')
    encoded_docs = self.tokenizer.texts_to_sequences(X)
    X_test = pad_sequences(encoded_docs, maxlen=self.max_length, padding='post')
    predictions = self.model.predict(X_test, verbose=1)
    print('Predicted!')
    return predictions
</code></pre>
<p>X in this case is just an array of strings (the texts). They could be 1000 words, but most of them are around 10K words. <code>y</code> is array of arrays with 5 values that I mentioned. Each of them is between 0 and 100. This model achieves 21% accuracy, but previously I used a TfIdf + PCA and basic Dense network and I achieved 62% accuracy on the same data. I would appreciate anyone experience in this field to give a professional advice. Thank you in advance!</p>
",Vectorization & Embeddings,nlp cnn embedding predicting value twitter text bunch twitter text around k around k word le created kera architecture predict value twitter text trained k however accuracy defined follows match pred pred happens respective value difference test data comply mentioned condition sure architecture tokenizer parameter appropriate problem provide code ask help would appreciate someone could help figure accuracy low model x case array string text could word around k word array array value mentioned model achieves accuracy previously used tfidf pca basic dense network achieved accuracy data would appreciate anyone experience field give professional advice thank advance
What does tokens and vocab mean in glove embeddings?,"<p>I am using glove embeddings and I am quite confused about <code>tokens</code> and <code>vocab</code> in the embeddings. Like this one:</p>

<pre><code>Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)
</code></pre>

<p>what does <code>tokens</code> and <code>vocab</code> mean, respectively? What is the difference?</p>
",Vectorization & Embeddings,doe token vocab mean glove embeddings using glove embeddings quite confused embeddings like one doe mean respectively difference
Text to Matrix in JavaScript,"<p>How can i convert text to matrix just like python devs do using tensor flow text Preprocessing for converting text to matrix.</p>
<p>Even tried 'natural', 'danof' and 'tensorflow' for any utility which can do this but still cant find it.</p>
<p>How its done in python (tfidf)
Source: <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"" rel=""nofollow noreferrer"">text_to_matrix</a></p>
<pre><code>def texts_to_matrix(self, texts, mode='binary'):
    &quot;&quot;&quot;Convert a list of texts to a Numpy matrix.
    # Arguments
        texts: list of strings.
        mode: one of &quot;binary&quot;, &quot;count&quot;, &quot;tfidf&quot;, &quot;freq&quot;.
    # Returns
        A Numpy matrix.
    &quot;&quot;&quot;
    sequences = self.texts_to_sequences(texts)
    return self.sequences_to_matrix(sequences, mode=mode)

def sequences_to_matrix(self, sequences, mode='binary'):
    &quot;&quot;&quot;Converts a list of sequences into a Numpy matrix.
    # Arguments
        sequences: list of sequences
            (a sequence is a list of integer word indices).
        mode: one of &quot;binary&quot;, &quot;count&quot;, &quot;tfidf&quot;, &quot;freq&quot;
    # Returns
        A Numpy matrix.
    # Raises
        ValueError: In case of invalid `mode` argument,
            or if the Tokenizer requires to be fit to sample data.
    &quot;&quot;&quot;
    if not self.num_words:
        if self.word_index:
            num_words = len(self.word_index) + 1
        else:
            raise ValueError('Specify a dimension (`num_words` argument), '
                             'or fit on some text data first.')
    else:
        num_words = self.num_words

    if mode == 'tfidf' and not self.document_count:
        raise ValueError('Fit the Tokenizer on some data '
                         'before using tfidf mode.')

    x = np.zeros((len(sequences), num_words))
    for i, seq in enumerate(sequences):
        if not seq:
            continue
        counts = defaultdict(int)
        for j in seq:
            if j &gt;= num_words:
                continue
            counts[j] += 1
        for j, c in list(counts.items()):
            if mode == 'count':
                x[i][j] = c
            elif mode == 'freq':
                x[i][j] = c / len(seq)
            elif mode == 'binary':
                x[i][j] = 1
            elif mode == 'tfidf':
                # Use weighting scheme 2 in
                # https://en.wikipedia.org/wiki/Tf%E2%80%93idf
                tf = 1 + np.log(c)
                idf = np.log(1 + self.document_count /
                             (1 + self.index_docs.get(j, 0)))
                x[i][j] = tf * idf
            else:
                raise ValueError('Unknown vectorization mode:', mode)
    return x
</code></pre>
",Vectorization & Embeddings,text matrix javascript convert text matrix like python devs using tensor flow text preprocessing converting text matrix even tried natural danof tensorflow utility still cant find done python tfidf source text matrix
Jaccard vs. Cosine similarity for measuring distance between two words (fasttext),"<p>These two distance measurements seem to be the most common in NLP from what I've read. I'm currently using cosine similarity (as does the gensim.fasttext distance measurement). Is there any case to be made for the use of Jaccard instead? Does it even work with only single words as input (with the use of ngrams I suppose)?</p>
<pre class=""lang-py prettyprint-override""><code>ft = fasttext.load_model('cc.en.300.bin')
distance = scipy.spatial.distance.cosine(ft['word1'], ft['word2'])
</code></pre>
",Vectorization & Embeddings,jaccard v cosine similarity measuring distance two word fasttext two distance measurement seem common nlp read currently using cosine similarity doe gensim fasttext distance measurement case made use jaccard instead doe even work single word input use ngrams suppose
"Input Text Data Formatting for CNN in Flux, in Julia","<p>I am implementing Yoon Kim's CNN (<a href=""https://arxiv.org/abs/1408.5882"" rel=""noreferrer"">https://arxiv.org/abs/1408.5882</a>) for text classification in Julia, using Flux as the deep learning framework, with individual sentences as input datapoints. The model zoo (<a href=""https://github.com/FluxML/model-zoo"" rel=""noreferrer"">https://github.com/FluxML/model-zoo</a>) has proven useful to an extent, but it does not have an NLP example with CNNs. I'd like to check if my input data format is the correct one.  </p>

<p>There is no explicit implementation in Flux of a 1D Conv, so I'm using Conv found in <a href=""https://github.com/FluxML/Flux.jl/blob/master/src/layers/conv.jl"" rel=""noreferrer"">https://github.com/FluxML/Flux.jl/blob/master/src/layers/conv.jl</a>
Here is part of the docstring that explains the input data format:</p>

<pre><code>Data should be stored in WHCN order (width, height, # channels, # batches).
In other words, a 100×100 RGB image would be a `100×100×3×1` array,
and a batch of 50 would be a `100×100×3×50` array.
</code></pre>

<p>My format is as follows:</p>

<pre><code>1. width: since text in a sentence is 1D, the width is always 1 
2. height: this is the maximum number of tokens allowable in a sentence
3. \# of channels: this is the embedding size
4. \# of batches: the number of sentences in each batch
</code></pre>

<p>Following the MNIST example in the model zoo, I have </p>

<pre><code>function make_minibatch(X, Y, idxs)
    X_batch = zeros(1, num_sentences, emb_dims, MAX_LEN)

    function get_sentence_matrix(sentence)
        embeddings = Vector{Array{Float64, 1}}()
        for word in sentence
            embedding = get_embedding(word)
            push!(embeddings, embedding)
        end
        embeddings = hcat(embeddings...)
        return embeddings
    end

    for i in 1:length(idxs)
        X_batch[1, i, :, :] = get_sentence_matrix(X[idxs[i]])
    end
    Y_batch = [Flux.onehot(label+1, 1:2) for label in Y[idxs]]
    return (X_batch, Y_batch)
end
</code></pre>

<p>where the X is an array of arrays of words and the get_embedding function returns an embedding as an array. </p>

<p><code>X_batch</code> is then a <code>Array{Float64,4}</code>. Is this the correct approach?</p>
",Vectorization & Embeddings,input text data formatting cnn flux julia implementing yoon kim cnn text classification julia using flux deep learning framework individual sentence input datapoints model zoo ha proven useful extent doe nlp example cnns like check input data format correct one explicit implementation flux conv using conv found part docstring explains input data format format follows following mnist example model zoo x array array word get embedding function return embedding array correct approach
Pytorch Roberta kernal died immediately when running &quot; out = model(inputs)&quot;,"<p>I have a text dataset, which I trained on to get tokernizer, called &quot;bert_tokenizer&quot;. Then I try to give a new word and get the word embedding out.</p>
<pre><code>from transformers import RobertaConfig

config = RobertaConfig(
    vocab_enter code heresize=tokenizer.get_vocab_size(),
    max_position_embeddings=514,
    num_attention_heads=12,
    num_hidden_layers=6,
    type_vocab_size=1,)

#re-create tokenizer in transformers
from transformers import RobertaTokenizerFast

tokenizer = RobertaTokenizerFast.from_pretrained(&quot;bert_tokenizer&quot;, output_hidden_states =True, max_len=512)

#initialise model
from transformers import RobertaForMaskedLM

model = RobertaForMaskedLM(config=config)
model.eval()

word = tokenizer.encode('test test')
input = torch.LongTensor(word)
out = model(input_ids=input)
</code></pre>
<p>Failed the last line <code>out = model(input_ids=input)</code> , immediately. Error: kernel died.
My training dataset is very small, is that a problem? Or other reasons?</p>
<p>I am following tutorial here: <a href=""https://github.com/BramVanroy/bert-for-inference/blob/master/introduction-to-bert.ipynb"" rel=""nofollow noreferrer"">https://github.com/BramVanroy/bert-for-inference/blob/master/introduction-to-bert.ipynb</a></p>
<p>Thank you.</p>
",Vectorization & Embeddings,pytorch roberta kernal died immediately running model input text dataset trained get tokernizer called bert tokenizer try give new word get word embedding failed last line immediately error kernel died training dataset small problem reason following tutorial thank
Clustering with UMAP and HDBScan,"<p>I have a somewhat large amount of textual data, input by approximately 5000 people. I've assigned each person a vector using Doc2vec, reduced to two dimensions using UMAP and highlighted groups contained within using HDBSCAN. The intention is to highlight groups with similar topic similarity. This has resulted in the scatter plot seen below.</p>
<p><a href=""https://i.sstatic.net/VcHn8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VcHn8.png"" alt=""Scatter plot created with UMAP and HDBSCAN"" /></a></p>
<p>This looks acceptable. However, when I used the same data in Bokeh (in order to create an interactive graph) the output looked very different. Despite using the same co-ordinates and groups as before,  the clear groupings seen before were gone. Instead the graph is a mess, with colours all mixed.</p>
<p><a href=""https://i.sstatic.net/BNUpU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BNUpU.png"" alt=""Bokeh plot"" /></a></p>
<p>When a filter was applied to select a random group, the points were quite distributed through the whole plot and didn't resemble a cohesive &quot;group&quot; by any means. Group 41, for instance, had points near each corner of the plot.</p>
<p>Document vectors were reduced to X, Y co-ordinates with this code:</p>
<pre><code>clusterable_embedding = umap.UMAP(
n_neighbors=150,
min_dist=0,
n_components=2,
random_state=42,
repulsion_strength=1.0,).fit_transform(model.dv.vectors)
</code></pre>
<p>And assigned groups using this code:</p>
<pre><code>labels = hdbscan.HDBSCAN(
min_samples=1,
min_cluster_size=10,
).fit_predict(clusterable_embedding)
</code></pre>
<p>The Matplotlib plot with clear groups as generated with this code:</p>
<pre><code>clustered = (labels &gt;= 0)
from matplotlib.pyplot import figure
figure(figsize=(10, 10), dpi=80)
plt.scatter(clusterable_embedding[~clustered, 0],
            clusterable_embedding[~clustered, 1],
            c=(0.5, 0.5, 0.5),
            s=10,
            alpha=0.5)
plt.scatter(clusterable_embedding[clustered, 0],
            clusterable_embedding[clustered, 1],
            c=(labels[clustered]),
            s=20,
            cmap='Spectral');
</code></pre>
<p>This was then inserted into a Pandas Dataframe:</p>
<pre><code>for item in list(clusterable_embedding[clustered]):
    x = item[0]
    y = item[1]
    group = labels[int(len(all_data))]
    topic = topiclist(group)
    all_data.loc[len(all_data)] = [x, y, group, topic] 
</code></pre>
<p>And a Bokeh plot created:</p>
<pre><code>datasource = ColumnDataSource(all_data)

yfig = figure(
    plot_width=600,
    plot_height=600,
    tools=('pan, wheel_zoom, reset')
)

yfig.add_tools(HoverTool(tooltips=&quot;&quot;&quot;
&lt;div&gt;
    &lt;div&gt;
        &lt;span style='font-size: 16px; color: #224499'&gt;Group: &lt;/span&gt;
        &lt;span style='font-size: 18px'&gt;@group&lt;/span&gt;
    &lt;/div&gt;
    &lt;div&gt;
        &lt;span style='font-size: 16px; color: #224499'&gt;Topic: &lt;/span&gt;
        &lt;span style='font-size: 18px'&gt;@topic&lt;/span&gt;
    &lt;/div&gt;
&lt;/div&gt;
&quot;&quot;&quot;))

color_mapper = LinearColorMapper(palette='Magma256', low=min(groups), high=max(groups))
    
yfig.circle(
    'x',
    'y',
    source=datasource,
    color={'field': 'group', 'transform': color_mapper},
    line_alpha=0.6,
    fill_alpha=0.6,
    size=4
)
show(yfig)
</code></pre>
<p>Am I doing something wrong here? Or is this a limitation of the technology or the data? Are my coloured groups in the initial plot really being grouped by their group, and if so, why aren't the ones in the Bokeh plot?</p>
<p>Any help at all will be greatly appreciated.</p>
",Vectorization & Embeddings,clustering umap hdbscan somewhat large amount textual data input approximately people assigned person vector using doc vec reduced two dimension using umap highlighted group contained within using hdbscan intention highlight group similar topic similarity ha resulted scatter plot seen look acceptable however used data bokeh order create interactive graph output looked different despite using co ordinate group clear grouping seen gone instead graph mess colour mixed filter wa applied select random group point quite distributed whole plot resemble cohesive group mean group instance point near corner plot document vector reduced x co ordinate code assigned group using code matplotlib plot clear group generated code wa inserted panda dataframe bokeh plot created something wrong limitation technology data coloured group initial plot really grouped group one bokeh plot help greatly appreciated
getting incompatible shape error in tensorflow functional model,"<p>I am trying to implement a deep model using tensorflow.keras which contains an embedding layer + Conv1D + 2 BiLstm layers. This is the implementation in sequential mode:</p>
<pre><code>model = models.Sequential()
model.add(layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=limit_on_length, trainable=False))
model.add(layers.Conv1D(50, 4, padding='same', activation='relu'))
model.add(layers.Bidirectional(layers.LSTM(units=200, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))
model.add(layers.Bidirectional(layers.LSTM(units=200, return_sequences=False,dropout=0.2, recurrent_dropout=0.2)))
model.add(layers.Dense(len(set(tr_intents)), activation='softmax'))
model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;])
</code></pre>
<p>And I fit the model like this:</p>
<pre><code>model.fit(train_padded, to_categorical(tr_intents), epochs=15, batch_size=32, validation_split=0.1)
</code></pre>
<p>Everything goes on very good in this sequential mode. But when I implement the model in functional mode, I get this kind of error:</p>
<pre><code>ValueError: Shapes (32, 22) and (32, 11, 22) are incompatible
</code></pre>
<p>And here is my implementation in functional structure:</p>
<pre><code>input_layer = layers.Input(shape=(None,))
x = layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=limit_on_length, trainable=False)(input_layer)
x = layers.Conv1D(50, 4, padding='same', activation='relu')(x)
x = layers.Bidirectional(layers.LSTM(units=200, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)
x = layers.Bidirectional(layers.LSTM(units=200, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)
intents_out = layers.Dense(n_intents, activation='softmax')(x)
model = models.Model(input_layer, intents_out)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
</code></pre>
<p>Can anybody help me with this error? I need to implement the model in functional mode cause I have to add one more output layer to it.
Number of my intents(or labels) is 22 and each sentence has length of 11.</p>
",Vectorization & Embeddings,getting incompatible shape error tensorflow functional model trying implement deep model using tensorflow kera contains embedding layer conv bilstm layer implementation sequential mode fit model like everything go good sequential mode implement model functional mode get kind error implementation functional structure anybody help error need implement model functional mode cause add one output layer number intent label sentence ha length
How can I check the remaining words after applying TFidfVectorizer in Python?,"<p>This is a pretty straightforward question, but I couldn't find any related posts. I hope I'm not generating duplicates, but here's the issue, I'm building a text classifier, which is derived from a table looking like this:</p>
<pre><code>id    text                      cat1    cat2     target
111   'A mí, no me gusta...'    A       1        1
888   'Bueníssimo...'           A       2        0
999   'Yo pienso que...'        B       2        1
132   'Lo que no me...'         C       1        0
555   'Terrible...'             C       2        0
.
.
.
</code></pre>
<p>One can think of this as the public opinion of your product. I'm trying to put text (text column) and categorical variables (cat1 and cat2 columns) together. Target is 1 if the customer recommends the product, 0 if doesn't.</p>
<p>And then I've just applied these methods:</p>
<pre><code>tfv = TfidfVectorizer(stop_words=stopwords.words(&quot;spanish&quot;))

tfv_train = tfv.fit_transform(X_train.text).toarray()
tfv_test = tfv.transform(X_test.text).toarray()

# dimensionality reduction to improve sparse matrix
svd = TruncatedSVD()
trainsvd = svd.fit_transform(tfv_train)
testsvd = svd.transform(tfv_test)

# putting categorical and text-transformed variables together
features_train = np.hstack([train.drop(columns=&quot;text&quot;).values, trainsvd])
features_test = np.hstack([test.drop(columns=&quot;text&quot;).values, testsvd])
</code></pre>
<p>Since these packages are usually better for English transformations, I wanted to see which words are still in the text after the removal of the stopwords. So I was trying to plot the wordcloud, but I just found out that I don't know how to do this after applying the tfidf transformation. This is what I would normally do:</p>
<pre><code>%matplotlib inline

from wordcloud import WordCloud

words = &quot; &quot;.join([text for text in df.request_text])

word_cloud = WordCloud().generate(words)

plt.figure()
plt.imshow(word_cloud)
</code></pre>
<p>Any clues?</p>
<p>Ps.: Sorry about the oversimplification of the data, but I can't post the real one. I hope someone is able to understand and help.</p>
",Vectorization & Embeddings,check remaining word applying tfidfvectorizer python pretty straightforward question find related post hope generating duplicate issue building text classifier derived table looking like one think public opinion product trying put text text column categorical variable cat cat column together target customer recommends product applied method since package usually better english transformation wanted see word still text removal stopwords wa trying plot wordcloud found know applying tfidf transformation would normally clue p sorry oversimplification data post real one hope someone able understand help
Why use cosine similarity in Word2Vec when its trained using dot-product similarity,"<p>According to several posts I found on stackoverflow (for instance this <a href=""https://stackoverflow.com/questions/38423387/why-does-word2vec-use-cosine-similarity"">Why does word2Vec use cosine similarity?</a>), it's common practice to calculate the cosine similarity between two word vectors after we have trained a word2vec (either CBOW or Skip-gram) model. However, this seems a little odd to me since the model is actually trained with dot-product as a similarity score. One evidence of this is that the norm of the word vectors we get after training are actually meaningful. So why is it that people still use cosine-similarity instead of dot-product when calculating the similarity between two words?</p>
",Vectorization & Embeddings,use cosine similarity word vec trained using dot product similarity according several post found stackoverflow instance href doe word vec use cosine similarity common practice calculate cosine similarity two word vector trained word vec either cbow skip gram model however seems little odd since model actually trained dot product similarity score one evidence norm word vector get training actually meaningful people still use cosine similarity instead dot product calculating similarity two word
Sense similarity matrix using WordNet,"<p>I have a vocabulary of unique words (excluding the stopwords) used over the entire document collection. I want to perform query expansion. In some approaches I have found that for every word in the query its top k synonyms (usually k=3) is augmented to the query. However, I am using a vector space model based on TFIDF document representation so adding words to the query which are not in the vocabulary will eventually get dropped off. Also, since it would not use a word sense disambiguation technique hence adding synonyms would not guarantee that the sense in which the words in the query are used is retained by the added synonyms thereby leading to query drifting. Hence I am thinking to create a Sense Similarity Matrix which will consist of similarity score between the query and all possible senses in which the words in the vocabulary have been used over the entire corpus. The similarity score would be calculated either on the basis of information theoretic or path based approach.</p>
<p>However, I am unable to understand how to find all the senses in which the words in the vocabulary have been used. Also, is my approach correct? Can someone please guide me in this by pointing to some relevant resources?</p>
",Vectorization & Embeddings,sense similarity matrix using wordnet vocabulary unique word excluding stopwords used entire document collection want perform query expansion approach found every word query top k synonym usually k augmented query however using vector space model based tfidf document representation adding word query vocabulary eventually get dropped also since would use word sense disambiguation technique hence adding synonym would guarantee sense word query used retained added synonym thereby leading query drifting hence thinking create sense similarity matrix consist similarity score query possible sens word vocabulary used entire corpus similarity score would calculated either basis information theoretic path based approach however unable understand find sens word vocabulary used also approach correct someone please guide pointing relevant resource
Identifying the most useful words in differentiating between classes,"<p>Is it possible to use tfidf (tfidfvectorizer in Python) to figure out which words are most important when trying to distinguish <em>between</em> two text classes (i.e., positive or negative sentiment, etc.)? For example, which words were most important for identifying the positive class, and then separately, which were most useful for identifying the negative class?</p>
",Vectorization & Embeddings,identifying useful word differentiating class possible use tfidf tfidfvectorizer python figure word important trying distinguish two text class e positive negative sentiment etc example word important identifying positive class separately useful identifying negative class
BERT sentence embedding by summing last 4 layers,"<p>I used Chris Mccormick tutorial on BERT using <code>pytorch-pretained-bert</code> to get a sentence embedding as follows: </p>

<pre class=""lang-py prettyprint-override""><code>tokenized_text = tokenizer.tokenize(marked_text)
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
segments_ids = [1] * len(tokenized_text)
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
model = BertModel.from_pretrained('bert-base-uncased')
model.eval()

with torch.no_grad():
    encoded_layers, _ = model(tokens_tensor, segments_tensors)
    # Holds the list of 12 layer embeddings for each token
    # Will have the shape: [# tokens, # layers, # features]
    token_embeddings = []

    # For each token in the sentence...
    for token_i in range(len(tokenized_text)):
        # Holds 12 layers of hidden states for each token
        hidden_layers = []

        # For each of the 12 layers...
        for layer_i in range(len(encoded_layers)):

                # Lookup the vector for `token_i` in `layer_i`
                vec = encoded_layers[layer_i][batch_i][token_i]

                hidden_layers.append(vec)

        token_embeddings.append(hidden_layers)
</code></pre>

<p>Now, I am trying to get the final sentence embedding by summing the last 4 layers as follows: </p>

<pre class=""lang-py prettyprint-override""><code>summed_last_4_layers = [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings]
</code></pre>

<p>But instead of getting a single torch vector of length 768 I get the following: </p>

<pre><code>[tensor([-3.8930e+00, -3.2564e+00, -3.0373e-01,  2.6618e+00,  5.7803e-01,
-1.0007e+00, -2.3180e+00,  1.4215e+00,  2.6551e-01, -1.8784e+00,
-1.5268e+00,  3.6681e+00, ...., 3.9084e+00]), tensor([-2.0884e+00, -3.6244e-01,  ....2.5715e+00]), tensor([ 1.0816e+00,...-4.7801e+00]), tensor([ 1.2713e+00,.... 1.0275e+00]), tensor([-6.6105e+00,..., -2.9349e-01])]
</code></pre>

<p>What did I get here? How do I pool the sum of the last for layers? </p>

<p>Thank you!</p>
",Vectorization & Embeddings,bert sentence embedding summing last layer used chris mccormick tutorial bert using get sentence embedding follows trying get final sentence embedding summing last layer follows instead getting single torch vector length get following get pool sum last layer thank
How K-Means clustering help in the analysis of word2vec embeddings?,"<p>I am new to NLP. I have a yelp-review dataset. I have used a word2vector embedding on the text column of the yelp-review. I have used K-means and PCA to visualise the data and have obtained 6 clusters which are well separated. Now I want to know, what this six clusters represent. In other words, I want to see which &quot;words&quot; belongs to cluster 0 and so on.
I have used this code but the output is a lot of words.</p>
<pre><code>for i, word in enumerate(words):  
    print (word + &quot;:&quot; + str(labels[i]))
</code></pre>
<p>labels are the k-means labels. I am thinking of an idea of a
word cloud with respect to the K-means cluster labels.</p>
<p>Please can one one give an idea how to do proceed.</p>
<p>I also a photo of the data frame head.</p>
<p><a href=""https://i.sstatic.net/4mNwP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4mNwP.png"" alt=""enter image description here"" /></a></p>
<p>Thank you.</p>
",Vectorization & Embeddings,k mean clustering help analysis word vec embeddings new nlp yelp review dataset used word vector embedding text column yelp review used k mean pca visualise data obtained cluster well separated want know six cluster represent word want see word belongs cluster used code output lot word label k mean label thinking idea word cloud respect k mean cluster label please one one give idea proceed also photo data frame head thank
Question about input_dim in keras embedding layer,"<p>From the documentation on <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"" rel=""nofollow noreferrer"">tf.keras.layers.Embedding</a> :</p>
<blockquote>
<p><code>input_dim</code>:</p>
<p>Integer. Size of the vocabulary, i.e. maximum integer index + 1.</p>
<p><code>mask_zero</code>:</p>
<p>Boolean, whether or not the input value 0 is a special “padding” value that should be masked
out. This is useful when using recurrent layers which may take variable length input. If this
is True, then all subsequent layers in the model need to support masking or an exception will
be raised. If mask_zero is set to True, as a consequence, index 0 cannot be used in the
vocabulary (input_dim should equal size of vocabulary + 1).</p>
</blockquote>
<ol>
<li><p>I was reading <a href=""https://stackoverflow.com/questions/43227938/keras-embedding-layer-masking-why-does-input-dim-need-to-be-vocabulary-2"">this answer</a> but I'm still confused. If my vocabulary size is n but they are encoded with index values from <code>1</code> to <code>n</code> (0 is left for padding), is input_dim equal to <code>n</code> or <code>n+1</code>?</p>
</li>
<li><p>If the inputs are padded with zeroes, what are the consequences of leaving <code>mask_zero = False</code>?</p>
</li>
<li><p>If <code>mask_zero = True</code>, based on the documentation, I would have to increment the answer from my first question by one? What is the expected behaviour if this was not done?</p>
</li>
</ol>
",Vectorization & Embeddings,question input dim kera embedding layer documentation tf kera layer embedding integer size vocabulary e maximum integer index boolean whether input value special padding value masked useful using recurrent layer may take variable length input true subsequent layer model need support masking exception raised mask zero set true consequence index used vocabulary input dim equal size vocabulary wa reading href answer still confused vocabulary size n encoded index value left padding input dim equal input padded zero consequence leaving based documentation would increment answer first question one expected behaviour wa done
Django slow loading page due to lots of codes,"<p>I am implememting a small recommendation feature on my home page which will suggest jobs to candidates. To accomplish the latter, I have used CountVectorizer and Cosine Similarity. However, to perform this, I had to concatenate lots of fields from the candidate's profile and job description.</p>
<p>This code is defined in the views.py. Every time I load the home page, this view gets executed and eventually, it takes time to load.</p>
<p>Is there something I can do to improve the loading speed? Thank you guys ;)</p>
",Vectorization & Embeddings,django slow loading page due lot code implememting small recommendation feature home page suggest job candidate accomplish latter used countvectorizer cosine similarity however perform concatenate lot field candidate profile job description code defined view py every time load home page view get executed eventually take time load something improve loading speed thank guy
calculating semantic similarity between sets of sentences,"<p>I have two sets of short messages, I want to compute the similarity between these two sets and identify if they are talking about the same sub-topic based on their semantic similarity. I know how to use pairwise similarity, my problem I want to compute the overall similarity among all the sentences in the two sets not for 2 sentences. Is there a way to use tf-idf or word2vec/doc2vec with cosine similarity to calculate the overall score?</p>
",Vectorization & Embeddings,calculating semantic similarity set sentence two set short message want compute similarity two set identify talking sub topic based semantic similarity know use pairwise similarity problem want compute overall similarity among sentence two set sentence way use tf idf word vec doc vec cosine similarity calculate overall score
"Invalid argument: Input to reshape is a tensor with 14155776 values, but the requested shape has 262144","<p>I am trying to use ELMO embedding to train my Network with LSTM but i have a problem with the shape of the tensor
<strong>y-train with shape (67689, 5) encoded with 1 hot vector (the output is 5 classes)
x-train with shape (67689,) is text formate</strong>
this the code:</p>
<pre><code>from  tensorflow.keras.layers import Input, Lambda, Dense
from  tensorflow.keras.models import Model
import  tensorflow.keras.backend as K

batch_size=32
sequence_length=58
def ElmoEmbedding(x):
    return elmo_model(inputs={
                            &quot;tokens&quot;: tf.squeeze(tf.cast(x, tf.string)),
                            &quot;sequence_len&quot;: tf.constant(batch_size*[sequence_length])
                      },
                      signature=&quot;tokens&quot;,
                      as_dict=True)[&quot;elmo&quot;]

input_text = Input(shape=(1,), dtype=tf.string)
embedding = Lambda(ELMoEmbedding, output_shape=(58,1024,))(input_text)
reshape_2 = Reshape((1024, 1,))(embedding)
lstm = Bidirectional(LSTM(units=100, recurrent_activation='relu', return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(reshape_2)
reshape_3 = Reshape(( 5,))(lstm)
pred = Dense(5, activation='softmax')(reshape_3)
model = Model(inputs=[input_text], outputs=pred)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
with tf.Session() as session:
    K.set_session(session)
    session.run(tf.global_variables_initializer())  
    session.run(tf.tables_initializer())


   
    history = model.fit(x_train, y_train, epochs=3, batch_size=256,validation_split = 0.2, shuffle=True)
    loss, accuracy = model.evaluate(x_train, y_train, verbose=False)
    print(&quot;Training Accuracy: {:.4f}&quot;.format(accuracy))
    loss, accuracy = model.evaluate(x_test, y_test, verbose=False)
    print(&quot;Testing Accuracy:  {:.4f}&quot;.format(accuracy))
</code></pre>
<p>and I got this error</p>
<blockquote>
<p>InvalidArgumentError: 2 root error(s) found.   (0) Invalid argument:
Input to reshape is a tensor with 52428800 values, but the requested
shape has 1280     [[{{node reshape_45/Reshape}}]]<br />
[[loss_49/mul/_847]]   (1) Invalid argument: Input to reshape is a
tensor with 52428800 values, but the requested shape has 1280<br />
[[{{node reshape_45/Reshape}}]] 0 successful operations. 0 derived
errors ignored.</p>
</blockquote>
",Vectorization & Embeddings,invalid argument input reshape tensor value requested shape ha trying use elmo embedding train network lstm problem shape tensor train shape encoded hot vector output class x train shape text formate code got error invalidargumenterror root error found invalid argument input reshape tensor value requested shape ha node reshape reshape loss mul invalid argument input reshape tensor value requested shape ha node reshape reshape successful operation derived error ignored
How to use Word2Vec CBOW in statistical algorithm?,"<p>I have seen few examples of using CBOW in Neural Networks models (although I did not understand it)</p>
<p>I know that Word2Vec is not similar to BOW or TFIDF, as there is no single value for CBOW</p>
<p>and all examples I saw were using Neural Network.</p>
<p>I have 2 questions</p>
<p>1- Can we convert the vector to a single value and put it in a dataframe so we can use it in Logistic Regression Model?</p>
<p>2- Is there any simple code for CBOW usage with logistic Regression?</p>
<p>More Explanation.</p>
<p>In my case, I have a corpus that I want to make a comparison between top features in BOW and CBOW</p>
<p>after converting to BOW</p>
<p>I get this dataset</p>
<pre><code>RepID   Label   Cat   Dog   Snake   Rabbit  Apple Orange  ...
1       1       5     3     8       2       0 
2       0       1     0     0       6       9
3       1       4     1     5       1       7 
</code></pre>
<p>after converting to TFIDF</p>
<p>I get this dataset</p>
<pre><code>RepID   Label   Cat   Dog   Snake   Rabbit  Apple Orange  ...
1       1       0.38     0.42    0.02    0.22   0.00   0.19
2       0       0.75     0.20    0.08    0.12   0.37   0.21
3       1       0.17     0.84    0.88    0.11   0.07   0.44
</code></pre>
<p>I am observing the results of top 3 features in each model</p>
<p>so my dataset become like this</p>
<p>BOW (I put null here for the values that will be omitted)</p>
<pre><code>RepID   Label    Cat   Dog   Snake   Rabbit  Apple Orange  ...
1       1        5      null    8    null   null   7
2       0        null   null    null    6   9   2
3       1        4      null    5    null   7   null
</code></pre>
<p>TFIDF (I put null here for the values that will be omitted)</p>
<pre><code>RepID   Label   Cat   Dog   Snake   Rabbit  Apple Orange  ...
1       1       0.38     0.42    null    0.22   null   null
2       0       0.75     null    null    null   0.37   0.21
3       1       null     0.84    0.88    null   null   0.44
</code></pre>
<p>I want now to do the same with Word2Ven CBOW</p>
<p>I want to take the highest values in the CBOW model</p>
<pre><code>RepID   Label  Cat   Dog   Snake   Rabbit  Apple Orange  ...
1       1      v11     v12    v13    v14   v15   v16
2       0      v21     v22    v23    v24   v25   v26
3       1      v31     v32    v33    v34   v35   v36
</code></pre>
<p>to be like this</p>
<pre><code>RepID   Label    Cat   Dog   Snake   Rabbit  Apple Orange  ...
1       1        v11     null    v13    null   v15   null
2       0        null     null    v23    null   v25   v26
3       1        v31     null    v33    v34   null   null
</code></pre>
",Vectorization & Embeddings,use word vec cbow statistical algorithm seen example using cbow neural network model although understand know word vec similar bow tfidf single value cbow example saw using neural network question convert vector single value put dataframe use logistic regression model simple code cbow usage logistic regression explanation case corpus want make comparison top feature bow cbow converting bow get dataset converting tfidf get dataset observing result top feature model dataset become like bow put null value omitted tfidf put null value omitted want word ven cbow want take highest value cbow model like
Count occurrences of overlapping words between txt files,"<p>So I'm attempting to write code that compares two lists (firstfile from firstfile.txt; secondfile from secondfile.txt) and prints the number of times overlapping words from firstfile appear in secondfile.</p>
<p>So, for example, if in firstfile(.txt) I have:</p>
<pre><code>'beautiful', 'day', 'neighborhood', 'sun'
</code></pre>
<p>and in secondfile(.txt) I have</p>
<pre><code>&quot;It's a beautiful day in the neighborhood today. The sun is shining brightly, and the birds are singing. 
And now I run into this beautiful lady, whose skin gleams in the sun light.&quot;
</code></pre>
<p>So the outcome I'd expect would be something like:</p>
<pre><code>beautiful: 2
day: 1
neighborhood: 1
sun: 2
</code></pre>
<p>So far, I've come up with the following:</p>
<pre><code>results = {}
for i in firstfile:
        results[i] = secondfile.count(i) 
print(results)
</code></pre>
<p>but it prints out something like</p>
<pre><code>{'beautiful': 0, 'day': 0, 'neighborhood': 0, 'sun': 0}
</code></pre>
<p>which is obviously incorrect.</p>
<p>What am I doing wrong? I've tried with a dozen of different approaches but none of them seem to return anything other than 0 for the overlapping words. Is there something wrong with the code, or should I go about creating the lists from firstfile.txt and secondfile.txt in a specific way? Thanks, everyone! (Also, I'm a complete newbie to Python (and programming in general), so please forgive me if this is a stupid question or if I'm not putting things clearly!!)</p>
",Vectorization & Embeddings,count occurrence overlapping word txt file attempting write code compare two list firstfile firstfile txt secondfile secondfile txt print number time overlapping word firstfile appear secondfile example firstfile txt secondfile txt outcome expect would something like far come following print something like obviously incorrect wrong tried dozen different approach none seem return anything overlapping word something wrong code go creating list firstfile txt secondfile txt specific way thanks everyone also complete newbie python programming general please forgive stupid question putting thing clearly
How to access to FastText classifier pipeline?,"<p>As we know <code>Facebook</code>'s <a href=""https://fasttext.cc/"" rel=""nofollow noreferrer"">FastText</a> is a great open-source, free, lightweight library which can be used for text classification. But here a problem is the pipeline seem to be end-to end black-box. Yes, we can change the hyper-parameters from these <a href=""https://fasttext.cc/docs/en/options.html"" rel=""nofollow noreferrer"">options</a> for setting training configuration. But I couldn't manage to find a way to access to the vector embedding it generates internally.</p>
<p>Actually I want to do some manipulation on the vector embedding - like introducing <code>tf-idf</code> weighting apart from these <code>word2vec</code> representations and another thing I want to to is oversampling using <code>SMOTE</code> which requires numerical representation. For these reasons I need to introduce my custom code in between the overall pipeline which seems to be inaccessible for me. How introduce custom steps in this pipeline?</p>
",Vectorization & Embeddings,access fasttext classifier pipeline know fasttext great open source free lightweight library used text classification problem pipeline seem end end black box yes change hyper parameter option setting training configuration manage find way access vector embedding generates internally actually want manipulation vector embedding like introducing weighting apart representation another thing want oversampling using requires numerical representation reason need introduce custom code overall pipeline seems inaccessible introduce custom step pipeline
Python/Gensim - What is the meaning of syn0 and syn0norm?,"<p>I know that in <em>gensims</em> <em><code>KeyedVectors</code>-model</em>, one can access the embedding matrix by the attribute <code>model.syn0</code>. There is also a <code>syn0norm</code>, which doesn't seem to work for the <em>glove</em> model I recently loaded. I think I also have seen <code>syn1</code> somewhere previously. </p>

<p>I haven't found a doc-string for this and I'm just wondering what's the logic behind this?</p>

<p>So if <code>syn0</code> is the embedding matrix, what is <code>syn0norm</code>? What would then <code>syn1</code> be and generally, what does <code>syn</code> stand for?</p>
",Vectorization & Embeddings,python gensim meaning syn syn norm know gensims model one access embedding matrix attribute also seem work glove model recently loaded think also seen somewhere previously found doc string wondering logic behind embedding matrix would generally doe stand
How to control randomness when training word2vec in Gensim?,"<p>I'm working on the gensim’s word2vec model, but different runs on the same dataset produce the different model. I tried to set seed to a fixed number, including  PYTHONHASHSEED and set the number of workers being one. But all the above methods are not working.</p>
<p>I included my code here:</p>
<pre><code>def word2vec_model(data):
    model = gensim.models.Word2Vec(data, size=300, window=20, workers=4, min_count=1)
    model.wv.save(&quot;word2vec.wordvectors&quot;)
    embed = gensim.models.KeyedVectors.load(&quot;word2vec.wordvectors&quot;, mmap='r')
    return embed
</code></pre>
<p>I checked the following output:</p>
<pre><code>Cooking.similar_by_vector(Cooking['apple'], topn=10, restrict_vocab=None)
</code></pre>
<p>example output:</p>
<pre><code>[('apple', 0.9999999403953552),
 ('charcoal', 0.2554503381252289),
 ('response', 0.25395694375038147),
 ('boring', 0.2537640631198883),
 ('healthy', 0.24807702004909515),
 ('wrong', 0.24783077836036682),
 ('juice', 0.24270494282245636),
 ('lacta', 0.2373320758342743),
 ('saw', 0.2359238862991333),
 ('insufferable', 0.23015251755714417)]
</code></pre>
<p>Each run, I got different similar words.</p>
<p>Does anyone know how to solve it？I appreciate any direct codes or documentation. Thank you in advance!</p>
",Vectorization & Embeddings,control randomness training word vec gensim working gensim word vec model different run dataset produce different model tried set seed fixed number including pythonhashseed set number worker one method working included code checked following output example output run got different similar word doe anyone know solve appreciate direct code documentation thank advance
Does adding a list of Word2Vec embeddings give a meaningful represenation?,"<p>I'm using a pre-trained word2vec model (word2vec-google-news-300) to get the embeddings for a given list of words. Please note that this is NOT a list of words that we get after tokenizing a  sentence, it is just a list of words that describe a given image.</p>
<p>Now I'd like to get a single vector representation for the entire list. Does adding all the individual word embeddings make sense? Or should I consider averaging?
Also, I would like the vector to be of a constant size so concatenating the embeddings is not an option.</p>
<p>It would be really helpful if someone can explain the intuition behind considering either one of the above approaches.</p>
",Vectorization & Embeddings,doe adding list word vec embeddings give meaningful represenation using pre trained word vec model word vec google news get embeddings given list word please note list word get tokenizing sentence list word describe given image like get single vector representation entire list doe adding individual word embeddings make sense consider averaging also would like vector constant size concatenating embeddings option would really helpful someone explain intuition behind considering either one approach
TFIDF for Large Dataset,"<p>I have a corpus which has around 8 million news articles, I need to get the TFIDF representation of them as a sparse matrix. I have been able to do that using scikit-learn for relatively lower number of samples, but I believe it can't be used for such a huge dataset as it loads the input matrix into memory first and that's an expensive process.</p>

<p>Does anyone know, what would be the best way to extract out the TFIDF vectors for large datasets?</p>
",Vectorization & Embeddings,tfidf large dataset corpus ha around million news article need get tfidf representation sparse matrix able using scikit learn relatively lower number sample believe used huge dataset load input matrix memory first expensive process doe anyone know would best way extract tfidf vector large datasets
How to find gender bias in word embeddings?,"<p>I have <code>glove.twitter.27B.200d.txt</code> word embeddings. These embeddings in <code>GloVe</code> format. I transfered it to <code>w2v</code> format using this code:</p>
<pre><code>model = KeyedVectors.load_word2vec_format(
&quot;data/glove.twitter.27B.200d.w2v.txt&quot;, binary=False
)
</code></pre>
<p><code>len(model.vocab) == 1193514</code></p>
<p>There is a gender bias in this word embeddings:</p>
<p><code>model.similarity(&quot;man&quot;, &quot;kitchen&quot;) == 0.32785824</code></p>
<p><code>model.similarity(&quot;woman&quot;, &quot;kitchen&quot;) == 0.40180725</code></p>
<p>I want to find a gender bias direction in this word embeddings, but not sure how.</p>
",Vectorization & Embeddings,find gender bias word embeddings word embeddings embeddings format transfered format using code gender bias word embeddings want find gender bias direction word embeddings sure
Distractor Generation for Multiple choice question,"<p>I'm currently working on generating distractor for multiple choice questions. Training set consists of question, answer and 3 distractor and I need to predict 3 distractor for test set. I have gone through many research papers regarding this but the problem in my case is unique. Here the problem is the questions and answers are for a comprehension(usually a big passage of text story) but the comprehension based on which is not given nor any supporting text is given for the question. Moreover, the answers and distractor are not a single word but sentences. The research paper I went mostly worked with some kind of support text. Even the SciQ dataset had some supporting text but the problem im working is different</p>
<p><a href=""http://www.personal.psu.edu/cul226/files/naacl18_bea_distractor.pdf"" rel=""nofollow noreferrer"">This</a> research paper was the one which I thought closely went by what I wanted and I'm planning to implement this. Below is an excerpt from the paper which the authors say worked better than NN models.</p>
<blockquote>
<p>We solve DG as the following ranking problem: Problem.
Given a candidate distractor set D and a MCQ dataset M = {(qi , ai , {di1, ..., dik})} N i=1, where qi is the question stem, ai is the key, Di = {di1...dik} ⊆ D are the distractors associated with qi and ai , find a point-wise ranking function r: (qi , ai , d) → [0, 1] for d ∈ D, such that distractors in Di are ranked higher than those in D − Di.</p>
</blockquote>
<p>My questions are a) From what I understood, The above lines says we first create a big list containing all the distractors in the dataset and then we create a pointwise ranking function with respect to all distractors for every question? So if we have n questions and d distractors. We will have a (nxd) matrix where pointwise function values range between o and 1. Also, a question's own distractors should be ranked higher than the rest. Right?</p>
<blockquote>
<p>To learn the ranking function, we investigate two types of models: feature-based models and NNbased models.</p>
<p>Feature-based Models: Given a tuple (q, a, d), a feature-based model first transforms it to a feature vector φ(q, a, d) ∈ R d with the function φ. We design the following features for DG, resulting in a 26-dimension feature vector:</p>
</blockquote>
<ul>
<li>Emb Sim. Embedding similarity between q and d and the similarity
between a and d.</li>
<li>POS Sim. Jaccard similarity between a and d’s POS tags.</li>
<li>ED. The edit distance between a and d.</li>
<li>Token Sim. Jaccard similarities between q and d’s tokens, a and d’s tokens, and q and a’s tokens.</li>
<li>Length. a and d’s character and token lengths and the difference
of lengths.</li>
<li>Suffix. The absolute and relative length of a and d’s longest
common suffix.</li>
<li>Freq. Average word frequency in a and d.</li>
<li>Single. Singular/plural consistency of a and d. This</li>
<li>Wiki Sim.</li>
</ul>
<p>My questions: Will these feature generation idea applies to both word distractors and sentence distractors? ( As per the paper, they claim it will).</p>
<p>Apart from all of these, I have other simple questions such as should I remove stopwords here?</p>
<p>I'm new to NLP. So any suggestions about which SOTA implementation would work here would be very helpful. Thanks in advance.</p>
",Vectorization & Embeddings,distractor generation multiple choice question currently working generating distractor multiple choice question training set consists question answer distractor need predict distractor test set gone many research paper regarding problem case unique problem question answer comprehension usually big passage text story comprehension based given supporting text given question moreover answer distractor single word sentence research paper went mostly worked kind support text even sciq dataset supporting text problem im working different research paper wa one thought closely went wanted planning implement excerpt paper author say worked better nn model solve dg following ranking problem problem given candidate distractor set mcq dataset qi ai di dik n qi question stem ai key di di dik distractors associated qi ai find point wise ranking function r qi ai distractors di ranked higher di question understood line say first create big list containing distractors dataset create pointwise ranking function respect distractors every question n question distractors nxd matrix pointwise function value range also question distractors ranked higher rest right learn ranking function investigate two type model feature based model nnbased model feature based model given tuple q feature based model first transforms feature vector q r function design following feature dg resulting dimension feature vector emb sim embedding similarity q similarity po sim jaccard similarity po tag ed edit distance token sim jaccard similarity q token token q token length character token length difference length suffix absolute relative length longest common suffix freq average word frequency single singular plural consistency wiki sim question feature generation idea applies word distractors sentence distractors per paper claim apart simple question remove stopwords new nlp suggestion sota implementation would work would helpful thanks advance
Counting in how many documents does a word appear,"<p>I'm trying to implement a TFIDF vectorizer without sklearn. I want to count the number of documents(list of strings) in which a  word appears, and so on for all the words in that corpus.
Example:</p>
<pre><code>corpus = [
     'this is the first document',
     'this document is the second document',
     'and this is the third one',
     'is this the first document',
]
</code></pre>
<p><strong>Desired OP:</strong> <code>{this : 4, is : 4}</code> and so on for every word</p>
<p><strong>My code:</strong></p>
<pre><code>def docs(corpus):
    doc_count = dict()
    for line in corpus:
        for word in line.split():
            if word in line:
                doc_count[word] +=1
            else:
                doc_count[word] = 1
        print(counts)

docs(corpus)
</code></pre>
<p><strong>Error I'm facing:</strong></p>
<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-70-6bf2b69708bc&gt; in &lt;module&gt;
      9         print(counts)
     10 
---&gt; 11 docs(corpus)

&lt;ipython-input-70-6bf2b69708bc&gt; in docs(corpus)
      4         for word in line.split():
      5             if word in line.split():
----&gt; 6                 doc_count[word] +=1
      7             else:
      8                 doc_count[word] = 1

KeyError: 'this'
</code></pre>
<p>Please let me know where I'm lacking and if I'm not iterating properly. Thank you!</p>
",Vectorization & Embeddings,counting many document doe word appear trying implement tfidf vectorizer without sklearn want count number document list string word appears word corpus example desired op every word code error facing please let know lacking iterating properly thank
How did online training work in the Word2vec model using Genism,"<p>Using the Genism library, we can load the model and update the vocabulary when the new sentence will be added. That’s means If you save the model you can continue training it later. I checked with sample data, let’s say I have a word in my vocabulary that was previously trained (i.e. “women”). And after that let’s say I have new sentences and using model.build_vocab(new_sentence, update=True) and model.train(new_sentence), the model is updated. Now, in my new_sentence I have some word that already exists(“women”) in the previous vocabulary list and have some new word(“girl”) that not exists in the previous vocabulary list. After updating the vocabulary, I have both old and new words in the corpus. And I checked using model.wv[‘women’], the vector is updated after update and training new sentence. Also, get the word embedding vector for a new word i.e. model.wv[‘girl’]. All other words that were previously trained and not in the new_sentence, those word vectors not changed.</p>
<pre><code>model = Word2Vec(old_sentences, vector_size=100,window=5, min_count=1) 
model.save(&quot;word2vec.model&quot;)
model = Word2Vec.load(&quot;word2vec.model&quot;) //load previously save model 
model.build_vocab(new_sentences,update=True,total_examples=model.corpus_count, epochs=model.epochs)   
model.train(new_sentences)
</code></pre>
<p>However, just don’t understand the inside depth explanation of how the online training is working. Please let me know if anybody knows in detail. I get the code but want to understand how the online training working in theoretically. Is it re-train the model on the old and new training data from scratch?</p>
<p>Here is the link that I followed: <a href=""https://rutumulkar.com/blog/2015/word2vec/"" rel=""nofollow noreferrer"">Online training</a></p>
",Vectorization & Embeddings,online training work word vec model using genism using genism library load model update vocabulary new sentence added mean save model continue training later checked sample data let say word vocabulary wa previously trained e woman let say new sentence using model build vocab new sentence update true model train new sentence model updated new sentence word already exists woman previous vocabulary list new word girl exists previous vocabulary list updating vocabulary old new word corpus checked using model wv woman vector updated update training new sentence also get word embedding vector new word e model wv girl word previously trained new sentence word vector changed however understand inside depth explanation online training working please let know anybody know detail get code want understand online training working theoretically train model old new training data scratch link followed online training
How do I create embeddings for every sentence in a list and not for the list as a whole?,"<p>I need to generate embeddings for documents in lists, calculate the Cosine Similarity between every sentence of corpus 1 with every sentence of corpus2, rank them and give out the best fit:</p>
<pre><code>embed = hub.load(&quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot;)

embeddings1 = [&quot;I'd like an apple juice&quot;,
                                &quot;An apple a day keeps the doctor away&quot;,
                                 &quot;Eat apple every day&quot;,
                                 &quot;We buy apples every week&quot;,
                                 &quot;We use machine learning for text classification&quot;,
                                 &quot;Text classification is subfield of machine learning&quot;]
embeddings1 = embed(embeddings1)

embeddings2 = [&quot;I'd like an orange juice&quot;,
                                &quot;An orange a day keeps the doctor away&quot;,
                                 &quot;Eat orange every day&quot;,
                                 &quot;We buy orange every week&quot;,
                                 &quot;We use machine learning for document classification&quot;,
                                 &quot;Text classification is some subfield of machine learning&quot;]
embeddings2 = embed(embeddings2)

print(cosine_similarity(embeddings1, embeddings2))
</code></pre>
<p>The vectors seem to work fine (due to the shape of the array) and also the calculation of the cosine similarity.
My problem is that the Universal Sentence Encoder does not give them out with the respective strings which is crucial. It always has to find the right fit and I must be able to order after the value of Cosine Similarity</p>
<pre><code>array([[ 0.7882168 ,  0.3366559 ,  0.22973989,  0.15428472, -0.10180502,
                                                         -0.04344492],
       [ 0.256085  ,  0.7713026 ,  0.32120776,  0.17834462, -0.10769081,
                                                         -0.09398925],
       [ 0.23850328,  0.446203  ,  0.62606746,  0.25242645, -0.03946173,
                                                         -0.00908459],
       [ 0.24337521,  0.35571027,  0.32963073,  0.6373588 ,  0.08571904,
                                                         -0.01240187],
       [-0.07001016, -0.12002315, -0.02002328,  0.09045915,  0.9141338 ,
                                                          0.8373743 ],
       [-0.04525191, -0.09421931, -0.00631144, -0.00199519,  0.75919366,
                                                          0.9686416 ]]
</code></pre>
<p>The goal is that the code finds out itself that the highest cosine similarity of &quot;I'd like an apple juice&quot; in the second corpus is &quot;I'd like an orange juice&quot; and matches them.</p>
<p>I tried for loops, for instance:</p>
<pre><code>for sentence in embeddings1:
    print(sentence, embed(sentence))
</code></pre>
<p>resulting in this error:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError:  input must be a vector, got shape: []
     [[{{node StatefulPartitionedCall/StatefulPartitionedCall/text_preprocessor/tokenize/StringSplit/StringSplit}}]] [Op:__inference_restored_function_body_5285]

Function call stack:
restored_function_body
</code></pre>
",Vectorization & Embeddings,create embeddings every sentence list list whole need generate embeddings document list calculate cosine similarity every sentence corpus every sentence corpus rank give best fit vector seem work fine due shape array also calculation cosine similarity problem universal sentence encoder doe give respective string crucial always ha find right fit must able order value cosine similarity goal code find highest cosine similarity like apple juice second corpus like orange juice match tried loop instance resulting error
How do I order vectors from sentence embeddings and give them out with their respective input?,"<p>I managed to generate vectors for every sentence in my two corpora and calculate the Cosine Similarity between every possible pair (dot product):</p>
<pre><code>import tensorflow_hub as hub
from sklearn.metrics.pairwise import cosine_similarity

embed = hub.load(&quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot;)

embeddings1 = [&quot;I'd like an apple juice&quot;,
                                &quot;An apple a day keeps the doctor away&quot;,
                                 &quot;Eat apple every day&quot;,
                                 &quot;We buy apples every week&quot;,
                                 &quot;We use machine learning for text classification&quot;,
                                 &quot;Text classification is subfield of machine learning&quot;]
embeddings1 = embed(embeddings1)

embeddings2 = [&quot;I'd like an orange juice&quot;,
                                &quot;An orange a day keeps the doctor away&quot;,
                                 &quot;Eat orange every day&quot;,
                                 &quot;We buy orange every week&quot;,
                                 &quot;We use machine learning for document classification&quot;,
                                 &quot;Text classification is some subfield of machine learning&quot;]
embeddings2 = embed(embeddings2)

print(cosine_similarity(embeddings1, embeddings2))

array([[ 0.7882168 ,  0.3366559 ,  0.22973989,  0.15428472, -0.10180502,
                                                         -0.04344492],
       [ 0.256085  ,  0.7713026 ,  0.32120776,  0.17834462, -0.10769081,
                                                         -0.09398925],
       [ 0.23850328,  0.446203  ,  0.62606746,  0.25242645, -0.03946173,
                                                         -0.00908459],
       [ 0.24337521,  0.35571027,  0.32963073,  0.6373588 ,  0.08571904,
                                                         -0.01240187],
       [-0.07001016, -0.12002315, -0.02002328,  0.09045915,  0.9141338 ,
                                                          0.8373743 ],
       [-0.04525191, -0.09421931, -0.00631144, -0.00199519,  0.75919366,
                                                          0.9686416 ]]
</code></pre>
<p>In order to have a meaningful output I would need to order them, then return them with the respective input sentences. Does anyone have an idea how doing that? I did not find any tutorial for that task.</p>
",Vectorization & Embeddings,order vector sentence embeddings give respective input managed generate vector every sentence two corpus calculate cosine similarity every possible pair dot product order meaningful output would need order return respective input sentence doe anyone idea find tutorial task
How do I graph to see the similarity matrix for summarizing with TextRank algorithm,"<p>I am using the TextRan algorithm for inferential text summarization.I used Glove model for word embedding.
I wrote a code that can draw a  graph to display how a similarity matrix.Although I get the same summary text every time I run the code, I see a different graph drawing each time. What is the reason of this? Is this a problem or is it normal? If it is a problem, how can I provide a solution? If you have information, I would appreciate it if you reply.</p>
<pre><code>def GloveİleSentenceEmbed(sentence_list,sentence_stem):
  word_embeddings = {} 
  f = open('/content/drive/MyDrive/MetinAnalizi/glove.6B.100d.txt', encoding='utf-8')
  for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    word_embeddings[word] = coefs
  f.close()
  sentence_vectors = []  
  for i in sentence_stem:
    if len(i) != 0:
      v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i])/(len(i)+0.001)
    else:   
      v = np.zeros((100,))
    sentence_vectors.append(v)
  return sentence_vectors

def SimilariytMatrix(sentence_list,sentence_vectors):
  similarity_matrix=np.zeros([len(sentence_list), len(sentence_list)])
  for i in range(len(sentence_list)):
    for j in range(len(sentence_list)):
      if (i!=j):
         similarity_matrix[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]
  similarity_matrix = np.round(similarity_matrix,3)
  return similarity_matrix

def PageRankAlgorithm(similarity_matrix,sentence_list):
    nx_graph = nx.from_numpy_array(similarity_matrix)
    plt.figure(figsize=(10, 10))
    pos = nx.spring_layout(nx_graph)
    nx.draw(nx_graph, with_labels=True, font_weight='bold')
    plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/NBsrD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NBsrD.png"" alt=""enter image description here"" /></a></p>
",Vectorization & Embeddings,graph see similarity matrix summarizing textrank algorithm using textran algorithm inferential text summarization used glove model word embedding wrote code draw graph display similarity matrix although get summary text every time run code see different graph drawing time reason problem normal problem provide solution information would appreciate reply
Calculate cosine similarity for between all cases in a dataframe fast,"<p>I'm working on an NLP project  where I have to compare the similarity between many sentences
E.G. from this dataframe:</p>
<p><a href=""https://i.sstatic.net/lCJDb.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/lCJDb.png"" alt=""enter image description here"" /></a></p>
<p>The first thing I tried was to make a join of the dataframe with itself to get the bellow format and compare row by row:</p>
<p><a href=""https://i.sstatic.net/KMz2H.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/KMz2H.png"" alt=""enter image description here"" /></a></p>
<p>The problem with this that I get out of memory quickly for big medium/big datasets,
e.g. for a 10k rows join I will get 100MM rows which I can not fit in ram</p>
<p>My current aproach is to iterate over the dataframe with as:</p>
<pre><code>final = pd.DataFrame()

### for each row 
for i in range(len(df_sample)):

    ### select the corresponding vector to compare with 
    v =  df_sample[df_sample.index.isin([i])][&quot;use_vector&quot;].values
    ### compare all cases agains the selected vector
    df_sample.apply(lambda x:  cosine_similarity_numba(x.use_vector,v[0])  ,axis=1)

    ### kept the cases with a similarity over a given th, in this case 0.6
    temp = df_sample[df_sample.apply(lambda x:  cosine_similarity_numba(x.use_vector,v[0])  ,axis=1) &gt; 0.6]  
    ###  filter out the base case 
    temp = temp[~temp.index.isin([i])]
    temp[&quot;original_question&quot;] = copy.copy(df_sample[df_sample.index.isin([i])][&quot;questions&quot;].values[0])
    ### append the result     
    final = pd.concat([final,temp])
</code></pre>
<p>But this aproach is not fast either.
How can I improve the performance of this process?</p>
",Vectorization & Embeddings,calculate cosine similarity case dataframe fast working nlp project compare similarity many sentence e g dataframe first thing tried wa make join dataframe get bellow format compare row row problem get memory quickly big medium big datasets e g k row join get mm row fit ram current aproach iterate dataframe aproach fast either improve performance process
Text Similarity between two dataframe of unequal size,"<p>I have two dataframe containing id and embedding of text, I wanted to check the similarity on cross dataframe. The length of data1(2000) is less than data2(0.5 million).</p>
<p>I wanted to the similarity between each row to all rows of data2 like row1 of data1 to all rows of data2 and row2 of data1 to all rows of data2 and so on.</p>
<p>For each iteration, I wanted to store the best matching and ID from both columns.</p>
<pre><code>data1
ID_1, title_embeddings
1, 'dbhbhbc jcdwc dnwc, 0.5, 0.6, 0.8, 0.8
2, 'hbwdbhbc jcdwc dnwc, 0.15, 0.65, 0.85, 0.348
..

data2
ID_2, text1, tweet_embeddings
1, 'dbhbc jcdwc dnwc, '0.5, 0.6, 0.8, 0.8
2, 'dbhbc jcdwc dnwc, 0.15, 0.65, 0.85, 0.348
3, 'dbhbnec jcdwc dnwc, 0.565, 0.346, 0.28, 0.18
4, 'dbhbc jcdwc dnwc, 0.165, 0.365, 0.785, 0.348
</code></pre>
<pre class=""lang-py prettyprint-override""><code>X=data2['title_embeddings']
Y=data1.head()

from sklearn.metrics.pairwise import cosine_similarity
from scipy import spatial

for i, row in Y.iterrows():
   print('number ' +str(i))
   sim_score=[]
   for j in range(0,len(X)):
       a= 1 - spatial.distance.cosine(row['tweet_embeddings'], X[j])
       sim_score.append(a)
       print(max(sim_score))
</code></pre>
<pre><code>Expected output
ID_1, ID_2, tweet_embeddings, sim_score
1      4    'dbhbc jcdwc dnwc, 0.5
</code></pre>
<p>Currently, I am not able to find the result with my approach.</p>
",Vectorization & Embeddings,text similarity two dataframe unequal size two dataframe containing id embedding text wanted check similarity cross dataframe length data le data million wanted similarity row row data like row data row data row data row data iteration wanted store best matching id column currently able find result approach
keras max pooling layer after embedding layer,"<p>I have a 7 word embedding, each in dimension 10, so overall I have a matrix of 7 rows and 10 columns.
After applying this embedding (using embedding layer), I get the output of dimension (None, 7, 1, 10).</p>
<p>I want to have a vector, such that its i'th element is the maximum of row i, for each i in the range of {0,6}. Hence, it should be a vector of length 7 (and dimension like (None,7)).</p>
<p>For this purpose, I first use a reshape layer, to change dimension to (7,10) (otherwise, it does not work).</p>
<p>Then, I've tried to use GlobalMaxPooling1D for this purpose (also tried MaxPooling1D), and the output does not
have the dimension that I wanted. It is  (None, 10) instead of (None,7).</p>
<p>I've checked the GlobalMaxPooling1D on a separate example, and it works well. It must be something with the additional dimension that the embedding layer pushes in the middle.</p>
<p>p.s. A similar question has been asked here:
<a href=""https://stackoverflow.com/questions/41958115/max-over-time-pooling-in-keras"">Max over time pooling in Keras</a>
But I don't think that the suggested answer there answer the original question (it seems to return the maximum of the entire matrix).</p>
<p><a href=""https://i.sstatic.net/AIHUT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AIHUT.png"" alt=""enter image description here"" /></a></p>
",Vectorization & Embeddings,kera max pooling layer embedding layer word embedding dimension overall matrix row column applying embedding using embedding layer get output dimension none want vector th element maximum row range hence vector length dimension like none purpose first use reshape layer change dimension otherwise doe work tried use globalmaxpooling purpose also tried maxpooling output doe dimension wanted none instead none checked globalmaxpooling separate example work well must something additional dimension embedding layer push middle p similar question ha asked
Gensim Compute centroid from list of words,"<p>How to compute the centroid of given 5 words from the word-embedding and then find the most similar words from that centroid. (In gensim)</p>
",Vectorization & Embeddings,gensim compute centroid list word compute centroid given word word embedding find similar word centroid gensim
Word-embedding does not provide expected relations between words,"<p>I am trying to train a word embedding to a list of repeated sentences where only the subject changes. I expected that the generated vectors corresponding the subjects provide a strong correlation after training as it is expected from a word embedding. However, the angle between the vectors of subjects is not always larger than the angle between subjects and a random word.</p>
<pre><code>Man   is going to write a very long novel that no one can read.
Woman is going to write a very long novel that no one can read.
Boy   is going to write a very long novel that no one can read.
</code></pre>
<p>The code is based on <a href=""https://raw.githubusercontent.com/pytorch/tutorials/master/beginner_source/nlp/word_embeddings_tutorial.py"" rel=""nofollow noreferrer"">pytorch tutorial</a>:</p>
<pre><code>import torch
from torch import nn
import torch.nn.functional as F
import numpy as np

class EmbedTrainer(nn.Module):
    def __init__(self, d_vocab, d_embed, d_context):
        super(EmbedTrainer, self).__init__()
        self.embed = nn.Embedding(d_vocab, d_embed)
        self.fc_1 = nn.Linear(d_embed * d_context, 128)
        self.fc_2 = nn.Linear(128, d_vocab)

    def forward(self, x):
        x = self.embed(x).view((1, -1)) # flatten after embedding
        x = self.fc_2(F.relu(self.fc_1(x)))
        x = F.log_softmax(x, dim=1)
        return x

text = &quot; &quot;.join([&quot;{} is going to write a very long novel that no one can read.&quot;.format(x) for x in [&quot;Man&quot;, &quot;Woman&quot;, &quot;Boy&quot;]])
text_split = text.split()
trigrams = [([text_split[i], text_split[i+1]], text_split[i+2]) for i in range(len(text_split)-2)]
dic = list(set(text.split()))
tok_to_ids = {w:i for i, w in enumerate(dic)}
tokens_text = text.split(&quot; &quot;)
d_vocab, d_embed, d_context = len(dic), 10, 2

&quot;&quot;&quot; Train &quot;&quot;&quot;
loss_func = nn.NLLLoss()
model = EmbedTrainer(d_vocab, d_embed, d_context)
print(model)
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)

losses = []
epochs = 10
for epoch in range(epochs):
    total_loss = 0
    for input, target in trigrams:
        tok_ids = torch.tensor([tok_to_ids[tok] for tok in input], dtype=torch.long)
        target_id = torch.tensor([tok_to_ids[target]], dtype=torch.long)
        model.zero_grad()
        log_prob = model(tok_ids)
        #if total_loss == 0: print(&quot;train &quot;, log_prob, target_id)
        loss = loss_func(log_prob, target_id)
        total_loss += loss.item()
        loss.backward()
        optimizer.step()
    print(total_loss)
    losses.append(total_loss)

embed_map = {}
for word in [&quot;Man&quot;, &quot;Woman&quot;, &quot;Boy&quot;, &quot;novel&quot;]:
    embed_map[word] = model.embed.weight[tok_to_ids[word]]
    print(word, embed_map[word])

def angle(a, b):
    from numpy.linalg import norm
    a, b = a.detach().numpy(), b.detach().numpy()
    return np.dot(a, b) / norm(a) / norm(b)

print(&quot;man.woman&quot;, angle(embed_map[&quot;Man&quot;], embed_map[&quot;Woman&quot;]))
print(&quot;man.novel&quot;, angle(embed_map[&quot;Man&quot;], embed_map[&quot;novel&quot;]))
</code></pre>
",Vectorization & Embeddings,word embedding doe provide expected relation word trying train word embedding list repeated sentence subject change expected generated vector corresponding subject provide strong correlation training expected word embedding however angle vector subject always larger angle subject random word code based pytorch tutorial
Why does KNN algorithm perform better on Word2Vec than on TF-IDF vector representation?,"<p>I am doing a project on multi-class text classification and could do with some advice.</p>
<p>I have a dataset of reviews which are classified into 7 product categories.</p>
<p>Firstly, I create a term document matrix using TF-IDF (tfidfvectorizer from sklearn). This generates a matrix of n x m where n in the number of reviews in my dataset and m is the number of features.</p>
<p>Then after splitting term document matrix into 80:20 train:test, I pass it through the K-Nearest Neighbours (KNN) algorithm and achieve an accuracy of 53%.</p>
<p>In another experiment, I used the Google News Word2Vec pretrained embedding (300 dimensional) and averaged all the word vectors for each review. So, each review consists of x words and each of the words has a 300 dimensional vector. Each of the vectors are averaged to produce one 300 dimensional vector per review.</p>
<p>Then I pass this matrix through KNN. I get an accuracy of 72%.</p>
<p>As for other classifiers that I tested on the same dataset, all of them performed better on the TF-IDF method of vectorization. However, KNN performed better on word2vec.</p>
<p>Can anyone help me understand why there is a jump in accuracy for KNN in using the word2vec method as compared to when using the tfidf method?</p>
",Vectorization & Embeddings,doe knn algorithm perform better word vec tf idf vector representation project multi class text classification could advice dataset review classified product category firstly create term document matrix using tf idf tfidfvectorizer sklearn generates matrix n x n number review dataset number feature splitting term document matrix train test pas k nearest neighbour knn algorithm achieve accuracy another experiment used google news word vec pretrained embedding dimensional averaged word vector review review consists x word word ha dimensional vector vector averaged produce one dimensional vector per review pas matrix knn get accuracy classifier tested dataset performed better tf idf method vectorization however knn performed better word vec anyone help understand jump accuracy knn using word vec method compared using tfidf method
what is the difference between len(tokenizer) and tokenizer.vocab_size,"<p>I'm trying to add a few new words to the vocabulary of a pretrained HuggingFace Transformers model. I did the following to change the vocabulary of the tokenizer and also increase the embedding size of the model:</p>
<pre><code>tokenizer.add_tokens(['word1', 'word2', 'word3', 'word4'])
model.resize_token_embeddings(len(tokenizer))
print(len(tokenizer)) # outputs len_vocabulary + 4
</code></pre>
<p>But after training the model on my corpus and saving it, I found out that the saved tokenizer vocabulary size hasn't changed. After checking again I found out that the abovementioned code does not change the vocabulary size (tokenizer.vocab_size is still the same) and only the len(tokenizer) has changed.</p>
<p>So now my question is; what is the difference between tokenizer.vocab_size and len(tokenizer)?</p>
",Vectorization & Embeddings,difference len tokenizer tokenizer vocab size trying add new word vocabulary pretrained huggingface transformer model following change vocabulary tokenizer also increase embedding size model training model corpus saving found saved tokenizer vocabulary size changed checking found abovementioned code doe change vocabulary size tokenizer vocab size still len tokenizer ha changed question difference tokenizer vocab size len tokenizer
Can we have inputs that is more than 1D in Pytorch (e.g word-embedding),"<p>Say I have some text and I want to classify them into three groups <code>food, sports, science</code>. If I have a sentence <code>I dont like to each mushrooms</code> we can use wordembedding (say 100 dimensions) to create a <code>6x100</code> matrix for this particular sentense.</p>
<p>Ususally when training a neural-network our data is a 2D array with the dimensions <code>n_obs x m_features</code></p>
<p>If I want to train a neural network on wordembedded sentences(i'm using Pytorch) then our input is 3D <code>n_obs x (m_sentences x k_words)</code></p>
<p>e.g</p>
<pre><code>#Say word-embedding is 3-dimensions
I = [1,2,3]
dont = [4,5,6]
eat = [7,8,9]
mushrooms = [10,11,12]

&quot;I dont eat mushrooms&quot; = [I,dont,eat,mushrooms] #First observation
</code></pre>
<p>Is the best way, when we have N&gt;2 dimensions, to do some kind of pooling e.g mean, or can we use the actual 2D-features as input?</p>
",Vectorization & Embeddings,input pytorch e g word embedding say text want classify three group sentence use wordembedding say dimension create matrix particular sentense ususally training neural network data array dimension want train neural network wordembedded sentence using pytorch input e g best way n dimension kind pooling e g mean use actual feature input
How do we use a Random Forest for sentence-classification using word-embedding,"<p>When we have a random forest, we have n-inputs and m-features e.g for 3 observations and 2 features we have</p>
<pre class=""lang-py prettyprint-override""><code>X = [[1,23],[0,-12],[-0.5,29]]
y = [1,0,1]
</code></pre>
<p>and we can train a RandomForest with</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.ensemble import RandomForestClassifier
model = RandomForest()
model.fit(X,y)
</code></pre>
<p>If I have made a word-embedding using, say, a 100-dimensional vector, how do we create the <code>X</code> matrice, where each input is a sentence?</p>
<p>Say we have the following 3-dimensional embedding of the words <code>[&quot;I&quot;,&quot;like&quot;,&quot;dogs&quot;,&quot;cats&quot;]</code>:</p>
<pre class=""lang-py prettyprint-override""><code>I = [-0.5,0,1]
like = [5,2,3]
dogs = [1,2,3]
cats = [3,2,1]
</code></pre>
<p>then the dataset [&quot;I like dogs&quot;,&quot;I like cats&quot;] would be</p>
<pre class=""lang-py prettyprint-override""><code>X = [
[[-0.5,0,1], [5,2,3], [1,2,3]],
[[-0.5,0,1], [5,2,3], [3,2,1]]
]
y = [&quot;dog-lover&quot;,&quot;cat-lover&quot;]
</code></pre>
<p>which a RF naturally cannot train thus giving the erropr <code>ValueError: Found array with dim 3. Estimator expected &lt;= 2.</code></p>
<p>Apart from RF might not be suitable for NLP - is there a way to do so?</p>
",Vectorization & Embeddings,use random forest sentence classification using word embedding random forest n input feature e g observation feature train randomforest made word embedding using say dimensional vector create matrice input sentence say following dimensional embedding word dataset like dog like cat would rf naturally train thus giving erropr apart rf might suitable nlp way
How to handle variable length sentences in PyTorch with Glove embedding layer?,"<p>I am building a text classifier using an RNN in PyTorch. The embeddings i'm using are GLOVE. However i am feeding variable length index references in to the model. This will lead to variable length embeddings, which i take it will not work. How do i get around this and make the embedding output the same length for all sentences?</p>
<pre><code>def forward(self, sentence):
        embeds = self.embedding(sentence)
        hidden = self.__init__hidden(size) 
        output, hidden = self.rnn(embeds, hidden)
        out = self.hidden2out(output)
</code></pre>
<p>Also, if someone could tell me how to choose the hidden layer size that would be great.</p>
",Vectorization & Embeddings,handle variable length sentence pytorch glove embedding layer building text classifier using rnn pytorch embeddings using glove however feeding variable length index reference model lead variable length embeddings take work get around make embedding output length sentence also someone could tell choose hidden layer size would great
Is there an NLP framework for extracting text from larger content,"<p>I have some technical documents that I need to extract the text from regarding a specific set of procedures.  Is there an easy off-the-shelf way to 'show' a language model the text to be extracted and each of the documents and then have it extract that text programmatically?</p>
<p>I was thinking of taking each paragraph and taking like a mean of all the word embeddings within the paragraph to create a 'paragraph embedding' and essentially comparing those to the 'paragraph embeddings' of the training set extracted text but I didn't know if there was a more robust way of doing that.</p>
",Vectorization & Embeddings,nlp framework extracting text larger content technical document need extract text regarding specific set procedure easy shelf way show language model text extracted document extract text programmatically wa thinking taking paragraph taking like mean word embeddings within paragraph create paragraph embedding essentially comparing paragraph embeddings training set extracted text know wa robust way
Word2vec on documents each one containing one sentence,"<p>I have some unsupervised data (100.000 files) and each file has a paragraph containing one sentence. The preprocessing went wrong and deleted all stop points (.).
I used word2vec on a small sample (2000 files) and it treated each document as one sentence.
Should I continue the process on all remaining files? Or this would result to a bad model ?</p>
<p>Thank you</p>
",Vectorization & Embeddings,word vec document one containing one sentence unsupervised data file file ha paragraph containing one sentence preprocessing went wrong deleted stop point used word vec small sample file treated document one sentence continue process remaining file would result bad model thank
Randomly select vector in gensim word2vec,"<p>I trained a word2vec model using gensim and I want to randomly select vectors from it, and find the corresponding word.
What is the best what to do so?</p>
",Vectorization & Embeddings,randomly select vector gensim word vec trained word vec model using gensim want randomly select vector find corresponding word best
How to use BERT and Elmo embedding with sklearn,"<p>I created a text classifier that uses Tf-Idf using sklearn, and I want to use BERT and Elmo embedding instead of Tf-Idf.</p>
<p>How would one do that ?</p>
<p>I'm getting Bert embedding using the code below:</p>
<pre class=""lang-py prettyprint-override""><code>from flair.data import Sentence
from flair.embeddings import TransformerWordEmbeddings

# init embedding
embedding = TransformerWordEmbeddings('bert-base-uncased')

# create a sentence
sentence = Sentence('The grass is green .')

# embed words in sentence
embedding.embed(sentence)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression

column_trans = ColumnTransformer([
    ('tfidf', TfidfVectorizer(), 'text'),
    ('number_scaler', MinMaxScaler(), ['number'])
])

# Initialize data
data = [
    ['This process, however, afforded me no means of.', 20, 1],
    ['another long description', 21, 1],
    ['It never once occurred to me that the fumbling', 19, 0],
    ['How lovely is spring As we looked from Windsor', 18, 0]
]

# Create DataFrame
df = pd.DataFrame(data, columns=['text', 'number', 'target'])

X = column_trans.fit_transform(df)
X = X.toarray()
y = df.loc[:, &quot;target&quot;].values

# Perform classification

classifier = LogisticRegression(random_state=0)
classifier.fit(X, y)
</code></pre>
",Vectorization & Embeddings,use bert elmo embedding sklearn created text classifier us tf idf using sklearn want use bert elmo embedding instead tf idf would one getting bert embedding using code
How to explain gensim word2vec output?,"<p>I run the following code and just wonder why the top 3 most similar words for &quot;exposure&quot; don't include &quot;charge&quot; and &quot;lend&quot;?</p>
<pre><code>from gensim.models import Word2Vec
corpus = [['total', 'exposure', 'charge', 'lend'],
          ['customer', 'paydown', 'rate', 'months', 'month']]
gens_mod = Word2Vec(corpus, min_count=1, vector_size=300, window=2, sg=1, workers=1, seed=1)
keyword=&quot;exposure&quot;
gens_mod.wv.most_similar(keyword)

Output:
[('customer', 0.12233059108257294),
 ('month', 0.008674687705934048),
 ('total', -0.011738087050616741),
 ('rate', -0.03600010275840759),
 ('months', -0.04291829466819763),
 ('paydown', -0.044823747128248215),
 ('lend', -0.05356598272919655),
 ('charge', -0.07367636263370514)]
</code></pre>
",Vectorization & Embeddings,explain gensim word vec output run following code wonder top similar word exposure include charge lend
Python package to extract sentence from a textfile based on keyword,"<p>I need a python package that could get the related sentence from a text, based on the keywords provided.</p>
<p>For example, below is the Wikipedia page of J.J Oppenheimer -</p>
<pre><code>Early life

Childhood and education
J. Robert Oppenheimer was born in New York City on April 22, 1904,[note 1][7] to Julius Oppenheimer, a wealthy Jewish textile importer who had immigrated to the United States from Germany in 1888, and Ella Friedman, a painter. 
Julius came to the United States with no money, no baccalaureate studies, and no knowledge of the English language. He got a job in a textile company and within a decade was an executive with the company. Ella was from Baltimore.[8] The Oppenheimer were non-observant Ashkenazi Jews.[9] 

The first atomic bomb was successfully detonated on July 16, 1945, in the Trinity test in New Mexico. 
Oppenheimer later remarked that it brought to mind words from the Bhagavad Gita: &quot;Now I am become Death, the destroyer of worlds.
</code></pre>
<p><strong>If my passed string is</strong> - &quot;<em>JJ Oppenheimer birth date</em>&quot;, it should return &quot;<strong>J. Robert Oppenheimer was born in New York City on April 22, 1904</strong>&quot;</p>
<p><strong>If my passed string is</strong> - &quot;JJ Openheimer Trinity test&quot;, it should return &quot;<strong>The first atomic bomb was successfully detonated on July 16, 1945, in the Trinity test in New Mexico</strong>&quot;</p>
<p>I tried searching a lot but nothing comes closer to what I want and I don't know much about NLP vectorization techniques. It would be great if someone please suggest some package if they know(or exist).</p>
",Vectorization & Embeddings,python package extract sentence textfile based keyword need python package could get related sentence text based keywords provided example wikipedia page j j oppenheimer passed string jj oppenheimer birth date return j robert oppenheimer wa born new york city april passed string jj openheimer trinity test return first atomic bomb wa successfully detonated july trinity test new mexico tried searching lot nothing come closer want know much nlp vectorization technique would great someone please suggest package know exist
words not available in corpus for Word2Vec training,"<p>I am totally new to Word2Vec. I want to find cosine similarity between word pairs in my data. My codes are as follows:</p>
<pre><code>import pandas as pd
from gensim.models import Word2Vec
model = Word2Vec(corpus_file=&quot;corpus.txt&quot;, sg=0, window =7, size=100, min_count=10, iter=4)
vocabulary = list(model.wv.vocab)
data=pd.read_csv(&quot;experiment.csv&quot;)
cos_similarity = model.wv.similarity(data['word 1'], data['word 2'])
</code></pre>
<p>The problem is some words in the data columns of my &quot;experiment.csv&quot; file: &quot;word 1&quot; and &quot;word 2&quot; are not present in the corpus file (&quot;corpus.txt&quot;). So this error is returned:</p>
<pre><code>&quot;word 'setosa' not in vocabulary&quot;
</code></pre>
<p>What should I do to handle words that are not present in my input corpus? I want to assign words in my experiment that are not present in the input corpus the vector zero, but I am stuck how to do it.</p>
<p>Any ideas for my problems?</p>
",Vectorization & Embeddings,word available corpus word vec training totally new word vec want find cosine similarity word pair data code follows problem word data column experiment csv file word word present corpus file corpus txt error returned handle word present input corpus want assign word experiment present input corpus vector zero stuck idea problem
BERT: Weights of input embeddings as part of the Masked Language Model,"<p>I looked through different implementations of BERT's Masked Language Model.
For pre-training there are <strong>two</strong> common versions:</p>
<ol>
<li>Decoder would simply take the final embedding of the [MASK]ed token and pass it throught a linear layer (without any modifications):</li>
</ol>
<pre><code>    class LMPrediction(nn.Module):
        def __init__(self, hidden_size, vocab_size):
            super().__init__()
            self.decoder = nn.Linear(hidden_size, vocab_size, bias = False)
            self.bias = nn.Parameter(torch.zeros(vocab_size))
            self.decoder.bias = self.bias
        def forward(self, x):
             return self.decoder(x)
</code></pre>
<ol start=""2"">
<li>Some implementations would use the weights of the input embeddings as weights of the decoder-linear-layer:</li>
</ol>
<pre><code>    class LMPrediction(nn.Module):
        def __init__(self, hidden_size, vocab_size, embeddings):
            super().__init__()
            self.decoder = nn.Linear(hidden_size, vocab_size, bias = False)
            self.bias = nn.Parameter(torch.zeros(vocab_size))
            self.decoder.weight = embeddings.weight ## &lt;- THIS LINE
            self.decoder.bias = self.bias
        def forward(self, x):
             return self.decoder(x)
</code></pre>
<p>Which one is correct? Mostly, I see the first implementation. However, the second one makes sense as well - but I cannot find it mentioned in any papers (I would like to see if the second version is somehow superior to the first one)</p>
",Vectorization & Embeddings,bert weight input embeddings part masked language model looked different implementation bert masked language model pre training two common version decoder would simply take final embedding mask ed token pas throught linear layer without modification implementation would use weight input embeddings weight decoder linear layer one correct mostly see first implementation however second one make sense well find mentioned paper would like see second version somehow superior first one
NLP Transformers: Best way to get a fixed sentence embedding-vector shape?,"<p>I'm loading a language model from torch hub (<a href=""https://camembert-model.fr/#about"" rel=""nofollow noreferrer"">CamemBERT</a> a French RoBERTa-based model) and using it do embed some french sentences:  </p>

<pre class=""lang-py prettyprint-override""><code>import torch
camembert = torch.hub.load('pytorch/fairseq', 'camembert.v0')
camembert.eval()  # disable dropout (or leave in train mode to finetune)


def embed(sentence):
   tokens = camembert.encode(sentence)
   # Extract all layer's features (layer 0 is the embedding layer)
   all_layers = camembert.extract_features(tokens, return_all_hiddens=True)
   embeddings = all_layers[0]
   return embeddings

# Here we see that the shape of the embedding vector depends on the number of tokens in the sentence

u = embed(sentence=""Bonjour, ça va ?"")
u.shape # torch.Size([1, 7, 768])
v = embed(sentence=""Salut, comment vas-tu ?"")
v.shape # torch.Size([1, 9, 768])

</code></pre>

<p>Imagine now in order to do some <strong>semantic search</strong>, I want to calculate the <code>cosine distance</code> between the vectors (tensors in our case) <code>u</code> and <code>v</code> : </p>

<pre class=""lang-py prettyprint-override""><code>cos = torch.nn.CosineSimilarity(dim=1)
cos(u, v) # will throw an error since the shape of `u` is different from the shape of `v`
</code></pre>

<p>I'm asking what is the best method to use in order to always get the <strong>same embedding shape</strong> for a sentence <strong>regardless the count of its tokens</strong>?</p>

<p>=> The first solution I'm thinking of is calculating the <code>mean on axis=1</code> (embedding of a sentence is the mean embedding its tokens) since axis=0 and axis=2 have always the same size:</p>

<pre class=""lang-py prettyprint-override""><code>cos = torch.nn.CosineSimilarity(dim=1)
cos(u.mean(axis=1), v.mean(axis=1)) # works now and gives 0.7269
</code></pre>

<p>But, I'm afraid that I'm hurting the embedding of the sentence when calculating the mean since it gives the same weight for each token (maybe multiplying by <strong>TF-IDF</strong>?).</p>

<p>=> The second solution is to pad shorter sentences out. That means:  </p>

<ul>
<li>giving a list of sentences to embed at a time (instead of embedding sentence by sentence)</li>
<li>look up for the sentence with the longest tokens and embed it, get its shape <code>S</code></li>
<li>for the rest of sentences embed then pad zero to get the same shape <code>S</code> (the sentence has 0 in the rest of dimensions)</li>
</ul>

<p>What are your thoughts?
What other techniques would you use and why?</p>

<p>Thanks in advance!</p>
",Vectorization & Embeddings,nlp transformer best way get fixed sentence embedding vector shape loading language model torch hub camembert french roberta based model using embed french sentence imagine order semantic search want calculate vector tensor case asking best method use order always get embedding shape sentence regardless count token first solution thinking calculating embedding sentence mean embedding token since axis axis always size afraid hurting embedding sentence calculating mean since give weight token maybe multiplying tf idf second solution pad shorter sentence mean giving list sentence embed time instead embedding sentence sentence look sentence longest token embed get shape rest sentence embed pad zero get shape sentence ha rest dimension thought technique would use thanks advance
Multi-class text classification with one training example per class,"<p>I am trying to solve a multi-class single-label document classification problem assigning a single class to a document. Documents are domain-specific technical documents, with technical terms:</p>
<ul>
<li>Train: I have 19 classes with a single document in each class.</li>
<li>Target: I have 77 documents without labels I want to classify to the 19 known classes.</li>
<li>Documents have between 60-3000 tokens after pre-processing.</li>
<li>My entire corpus (19+77 documents) have 65k terms (uni/bi/tri-grams) with 4.5k terms in common (between train and target)</li>
</ul>
<p>Currently, I am vectorizing documents using a tf-idf vectorizer and reducing dimensions to common terms. Then doing a cosine similarity between train and target.</p>
<p>I am wondering if there is a better way? I cannot use sklearn classifiers due to a single document in each class in train. Any ideas on a possible improvement/direction? Especially:</p>
<ul>
<li>Does it make sense to use word-embeddings/doc2vec given the small corpus?</li>
<li>Does it make sense to generate synthetic train data from the terms in the training set?</li>
<li>Any other ideas?</li>
</ul>
<p>Thanks in advance!</p>
",Vectorization & Embeddings,multi class text classification one training example per class trying solve multi class single label document classification problem assigning single class document document domain specific technical document technical term train class single document class target document without label want classify known class document token pre processing entire corpus document k term uni bi tri gram k term common train target currently vectorizing document using tf idf vectorizer reducing dimension common term cosine similarity train target wondering better way use sklearn classifier due single document class train idea possible improvement direction especially doe make sense use word embeddings doc vec given small corpus doe make sense generate synthetic train data term training set idea thanks advance
Is there a way to use bert-large as a text classification tool without fine-tuning?,"<p>I'm currently have a task of converting a keras BERT-based model for any text classification problem to the .pb file. For this I already have a function, that takes in the keras model, but the point is that when I'm trying to download any pre-trained versions of BERT they always end up without any top layers for classification, hence I should manually add <code>tf.keras.layers.Input</code> layers before and any neural network architecture above the BERT (after [CLS]'s embedding). My goal is ultimately escape the need for fine-tuning and get some ready model, that has already been fine-tuned. I've found that transformer library might be useful for this, as they have some BERT-based models ready for some datasets. Anyway, using the following code from their documentation gives back the tensor of shape 1 by number of tokens by hidden dimensionality.</p>
<pre><code>from transformers import BertTokenizer, TFBertModel
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')
model = TFBertModel.from_pretrained(&quot;bert-large-uncased&quot;)
text = &quot;Replace me by any text you'd like.&quot;
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
</code></pre>
<p>So, I eventually have to find some dataset and do fine-tuning. Even usage of models like distilbert-base-uncased-finetuned-sst-2-english still produce the embedding for each input token. Is there a way of getting ready model?</p>
",Vectorization & Embeddings,way use bert large text classification tool without fine tuning currently task converting kera bert based model text classification problem pb file already function take kera model point trying download pre trained version bert always end without top layer classification hence manually add layer neural network architecture bert cl embedding goal ultimately escape need fine tuning get ready model ha already fine tuned found transformer library might useful bert based model ready datasets anyway using following code documentation give back tensor shape number token hidden dimensionality eventually find dataset fine tuning even usage model like distilbert base uncased finetuned sst english still produce embedding input token way getting ready model
comparing text similarity using Sequence Matcher: why do results differ for lowercase and uppercase strings?,"<p>I have a set of text (alphanumeric) in a pandas dataframe and I would like to calculate the similarity scores for pairs of text (e.g. text 1 and 2, 2 and 3, 3 and 4...).</p>
<p>I am using Sequence Matcher for the calculations and was able to get a score.  I thought it would also be worth comparing text after they have been converted into lowercase. I created 2 score columns to compare the results:
score 1 = similarity score from pairs of non-lowercase text (original case of text retained)
score 2 = similarity score from pairs of lowercase text.</p>
<p>I expected for the similarity scores for the lowercase text pairs (score 2) to be higher, since there would be more characters matching (e.g. &quot;This SamPle pHrase&quot; would match with &quot;this sample phrase&quot; therefore higher similarity). However, I found in some instances that score 1 is way higher than score 2. Would any of you have ideas on why this might be the case?</p>
<p>I have tried to search in python documentation and googled for possible reasons, but could not find any. What am I missing? I'd like to understand this more, I'd appreciate any ideas/suggestions! Cheers</p>
<p>Btw I used str.lower to convert the text in the dataframe into lowercase.</p>
",Vectorization & Embeddings,comparing text similarity using sequence matcher result differ lowercase uppercase string set text alphanumeric panda dataframe would like calculate similarity score pair text e g text using sequence matcher calculation wa able get score thought would also worth comparing text converted lowercase created score column compare result score similarity score pair non lowercase text original case text retained score similarity score pair lowercase text expected similarity score lowercase text pair score higher since would character matching e g sample phrase would match sample phrase therefore higher similarity however found instance score way higher score would idea might case tried search python documentation googled possible reason could find missing like understand appreciate idea suggestion cheer btw used str lower convert text dataframe lowercase
Is there any NLP question answering dataset with multiple answers?,"<p>I'm building a QA machine. I have a problem that one question maybe have multiple answers, and the answers are located in different position in context. For example:</p>
<p><strong>Question</strong>: What does Chris have to do?</p>
<p><strong>Context</strong>: ....Chris have to wash dishes....(more text)....Chris have to do his homework....</p>
<p><strong>Correct answers</strong>:</p>
<ul>
<li>wash dishes</li>
<li>do homework</li>
</ul>
<p>When I got the answers out for a question, I use a clustering algorithm to deduplicate and get &quot;separate&quot; answers. Therefore, I need a dataset having some pair of 1 question - many answers like above to evaluate my clustering algorithm and sentence embedding model.</p>
<p>Is there any public dataset that support a pair of one question - multiple correct answers (not duplicated)? I tried MS MARCO but most of multiple answers in this dataset are duplicated.</p>
",Vectorization & Embeddings,nlp question answering dataset multiple answer building qa machine problem one question maybe multiple answer answer located different position context example question doe chris context chris wash dish text chris homework correct answer wash dish homework got answer question use clustering algorithm get separate answer therefore need dataset pair question many answer like evaluate clustering algorithm sentence embedding model public dataset support pair one question multiple correct answer duplicated tried marco multiple answer dataset duplicated
Python: How to pass Dataframe Columns as parameters to a function?,"<p>I have a dataframe <code>df</code> with 2 columns of text embeddings namely <code>embedding_1</code> and <code>embedding_2</code>. I want to create a third column in <code>df</code> named <code>distances</code> which should contain the cosine_similarity between every row of <code>embedding_1</code> and <code>embedding_2</code>.</p>
<p>But when I try to implement this using the following code I get a <code>ValueError</code>.</p>
<p>How to fix it?</p>
<p><strong>Dataframe <code>df</code></strong></p>
<pre><code>           embedding_1              |            embedding_2                                 
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]
 [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]
</code></pre>
<p><strong>Code to Calculate Cosine Similarity</strong></p>
<pre><code>df['distances'] = cosine_similarity(df['embeddings_1'], df['embeddings_2'])
</code></pre>
<p><strong>Error</strong></p>
<pre><code>ValueError: setting an array element with a sequence.
</code></pre>
<p><strong>Required Dataframe</strong></p>
<pre><code>       embedding_1              |            embedding_2                 |  distances                        
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]   |    0.427
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]   |    0.673
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]  |    0.882
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]     |    0.665
 [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]    |    0.312
</code></pre>
",Vectorization & Embeddings,python pas dataframe column parameter function dataframe column text embeddings namely want create third column named contain cosine similarity every row try implement using following code get fix dataframe code calculate cosine similarity error required dataframe
&#39;FastText&#39; object has no attribute &#39;vocab&#39;,"<p>I want to use Fasttext in my program, but that error prevent me to do it. I want to create embedding matrix for my program, with following code:</p>
<pre><code>model = gensim.models.fasttext.load_facebook_model(EMBEDDING_FILE)

EMBEDDING_DIM = 300
nb_words = min(MAX_NB_WORDS, len(word_index)) + 1
embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))
for word, i in word_index.items():
    if word in model.vocab:
        embedding_matrix[i] = model.word_vec(word)
print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))

</code></pre>
",Vectorization & Embeddings,fasttext object ha attribute vocab want use fasttext program error prevent want create embedding matrix program following code
Scale cosine distance to 0-1 using Gensim,"<p>I've built a Doc2Vec model with around 3M documents, now I want to compare it to another model I've previously built. The second model has been scaled to 0-1 so I now also want to scale the gensim model to the same range so that they are comparable.
This is my first time using gensim so I'm not sure how this is done. It's nothing fancy but this is the code I have so far (model generation code ommited). I thought about scaling (minmax scaling with max/min in the union of  vectors) the inferred vectors (v1 and v2) but I don't think this would be correct approach.
The idea here is to compare two documents (with tokens likely to be in the corpus) and output a similarity score between them. I've seen a few Gensim's tutorials and they often compare a single string to the corpus' documents, which is not really the idea here.</p>
<blockquote>
<pre><code> def get_similarity_score(self,string_1, string_2):
    split_tokens1 = string_1.split()
    split_tokens2 = string_2.split()
    v1 = self.model.infer_vector(split_tokens1)
    v2 = self.model.infer_vector(split_tokens2)
    text_score = nltk.cluster.util.cosine_distance(v1, v2)
    return text_score
</code></pre>
</blockquote>
<p>Any recommendations?</p>
",Vectorization & Embeddings,scale cosine distance using gensim built doc vec model around document want compare another model previously built second model ha scaled also want scale gensim model range comparable first time using gensim sure done nothing fancy code far model generation code ommited thought scaling minmax scaling max min union vector inferred vector v v think would correct approach idea compare two document token likely corpus output similarity score seen gensim tutorial often compare single string corpus document really idea recommendation
create representation of questions using LSTM via a pre-trained word embedding such as GloVe,"<p>I am new in LSTM and python. My goal is to represent the sentence using LSTM.
Could u tell me I am doing the right? how to fix the error when running the following code ?</p>
<p>&quot;TypeError: embedding(): argument 'indices' (position 2) must be Tensor, not str&quot;</p>
<pre><code>import torch
import torch.nn as nn
import numpy as np
from torch import optim
from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence
import torchvision.datasets as datasets  # Standard datasets
import torchvision.transforms as transforms
import json

class RNN_LSTM(nn.Module)
    
    def __init__(self, input_size, hidden_size, num_layers, num_classes, vocab_size,
                 lstm_dropout, device, word_emb_file):
        super(RNN_LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm_dropout = lstm_dropout
        self.lstm_drop = nn.Dropout(p=self.lstm_dropout)
        self.word_emb_file = word_emb_file
        self.device = device
        
        # initialize text embeddings
        self.word_embeddings = nn.Embedding(vocab_size, input_size)
        self.word_embeddings.weight = nn.Parameter(
                torch.from_numpy(
                    np.pad(np.load(self.word_emb_file), ((0, 1), (0, 0)), 'constant')).type(
                    'torch.FloatTensor'))
        
    
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)   
    
    def forward(self, sentence, question_len):
        embeds = self.word_embeddings(sentence)
        packed_output = pack_padded_sequence(embeds, question_len, batch_first=True)
        outputs, (hidden, cell_state) = self.lstm(packed_output)
        outputs, outputs_length = pad_packed_sequence(outputs, batch_first=True)
        outputs = torch.cat([hidden[0,:,:], hidden[1,:,:]], dim=-1)
        return outputs

lstm_dropout = 0.3
input_size = 300
hidden_size = 256
num_classes = 10
num_layers = 1
device = 'cpu'
vocab_size = 2000
word_emb_file = &quot;/home/project/word_emb_300d.npy&quot;

model = RNN_LSTM(input_size, hidden_size, num_layers, num_classes, vocab_size, lstm_dropout, device, word_emb_file)

model.word_embeddings('Blue Skye')
</code></pre>
",Vectorization & Embeddings,create representation question using lstm via pre trained word embedding glove new lstm python goal represent sentence using lstm could u tell right fix error running following code typeerror embedding argument index position must tensor str
Interpreting the output tokenization of BERT for a given word,"<pre class=""lang-py prettyprint-override""><code>from bert_embedding import BertEmbedding
bert_embedding = BertEmbedding(model='bert_12_768_12', dataset_name='wiki_multilingual_cased')
output = bert_embedding(&quot;any&quot;)
</code></pre>
<p>I need clarification on the output of mBERT embeddings. I'm aware that WordPiece tokenization is used to break up the input text. Also I observed that on providing a single word (say &quot;any&quot;) as input, the output has length equal to the number of characters in the input (in our case, 3). <code>output[i]</code> is a tuple of lists where the first list contains the character at i<sup>th</sup>  position with the 'unknown' token preceding and following it as different elements in the array. Following this are three (= length of the input word) arrays (embeddings) of size 768 each. Why does the output seem to be tokenized character-wise (rather than wordpiece tokenized)?</p>
<p>Also found out the output form changes when the input is given in a list as:<code>bert_embedding([&quot;any&quot;])</code>. The output now is a single tuple with ['[UNK]', 'state', '[UNK]'] as the first element followed by three different embeddings conceivably corresponding to the three tokens listed above.</p>
<p>If I need the embedding of the last subword (not simply of the last character or the whole word) for a given input word, how do I access it?</p>
",Vectorization & Embeddings,interpreting output tokenization bert given word need clarification output mbert embeddings aware wordpiece tokenization used break input text also observed providing single word say input output ha length equal number character input case tuple list first list contains character ith position unknown token preceding following different element array following three length input word array embeddings size doe output seem tokenized character wise rather wordpiece tokenized also found output form change input given list output single tuple unk state unk first element followed three different embeddings conceivably corresponding three token listed need embedding last subword simply last character whole word given input word access
Combining vectors in Gensim Word2Vec vocabulary,"<p>Gensim Word2Vec Model has a great method which allows you to find the top n most similar words in the models vocabulary given a list of positive words and negative words.</p>
<pre><code>wv.most_similar(positive=['word1', 'word2', 'word3'], 
                negative=['word4','word5'], topn=10)
</code></pre>
<p>What I am looking to do is create word vector that represents an averaged or summed vector of the input positive and negative words. I am hoping to use this new vector to compare to other vectors.
Something like this:</p>
<pre><code>newVector = 'word1' + 'word2' + 'word3' - 'word4' - 'word5'
</code></pre>
<p>I know that vectors can be summed, but I am not sure if that is the best option. I am hoping to find out exactly how the above function (most_similar) combines the positive vectors and negative vectors, and if Gensim has a function to do so. Thank you in advance.</p>
",Vectorization & Embeddings,combining vector gensim word vec vocabulary gensim word vec model ha great method allows find top n similar word model vocabulary given list positive word negative word looking create word vector represents averaged summed vector input positive negative word hoping use new vector compare vector something like know vector summed sure best option hoping find exactly function similar combine positive vector negative vector gensim ha function thank advance
Error when I add an Embedding Layer to my ANN(Keras Functional API),"<p>Error looks something like this,</p>
<p><em><strong>InvalidArgumentError</strong></em>:  indices[14,1] = -34 is not in [0, 6505)
[[node model_12/embedding_16/embedding_lookup (defined at :3) ]] [Op:__inference_train_function_14552]
Errors may have originated from an input operation.
Input Source operations connected to node model_12/embedding_16/embedding_lookup:
model_12/embedding_16/embedding_lookup/13394
Function call stack:
train_function</p>
<p>Code:</p>
<pre><code>input_tensor = Input(shape=(train_X_ann.shape[1],))

x = layers.Embedding(6505, 300, input_length=max_length,weights=[embedding_matrix], trainable=False)(input_tensor)
y = layers.Flatten()(x)
y1 = layers.Dense(units=units,kernel_initializer=kernel_initializer)(y)
y = layers.BatchNormalization()(y1)
y = layers.Activation(activation)(y)
y = layers.Dropout(rate=drop_rate1)(y)```

Shape of the embedding matrix is (6505,300)

Shape of_train_x_ann (64687,3594)
And it looks something like this
[Snapshot of my training data which includes features too][1]


  [1]: https://i.sstatic.net/1KeQM.png
</code></pre>
",Vectorization & Embeddings,error add embedding layer ann kera functional api error look something like invalidargumenterror index node model embedding embedding lookup defined op inference train function error may originated input operation input source operation connected node model embedding embedding lookup model embedding embedding lookup function call stack train function code
what does &#39;corpus_count&#39; in gensim word2vec?,"<p>I want to train my word Embedding from scratch and I use gensim.models.word2vec as my model.
My corpus is so large that I can not read it at once , so I divide my corpus file into <strong>many parts</strong> and train my model iteratively。I find this is helpful:</p>
<pre><code>train(corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)
</code></pre>
<h2>I confused about the parameter &quot;total_words&quot; .
Is it means total words of all my corpus or the part corpus trained now?</h2>
<p>UPDATE:</p>
<p>my code is like this:</p>
<pre><code>model =  gensim.models.word2vec.Word2Vec.load(init_model)  
for i in range(parts):
    model.build_vocab(corpus_file=this_part_file_name, update=True)
    model.train(corpus_file = this_part_file_name, 
                   total_words=word_count(this_part_file_name) )

</code></pre>
<p>Should the parameter total_words be <code>word_count(this_part_file_name)</code> or <code>word_count(ALL_my_corpus_file)</code> ?</p>
",Vectorization & Embeddings,doe corpus count gensim word vec want train word embedding scratch use gensim model word vec model corpus large read divide corpus file many part train model iteratively find helpful confused parameter total word mean total word corpus part corpus trained update code like parameter total word
Get Bert Embeddings for every Token in a Sentence,"<p>I have a dataframe in python in which i have a column of textual data. I need to run a loop where i would take each row in that textual column and get the bert embedding for every token in that particular row. I then need to append those vector embeddings and try it out for some purpose.</p>
<p>e.g &quot; My name is Obama&quot;
get 768 vector embedding for 'My'
get 768 vector embedding for 'name'
get 768 vector embedding for 'is'
get 768 vector embedding for 'Obama'</p>
<p>final output: vector embedding of size 768*4 = 3072
assume every row has exact number of words present</p>
",Vectorization & Embeddings,get bert embeddings every token sentence dataframe python column textual data need run loop would take row textual column get bert embedding every token particular row need append vector embeddings try purpose e g name obama get vector embedding get vector embedding name get vector embedding get vector embedding obama final output vector embedding size assume every row ha exact number word present
How to implement LIME in a Bert model?,"<p>I am new to machine learning. I noticed that such questions have been asked before as well but did not receive a proper solution. Below is the code for semantic similarity and I want to implement LIME as a base. Please, help me out.</p>
<pre><code>from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('paraphrase-distilroberta-base-v1')

# Two lists of sentences
sentences1 = ['The cat sits outside',
             'A man is playing guitar',
             'The new movie is awesome']

sentences2 = ['The cat sits outside',
              'A woman watches TV',
              'The new movie is so great']

#Compute embedding for both lists
embeddings1 = model.encode(sentences1, convert_to_tensor=True)
embeddings2 = model.encode(sentences2, convert_to_tensor=True)

#Compute cosine-similarits
cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)

#Output the pairs with their score
for i in range(len(sentences1)):
    print(&quot;{} \t\t {} \t\t Score: {:.4f}&quot;.format(sentences1[i], sentences2[i], cosine_scores[i][i]))
</code></pre>
",Vectorization & Embeddings,implement lime bert model new machine learning noticed question asked well receive proper solution code semantic similarity want implement lime base please help
Gensim: word mover distance with string as input instead of list of string,"<p>I'm trying to find out how similar are 2 sentences.
For doing it i'm using gensim word mover distance and since what i'm trying to find it's a similarity i do like it follow:</p>
<pre><code>sim = 1 - wv.wmdistance(sentence_obama, sentence_president)
</code></pre>
<p>What i give as an input are 2 strings:</p>
<pre><code>    sentence_obama = 'Obama speaks to the media in Illinois'
    sentence_president = 'The president greets the press in Chicago'
</code></pre>
<p>The model i'm using is the one that you can find on the web: word2vec-google-news-300
I load it with this code:</p>
<pre><code>wv = api.load(&quot;word2vec-google-news-300&quot;)
</code></pre>
<p>It give me reasonable results.
Here it's where the problem starts.
For what i can read from the documentation <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html"" rel=""nofollow noreferrer"">here</a> it seems the wmd take as input a list of string and not a string like  i do!</p>
<pre><code>def preprocess(sentence):
   return [w for w in sentence.lower().split() if w not in stop_words]

sentence_obama = preprocess(sentence_obama)
sentence_president = preprocess(sentence_president)
sim = 1 - wv.wmdistance(sentence_obama, sentence_president)
</code></pre>
<p>When i follow the documentation i get results really different:</p>
<pre><code>wmd using string as input: 0.5562025871542842
wmd using list of string as input: -0.0174646259300113
</code></pre>
<p>I'm really confused. Why is it working with string as input and it works better than when i give what the documentation is asking for?</p>
",Vectorization & Embeddings,gensim word mover distance string input instead list string trying find similar sentence using gensim word mover distance since trying find similarity like follow give input string model using one find web word vec google news load code give reasonable result problem start read documentation seems wmd take input list string string like follow documentation get result really different really confused working string input work better give documentation asking
how do i improve my nlp model to classify 4 different mental illness?,"<p>I have a dataset in csv containing 2 columns: 1 is the label which determines the type of mental illness of the patient and the other is the corresponding reddit posts from a certain time period of that user.
These are the total number of patients in each group of illness:</p>
<ol>
<li>control:          3000</li>
<li>depression:       2118</li>
<li>bipolar:          1062</li>
<li>ptsd:              330</li>
<li>schizophrenia:     148</li>
</ol>
<p>for starters I tried binary classification between my depression and bipolar patients. I used tfidf vectors and fed it into 2 different types of classifiers: MultinomialNB and SVM.
here is a sample of the code:
using MultinomialNB:</p>
<pre><code>text_clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])
text_clf = text_clf.fit(x_train, y_train)
</code></pre>
<p>using SVM:</p>
<pre><code>text_clf_svm = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42)),])
text_clf_svm = text_clf_svm.fit(x_train, y_train)
</code></pre>
<p>these are my results:</p>
<pre><code>              precision    recall  f1-score   support

 bipolar       0.00      0.00      0.00       304

depression     0.68      1.00      0.81       650

accuracy                           0.68       954


macro avg       0.34      0.50      0.41       954

weighted avg    0.46      0.68      0.55       954
</code></pre>
<p>The problem is the models are simply just predicting all the patients to be in the class of the larger data sample, in this case all are being predicted as depressed patients. I have tried using BERT as well but I get the same accuracy. I have read papers on them using LIWC lexicon, These categories include variables that characterize linguistic style as well as psychological aspects of language.
I don't understand if what I am doing is correct or is there a better way at classifying using NLP, if so please enlighten me.
Thanking anybody who comes across such a big post and shares their idea beforehand!</p>
",Vectorization & Embeddings,improve nlp model classify different mental illness dataset csv containing column label determines type mental illness patient corresponding reddit post certain time period user total number patient group illness control depression bipolar ptsd schizophrenia starter tried binary classification depression bipolar patient used tfidf vector fed different type classifier multinomialnb svm sample code using multinomialnb using svm result problem model simply predicting patient class larger data sample case predicted depressed patient tried using bert well get accuracy read paper using liwc lexicon category include variable characterize linguistic style well psychological aspect language understand correct better way classifying using nlp please enlighten thanking anybody come across big post share idea beforehand
combine two lists to PCollection,"<p>I'm using Apache Beam. When writing to tfRecord I need to include the ID of the item along with its text and embedding.
The tutorial works with just one list of text but I also have a list of the IDs to match the list of text so I was wondering how I could pass the ID to the following function:</p>
<pre><code>  def to_tf_example(entries):
  examples = []

  text_list, embedding_list = entries
  for i in range(len(text_list)):
    text = text_list[i]
    embedding = embedding_list[i]

    features = {
        # need to pass in ID here like so:
        'id': tf.train.Feature(
            bytes_list=tf.train.BytesList(value=[ids.encode('utf-8')])),
        'text': tf.train.Feature(
            bytes_list=tf.train.BytesList(value=[text.encode('utf-8')])),
        'embedding': tf.train.Feature(
            float_list=tf.train.FloatList(value=embedding.tolist()))
    }
  
    example = tf.train.Example(
        features=tf.train.Features(
            feature=features)).SerializeToString(deterministic=True)
  
    examples.append(example)
  
  return examples
</code></pre>
<p>My first thought was just to include the ids in the text column of my database and then extract them via slicing or regex or something but was wondering if there was a better way, I assume converting to a PCollection but don't know where to start. Here is the pipeline:</p>
<pre><code>    with beam.Pipeline(args.runner, options=options) as pipeline:
        query_data = pipeline | 'Read data from BigQuery' &gt;&gt; 
        beam.io.Read(beam.io.BigQuerySource(project='my-project', query=get_data(args.limit), use_standard_sql=True))
        # list of texts
        text = query_data | 'get list of text' &gt;&gt; beam.Map(lambda x: x['text'])
        # list of ids
        ids = query_data | 'get list of ids' &gt;&gt; beam.Map(lambda x: x['id'])
    
        ( text
            | 'Batch elements' &gt;&gt; util.BatchElements(
            min_batch_size=args.batch_size, max_batch_size=args.batch_size)
            | 'Generate embeddings' &gt;&gt; beam.Map(
            generate_embeddings, args.module_url, args.random_projection_matrix)
            | 'Encode to tf example' &gt;&gt; beam.FlatMap(to_tf_example)
            | 'Write to TFRecords files' &gt;&gt; beam.io.WriteToTFRecord(
            file_path_prefix='{0}'.format(args.output_dir),
            file_name_suffix='.tfrecords')
        )

        query_data | 'Convert to entity and write to datastore' &gt;&gt; beam.Map(
                lambda input_features: create_entity(
                    input_features, args.kind))
</code></pre>
",Vectorization & Embeddings,combine two list pcollection using apache beam writing tfrecord need include id item along text embedding tutorial work one list text also list id match list text wa wondering could pas id following function first thought wa include id text column database extract via slicing regex something wa wondering wa better way assume converting pcollection know start pipeline
Gensim Word2Vec Embedding instead of GloVe TypeError: not all arguments converted during string formatting,"<p>I am trying to generate text for which I used custom Gensim Word2Vec embedding. I am trying to fit it instead of GloVe embedding.</p>
<p>Code :</p>
<pre><code>glove_path = &quot;/content/drive/MyDrive/Dataset/Bangla AI/custom_bangla_embedding.txt&quot;

</code></pre>
<pre><code>BATCH_SIZE = 64 # number of data points to consider to train at a single point of time
LATENT_DIM = 200 # the size of the hidden state/vector
EMBEDDING_DIM = 1000 # size of the word embeddings - comes into various sizes 50, 100 or 200
MAX_VOCAB_SIZE = 30000 # the maximum number of words to consider
VALIDATION_SPLIT = 0.2 # % of validation dataset```

</code></pre>
<p>class SequenceGenerator():</p>
<pre><code>def __init__(self, input_lines, target_lines, max_seq_len=None, max_vocab_size=10000, embedding_dim=200):        
    self.input_lines = input_lines
    self.target_lines = target_lines
    
    self.MAX_SEQ_LEN = max_seq_len
    self.MAX_VOCAB_SIZE = max_vocab_size
    self.EMBEDDING_DIM = embedding_dim


def initialize_embeddings(self):
    
    # load the word embeddings
    self.word2vec = {}
    with open(glove_path%self.EMBEDDING_DIM, 'r') as file:
        for line in file:
            vectors = line.split()
            self.word2vec[vectors[0]] = np.asarray(vectors[1:], dtype=&quot;float32&quot;)

            
    # get the embeddings matrix
    self.num_words = min(self.MAX_VOCAB_SIZE, len(self.word2idx)+1)
    self.embeddings_matrix = np.zeros((self.num_words, self.EMBEDDING_DIM))
    
    for word, idx in self.word2idx.items():
        if idx &lt;= self.num_words:
            word_embeddings = self.word2vec.get(word)
            if word_embeddings is not None:
                self.embeddings_matrix[idx] = word_embeddings
                
    self.idx2word = {v:k for k,v in self.word2idx.items()}


def prepare_sequences(self, filters=''):
    
    # train the tokenizer
    self.tokenizer = Tokenizer(num_words=self.MAX_VOCAB_SIZE, filters='')
    self.tokenizer.fit_on_texts(self.input_lines+self.target_lines)
    
    # get the word-index mapping and initialize embeddings
    self.word2idx = self.tokenizer.word_index
    self.initialize_embeddings()
    
    # tokenize the input and target lines
    self.input_sequences = self.tokenizer.texts_to_sequences(self.input_lines)
    self.target_sequences = self.tokenizer.texts_to_sequences(self.target_lines)
    
    # get the max sequence len from the data
    max_seq_len = max(list(map(len, self.input_lines+self.target_lines)))
    if self.MAX_SEQ_LEN:
        self.MAX_SEQ_LEN = min(self.MAX_SEQ_LEN, max_seq_len)
    else:
        self.MAX_SEQ_LEN = max_seq_len
        
    # pad the sequences
    self.input_sequences = pad_sequences(self.input_sequences, maxlen=self.MAX_SEQ_LEN, padding=&quot;post&quot;)
    self.target_sequences = pad_sequences(self.target_sequences, maxlen=self.MAX_SEQ_LEN, padding=&quot;post&quot;)
    
    print(&quot;1st input sequence: &quot;, self.input_sequences[0])
    print(&quot;1st target sequence: &quot;, self.target_sequences[0])
    
    
def one_hot_encoding(self):
    &quot;Creates the One-hot encoding for the target sequence.&quot;
    
    # it will be a 3 dimensional array where
    # first-dim is the number of target lines
    # second-dim is the size of the sequences
    # third-dim is the number of words in the dataset
    self.one_hot_targets = np.zeros((len(self.target_sequences), self.MAX_SEQ_LEN, self.num_words))
    
    for seq_idx, seq in enumerate(self.target_sequences):
        for word_idx, word_id in enumerate(self.target_sequences[seq_idx]):
            if word_id &gt; 0:
                self.one_hot_targets[seq_idx, word_idx, word_id] = 1


def get_closest_word(self, word_vec):
    &quot;&quot;&quot;
        Find the nearest word to the provided vector. The distance between the vectors is 
        calculated using the cosine-distance.
        
        Parameters:
            word_vec (np.array): a vector of size EMBEDDING_DIM
            
        Returns:
            Str: the closest word to the provided vector
    &quot;&quot;&quot;
    
    max_dist = 9999999999
    closest_word = &quot;NULL&quot;
    
    # iterate overall the words and find the closest one
    for word, vec in self.word2vec.items():
        
        # get the cosine distance between the words
        dist = spatial.distance.cosine(word_vec, vec)
        
        # compare the distance and keep the minimum
        if dist &lt; max_dist:
            max_dist = dist
            closest_word = word
    
    return closest_word```
</code></pre>
<h1>create an object of the class</h1>
<pre><code>                           max_vocab_size=MAX_VOCAB_SIZE, embedding_dim=EMBEDDING_DIM)```

# prepare the input &amp; target sequences
```sg_obj.prepare_sequences()```
# create the One-hot encoding on the target sequences
```sg_obj.one_hot_encoding()```

# make sure the tokenized words contains &lt;sos&gt; &amp; &lt;eos&gt;
```assert '&lt;sos&gt;' in sg_obj.word2idx
assert '&lt;eos&gt;' in sg_obj.word2idx```




But getting the following error:

</code></pre>
<p>TypeError                                 Traceback (most recent call last)</p>
<pre><code>```&lt;ipython-input-36-3edfcb198239&gt; in &lt;module&gt;()
    133 
    134 # prepare the input &amp; target sequences
--&gt; 135 sg_obj.prepare_sequences()
    136 # create the One-hot encoding on the target sequences
    137 sg_obj.one_hot_encoding()```

```1 frames

&lt;ipython-input-36-3edfcb198239&gt; in initialize_embeddings(self)
     29         # load the word embeddings
     30         self.word2vec = {}
---&gt; 31         with open(glove_path%self.EMBEDDING_DIM, 'r') as file:
     32             for line in file:
     33                 vectors = line.split()```

```TypeError: not all arguments converted during string formatting
</code></pre>
<p>Looking for kind help. Thanks in advance.</p>
",Vectorization & Embeddings,gensim word vec embedding instead glove typeerror argument converted string formatting trying generate text used custom gensim word vec embedding trying fit instead glove embedding code create object class looking kind help thanks advance
BertModel or BertForPreTraining,"<p>I want to use Bert only for embedding and use the Bert output as an input for a classification net that I will build from scratch.</p>
<p>I am not sure if I want to do finetuning for the model.</p>
<p>I think the relevant classes are BertModel or BertForPreTraining.</p>
<p><a href=""https://dejanbatanjac.github.io/bert-word-predicting/"" rel=""nofollow noreferrer"">BertForPreTraining</a>  head contains two &quot;actions&quot;:
self.predictions is MLM (Masked Language Modeling) head is what gives BERT the power to fix the grammar errors, and self.seq_relationship is NSP (Next Sentence Prediction); usually refereed as the classification head.</p>
<pre><code>class BertPreTrainingHeads(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.predictions = BertLMPredictionHead(config)
        self.seq_relationship = nn.Linear(config.hidden_size, 2)
</code></pre>
<p>I think the NSP isn't relevant for my task so I can &quot;override&quot; it.
what does the MLM do and is it relevant for my goal or should I use the BertModel?</p>
",Vectorization & Embeddings,bertmodel bertforpretraining want use bert embedding use bert output input classification net build scratch sure want finetuning model think relevant class bertmodel bertforpretraining bertforpretraining head contains two action self prediction mlm masked language modeling head give bert power fix grammar error self seq relationship nsp next sentence prediction usually refereed classification head think nsp relevant task override doe mlm relevant goal use bertmodel
Word2Vec average word vectors as input to LSTM,"<p>I am running an experiment using NLP. I am using Word2vec in order to have a distributed vector representation of the input text and then feed these representations in different Machine Learning (ML) and Deep Learning (DL) algorithms in order to measure the performance. It's a binary classification task and I have the target labels.</p>
<p>What regards the ML models I have used the approach in <a href=""https://www.kaggle.com/arunava21/word2vec-and-random-forest-classification"" rel=""nofollow noreferrer"">this</a> post, which basically calculates the average word vector per observation and feeds it as an input, to the post's case, to a RandomForestClassifier.</p>
<p>What regards DL approaches and implementations such as CNN or LSTM I have encountered implementations such as <a href=""https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-11-cnn-word2vec-41f5e28eda74"" rel=""nofollow noreferrer"">this</a> which the author constructs an embedding matrix which acts as a dictionary for returning the corresponding vector representation for each word in the input token sequence. The code is summarized as:</p>
<pre><code>num_words = 100000
embedding_matrix = np.zeros((num_words, 200))
for word, i in tokenizer.word_index.items():
    if i &gt;= num_words:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
</code></pre>
<p>I am trying to implement an LSTM model which shall use the approach of the average vector per observation (input token sequence). However, there is an error being thrown which I can't figure out how to resolve. In addition, I have seen <a href=""https://stackoverflow.com/questions/54217503/averaging-a-sentence-s-word-vectors-in-keras-pre-trained-word-embedding"">this</a> and especially <a href=""https://stackoverflow.com/questions/52525990/lstm-network-on-pre-trained-word-embedding-gensim"">this</a> post where the author has exactly the same issue. I tried the the answered propositions but still I can't make it work.</p>
<p>What I have tried so far:</p>
<ul>
<li>Generate trainDataVectors,testDataVectors just like in ML approaches and feed them directly to the LSTM model without adding the Embedding layer. Does not work, shape mismatch error.</li>
<li>I can't understand the answer from the <a href=""https://stackoverflow.com/questions/52525990/lstm-network-on-pre-trained-word-embedding-gensim"">last post</a> I have different syntax.</li>
</ul>
<p>My implementation for the LSTM goes like this:</p>
<pre><code>model = Sequential()

model.add(Embedding(input_dim=num_words,
                    output_dim=embedding_size,
                    weights= [embedding_matrix],
                    input_length=max_tokens,        
                    trainable=False,              #the layer is not trained
                    name='embedding_layer'))
model.add(LSTM(units = embedding_size, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(train_seq_pad, y_train, validation_data=(test_seq_pad, y_test), epochs=20, batch_size=30)
</code></pre>
<p>The <em>embedding_matrix</em> is constructed as shown above in the post. The <em>train_seq_pad</em> contains the input token sequences. The shape of it is [number_of_observations, max_token_length]. One instance of it looks like this:</p>
<pre><code>array([   1,    2, 1481,   20,  795, 1073,    3,    9,   11, 1073,   91,
     10,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
      0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
      0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
      0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
      0,    0,    0,    0])
</code></pre>
<p>I am padding zeros at the end of each sequence in order to meet the max_token_length when needed. The numbers represent the indices in the embedding matrix so to know which word vectors to recover from each input sequence.</p>
<p>I just want to modify the latter implementation, in order to take as input the average vector from each input token sequence just as it is performed in the ML approaches.</p>
<p>Any suggestions or hints ?</p>
",Vectorization & Embeddings,word vec average word vector input lstm running experiment using nlp using word vec order distributed vector representation input text feed representation different machine learning ml deep learning dl algorithm order measure performance binary classification task target label regard ml model used approach post basically calculates average word vector per observation feed input post case randomforestclassifier regard dl approach implementation cnn lstm encountered implementation author construct embedding matrix act dictionary returning corresponding vector representation word input token sequence code summarized trying implement lstm model shall use approach average vector per observation input token sequence however error thrown figure resolve addition seen tried far generate traindatavectors testdatavectors like ml approach feed directly lstm model without adding embedding layer doe work shape mismatch error understand answer href post different syntax implementation lstm go like embedding matrix constructed shown post train seq pad contains input token sequence shape number observation max token length one instance look like padding zero end sequence order meet max token length needed number represent index embedding matrix know word vector recover input sequence want modify latter implementation order take input average vector input token sequence performed ml approach suggestion hint
pass from a model of type gensim.models.keyedvectors.Word2VecKeyedVectors to a model of type gensim.models.word2vec.Word2Vec,"<p>I downloaded a word embedding already train in &quot;glove.txt&quot; format
I imported it in as a model of type gensim.models.keyedvectors.Word2VecKeyedVectors thanks to this documentation :</p>
<p><a href=""https://radimrehurek.com/gensim/scripts/glove2word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/scripts/glove2word2vec.html</a></p>
<p>But I would like a model of type gensim.models.word2vec.Word2Vec</p>
<p>Will there be a way to convert it or import it directly into the desired format?</p>
",Vectorization & Embeddings,pas model type gensim model keyedvectors word veckeyedvectors model type gensim model word vec word vec downloaded word embedding already train glove txt format imported model type gensim model keyedvectors word veckeyedvectors thanks documentation would like model type gensim model word vec word vec way convert import directly desired format
Cosine Similarity and LDA topics,"<p>I want to compute Cosine Similarity between LDA topics. In fact, gensim function .matutils.cossim can do it but I dont know  which parameter (vector ) I can use for this function?</p>

<p>Here is a snap of  code :</p>

<pre><code>import numpy as np
import lda
from sklearn.feature_extraction.text import CountVectorizer

cvectorizer = CountVectorizer(min_df=4, max_features=10000, stop_words='english')
cvz = cvectorizer.fit_transform(tweet_texts_processed)

n_topics = 8
n_iter = 500
lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)
X_topics = lda_model.fit_transform(cvz)

n_top_words = 6
topic_summaries = []

topic_word = lda_model.topic_word_  # get the topic words
vocab = cvectorizer.get_feature_names()
for i, topic_dist in enumerate(topic_word):
    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]
    topic_summaries.append(' '.join(topic_words))
    print('Topic {}: {}'.format(i, ' '.join(topic_words)))

doc_topic = lda_model.doc_topic_
lda_keys = []
for i, tweet in enumerate(tweets):
    lda_keys += [X_topics[i].argmax()]

import gensim
from gensim import corpora, models, similarities
#Cosine Similarity between LDA topics
 **sim = gensim.matutils.cossim(LDA_topic[1], LDA_topic[2])** 
</code></pre>
",Vectorization & Embeddings,cosine similarity lda topic want compute cosine similarity lda topic fact gensim function matutils cossim dont know parameter vector use function snap code
How to use pre-trained FastText embeddings with existing Seq2Seq model?,"<p>I'm new in NLP and I am trying to understand how to use pre-trained word embeddings like fastText with the existing Seq2Seq model. The Seq2Seq model I'm working with is the following. The encoder is simple and the decoder is Pointer Generator Network with CRF on the top. Both of them use an embedding layer.</p>
<p>The question: If I have my own dataset &amp; vocab, how do I use both my own vocab and the one from the fastText? Do I have to use fastText weights in both the encoder and decoder?</p>
",Vectorization & Embeddings,use pre trained fasttext embeddings existing seq seq model new nlp trying understand use pre trained word embeddings like fasttext existing seq seq model seq seq model working following encoder simple decoder pointer generator network crf top use embedding layer question dataset vocab use vocab one fasttext use fasttext weight encoder decoder
What should be used between Doc2Vec and Word2Vec when analyzing product reviews?,"<p>I collected some product reviews of a website from different users, and I'm trying to find similarities between products through the use of the embeddings of the words used by the users.
I grouped each review per product, such that I can have different reviews succeeding one after the other in my dataframe (i.e: different authors for one product). Furthermore, I also already tokenized the reviews (and all other pre-processing methods). Below is a mock-up dataframe of what I'm having (the list of tokens per product is actually very high, as well as the number of products):</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Product</th>
<th>reviews_tokenized</th>
</tr>
</thead>
<tbody>
<tr>
<td>XGame3000</td>
<td>absolutely amazing simulator feel inaccessible ...</td>
</tr>
<tr>
<td>Poliamo</td>
<td>production value effect tend cover rather  ...</td>
</tr>
<tr>
<td>Artemis</td>
<td>absolutely fantastic possibly good oil ...</td>
</tr>
<tr>
<td>Ratoiin</td>
<td>ability simulate emergency operator town ...</td>
</tr>
</tbody>
</table>
</div>
<p>However, I'm not sure of what would be the most efficient between doc2Vec and Word2Vec. I would initially go for Doc2Vec, since it has the ability to find similarities by taking into account the paragraph/sentence, and find the topic of it (which I'd like to have, since I'm trying to cluster products by topics), but I'm a bit worry about the fact that the reviews are from different authors, and thus might bias the embeddings? Note that I'm quite new to NLP and embeddings, so some notions may escape me. Below is my code for Doc2Vec, which giving me a quite good silhouette score (~0.7).</p>
<pre><code>product_doc = [TaggedDocument(doc.split(' '), [i]) for i, doc in enumerate(df.tokens)]
model3 = Doc2Vec(min_count=1, seed = SEED, ns_exponent = 0.5)
model3.build_vocab(product_doc)
model3.train(product_doc, total_examples=model3.corpus_count, epochs=model3.epochs)
product2vec = [model3.infer_vector((df['tokens'][i].split(' '))) for i in range(0,len(df['tokens']))]
dtv = np.array(product2vec)
</code></pre>
<p>What do you think would be the most efficient method to tackle this? If something is not clear enough, or else, please tell me.</p>
<p>Thank you for your help.</p>
<p>EDIT: Below is the clusters I'm obtaining:
<a href=""https://i.sstatic.net/STJq5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/STJq5.png"" alt=""Clusters obtained"" /></a></p>
",Vectorization & Embeddings,used doc vec word vec analyzing product review collected product review website different user trying find similarity product use embeddings word used user grouped review per product different review succeeding one dataframe e different author one product furthermore also already tokenized review pre processing method mock dataframe list token per product actually high well number product product review tokenized xgame absolutely amazing simulator feel inaccessible poliamo production value effect tend cover rather artemis absolutely fantastic possibly good oil ratoiin ability simulate emergency operator town however sure would efficient doc vec word vec would initially go doc vec since ha ability find similarity taking account paragraph sentence find topic like since trying cluster product topic bit worry fact review different author thus might bias embeddings note quite new nlp embeddings notion may escape code doc vec giving quite good silhouette score think would efficient method tackle something clear enough else please tell thank help edit cluster obtaining
How to average the vector when merging (with retokenize) custom noun chunks in spaCy?,"<p>I am generating noun chunks using spaCy's statistical models (e.g. noun chunks based on the part-of-speech tags and dependencies) and rule-based matching to capture (user supplied) context specific noun chunks.</p>
<p>For other downstream tasks, I am retokenizing these noun chunks (spans), which works fine for the most part. However, the token's vector representation (token.vector) gets set to all zeros. Is there a way to retain the vector information e.g. by averaging the individual token vectors and assigning it to the retokenised token?</p>
<p>I tried this with the code...</p>
<pre><code>def tokenise_noun_chunks(doc)
    if not doc.has_annotation(&quot;DEP&quot;):
        return doc

    all_noun_chunks = list(doc.noun_chunks) + doc._.custom_noun_chunks

    with doc.retokenize() as retokenizer:
        for span in all_noun_chunks:
            # if I print(span.vector) here, I get the correctly averaged vector
            attrs = {&quot;tag&quot;: span.root.tag, &quot;dep&quot;: span.root.dep}
            retokenizer.merge(np, attrs=attrs)
        return doc
</code></pre>
<p>...but when I check the returned vectors for the noun chunks, I get a zeros array. I've modelled this (the above code) on the built-in merge_noun_chunks component (just modified to include my own custom noun chunks), so I can confirm that the built in component gives the same results.</p>
<p>Is there any way to keep the word vector information? Will I need to add the term/average vector to the Vector store?</p>
",Vectorization & Embeddings,average vector merging retokenize custom noun chunk spacy generating noun chunk using spacy statistical model e g noun chunk based part speech tag dependency rule based matching capture user supplied context specific noun chunk downstream task retokenizing noun chunk span work fine part however token vector representation token vector get set zero way retain vector information e g averaging individual token vector assigning retokenised token tried code check returned vector noun chunk get zero array modelled code built merge noun chunk component modified include custom noun chunk confirm built component give result way keep word vector information need add term average vector vector store
tfidf.idf_ what is meaning of this in the code,"<p>tfidf = TfidfVectorizer(lowercase=False, )
tfidf.fit_transform(questions)</p>
<h1>dict key:word and value:tf-idf score</h1>
<p>word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))</p>
",Vectorization & Embeddings,tfidf idf meaning code tfidf tfidfvectorizer lowercase false tfidf fit transform question dict key word value tf idf score word tfidf dict zip tfidf get feature name tfidf idf
How does one tackle over-fitting in NLP based CNN models for multiclass text classification with word embeddings?,"<p>(Problem: Overfitting issues in a multiclass text classification problem)<br/>
In my personal project, the objective is to classify the industry tags of a company based on the company description. The steps I've taken are:</p>
<ol>
<li>Removing stopwords, punctuations, spaces, etc, and splitting the description into tokens.</li>
<li>Converted the labels and tokens into word vectors.</li>
<li>Convert the tokens into a word embedding model.</li>
<li>Set up the CNN with 62 output nodes. (62 distinct industry tags to classify)</li>
</ol>
<hr />
<p>Image/Dataset Link for reference: <a href=""https://drive.google.com/drive/folders/1yLW2YepoHvSp_koHDDzcAAJBIaYQIen0?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/1yLW2YepoHvSp_koHDDzcAAJBIaYQIen0?usp=sharing</a></p>
<hr />
<p>The issue I face is that the model overfits regardless of the alterations I make. (Ends early due to callback I set up for loss)
[CNN accuracy][7]</p>
<pre><code>max_features = 700
maxlen = 200
embedding_dims = 50
filters = 200
kernel_size = 3
hidden_dims = 160
es_callback = EarlyStopping(monitor='val_loss', patience=5)

model = Sequential()
model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False))
model.add(Dropout(0.4))

model.add(Conv1D(filters,
                 kernel_size,
                 padding='valid',
                 activation='relu',
                 strides=1))
model.add(GlobalMaxPooling1D())

model.add(Dense(hidden_dims))
model.add(Dropout(0.4))
model.add(Activation('relu'))

model.add(Dense(62))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model.fit(X_train, y_label_train,
              batch_size=64,
              epochs=50,
              validation_data=(X_test, y_label_test),
              callbacks=[es_callback])
</code></pre>
<p>Code Link: <a href=""https://colab.research.google.com/drive/1YqbhPX6e4bJ5FnbfHj7fTQH4tUe6diva?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1YqbhPX6e4bJ5FnbfHj7fTQH4tUe6diva?usp=sharing</a></p>
",Vectorization & Embeddings,doe one tackle fitting nlp based cnn model multiclass text classification word embeddings problem overfitting issue multiclass text classification problem personal project objective classify industry tag company based company description step taken removing stopwords punctuation space etc splitting description token converted label token word vector convert token word embedding model set cnn output node distinct industry tag classify image dataset link reference issue face model overfits regardless alteration make end early due callback set loss cnn accuracy code link
Embedded feature vector components: position-dependent?,"<p>I have a question about the position of vector components of embedded features.</p>
<p>After the embedding layer, we usually flatten the embedded features to generate a 1D-shaped vector.</p>
<p>Assume we do not use CNN or RNN layers to process the embedded features, but go directly to the fully-connected layers to do classification:</p>
<p><strong>I wonder if the position of every vector component would affect the meaning of the whole vector to the machine-learning algorithm (e.g. in Keras)?</strong></p>
<p>Background of my question: In NLP, sentences have different lengths and the position of words with similar meaning does not always align. After vectorization of n-gram of words with embeddings, I wonder if it is possible to join the flattened embedded vector to the dense layer to do classification without CNN or RNN layer, which increases the size of my network + potentially cause overfitting to the training set.</p>
",Vectorization & Embeddings,embedded feature vector component position dependent question position vector component embedded feature embedding layer usually flatten embedded feature generate shaped vector assume use cnn rnn layer process embedded feature go directly fully connected layer classification wonder position every vector component would affect meaning whole vector machine learning algorithm e g kera background question nlp sentence different length position word similar meaning doe always align vectorization n gram word embeddings wonder possible join flattened embedded vector dense layer classification without cnn rnn layer increase size network potentially cause overfitting training set
im trying to transform using tfidf but it is giving error,"<p>I'm just trying to fit but it is giving this error. stemming is done</p>
<pre><code>vectorizer_tf =TfidfVectorizer(df['Stemmed'],max_df=0.75,max_features=2000,lowercase=False, ngram_range=(1,2))
train_vec = vectorizer_tf.fit_transform(df['Description_no_sw'])
</code></pre>
<p>error  : The truth value of a Series is ambiguous. Use a.empty, a.bool(),
a.item(), a.any() or a.all()</p>
<p>I'm just trying to fit but it is giving this error. stemming is done</p>
",Vectorization & Embeddings,im trying transform using tfidf giving error trying fit giving error stemming done error truth value series ambiguous use empty bool item trying fit giving error stemming done
How to insert information on a word embedding?,"<p>I saw that transformers inserted the positional embedding of a word on its word embedding. How can I insert another things like te word size for example? Is it possible to view the internal code of a transformer framework like BERT for example to edit something like that?</p>
",Vectorization & Embeddings,insert information word embedding saw transformer inserted positional embedding word word embedding insert another thing like te word size example possible view internal code transformer framework like bert example edit something like
Python Nested Loop Alternative,"<p>I have two large lists containing text.
X = [30,000 entries] and
Y = [400 entries]</p>
<p>I want to find out the text which are similar in both the list using cosine similarity.
Below is the code which I am trying to execute using nested for loops</p>
<pre><code>vectorizer = CountVectorizer()
found_words = []
for x in X:
    for y in Y:
       vector1 = vectorizer(x.lower())
       vector2 = vectorizer(y.lower())
       sim = cosine_similarity(vector1, vector2)
       if sim &gt; 0.9:
           found_words.append(x.capitalize()) 
</code></pre>
<p>The above code works fine but takes a lot of time to execute. Is there any other way which can be efficient in time as well as space complexity. Thank you</p>
",Vectorization & Embeddings,python nested loop alternative two large list containing text x entry entry want find text similar list using cosine similarity code trying execute using nested loop code work fine take lot time execute way efficient time well space complexity thank
Pretrained Word Embeddings For Each Year,"<p>I am running a task where it would be nice to have different versions of word embeddings across different time periods e.g. embeddings for 2013, 2014, 2015, 2016 ... 2020. This is because I don't want to bias my algorithm by using embeddings which have been trained on a time period before I run my target task e.g. using embeddings from 2019 when I am running a task on data from 2013 (i.e. words will have different meanings).</p>
<p>Is anybody aware of a project which has pretrained embeddings across time? The data type would preferably be social media (e.g. Twitter).</p>
",Vectorization & Embeddings,pretrained word embeddings year running task would nice different version word embeddings across different time period e g embeddings want bias algorithm using embeddings trained time period run target task e g using embeddings running task data e word different meaning anybody aware project ha pretrained embeddings across time data type would preferably social medium e g twitter
BERT embeddings for entire sentences vs. verbs,"<p>First off, I am drawing upon assumption that majority of the semantic <em>value</em> of the sentence is mediated by verbs that connect the subject and the object of said verb. I am aware that I simplify a bit here as there can be multiple verbs and such. But abstracting from that, I'd be curious whether it applies that a BERT generated embedding vector for entire sentence, say <strong>&quot;Two halves make a whole&quot;</strong> would be measurably more similar to an embedding vector for a singular verb like <strong>&quot;make&quot;</strong> as compared to say a vector for verb <strong>&quot;eat&quot;</strong>.</p>
",Vectorization & Embeddings,bert embeddings entire sentence v verb first drawing upon assumption majority semantic value sentence mediated verb connect subject object said verb aware simplify bit multiple verb abstracting curious whether applies bert generated embedding vector entire sentence say two half make whole would measurably similar embedding vector singular verb like make compared say vector verb eat
&#39;list&#39; object has no attribute &#39;shape,"<p>I am passing an embedding matrix to the embedding layer in Keras</p>
<pre><code>model = Sequential()
model.add(Embedding(max_words, 30, input_length=max_len, weights=[all]))
model.add(BatchNormalization())
model.add(Activation('tanh'))
model.add(SpatialDropout1D(0.5))
model.add(Conv1D(32, kernel_size=3, activation='relu'))
model.add(Bidirectional(LSTM(32, return_sequences=True)))
model.add(BatchNormalization())
model.add(Activation('tanh'))
model.add(Dropout(0.8))
model.add(Dense(1, activation='sigmoid'))
model.summary()
</code></pre>
<p>Here <code>all</code> in the embedding layer is my embedding matrix. When I pass this, it gives the following error</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-45-6e84a6e5254a&gt; in &lt;module&gt;()
      1 model = Sequential()
----&gt; 2 model.add(Embedding(max_words, 30, input_length=max_len, weights=[all]))
      3 model.add(BatchNormalization())
      4 model.add(Activation('tanh'))
      5 model.add(SpatialDropout1D(0.5))

4 frames
/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py in set_weights(self, weights)
   1120         param_values = K.batch_get_value(params)
   1121         for pv, p, w in zip(param_values, params, weights):
-&gt; 1122             if pv.shape != w.shape:
   1123                 raise ValueError('Layer weight shape ' +
   1124                                  str(pv.shape) +

AttributeError: 'list' object has no attribute 'shape'
</code></pre>
",Vectorization & Embeddings,list object ha attribute shape passing embedding matrix embedding layer kera embedding layer embedding matrix pas give following error
Is there a way to get only the IDF values of words using scikit or any other python package?,"<p>I have a text column in my dataset and using that column I want to have a IDF calculated for all the words that are present. TFID implementations in scikit, like <em><code>tfidf</code> vectorize</em>, are giving me TFIDF values directly as against just word IDFs. Is there a way to get word IDFs give a set of documents?</p>
",Vectorization & Embeddings,way get idf value word using scikit python package text column dataset using column want idf calculated word present tfid implementation scikit like vectorize giving tfidf value directly word idf way get word idf give set document
Parallelizing ML models which rely on stochastic gradient descent?,"<p>I have been reading about different NLP models like word2vec and GloVe, and how these can be parallelized because they are mostly just dot products. However, I am a bit confused by this, because computing the gradient &amp; updating the model depends on the current values of the parameters/vectors. How is this done in parallel/asynchronously? How do you know when to update the global parameters using the gradients being computed stochastically by each of the threads?</p>
",Vectorization & Embeddings,parallelizing ml model rely stochastic gradient descent reading different nlp model like word vec glove parallelized mostly dot product however bit confused computing gradient updating model depends current value parameter vector done parallel asynchronously know update global parameter using gradient computed stochastically thread
How to get the hidden states of a fine-tuned TFBertModel?,"<p>I first fine-tune the Bert model on a text classification task and but then I want to get the embeddings of the fine-tuned model in TensorFlow. Unfortunately, I can only say <code>output_hidden_states=True</code>, in the first line where I download the pre-trained Bert model but not in the second stage where I create a <code>tf.Keras.Model</code>. Here is the code for how I make and train the model:</p>
<pre><code>max_len = 55

from transformers import BertConfig, BertTokenizer, TFBertModel

def build_custome_model():
    bert_encoder = TFBertModel.from_pretrained(Base_BERT_Path)

    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_word_ids&quot;)
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_mask&quot;)
    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_type_ids&quot;)
    
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    clf_output = embedding[:,0,:]

    net = tf.keras.layers.Dropout(0.4)(clf_output)
    output = tf.keras.layers.Dense(5, activation='softmax')(net)
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)

    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model
</code></pre>
<p>Then I train the model on a dataset with 2 sentences and as score for their similarity</p>
<pre><code>#------Training with stratifiedkfold-------


k = 5
kfold = StratifiedKFold(n_splits = k, shuffle = True)

for i, (train_idx, val_idx) in enumerate(kfold.split(first_sentences, labels.score), 1):

    epoch_evaluation = {}

    train_input = create_input(np.array(first_sentences)[train_idx], np.array(second_sentences)[train_idx], tokenizer, max_len=max_seq_length)
    validation_input = create_input(np.array(first_sentences)[val_idx], np.array(second_sentences)[val_idx], tokenizer, max_len=max_seq_length)

    history = model.fit(x = train_input, y = labels.loc[train_idx, 'score'],
                        validation_data= (validation_input, labels.loc[val_idx, 'score']),
                        epochs = 5,
                        verbose = 1,
                        batch_size = 8)
</code></pre>
<p>My goal is to have a model at the end that is trained on this dataset and can output the embeddings (first layer of the hidden states (output[2][0])) whenever I give it a sentence so that I can get the mean of all the fine-tuned token embeddings of a sentence.</p>
",Vectorization & Embeddings,get hidden state fine tuned tfbertmodel first fine tune bert model text classification task want get embeddings fine tuned model tensorflow unfortunately say first line download pre trained bert model second stage create code make train model train model dataset sentence score similarity goal model end trained dataset output embeddings first layer hidden state output whenever give sentence get mean fine tuned token embeddings sentence
NaN value in WordEmbedding,"<p>In my model I have a simple word embedding done like this:</p>
<pre><code>self.word_embedding = nn.Embedding(
            num_embeddings=len(self.word_vocab),       # 381
            embedding_dim=self.word_emb_size,          # 100
            padding_idx=self.word_vocab.pad,           # 0
        )
</code></pre>
<p>after the first batch iteration in the first epoch, its values begin to be NaN.
In the <code>forward()</code> it is used like this:</p>
<pre><code>we = self.word_embedding(w)
</code></pre>
<p>where <code>w</code> is a tensor of shape <code>([50,61])</code> and its values are never NaN.</p>
<p>How can I do to solve this problem that I propagate the NaN values around the whole model?</p>
<p>Thank you!</p>
<p>EDIT:
Trying to retrace all the steps of the training I noticed that after the <code>loss.backward()</code> and before the <code>optimizer.step()</code> the minimum and maximum values for the word embedding gradient are already Nan:</p>
<pre><code>for epoch in range(args.epochs):
    for batch in chunker(train, args.batch_size):
        # clear gradients
        optimizer.zero_grad()
        # calculate loss
        loss = pat.train_conll(batch)
        # back propagate
        loss.backward()

        for name, params in pat.named_parameters():
            print(&quot;--&gt;name:&quot;, name, &quot;--&gt;max_grad:&quot;, params.grad.max(), &quot;--&gt;min_grad:&quot;, params.grad.min())

        # optimize parameters
        optimizer.step()
</code></pre>
<p>output:</p>
<pre><code>--&gt;name: word_embedding.weight --&gt;max_grad: tensor(nan, device='cuda:0') --&gt;min_grad: tensor(nan, device='cuda:0')
</code></pre>
<p>EDIT2:
thanks to the helpful comment of @Nerveless_child.
The piece of code that was causing the problem is the following:</p>
<pre class=""lang-py prettyprint-override""><code>class Norm(nn.Module):
    def __init__(self, d_model, eps=1e-6):
        super().__init__()

        self.size = d_model
        # create two learnable parameters to calibrate normalisation
        self.alpha = nn.Parameter(torch.ones(self.size))
        self.bias = nn.Parameter(torch.zeros(self.size))
        self.eps = eps

    def forward(self, x):
        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \
               / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias
        return norm
</code></pre>
<p>used after a self-attention BiLSTM.</p>
",Vectorization & Embeddings,nan value wordembedding model simple word embedding done like first batch iteration first epoch value begin nan used like tensor shape value never nan solve problem propagate nan value around whole model thank edit trying retrace step training noticed minimum maximum value word embedding gradient already nan output edit thanks helpful comment nerveless child piece code wa causing problem following used self attention bilstm
How to pass HTML form data stored in a variable to a Python script in Flask?,"<p>I am building a data product (an NLP chat application) for which I am learning Flask so that the user can have a better UI to interact with my product. </p>

<p>I have written down the following code in Flask to get the user input and stored it in a variable.</p>

<p><strong>main.py</strong></p>

<pre><code>from flask import Flask, render_template, request
app = Flask(__name__)

@app.route('/')
def index():
   return render_template('init.html')

@app.route('/handle_data', methods = ['POST', 'GET'])
def handle_data():
    userQuestion = request.form['userQuestion']
    print(userQuestion)
    return render_template('init.html', userQuestion = userQuestion)

if __name__ == '__main__':
   app.run()
</code></pre>

<p><strong>init.html</strong></p>

<pre><code>&lt;!DOCTYPE HTML&gt;
&lt;html&gt;
&lt;body&gt;


&lt;form action=""{{ url_for('handle_data') }}"" method=""post""&gt;
    &lt;input type=""text"" name=""userQuestion""&gt;
    &lt;input type=""submit""&gt;
&lt;/form&gt;

&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>I've handled the form data and stored it in a variable <code>userQuestion</code>. I want to pass this variable to another python script which contains the code of my training model. </p>

<p><strong>doc2vec_main.py</strong></p>

<pre><code>import gensim
import nltk
import numpy
from gensim import models
from gensim import utils
from gensim import corpora
from nltk.stem import PorterStemmer
ps = PorterStemmer()

sentence0 = models.doc2vec.LabeledSentence(words=[u'sampling',u'what',u'is',u'sampling'],tags=[""SENT_0""])
sentence1 = models.doc2vec.LabeledSentence(words=[u'sampling',u'tell',u'me',u'about',u'sampling'],tags=[""SENT_1""])
sentence2 = models.doc2vec.LabeledSentence(words=[u'elig',u'what',u'is',u'my',u'elig'],tags=[""SENT_2""])
sentence3 = models.doc2vec.LabeledSentence(words=[u'eligiblity',u'limit', u'what',u'is',u'my'],tags=[""SENT_3""])
sentence4 = models.doc2vec.LabeledSentence(words=[u'eligiblity',u'claim',u'how',u'much',u'can',u'I'],tags=[""SENT_4""])
sentence5 = models.doc2vec.LabeledSentence(words=[u'retir',u'eligibility',u'claim',u'i',u'am',u'how',u'much',u'can',u'i'],tags=[""SENT_5""])
# ... list of all the training set.

# User inputs a question
document = input(""Ask a question:"")
tokenized_document = list(gensim.utils.tokenize(document, lowercase = True, deacc = True))
stemmed_document = []
for w in tokenized_document:
    stemmed_document.append(ps.stem(w))

sentence19 = models.doc2vec.LabeledSentence(words= stemmed_document, tags=[""SENT_19""])

sentences = [sentence0,sentence1,sentence2,sentence3, sentence4, sentence5,sentence6, sentence7, sentence8, sentence9, sentence10, sentence11, sentence12, sentence13, sentence14, sentence15, sentence16, sentence17, sentence18, sentence19]

model = models.Doc2Vec(size=4, alpha=0.25, min_alpha=.025, min_count=1)
model.build_vocab(sentences)
for epoch in range(30):
    model.train(sentences, total_examples=model.corpus_count, epochs = 
    model.iter)
    model.alpha -= 0.002
    model.min_alpha = model.alpha
model.save(""my_model.doc2vec"")
model_loaded = models.Doc2Vec.load('my_model.doc2vec')
print (model.docvecs.most_similar([""SENT_19""]))
</code></pre>

<p>My problem is I cannot find a way to connect <code>doc2vec_main.py</code> to <code>main.py</code> and pass the value of <code>userQuestion</code> to the <code>document</code> variable in <code>doc2main.py</code> 
script. That is when a user inputs a question in the form and clicks submit, the value of the form get passed down to <code>document</code> in <code>doc2vec_main.py</code> and the remaining script runs. </p>

<p>I have searched a lot on the internet but it didn't help. Can you suggest me a way to do it? I'm a complete beginner in Flask so forgive me for any mistake.</p>
",Vectorization & Embeddings,pas html form data stored variable python script flask building data product nlp chat application learning flask user better ui interact product written following code flask get user input stored variable main py init html handled form data stored variable want pas variable another python script contains code training model doc vec main py problem find way connect pas value variable script user input question form click submit value form get passed remaining script run searched lot internet help suggest way complete beginner flask forgive mistake
Can I use BERT or Doc2Vec for comparing lists of potentially unrelated words?,"<p><strong>Context</strong></p>
<p>I'm building a sample project to match users with similar interests. Given any two users with a given list of interests, I'd like to create a similarity score between those users. It seems like the right way to do this is convert interests into vectors using NLP then compare using cosine similarity.</p>
<p><strong>Question</strong></p>
<p>Can I use BERT (ex: <a href=""https://github.com/hanxiao/bert-as-service"" rel=""nofollow noreferrer"">BERT as a service</a>) or Doc2Vec (ex: <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html"" rel=""nofollow noreferrer"">Gensim</a>) to create a single vector for each user's list of interests?</p>
<p>Ex: an input of <code>user_interests = ['python', 'photography', 'running']</code> would produce a single vector representing all the user's interests which could then be compared to other users' vectors using cosine similarity.</p>
<p><strong>Some key points I'm trying to suss through:</strong></p>
<ul>
<li>Given I'm trying to match two users, I'd ideally like to create a single similarity score (ex: I've also considered converting each interest into its own vector then comparing individual interests, but that results in multiple similarity score for any two users. I'm not certain what I would do next here.)</li>
<li>Given that each list of interests could contain a number of unrelated terms ex: <code>user_interests = ['python', 'photography', 'running']</code> does it make sense to create a single vector for the full list of terms?</li>
<li>Given the same as above, is BERT / Doc2Vec a useful model for creating a vector?</li>
<li>I'd like to maintain a high similarity score for two users with related but not equivalent interests ex: <code>user_interests_1 = ['python', 'photography', 'running']</code> should receive a high similarity score when compared to <code>user_interests_2 = ['coding', 'art', 'exercise']</code></li>
</ul>
",Vectorization & Embeddings,use bert doc vec comparing list potentially unrelated word context building sample project match user similar interest given two user given list interest like create similarity score user seems like right way convert interest vector using nlp compare using cosine similarity question use bert ex bert service doc vec ex gensim create single vector user list interest ex input would produce single vector representing user interest could compared user vector using cosine similarity key point trying sus given trying match two user ideally like create single similarity score ex also considered converting interest vector comparing individual interest result multiple similarity score two user certain would next given list interest could contain number unrelated term ex doe make sense create single vector full list term given bert doc vec useful model creating vector like maintain high similarity score two user related equivalent interest ex receive high similarity score compared
evaluating Doc2Vec - cosine similarity matrix,"<p>I'm training my Doc2Vec model on 106k documents (100-600 words per document). The goal is to retrieve similar documents for a target document.</p>
<p>Since Doc2Vec is an unsupervised model there is no real evaluation possible except to test how it performs on your downstream task.
So, I created a small dataset containing about 200 target documents and 5 similar documents per target.</p>
<p>My idea is to calculate the cosine similarity for every document against all other documents in my test dataset and get top 5 similar documents per target document.</p>
<p>Is there an efficient way to create a cosine similarity matrix with Doc2Vec? The <code>most_similar</code> function is impractical as it retrieves every similar document used for training.</p>
",Vectorization & Embeddings,evaluating doc vec cosine similarity matrix training doc vec model k document word per document goal retrieve similar document target document since doc vec unsupervised model real evaluation possible except test performs downstream task created small dataset containing target document similar document per target idea calculate cosine similarity every document document test dataset get top similar document per target document efficient way create cosine similarity matrix doc vec function impractical retrieves every similar document used training
is there a method to perform a custom TFIDF for sentence negation?,"<p>i used TFIDF to compute similarity between articles but i have a problem that it consider that this two sentences are similar :</p>
<pre><code>I am against this project
I am for this project
</code></pre>
<p>how can i ameliorate my approach to take into account this sentence negation please ?</p>
",Vectorization & Embeddings,method perform custom tfidf sentence negation used tfidf compute similarity article problem consider two sentence similar ameliorate approach take account sentence negation please
Using tfidf as feature,"<p>I want to classify two groups of documents using n-grams. One approach is to extract the important words of each document using  <code>tfidf</code>, and then make a csv file like below:</p>
<pre><code>document, ngram1, ngram2, ngram3, ..., label
1, 0.0, 0.0, 0.0, ..., 0
2, 0.0, 0.0, 0.0, ..., 1
...
</code></pre>
<p>But due to number of documents, the file will be huge and sparse. The other approach is to merge all documents in each group and extract the ngrams. After that, I can count the occurrence of each ngram in each document but I'm not sure this is the best way. Please provide your suggested solution.</p>
",Vectorization & Embeddings,using tfidf feature want classify two group document using n gram one approach extract important word document using make csv file like due number document file huge sparse approach merge document group extract ngrams count occurrence ngram document sure best way please provide suggested solution
Confusion in understanding the output of BERTforTokenClassification class from Transformers library,"<p>It is the example given in the documentation of transformers pytorch library</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForTokenClassification
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForTokenClassification.from_pretrained('bert-base-uncased', 
                      output_hidden_states=True, output_attentions=True)

input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"", 
                         add_special_tokens=True)).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)

loss, scores, hidden_states,attentions = outputs
</code></pre>

<p>Here <code>hidden_states</code> is a tuple of length 13 and contains hidden-states of the model at the output of each layer plus the initial embedding outputs. I would like to know, <strong>whether hidden_states[0] or hidden_states[12] represent the final hidden state vectors</strong>?</p>
",Vectorization & Embeddings,confusion understanding output bertfortokenclassification class transformer library example given documentation transformer pytorch library tuple length contains hidden state model output layer plus initial embedding output would like know whether hidden state hidden state represent final hidden state vector
Can we build word2vec model in a distributed way?,"<p>Currently I have 1.2tb text data to build gensim's word2vec model. It is almost taking 15 to 20 days to complete. </p>

<p>I want to build model for 5tb of text data, then it might take few months to create model. I need to minimise this execution time. Is there any way we can use multiple big systems to create model? </p>

<p>Please suggest any way which can help me in reducing the execution time.</p>

<p>FYI, I have all my data in S3 and I use smart_open module to stream the data.</p>
",Vectorization & Embeddings,build word vec model distributed way currently tb text data build gensim word vec model almost taking day complete want build model tb text data might take month create model need minimise execution time way use multiple big system create model please suggest way help reducing execution time fyi data use smart open module stream data
Doc2Vec build_vocab method fails,"<p>I am following <a href=""https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"" rel=""nofollow noreferrer"">this guide</a> on building a <code>Doc2Vec gensim</code> model.</p>
<p>I have created an <a href=""https://stackoverflow.com/help/minimal-reproducible-example"">MRE</a> that should highlight this problem:</p>
<pre><code>import pandas as pd, numpy as np, warnings, nltk, string, re, gensim
from tqdm import tqdm
tqdm.pandas(desc=&quot;progress-bar&quot;)
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from sklearn.model_selection import train_test_split
from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedDocument

def get_words(para):   
    pattern = '([\d]|[\d][\d])\/([\d]|[\d][\d]\/([\d]{4}))'
    stop_words = set(stopwords.words('english'))
    stemmer = SnowballStemmer('english')
    no_dates = [re.sub(pattern, '', i) for i in para.lower().split()]
    no_punctuation = [nopunc.translate(str.maketrans('', '', string.punctuation)) for nopunc in no_dates]
    stemmed_tokens = [stemmer.stem(word) for word in no_punctuation if word.strip() and len(word) &gt; 1 and word not in stop_words]
    
    return stemmed_tokens

data_dict = {'ID': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10},
 'Review': {0: &quot;Even though the restauraunt was gross, the food was still good and I'd recommend it&quot;,
  1: 'My waiter was awful, my food was awful, I hate it all',
  2: 'I did not enjoy the food very much but I thought the waitstaff was fantastic',
  3: 'Even though the cleanliness level was fantastic, my food was awful',
  4: 'Everything was mediocre, but I guess mediocre is better than bad nowadays',
  5: &quot;Honestly there wasn't a single thing that was mediocre about this place&quot;,
  6: 'I could not have enjoyed it more! Perfect',
  7: 'This place is perfectly awful. I think it should shut down to be honest',
  8: &quot;I can't understand how anyone would say something negative&quot;,
  9: &quot;It killed me. I'm writing this review as a ghost. That's how bad it was.&quot;},
 'Bogus Field 1': {0: 'foo71',
  1: 'foo92',
  2: 'foo25',
  3: 'foo88',
  4: 'foo54',
  5: 'foo10',
  6: 'foo48',
  7: 'foo76',
  8: 'foo4',
  9: 'foo11'},
 'Bogus Field 2': {0: 'foo12',
  1: 'foo66',
  2: 'foo94',
  3: 'foo90',
  4: 'foo97',
  5: 'foo87',
  6: 'foo10',
  7: 'foo4',
  8: 'foo16',
  9: 'foo86'},
 'Sentiment': {0: 1, 1: 0, 2: 1, 3: 0, 4: 1, 5: 0, 6: 1, 7: 0, 8: 1, 9: 0}}    

 df = pd.DataFrame(data_dict, columns=data_dict.keys())
 train, test = train_test_split(df, test_size=0.3, random_state=8)
 train_tagged = train.apply(lambda x: TaggedDocument(words=get_words(x['Review']), 
                                                    tags=x['Sentiment']), axis=1,)

model_dbow = Doc2Vec(dm=0, vector_size=50, negative=5, hs=0, min_count=1, sample=0, workers=8)
model_dbow.build_vocab([x for x in train_tagged.values])
</code></pre>
<p>Which produces:</p>
<pre><code>--------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-18-590096b99bf9&gt; in &lt;module&gt;
----&gt; 1 model_dbow.build_vocab([x for x in train_tagged.values])

c:\python367-64\lib\site-packages\gensim\models\doc2vec.py in build_vocab(self, documents, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)
    926         total_words, corpus_count = self.vocabulary.scan_vocab(
    927             documents=documents, corpus_file=corpus_file, docvecs=self.docvecs,
--&gt; 928             progress_per=progress_per, trim_rule=trim_rule
    929         )
    930         self.corpus_count = corpus_count

c:\python367-64\lib\site-packages\gensim\models\doc2vec.py in scan_vocab(self, documents, corpus_file, docvecs, progress_per, trim_rule)
   1123             documents = TaggedLineDocument(corpus_file)
   1124 
-&gt; 1125         total_words, corpus_count = self._scan_vocab(documents, docvecs, progress_per, trim_rule)
   1126 
   1127         logger.info(

c:\python367-64\lib\site-packages\gensim\models\doc2vec.py in _scan_vocab(self, documents, docvecs, progress_per, trim_rule)
   1069             document_length = len(document.words)
   1070 
-&gt; 1071             for tag in document.tags:
   1072                 _note_doctag(tag, document_length, docvecs)
   1073 

TypeError: 'int' object is not iterable
</code></pre>
<p>I do not understand where the <code>int</code> type is coming from, as a:
<code>print(set([type(x) for x in train_tagged]))</code> yields: <code>{&lt;class 'gensim.models.doc2vec.TaggedDocument'&gt;}</code></p>
<p>Note, additional troubleshooting such as:</p>
<pre><code>train_tagged = train.apply(lambda x: TaggedDocument(words=[get_words(x['Review'])], 
                                                    tags=[x['Sentiment']]), axis=1,)
</code></pre>
<p>yields:</p>
<pre><code>--------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-25-7bd5804d8d95&gt; in &lt;module&gt;
----&gt; 1 model_dbow.build_vocab(train_tagged)

c:\python367-64\lib\site-packages\gensim\models\doc2vec.py in build_vocab(self, documents, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)
    926         total_words, corpus_count = self.vocabulary.scan_vocab(
    927             documents=documents, corpus_file=corpus_file, docvecs=self.docvecs,
--&gt; 928             progress_per=progress_per, trim_rule=trim_rule
    929         )
    930         self.corpus_count = corpus_count

c:\python367-64\lib\site-packages\gensim\models\doc2vec.py in scan_vocab(self, documents, corpus_file, docvecs, progress_per, trim_rule)
   1123             documents = TaggedLineDocument(corpus_file)
   1124 
-&gt; 1125         total_words, corpus_count = self._scan_vocab(documents, docvecs, progress_per, trim_rule)
   1126 
   1127         logger.info(

c:\python367-64\lib\site-packages\gensim\models\doc2vec.py in _scan_vocab(self, documents, docvecs, progress_per, trim_rule)
   1073 
   1074             for word in document.words:
-&gt; 1075                 vocab[word] += 1
   1076             total_words += len(document.words)
   1077 

TypeError: unhashable type: 'list'
</code></pre>
",Vectorization & Embeddings,doc vec build vocab method fails following guide building model created href highlight problem p produce understand type coming yield note additional troubleshooting yield
Normalize vectors in gensim model,"<p>I have a pre-trained word embedding with vectors of different norms, and I want to normalize all vectors in the model. I am doing it with a for loop that iterates each word and normalizes its vector, but the model us huge and takes too much time. Does <code>gensim</code> include any way to do this faster? I cannot find it.</p>
<p>Thanks!!</p>
",Vectorization & Embeddings,normalize vector gensim model pre trained word embedding vector different norm want normalize vector model loop iterates word normalizes vector model u huge take much time doe include way faster find thanks
Do gensim Doc2Vec distinguish between same Sentence with positive and negative context.?,"<p>While learning Doc2Vec library, I got stuck on the following question.</p>

<p><strong>Do gensim Doc2Vec distinguish between the same Sentence with positive and negative context?</strong></p>

<p>For Example:</p>

<p>Sentence A: ""I love Machine Learning""</p>

<p>Sentence B: ""I do not love Machine Learning""</p>

<p>If I train sentence A and B with doc2vec and find cosine similarity between their vectors:</p>

<ol>
<li>Will the model be able to distinguish the sentence and give a cosine similarity very less than 1 or negative?</li>
<li>Or Will the model represent both the sentences very close in vector space and give cosine similarity close to 1, as mostly all the words are same except the negative word (do not).</li>
</ol>

<p>Also, If I train only on sentence A and try to infer Sentence B, will both vectors be close to each other in vector space.?</p>

<p>I would request the NLP community and Doc2Vec experts for helping me out in understanding this.</p>

<p>Thanks in Advance !!</p>
",Vectorization & Embeddings,gensim doc vec distinguish sentence positive negative context learning doc vec library got stuck following question gensim doc vec distinguish sentence positive negative context example sentence love machine learning sentence b love machine learning train sentence b doc vec find cosine similarity vector model able distinguish sentence give cosine similarity le negative model represent sentence close vector space give cosine similarity close mostly word except negative word also train sentence try infer sentence b vector close vector space would request nlp community doc vec expert helping understanding thanks advance
Why are there rows with all values ​0 in the embedding matrix?,"<p>I created the word embedding vector for sentiment analysis. But I'm not sure about the code I wrote. If you see my mistakes while creating Word2vec or embedding matrix, please let me know.</p>
<pre><code>EMBEDDING_DIM=100 
review_lines = [sub.split() for sub in reviews]    
model = gensim.models.Word2Vec(sentences=review_lines,size=EMBEDDING_DIM,window=6,workers=6,min_count=3,sg=1) 
print('Words close to the given word:',model.wv.most_similar('film'))    
words=list(model.wv.vocab) 
print('Words:' , words)

file_name='embedding_word2vec.txt'
model.wv.save_word2vec_format(file_name,binary=False)     
embeddings_index = {}    
f=open(os.path.join('','embedding_word2vec.txt'),encoding=&quot;utf-8&quot;)    
for line in f:    
  values =line.split()    
  word=values[0]   
  coefs=np.asarray(values[1:],dtype='float32')   
  embeddings_index[word]=coefs    
f.close()  
print(&quot;Number of word vectors found:&quot;,len(embeddings_index))
  
embedding_matrix = np.zeros((len(word_index)+1,EMBEDDING_DIM))
for word , i in word_index.items():
  embedding_vector= embeddings_index.get(word)
  if embedding_vector is not None:
    embedding_matrix[i]=embedding_vector

OUTPUT:
array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,
         0.        ,  0.        ],
       [ 0.1029947 ,  0.07595579, -0.06583303, ...,  0.10382118,
        -0.56950015, -0.17402627],
       [ 0.13758609,  0.05489254,  0.0969701 , ...,  0.18532865,
        -0.49845088, -0.23407038],
       ...,
       [ 0.        ,  0.        ,  0.        , ...,  0.        ,
         0.        ,  0.        ]])
</code></pre>
",Vectorization & Embeddings,row value embedding matrix created word embedding vector sentiment analysis sure code wrote see mistake creating word vec embedding matrix please let know
Gensim: how to retrain doc2vec model using previous word2vec model,"<p>With Doc2Vec modelling, I have trained a model and saved following files:</p>

<pre><code>1. model
2. model.docvecs.doctag_syn0.npy
3. model.syn0.npy
4. model.syn1.npy
5. model.syn1neg.npy
</code></pre>

<p>However, I have a new way to label the documents and want to train the model again. since the word vectors already obtained from previous version. Is there any way to reuse that model (e.g., taking the previous w2v results as initial vectors for training)? Any one know how to do it? </p>
",Vectorization & Embeddings,gensim retrain doc vec model using previous word vec model doc vec modelling trained model saved following file however new way label document want train model since word vector already obtained previous version way reuse model e g taking previous w v result initial vector training one know
Maximum value for n_clusters in K Means algorithm,"<p>I have a dataset with 28000 records. The data is of an e-commerce store menu items. The challenge is the following:</p>
<blockquote>
<p>Multiple stores have similar products but with different names. For example, 'HP laptop 1102' is present in different stores as 'HP laptop 1102', 'Hewlett-Packard laptop 1102', 'HP notebook 1102' and many other different names.</p>
</blockquote>
<p>I have opted to convert the product list as a tfidf vector and use KMeans clustering to group similar products together. I am also using some other features like product category, sub category etc. (I have one hot encoded all the categorical features)</p>
<p>Now my challenge is to estimate the optimal n_clusters in KMeans algorithm. As the clustering should occur at product level, I'm assuming I need a high n_clusters value. Is there any upper limit for the n_clusters?</p>
<p>Also any suggestions and advice on the solution approach would be really helpful.
Thanks in advance.</p>
",Vectorization & Embeddings,maximum value n cluster k mean algorithm dataset record data e commerce store menu item challenge following multiple store similar product different name example hp laptop present different store hp laptop hewlett packard laptop hp notebook many different name opted convert product list tfidf vector use kmeans clustering group similar product together also using feature like product category sub category etc one hot encoded categorical feature challenge estimate optimal n cluster kmeans algorithm clustering occur product level assuming need high n cluster value upper limit n cluster also suggestion advice solution approach would really helpful thanks advance
Persisting Spacy Vector Representations in Neo4j,"<p>I have nodes with text as a property stored in a Neo4j database. However, when I read text and create Spacy objects out of them to compute similarity between them and one text entered by user in memory, it takes too much time to execute. I wonder what is the best way to solve this problem?</p>
<p>I know that GraphAware exists, but it only supports OpenNLP and Stanford coreNLP. Another way I would think of is storing Spacy objects as another property to this node, but I do not know about performance when the size of the property is huge.</p>
<p>Thank you!</p>
<p>Bader</p>
",Vectorization & Embeddings,persisting spacy vector representation neo j node text property stored neo j database however read text create spacy object compute similarity one text entered user memory take much time execute wonder best way solve problem know graphaware exists support opennlp stanford corenlp another way would think storing spacy object another property node know performance size property huge thank bader
"How does the BERT tokenizer result in an input tensor shape of (b, 24, 768)?","<p>I understand how the BERT tokenizer works thanks to this article:
<a href=""https://albertauyeung.github.io/2020/06/19/bert-tokenization.html"" rel=""nofollow noreferrer"">https://albertauyeung.github.io/2020/06/19/bert-tokenization.html</a></p>
<p>However, I am confused about how this ends up as the final input shape (b, 24, 768).</p>
<p>When reading the <a href=""https://github.com/codertimo/BERT-pytorch/blob/master/bert_pytorch/model/embedding/bert.py"" rel=""nofollow noreferrer"">code of BERT</a> I noticed this comment about the embeddings.</p>
<pre><code>BERT Embedding which is consisted with under features
    1. TokenEmbedding : normal embedding matrix
    2. PositionalEmbedding : adding positional information using sin, cos
    2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)
    sum of all these features are output of BERTEmbedding
</code></pre>
<p>Does this mean that BERT does the following?</p>
<ol>
<li>Tokenizes a sentence</li>
<li>Put these tokens through a <em><strong>separate system</strong></em>(?) that produces high dimensional embeddings</li>
<li>Creates a positional embedding based on word position in the sentence.</li>
<li>And (I also get confused here) creates a segment embedding providing information about the sentence as a whole (what information?)</li>
<li>This is all added together to create a tensor of shape (b, 24, 768) where 24 words/tokens (plus padding) are each represented in 768-dimensional space.</li>
</ol>
<p>Is this correct? What is the segment embedding information?</p>
",Vectorization & Embeddings,doe bert tokenizer result input tensor shape b understand bert tokenizer work thanks article however confused end final input shape b reading code bert noticed comment embeddings doe mean bert doe following tokenizes sentence put token separate system produce high dimensional embeddings creates positional embedding based word position sentence also get confused creates segment embedding providing information sentence whole information added together create tensor shape b word token plus padding represented dimensional space correct segment embedding information
How to implement word embedding for persian language,"<p>I have this code that works for English language but does not work for Persian language</p>

<pre><code>from gensim.models import Word2Vec as wv
for sentence in sentences:
    tokens = sentence.strip().lower().split("" "")
    tokenized.append(tokens)
model = wv(tokenized
    ,size=5,
          min_count=1)
print('done2')
model.save('F:/text8/text8-phrases1')
print('done3')
print(model)
model = wv.load('F:/text8/text8-phrases1')

print(model.wv.vocab)
</code></pre>

<p>output</p>

<pre><code>&gt; 'بر': &lt;gensim.models.keyedvectors.Vocab object at 0x0000027716EEB0B8&gt;,
&gt; 'اساس': &lt;gensim.models.keyedvectors.Vocab object at
&gt; 0x0000027716EEB160&gt;, 'قوانين': &lt;gensim.models.keyedvectors.Vocab
&gt; object at 0x0000027716EEB198&gt;, 'دانشگاه':
&gt; &lt;gensim.models.keyedvectors.Vocab object at 0x0000027716EEB1D0&gt;,
&gt; 'اصفهان،': &lt;gensim.models.keyedvectors.Vocab object at
&gt; 0x0000027716EEB208&gt;, 'نويسنده': &lt;gensim.models.keyedvectors.Vocab
&gt; object at 0x0000027716EEB240&gt;, 'مسؤول':
&gt; &lt;gensim.models.keyedvectors.Vocab object at 0x0000027716EEB278&gt;,
&gt; 'مقاله': &lt;gensim.models.keyedvectors.Vocab object at
&gt; 0x0000027716EEB2B0&gt;, 'بايد'
</code></pre>

<p>plesae take example with code
thanks</p>
",Vectorization & Embeddings,implement word embedding persian language code work english language doe work persian language output plesae take example code thanks
calculate cosine similarity in Pytorch,"<p>I have a candidate document embedding tensor, namely <code>cdd_doc_embeddings</code> of size <code>[batch_size, cdd_size, signal_length, embedding_dim]</code>, a history clicked document embedding tensor, namely <code>his_doc_embeddings</code> of size <code>[batch_size, his_size, signal_length, embedding_dim]</code>.</p>
<p>Now I want to compute the <code>cosine similarity</code> between them, yielding a tensor <code>fusion_matrix</code> of size <code>[batch_size, cdd_size, his_size, signal_length, signal_length]</code> where <strong>entry [ b,i,j,u,v ] denotes the cosine similarity between the <em>u</em> th word in <em>i</em> th candidate document in <em>b</em> th batch and the <em>v</em> th word in <em>j</em> th history clicked document in <em>b</em> th batch</strong>.</p>
<p>How can I do it efficiently with <strong><code>PyTorch</code></strong>?</p>
",Vectorization & Embeddings,calculate cosine similarity pytorch candidate document embedding tensor namely size history clicked document embedding tensor namely size want compute yielding tensor size entry b j u v denotes cosine similarity u th word th candidate document b th batch v th word j th history clicked document b th batch efficiently
Understanding gensim word2vec&#39;s most_similar,"<p>I am unsure how I should use the most_similar method of gensim's Word2Vec. Let's say you want to test the tried-and-true example of: <em>man stands to king as woman stands to X</em>; find X. I thought that is what you could do with this method, but from the results I am getting I don't think that is true.</p>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar"" rel=""nofollow noreferrer"">The documentation</a> reads:</p>

<blockquote>
  <p>Find the top-N most similar words. Positive words contribute
  positively towards the similarity, negative words negatively.</p>
  
  <p>This method computes cosine similarity between a simple mean of the
  projection weight vectors of the given words and the vectors for each
  word in the model. The method corresponds to the word-analogy and
  distance scripts in the original word2vec implementation.</p>
</blockquote>

<p>I assume, then, that <code>most_similar</code> takes the positive examples and negative examples, and tries to find points in the vector space that are as close as possible to the positive vectors and as far away as possible from the negative ones. Is that correct?</p>

<p>Additionally, is there a method that allows us to map the relation between two points to another point and get the result (cf. the man-king woman-X example)?</p>
",Vectorization & Embeddings,understanding gensim word vec similar unsure use similar method gensim word vec let say want test tried true example man stand king woman stand x find x thought could method result getting think true documentation read find top n similar word positive word contribute positively towards similarity negative word negatively method computes cosine similarity simple mean projection weight vector given word vector word model method corresponds word analogy distance script original word vec implementation assume take positive example negative example try find point vector space close possible positive vector far away possible negative one correct additionally method allows u map relation two point another point get result cf man king woman x example
How to normalize word embeddings (word2vec),"<p>I have a pre trained Word2Vec model with embeddings. I need to normalize some embeddings to do analyses with the words. Is there a simple line (or block) of code to do this? I've been searching online but can't find a simple answer.</p>
",Vectorization & Embeddings,normalize word embeddings word vec pre trained word vec model embeddings need normalize embeddings analysis word simple line block code searching online find simple answer
faster sklearn tf-idf vectorizer,"<p>I was trying to use sklearn's TfidfVectorizer for a project, but Tfidf Vectorizer seems to take up a lot of time...</p>
<pre><code>import spacy
from sklearn.feature_extraction.text import TfidfVectorizer

def tokenize_spacy(sentence):
    nlp = spacy.load('ja_core_news_lg')
    doc = nlp(sentence)
    return [w.text for w in doc]

def read_corpus(filename):
    corpus = []
    with open(filename, 'r', encoding='utf-8') as fin:
        for line in fin:
            line = line.rstrip('\n')
            corpus.append(line)
    return corpus

vectorizer = TfidfVectorizer(tokenizer=tokenize_spacy, ngram_range=(1, 4), stop_words=stop_words)
corpus = read_corpus(args.corpus)
matrix = vectorizer.fit_transform(corpus)

</code></pre>
<p>The model <code>'ja_core_news_lg'</code> is from <a href=""https://spacy.io/models/ja"" rel=""nofollow noreferrer"">here</a>, and the size of the corpus file is 2.7 GB, <code>stop_words</code> is an array whose length is less than 100. The vectorizer has been running for more than 48 hours so I was wondering whether there's a way to more efficiently fit the vectorizer or whether there's a faster substitution.</p>
<p>I have 56 CPUs but this program seems only to run on one of them. I've seen <a href=""https://stackoverflow.com/questions/28396957/sklearn-tfidf-vectorizer-to-run-as-parallel-jobs/36271381#36271381"">this</a> answer but since I need to do <code>vectorizer.get_feature_names()</code>afterward so using <code> HashingVectorizer</code> doesn't seem to be a good fit for me.</p>
<p>Any help will be appreciated and thanks a lot!</p>
",Vectorization & Embeddings,faster sklearn tf idf vectorizer wa trying use sklearn tfidfvectorizer project tfidf vectorizer seems take lot time model size corpus file gb array whose length le vectorizer ha running hour wa wondering whether way efficiently fit vectorizer whether faster substitution cpu program seems run one seen href answer since need afterward using seem good fit p help appreciated thanks lot
"What exactly contains the word vector of Word2Vec (or generally of word embeddings, but we can stay with Word2Vec)","<p>Is is possible to explain it? I have a property that I always need to understand everything. I read somewhere that it is the &quot;context&quot; or the content of hidden neural layer (its weights), but still is it possible to sufficiently explain what exactly is in the vector of Word2Vec?</p>
<p>Is it really some random weights in it nothing more, no smart intention?</p>
<p>Btw I was now checking some basic Neural network and I noticed that in the simplest case is mapped one input to one neuron, meaning one neuron = one weight, but on further diagram i noticed some &quot;fully connection&quot; that all inputs are mapped to all neurons, then it seems one neuron = N weights where N is the number of inputs to it.</p>
<p>But I wonder if Word2vec vector would be really just the hidden layer (the weights) how it can be just let's say 100 weights in case of fully connected inputs, where as I say 1 neuron is more than 1 weight.</p>
<p><a href=""https://i.sstatic.net/cpLIE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cpLIE.png"" alt=""enter image description here"" /></a></p>
",Vectorization & Embeddings,exactly contains word vector word vec generally word embeddings stay word vec possible explain property always need understand everything read somewhere context content hidden neural layer weight still possible sufficiently explain exactly vector word vec really random weight nothing smart intention btw wa checking basic neural network noticed simplest case mapped one input one neuron meaning one neuron one weight diagram noticed fully connection input mapped neuron seems one neuron n weight n number input wonder word vec vector would really hidden layer weight let say weight case fully connected input say neuron weight
Normalizing Fasttext Pretrained Fasttext word embedding,"<p>I am trying to normalize a fasttext word vector to another range so it can be combined with other data.</p>
<p>I first access the pretrained model like this:</p>
<pre><code>fasttext.util.download_model('en', if_exists='ignore')  # English
ft = fasttext.load_model('cc.en.300.bin')
</code></pre>
<p>I am then reducing the model a shorter vector length</p>
<pre><code>fasttext.util.reduce_model(ft, 36)
</code></pre>
<p>In order to normalize I am trying to do something like the following</p>
<p>((value - current_min)/(current_max - current_min)) * (desired_max - desired_min) + desired_min</p>
<p>I was thinking I would just normalize the vectors as I needed them so I wasn't trying to normalize them all at once and then save off another big fasttext object, but in order to do this I need to find the current max and min values in the fasttext object? Is the best way to do this to iterate over every array in the object to find the max and min? I'm confused by what exactly a fasttext object is and the normal min() max() functions won't work on it.</p>
<p>Edit: Forgot to include my current best idea for finding the max and min</p>
<pre><code>fasttext_words = ft.get_words()
max_num = 0
min_num = 0
for word in fasttext_words:
    temp_max = max(ft.get_word_vector(word))
    temp_min = min(ft.get_word_vector(word))
    if temp_max &gt; max_num:
        max_num = temp_max
    if temp_min &lt; min_num:
        min_num = temp_min
</code></pre>
",Vectorization & Embeddings,normalizing fasttext pretrained fasttext word embedding trying normalize fasttext word vector another range combined data first access pretrained model like reducing model shorter vector length order normalize trying something like following value current min current max current min desired max desired min desired min wa thinking would normalize vector needed trying normalize save another big fasttext object order need find current max min value fasttext object best way iterate every array object find max min confused exactly fasttext object normal min max function work edit forgot include current best idea finding max min
Preprocessing a corpus for different Word Embedding Algorithms,"<p>For my Bachelorthesis I need to train different word embedding algorithms on the same corpus to benchmark them.
I am looking to find preprocessing steps but am not sure which ones to use and which ones might be less useful.</p>
<p><em>I already looked for some studies but also wanted to ask if someone has experience with this.</em></p>
<p>My objective is to train Word2Vec, FastText and GloVe Embeddings on the same corpus. Not too sure which one now, but I think of Wikipedia or something similar.</p>
<p>In my opinion:</p>
<ul>
<li>POS-Tagging</li>
<li>remove non-alphabetic characters with regex or similar</li>
<li>Stopword removal</li>
<li>Lemmatization</li>
<li>catching Phrases</li>
</ul>
<p>are the logical options.</p>
<p>But I heard that stopword removal can be kind of tricky, because there is a chance that some embeddings still contain stopwords due to the fact that automatic stopword removal might not fit to any model/corpus.</p>
<p>Also I have not decided if I want to choose spacy or nltk as library, spacy is mightier but nltk is mainly used at the chair I am writing.</p>
",Vectorization & Embeddings,preprocessing corpus different word embedding algorithm bachelorthesis need train different word embedding algorithm corpus benchmark looking find preprocessing step sure one use one might le useful already looked study also wanted ask someone ha experience objective train word vec fasttext glove embeddings corpus sure one think wikipedia something similar opinion po tagging remove non alphabetic character regex similar stopword removal lemmatization catching phrase logical option heard stopword removal kind tricky chance embeddings still contain stopwords due fact automatic stopword removal might fit model corpus also decided want choose spacy nltk library spacy mightier nltk mainly used chair writing
How is the window size affect word2vec and how do we choose window size according to different tasks?,"<p>For example, if I choose two window size, 5 and 50, and train the word2vec model, will the 50 one takes more time to train? Will the embeddings of the 50 one concentrates more on semantics of the text and the 5 one concentrates more on single word?
BTW, above two questions are just my thinking/exmaples of what I am seeking. My real question is just the title &quot;How is the window size affect word2vec and how do we choose window size according to different tasks?&quot;</p>
",Vectorization & Embeddings,window size affect word vec choose window size according different task example choose two window size train word vec model one take time train embeddings one concentrate semantics text one concentrate single word btw two question thinking exmaples seeking real question title window size affect word vec choose window size according different task
Deal with Out of vocabulary word with Gensim pretrained GloVe,"<p>I am working on an NLP assignment and loaded the GloVe vectors provided by Gensim:</p>
<pre><code>import gensim.downloader
glove_vectors = gensim.downloader.load('glove-twitter-25')
</code></pre>
<p>I am trying to get the word embedding for each word in a sentence, but some of them are not in the vocabulary.</p>
<p>What is the best way to deal with it working with the Gensim API?</p>
<p>Thanks!</p>
",Vectorization & Embeddings,deal vocabulary word gensim pretrained glove working nlp assignment loaded glove vector provided gensim trying get word embedding word sentence vocabulary best way deal working gensim api thanks
How can a Word2Vec pretrained model be loaded in Gensim faster?,"<p>I'm loading the model using:</p>
<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) 
</code></pre>
<p>Now every time i run the file in Pycharm, it loads the model again.</p>
<p>So, is there a way to load it once and be available whenever i run things like  <code>model['king']</code>  and <code>model.doesnt_match(&quot;house garage store dog&quot;.split())</code></p>
<p>because it takes alot of time whenever i wana check the similarity or words that don't match.
When i ran <code>model.most_similar('finance')</code> it was really slow and the whole laptop freezed for like 2 min. So, is there a way to make things faster, 'cause i wana use it in my project, but i can't let the user wait for this long.</p>
<p>Any suggestions?</p>
",Vectorization & Embeddings,word vec pretrained model loaded gensim faster loading model using every time run file pycharm load model way load available whenever run thing like take alot time whenever wana check similarity word match ran wa really slow whole laptop freezed like min way make thing faster cause wana use project let user wait long suggestion
What does build_vocab() do exactly?,"<p>I am trying to build a Doc2Vec model. I have a list of sentences with their labels, labeled using Gensim’s LabeledSentence() function. After building the model, I see that they used build_vocab() on the labeled sentences before training the model.</p>
<p>Can someone explain what does build_vocab() do and what happens if I don't use it !?</p>
<p>Please check out the following pictures:</p>
<p><a href=""https://i.sstatic.net/1OPNV.png"" rel=""nofollow noreferrer"">labeled sentences</a></p>
<p><a href=""https://i.sstatic.net/SsedC.png"" rel=""nofollow noreferrer"">model</a></p>
",Vectorization & Embeddings,doe build vocab exactly trying build doc vec model list sentence label labeled using gensim labeledsentence function building model see used build vocab labeled sentence training model someone explain doe build vocab happens use please check following picture labeled sentence model
"Input 0 of layer lstm_5 is incompatible with the layer: expected ndim=3, found ndim=2","<p>I am trying to create an image captioning model. Could you please help with this error? input1 is the image vector, input2 is the caption sequence. 32 is the caption length. I want to concatenate the image vector with the embedding of the sequence and then feed it to the decoder model.</p>

<pre><code>
    def define_model(vocab_size, max_length):
      input1 = Input(shape=(512,))
      input1 = tf.keras.layers.RepeatVector(32)(input1)
      print(input1.shape)

      input2 = Input(shape=(max_length,))
      e1 = Embedding(vocab_size, 512, mask_zero=True)(input2)
      print(e1.shape)

      dec1 = tf.concat([input1,e1], axis=2)
      print(dec1.shape)

      dec2 = LSTM(512)(dec1)
      dec3 = LSTM(256)(dec2)
      dec4 = Dropout(0.2)(dec3)
      dec5 = Dense(256, activation=""relu"")(dec4)
      output = Dense(vocab_size, activation=""softmax"")(dec5)
      model = tf.keras.Model(inputs=[input1, input2], outputs=output)
      model.compile(loss=""categorical_crossentropy"", optimizer=""adam"")
      print(model.summary())
      return model

</code></pre>

<pre><code>ValueError: Input 0 of layer lstm_5 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 512]
</code></pre>
",Vectorization & Embeddings,input layer lstm incompatible layer expected ndim found ndim trying create image captioning model could please help error input image vector input caption sequence caption length want concatenate image vector embedding sequence feed decoder model
Using Word2Vec for topic modeling,"<p>I have read that the most common technique for topic modeling (extracting possible topics from text) is Latent Dirichlet allocation (LDA).</p>

<p>However, I am interested whether it is a good idea to try out topic modeling with Word2Vec as it clusters words in vector space. Couldn't the clusters therefore be regarded as topics?</p>

<p>Do you think it makes sense to follow this approach for the sake of some research? In the end what I am interested in is to extract keywords from text according to topics.</p>
",Vectorization & Embeddings,using word vec topic modeling read common technique topic modeling extracting possible topic text latent dirichlet allocation lda however interested whether good idea try topic modeling word vec cluster word vector space cluster therefore regarded topic think make sense follow approach sake research end interested extract keywords text according topic
Calculate cosine similarity between sets of document and key words (e.g. &quot;innovate&#39; &quot;fast&quot;),"<p>I have a set of documents that describe different dimensions of corporate culture. Tokenized examples below:</p>
<pre><code>sent1=['innovative','culture','fast','moving','company']
sent2=['manager','micromanage','all','time']
sent3=['slow','response','customer']
</code></pre>
<p>I've already applied Glove and Gensim w2v to the above documents. I'd like to identify documents that have high cosine similarity score to a sets of word, such as
<code>Innovation =['innovate','innovative','fast']</code></p>
<p>How do I calculate the cosine similarities between each document (e.g. sent1, sent2) and <code>Innovation</code> using Gensim?</p>
<p>Ideal Output:</p>
<pre><code>       innovation
sent1  0.98
sent2  0.45
sent3  -0.2
</code></pre>
",Vectorization & Embeddings,calculate cosine similarity set document key word e g innovate fast set document describe different dimension corporate culture tokenized example already applied glove gensim w v document like identify document high cosine similarity score set word calculate cosine similarity document e g sent sent using gensim ideal output
word2vec cosine similarity greater than 1 arabic text,"<p>I have trained my <code>word2vec</code> model from <code>gensim</code> and I am getting the nearest neighbors for some words in the corpus. Here are the similarity scores:</p>
<pre><code> top neighbors for الاحتلال:
الاحتلال: 1.0000001192092896
الاختلال: 0.9541053175926208
الاهتلال: 0.872565507888794
الاحثلال: 0.8386293649673462
الاكتلال: 0.8209128379821777
</code></pre>
<p>It is odd to get a similarity greater than 1. I cannot apply any stemming to my text because the text includes many OCR spelling mistakes (I got the text from ORC-ed documents). How can I fix the issue ?</p>
<p><strong>Note</strong> I am using <code>model.similarity(t1, t2)</code></p>
<p><strong>This is how I trained my Word2Vec Model:</strong></p>
<pre><code>    documents = list()
    tokenize = lambda x: gensim.utils.simple_preprocess(x)
    t1 = time.time()
    docs = read_files(TEXT_DIRS, nb_docs=5000)
    t2 = time.time()
    print('Reading docs took: {:.3f} mins'.format((t2 - t1) / 60))
    print('Number of documents: %i' % len(docs))

    # Training the model
    model = gensim.models.Word2Vec(docs, size=EMBEDDING_SIZE, min_count=5)
    if not os.path.exists(MODEL_DIR):
        os.makedirs(MODEL_DIR)
    model.save(os.path.join(MODEL_DIR, 'word2vec'))

    weights = model.wv.vectors
    index_words = model.wv.index2word

    vocab_size = weights.shape[0]
    embedding_dim = weights.shape[1]

    print('Shape of weights:', weights.shape)
    print('Vocabulary size: %i' % vocab_size)
    print('Embedding size: %i' % embedding_dim)

</code></pre>
<p>Below is the read_files function I defined:</p>
<pre><code>def read_files(text_directories, nb_docs):
    &quot;&quot;&quot;
    Read in text files
    &quot;&quot;&quot;
    documents = list()
    tokenize = lambda x: gensim.utils.simple_preprocess(x)
    print('started reading ...')
    for path in text_directories:
        count = 0
        # Read in all files in directory
        if os.path.isdir(path):
            all_files = os.listdir(path)
            for filename in all_files:
                if filename.endswith('.txt') and filename[0].isdigit():
                    count += 1
                    with open('%s/%s' % (path, filename), encoding='utf-8') as f:
                        doc = f.read()
                        doc = clean_text_arabic_style(doc)
                        doc = clean_doc(doc)
                        documents.append(tokenize(doc))
                        if count % 100 == 0:
                            print('processed {} files so far from {}'.format(count, path))
                if count &gt;= nb_docs and count &lt;= nb_docs + 200:
                    print('REACHED END')
                    break
        if count &gt;= nb_docs and count &lt;= nb_docs:
            print('REACHED END')
            break

    return documents

</code></pre>
<p>I tried <a href=""https://stackoverflow.com/questions/41387000/cosine-similarity-of-word2vec-more-than-1"">this</a> thread but it won't help me because I rather have <code>arabic</code> and <strong>misspelled</strong> text</p>
<p><strong>Update</strong>
I tried the following: (getting the similarity between the exact same word)</p>
<pre><code>print(model.similarity('الاحتلال','الاحتلال'))
</code></pre>
<p>and it gave me the following result:</p>
<pre><code>1.0000001
</code></pre>
",Vectorization & Embeddings,word vec cosine similarity greater arabic text trained model getting nearest neighbor word corpus similarity score odd get similarity greater apply stemming text text includes many ocr spelling mistake got text orc ed document fix issue note using trained word vec model read file function defined tried href thread help rather strong misspelled text update tried following getting similarity exact word gave following result
Measure similarity between two documents using Doc2Vec,"<p>I have already trained gensim doc2Vec model, which is finding most similar documents to an unknown one.</p>

<p>Now I need to find the similarity value between two unknown documents (which were not in the training data, so they can not be referenced by doc id)</p>

<pre><code>d2v_model = doc2vec.Doc2Vec.load(model_file)

string1 = 'this is some random paragraph'
string2 = 'this is another random paragraph'

vec1 = d2v_model.infer_vector(string1.split())
vec2 = d2v_model.infer_vector(string2.split())
</code></pre>

<p>in the code above vec1 and vec2 are successfully initialized to some values and of size - 'vector_size'</p>

<p>now looking through the gensim api and examples I could not find method that works for me, all of them are expecting TaggedDocument</p>

<p>Can I compare the feature vectors value by value and if they are closer => the texts are more similar?</p>
",Vectorization & Embeddings,measure similarity two document using doc vec already trained gensim doc vec model finding similar document unknown one need find similarity value two unknown document training data referenced doc id code vec vec successfully initialized value size vector size looking gensim api example could find method work expecting taggeddocument compare feature vector value value closer text similar
Right way to calculate the cosine similarity of two word-frequency-dictionaries in python?,"<p>I'm trying to iterate through a file containing text and calculate the cosine similarity between the current line and a query the user raised. I have already tokenized the query and the line and saved the union of their words into a set.</p>

<p>Example:</p>

<pre><code>line_tokenized = ['Karl', 'Donald', 'Ifwerson']

query_tokenized = ['Donald', 'Trump']

word_set = ['Karl', 'Donald', 'Ifwerson', 'Trump']
</code></pre>

<p>Now I have to create a dictionary each for the line and the query, containing word-frequency pairs. I thought about something ike this:</p>

<pre><code>line_dict = {'Karl': 1, 'Donald': 1, 'Ifwerson': 1, 'Trump': 0}
query_dict = {'Karl': 0, 'Donald': 1, 'Ifwerson': 0, 'Trump': 1}
</code></pre>

<p>But the cosine similarity won't be calculated properly as the key-value pairs are unordered. I came across <code>OrderedDict()</code>, but I don't understand how to implement some things as it's elements are stored as tuples: </p>

<p>So my questions are:</p>

<ul>
<li>How can I set the key-value pairs and have access to them afterwards?</li>
<li>How can I increment the value of a certain key?</li>
<li>Or is there any other more easier way to do this?</li>
</ul>
",Vectorization & Embeddings,right way calculate cosine similarity two word frequency dictionary python trying iterate file containing text calculate cosine similarity current line query user raised already tokenized query line saved union word set example create dictionary line query containing word frequency pair thought something ike cosine similarity calculated properly key value pair unordered came across understand implement thing element stored tuples question set key value pair access afterwards increment value certain key easier way
SciSpacy equivalent of Gensim&#39;s functions/parameters,"<p>With Gensim, there are three functions I use regularly, for example this one:</p>
<pre><code>model = gensim.models.Word2Vec(corpus,size=100,min_count=5)
</code></pre>
<p>The output from gensim, but I cannot understand how to set the size and min_count parameters in the equivalent SciSpacy command of:</p>
<pre><code>model = spacy.load('en_core_web_md')
</code></pre>
<p>(The output is a model of embeddings (too big to add here))).</p>
<p>This is another command I regularly use:</p>
<pre><code>model.most_similar(positive=['car'])
</code></pre>
<p>and this is the output from gensim/Expected output from SciSpacy:</p>
<pre><code>[('vehicle', 0.7857330441474915),
 ('motorbike', 0.7572781443595886),
 ('train', 0.7457204461097717),
 ('honda', 0.7383008003234863),
 ('volkswagen', 0.7298516035079956),
 ('mini', 0.7158907651901245),
 ('drive', 0.7093928456306458),
 ('driving', 0.7084407806396484),
 ('road', 0.7001082897186279),
 ('traffic', 0.6991947889328003)]
</code></pre>
<p>This is the third command I regularly use:</p>
<pre><code>print(model.wv['car'])
</code></pre>
<p>Output from Gensim/Expected output from SciSpacy (in reality this vector is length 100):</p>
<pre><code>    [ 1.0942473   2.5680697  -0.43163642 -1.171171    1.8553845  -0.3164575
  1.3645878  -0.5003705   2.912658    3.099512    2.0184739  -1.2413547
  0.9156444  -0.08406237 -2.2248871   2.0038593   0.8751471   0.8953876
  0.2207374  -0.157277   -1.4984075   0.49289042 -0.01171476 -0.57937795...]
</code></pre>
<p>Could someone show me the equivalent commands for SciSpacy? For example, for 'gensim.models.Word2Vec' I can't find how to specify the length of the vectors (size parameter), or the minimum number of times the word should be in the corpus (min_count) in SciSpacy (e.g. I looked <a href=""https://spacy.io/usage/vectors-similarity"" rel=""nofollow noreferrer"">here</a> and <a href=""https://github.com/allenai/scispacy"" rel=""nofollow noreferrer"">here</a>), but I'm not sure if I'm missing them?</p>
",Vectorization & Embeddings,scispacy equivalent gensim function parameter gensim three function use regularly example one output gensim understand set size min count parameter equivalent scispacy command output model embeddings big add another command regularly use output gensim expected output scispacy third command regularly use output gensim expected output scispacy reality vector length could someone show equivalent command scispacy example gensim model word vec find specify length vector size parameter minimum number time word corpus min count scispacy e g looked sure missing
Applying word embedding function on a large pandas dataset,"<p>We have a pandas dataframe with one column ('message') and 3.9 million rows and need to convert these messages to their word embeddings using Google's Universal Sentence encoder.</p>
<p>We tried the following options but got the following issues</p>
<pre><code>import pandas as pd
import tensorflow_hub as hub

url = 'https://tfhub.dev/google/universal-sentence-encoder/4'
embed = hub.load(url)

def __return_embedding(entry):
  return embed([entry])[0].numpy().tolist()

csv_files = [csvfile for csvfile in os.listdir(os.getcwd()) if csvfile.endswith(&quot;.&quot; + 'csv')]
csvs = [pd.read_csv(f'{os.getcwd()}/{csv}', low_memory=False) for csv in csv_files]
dataframe = pd.concat(csvs, axis=0, ignore_index=True)

embeddings = __return_embedding(dataframe.message) # this says InvalidArgumentError:  input must be a vector, got shape: [1,3900000]

dataframe['embedding'] = np.vectorize(__return_embedding)(dataframe['message']) # This one blows up the RAM on google colab and it crashes.

</code></pre>
<p>Does someone have a better way to get this done ?</p>
",Vectorization & Embeddings,applying word embedding function large panda dataset panda dataframe one column message million row need convert message word embeddings using google universal sentence encoder tried following option got following issue doe someone better way get done
How to predict next word using Embedding,"<p>I want to predict the next word in Tensorflow. Before, I was saving one vector for each word as much as all the unique words, but this takes up a lot of memory, so I want to use embedding for this, but I'm a little confused about the dimensions of the vectors because in this method We use Integer numbers instead of 0 and 1.</p>
<p>I wrote this code:</p>
<pre><code># previous_words_list is a list of 5 consecutive words (inputs)
# next_words is a list of the sixth word in each word sequence (labels)
words_length_embedded = 50
number_of_previous_words = 5
inputs = np.zeros((len(previous_words_list), number_of_previous_words), dtype=float)
labels = np.zeros((len(next_words), len(unique_words)), dtype=float)
print(inputs.shape)
print(labels.shape)
for i, each_words in enumerate(previous_words_list):
    for j, each_word in enumerate(each_words):
        inputs[i, j] =  unique_word_index[each_word]
    labels[i] = unique_word_index[next_words[i]]
# for example: &quot;inputs&quot; is: [[22,1,34,5,7], ...] and &quot;labels&quot; is: [44, ...]      ???????

model = Sequential()
model.add(Embedding(len(unique_words), words_length_embedded, input_length=number_of_previous_words, trainable=True))
model.add(LSTM(256, input_shape=(number_of_previous_words, words_length_embedded), return_sequences=False))
model.add(Dense(len(unique_words)))    #  ???????
model.add(Activation('softmax'))
optimizer = 'adam'
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
</code></pre>
<p>I know that input of Dense layer or inputs and labels dimensions is not true but what should these be?</p>
",Vectorization & Embeddings,predict next word using embedding want predict next word tensorflow wa saving one vector word much unique word take lot memory want use embedding little confused dimension vector method use integer number instead wrote code know input dense layer input label dimension true
Random forest model not working for prediction,"<p>I training a random forest model to predict title cluster. The issue is running in notebook, the predicted cluster is correct. But when uploading random forest model to the flask, the predicted becomes same for all input.
Would you like to give some suggestions? Thanks.</p>
<pre><code>feature_dim = 2 ** 10
vectorizer = TfidfVectorizer(max_features=feature_dim)
vectorizer.fit_transform(df['text'].values)


text = df['text'].values
X = vectorizer.fit_transform(text)   

rf_model = RandomForestClassifier(n_estimators=100)
rf_model.fit(X1_train, y1_train)

pickle.dump(rf_model, open('rf_model.sav', 'wb'))

rf_model = load('rf_model.sav')

titles = [
    &quot;title_1&quot;
    &quot;title_2&quot;,
]

X_ti = vectorizer.transform(titles)
y_rf = rf_model.predict(X_ti)
print(y_rf)
</code></pre>
<p>Results look like: [8 8 8 8 8 8 8]</p>
<p>Is it caused by not dumping tfidf vector feature?</p>
",Vectorization & Embeddings,random forest model working prediction training random forest model predict title cluster issue running notebook predicted cluster correct uploading random forest model flask predicted becomes input would like give suggestion thanks result look like caused dumping tfidf vector feature
Mapping text to Mel Spectrogram and conversion of text to input feature representation in Tacotron 2,"<p>I'm trying to understand how text is converted to Mel spectrograms.</p>
<p>I'm having difficulty understanding how the <strong>text</strong> is mapped to the <strong>Mel spectrogram</strong> according to the figure attached and also what each of the blocks inside (character embedding, 3 conv layers and bi-directional LSTM) from another figure is doing to convert text into input feature representation according to the paper <strong>Tacotron-2</strong>.</p>
<p>I have looked upon several online articles but haven't found an explanation. So please help in explaining how it works.</p>
<p><a href=""https://i.sstatic.net/NfvCY.jpg"" rel=""nofollow noreferrer"">Text mapping to Mel Spectrogram</a>,
<a href=""https://i.sstatic.net/Q3KBS.jpg"" rel=""nofollow noreferrer"">Part of Architecture from Tacotron-2</a></p>
",Vectorization & Embeddings,mapping text mel spectrogram conversion text input feature representation tacotron trying understand text converted mel spectrogram difficulty understanding text mapped mel spectrogram according figure attached also block inside character embedding conv layer bi directional lstm another figure convert text input feature representation according paper tacotron looked upon several online article found explanation please help explaining work text mapping mel spectrogram part architecture tacotron
What is a projection layer in the context of neural networks?,"<p>I am currently trying to understand the architecture behind the <em>word2vec</em> neural net learning algorithm, for representing words as vectors based on their context.</p>

<p>After reading <a href=""http://arxiv.org/pdf/1301.3781v3.pdf"" rel=""noreferrer"">Tomas Mikolov paper</a> I came across what he defines as a <strong>projection layer</strong>. Even though this term is widely used when referred to <em>word2vec</em>, I couldn't find a precise definition of what it actually is in the neural net context.</p>

<p><a href=""https://i.sstatic.net/W46yb.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/W46yb.png"" alt=""Word2Vec Neural Net architecture""></a></p>

<p>My question is, in the neural net context, what is a projection layer? Is it the name given to a hidden layer whose links to previous nodes share the same weights? Do its units actually have an activation function of some kind?</p>

<p>￼Another resource that also refers more broadly to the problem can be found in <a href=""http://www.coling-2014.org/COLING%202014%20Tutorial-fix%20-%20Tomas%20Mikolov.pdf"" rel=""noreferrer"">this tutorial</a>, which also refers to a <em>projection layer</em> around page 67.</p>
",Vectorization & Embeddings,projection layer context neural network currently trying understand architecture behind word vec neural net learning algorithm representing word vector based context reading tomas mikolov paper came across defines projection layer even though term widely used referred word vec find precise definition actually neural net context question neural net context projection layer name given hidden layer whose link previous node share weight unit actually activation function kind another resource also refers broadly problem found tutorial also refers projection layer around page
"gensim most_similar with positive and negative, how does it work?","<p>I was reading <a href=""https://stackoverflow.com/a/54581599/7339624"">this answer</a> That says about Gensim <code>most_similar</code>:</p>
<blockquote>
<p>it performs vector arithmetic: adding the positive vectors,
subtracting the negative, then from that resulting position, listing
the known-vectors closest to that angle.</p>
</blockquote>
<p>But when I tested it, that is not the case. I trained a Word2Vec with Gensim <code>&quot;text8&quot;</code> dataset and tested these two:</p>
<pre><code>model.most_similar(positive=['woman', 'king'], negative=['man'])

&gt;&gt;&gt; [('queen', 0.7131118178367615), ('prince', 0.6359186768531799),...]
</code></pre>
<hr />
<pre><code>model.wv.most_similar([model[&quot;king&quot;] + model[&quot;woman&quot;] - model[&quot;man&quot;]])

&gt;&gt;&gt; [('king', 0.84305739402771), ('queen', 0.7326322793960571),...]
</code></pre>
<p>They are clearly not the same. even the queen score in the first is <code>0.713</code> and on the second <code>0.732</code> which are not the same.</p>
<p><strong>So</strong> I ask the question again, How does Gensim <code>most_similar</code> work? why the result of the two above are different?</p>
",Vectorization & Embeddings,gensim similar positive negative doe work wa reading href answer say gensim performs vector arithmetic adding positive vector subtracting negative resulting position listing known vector closest angle tested case trained word vec gensim dataset tested two clearly even queen score first second ask question doe gensim work result two different
Can we use GPT-2 sentence embedding for classification tasks?,"<p>I am experimenting on the use of transformer embeddings in sentence classification tasks <strong>without finetuning them</strong>. I have used BERT embeddings and those experiments gave me very good results. Now I want to use GPT-2 embeddings (without fine-tuning). So I have two questions,</p>

<ol>
<li>Can I use GPT-2 embeddings like that (because I know Gpt-2 is
trained on the left to right) </li>
<li>Is there any example uses of GPT-2 in
    classification tasks other than generation tasks?</li>
<li>If I can use GPT-2embeddings, how should I do it?</li>
</ol>
",Vectorization & Embeddings,use gpt sentence embedding classification task experimenting use transformer embeddings sentence classification task without finetuning used bert embeddings experiment gave good result want use gpt embeddings without fine tuning two question use gpt embeddings like know gpt trained left right example us gpt classification task generation task use gpt embeddings
Is there a function to print out the most similar sentence in spaCy?,"<p>I have a txt file containing 10 movie synopses. I have a separate synopsis for the Hulk movie stored as a string in a variable. I need to compare the 10 synopses to that of the Hulk, to find the most similar movie to recommend. My code is as below:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy

nlp = spacy.load('en_core_web_lg')

hulk_description = &quot;&quot;&quot;Will he save their world or destroy it? When the Hulk becomes too dangerous for the
Earth, the Illuminati trick Hulk into a shuttle and launch him into space to a
planet where the Hulk can live in peace. Unfortunately, Hulk land on the
planet Sakaar where he is sold into slavery and trained as a gladiator.&quot;&quot;&quot;

hulk = nlp(hulk_description)

movies = []

with open('movies.txt', 'r') as f_in:
    for line in map(str.strip, f_in):
        if not line:
            continue
        tmp = line.split()
        movies.append(line)

for token in movies:
    token = nlp(token)
    print(token.similarity(hulk))
</code></pre>
<p>So this works, and it prints out the following:</p>
<pre><code>0.9299734027118595
0.9045154830561336
0.9248706809139479
0.6760996697288897
0.8521583959686228
0.9340271750528514
0.9251483541429658
0.8806094116148976
0.8709798309015676
0.8489256857995392
</code></pre>
<p>I can see that the 6th movie synopsis is the most similar at 0.9340271750528514. But my question is; is there a function in spaCy that would allow me to print out only the most similar sentence after I've done the comparison? i.e I basically want to compare all of them and then recommend the most similar movie by showing its synopsis.</p>
",Vectorization & Embeddings,function print similar sentence spacy txt file containing movie synopsis separate synopsis hulk movie stored string variable need compare synopsis hulk find similar movie recommend code work print following see th movie synopsis similar question function spacy would allow print similar sentence done comparison e basically want compare recommend similar movie showing synopsis
Implementing algorithm for closest vector search with O(log(n)),"<ol>
<li>Assume I have n documents represented as unit vectors, call it X.</li>
<li>I have the vector representation of one document, call it Xi. </li>
<li>How can I find the closest* vector in X to Xi without brut-force search (linear time).</li>
</ol>

<p>*Distance can be L2; proportionally equals cosine-similarity when we talk about unit vectors.</p>

<p>My approximate approach (constant time):
 1. Sort all the documents for each vector dimension.
 2. The use the sorting index to brute-force only through a subset of the data: f.e. include all the closest 1000 documents for each vector dimension, brut-force calculate L2 distance through those documents (1000) which appear close in all (or most) dimensions. (max. 1000)</p>

<p>I would like to know however if there is a ""cleaner"" exact solution like the divide and conquer algorithm for the Closest Pair of Points problem, which runs in log(n) time.</p>

<p>PS: Memory should scale linearly as well. But this should be fine. </p>

<p>Example: I store the 100 dimension vector representations for 1M documents as 32bit floats.</p>

<ul>
<li>Vector representations: 1M*100 dims*32bit = 3.2Gbit = 400MB</li>
<li>Sorting indexes: 1M*100 sorts*32bit = 3.2Gbit = 400MB</li>
</ul>
",Vectorization & Embeddings,implementing algorithm closest vector search log n assume n document represented unit vector call x vector representation one document call xi find closest vector x xi without brut force search linear time distance l proportionally equal cosine similarity talk unit vector approximate approach constant time sort document vector dimension use sorting index brute force subset data f e include closest document vector dimension brut force calculate l distance document appear close dimension max would like know however cleaner exact solution like divide conquer algorithm closest pair point problem run log n time p memory scale linearly well fine example store dimension vector representation document bit float vector representation dims bit gbit mb sorting index sort bit gbit mb
Add word embedding to word2vec gensim model,"<p>I'm looking for a way to dinamically add pre-trained word vectors to a word2vec gensim model.</p>

<p>I have a pre-trained word2vec model in a txt (words and their embedding) and I need to get Word Mover's Distance (for example via <a href=""https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.wmdistance.html"" rel=""noreferrer"" title=""gensim.models.Word2Vec.wmdistance"">gensim.models.Word2Vec.wmdistance</a>) between documents in a specific corpus and a new document. </p>

<p>To prevent the need to load the whole vocabulary, I would want to load only the subset of the pre-trained model's words that are found in the corpus. But if the new document has words that are not found in the corpus but they are in the original model vocabulary add them to the model so they are considered in the computation.</p>

<p>What I want is to save RAM, so possible things that would help me:</p>

<ul>
<li>Is there a way to add the word vectors directly to the model?</li>
<li>Is there a way to load to gensim from a matrix or another object? I could have that object in RAM and append to it the new words before loading them in the model</li>
<li>I don't need it to be on gensim, so if you know a different implementation for WMD that gets the vectors as input that would work (though I do need it in Python)</li>
</ul>

<p>Thanks in advance. </p>
",Vectorization & Embeddings,add word embedding word vec gensim model looking way dinamically add pre trained word vector word vec gensim model pre trained word vec model txt word embedding need get word mover distance example via gensim model word vec wmdistance document specific corpus new document prevent need load whole vocabulary would want load subset pre trained model word found corpus new document ha word found corpus original model vocabulary add model considered computation want save ram possible thing would help way add word vector directly model way load gensim matrix another object could object ram append new word loading model need gensim know different implementation wmd get vector input would work though need python thanks advance
Why most_similar in word2vec doesn&#39;t consider the term itself?,"<p>I am working on a entity similarity project. The <code>most_similar</code> in word2vec gensim model works fine in this regard. However, I also want the search term itself to be included in the outcome. It should be something like this:</p>
<pre><code>&gt;&gt;&gt; model = Word2Vec(sw_token, min_count=2)
&gt;&gt;&gt; model = gensim.models.KeyedVectors.load(&quot;model.bin&quot;)
&gt;&gt;&gt; model.wv.most_similar(&quot;melanoma&quot;, topn=5)

[('melanoma', 1.000000),
 ('cutaneous', 0.6512814164161682),
 ('uveal', 0.6295092701911926),
 ('gp100', 0.617050290107727),
 ('ligand-bearing', 0.614188551902771)]
</code></pre>
<p>The official documents doesn't shows anything which can help me here. Also, if there are terms such as <code>melanoma xyz</code> how can we get such a word as closer in most_similar? I understand that it will take word into account so two words count as 2 not one therefore, they are not similar here. maybe. Thanks.</p>
",Vectorization & Embeddings,similar word vec consider term working entity similarity project word vec gensim model work fine regard however also want search term included outcome something like official document show anything help also term get word closer similar understand take word account two word count one therefore similar maybe thanks
Google BERT and antonym detection,"<p>I recently learned about the following phenomenon: Google BERT word embeddings of well-known state-of-the-art models seem to ignore the measure of semantical contrast between antonyms in terms of the natural distance(norm2 or cosine distance) between the corresponding embeddings. For example:</p>
<p><a href=""https://i.sstatic.net/c59kj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/c59kj.png"" alt=""enter image description here"" /></a></p>
<p>The measure is the &quot;cosine distance&quot; (as oppose to the &quot;cosine similarity&quot;), that means closer vectors are supposed to have smaller distance between them. As one can see, BERT states &quot;weak&quot; and &quot;powerful&quot; to be closer than &quot;strong&quot; and &quot;powerfull&quot;. The same behavior reproduces with the norm2 distance:</p>
<p><a href=""https://i.sstatic.net/gQPvt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gQPvt.png"" alt=""enter image description here"" /></a></p>
<p>Is anyone familiar with this behavior and how one can overcome it and detect a contrast between word senses?</p>
",Vectorization & Embeddings,google bert antonym detection recently learned following google bert word embeddings well known state art model seem ignore measure semantical contrast antonym term natural distance norm cosine distance corresponding embeddings example measure cosine distance oppose cosine similarity mean closer vector supposed smaller distance one see bert state weak powerful closer strong powerfull behavior reproduces norm distance anyone familiar behavior one overcome detect contrast word sens
Imbalance Dataset Model Predicting only One Class,"<p>I have an imabalance dataset of sentiment analysis on news, this is the distribution:</p>
<pre><code>negative: 0
neutral: 1
positive: 2

1    92990
0      158
2       91
Name: sentiment, dtype: int64
</code></pre>
<p>In order to work with this dataset, I first oversample the dataset with <code>imblearn</code> library, so I get this distribution of the classes:</p>
<pre><code>1    92990
0    92990
2    92990
Name: sentiment, dtype: int64
</code></pre>
<p>For my model I'm using a Transformes with attention and a pretrained embedding layer. For the loss funcition I'm using <a href=""https://github.com/maozezhong/focal_loss_multi_class"" rel=""nofollow noreferrer"">this</a> focal loss:</p>
<p>And I'm also defining the classes weights for each class with <code>sklearn</code>:</p>
<pre><code>from sklearn.utils import class_weight
class_weights = class_weight.compute_class_weight('balanced',np.unique(y),y)
</code></pre>
<p>I have been conbining this methods in order to solve the imbalance issue but none of this seems to be working. The best result I got was using the oversampled dataset, combined with focal loss and setting class weigths, and this is my confusion matrix:</p>
<pre><code>from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix

y_pred = [np.argmax(p) for p in model.predict(y)]
cm = confusion_matrix(y_true=[np.argmax(i) for i in y], y_pred=y_pred, labels=[0,1,2])

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1,2]).plot(include_values=True)
</code></pre>
<p><a href=""https://i.sstatic.net/qG7Om.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qG7Om.png"" alt=""enter image description here"" /></a></p>
<p>I don't know if I'm doing something wrong calculating the confusion matrix but my model doesn't seems to be learning, any suggestions to improve?</p>
<p>EDIT: <a href=""https://github.com/dr-alberto/ImbalanceSentiment"" rel=""nofollow noreferrer"">Here</a> is a repo with the datasets and the notebook, feel free to take a look and check what's wrong</p>
",Vectorization & Embeddings,imbalance dataset model predicting one class imabalance dataset sentiment analysis news distribution order work dataset first oversample dataset library get distribution class model using transformes attention pretrained embedding layer loss funcition using focal loss also defining class weight class conbining method order solve imbalance issue none seems working best result got wa using oversampled dataset combined focal loss setting class weigths confusion matrix know something wrong calculating confusion matrix model seems learning suggestion improve edit repo datasets notebook feel free take look check wrong
"(NLP, TextCNN) Why in_channels equals embedding size in torch.nn.Conv1d?","<p>If we apply <code>torch.nn.Conv2d</code> to a &quot;RGB&quot; image which can be understood as 3 two-dimensional matrices, so parameter <code>in_channels</code> corresponds to the 3 channels 'R', 'G' and 'B'. And in my view, an embedded sentence whose shape is <code>[sentence length, embedding size]</code> should be considered as 1 two-dimensional matrix, so in this case, why parameter <code>in_channels</code> is not 1 but embedding size in <code>torch.nn.Conv1d</code>, not the same meaning as <code>torch.nn.Conv2d</code>?</p>
<p>Could you explain what's the true meaning of <code>in_channels</code> in <code>torch.nn.Conv1d</code> in nlp / TextCNN? Why it's different from <code>torch.nn.Conv2d</code>?</p>
<p>Thanks!</p>
",Vectorization & Embeddings,nlp textcnn channel equal embedding size torch nn conv apply rgb image understood two dimensional matrix parameter corresponds channel r g b view embedded sentence whose shape considered two dimensional matrix case parameter embedding size meaning could explain true meaning nlp textcnn different thanks
Difference between Text Embedding and Word Embedding,"<p>I am working on a dataset of amazon alexa reviews and wish to cluster them in positive and negative clusters. I am using Word2Vec for vectorization so wanted to know the difference between <strong>Text Embedding</strong> and <strong>Word Embedding</strong>. Also, which one of them will be useful for my clustering of reviews (Please consider that I want to predict the cluster of any reviews that I enter.)
Thanks in advance!</p>
",Vectorization & Embeddings,difference text embedding word embedding working dataset amazon alexa review wish cluster positive negative cluster using word vec vectorization wanted know difference text embedding word embedding also one useful clustering review please consider want predict cluster review enter thanks advance
How to find Sentence Similarity using deep learning?,"<p>I am trying to find sentence similarity through word emebeddings and then applying cosine similarity score. Tried CBOW/Skip Gram methods for embedding but did not solve the problem.</p>

<p>I am doing this for product review data. I have two columns:</p>

<pre><code>SNo         Product_Title                                Customer_Review   
 1       101.x battery works well                    I have an Apple phone and it's not that
          with Samsung smart phone                     that great.

 2       112.x battery works well                     I have samsung smart tv and I tell that it's
         with Samsung smart phone                     not wort buying.

 3      112.x battery works well                      This charger works very well with samsung 
        with Samsung smart phone.                      phone. It is fast charging.
</code></pre>

<p>The first two reviews are <code>irrelevant</code> as semantic meaning of <code>Product_Title</code> and <code>Customer_Review</code>  are completely different.</p>

<p>How can an algorithm find this semantic meaning of sentences and score them.</p>

<p>My Approach:</p>

<ol>
<li><p>Text pre-processing</p></li>
<li><p>Train CBOW/Skip gram using Gensim on my data-set</p></li>
<li><p>Do Sentence level encoding via averaging all word vectors in that sentence </p></li>
<li><p>Take cosine similarity of <code>product_title</code> and <code>reviews</code>.</p></li>
</ol>

<p>Problem: It was not able to find the context from the sentence and hence the result was very poor.</p>

<p>Approch 2:</p>

<p>Used pre-trained BERT without pre-processing sentences. The result was not improving either.</p>

<p>1.Any other approach that would capture the context/semantics of sentences.</p>

<p>2.How can we train BERT on our data-set from scratch without using pre-trained model?</p>
",Vectorization & Embeddings,find sentence similarity using deep learning trying find sentence similarity word emebeddings applying cosine similarity score tried cbow skip gram method embedding solve problem product review data two column first two review semantic meaning completely different algorithm find semantic meaning sentence score approach text pre processing train cbow skip gram using gensim data set sentence level encoding via averaging word vector sentence take cosine similarity problem wa able find context sentence hence result wa poor approch used pre trained bert without pre processing sentence result wa improving either approach would capture context semantics sentence train bert data set scratch without using pre trained model
How to use masking with Convolution1D layer in keras?,"<p>I am trying to perform a sentiment classification task for which I am using Attention based architecture which has both Convolution layer and BiLSTM layers. The first layer of my model is a Embedding layer followed by a Convolution1D layer. I have used <code>mask_zero=True</code> for the Embedding layer since I have padded the sequence with zeros. This however creates an error for the Convolution1D layer since this layer does not support masking. However, I do need to mask the zero inputs since I have LSTM layers after the convolutional layers. Does anyone have any solution for this. I have attached a sample code of my model till the Convolution1D layer for reference.</p>
<pre><code>wordsInputs = Input(shape=(maxSeq,), name='words_input')
embed_reg = l2(l=0.001)
emb = Embedding(vocabSize, 300, mask_zero=True, init='glorot_uniform', W_regularizer=embed_reg)(wordsInputs)
convOutput = Convolution1D(nb_filter=100, filter_length=3, activation='relu', border_mode='same')(emb)
</code></pre>
",Vectorization & Embeddings,use masking convolution layer kera trying perform sentiment classification task using attention based architecture ha convolution layer bilstm layer first layer model embedding layer followed convolution layer used embedding layer since padded sequence zero however creates error convolution layer since layer doe support masking however need mask zero input since lstm layer convolutional layer doe anyone solution attached sample code model till convolution layer reference
Predicting cluster for new data point using k-means and Word2Vec,"<p>I have been working on a project of clustering reviews (using k-means) in positive and negative clusters.(Dataset is of amazon alexa reviews).
The vectorization part was done in two ways separately:</p>
<ol>
<li>Using <code>CountVectorizer()</code></li>
<li>Using <code>Word2Vec</code></li>
</ol>
<p>Now, I was able to make clusters with both of these vectorization techniques but when predicting the cluster for a new data point (review in my case), I did it with the following code for the <code>CountVectorizer()</code></p>
<p><strong>VECTORIZING THE DATA WITH <code>Countvectorizer()</code></strong></p>
<pre><code>vect = CountVectorizer(max_df = 0.95, min_df = 50)
count = vect.fit_transform(data_copy['reviews.text'])
count0 = count.todense()
count1 = count0.tolist()
count_df = pd.DataFrame(count1, columns = vect.get_feature_names())
</code></pre>
<p><strong>PREDICTING CLUSTER FOR NEW REVIEW</strong></p>
<pre><code>print('Prediction')
X = vect.transform(['worst'])
predicted = model.predict(X)
print(predicted)
</code></pre>
<p><strong>VECTORIZING THE DATA USING <code>Word2Vec</code></strong></p>
<pre><code>w2v_model = Word2Vec(min_count=3,
                     window=4,
                     size=300,
                     sample=1e-5, 
                     alpha=0.03, 
                     min_alpha=0.0007, 
                     negative=20,
                     workers=multiprocessing.cpu_count()-1)
w2v_model.build_vocab(sentences, progress_per=50000)
w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)

w2v_model.init_sims(replace=True)
word_vectors = Word2Vec.load(&quot;word2vec.model&quot;).wv
</code></pre>
<p>Now my issue is, how can I predict a new data point after applying <code>Word2Vec</code> just like I did after applying <code>CountVectorizer()</code>.</p>
<p>=&gt; How can I vectorize my new data point with the already built <code>Word2Vec</code> model? (Basically, what is the <code>Word2Vec</code> equivalent of <code>X = vect.transform(['worst'])</code>).</p>
<p>Thanks in advance!</p>
",Vectorization & Embeddings,predicting cluster new data point using k mean word vec working project clustering review using k mean positive negative cluster dataset amazon alexa review vectorization part wa done two way separately using using wa able make cluster vectorization technique predicting cluster new data point review case following code vectorizing data predicting cluster new review vectorizing data using issue predict new data point applying like applying vectorize new data point already built model basically equivalent thanks advance
what does workers means in gensim Word2Vec?,"<p>I'm trying to create a Word2Vec model using gensim but I don't understand what Workers mean.
This is an examples from radimrehurek.com with <code>workers = 4</code> but there is no explanation for that.</p>
<p><code>Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)</code></p>
<p>I would be very thankful of anyone can help me.Thxx</p>
",Vectorization & Embeddings,doe worker mean gensim word vec trying create word vec model using gensim understand worker mean example radimrehurek com explanation would thankful anyone help thxx
"Getting an &quot;Expected a 2D array, got a 1D array&quot; error when computing LSA","<p>I am writing a pre-processing function in natural language processing for LSA (Latent Semantic Analysis). All the other functions such as tfidf, remove_stopwords work with the unit tests that I created. However the LSA function keeps giving me the following error when testing its functionality:</p>
<p>&quot;Expected 2D array, got 1D array instead:
array=['I ate dinner at Olive Garden', 'we are buying a house',
'I did not eat dinner at Olive Garden', 'our neighbors are buying a house'].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.&quot;</p>
<p>Here is my code for the LSA function and the test code:</p>
<pre><code>import pandas as pd
import nltk
import string
import sklearn
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import Normalizer
from sklearn.feature_extraction.text import TfidfVectorizer

def LSA(data, tfidf = True, remove_stopwords=True):
    # done with stop word removal and tf-idf weighting keeping the 100 most common concepts
    text = data.iloc[:,-1] #isolate text column
    
     
    #Define the LSA function
    vectors = sklearn.decomposition.TruncatedSVD(n_components = 2, algorithm = 'randomized', n_iter = 100, random_state = 100)

    vectors.fit(text.tolist())
    svd_matrix = vectors.fit_transform(text.tolist())
    svd_matrix = Normalizer(copy=False).fit_transform(text.tolist())

    dense = svd_matrix.todense()
    denselist = dense.tolist()
    
    data[&quot;cleaned_vectorized_document&quot;] = denselist
    return data
</code></pre>
<p>Here is the test code that I am using that throws the error:</p>
<pre><code>p = pd.DataFrame({'two':[1,2,3,4],'test':['I ate dinner at Olive Garden', 'we are buying a house',
'I did not eat dinner at Olive Garden', 'our neighbors are buying a house']})

print(LSA(p))
</code></pre>
",Vectorization & Embeddings,getting expected array got array error computing lsa writing pre processing function natural language processing lsa latent semantic analysis function tfidf remove stopwords work unit test created however lsa function keep giving following error testing functionality expected array got array instead array ate dinner olive garden buying house eat dinner olive garden neighbor buying house reshape data either using array reshape data ha single feature array reshape contains single sample code lsa function test code test code using throw error
Ignore padding class (0) during multi class classification,"<p>I have a problem where given a set of tokens, predict another token. For this task I use an embedding layer with <code>Vocab-size + 1</code> as <code>input_size</code>. The <code>+1</code> is because the sequences are padded with zeros. Eg. given a <code>Vocab-size</code> of 10 000 and <code>max_sequence_len=6</code>, <code>x_train</code> looks like:</p>
<pre><code>array([[    0,     0,     0,    11,    22,     4],
       [    29,     6,     12,    29,  1576,    29],
       ...,
       [    0,     0,     67,    8947,  7274,  7019],
       [    0,     0,     0,    15,  10000,    50]])
</code></pre>
<p><code>y_train</code> consists of integers between 1 and 10000, with other words, this becomes a multi-class classification problem with 10000 classes.</p>
<p><strong>My problem</strong>: When I specify the output size in the output layer, I would like to specify 10000, but the model will predict the classes 0-9999 if I do this. Another approach is to set output size to 10001, but then the model can predict the 0-class (padding), which is unwanted.</p>
<p>Since <code>y_train</code> is mapped from 1 to 10000, I could remap it to 0-9999, but since they share mapping with the input, this seems like an unnecessary workaround.</p>
<p>EDIT:<br />
I realize, and which @Andrey pointed out in the comments, that I could allow for 10001 classes, and simply add padding to the vocabulary, although I am never interested in the network predicting <code>0</code>'s.</p>
<p>How can I tell the model to predict on the labels 1-10000, whilst at the meantime have 10000 classes, not 10001?</p>
",Vectorization & Embeddings,ignore padding class multi class classification problem given set token predict another token task use embedding layer sequence padded zero eg given look like consists integer word becomes multi class classification problem class problem specify output size output layer would like specify model predict class another approach set output size model predict class padding unwanted since mapped could remap since share mapping input seems like unnecessary workaround edit realize andrey pointed comment could allow class simply add padding vocabulary although never interested network predicting tell model predict label whilst meantime class
How to backpropagate through embedding layer?,"<p>I want to implement a Word2Vec using negative sampling with pure TensorFlow 2. The job is fairly simple, I have two embeddings <code>target_amb</code> and <code>context_amb</code>, I take some values out of them, dot product them, and by comparing to <code>y</code>(which is <code>(1, 0, 0)</code> here) I compute the loss.</p>
<p>The problem is in the backpropagation step. I don't know how to backpropagate through the vectors I've dot produced before. It is because the vectors (<code>middle</code> and <code>neighbor_choices</code>) are <strong>Tensors</strong> but they should be <strong>tf.Vatriable</strong>. I don't think It is good to convert them to tf.Variable, because then in learning I'll have thousands of tf.Variable and I'll go out of memory.</p>
<blockquote>
<p>What shall I do to backpropagate through the embedding layer?</p>
</blockquote>
<p>The code is simple but when I run it I'll have an error due to <code>middle</code> and <code>neighbor_choices</code> being Tensors.</p>
<pre><code>import tensorflow as tf
import numpy as np

x, y = (('self', 'the'), ('self', 'violent'), ('self', 'any')), (1, 0, 0)
y = tf.convert_to_tensor(y, dtype='float32')

target_amb = tf.keras.layers.Embedding(len(words_lst), embeding_size)
context_amb = tf.keras.layers.Embedding(len(words_lst), embeding_size)

optimizer = tf.keras.optimizers.Adam()
with tf.GradientTape() as t:    
      middle = target_amb(word2index[x[0][0]])
      neighbor_choices = context_amb(np.asarray([[word2index[i[1]] for i in x]]))

      scores = tf.tensordot(neighbor_choices, middle, 1)
      prediction = tf.nn.sigmoid(scores)
      print(prediction)
      loss = tf.keras.losses.categorical_crossentropy(tf.expand_dims(y, axis=0), prediction)

      g_embed, g_context = t.gradient(loss, [middle, neighbor_choices])

      optimizer.apply_gradients(zip([g_embed, g_context], [middle, neighbor_choices]))
</code></pre>
<hr />
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-17-1559a622fe62&gt; in &lt;module&gt;()
     20       g_embed, g_context = t.gradient(loss, [middle, neighbor_choices])
     21 
---&gt; 22       optimizer.apply_gradients(zip([g_embed, g_context], [middle, neighbor_choices]))

4 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _var_key(var)
   1295   if hasattr(var, &quot;_distributed_container&quot;):
   1296     var = var._distributed_container
-&gt; 1297   if var._in_graph_mode:
   1298     return var._shared_name
   1299   return var._unique_id

AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_in_graph_mode'
</code></pre>
",Vectorization & Embeddings,backpropagate embedding layer want implement word vec using negative sampling pure tensorflow job fairly simple two embeddings take value dot product comparing compute loss problem backpropagation step know backpropagate vector dot produced vector tensor tf vatriable think good convert tf variable learning thousand tf variable go memory shall backpropagate embedding layer code simple run error due tensor
Predicting the next word with Keras: how to retrieve prediction for each input word,"<p>I am having some problems understanding how to retrieve the predictions from a Keras model.</p>

<p>I want to build a simple system that predicts the next word, but I don't know how to output the complete list of probabilities for each word.</p>

<p>This is my code right now:</p>

<pre><code>model = Sequential()
model.add(Embedding(vocab_size, embedding_size, input_length=55, weights=[pretrained_weights])) 
model.add(Bidirectional(LSTM(units=embedding_size)))
model.add(Dense(23690, activation='softmax')) # 23690 is the total number of classes 

model.compile(loss='categorical_crossentropy',
          optimizer = RMSprop(lr=0.0005),
          metrics=['accuracy'])

# fit network
model.fit(np.array(X_train), np.array(y_train), epochs=10)
score = model.evaluate(x=np.array(X_test), y=np.array(y_test), batch_size=32)
prediction = model.predict(np.array(X_test), batch_size=32)
</code></pre>

<p>First question:
Training set: list of sentences (vectorized and transformed to indices).
I saw some examples online where people divide X_train and y_train like this:</p>

<pre><code>X, y = sequences[:,:-1], sequences[:,-1]
y = to_categorical(y, num_classes=vocab_size)
</code></pre>

<p>Should I instead transform the X_train and the y_train in order to have sliding sequences, where for example I have </p>

<pre><code>X = [[10, 9, 4, 5]]
X_train = [[10, 9], [9, 4], [4, 5]]
y_train = [[9], [4], [5]]
</code></pre>

<p>Second question:
Right now the model returns only one element for each input. How can I return the predictions for each word? I want to be able to have an array of output words for each word, not a single output.
I read that I could use a TimeDistributed layer, but I have problems with the input, because the Embedding layer takes a 2D input, while the TimeDistributed takes a 3D input.</p>

<p>Thank you for the help!</p>
",Vectorization & Embeddings,predicting next word kera retrieve prediction input word problem understanding retrieve prediction kera model want build simple system predicts next word know output complete list probability word code right first question training set list sentence vectorized transformed index saw example online people divide x train train like instead transform x train train order sliding sequence example second question right model return one element input return prediction word want able array output word word single output read could use timedistributed layer problem input embedding layer take input timedistributed take input thank help
finding semantic similarity between 2 statements,"<p>I am currently working with small application in <strong>python</strong> and my application has search functionality (currently using <strong>difflib</strong>) but I want to create <strong>Semantic Search</strong> which can give top 5 or 10 results from my database, based on user inputted text. It is same as google search engine works. I found some solutions <a href=""https://stackoverflow.com/questions/2037832/semantic-similarity-between-sentences"">Here</a>. </p>

<p>But the problem is, below two statements from one of solution are semantically incorrect. And I don't care about this. because they are making things too hard which I don't want And also solution will be some pretrained neural network model or library from which I can implement easily.</p>

<ul>
<li>Pete and Rob have found a dog near the station.</li>
<li>Pete and Rob have never found a dog near the station </li>
</ul>

<p>And also I found some solutions which are showing using <code>gensim</code> and <code>Glove</code> embeddings and finding similarity between words and not sentences. </p>

<h1>What I wanted ?</h1>

<p>Suppose my db has statement <code>display classes</code> and user inputs <code>show</code>, <code>showed</code>, <code>displayed</code>, <code>displayed class</code>, <code>show types</code> etc are same. And if above 2 statements are given as same then also I don't care. <code>displayed</code> and <code>displayed class</code> already showing in difflib.</p>

<h1>Points to be noted</h1>

<ul>
<li>Find from fixed set of statements but user inputted statements can differ</li>
<li>Must work for statements</li>
</ul>
",Vectorization & Embeddings,finding semantic similarity statement currently working small application python application ha search functionality currently using difflib want create semantic search give top result database based user inputted text google search engine work found solution href p problem two statement one solution semantically incorrect care making thing hard want also solution pretrained neural network model library implement easily pete rob found dog near station pete rob never found dog near station also found solution showing using embeddings finding similarity word sentence wanted suppose db ha statement user input etc statement given also care already showing difflib point noted find fixed set statement user inputted statement differ must work statement
Multilabel Classification using random forest,"<p>I have 5 labels and 499 datasets.</p>
<p>I tried using random forest classier. I have two inputs for the model: output of tfidf and review_length.</p>
<pre><code>train_tfIdf = vectorizer_tfidf.fit_transform(X_train.values.astype('U'))
x_train=['train_tfIdf',question['review_len']]
</code></pre>
<p>this is the error that is displayed when I execute:</p>
<pre><code>classifier.fit(x_train, y_train)
ValueError: could not convert string to float: 'train_tfIdf'
</code></pre>
",Vectorization & Embeddings,multilabel classification using random forest label datasets tried using random forest classier two input model output tfidf review length error displayed execute
Why parallel processing taking longer than usual code?,"<p>The idea is to update a particular pre-trained word2vec model with different sets of new corpus. I have the following</p>
<pre><code># c1, c2 are each a list of 100 files
filelist = [c1, c2, c3, c4, c5, c6, c7, c8, c9, c10]

def update_model(files):
    # loading a pre-trained model
    trained_model = gensim.models.Word2Vec.load(&quot;model_both_100&quot;)
    # Document feeder is an iterable
    docs = DocumentFeeder(files)
    trained_model.build_vocab(docs, update=True)
    trained_model.train(docs, total_examples=trained_model.corpus_count, epochs=trained_model.epochs)

with Pool(processes=10) as P:
    P.map(update_model, filelist)
</code></pre>
<p>it takes about ~13 minutes to run. But the non-parallel version (looping over <code>filelist</code>) takes ~11 min. Why is this happening? Running on a 12 core cpu.</p>
",Vectorization & Embeddings,parallel processing taking longer usual code idea update particular pre trained word vec model different set new corpus following take minute run non parallel version looping take min happening running core cpu
BERT sentence embeddings using pretrained models for Non-English text,"<p>I am trying to apply <code>BERT</code> sentence embeddings to find similar sentences given a text piece in Swedish from a corpus of text strings in Swedish.
Sentence <code>BERT</code> from sentence_transformers (<code>SBERT</code>) seems to be the ideal choice. They have various pretrained models and give excellent examples:
(<a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a>)</p>
<pre><code>from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')

sentences = ['This framework generates embeddings for each input sentence',
    'Sentences are passed as a list of string.', 
    'The quick brown fox jumps over the lazy dog.']
sentence_embeddings = model.encode(sentences)
</code></pre>
<p>Their choice for non-English Sentence Transformers, however, seems to be limited. I was wondering whether it's possible and perhaps more accurate to apply a BERT model from the Hugging Face library specifically trained on Swedish text or any other Non-English language for that matter on extracting sentence embeddings? Wouldn't a BERT model specifically trained on a language yield more accurate sentence embeddings for that language? In the end all models are pretrained on the same tasks MLM and NSP in their respective language to understand that language, right?</p>
<p>Any thoughts or experiences?</p>
<p>The ultimate goal is to use cosine similarity on the sentence embeddings to rank similar sentences/paragraphs</p>
",Vectorization & Embeddings,bert sentence embeddings using pretrained model non english text trying apply sentence embeddings find similar sentence given text piece swedish corpus text string swedish sentence sentence transformer seems ideal choice various pretrained model give excellent example choice non english sentence transformer however seems limited wa wondering whether possible perhaps accurate apply bert model hugging face library specifically trained swedish text non english language matter extracting sentence embeddings bert model specifically trained language yield accurate sentence embeddings language end model pretrained task mlm nsp respective language understand language right thought experience ultimate goal use cosine similarity sentence embeddings rank similar sentence paragraph
Extract most important keywords from a set of documents,"<p>I have a set of 3000 text documents and I want to extract top 300 keywords (could be single word or multiple words).</p>

<p>I have tried the below approaches - </p>

<p><a href=""https://www.airpair.com/nlp/keyword-extraction-tutorial"" rel=""nofollow noreferrer"">RAKE</a>: It is a Python based keyword extraction library and it failed miserably.</p>

<p><a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">Tf-Idf</a>: It has given me good keywords per document, but it is not able to aggregate them and find keywords that represent the whole group of documents.
Also, just selecting top k words from each document based on Tf-Idf score won't help, right?</p>

<p><a href=""https://deeplearning4j.org/word2vec"" rel=""nofollow noreferrer"">Word2vec</a>: I was able to do some cool stuff like find similar words but not sure how to find important keywords using it.</p>

<p>Can you please suggest some good approach (or elaborate on how to improve any of the above 3) to solve this problem? Thanks :)</p>
",Vectorization & Embeddings,extract important keywords set document set text document want extract top keywords could single word multiple word tried approach rake python based keyword extraction library failed miserably tf idf ha given good keywords per document able aggregate find keywords represent whole group document also selecting top k word document based tf idf score help right word vec wa able cool stuff like find similar word sure find important keywords using please suggest good approach elaborate improve solve problem thanks
"Why keras Tokenizer with unknown token requiring embedding&#39;s input_dim to be vocab_size +2, instead of vocal_size+1","<p>I am working with keras embedding and using Keras tokenizer. At first, I wasnt using oov_token (for unknown token) and I was having length of my tokenizer's <code>word_counts</code> as <code>54</code>.</p>
<p>For embedding I used to give <code>my_tokenizer.word_counts+1</code> as <code>input_dim</code>, later I had a need to tackle unknown tokens, so I change my code to following</p>
<pre><code>my_tokenizer = Tokenizer(oov_token=&quot;&lt;UKN&gt;&quot;) # First it was Tokenizer()    
my_tokenizer.fit_on_texts(my_tokens)
my_sequences = module_tokenizer.texts_to_sequences(my_tokens)
</code></pre>
<p>But after adding the unknown token (which got index 1 as <code>{'&lt;UNK':1,..}</code> ) with <code>my_tokenizer.word_counts+1</code> as <code>input_dim</code>, I got error for index like</p>
<pre><code>55 not in indices [0,55]
</code></pre>
<p>My word_count(<code>my_tokenizer.word_counts+1</code>) is <code>55</code> (one extra than last(54) approach without unkonwn token).</p>
<p>Now if I add <code>2</code> in my vocabulary size (<code>my_tokenizer.word_counts</code>) it works fine</p>
<pre><code>layers.Embedding( my_tokenizer.word_counts+2 , ... )
</code></pre>
<p>but I dont understand why I need to add 2 (make it <code>56</code>)</p>
<p>I would be very thankful for the help</p>
",Vectorization & Embeddings,kera tokenizer unknown token requiring embedding input dim vocab size instead vocal size working kera embedding using kera tokenizer first wasnt using oov token unknown token wa length tokenizer embedding used give later need tackle unknown token change code following adding unknown token got index got error index like word count one extra last approach without unkonwn token add vocabulary size work fine dont understand need add make would thankful help
Inefficient tokenization leading to better results,"<p>I am following the code from <a href=""https://medium.com/better-programming/introduction-to-gensim-calculating-text-similarity-9e8b55de342d"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I have a csv file of 8000 questions and answers and I have made an LSI model with 1000 topics, from a tfidf corpus, using gensim as follows. I only consider questions as part of the text not the answers.</p>
<pre><code>texts = [jieba.lcut(text) for text in document]
# tk = WhitespaceTokenizer() 
# texts = [tk.tokenize(text) for text in document]
dictionary = corpora.Dictionary(texts)
feature_cnt = len(dictionary.token2id)
corpus = [dictionary.doc2bow(text) for text in texts]
tfidf = models.TfidfModel(corpus)
tfidf_corpus = tfidf[corpus]
lsi_model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=1000)
corpus_lsi = lsi_model[tfidf_corpus]
index = similarities.SparseMatrixSimilarity(corpus_lsi, num_features = feature_cnt)
</code></pre>
<p>Before this I am also preprocessing the data by removing stopwords using nltk and replacing punctuations using regex and lemmatizing, using wordnet and nltk.</p>
<p>I understand that jieba is not a tokenizer suited for english because it tokenizes spaces as well like this:</p>
<pre><code>Sample: This is untokenized text
Tokenized: 'This',' ','is',' ','untokenized', ' ', 'text'
</code></pre>
<p>When I switch from jieba to nltk whitespace tokenizer, strange thing happens, my accuracy suddenly drops that is when I a new sentence using the following code I get worse results</p>
<pre><code>keyword = &quot;New sentence the similarity of which is to be found to the main corpus&quot;
kw_vector = dictionary.doc2bow(jieba.lcut(keyword)) # jieba.lcut can be replaced by tk.tokenize()
sim = index[lsi_model[tfidf[kw_vector]]]                         
x = [sim[i] for i in np.argsort(sim)[-2:]]
</code></pre>
<p>My understanding is that extra and useless words and characters like whitespaces should decrease accuracy but here I observe an opposite effect. What could be the possible reasons?</p>
<p>One possible explanation I came up with is that most of the questions are short only 5 to 6 words like</p>
<ol>
<li>What is the office address?</li>
<li>Who to contact for X?</li>
<li>Where to find document Y?</li>
</ol>
",Vectorization & Embeddings,inefficient tokenization leading better result following code csv file question answer made lsi model topic tfidf corpus using gensim follows consider question part text answer also preprocessing data removing stopwords using nltk replacing punctuation using regex lemmatizing using wordnet nltk understand jieba tokenizer suited english tokenizes space well like switch jieba nltk whitespace tokenizer strange thing happens accuracy suddenly drop new sentence using following code get worse result understanding extra useless word character like whitespaces decrease accuracy observe opposite effect could possible reason one possible explanation came question short word like office address contact x find document
Sentence Embedding Clustering,"<p>I am working on a small project in which I need to eliminate irrelevant information (ads for instance) from the html content I extracted from the websites. Since I am a beginner in NLP, I came up with a simple approach after doing some research.</p>
<p>The language used in the websites is mainly Chinese and I stored each sentence (separated by comma) into a list. I used a model called HanLP to do semantic parsing on my sentences. Something like this:</p>
<pre><code>[['萨哈夫', '说', '，', '伊拉克', '将', '同', '联合国', '销毁', '伊拉克', '大', '规模', '杀伤性', '武器', '特别', '委员会', '继续', '保持', '合作', '。'], 
 ['上海', '华安', '工业', '（', '集团', '）', '公司', '董事长', '谭旭光', '和', '秘书', '张晚霞', '来到', '美国', '纽约', '现代', '艺术', '博物馆', '参观', '。']]
</code></pre>
<p>I found a pretrained Chinese word embedding database to get the word embeddings in my list. Then my approach is to get the sentence embedding by calculating the element-wise average in that sentence. Now I got a list with sentence embedding vector of each individual sentence I parsed.</p>
<pre><code>sentence: ['各国', '必须', '“', '大', '规模', '”', '支出', '》', '的', '报道', '称']
sentence embedding: [0.08130878633396192, -0.07660450288941237, 0.008989107615145093, 0.07014013996178453, 0.028158639980988068, 0.01821030060422014, 0.017793822186914356, 0.04148909364911643, 0.019383941353722053, 0.03080177273262631, -0.025636445207055658, -0.019274188523096116, 0.0007501963356679136, 0.00476544528183612, -0.024648051539605313, -0.011124626140702854, -0.0009071269834583455, -0.08850407109341839, 0.016131568784740837, -0.025241035714068195, -0.041586867829954084, -0.0068722023954085835, -0.010853541125966744, 0.03994347004812549, 0.04977656596086242, 0.029051605612039566, -0.031031965550606732, 0.05125975541093133, 0.02666312647687102, 0.0376262941096105, -0.00833959155716002, 0.035523645325817844, -0.0026961421932686458, 0.04742895790629766, -0.07069634984840047, -0.054931600324132225, 0.0727336619218642, 0.0434290729039772, -0.09277284060689536, -0.020194332538680596, 0.0011523241092535582, 0.035080605863847515, 0.13034072890877724, 0.06350403482263739, -0.04108352984555743, 0.03208382343026725, -0.08344872626052662, -0.14081071757457472, -0.010535095733675089, -0.04253014939075166, -0.06409504175694151, 0.04499104322696274, -0.1153958263722333, 0.011868207969448784, 0.032386500388383865, -0.0036963022192305125, 0.01861521213802255, 0.05440248447385701, 0.026148285970769146, 0.011136160687204789, 0.04259885661303997, 0.09219381585717201, 0.06065366725141013, -0.015763109010136264, -0.0030524068596688185, 0.0031816939061338253, -0.01272551697382534, 0.02884035756472837, -0.002176688645373691, -0.04119681418788704, -0.08371328799562021, 0.007803680078888481, 0.0917377421124415, 0.027042210250246255, -0.0168504383076321, -0.0005781924013387073, 0.0075592477594248276, 0.07226487367667934, 0.005541681396690282, 0.001809495755217292, 0.011297995647923513, 0.10331092673269185, 0.0034428672357039018, 0.07364177612841806, 0.03861967177892273, -0.051503680434755304, -0.025596174390309236, 0.014137779785828157, -0.08445698734034192, -0.07401955000717532, 0.05168289600194178, -0.019313615386966954, 0.007136409255591306, -0.042960755484686655, 0.01830706542188471, -0.001172357662157579, -0.008949846103364094, -0.02356141348454085, -0.05277112944432619, 0.006653293967247009, -0.00572453092106364, 0.049479073389771984, -0.03399876727913083, 0.029434629207984966, -0.06990156170319427, 0.0924786920659244, 0.015472117049450224, -0.10265431468459693, -0.023421658562834968, 0.004523425542918796, -0.008990391665561632, -0.06445665437389504, 0.03898039324717088, -0.025552247142927212, 0.03958867977119305, -0.03243451675569469, -0.03848901360338046, -0.061713250523263756, -0.00904815017499707, -0.03730008362750099, 0.02715366007760167, -0.08498009599067947, -0.00397337388924577, -0.0003402943098494275, 0.008005982349542055, 0.05871503853069788, -0.013795949010686441, 0.007956360128115524, -0.024331797295334665, 0.03842244771393863, -0.04393653944134712, 0.02677931230176579, 0.07715398648923094, -0.048624055216681554, -0.11324723844882101, -0.08751555024222894, -0.02469049582511864, -0.08767948790707371, -0.021930147846102376, 0.011519658294591036, -0.08155732788145542, -0.10763703049583868, -0.07967398501932621, -0.03249315629628571, 0.02701333300633864, -0.015305672687563028, 0.002375963249836456, 0.012275356545367024, -0.02917095824060115, 0.02626959386874329, -0.0158629031767222, -0.05546591058373451, -0.023678493686020374, -0.048296650278974666, -0.06167154920033433, 0.004435380412773652, 0.07418209609617903, 0.03524015434297987, 0.063185997529548, -0.05814945189790292, 0.13036084697920491, -0.03370768073099581, 0.03256692289671099, 0.06808869439092549, 0.0563600350340659, 5.7854774323376745e-05, -0.0793171048333699, 0.03862177783792669, 0.007196083004766313, 0.013824320821599527, 0.02798982642707415, -0.00918149473992261, -0.00839392692697319, 0.040496235374699936, -0.007375971498814496, -0.03586547057652338, -0.03411220566538924, -0.025101724758066914, -0.005714270286262035, 0.07351569867354225, -0.024216756182299418, 0.0066968070935796604, -0.032809603959321976, 0.05006068360737779, 0.0504626590250568, 0.04525104385208, -0.027629732069644062, 0.10429493219337681, -0.021474285961382768, 0.018212029964409092, 0.07260083373297345, 0.026920156976716084, 0.043199389770796355, -0.03641596379351209, 0.0661080302670598, 0.09141866947439584, 0.0157452768815512, -0.04552285996297459, -0.03509725736115466, 0.02857604629190808]
</code></pre>
<p>My next step is to cluster these sentence embedding vectors and find out sentences that clearly have irrelevant content compared to the others.</p>
<p>Does my approach even make sense? If it does, what tools can I use to cluster my sentence embedding values? I saw there are approaches such as K-means or calculate L2 distances but I am not sure how to implement.</p>
<p>Thanks!</p>
",Vectorization & Embeddings,sentence embedding clustering working small project need eliminate irrelevant information ad instance html content extracted website since beginner nlp came simple approach research language used website mainly chinese stored sentence separated comma list used model called hanlp semantic parsing sentence something like found pretrained chinese word embedding database get word embeddings list approach get sentence embedding calculating element wise average sentence got list sentence embedding vector individual sentence parsed next step cluster sentence embedding vector find sentence clearly irrelevant content compared others doe approach even make sense doe tool use cluster sentence embedding value saw approach k mean calculate l distance sure implement thanks
Sentence iterator to pass to Gensim language model,"<p>I am relatively new to NLP and I am trying to create my own words embeddings trained in my personal corpus of docs.</p>

<p>I am trying to implement the following code to create my own wordembedings:</p>

<pre><code>model = gensim.models.Word2Vec(sentences)
</code></pre>

<p>with sentences being a list of sentences.
Since I can not pass thousands and thousands of sentences I need an iterator</p>

<pre><code># with mini batch_dir a directory with the text files
# MySentences is a class iterating over sentences.
sentences = MySentences(minibatch_dir) # a memory-friendly iterator
</code></pre>

<p>I found this solution by the creator of gensim:</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()
</code></pre>

<p>It does not work for me.
How can I create an iterator if I know how to get the list of sentences from every document?</p>

<p>And second very related question:
If I am aiming to compare documents similarity in a particular corpus, is always better to create from scratch word embeddings with all the documents of that particular corpus than using GloVec or word2vec? 
The amount of docs is around 40000.</p>

<p>cheers</p>

<p>More pre</p>
",Vectorization & Embeddings,sentence iterator pas gensim language model relatively new nlp trying create word embeddings trained personal corpus doc trying implement following code create wordembedings sentence list sentence since pas thousand thousand sentence need iterator found solution creator gensim doe work create iterator know get list sentence every document second related question aiming compare document similarity particular corpus always better create scratch word embeddings document particular corpus using glovec word vec amount doc around cheer pre
"What are the best ways to join two vectors without changing dimensions? sum, mean, median..etc","<p>I am looking for a way to represent 2 same dimension vectors as 1 like summation of corresponding elements of vector. </p>

<p><strong>EDIT</strong></p>

<p>I have 5 vectors from 5 word embedding models(string2vec, word2vec, doc2vec, topic2vec, and glove2vec) and now I want to combine them to form one representational vector to be fed into ml classification models. I have tried addition, multiplication, mean, median and distance formula. I am looking for techniques to join vectors other than the ones i mentioned</p>
",Vectorization & Embeddings,best way join two vector without changing dimension sum mean median etc looking way represent dimension vector like summation corresponding element vector edit vector word embedding model string vec word vec doc vec topic vec glove vec want combine form one representational vector fed ml classification model tried addition multiplication mean median distance formula looking technique join vector one mentioned
How to compare two large text files in Python?,"<p><strong>Datasets</strong>: I have two different text datasets(large text files for train and test that each one includes 30,000 sentences). a part of data is like the following:
""
the fulton county grand jury said friday an investigation of atlanta's recent primary election produced `` no evidence '' that any irregularities took place .
""</p>

<p><strong>Question</strong>: How can I replace every word in the test data not seen in training with the word ""unk"" in Python?</p>

<p><strong>My solution</strong>: Should I use the ""nested for-loops"" to compare all words of the train data with all words of the test data and also the ""if-statement"" to say if any word in test data is not in train data then replace with ""unk"" ?</p>

<pre><code>#open text file and assign it to varaible with the name ""readfile""
readfile1= open('train.txt','r')
#create the new empty text file with the new name and then assign it to variable 
# with the name ""writefile"". now this file is ready for writing in that
writefile=open('test.txt','w')
for word1 in readfile1:
    for word2 in readfile2:
        if (word1!=word2):
            word2='unk'
writefile.close()
</code></pre>
",Vectorization & Embeddings,compare two large text file python datasets two different text datasets large text file train test one includes sentence part data like following fulton county grand jury said friday investigation atlanta recent primary election produced evidence irregularity took place question replace every word test data seen training word unk python solution use nested loop compare word train data word test data also statement say word test data train data replace unk
Encoding a lot of categorical variables,"<p>I have 10 million categorical variables (each variable has 3 categories). What is the best way to encode these 10 million variables to train a deep learning model on them? (If I use one hot encoding, then I will end up having 30 million variables. Also, embedding layer with one output makes no sense (it is similar to integer encoding and there is no order between these categories) and embedding layer with two outputs does not make that much difference. Usually, we use embedding layer when number of categories is a lot). Please give me your opinion.</p>
",Vectorization & Embeddings,encoding lot categorical variable million categorical variable variable ha category best way encode million variable train deep learning model use one hot encoding end million variable also embedding layer one output make sense similar integer encoding order category embedding layer two output doe make much difference usually use embedding layer number category lot please give opinion
why we use input-hidden weight matrix to be the word vectors instead of hidden-output weight matrix?,"<p>In word2vec, after training, we get two weight matrixes:1.input-hidden weight matrix; 2.hidden-output weight matrix. and people will use the input-hidden weight matrix as the word vectors(each row corresponds to a word, namely, the word vectors).Here comes to my confusions:</p>

<ol>
<li>why people use input-hidden weight matrix as the word vectors instead of the hidden-output weight matrix.</li>
<li>why don't we just add softmax activation function to the hidden layers rather than output layers, thus preventing time-consuming.</li>
</ol>

<p>Plus, clarifying remarks on the intuition of how word vectors can be obtained like this will be appreciated.</p>
",Vectorization & Embeddings,use input hidden weight matrix word vector instead hidden output weight matrix word vec training get two weight matrix input hidden weight matrix hidden output weight matrix people use input hidden weight matrix word vector row corresponds word namely word vector come confusion people use input hidden weight matrix word vector instead hidden output weight matrix add softmax activation function hidden layer rather output layer thus preventing time consuming plus clarifying remark intuition word vector obtained like appreciated
Doc2vecC predicting vectors for unseen documents,"<p>I have trained a set of documents using Doc2vecc.</p>

<pre><code>https://github.com/mchen24/iclr2017
</code></pre>

<p>I am trying to generate the embedding vector for the unseen documents.I have trained the documents as mentioned in the go.sh.</p>

<pre><code>""""""
time ./doc2vecc -train ./aclImdb/alldata-shuf.txt -word 
wordvectors.txt -output docvectors.txt -cbow 1 -size 100 -window 10 - 
negative 5 -hs 0 -sample 0 -threads 4 -binary 0 -iter 20 -min-count 10 
-test ./aclImdb/alldata.txt -sentence-sample 0.1 -save-vocab 
alldata.vocab
""""""
</code></pre>

<p>I get the docvectors.txt  and wordvectors.txt for the train set. Now from here how do I generate vectors for unseen test using the same model without retraining.</p>
",Vectorization & Embeddings,doc vecc predicting vector unseen document trained set document using doc vecc trying generate embedding vector unseen document trained document mentioned go sh get docvectors txt wordvectors txt train set generate vector unseen test using model without retraining
Why 2 almost equal Keras CNN returns 2 quite different results,"<p>I'm addressing a <strong>sentence-level binary classification task</strong>. My data consists of 3 subarrays of tokens: left context, core, and right context.</p>

<p>I used <strong>Keras</strong> to devise several alternatives of <strong>Convolutional Neural Networks</strong> and validate which one best fit my problem.</p>

<p>I'm a newbie in Python and Keras and I decided to start with simpler solutions in order to test which changes improve my metrics (accuracy, precision, recall, f1 and auc-roc). The first simplification was regarding input data: I decided to ignore contexts to create a Sequential model of Keras:</p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 500)               0         
_________________________________________________________________
masking_1 (Masking)          (None, 500)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 500, 100)          64025600  
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 497, 128)          51328     
_________________________________________________________________
average_pooling1d_1 (Average (None, 62, 128)           0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 62, 128)           0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 61, 256)           65792     
_________________________________________________________________
dropout_2 (Dropout)          (None, 61, 256)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 54, 32)            65568     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 32)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                528       
_________________________________________________________________
dropout_3 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 2)                 34        
=================================================================
</code></pre>

<p>As you can see, I use a fixed size of inputs so I applied a padding preprocessing. I also used an embedding layer with a Word2Vec model.</p>

<p>This model returns the following results:</p>

<pre><code>P       0.875457875
R       0.878676471
F1      0.87706422
AUC-ROC 0.906102654
</code></pre>

<p>I wished to implement how to select a subarray of input data inside my CNN by means of Lambda layers. I use the following definition of my Lambda layer:</p>

<pre><code>Lambda(lambda x: x[:, 1], output_shape=(500,))(input)
</code></pre>

<p>And this is the summary of my new CNN (as you can see it's almost the same than the prior):</p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 3, 500)            0         
_________________________________________________________________
lambda_1 (Lambda)            (None, 500)               0         
_________________________________________________________________
masking_1 (Masking)          (None, 500)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 500, 100)          64025600  
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 497, 128)          51328     
_________________________________________________________________
average_pooling1d_1 (Average (None, 62, 128)           0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 62, 128)           0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 61, 256)           65792     
_________________________________________________________________
dropout_2 (Dropout)          (None, 61, 256)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 54, 32)            65568     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 32)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                528       
_________________________________________________________________
dropout_3 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 2)                 34        
=================================================================
</code></pre>

<p>But the results were disgusting because accuracy stops at <strong>60%</strong> and obviously, precision, recall and f1 were too low (&lt; 0.10) regarding the first model results.</p>

<p>I don't know what's happening and I don't know if these networks are more different that I thought.</p>

<p>Any clue regarding this issue?</p>
",Vectorization & Embeddings,almost equal kera cnn return quite different result addressing sentence level binary classification task data consists subarrays token left context core right context used kera devise several alternative convolutional neural network validate one best fit problem newbie python kera decided start simpler solution order test change improve metric accuracy precision recall f auc roc first simplification wa regarding input data decided ignore context create sequential model kera see use fixed size input applied padding preprocessing also used embedding layer word vec model model return following result wished implement select subarray input data inside cnn mean lambda layer use following definition lambda layer summary new cnn see almost prior result disgusting accuracy stop obviously precision recall f low regarding first model result know happening know network different thought clue regarding issue
Tensorflow Embedding for training and inference,"<p>I am trying to code a simple Neural machine translation using tensorflow. But I am a little stuck regarding the understanding of the embedding on tensorflow :</p>

<ul>
<li>I do not understand the difference between <code>tf.contrib.layers.embed_sequence(inputs, vocab_size=target_vocab_size,embed_dim=decoding_embedding_size)</code></li>
</ul>

<p>and</p>

<pre><code> dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))
 dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)
</code></pre>

<p>In which case should I use one to another ?</p>

<ul>
<li>The second thing I do not understand is about tf.contrib.seq2seq.TrainingHelper and tf.contrib.seq2seq.GreedyEmbeddingHelper. I know that in the case of translation, we use mainly TrainingHelper for the training step (use the previous target to predict the next target) and GreedyEmbeddingHelper for the inference step (use the previous timestep to predict the next target). 
But I do not understand how does it work. In particular the different parameters used. For example why do we need a sequence length in the case of TrainingHelper (why do we not used an EOS)? Why both of them do not use the embedding_lookup or embedding_sequence as input ?</li>
</ul>
",Vectorization & Embeddings,tensorflow embedding training inference trying code simple neural machine translation using tensorflow little stuck regarding understanding embedding tensorflow understand difference case use one another second thing understand tf contrib seq seq traininghelper tf contrib seq seq greedyembeddinghelper know case translation use mainly traininghelper training step use previous target predict next target greedyembeddinghelper inference step use previous timestep predict next target understand doe work particular different parameter used example need sequence length case traininghelper used eos use embedding lookup embedding sequence input
How to get random word2vec vector for unknow word?,"<p>I train word2vec on train data, but there are some words from test data not in the train data, so how can I produce the word vector that match the data distribution or number range like original?</p>
",Vectorization & Embeddings,get random word vec vector unknow word train word vec train data word test data train data produce word vector match data distribution number range like original
Word Embedding for text classification,"<p>I am new in the NLP community and need more light on something.</p>
<p>I saw that Keras has an Embedding layer that is generally used before the LSTM layer. But what algorithm hides behind it? Is it Word2Vec, Glove or something else?</p>
<p>My task is a supervised text classification problem.</p>
",Vectorization & Embeddings,word embedding text classification new nlp community need light something saw kera ha embedding layer generally used lstm layer algorithm hide behind word vec glove something else task supervised text classification problem
Word2vec in pandas dataframe,"<p>I am trying to apply word2vec to check similarity of two columns per each row of my dataset.</p>
<p>For instance:</p>
<pre><code>Sent1                                     Sent2
It is a sunny day                         Today the weather is good. It is warm outside
What people think about democracy         In ancient times, Greeks were the first to propose democracy  
I have never played tennis                I do not know who Roger Feder is 
</code></pre>
<p>To apply word2vec, I consider the following:</p>
<pre><code>import numpy as np

words1 = sentence1.split(' ')
words2 = sentence2.split(' ')
#The meaning of the sentence can be interpreted as the average of its words
sentence1_meaning = word2vec(words1[0])
count = 1
for w in words1[1:]:

    sentence1_meaning = np.add(sentence1_meaning, word2vec(w))
    count += 1
sentence1_meaning /= count

sentence2_meaning = word2vec(words1[0])
count = 1

for w in words1[1:]:
    sentence1_meaning = np.add(sentence1_meaning, word2vec(w))
    count += 1
sentence1_meaning /= count

sentence2_meaning = word2vec(words2[0])
count = 1
sentence2_meaning = word2vec(words2[0])
count = 1
for w in words2[1:]:
    sentence2_meaning = np.add(sentence2_meaning, word2vec(w))
    count += 1
sentence2_meaning /= count

#Similarity is the cosine between the vectors
similarity = np.dot(sentence1_meaning, sentence2_meaning)/(np.linalg.norm(sentence1_meaning)*np.linalg.norm(sentence2_meaning))
</code></pre>
<p>However, this should work for two sentences not in a pandas dataframe.</p>
<p>Can you please tell me what I need to do for applying word2vec in case of a pandas dataframe to check similarity between sent1 and sent2? I would like a new column for the results.</p>
",Vectorization & Embeddings,word vec panda dataframe trying apply word vec check similarity two column per row dataset instance apply word vec consider following however work two sentence panda dataframe please tell need applying word vec case panda dataframe check similarity sent sent would like new column result
"Imbalanced dataset, size limitation of 60mb, email categorization","<p>I have a highly imbalanced dataset(approx - 1:100) of 1gb of raw emails, have to categorize these mails in 15 categories.</p>

<p>Problem that i have is that the size limit of file which will be used to train the model can not be more than 40mb.</p>

<p>So i want to filter out mails for each category which best represent the whole category.</p>

<p>For eg: for a category A, there are 100 emails in the dataset, due to size limitation i want to filter out only 10 emails which will represent the max features of all 100 emails.</p>

<p>I read that tfidf can be used to do this, for all the categories create a corpus of all the emails for that particular category and then try to find the emails that best represent but not sure how to do that. A code snippet will be of great help.</p>

<p>plus there are a lot of junk words and hash values in the dataset, should i clean all of those, even if i try its a lot to clean and manually its hard.</p>
",Vectorization & Embeddings,imbalanced dataset size limitation mb email categorization highly imbalanced dataset approx gb raw email categorize mail category problem size limit file used train model mb want filter mail category best represent whole category eg category email dataset due size limitation want filter email represent max feature email read tfidf used category create corpus email particular category try find email best represent sure code snippet great help plus lot junk word hash value dataset clean even try lot clean manually hard
Identifying personnal information from column description,"<p>I have a question about the identification of GDPR (General Data Protection Regulation) related sentences.
Is there a tool / method in Python, Java, ... that identifies whether a database column contains personnally identifiable information from its description only ?</p>
<p>We may think about using word embedding to get the &quot;most_similar&quot; or &quot;most_similar_cosmul&quot; words given a sentence and afterwards identifying keywords related to GDPR (biometric, personnal, id, photo...) but the results depend on the robustness of the word embedding model.</p>
<p>Thank you in advance,</p>
",Vectorization & Embeddings,identifying personnal information column description question identification gdpr general data protection regulation related sentence tool method python java identifies whether database column contains personnally identifiable information description may think using word embedding get similar similar cosmul word given sentence afterwards identifying keywords related gdpr biometric personnal id photo result depend robustness word embedding model thank advance
Word2Vec- does the word embedding change?,"<p>just wanted to know if there are 2 sentences-</p>
<ol>
<li>The <em><strong>bank</strong></em> remains closed on public holidays</li>
<li>Don't go near the river <em><strong>bank</strong></em></li>
</ol>
<p>The word 'bank' will have different word embeddings or same? If we use word2vec or glove?</p>
",Vectorization & Embeddings,word vec doe word embedding change wanted know sentence bank remains closed public go near river bank word bank different word embeddings use word vec glove
KMeans for Sentence Embeddings,"<h1><strong>K-MEANS Clustering b/w 2D NUMPY ARRAYS</strong></h1>
<p>I have been looking for a solution for a while and I can sense there must be something silly I might be missing so here goes.
I have obtained sentence embeddings after training an embedding layer using Keras Sequential Layers.</p>
<p><em>Dummy Example</em></p>
<p>Let's say we have embeddings which looks like this:</p>
<pre><code>Sentence 1 : np.array ([[6, 2], [3, 1], [7, 4], [8, 1], [5, 4], [9, 3], [5, 1]])

Sentence 2 : np.array ([[2, 5], [5, 7], [6, 5], [3, 1], [1, 1], [6,2], [2, 1]])
</code></pre>
<p>Basically, in a file with several sentences, I would want such sentence embeddings to be clustered so that similar sentences are clustered together.</p>
<p>I know this is the method we would use to cluster 1d arrays</p>
<pre><code>from sklearn.cluster import KMeans
import numpy as np

X = np.array([[1, 1], [-1, -1], [1, -1]])

kmeans = KMeans(n_clusters=3, random_state=0).fit(X)
</code></pre>
<p>I tried this:</p>
<pre><code>x = np.array([ [[6, 2], [3, 1], [7, 4], [8, 1], [5, 4], [11, 3], [5, 1]] , 
               [[6, 5], [8, 1], [7, 4],[8, 1], [5, 4], [11, 3], [5, 1]] ])

kmeans = KMeans(n_clusters=k, random_state=0).fit(x)
</code></pre>
<p>which throws <strong>ValueError: Found array with dim 3. Estimator expected &lt;= 2.</strong></p>
<p>Is it even possible to do k means clustering on such data or is there any other methodology I should follow?</p>
<p>One solution and the only one I can think of is to Average the Sentence Embeddings and use np.squeeze to squeeze the dimension of each sentence to a 1D ARRAY before clustering but it would mean losing all the positional information of the words in a sentence.</p>
<p><em>&quot;I am a dog&quot;  would be same as  &quot;Am I a dog&quot;</em> which is wrong</p>
",Vectorization & Embeddings,kmeans sentence embeddings k mean clustering b w numpy array looking solution sense must something silly might missing go obtained sentence embeddings training embedding layer using kera sequential layer dummy example let say embeddings look like basically file several sentence would want sentence embeddings clustered similar sentence clustered together know method would use cluster array tried throw valueerror found array dim estimator expected even possible k mean clustering data methodology follow one solution one think average sentence embeddings use np squeeze squeeze dimension sentence array clustering would mean losing positional information word sentence dog would dog wrong
How to get the highest tf-idf values of words for each class after using tfidf.vectorizer,"<p>So I have a dataset with 3 label: Soccer, Music and Movies
I used tfidf.vectorizer and then logistic regression to train my model, now I want to get a list of the 5 words that have the highest tfidf for each label.  (5 highest for soccer, 5 for Music ... )
I couldn't find a way to get them.</p>
<p>This is the code that I have</p>
<pre><code>le = preprocessing.LabelEncoder()
y = le.fit_transform(data[&quot;Label&quot;])
tfidf = TfidfVectorizer(analyzer = 'word')
X = tfidf.fit_transform(data[&quot;text&quot;])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)
lr = LogisticRegression()
lr.fit(X_train, y_train)
lr.score(X_test,y_test)
</code></pre>
<p>Thanks for the help.</p>
",Vectorization & Embeddings,get highest tf idf value word class using tfidf vectorizer dataset label soccer music movie used tfidf vectorizer logistic regression train model want get list word highest tfidf label highest soccer music find way get code thanks help
How to replace Character Embedding using LSTM with CharCNN?,"<p>I'm working with this repo <a href=""https://github.com/Franck-Dernoncourt/NeuroNER"" rel=""nofollow noreferrer"">https://github.com/Franck-Dernoncourt/NeuroNER</a>
It is using LSTM for Char-Level-Embedding and I want to use CNN for this.</p>
<p><a href=""https://github.com/Franck-Dernoncourt/NeuroNER/blob/3817feaf290c1f6e03ae23ea964e68c88d0e7a88/neuroner/entity_lstm.py#L92"" rel=""nofollow noreferrer"">Link</a> where it is using LSTM for Char-Level-Embedding</p>
<p>I tried using multiple implementation of CharCNN like this</p>
<pre class=""lang-py prettyprint-override""><code>self.character_embedding_weights = tf.get_variable(
                &quot;character_embedding_weights&quot;,
                shape=[dataset.alphabet_size, parameters['character_embedding_dimension']],
                initializer=initializer)
            embedded_characters = tf.nn.embedding_lookup(self.character_embedding_weights,
                                                         self.input_token_character_indices, name='embedded_characters')
            if self.verbose:
                print(&quot;embedded_characters: {0}&quot;.format(embedded_characters))
            utils_tf.variable_summaries(self.character_embedding_weights)
            s = tf.shape(embedded_characters)
            print(&quot;dimension-&quot;,s)
            char_embeddings = tf.reshape(embedded_characters, shape=[-1, 25, 20])

            # Conv #1
            conv1 = tf.layers.conv1d(
                inputs=char_embeddings,
                filters=30,
                kernel_size=3,
                padding=&quot;valid&quot;,
                activation=tf.nn.relu)

            # Conv #2
            conv2 = tf.layers.conv1d(
                inputs=conv1,
                filters=30,
                kernel_size=3,
                padding=&quot;valid&quot;,
                activation=tf.nn.relu)
            pool2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2)
            #
            # # Dense Layer
            character_cnn_output = tf.layers.dense(inputs=pool2, units=32, activation=tf.nn.relu)
</code></pre>
<p>Getting this issue when I concatenate The Char embedding with Word Embedding.</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\DeLL\Desktop\NLP\NeuroNER-master\venv\lib\site-packages\tensorflow_core\python\framework\ops.py&quot;, line 1607, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 3 but is rank 2 for 'concatenate_token_and_character_vectors/token_lstm_input' (op: 'ConcatV2') with input shapes: [?,10,32], [?,100], [].
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;C:\Users\DeLL\Desktop\NLP\NeuroNER-master\neuroner\neuromodel.py&quot;, line 483, in __init__
    self.model = EntityLSTM(self.modeldata, self.parameters)
  File &quot;C:\Users\DeLL\Desktop\NLP\NeuroNER-master\neuroner\entity_lstm.py&quot;, line 176, in __init__
    axis=1, name='token_lstm_input')
  File &quot;C:\Users\DeLL\Desktop\NLP\NeuroNER-master\venv\lib\site-packages\tensorflow_core\python\util\dispatch.py&quot;, line 180, in wrapper
    return target(*args, **kwargs)
  File &quot;C:\Users\DeLL\Desktop\NLP\NeuroNER-master\venv\lib\site-packages\tensorflow_core\python\ops\array_ops.py&quot;, line 1420, in concat
    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
  File &quot;C:\Users\DeLL\Desktop\NLP\NeuroNER-master\venv\lib\site-packages\tensorflow_core\python\ops\gen_array_ops.py&quot;, line 1257, in concat_v2
    &quot;ConcatV2&quot;, values=values, axis=axis, name=name)
  File &quot;C:\Users\DeLL\Desktop\NLP\NeuroNER-master\venv\lib\site-packages\tensorflow_core\python\framework\op_def_library.py&quot;, line 794, in _apply_op_helper
    op_def=op_def)
  File &quot;C:\Users\DeLL\Desktop\NLP\NeuroNER-master\venv\lib\site-packages\tensorflow_core\python\util\deprecation.py&quot;, line 507, in new_func
    return func(*args, **kwargs)
  File &quot;C:\Users\DeLL\Desktop\NLP\NeuroNER-master\venv\lib\site-packages\tensorflow_core\python\framework\ops.py&quot;, line 3357, in create_op
    attrs, op_def, compute_device)
  File &quot;C:\Users\DeLL\Desktop\NLP\NeuroNER-master\venv\lib\site-packages\tensorflow_core\python\framework\ops.py&quot;, line 3426, in _create_op_internal
    op_def=op_def)
  File &quot;C:\Users\DeLL\Desktop\NLP\NeuroNER-master\venv\lib\site-packages\tensorflow_core\python\framework\ops.py&quot;, line 1770, in __init__
    control_input_ops)
  File &quot;C:\Users\DeLL\Desktop\NLP\NeuroNER-master\venv\lib\site-packages\tensorflow_core\python\framework\ops.py&quot;, line 1610, in _create_c_op
    raise ValueError(str(e))
ValueError: Shape must be rank 3 but is rank 2 for'concatenate_token_and_character_vectors/token_lstm_input' (op: 'ConcatV2') with input shapes:[?,10,32], [?,100], [].**

</code></pre>
<p>This is the code snippit where I'm concatenating the output of CharCNN and word Embedding which led to an error.</p>
<pre><code>        if not parameters['use_character_lstm']:
            with tf.variable_scope(&quot;concatenate_token_and_character_vectors&quot;):

                if self.verbose: 
                    print('embedded_tokens: {0}'.format(embedded_tokens))
                
//This is the line where the error begins
                token_lstm_input = tf.concat([character_cnn_output, embedded_tokens], 
                    axis=1, name='token_lstm_input')

                if self.verbose: 
                    print(&quot;token_lstm_input: {0}&quot;.format(token_lstm_input))
        else:
            token_lstm_input = embedded_tokens
</code></pre>
",Vectorization & Embeddings,replace character embedding using lstm charcnn working repo using lstm char level embedding want use cnn link using lstm char level embedding tried using multiple implementation charcnn like getting issue concatenate char embedding word embedding code snippit concatenating output charcnn word embedding led error
Can I add a layer of meta data in a text classification model?,"<p>I am trying to create a multiclass classifier to identify topics of Facebook posts from a group of parliament members.</p>
<p>I'm using SimpleTransformers to put together an XML-RoBERTa-based classification model. Is there any way to add an embedding layer with metadata to improve the classifier? (For example, adding the political party to each Facebook post, together with the text itself.)</p>
",Vectorization & Embeddings,add layer meta data text classification model trying create multiclass classifier identify topic facebook post group parliament member using simpletransformers put together xml roberta based classification model way add embedding layer metadata improve classifier example adding political party facebook post together text
text2vec word embeddings : compound some tokens but not all,"<p>I am using {text2vec} word embeddings to build a dictionary of similar terms pertaining to a certain semantic category.</p>
<p>Is it OK to compound some tokens in the corpus, but not all? For example, I want to calculate terms similar to “future generation” or “rising generation”, but these collocations occur as separate terms in the original corpus of course. I am wondering if it is bad practice to gsub &quot;rising generation&quot; --&gt; &quot;rising_generation&quot;, without compounding all other terms that occur frequently together such as “climate change.”</p>
<p>Thanks!</p>
",Vectorization & Embeddings,text vec word embeddings compound token using text vec word embeddings build dictionary similar term pertaining certain semantic category ok compound token corpus example want calculate term similar future generation rising generation collocation occur separate term original corpus course wondering bad practice gsub rising generation rising generation without compounding term occur frequently together climate change thanks
How do I train word embeddings within a large block of custom text using BERT?,"<p>I found a great tutorial to generate contextualized word embedding for a custom sentence here: <a href=""http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/"" rel=""nofollow noreferrer"">http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/</a></p>
<p>However, it does not tell me how to train this on a larger paragraph. I have around 1,000 tokens that I want the model to learn. How can I adapt the link's code and apply it to a whole paragraph, so that each word learns the context from the whole document?</p>
",Vectorization & Embeddings,train word embeddings within large block custom text using bert found great tutorial generate contextualized word embedding custom sentence however doe tell train larger paragraph around token want model learn adapt link code apply whole paragraph word learns context whole document
CSR Sparse Matrix and TruncatedSVD Fit - NLP / Topic Modeling,"<p>I'm working on a topic modeling project. In this case, I would like to decrease the document matrix's dimensionality using Truncated SVD before applying KMeans.</p>
<p>I've processed (i.e. cleaned, tokenized, lemmatized) and vectorized the raw documents. I've applied a Tf-Idf vectorizer. As an FYI, I've successfully implemented the Tfidf vectorizer with LDA and NMF.</p>
<p>I've tried two approaches:</p>
<p><strong>FIRST APPROACH</strong></p>
<pre><code>docs_raw = wallstreet['processed_text']
n_topics = 20
max_doc_freq = 0.7
min_doc_freq = 4
max_features = 10000
ngram_rng = [1, 2]
max_iterations = 25

docs_vectorized, vectorizer = nlp_topic_utils.tfidf_vectorizer(docs_raw, min_doc_freq, max_doc_freq, max_features, ngram_rng)

tsvd = TruncatedSVD(n_components=docs_vectorized.shape[1]-1)
docs_tsvd = tsvd.fit(docs_vectorized)
</code></pre>
<p>This approach resulted in a Memory Error:</p>
<pre><code>MemoryError: Unable to allocate 3.38 GiB for an array with shape (45372, 10009) and data type float64
</code></pre>
<p>Also, I am unclear as to why <strong>docs_vectorized</strong> has 10,009 columns instead of 10,000.</p>
<p>I then attempted to use a Dask array, which is causing an Index Error.</p>
<p><strong>SECOND APPROACH</strong></p>
<pre><code>import dask.array as da

docs_vectorized, vectorizer = nlp_topic_utils.tfidf_vectorizer(docs_raw, min_doc_freq, max_doc_freq, max_features, ngram_rng)

docs_dask_arr = da.from_array(docs_vectorized, chunks='auto', asarray=True)

tsvd = TruncatedSVD(n_components=docs_dask_arr.shape[1]-1, random_state=random_state)
docs_tsvd = tsvd.fit(docs_dask_arr)
</code></pre>
<p>The corresponding error:</p>
<pre><code>IndexError: tuple index out of range
</code></pre>
<p>What am I missing?</p>
",Vectorization & Embeddings,csr sparse matrix truncatedsvd fit nlp topic modeling working topic modeling project case would like decrease document matrix dimensionality using truncated svd applying kmeans processed e cleaned tokenized lemmatized vectorized raw document applied tf idf vectorizer fyi successfully implemented tfidf vectorizer lda nmf tried two approach first approach approach resulted memory error also unclear doc vectorized ha column instead attempted use dask array causing index error second approach corresponding error missing
Doc2Vec most similar vectors don&#39;t match an input vector,"<p>I've got a dataset of job postings with about 40 000 records. I extracted skills from descriptions using NER with about 30 000 skills in the dictionary. Every skill is represented as an unique identificator.</p>
<p>The distribution of skills number for a posting looks like that:</p>
<p>mean        15.12 |
std         11.22 |
min          1.00 |
25%          7.00 |
50%         13.00 |
75%         20.00 |</p>
<p>I've trained a word2vec model using only skill ids and it works more or less fine. I can find most similar skills to a given one and the result looks okay.</p>
<p>But when it comes to a doc2vec model I'm not satisfied with the result.</p>
<p>I have about 3200 unique job titles, most of them have only few entries and there are quite a few of them being from the same field ('front end developer', 'senior javascript developer', 'front end engineer'). I delibirately went for a variety of job titles which I use as tags in doc2vec.TaggedDocument(). My goal is to see a number of relevant job titles when I input a vector of skills into docvecs.most_similar().</p>
<p>After training a model (I've tried different number of epochs (100,500,1000) and vector sizes (40 and 100)) sometimes it works correctly, but most of the time it doens't. For example for a skills set like [numpy, postgresql, pandas, xgboost, python, pytorch] I get the most similar job title with a skill set like [family court, acting, advising, social work].</p>
<p>Can it be a problem with the size of my dataset? Or the size of docs (I consider that I have short texts)? I also think that I misunderstand something about doc2vec mechanism and just ignore it. I'd also like to ask if you know any other, maybe more advanced, ideas how I can get relevant job titles from a skill set and compare two skill set vectors if they are close or far.</p>
<p>UPD:</p>
<p>Job titles from my data are 'tags' and skills are 'words'. Each text has a single tag. There are 40 000 documents with 3200 repeating tags. 7881 unique skill ids appear in the documents. The average number of skill words per document is 15.</p>
<p>My data example:</p>
<pre><code>         job_titles                                             skills
1  business manager                 12 13 873 4811 482 2384 48 293 48
2    java developer      48 2838 291 37 484 192 92 485 17 23 299 23...
3    data scientist      383 48 587 475 2394 5716 293 585 1923 494 3
</code></pre>
<p>The example of my code:</p>
<pre><code>def tagged_document(df):
    #tagging documents
    for index, row in df.iterrows():
        yield gensim.models.doc2vec.TaggedDocument(row['skills'].split(), [row['job_title']])


data_for_training = list(tagged_document(job_data[['job_titles', 'skills']])

model_d2v = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)

model_d2v.train(data_for_training, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)

#the skill set contains close skills which represent a front end developer
skillset_ids = '12 34 556 453 1934'.split()                                                  
new_vector = model_d2v.infer_vector(skillset_ids, epochs=100)
model_d2v.docvecs.most_similar(positive=[new_vector], topn=30)
</code></pre>
<p>I've been experimenting recently and noticed that it performs a little better if I filter out documents with less than 10 skills. Still, there are some irrelevant job titles coming out.</p>
",Vectorization & Embeddings,doc vec similar vector match input vector got dataset job posting record extracted skill description using ner skill dictionary every skill represented unique identificator distribution skill number posting look like mean std min trained word vec model using skill id work le fine find similar skill given one result look okay come doc vec model satisfied result unique job title entry quite field front end developer senior javascript developer front end engineer delibirately went variety job title use tag doc vec taggeddocument goal see number relevant job title input vector skill docvecs similar training model tried different number epoch vector size sometimes work correctly time doens example skill set like numpy postgresql panda xgboost python pytorch get similar job title skill set like family court acting advising social work problem size dataset size doc consider short text also think misunderstand something doc vec mechanism ignore also like ask know maybe advanced idea get relevant job title skill set compare two skill set vector close far upd job title data tag skill word text ha single tag document repeating tag unique skill id appear document average number skill word per document data example example code experimenting recently noticed performs little better filter document le skill still irrelevant job title coming
Can doc2vec work on an artificial &quot;text&quot;?,"<p>I've created an artificial corpus (with 52624 documents). Each document is a list of objects (there are 461 of them).</p>
<p>So one possibility could be: <code>['chair', 'chair', 'chair', 'chair', 'chair', 'table', 'table']</code></p>
<p>Here's a bar plot (log-scale) of the vocabulary.</p>
<p><a href=""https://i.sstatic.net/B1CP3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/B1CP3.png"" alt=""enter image description here"" /></a></p>
<p>And this is how I defined the model:</p>
<pre><code>model = gensim.models.doc2vec.Doc2Vec(vector_size=8, workers=4, min_count=1,epochs=40, dm=0)
</code></pre>
<p>Observing at:
<code>model.wv.most_similar_cosmul(positive = [&quot;chair&quot;])</code></p>
<p>I see non related words</p>
<p>And it seems to me that the following works poorly as well:</p>
<pre><code>inferred_vector = model.infer_vector([&quot;chair&quot;])
model.docvecs.most_similar([inferred_vector])
</code></pre>
<p>Where has my model failed?</p>
<p><strong>UPDATE</strong></p>
<p>There is the data (JSON file):</p>
<p><a href=""https://gofile.io/d/bZDcPX"" rel=""nofollow noreferrer"">https://gofile.io/d/bZDcPX</a></p>
",Vectorization & Embeddings,doc vec work artificial text created artificial corpus document document list object one possibility could bar plot log scale vocabulary defined model observing see non related word seems following work poorly well ha model failed update data json file
what is workers parameter in word2vec in NLP,"<p>in below code .
i didn't understand the meaning of workers parameter .
model = Word2Vec(sentences, size=300000, window=2, min_count=5, workers=4)</p>
",Vectorization & Embeddings,worker parameter word vec nlp code understand meaning worker parameter model word vec sentence size window min count worker
Word vocabulary generated by Word2vec and Glove models are different for the same corpus,"<p>I'm using CONLL2003 dataset to generate word embeddings using Word2vec and Glove.
The number of words returned by word2vecmodel.wv.vocab is different(much lesser) than glove.dictionary.
Here is the code:
Word2Vec:</p>
<pre><code>word2vecmodel = Word2Vec(result ,size= 100, window =5, sg = 1)
X = word2vecmodel[word2vecmodel.wv.vocab]
w2vwords = list(word2vecmodel.wv.vocab)
</code></pre>
<p>Output len(w2vwords) = 4653</p>
<p>Glove:</p>
<pre><code>from glove import Corpus
from glove import Glove
import numpy as np
corpus = Corpus()
nparray = []
allwords = []
no_clusters=500
corpus.fit(result, window=5)
glove = Glove(no_components=100, learning_rate=0.05)
glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)
glove.add_dictionary(corpus.dictionary)
</code></pre>
<p>Output: len(glove.dictionary) = 22833</p>
<p>The input is a list of sentences. For example:
result[1:5] =</p>
<pre><code>['Peter', 'Blackburn'],
 ['BRUSSELS', '1996-08-22'],
 ['The',
  'European',
  'Commission',
  'said',
  'Thursday',
  'disagreed',
  'German',
  'advice',
  'consumers',
  'shun',
  'British',
  'lamb',
  'scientists',
  'determine',
  'whether',
  'mad',
  'cow',
  'disease',
  'transmitted',
  'sheep',
  '.'],
 ['Germany',
  &quot;'s&quot;,
  'representative',
  'European',
  'Union',
  &quot;'s&quot;,
  'veterinary',
  'committee',
  'Werner',
  'Zwingmann',
  'said',
  'Wednesday',
  'consumers',
  'buy',
  'sheepmeat',
  'countries',
  'Britain',
  'scientific',
  'advice',
  'clearer',
  '.']]
</code></pre>
<p>There are totally 13517 sentences in the result list.
Can someone please explain why the list of words for which the embeddings are created are drastically different in size?</p>
",Vectorization & Embeddings,word vocabulary generated word vec glove model different corpus using conll dataset generate word embeddings using word vec glove number word returned word vecmodel wv vocab different much lesser glove dictionary code word vec output len w vwords glove output len glove dictionary input list sentence example result totally sentence result list someone please explain list word embeddings created drastically different size
creating a common embedding for two languages,"<p>My task deals with multi-language like (english and hindi). For that I need a common embedding to represent both languages. </p>

<p>I know there are methods for learning multilingual embedding like 'MUSE', but this represents those two embeddings in a common vector space, obviously they are similar, but not the same.</p>

<p>So I wanted to know if there is any method or approach that can learn to represent both embedding in form of a single embedding that represents the both the language.</p>

<p>Any lead is strongly appreciated!!!</p>
",Vectorization & Embeddings,creating common embedding two language task deal multi language like english hindi need common embedding represent language know method learning multilingual embedding like muse represents two embeddings common vector space obviously similar wanted know method approach learn represent embedding form single embedding represents language lead strongly appreciated
Working of embedding layer in Tensorflow,"<p>Can someone please explain me the inputs and outputs along with the working of the layer mentioned below</p>
<pre><code>model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))
</code></pre>
<blockquote>
<p>total_words = 263</p>
<p>max_sequence_len=11</p>
</blockquote>
<p>Is 64, the number of dimensions?</p>
<p>And why is the output of this layer (None, 10, 64)</p>
<p>Shouldn't it be a 64 dimension vector for each word, i.e (None, 263, 64)</p>
",Vectorization & Embeddings,working embedding layer tensorflow someone please explain input output along working layer mentioned total word max sequence len number dimension output layer none dimension vector word e none
Distances between embedded word vector,"<p>I would need to find a relationship between the word <code>4G</code> , 5G and <code>mobile phones</code> or <code>Internet</code> in order to cluster sentences about technology all together.
Following a suggestion, I used for this purpose word2vec.</p>
<p>I have tried as follows:</p>
<pre><code>from time import time
start_nb = time()

# Initialize logging.
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')

sentence_1 = '4G is the fourth generation of broadband network.'
sentence_2 = 'I bought a new mobile phone. '
sentence_1 = sentence_1.lower().split()
sentence_2 = sentence_2.lower().split()

# Import and download stopwords from NLTK.
from nltk.corpus import stopwords
from nltk import download
download('stopwords')  # Download stopwords list.

# Remove stopwords.
stop_words = stopwords.words('english')
sentence_1 = [w for w in sentence_1 if w not in stop_words]
sentence_2 = [w for w in sentence_2 if w not in stop_words]


start = time()
import os

from gensim.models import Word2Vec
if not os.path.exists('/data/w2v_googlenews/GoogleNews-vectors-negative300.bin.gz'):
    raise ValueError(&quot;SKIP: You need to download the google news model&quot;)
    
model = Word2Vec.load_word2vec_format('/data/w2v_googlenews/GoogleNews-vectors-negative300.bin.gz', binary=True)


distance = model.wmdistance(sentence_1, sentence_2)
print 'distance = %.4f' % distance

sentence_3 = '5G is dangerous!'
sentence_3 = sentence_3.lower().split()
sentence_3 = [w for w in sentence_3 if w not in stop_words]

distance = model.wmdistance(sentence_1, sentence_3)
distance = model.wmdistance(sentence_2, sentence_3)
</code></pre>
<p>The results do not show a relevant connection between 4G, 5G and mobile phone.  I would like to show their relevant connection visually, for example like in this plot</p>
<p><a href=""https://i.sstatic.net/9YeAP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9YeAP.png"" alt=""enter image description here"" /></a></p>
<p>but with 4G, 5G and mobile phones.</p>
<p>However the main problem (this is my question) is how to improve the connection/distance between these words.</p>
",Vectorization & Embeddings,distance embedded word vector would need find relationship word g order cluster sentence technology together following suggestion used purpose word vec tried follows result show relevant connection g g mobile phone would like show relevant connection visually example like plot g g mobile phone however main problem question improve connection distance word
word2vec - find a word by a specific vector,"<p>I trained a gensim Word2Vec model.
Let's say I have a certain vector and I want the find the word it represents - what is the best way to do so?</p>

<p>Meaning, for a specific vector:</p>

<pre><code>vec = array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)
</code></pre>

<p>I want to get a word:</p>

<pre><code> 'computer' = model.vec2word(vec)
</code></pre>
",Vectorization & Embeddings,word vec find word specific vector trained gensim word vec model let say certain vector want find word represents best way meaning specific vector want get word
Is there a way of creating a cosine similarity matrix between sentence embeddings having different values?,"<p>I want to create a cosine similarity matrix of size 7x7 where each element of the matrix will be the cosine similarity of two arrays of size 1024.</p>
<pre><code>[[ 0.1463873   0.6160218  -0.8804966  ...  1.520877    0.09114664
   0.14081596]]
[[ 0.54208326  0.7649026  -1.4366877  ...  1.6818116  -0.20427406
   0.3631045 ]]
[[ 0.32065052  0.67767006 -1.2465438  ...  0.6658634  -0.17746
   0.39568862]]
[[ 0.25573847  0.70055985 -1.1845624  ...  1.4804083  -0.34156996
   0.04723666]]
[[ 0.62882924  1.3213214  -1.4690932  ...  1.3146497  -0.1773764
  -0.4018889 ]]
[[ 0.82711285  1.1108592  -1.1221949  ...  1.4259428  -0.41509023
  -0.03925738]]
[[-0.04750526  0.42094198 -1.2134333  ...  0.7967724  -0.08025895
   0.32510945]]
</code></pre>
<p>Suppose these are the 7 arrays of size 1024 each. I want to generate a cosine similarity matrix between each of them such that the size of matrix is 7x7. Is there any way to do it?</p>
",Vectorization & Embeddings,way creating cosine similarity matrix sentence embeddings different value want create cosine similarity matrix size x element matrix cosine similarity two array size suppose array size want generate cosine similarity matrix size matrix x way
Understanding get_sentence_vector() and get_word_vector() for fasttext,"<p>The thing I want to do is to get embeddings of a pair of words or phrases and calculate similarity.</p>
<p>I observed that the similarity is the same when I switch between get_sentence_vector() and get_word_vector() for a word. For example, I can switch the method when calculating embedding_2 or embedding_3, but embedding_2 and embedding_3 are not euqal, which is weird:</p>
<pre class=""lang-py prettyprint-override""><code>    from scipy.spatial.distance import cosine
    import numpy as np
    import fasttext
    import fasttext.util
    
    # download an english model
    fasttext.util.download_model('en', if_exists='ignore')  # English
    model = fasttext.load_model('cc.en.300.bin')
    
    # Getting word vectors for 'one' and 'two'.
    embedding_1 = model.get_sentence_vector('baby dog')
    embedding_2 = model.get_word_vector('puppy')
    embedding_3 = model.get_sentence_vector('puppy')
    
    def cosine_similarity(embedding_1, embedding_2):
        # Calculate the cosine similarity of the two embeddings.
        sim = 1 - cosine(embedding_1, embedding_2)
        print('Cosine similarity: {:.2}'.format(sim))
        
    # compare the embeddings
    cosine_similarity(embedding_1, embedding_2)
    # compare the embeddings
    cosine_similarity(embedding_1, embedding_3)
    
    
    # Checking if the two approaches yield the same result.
    is_equal = np.array_equal(embedding_2, embedding_3)
    
    # Printing the result.
    print(is_equal)
</code></pre>
<p>If I switch methods, similarity is always 0.76 but is_equal is false. I have two questions:</p>
<p>(1) I probably have to use get_sentence_vector() for phrases, but in terms of words, which one should I use? What happens when I call get_sentence_vector() for a word?</p>
<p>(2) I use fasttext because it can handle out of vocabulary, is it a good idea to use fasttext's embedding for cosine similarity comparison?</p>
",Vectorization & Embeddings,understanding get sentence vector get word vector fasttext thing want get embeddings pair word phrase calculate similarity observed similarity switch get sentence vector get word vector word example switch method calculating embedding embedding embedding embedding euqal weird switch method similarity always equal false two question probably use get sentence vector phrase term word one use happens call get sentence vector word use fasttext handle vocabulary good idea use fasttext embedding cosine similarity comparison
Looking for an effective NLP Phrase Embedding model,"<p>The goal I want to achieve is to find a good word_and_phrase embedding model that can do:
(1) For the words and phrases that I am interested in, they have embeddings.
(2) I can use embeddings to compare similarity between two things(could be word or phrase)</p>
<p>So far I have tried two paths:</p>
<p>1: Some Gensim-loaded pre-trained models, for instance:</p>
<pre><code>from gensim.models.word2vec import Word2Vec
import gensim.downloader as api
# download the model and return as object ready for use
model_glove_twitter = api.load(&quot;fasttext-wiki-news-subwords-300&quot;)
model_glove_twitter.similarity('computer-science', 'machine-learning')
</code></pre>
<p>The problem with this path is that I do not know if a phrase has embedding. For this example, I got this error:</p>
<pre><code>KeyError: &quot;word 'computer-science' not in vocabulary&quot;
</code></pre>
<p>I will have to try different pre-trained models, such as word2vec-google-news-300, glove-wiki-gigaword-300, glove-twitter-200, etc. Results are similar, there are always phrases of interests not having embeddings.</p>
<ol start=""2"">
<li>Then I tried to use some BERT-based sentence embedding method: <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a>.</li>
</ol>
<pre><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('distilbert-base-nli-mean-tokens')

from scipy.spatial.distance import cosine

def cosine_similarity(embedding_1, embedding_2):
    # Calculate the cosine similarity of the two embeddings.
    sim = 1 - cosine(embedding_1, embedding_2)
    print('Cosine similarity: {:.2}'.format(sim))

phrase_1 = 'baby girl'
phrase_2 = 'annual report'
embedding_1 = model.encode(phrase_1)
embedding_2 = model.encode(phrase_2)
cosine_similarity(embedding_1[0], embedding_2[0])
</code></pre>
<p>Using this method I was able to get embeddings for my phrases, but the similarity score was 0.93, which did not seem to be reasonable.</p>
<p>So what can I try else to achieve the two goals mentioned above?</p>
",Vectorization & Embeddings,looking effective nlp phrase embedding model goal want achieve find good word phrase embedding model word phrase interested embeddings use embeddings compare similarity two thing could word phrase far tried two path gensim loaded pre trained model instance problem path know phrase ha embedding example got error try different pre trained model word vec google news glove wiki gigaword glove twitter etc result similar always phrase interest embeddings tried use bert based sentence embedding method using method wa able get embeddings phrase similarity score wa seem reasonable try else achieve two goal mentioned
neural network input shape,"<p>consider if my corpus has 5 sentences, where maximum sentence size is 10 words.</p>
<p>Hence, will the embedding matrix be 5x10.</p>
<p>And what will the input shape of the input layer of a neural network, or how the data will be given as input to neural network.</p>
",Vectorization & Embeddings,neural network input shape consider corpus ha sentence maximum sentence size word hence embedding matrix x input shape input layer neural network data given input neural network
Gensim Word2vec model is not updating the previous word&#39;s embedding weights during increased training,"<p>I want to train a previous-trained word2vec model in a increased way that is update the word's weights if the word has been seen in the previous training process and create and update the weights of the new words that has not been seen in the previous training process. For example:</p>
<pre><code>from gensim.models import Word2Vec
# old corpus
corpus = [[&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;], [&quot;2&quot;, &quot;3&quot;, &quot;1&quot;]]
# first train on old corpus
model = Word2Vec(sentences=corpus, size=2, min_count=0, window=2)
# checkout the embedding weights for word &quot;1&quot;
print(model[&quot;1&quot;])

# here comes a new corpus with new word &quot;4&quot; and &quot;5&quot;
newCorpus = [[&quot;4&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;], [&quot;1&quot;, &quot;5&quot;, &quot;2&quot;]]

# update the previous trained model
model.build_vocab(newCorpus, update=True)
model.train(newCorpus, total_examples=model.corpus_count, epochs=1)

# check if new word has embedding weights:
print(model[&quot;4&quot;])  # yes

# check if previous word's embedding weights are updated
print(model[&quot;1&quot;])  # output the same as before
</code></pre>
<p>It seems that the previous word's embedding is not updated even though the previous word's context has benn changed in the new corpus. Could someone tell me how to make the previous embedding weights updated?</p>
",Vectorization & Embeddings,gensim word vec model updating previous word embedding weight increased training want train previous trained word vec model increased way update word weight word ha seen previous training process create update weight new word ha seen previous training process example seems previous word embedding updated even though previous word context ha benn changed new corpus could someone tell make previous embedding weight updated
How to simplify text comparison for big data-set where text meaning is same but not exact - deduplicate text data,"<p>I have text data set (different menu items like chocolate, cake, coke etc) of around 1.8 million records which belongs to 6 different categories (category A, B, C, D, E, F). one of the category has around 700k records. Most of the menu items are mixed up in multiple categories to which they doesn't belong to, for example: cake belongs to category 'A' but it is found in category 'B' &amp; 'C' as well.</p>
<p>I want to identify those misclassified items and report to a personnel but the challenge is the item name is not always correct because it is totally human typed text. For example: Chocolate might be updated as hot chclt, sweet choklate, chocolat etc. There can also be items like chocolate cake ;)</p>
<p>so to handle this, I tried a simple method using cosine similarity to compare category-wise and identify those anomalies but it takes alot of time since I am comparing each items to 1.8 million records (Sample code is as shown below). Can anyone suggest a better way to deal with this problem?</p>
<pre><code>#Function
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 

def cos_similarity(a,b):
    X =a
    Y =b

    # tokenization 
    X_list = word_tokenize(X)  
    Y_list = word_tokenize(Y) 

    # sw contains the list of stopwords 
    sw = stopwords.words('english')  
    l1 =[];l2 =[] 

    # remove stop words from the string 
    X_set = {w for w in X_list if not w in sw}  
    Y_set = {w for w in Y_list if not w in sw} 

    # form a set containing keywords of both strings  
    rvector = X_set.union(Y_set)  
    for w in rvector: 
        if w in X_set: l1.append(1) # create a vector 
        else: l1.append(0) 
        if w in Y_set: l2.append(1) 
        else: l2.append(0) 
    c = 0

    # cosine formula  
    for i in range(len(rvector)): 
            c+= l1[i]*l2[i] 
    if float((sum(l1)*sum(l2))**0.5)&gt;0:
        cosine = c / float((sum(l1)*sum(l2))**0.5) 
    else:
        cosine = 0
    return cosine

#Base code
cos_sim_list = []
for i in category_B.index:
    ln_cosdegree = 0
    ln_degsem = []
    for j in category_A.index:
        ln_j = str(category_A['item_name'][j])
        ln_i = str(category_B['item_name'][i])
        degreeOfSimilarity = cos_similarity(ln_j,ln_i)
        if degreeOfSimilarity&gt;0.5:
            cos_sim_list.append([ln_j,ln_i,degreeOfSimilarity])
</code></pre>
<p>Consider text is already cleaned</p>
",Vectorization & Embeddings,simplify text comparison big data set text meaning exact text data text data set different menu item like chocolate cake coke etc around million record belongs different category category b c e f one category ha around k record menu item mixed multiple category belong example cake belongs category found category b c well want identify misclassified item report personnel challenge item name always correct totally human typed text example chocolate might updated hot chclt sweet choklate chocolat etc also item like chocolate cake handle tried simple method using cosine similarity compare category wise identify anomaly take alot time since comparing item million record sample code shown anyone suggest better way deal problem consider text already cleaned
Autotune Embedding with Fasttext,"<p>I have trained word embeddings using Fasttext - train.unsuperwised.
Is there a way to autotune the hyperparameters for this? Documentation gives autotuning for supervised training but I am not sure how supervised training can be done for embeddings.</p>
",Vectorization & Embeddings,autotune embedding fasttext trained word embeddings using fasttext train unsuperwised way autotune hyperparameters documentation give autotuning supervised training sure supervised training done embeddings
is word embedding in Keras a dimensionality reduction technique also?,"<p>I wanted to understand the purpose of <code>embedding_dim</code> vs using a one hot vector of the entire <code>vocab_size</code>, Is it a dimension reduction to the one hot vector from <code>vocab_size</code> dim to <code>embedding_dim</code> dimensions or is there any other utility intuitively? Also how should one decide the <code>embedding_dim</code> number?</p>
<p>Code -</p>
<pre><code>    vocab_size = 10000
    embedding_dim = 16
    max_length = 120
    
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(6, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
    model.summary()
</code></pre>
<p>O/P -</p>
<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 120, 16)           160000    
_________________________________________________________________
flatten (Flatten)            (None, 1920)              0         
_________________________________________________________________
dense (Dense)                (None, 6)                 11526     
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 7         
=================================================================
Total params: 171,533
Trainable params: 171,533
Non-trainable params: 0
_________________________________________________________________
</code></pre>
",Vectorization & Embeddings,word embedding kera dimensionality reduction technique also wanted understand purpose v using one hot vector entire dimension reduction one hot vector dim dimension utility intuitively also one decide number code p
Python tf-idf: fast way to update the tf-idf matrix,"<p>I have a dataset of several thousand rows of text, my target is to calculate the tfidf score and then cosine similarity between documents, this is what I did using gensim in Python followed the tutorial:</p>

<pre><code>dictionary = corpora.Dictionary(dat)
corpus = [dictionary.doc2bow(text) for text in dat]

tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
index = similarities.MatrixSimilarity(corpus_tfidf)
</code></pre>

<p>Let's say we have the tfidf matrix and similarity built, when we have a new document come in, I want to query for its most similar document in our existing dataset.</p>

<p>Question: is there any way we can update the tf-idf matrix so that I don't have to append the new text doc to the original dataset and recalculate the whole thing again? </p>
",Vectorization & Embeddings,python tf idf fast way update tf idf matrix dataset several thousand row text target calculate tfidf score cosine similarity document using gensim python followed tutorial let say tfidf matrix similarity built new document come want query similar document existing dataset question way update tf idf matrix append new text doc original dataset recalculate whole thing
How to retrieve array in Word2Vec,"<p>I am trying to retrieve the array/vector of a word in my trained word2vec model.  In SpaCy this is possible with model.vocab.get_vector(&quot;word&quot;), but I can't find a way to do it in word2Vec</p>
",Vectorization & Embeddings,retrieve array word vec trying retrieve array vector word trained word vec model spacy possible model vocab get vector word find way word vec
How to use BERT pre-trained model in Keras Embedding layer,"<p>How do I use a pre-trained BERT model like <code>bert-base-uncased</code> as weights in the Embedding layer in Keras?</p>
<p>Currently, I am generating word embddings using BERT model and it takes a lot of time. And I am assigning those weights like in the cide shown below</p>
<pre><code>model.add(Embedding(307200, 1536, input_length=1536, weights=[embeddings]))
</code></pre>
<p>I searched on internet but the method is given in PyTorch. I need to do it in Keras. Please help.</p>
",Vectorization & Embeddings,use bert pre trained model kera embedding layer use pre trained bert model like weight embedding layer kera currently generating word embddings using bert model take lot time assigning weight like cide shown searched internet method given pytorch need kera please help
how to reduce time complexity of clustering over doc2vec embedding?,"<p>I have a bunch of vectors of 300 dimensions each, as part of doc2vec embedding. Each vector is a representation of an article. My goal is to discard the duplicate articles. I was thinking of running <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.cluster.dbscan.html"" rel=""nofollow noreferrer"">DBSCAN</a> clustering over my dataset, and then for each cluster label, discard the duplicates. I chose DBSCAN because it's hierarchical, and it can take cosine distance as metric, which I think makes more sense than euclidean distance for document similarity detection.</p>
<p>Issue is, if I choose cosine distance, then sklearn DBSCAN implementation doesn't allow ‘ball_tree’ and ‘kd_tree’ as the algorithm, hence I'm left with ‘brute’, which I'm guessing has O(n^2) complexity when run for all vectors.</p>
<p>If I don't go the clustering route, then I see two options. Calculate similarity using the <code>most_similar</code> method for all vectors, either in vanilla doc2vec, or through annoy index. According to <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_annoy.html"" rel=""nofollow noreferrer"">this</a> doc vanilla doc2vec has linear complexity for similarity query, whereas annoy gives sub-linear time complexity. I'd like to know, which one would be the best choice given my use case? Is there a better approach I'm missing?</p>
",Vectorization & Embeddings,reduce time complexity clustering doc vec embedding bunch vector dimension part doc vec embedding vector representation article goal discard duplicate article wa thinking running dbscan clustering dataset cluster label discard duplicate chose dbscan hierarchical take cosine distance metric think make sense euclidean distance document similarity detection issue choose cosine distance sklearn dbscan implementation allow ball tree kd tree algorithm hence left brute guessing ha n complexity run vector go clustering route see two option calculate similarity using method vector either vanilla doc vec annoy index according doc vanilla doc vec ha linear complexity similarity query whereas annoy give sub linear time complexity like know one would best choice given use case better approach missing
Can BERT takes more than 2 sentences for word embeddings?,"<p>I'm trying to implement BERT word embedding to LDA topic model. The corpus consists of multiple sentences(more than two). I noticed BERT Model from hugging face requires segmentation id which differentiates sentences.</p>
<p>Is BertModel capable for more than two sentences?</p>
<p>If so, how to assign segment ids to tokens in different sentences?</p>
",Vectorization & Embeddings,bert take sentence word embeddings trying implement bert word embedding lda topic model corpus consists multiple sentence two noticed bert model hugging face requires segmentation id differentiates sentence bertmodel capable two sentence assign segment id token different sentence
Tokens returned in transformers Bert model from encode(),"<p>I have a small dataset for sentiment analysis. The classifier will be a simple KNN but I wanted to get the word embedding with the <code>Bert</code> model from the <code>transformers</code> library. Note that I just found out about this library - I am still learning.</p>
<p>So looking at online example, I am trying to understand the dimensions that are returned from the model.</p>
<p>Example:</p>
<pre><code>from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

tokens = tokenizer.encode([&quot;Hello, my dog is cute&quot;, &quot;He is really nice&quot;])
print(tokens)

tokens = tokenizer.encode(&quot;Hello, my dog is cute&quot;, &quot;He is really nice&quot;)
print(tokens)

tokens = tokenizer.encode([&quot;Hello, my dog is cute&quot;])
print(tokens)

tokens = tokenizer.encode(&quot;Hello, my dog is cute&quot;)
print(tokens)
</code></pre>
<p>The output is the following:</p>
<pre><code>[101, 100, 100, 102]

[101, 7592, 1010, 2026, 3899, 2003, 10140, 102, 2002, 2003, 2428, 3835, 102]

[101, 100, 102]

[101, 7592, 1010, 2026, 3899, 2003, 10140, 102]
</code></pre>
<p>I can't seem to find the docs for <code>encode()</code> - I have no idea why it returns different stuff when the input is passed as a list. What is this doing?</p>
<p>Additionally, is there a method to pass a word token and get the actual word back - to troubleshoot the above?</p>
<p>Thank you in advance</p>
",Vectorization & Embeddings,token returned transformer bert model encode small dataset sentiment analysis classifier simple knn wanted get word embedding model library note found library still learning looking online example trying understand dimension returned model example output following seem find doc idea return different stuff input passed list additionally method pas word token get actual word back troubleshoot thank advance
What if the word I chose doesnt have any embedded vector in pretrained word embedding matrix?,"<p>Suppose my text corpus includes a rare word which is not present in pre-trained word embedding. How do I tackle this obstacle?</p>
",Vectorization & Embeddings,word chose doesnt embedded vector pretrained word embedding matrix suppose text corpus includes rare word present pre trained word embedding tackle obstacle
NLP analysis for some pyspark dataframe columns by numpy vectorization,"<p>I would like to do some NLP analysis for a string column in pyspark dataframe.</p>
<p>df:</p>
<pre><code> year month u_id rating_score p_id review
 2010 09    tvwe  1           p_5  I do not like it because its size is not for me.  
 2011 11    frsa  1           p_7  I am allergic to the peanut elements.  
 2015 5     ybfd  1           p_2  It is a repeated one, please no more.
 2016 7     tbfb  2           p_2  It is not good for my oil hair.
 
</code></pre>
<p>Each p_id represents an item.
Each u_id may have some reviews for each item. The review can be several words, one sentence or a paragraph or even emoji.</p>
<p>I would like to find the root reasons that the items are rated low or high.
For example, how many &quot;u_id&quot;s complain the issue of item's size, chemical elements allergy or others, which are relevant to the items' features.</p>
<p>From <a href=""https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas"">How to iterate over rows in a DataFrame in Pandas</a>, I learn that it is more efficient to transform the dataframe to numpy array then use vectorization to do NLP analysis.</p>
<p>I am trying to use SparkNLP to extract adjectives and noun phrase for each comment by year, month, u_id, p_id.</p>
<p>I am not sure how to apply the numpy vectorization to do this very efficiently.</p>
<p>My py3 code:</p>
<pre><code>from sparknlp.pretrained import PretrainedPipeline
df = spark.sql('select year, month, u_id, p_id, comment from MY_DF where rating_score = 1 and isnull(comment) = false')
import numpy as np

trainseries = df['comment'].apply(lambda x : np.array(x.toArray())).as_matrix().reshape(-1,1)

text = np.apply_along_axis(lambda x : x[0], 1, trainseries) # TypeError: 'Column' object is not callable

pipeline_dl = PretrainedPipeline('explain_document_dl', lang='en') # 
result = pipeline_dl.fullAnnotate(text)
</code></pre>
<p>The code does not work.
I also need to keep the other columns (e.g. year, month, u_id, p_id) in the vectorization and assure that the NLP analysis results can be aligned with year, month, u_id, p_id well.</p>
<p>I do not like this
<a href=""https://stackoverflow.com/questions/58162761/how-to-convert-a-pyspark-dataframe-column-to-numpy-array"">How to convert a pyspark dataframe column to numpy array</a> because collect() is too slow.</p>
<p>Thanks</p>
",Vectorization & Embeddings,nlp analysis pyspark dataframe column numpy vectorization would like nlp analysis string column pyspark dataframe df p id represents item u id may review item review several word one sentence paragraph even emoji would like find root reason item rated low high example many u id complain issue item size chemical element allergy others relevant item feature collect slow thanks
Can we use sentence transformers to embed non english sentences without labels?,"<p>I was trying to use this project :</p>
<p><a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a></p>
<p>for embedding non english sentences, the language is not a human speaking language, its machine language (x86)</p>
<p>but the problem is i cannot find a simple example where it shows how can i embed sentences using a custom dataset without any labels or similarity values of the sentences.</p>
<p>basically i have an array of sentences lists without any labels for sentences or similarity values for them, and i want to embed them into vectors in a way that it preserves the semantic of the sentence the best way possible, so far i have used word2vec and doc2vec using gensim library so i wanted to try this method to see if its any better?</p>
<p>(also any other suggestion for methods to use is appreciated)</p>
",Vectorization & Embeddings,use sentence transformer embed non english sentence without label wa trying use project embedding non english sentence language human speaking language machine language x problem find simple example show embed sentence using custom dataset without label similarity value sentence basically array sentence list without label sentence similarity value want embed vector way preserve semantic sentence best way possible far used word vec doc vec using gensim library wanted try method see better also suggestion method use appreciated
Semantic similarity (text comparison) in NLP - best package or cloud service?,"<p>I am trying to find readymade service for text comparison (the meaning of sentences including consideration of synonyms) </p>

<p>I found out that AWS Comprehend does not support text comparison. I am looking for a trusted way to quickly implement.</p>
",Vectorization & Embeddings,semantic similarity text comparison nlp best package cloud service trying find readymade service text comparison meaning sentence including consideration synonym found aws comprehend doe support text comparison looking trusted way quickly implement
How do I apply tokenizer.fit_on_texts() to a data frame with two columns of objects/strings I need to train?,"<p>I need to pass two sets of data into <code>tokenizer.fit_on_texts()</code>, but having issues with it not recognizing the text. <code>tokenizer.word_index()</code> is returning is the number 2. I suspect the issue is occurring at <code>tokenizer.fit_on_texts()</code> as I am passing it a data frame with (33481, 2) of strings. Most of the examples I have looked at have used the IMBD data set.</p>
<p>Additional information:
I'm currently experimenting with multi classification problem where there are headline-article pairs with labels (agree, disagree, discuss, unrelated). I plan to use LSTM and the pre-trained Glove to create an index of words mapped to known embedding.</p>
<hr />
<h2><strong>Data</strong>:</h2>
<blockquote>
<p><code>f_data</code> -</p>
</blockquote>
<ul>
<li><p>Dataframe (33481, 2)</p>
</li>
<li><p>Columns = ['Headline','articleBody'].</p>
</li>
<li><p>Created from two other df [<code>x_train</code>(26784, 2),<code>val_train</code>(6697, 2)]</p>
</li>
<li><p><code>f_data[0]</code> returns</p>
<p>['did kim yo jong take kim jong un role north ko...' ,
'san francisco marketwatch north korean leader...']</p>
</li>
</ul>
<p>Here is a snippet of the creation <code>f_data</code>:</p>
<pre><code># This df will be fed into the fit_on_texts()
# Creating df to contain the train and validation set
f_data = pd.DataFrame(columns = ['Headline', 'articleBody'])
# Adding data from x_train to f_data
f_data['Headline'] = x_train['Headline']
f_data['articleBody'] = x_train['articleBody']
# Appending x_val headline and article body columns
f_data = f_data.append(x_val[['Headline', 'articleBody']])
f_data
</code></pre>
<hr />
<h2><strong>Keras/TF code Issue</strong></h2>
<p>Issue:
I am having issues is that when I print out the length of word_index it returns 2:</p>
<pre><code>tokenizer.fit_on_texts(f_data[['Headline', 'articleBody']]
sequences = tokenizer.texts_to_sequences(f_data[['Headline', 'articleBody']])
word_index = tokenizer.word_index
print('Vocab size:', len(word_index))
&gt;&gt; Vocab size: 2

data = pad_sequences(sequences, padding = 'post', maxlen = MAX_SEQ_LEN)
print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', y_train_cat.shape)
</code></pre>
<p>I have tried turning <code>f_data</code> into ndarray but get an attribute error.</p>
<pre><code>f_data_2 = np.array(f_data[['Headline', 'articleBody']]) # ndarray
sequences = tokenizer.texts_to_sequences(apple)
AttributeError: 'numpy.ndarray' object has no attribute 'lower'
</code></pre>
<p>Any suggestions? I have looked at some other questions, but they are dealing with a list of strings</p>
<hr />
<p><strong><s>Solution</s>:</strong>
I think I finally got something to works, but I'm not entirely sure this is correct.</p>
<pre><code>f_data = np.c_[(np.array(f_data['Headline']), np.array(f_data['articleBody']))]
f_data= f_data.tolist()
</code></pre>
<p>....</p>
<pre><code>sequences = tokenizer.texts_to_sequences(f_data)
word_index = tokenizer.word_index
print('Vocab size:', len(word_index))
print(word_index)
&gt;&gt; Vocab size: 3239
&gt;&gt; {...'judicial watch reported isis members crossed mexican border': 12,
   'isis beheads photojournalist james wright foley nasage us end intervention iraq': 13, ...} 
 #(there's 3239 strings)
</code></pre>
<hr />
<p><strong>Update1:</strong></p>
<p>Above is not solution. It seems that my tokenized sentences are only recording two value and the rest are 0:</p>
<pre><code>&gt;Tokenized sentences: 
 [1174  102    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    ....
    0    0    0    0]
 &gt;shape (200,)
</code></pre>
<hr />
<p>edit1:</p>
<pre><code>f_head = np.array(f_data['Headline'].tolist())
f_body = np.array(f_data['articleBody'].tolist())            

#Head Tok, Seq., Pad
toke_head = Tokenizer(num_words=Max_Num_Wrd_Head)
toke_head.fit_on_texts(f_head)

seq_head = toke_head.texts_to_sequences(f_head)
wrd_indx_head = toke_head.word_index

data_Head = pad_sequences(seq_head, padding= 'post', maxlen = Max_Seq_Len_Head)

#Body Tok, Seq., Pad
toke_body = Tokenizer(num_words=MAX_NUM_WRDS_BODY)
toke_body.fit_on_texts(f_body)

seq_body = toke_body.texts_to_sequences(f_body)
wrd_indx_body = toke_body.word_index

data_Body = pad_sequences(seq_body, padding= 'post', maxlen = Max_Num_Wrd_Body)
</code></pre>
",Vectorization & Embeddings,apply tokenizer fit text data frame two column object string need train need pas two set data issue recognizing text returning number suspect issue occurring passing data frame string example looked used imbd data set additional information currently experimenting multi classification problem headline article pair label agree disagree discus unrelated plan use lstm pre trained glove create index word mapped known embedding data dataframe column headline articlebody created two df return kim yo jong take kim jong un role north ko san francisco marketwatch north korean leader snippet creation kera tf code issue issue issue print length word index return tried turning ndarray get attribute error suggestion looked question dealing list string solution think finally got something work entirely sure correct update solution seems tokenized sentence recording two value rest edit
Dropout layer after embedding layer,"<pre><code>model = tf.keras.Sequential([
    tf.keras.layers.Embedding(1000, 16, input_length=20), 
    tf.keras.layers.Dropout(0.2),                           # &lt;- How does the dropout work?
    tf.keras.layers.Conv1D(64, 5, activation='relu'),
    tf.keras.layers.MaxPooling1D(pool_size=4),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
</code></pre>
<p>I can understand when dropout is applied between Dense layers, which randomly drops and prevents the former layer neurons from updating parameters. I don't understand how dropout works after an <code>Embedding layer</code>.</p>
<p>Let's say the output shape of the <code>Embedding layer</code> is <code>(batch_size,20,16)</code> or simply <code>(20,16)</code> if we ignore the batch size. How is dropout applied to the embedding layer's output?</p>
<p>Randomly dropout rows or columns?</p>
",Vectorization & Embeddings,dropout layer embedding layer understand dropout applied dense layer randomly drop prevents former layer neuron updating parameter understand dropout work let say output shape simply ignore batch size dropout applied embedding layer output randomly dropout row column
doc2vec: measurement of performance and &#39;workers&#39; parameter,"<p>I have an awfully large corpora as input to my doc2vec training, around 23mil documents streamed using an iterable function. I was wondering if it were at all possible to see the development of my training progress, possibly through finding out which iteration its currently on, words per second or some similar metric.</p>

<p>I was also wondering how to speed up the performance of doc2vec, other than reducing the size of the corpus. I discovered the <em>workers</em> parameter and I'm currently training on 4 processes; the intuition behind this number was that multiprocessing cannot take advantage of virtual cores. I was wondering if this was the case for the doc2vec <em>workers</em> parameter or if I could use 8 workers instead or even potentially higher (I have a quad-core processor, running Ubuntu).</p>

<p>I have to add that using the unix command <code>top -H</code> reports only around a 15% CPU usage per python process using 8 workers and around 27% CPU usage per process on 4 workers.</p>
",Vectorization & Embeddings,doc vec measurement performance worker parameter awfully large corpus input doc vec training around mil document streamed using iterable function wa wondering possible see development training progress possibly finding iteration currently word per second similar metric wa also wondering speed performance doc vec reducing size corpus discovered worker parameter currently training process intuition behind number wa multiprocessing take advantage virtual core wa wondering wa case doc vec worker parameter could use worker instead even potentially higher quad core processor running ubuntu add using unix command report around cpu usage per python process using worker around cpu usage per process worker
Efficient retrieval of documents represented in the form of multi-dimensional vectors,"<p>I've trained a deep neural network based model for information retrieval. At the end, my model represents the documents in the form of 128 dimensional vectors. Semantic representations of documents similar to word embedding representation for words (word2vec algorithm). When I give a query to my model, it also represents the query in the same 128 dimensional vector space. Now from the entire vector space, I want to retrieve top k documents closest the the query vector represented in the same vector space. </p>

<p>The similarity measure is cosine similarity which is defined as follows : </p>

<pre><code>sim(Q, D) = np.dot(Q.T, D)/(np.linalg.norm(Q) * np.linalg.norm(D))
</code></pre>

<p>where <code>sim(Q, D)</code> represents similarity between query Q and document D. In simple words, it is dot product of unit vectors of query and document. <br>
Now I have roughly 36 million documents, so calculating cosine similarity for all the documents and the sorting them is not a feasible option for efficient retrieval. I want to efficiently search for the most similar k documents for any query vector represented in the same 128 dimensional vector space. </p>
",Vectorization & Embeddings,efficient retrieval document represented form multi dimensional vector trained deep neural network based model information retrieval end model represents document form dimensional vector semantic representation document similar word embedding representation word word vec algorithm give query model also represents query dimensional vector space entire vector space want retrieve top k document closest query vector represented vector space similarity measure cosine similarity defined follows represents similarity query q document simple word dot product unit vector query document roughly million document calculating cosine similarity document sorting feasible option efficient retrieval want efficiently search similar k document query vector represented dimensional vector space
Embedding vector pruning: Obtaining a minimal set of embedding vectors required to describe a class,"<p>I'm using a neural network to classify items based on the output embedding vectors from the network. L2 distance between embedding vectors is used to compute which class an item belongs to. My dataset has given me lots of embedding vectors per class, on the order of 10,000 embedding vectors per class, and I have 10 classes in total. Most of these embedding vectors 'overlap' in multidimensional space and are redundant. <strong>What algorithms can I use to prune the the number of embedding vectors such that I can get a minimum viable set of vectors that fully describes each class?</strong></p>
<p>For example, 5 embedding vectors of a class might be sufficient to describe the class, how can I pick the 5 most optimal and descriptive vectors and prune the remaining 9995 (which mostly overlap with each other and are thus redundant)?</p>
",Vectorization & Embeddings,embedding vector pruning obtaining minimal set embedding vector required describe class using neural network classify item based output embedding vector network l distance embedding vector used compute class item belongs dataset ha given lot embedding vector per class order embedding vector per class class total embedding vector overlap multidimensional space redundant algorithm use prune number embedding vector get minimum viable set vector fully describes class example embedding vector class might sufficient describe class pick optimal descriptive vector prune remaining mostly overlap thus redundant
Does BERT and other language attention model only share cross-word information in the initial embedding stage?,"<p>I study visual attention models but have recently been reading up on BERT and other language attention models to fill a serious gap in my knowledge.</p>
<p>I am a bit confused by what I seem to be seeing in these model architectures. Given a sentence like &quot;the cat chased the dog&quot;. I would have expected cross information streams between the embeddings of each word. For example, I would have expected to see a point in the model where the embedding for &quot;cat&quot; is combined with the embedding for &quot;dog&quot;, in order to create the attention mask.</p>
<p>Instead what I seem to be seeing, (correct me if I am wrong) is that the embedding of a word like &quot;cat&quot; is initially set up to include information about the words around them. So that each embedding of each word includes all of the other words around them. Then each of these embeddings are passed through the model in parallel. This seems weird to me and redundant. Why would they set up the model in this way?</p>
<p>If we were to block out cat. &quot;the ... chased the dog.&quot; Would we then, during inference, only need to send the &quot;...&quot; embedding through the model?</p>
",Vectorization & Embeddings,doe bert language attention model share cross word information initial embedding stage study visual attention model recently reading bert language attention model fill serious gap knowledge bit confused seem seeing model architecture given sentence like cat chased dog would expected cross information stream embeddings word example would expected see point model embedding cat combined embedding dog order create attention mask instead seem seeing correct wrong embedding word like cat initially set include information word around embedding word includes word around embeddings passed model parallel seems weird redundant would set model way block cat chased dog would inference need send embedding model
What should I do when there are multiple inputs in seq2seq model?,"<p>I have three input text sequences, together to generate a target sequence. I want to encode three inputs independently. What is the correct way to do that? Is the following way work?
Each input goes into an Input layer, an Embedding layer and an LSTM layer, then concat them together use bi-LSTM again as the encoder.</p>
",Vectorization & Embeddings,multiple input seq seq model three input text sequence together generate target sequence want encode three input independently correct way following way work input go input layer embedding layer lstm layer concat together use bi lstm encoder
Unsupervised clustering of words in R without knowing k,"<p>As a beginner in NLP, I am trying to find the best way to cluster single words with unsupervised clustering, specifically where the number of clusters <code>k</code> is not known in advance.   I have a group of words that contains clusters of words are very similar to each other (off by one or two letters) - by this I mean cosine similarity (not semantic) -   I would like to be able to find the number of these clusters in the group without defining <code>k</code> in advance.</p>
<p>To take a basic example, I have tried using Levenshtein Distance, which takes the argument <code>k</code> in advance:</p>
<pre><code>#Levenshtein Distance

str = c('foo', 'food', 'fo', 'ten', 'zen')
d  &lt;- adist(str)
rownames(d) &lt;- str
hc &lt;- hclust(as.dist(d))
plot(hc)
rect.hclust(hc,k=2)
</code></pre>
<p>The algorithm performs well, but <code>k</code> is necessary to know.</p>
<p><a href=""https://i.sstatic.net/XhFKml.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XhFKml.png"" alt=""enter image description here"" /></a></p>
<p>Is there a good algorithm for clustering words?  Most of the documentation I've come across uses td-idf and pairwise distances in sentences, but this is a much simpler problem and nothing really addresses just clustering groups of single words without knowing <code>k</code>.  Any suggestions would be appreciated!</p>
",Vectorization & Embeddings,unsupervised clustering word r without knowing k beginner nlp trying find best way cluster single word unsupervised clustering specifically number cluster known advance group word contains cluster word similar one two letter mean cosine similarity semantic would like able find number cluster group without defining advance take basic example tried using levenshtein distance take argument advance algorithm performs well necessary know good algorithm clustering word documentation come across us td idf pairwise distance sentence much simpler problem nothing really address clustering group single word without knowing suggestion would appreciated
Finding the best similarity measure for a group of documents,"<p>As someone new to NLP, I am trying to find a solution to a problem that doesn't seem to be well documented - estimating the degree similarity for a <strong>group</strong> of documents as opposed to a pair of documents.</p>
<p>Say that I have two groups of words <code>a</code> and <code>b</code> , and I want to be able to claim that the words within one group are more similar to each other <em>as a whole</em> than the within the second group.  To use a simple example:</p>
<pre><code>library(stringdist)

a = c('foo', 'bar', 'baz', 'li')
b = c('foo', 'food', 'fo', 'fod')

a = as.data.frame(t(combn(a, 2)))
b= as.data.frame(t(combn(b, 2)))

distances = c()

for (i in 1:nrow(a)){
 cos_dist = stringdist(a$V1[i], a$V2[i], method=&quot;cosine&quot;)
 distances = c(distances, cos_dist)
}

print(mean(a_distances))
[1] 0.8888889

distances = c()

for (i in 1:nrow(b)){
  cos_dist = stringdist(b$V1[i], b$V2[i], method=&quot;cosine&quot;)
  distances = c(distances, cos_dist)
}

print(mean(b_distances))
[1] 0.1230863
</code></pre>
<p>Here, I am using the cosine similarity method (0 = identical, 1 = not similar) applied to all possible pairs of words within a group.</p>
<p>For those who are more experienced in NLP and string distance functions, does it make sense to use mean cosine distances for all pairs of documents as a measure of within-group similarity?</p>
",Vectorization & Embeddings,finding best similarity measure group document someone new nlp trying find solution problem seem well documented estimating degree similarity group document opposed pair document say two group word want able claim word within one group similar whole within second group use simple example using cosine similarity method identical similar applied possible pair word within group experienced nlp string distance function doe make sense use mean cosine distance pair document measure within group similarity
What is happening under the hood of fasttext supervised learning model?,"<p>We can train a supervised model in fasttext using</p>
<pre><code>import fasttext
model = fasttext.train_supervised(input=&quot;cooking.train&quot;)
</code></pre>
<p>My question is how it is representing the features(Bag of words or tf/idf or word embedding) and what algorithm is it using for text classification?</p>
",Vectorization & Embeddings,happening hood fasttext supervised learning model train supervised model fasttext using question representing feature bag word tf idf word embedding algorithm using text classification
Word2Vec/Doc2vec + K-Means clustering - how to extract &quot;semantically meaningful&quot; centroids?,"<p>I have a list of unique terms, say for example:</p>
<pre><code>['Dollars','Cash','International Currency','Credit card','Comics','loans','David Beckham','soccer','Iron Man','checks','Euros','World Cup','Marvel Cinematic Universe','Champions league','Superman'] 
</code></pre>
<p>Ultimately I want to achieve the following mapping:</p>
<pre><code>
['Dollars','Cash','International Currency','Credit card','loans','checks','Euros','World']: 'Money and finance'

['Comics','Iron Man','Marvel Cinematic Universe','Superman']: 'Comics and Superherores'

['David Beckham','soccer','World Cup','Champions league']: 'Soccer, Football'
</code></pre>
<p>My idea is to use a text embedding like word2vec or doc2vec, and then cluster the embeddings using K-Means. So far this is very straightforward. But then I would like to map the resulting embedding centroids to 2 or 3 relevant terms. Is there a way to go from the numerical embedding centroid to semantically meaningful terms?</p>
<p>If there is a better way to do this other than <code>Embedding &gt; Clustering &gt; Extract meaning from centroid</code> I could try that as well.</p>
<p>A couple of things to note about this is that: The terms in my lists are unique - either individual words, compound terms, or vert short sentences, not paragraphs or documents - so frequency or word count based methods are not applicable. And the lists also contain a lot of noise, e.g. &quot;xxx thx u&quot; and &quot;Hello Mr. Johnson&quot;, etc...</p>
<p>So my two asks are:</p>
<ul>
<li>What is the best way to achieve this mapping?</li>
<li>And how can we map a centroid from an embedding space to a small set of meaningful terms?</li>
</ul>
",Vectorization & Embeddings,word vec doc vec k mean clustering extract semantically meaningful centroid list unique term say example ultimately want achieve following mapping idea use text embedding like word vec doc vec cluster embeddings using k mean far straightforward would like map resulting embedding centroid relevant term way go numerical embedding centroid semantically meaningful term better way could try well couple thing note term list unique either individual word compound term vert short sentence paragraph document frequency word count based method applicable list also contain lot noise e g xxx thx u hello mr johnson etc two asks best way achieve mapping map centroid embedding space small set meaningful term
How to train a word embedding representation with gensim fasttext wrapper?,"<p>I would like to train my own word embeddings with fastext. However, after following the tutorial I can not manage to do it properly. So far I tried:</p>

<p>In:</p>

<pre><code>from gensim.models.fasttext import FastText as FT_gensim

# Set file names for train and test data
corpus = df['sentences'].values.tolist()

model_gensim = FT_gensim(size=100)

# build the vocabulary
model_gensim.build_vocab(sentences=corpus)
model_gensim
</code></pre>

<p>Out:</p>

<pre><code>&lt;gensim.models.fasttext.FastText at 0x7f6087cc70f0&gt;
</code></pre>

<p>In:</p>

<pre><code># train the model
model_gensim.train(
    sentences = corpus, 
    epochs = model_gensim.epochs,
    total_examples = model_gensim.corpus_count, 
    total_words = model_gensim.corpus_total_words
)

print(model_gensim)
</code></pre>

<p>Out:</p>

<pre><code>FastText(vocab=107, size=100, alpha=0.025)
</code></pre>

<p>However, when I try to look in a vocabulary words:</p>

<pre><code>print('return' in model_gensim.wv.vocab)
</code></pre>

<p>I get <code>False</code>, even the word is present in the sentences I am passing to the fast text model. Also, when I check the most similar words to return I am getting characters:</p>

<pre><code>model_gensim.most_similar(""return"")

[('R', 0.15871645510196686),
 ('2', 0.08545402437448502),
 ('i', 0.08142799884080887),
 ('b', 0.07969795912504196),
 ('a', 0.05666942521929741),
 ('w', 0.03705815598368645),
 ('c', 0.032348938286304474),
 ('y', 0.0319858118891716),
 ('o', 0.027745068073272705),
 ('p', 0.026891689747571945)]
</code></pre>

<p>What is the correct way of using gensim's fasttext wrapper?</p>
",Vectorization & Embeddings,train word embedding representation gensim fasttext wrapper would like train word embeddings fastext however following tutorial manage properly far tried however try look vocabulary word get even word present sentence passing fast text model also check similar word return getting character correct way using gensim fasttext wrapper
Positional Embedding in the Transformer model - does it change the word&#39;s meaning?,"<p>I am reading the Transformer paper, and the Positional Embeddings make me wonder a thing:</p>
<p>Assume that the word &quot;cat&quot; is pretrained to be embedded to the word vector <code>[2,3,1,4]</code>. If we use the positional encoding that turns the vector into a new one, like <code>[3,1,5,2]</code>, should not it change also the word's meaning in the word2vec matrix? Since the corpus is large, a slight change in the value can also change its meaning.</p>
",Vectorization & Embeddings,positional embedding transformer model doe change word meaning reading transformer paper positional embeddings make wonder thing assume word cat pretrained embedded word vector use positional encoding turn vector new one like change also word meaning word vec matrix since corpus large slight change value also change meaning
How is the semantic similarity score calculated in STS Benchmark dataset?,"<p>This is the GitHub repo : <a href=""https://github.com/brmson/dataset-sts"" rel=""nofollow noreferrer"">https://github.com/brmson/dataset-sts</a></p>

<p>The STS Benchmark dataset contains about 4000 pairs of similar and dissimilar  sentences along with their semantic similarity scores.</p>

<p>Task that I'm trying to do: 
I have another custom dataset which also has pairs of similar and dissimilar sentences. ( with just 200 pairs) </p>

<p>I want to combine these two datasets (STS &amp; my custom dataset) and use that for fine tuning a Bert model. 
(Bert sentence transformer: <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a>)</p>

<p>But the model needs the semantic similarity score of all the pairs of sentences. How do I compute that score for the sentences that I have in my custom dataset? </p>

<p>It has to be computed in the same way as how it was computed for the pairs of sentences in the STS Benchmark dataset.</p>

<p>This thread is very similar but it didn't quite answer the question that I am looking for : <a href=""https://stackoverflow.com/questions/59172532/bert-fine-tuned-for-semantic-similarity"">Bert fine-tuned for semantic similarity</a></p>
",Vectorization & Embeddings,semantic similarity score calculated sts benchmark dataset github repo sts benchmark dataset contains pair similar dissimilar sentence along semantic similarity score task trying another custom dataset also ha pair similar dissimilar sentence pair want combine two datasets sts custom dataset use fine tuning bert model bert sentence transformer model need semantic similarity score pair sentence compute score sentence custom dataset ha computed way wa computed pair sentence sts benchmark dataset thread similar quite answer question looking href fine tuned semantic similarity
Unsupervised finetuning of BERT for embeddings only?,"<p>I would like to fine-tuning BERT for a specific domain on unlabeled data and get the output layer to check the similarity between them. How can I do it? Do I need to fine-tuning first a classifier task (or question answer, etc..) and get the embeddings? Or can I just use a pre-trained Bert model without task and fine-tuning with my own data?</p>
",Vectorization & Embeddings,unsupervised finetuning bert embeddings would like fine tuning bert specific domain unlabeled data get output layer check similarity need fine tuning first classifier task question answer etc get embeddings use pre trained bert model without task fine tuning data
Accuracy of fine-tuning BERT varied significantly based on epochs for intent classification task,"<p>I used <code>Bert base uncased</code> as embedding and doing simple cosine similarity for intent classification in my dataset (around <code>400 classes and 2200 utterances, train:test=80:20</code>). The base BERT model performs 60% accuracy in the test dataset, but different epochs of fine-tuning gave me quite unpredictable results.</p>

<p>This is my setting:</p>

<pre><code>max_seq_length=150
train_batch_size=16
learning_rate=2e-5
</code></pre>

<p>These are my experiments:</p>

<pre><code>base model   accuracy=0.61
epochs=2.0   accuracy=0.30
epochs=5.0   accuracy=0.26
epochs=10.0  accuracy=0.15
epochs=50.0  accuracy=0.20
epochs=75.0  accuracy=0.92
epochs=100.0 accuracy=0.93
</code></pre>

<p>I don't understand while it behaved like this. I expect that any epochs of fine-tuning shouldn't be worse than the base model because I fine-tuned and inferred on the same dataset. Is there anything I misunderstand or should care about?</p>
",Vectorization & Embeddings,accuracy fine tuning bert varied significantly based epoch intent classification task used embedding simple cosine similarity intent classification dataset around base bert model performs accuracy test dataset different epoch fine tuning gave quite unpredictable result setting experiment understand behaved like expect epoch fine tuning worse base model fine tuned inferred dataset anything misunderstand care
Dependency parsing visualisation,"<p>How can I represent the following sentence:</p>
<pre><code>txt1=&quot;The chef cooks the soup&quot;
</code></pre>
<p>as follows:</p>
<p><a href=""https://i.sstatic.net/XUYEe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XUYEe.png"" alt=""enter image description here"" /></a></p>
<p>I would like to visualise the sentence like shown in the image, i.e. as a tree/network.
Any advice would be greatly appreciate. I am using nltk for word embedding.</p>
",Vectorization & Embeddings,dependency parsing visualisation represent following sentence follows would like visualise sentence like shown image e tree network advice would greatly appreciate using nltk word embedding
Sentence meaning similarity in python,"<p>I want to calculate the sentence meaning similarity. I am using cosine similarity but this method does not fulfill my needs.</p>
<p>For example, if I have these two sentences.</p>
<ol>
<li>He and his father are very close.</li>
<li>He shares a wonderful bond with his father.</li>
</ol>
<p>What I need is calculating the similarity between these sentences based on the meaning similarity and not just matching similar words</p>
<p>Is there a way to do this?</p>
",Vectorization & Embeddings,sentence meaning similarity python want calculate sentence meaning similarity using cosine similarity method doe fulfill need example two sentence father close share wonderful bond father need calculating similarity sentence based meaning similarity matching similar word way
Build a multiclass text classifier which takes vectors generated from word2vec as independent variables to predict a class,"<p>I am dealing with patient data. I want to predict the top N diseases given a set of symptoms.</p>
<p>This is a sample of my dataset: In total I have around 1200 unique Symptoms and around 200 unique Diagnosis</p>
<pre><code>     ID         Symptom combination                              Diagnosis
    Patient1: fever, loss of appetite, cold                        Flu
    Patient2: hair loss, blood pressure                           Thyroid
    Patient3: hair loss, blood pressure                            Flu
    Patient4: throat pain, joint pain                           Viral Fever

    ..
    ..
Patient30000: vomiting, nausea                                   Diarrohea
</code></pre>
<p>What I am planning to do with this dataset is to use the Symptoms column to generate word vectors using Word2vec for each row of patient data. After generating the vectors I want to build a classifier, with the vectors in each row being my independent variable and the Diagnosis being the target categorical variable.</p>
<p>Shall I take the average of the vectors to generate feature vectors generated from word2vec? If so, any clarifications on the same?</p>
",Vectorization & Embeddings,build multiclass text classifier take vector generated word vec independent variable predict class dealing patient data want predict top n disease given set symptom sample dataset total around unique symptom around unique diagnosis planning dataset use symptom column generate word vector using word vec row patient data generating vector want build classifier vector row independent variable diagnosis target categorical variable shall take average vector generate feature vector generated word vec clarification
How to interpret output from gensim&#39;s Word2vec most similar method and understand how it&#39;s coming up with the output values,"<p>I am trying to implement word2vec on a problem. I will briefly explain my problem statement:</p>
<p>I am dealing with clinical data. I want to predict the top N diseases given a set of symptoms.</p>
<pre><code>Patient1: ['fever', 'loss of appetite', 'cold', '#flu#']
Patient2: ['hair loss', 'blood pressure', '#thyroid']
Patient3: ['hair loss', 'blood pressure', '#flu]
..
..
Patient30000: ['vomiting', 'nausea', '#diarrohea']
</code></pre>
<p>Note:
1.words with #prefix are diagnosis and the rest are symptoms</p>
<ol start=""2"">
<li>My corpus doesn't have any sentences or paragraphs. It just contains symptom names and diagnosis for a patient</li>
</ol>
<p>Applying word2vec on this corpus, I am able to generate the top 10 diagnosis given a set of input symptoms. Now, I want to understand how that output is generated. I know it's cosine similarity by adding the input vectors but I am unable to validate this output. Or understand how to improve this.  Really want to understand what exactly is going on in the background which leads to these output.</p>
<p>Can anyone help me answer these questions or highlight what are the drawbacks/advantages of this approach</p>
",Vectorization & Embeddings,interpret output gensim word vec similar method understand coming output value trying implement word vec problem briefly explain problem statement dealing clinical data want predict top n disease given set symptom note word prefix diagnosis rest symptom corpus sentence paragraph contains symptom name diagnosis patient applying word vec corpus able generate top diagnosis given set input symptom want understand output generated know cosine similarity adding input vector unable validate output understand improve really want understand exactly going background lead output anyone help answer question highlight drawback advantage approach
Building a recommendation system using word2vec,"<p>Now, from the outputs, I am unable to understand the embeddings or how these embeddings are changing or will change with new data.</p>
<p>Is this way correct for solving the problem statement and if so how can I optimize this to find the best embeddings for my dataset. Can anyone provide any suggestions on the same</p>
",Vectorization & Embeddings,building recommendation system using word vec output unable understand embeddings embeddings changing change new data way correct solving problem statement optimize find best embeddings dataset anyone provide suggestion
Similarity between two words with pre-trained NLTK wordnet,"<p>I want to compare two words with similarity score.
I used wordnet from nltk.corpus.</p>
<pre><code>from nltk.corpus import wordnet
nltk.download('wordnet')    
w1 = wordnet.synset(&quot;price&quot; '.n.01')  #wordnet.lemmas(i2)[0]
w2 = wordnet.synset(&quot;amount&quot; + '.n.01') 
print(w1.wup_similarity(w2))
</code></pre>
<p>I got similarity score, but, it works with only between noun, but, what I need is to compare noun between adjective or other type of word.</p>
<p>For example bellow , I need to compare word like &quot;expensive&quot; (adjective) with &quot;price&quot;.</p>
<p>I want preferably a library with pre-entrained model because I need a model that can work with any words in any domain</p>
<p>What about word embedding ?</p>
",Vectorization & Embeddings,similarity two word pre trained nltk wordnet want compare two word similarity score used wordnet nltk corpus got similarity score work noun need compare noun adjective type word example bellow need compare word like expensive adjective price want preferably library pre entrained model need model work word domain word embedding
"python code to compare the pairs of sentences, and see if they are in the same or different blocks in text file","<p>I've 2 text files. Writing a Python Program for the following
1)I need to compare the pairs of sentences in the 1st file, and see if they are in the same or different blocks, and compare that to the 2nd text file.
2)I need to calculate the percentage of correct classification.
3) I need to  count:
% of sentence pairs correctly classified as in the same block, % of sentence pairs correctly classified as in different blocks</p>
",Vectorization & Embeddings,python code compare pair sentence see different block text file text file writing python program following need compare pair sentence st file see different block compare nd text file need calculate percentage correct classification need count sentence pair correctly classified block sentence pair correctly classified different block
Word/Phrase classification,"<p>I have a column containing 5000 string records. These records are individual words or phrases (not a sentence or paragraph). Most of these records are similar or contain similar elements(e.g. &quot;Office&quot;, &quot;offise&quot; &quot;ground floor office&quot;). Also, someone manually classified 300 of these records into five categories (i.e. Residential, Industrial, Office, Retail, Other) which means I can use it to develop a supervised machine learning model. I did a bit of study on word2vec, but it seems they work on texts, not individual words and phrases. Please advise me on how I can do the classification. Please note that the number of the records in the column is growing and new records will be added in the future, so the solution must be able to classify new records.</p>
<p>The sample input and the desired output is as below:</p>
<pre><code>'industrial' -&gt; 'Industrial'
'Warehouse' -&gt; 'Industrial'
'Workshop' -&gt; 'Industrial'
'rear warehouse' -&gt; 'Industrial'
'office suite' -&gt; 'office'
'office/warehouse' -&gt; 'office'
'office(b1)' -&gt; 'office'
'house' -&gt; 'Residential'
'suite' -&gt; 'Residential'
'restaurant' -&gt; 'Retail'
'retail unit with 3 bedroom dwelling above' -&gt; 'Retail'
'shoe shop' -&gt; 'Retail'
'unit 56' -&gt; 'Other'
'24 Hastings street' -&gt; 'Other'
</code></pre>
<p><a href=""https://i.sstatic.net/jsT8s.png"" rel=""nofollow noreferrer"">Input &amp; Output</a></p>
",Vectorization & Embeddings,word phrase classification column containing string record record individual word phrase sentence paragraph record similar contain similar element e g office offise ground floor office also someone manually classified record five category e residential industrial office retail mean use develop supervised machine learning model bit study word vec seems work text individual word phrase please advise classification please note number record column growing new record added future solution must able classify new record sample input desired output input output
Implementation of TextRank algorithm using Spark(Calculating cosine similarity matrix using spark),"<p>I am trying to implement textrank algorithm where I am calculating cosine-similarity matrix for all the sentences.I want to parallelize the task of similarity matrix creation using Spark but don't know how to implement it.Here is the code:</p>
<pre><code>    cluster_summary_dict = {}
    for cluster,sentences in tqdm(cluster_wise_sen.items()):
        sen_sim_matrix = np.zeros([len(sentences),len(sentences)])
        for row in range(len(sentences)):
            for col in range(len(sentences)):
                if row != col:
                    sen_sim_matrix[row][col] = cosine_similarity(cluster_dict[cluster]  
                                               [row].reshape(1,100), cluster_dict[cluster] 
                                               [col].reshape(1,100))[0,0]
        sentence_graph = nx.from_numpy_array(sen_sim_matrix)
        scores = nx.pagerank(sentence_graph) 
        pagerank_sentences = sorted(((scores[k],sent) for k,sent in enumerate(sentences)), 
                             reverse=True)
        cluster_summary_dict[cluster] = pagerank_sentences
</code></pre>
<p>Here,cluster_wise_sen is a dictionary that contains list of sentences for different clusters ({'cluster 1' : [list of sentences] ,...., 'cluster n' : [list of sentences]}). cluster_dict contains the 100d vector representation of the sentences. I have to compute the sentence similarity matrix for each cluster. Since it is time consuming, therefore looking to parallelize it using spark.</p>
",Vectorization & Embeddings,implementation textrank algorithm using spark calculating cosine similarity matrix using spark trying implement textrank algorithm calculating cosine similarity matrix sentence want parallelize task similarity matrix creation using spark know implement code cluster wise sen dictionary contains list sentence different cluster cluster list sentence cluster n list sentence cluster dict contains vector representation sentence compute sentence similarity matrix cluster since time consuming therefore looking parallelize using spark
How to solve difficult sentences for nlp sentiment analysis,"<p>Such as the following sentence,
&quot;Don't pay attention to people if they say it's no good.&quot;
As humans, we understand the overall sentiment from the sentence is positive.</p>
<ol>
<li><p>Technique of &quot;Bag of Words&quot; or BOW
Then, we have the two categories of &quot;positive&quot; words as Polarity of 1, &quot;negative&quot; words of Polarity of 0.
In this case, the word of &quot;good&quot; fits into category, but here it is accidentally correct.
Thus, this technique is eliminated.</p>
</li>
<li><p>Still use BOW technique (sort of &quot;Word Embedding&quot;)
But take into consideration of its surrounding words, in this case, the &quot;no&quot; word preceding it, thus, it's &quot;no good&quot;, not the adj alone &quot;good&quot;.  However, &quot;no good&quot; is not what the author intended from the context of the entire sentence.</p>
</li>
</ol>
<p>Thus, this question.  Thanks in advance.</p>
",Vectorization & Embeddings,solve difficult sentence nlp sentiment analysis following sentence pay attention people say good human understand overall sentiment sentence positive technique bag word bow two category positive word polarity negative word polarity case word good fit category accidentally correct thus technique eliminated still use bow technique sort word embedding take consideration surrounding word case word preceding thus good adj alone good however good author intended context entire sentence thus question thanks advance
Gensim training meaningless word embeddings,"<p>The word embeddings when training with Gensim are meaningless: the similarities on known relationships (King - Man + Woman -&gt; Queen) and others simply don't hold (not even close), they might as well be random.</p>
<p>Using the same training data and equivalent parameters, I get meaningful results with Facebook's FastText. In comparison, I've tried the Gensim FastText and Word2Vec classes, both returning meaningless embeddings.</p>
<p>To narrow things down, here are some of the settings:</p>
<ul>
<li>30GB training data text file. Space separated tokens, newline separated sentences.</li>
<li>Final vocab size after training: 12M+ word vectors</li>
<li>Dimension = 100</li>
<li>Disabled subwords</li>
<li>Disabled n-grams</li>
<li>Negative sampling = 8</li>
<li>Skip-gram</li>
<li>Min-count = 20</li>
</ul>
<p>Training code below:</p>
<pre class=""lang-py prettyprint-override""><code>print(&quot;Setting Model&quot;)
model = FastText(
# model = Word2Vec(
  sg=1,
  size=dim, 
  max_vocab_size=max_vocab_size,
  window=10, 
  negative=8,
  min_count=20,
  max_n=0,
  min_n=0,
  workers=16,
  alpha=0.1,
)
print(&quot;Building vocab&quot;)
model.build_vocab(
  corpus_file=corpus_file,
  progress_per=1_000_000
)
print(&quot;Training model&quot;)
model.train(
  corpus_file=corpus_file,
  total_words=model.corpus_total_words,
  total_examples=model.corpus_count,
  epochs=5,
  word_ngrams=0,
)
print(&quot;Saving model&quot;)
model.save(model_name)
</code></pre>
<p>I would rather avoid using Facebook's FastText as it would require me changing its source code to achieve this vocab size.</p>
<p>What could be going wrong, or how can I debug this? I have been able to import Facebook's FastText vectors into Gensim and it works fine.</p>
",Vectorization & Embeddings,gensim training meaningless word embeddings word embeddings training gensim meaningless similarity known relationship king man woman queen others simply hold even close might well random using training data equivalent parameter get meaningful result facebook fasttext comparison tried gensim fasttext word vec class returning meaningless embeddings narrow thing setting gb training data text file space separated token newline separated sentence final vocab size training word vector dimension disabled subwords disabled n gram negative sampling skip gram min count training code would rather avoid using facebook fasttext would require changing source code achieve vocab size could going wrong debug able import facebook fasttext vector gensim work fine
Will the document vectors generated by Doc2Vec be similar to document vectors obtained through Word2Vec?,"<p>I came across few blog posts stating that, Document vectors can be generated not only by Doc2Vec, but also by averaging the word vectors obtained by running Word2vec algorithm.
In that case, would the vectors generated through both the Algorithms be the same?
Which would be the most efficient way to generate the Document vectors and Why?</p>
<p>Any reference links in this regard would be of great help!!</p>
<p>Thanks in Advance</p>
",Vectorization & Embeddings,document vector generated doc vec similar document vector obtained word vec came across blog post stating document vector generated doc vec also averaging word vector obtained running word vec algorithm case would vector generated algorithm would efficient way generate document vector reference link regard would great help thanks advance
How do i get word2vec similarity from the mean vector?,"<p>For example, there are words for 'apple', 'banana', and 'orange'.</p>
<p>We will execute the code below to save the distance between apple and banana.</p>
<pre><code>model.similarity('apple', 'banana')
</code></pre>
<p>But what I want to know is the similarity between 'apple' and 'whole fruits'.
How do i get the similarity of apples and whole fruits?</p>
<p>I already got vectors for the whole fruit.
e.g. <code>whole fruits=[0, 0.4, 0.2, 0.2, 0.5, .....]</code></p>
",Vectorization & Embeddings,get word vec similarity mean vector example word apple banana orange execute code save distance apple banana want know similarity apple whole fruit get similarity apple whole fruit already got vector whole fruit e g
PCA on word2vec embeddings using pre existing model,"<p>I have a word2vec model trained on Tweets. I also have a list of words, and I need to get the embeddings from the words, compute the first two principal components, and plot each word on a 2 dimensional space.</p>
<p>I'm trying to follow tutorials such as this one: <a href=""https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"" rel=""nofollow noreferrer"">https://machinelearningmastery.com/develop-word-embeddings-python-gensim/</a></p>
<p>However in all such tutorials, they create a model based on a random sentence they use and then calculate PCA on all the words in the model. I don't want to do that, I only want to calculate and plot specific words. How can I use the model that I already have, which has thousands of words, and compute the first two principal components for a set list of words I have (around 20)?</p>
<p>So like in the link above, they have &quot;model&quot; with only the words from the sentence they wrote. And then they do &quot;X = model[model.wv.vocab]&quot;, then &quot;pca.fit_transform(X)&quot;. If I were to copy this code, I would do a PCA on the huge model, which I don't want to do. I just want to extract the embeddings of some words from that model and then compute PCA on those few words. Hopefully this makes sense, thanks in advance. Please let me know if I need to clarify anything.</p>
",Vectorization & Embeddings,pca word vec embeddings using pre existing model word vec model trained tweet also list word need get embeddings word compute first two principal component plot word dimensional space trying follow tutorial one however tutorial create model based random sentence use calculate pca word model want want calculate plot specific word use model already ha thousand word compute first two principal component set list word around like link model word sentence wrote x model model wv vocab pca fit transform x copy code would pca huge model want want extract embeddings word model compute pca word hopefully make sense thanks advance please let know need clarify anything
Order/context-aware document / sentence to vectors in Spacy,"<p>I would like to do some supervised binary classification tasks with sentences, and have been using spaCy because of its ease of use. I used spaCy to convert the text into vectors, and then fed the vectors to a machine learning model (e.g. XGBoost) to perform the classfication. However, the results have not been very satisfactory.</p>

<p>In spaCy, it is easy to load a <a href=""https://spacy.io/models/en-starters"" rel=""nofollow noreferrer"">model</a> (e.g. BERT / Roberta / XLNet) to convert words / sentences to nlp objects. Directly calling the vector of the object will however will <a href=""http://Doc.vector%20and%20Span.vector%20will%20default%20to%20an%20average%20of%20their%20token%20vectors."" rel=""nofollow noreferrer"">default</a> to an average of the token vectors. </p>

<p>Here are two questions:</p>

<p>1) Can we do better than simply getting the average of token vectors, like having context/order-aware sentence vectors using spaCy? For example, can we extract the sentence embedding from the previous layer of the BERT transformer instead of the final token vectors in spaCy?</p>

<p>2) Would it be better to directly use spaCy to train the downstream binary classification task? For example, here <a href=""https://spacy.io/usage/training"" rel=""nofollow noreferrer"">discusses</a> how to add a text classifier to a spaCy model. Or is it generally better to apply more powerful machine learning models like XGBoost?</p>

<p>Thanks in advance!</p>
",Vectorization & Embeddings,order context aware document sentence vector spacy would like supervised binary classification task sentence using spacy ease use used spacy convert text vector fed vector machine learning model e g xgboost perform classfication however result satisfactory spacy easy load model e g bert roberta xlnet convert word sentence nlp object directly calling vector object however default average token vector two question better simply getting average token vector like context order aware sentence vector using spacy example extract sentence embedding previous layer bert transformer instead final token vector spacy would better directly use spacy train downstream binary classification task example discus add text classifier spacy model generally better apply powerful machine learning model like xgboost thanks advance
List all sentences containing specific word and similar words with word2vec,"<p>I have a table as following:</p>
<pre><code>data = {'text':  ['The scent is nice','I like the smell', 'The smell is awesome', 'I find the scent amazing', 'I love the smell']}

df = pd.DataFrame (data, columns = ['text'])
</code></pre>
<p>I want to list all sentences that contain the word &quot;smell&quot;</p>
<pre><code>word = 'smell'
selected_list = []
for i in range(0, len(df)):
    if word in df.iloc[i,0]:
        selected_list.append(df.iloc[i,0])
selected_list
</code></pre>
<p>The output that I get is:</p>
<pre><code>['I like the smell', 'The smell is awesome', 'I love the smell']
</code></pre>
<p>However, I want to list also sentences that contain a similar word to &quot;smell&quot; such as &quot;scent&quot; and I want to use the pre-trained word2vec of Google and set up a condition, if the similarity is above 0.5 to list the sentence as well. Therefore, the desired output is:</p>
<pre><code>['The scent is nice', 'I like the smell', 'The smell is awesome', 'I find the scent amazing','I love the smell']
</code></pre>
<p>How can I add word2vec to the above code so that it scans not only for <code>&quot;smell&quot;</code> but also all similar words?</p>
",Vectorization & Embeddings,list sentence containing specific word similar word word vec table following want list sentence contain word smell output get however want list also sentence contain similar word smell scent want use pre trained word vec google set condition similarity list sentence well therefore desired output add word vec code scan also similar word
What do negative vectors mean on word2vec?,"<p>I am doing a research on travel reviews and used word2vec to analyze the reviews. However, when I showed my output to my adviser, he said that I have a lot of words with negative vector values and that only words with positive values are considered logical.</p>
<p>What could these negative values mean? Is there a way to ensure that all vector values I will get in my analysis would be positive?</p>
",Vectorization & Embeddings,negative vector mean word vec research travel review used word vec analyze review however showed output adviser said lot word negative vector value word positive value considered logical could negative value mean way ensure vector value get analysis would positive
Error in the conversion of Bidirectional LSTM Text Classification Model to TFLite Model,"<p>My model is trained on the &quot;imdb reviews dataset&quot; and works fine when predicting the sentiment of movie reviews. However, when I convert my model for Tensorflow Lite, it outputs:
None is only supported in the 1st dimension. Tensor 'embedding <em>1</em> input' has invalid shape '[None, None]'.
When training my model, I did not specify a specific shape, therefore I am unsure of what shape to pass for my model to work with my android app. (As long as I convert the embedding_input shape to something else, the TFLite model will be created, but does not work with my android app)</p>
<p>Code for model:</p>
<pre><code>from tensorflow import keras
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model, Sequential
from keras.layers import Convolution1D
from keras import initializers, regularizers, constraints, optimizers, layers

max_features = 6000
tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(df['Processed_Reviews'])
list_tokenized_train = tokenizer.texts_to_sequences(df['Processed_Reviews'])

maxlen = 130
X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)
y = df['sentiment']

embed_size = 128
model = Sequential()
model.add(Embedding(max_features, embed_size))
model.add(Bidirectional(LSTM(32, return_sequences = True)))
model.add(GlobalMaxPool1D())
model.add(Dense(20, activation=&quot;relu&quot;))
model.add(Dropout(0.05))
model.add(Dense(1, activation=&quot;sigmoid&quot;))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

batch_size = 100
epochs = 3
model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)

#Conversion Code
import tensorflow as tf

inference_model = tf.keras.models.load_model('imdb-reviews-final.h5')
#inference_model.input.set_shape((6000, 128)) --&gt; Reshaping allows model conversion to happen, but does not actually work with the app
converter = tf.lite.TFLiteConverter.from_keras_model(inference_model)
tflite_model = converter.convert()
open(&quot;model.tflite&quot;, &quot;wb&quot;).write(tflite_model)

</code></pre>
",Vectorization & Embeddings,error conversion bidirectional lstm text classification model tflite model model trained imdb review dataset work fine predicting sentiment movie review however convert model tensorflow lite output none supported st dimension tensor embedding input ha invalid shape none none training model specify specific shape therefore unsure shape pas model work android app long convert embedding input shape something else tflite model created doe work android app code model
Cosine similarity with synonyms,"<p>I have some words which are synonyms that I would like to consider similar to the original word. For instance, word <code>restaurant</code> and <code>bar</code> are considered synonyms in this example.</p>
<p>To apply cosine similarity under this scenario, I decided to keep the same word in both vectors, but if one word is considered synonym then I subtract a &quot;penalty&quot; to the counter. In this scenario, I have to compare the original v1=['cafe'] against v2=['restaurant']. Then, I have the following:</p>
<pre><code>v1=Counter({'cafe': 1})
v2=Counter({'cafe': 0.65}) #0.65 because word restaurant is synonym
</code></pre>
<p>However, if I apply this strategy, I ended up with similarity 1.0 (0.65/0.65). I need to get a similarity below 1.0 because restaurant is not considered the same word, but is synonym.</p>
<p>I implemented cosine similarity in the following way:</p>
<pre><code>from collections import Counter

def get_cosine(vec1, vec2):
    intersection = set(vec1.keys()) &amp; set(vec2.keys())
    numerator = sum([vec1[x] * vec2[x] for x in intersection])
    
    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])
    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])
    
    denominator = math.sqrt(sum1) * math.sqrt(sum2)

    if not denominator:
        return 0.0
    else:
        return float(numerator) / denominator
   
v1=Counter({'cafe': 1})
v2=Counter({'cafe': 0.65})
print(get_cosine(v1, v2))

</code></pre>
<p>How can I get similarity in synonyms? while keeping the control on which words are considered synonyms. Currently, I am getting those synonyms from database.</p>
",Vectorization & Embeddings,cosine similarity synonym word synonym would like consider similar original word instance word considered synonym example apply cosine similarity scenario decided keep word vector one word considered synonym subtract penalty counter scenario compare original v cafe v restaurant following however apply strategy ended similarity need get similarity restaurant considered word synonym implemented cosine similarity following way get similarity synonym keeping control word considered synonym currently getting synonym database
pytorch KLDivLoss loss is negative,"<p>my target is training a span prediction model</p>
<p>which can predict the position in the BERT output sequence</p>
<p>my input's shape is (batch_size, max_sequence_len(512),embedding_size(768))</p>
<p>output's shape will be (batch_size , max_sequence_len , 1) and the third dim is stand for kind a probability , then I will reshape output to (batch_size,max_sequence_len)</p>
<p>my label's shape is (batch_size , max_sequence_len), and in the max_sequence_len(512), only one position will be 1 and the others will be zero</p>
<p>and I've already check this</p>
<pre><code>(batch_size is 2)
start_pos_labels.sum(dim=1)
output &gt;&gt; 
tensor([1.0000, 1.0000], device='cuda:0', dtype=torch.float64)

start_pred.sum(dim=1)
tensor([1., 1.], device='cuda:0', dtype=torch.float64, grad_fn=&lt;SumBackward1&gt;)
</code></pre>
<p>but when I use nn.KLDivLoss() , output still be negative, I really dont know why</p>
<p>can somebody help me? thanks!</p>
<p>Here is my code
Model Code</p>
<pre><code>class posClassfication_new(nn.Module):
    def __init__(self):
        super(posClassfication_new, self).__init__()
        self.start_task = nn.Sequential(
            nn.Linear(768, 1),
#             nn.ReLU(),
#             nn.Linear(256, 128),
#             nn.ReLU(),
#             nn.Linear(128, 1)
        )    
        self.end_task = nn.Sequential(
            nn.Linear(768, 1),
#             nn.ReLU(),
#             nn.Linear(256, 128),
#             nn.ReLU(),
#             nn.Linear(128, 1)
        ) 
#             
    def forward(self, start_x,end_x):
        start_x = start_x.double()
        end_x = end_x.double()
        
        start_out = self.start_task(start_x)
        end_out = self.end_task(end_x)
        
        return start_out,end_out
</code></pre>
<p>training code</p>
<pre><code>BATCH_SIZE = 8
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
print(&quot;device:&quot;, device)

class PosTrainDataset(Dataset):
    def __init__(self, x, start_y,end_y):
        self.x = x
        self.start_y = start_y
        self.end_y = end_y

    def __getitem__(self,idx):
        x = self.x[idx]
        start_y = self.start_y[idx]
        end_y = self.end_y[idx]
        return x, start_y, end_y
    
    def __len__(self):
        return len(self.x)
    
trainset = PosTrainDataset(pos_train_x , start_pos_labels_train , end_pos_labels_train)
trainloader = DataLoader(trainset, batch_size=BATCH_SIZE)

pos_model = posClassfication_new()
pos_model = pos_model.to(device)
pos_model = pos_model.double()
pos_model.train()

pos_loss = nn.KLDivLoss()
# pos_loss = nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(pos_model.parameters(), lr=1e-5)

EPOCHS = 5
for epoch in range(EPOCHS):
    running_loss = 0.0
    for data in trainloader:
        x, start_pos_labels, end_pos_labels = [t.to(device) for t in data]
        mini_batch = x.size()[0]
        optimizer.zero_grad()
        
        
        start_pred , end_pred = pos_model(x,x)
        
        start_pred = start_pred.reshape((mini_batch,512))
        end_pred = end_pred.reshape((mini_batch,512))


        start_pred = torch.nn.functional.softmax(start_pred,dim=1)
        end_pred = torch.nn.functional.softmax(end_pred,dim=1)
        
        start_pos_labels = start_pos_labels + 0.0001
        start_pos_labels = torch.nn.functional.softmax(start_pos_labels,dim=1)
        
        end_pos_labels = end_pos_labels  + 0.0001
        end_pos_labels = torch.nn.functional.softmax(end_pos_labels,dim=1)
        
#         start_pos_labels = torch.argmax(start_pos_labels,dim=1)
#         end_pos_labels = torch.argmax(end_pos_labels,dim=1)

        start_loss = pos_loss(start_pred,start_pos_labels)
        end_loss = pos_loss(end_pred,end_pos_labels)
        
        
        loss = start_loss + end_loss
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        
    torch.save(pos_model,'pos_model_single_task.pkl')
    print('[epoch %d] loss: %.3f' %(epoch + 1, running_loss))
</code></pre>
",Vectorization & Embeddings,pytorch kldivloss loss negative target training span prediction model predict position bert output sequence input shape batch size max sequence len embedding size output shape batch size max sequence len third dim stand kind probability reshape output batch size max sequence len label shape batch size max sequence len max sequence len one position others zero already check use nn kldivloss output still negative really dont know somebody help thanks code model code training code
similarity score is way off using doc2vec embedding,"<p>I'm trying out document de-duplication on an <em>NY-Times</em> corpus that I've prepared very recently. It contains data related to financial fraud.</p>
<p>First, I convert the article snippets to a list of <code>TaggedDocument</code> objects.</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_sm&quot;)

def create_tagged_doc(doc, nlp):        
    toks = nlp(doc)
    lemmatized_toks = [tok.lemma_ for tok in toks if not tok.is_stop]
    return lemmatized_toks

df_fraud = pd.read_csv('...local_path...')
df_fraud_list = df_fraud['snippet'].to_list()
documents = [TaggedDocument(create_tagged_doc(doc, nlp), [i]) for i, doc in enumerate(df_fraud_list)]
</code></pre>
<p>A sample <code>TaggedDocument</code> looks as follows:</p>
<pre><code>TaggedDocument(words=['Chicago', 'woman', 'fall', 'mortgage', 'payment', 
'victim', 'common', 'fraud', 'know', 'equity', 'strip', '.'], tags=[1])
</code></pre>
<p>Now I compile and train the Doc2Vec model.</p>
<pre><code>cores = multiprocessing.cpu_count()
model_dbow = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample = 0, workers=cores)
model_dbow.build_vocab(documents)
model_dbow.train(documents, 
                total_examples=model_dbow.corpus_count, 
                epochs=model_dbow.epochs)
</code></pre>
<p>Let's define the cosine similarity:</p>
<pre><code>cosine_sim = lambda x, y: np.inner(x, y) / (norm(x) * norm(y))
</code></pre>
<p>Now, the trouble is, if I define two sentences which are almost similar and take their cosine similarity score, it's coming very low. E.g.</p>
<pre><code>a = model_dbow.infer_vector(create_tagged_doc('That was a fradulent transaction.', nlp))
b = model_dbow.infer_vector(create_tagged_doc('That transaction was fradulant.', nlp))

print(cosine_sim(a, b)) # 0.07102317
</code></pre>
<p>Just to make sure, I checked with exact same vector repeated, and it's proper.</p>
<pre><code>a = model_dbow.infer_vector(create_tagged_doc('That was a fradulent transaction.', nlp))
b = model_dbow.infer_vector(create_tagged_doc('That was a fradulent transaction.', nlp))

print(cosine_sim(a, b)) # 0.9980062
</code></pre>
<p>What's going wrong in here?</p>
",Vectorization & Embeddings,similarity score way using doc vec embedding trying document de duplication ny time corpus prepared recently contains data related financial fraud first convert article snippet list object sample look follows compile train doc vec model let define cosine similarity trouble define two sentence almost similar take cosine similarity score coming low e g make sure checked exact vector repeated proper going wrong
Fine tune doc2vec on gensim,"<p>I am new in NLU and I am doing a project on document embedding. I want to fine-tune the doc2vec model in gensim on my small dataset to see if it can help for document clustering. I read the tutorial on the website but they did not mention anything about fine-tuning. Where I can find doc2vec pertained on wikipedia or twitter on any big dataset.</p>
",Vectorization & Embeddings,fine tune doc vec gensim new nlu project document embedding want fine tune doc vec model gensim small dataset see help document clustering read tutorial website mention anything fine tuning find doc vec pertained wikipedia twitter big dataset
Confused with the return result of TfidfVectorizer.fit_transform,"<p>I wanted to learn more about NLP. I came across this piece of code. But I was confused about the outcome of <code>TfidfVectorizer.fit_transform</code> when the result is printed. I am familiar with what tfidf is but I could not understand what the numbers mean.</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import os
import io
import string
import requests
import csv
import nltk
from zipfile import ZipFile

sess = tf.Session()

batch_size = 100
max_features = 1000

save_file_name = os.path.join('smsspamcollection', 'SMSSpamCollection.csv')
if os.path.isfile(save_file_name):
    text_data = []
    with open(save_file_name, 'r') as temp_output_file:
        reader = csv.reader(temp_output_file)
        for row in reader:
            text_data.append(row)

else:
    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'
    r = requests.get(zip_url)
    z = ZipFile(io.BytesIO(r.content))
    file = z.read('SMSSpamCollection')

    # Format data 
    text_data = file.decode()
    text_data = text_data.encode('ascii', errors='ignore')
    text_data = text_data.decode().split('\n')
    text_data = [x.split('\t') for x in text_data if len(x) &gt;= 1]

    # And write to csv 
    with open(save_file_name, 'w') as temp_output_file:
        writer = csv.writer(temp_output_file)
        writer.writerows(text_data)

texts = [x[1] for x in text_data]
target = [x[0] for x in text_data]
target = [1 if x == 'spam' else 0 for x in target]

# Normalize the text
texts = [x.lower() for x in texts]  # lower
texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]  # remove punctuation
texts = [''.join(c for c in x if c not in '0123456789') for x in texts]  # remove numbers
texts = [' '.join(x.split()) for x in texts]  # trim extra whitespace


def tokenizer(text):
    words = nltk.word_tokenize(text)
    return words


tfidf = TfidfVectorizer(tokenizer=tokenizer, stop_words='english', max_features=max_features)
sparse_tfidf_texts = tfidf.fit_transform(texts)
print(sparse_tfidf_texts)
</code></pre>
<p>And the output is:</p>
<blockquote>
<p>(0, 630)  0.37172623140154337   (0, 160)  0.36805562944957004   (0,
38)   0.3613966215413548   (0, 545)   0.2561101665717327   (0,
326)  0.2645280991765623   (0, 967)   0.3277447602873963   (0,
421)  0.3896274380321477   (0, 227)   0.28102915589024796   (0,
323)  0.22032541100275282   (0, 922)  0.2709848154866997   (1,
577)  0.4007895093299793   (1, 425)   0.5970064521899725   (1,
943)  0.6310763941180291   (1, 878)   0.29102173465492637   (2,
282)  0.1771481430848552   (2, 243)   0.5517018054305785   (2,
955)  0.2920174942032025   (2, 138)   0.30143666813167863   (2,
946)  0.2269933441326121   (2, 165)   0.3051095293405041   (2,
268)  0.2820392223588522   (2, 780)   0.24119626642264894   (2,
823)  0.1890454397278538   (2, 674)   0.256251970757827   (2,
874)  0.19343834015314287   : :   (5569, 648) 0.24171652492226922<br />
(5569, 123)   0.23011909339432202   (5569, 957)   0.24817919217662862<br />
(5569, 549)   0.28583789844730134   (5569, 863)   0.3026729783085827<br />
(5569, 844)   0.20228305447951195   (5569, 146)   0.2514415602877767<br />
(5569, 595)   0.2463259875380789   (5569, 511)    0.3091904754885042<br />
(5569, 230)   0.2872728684768659   (5569, 638)    0.34151390143548765<br />
(5569, 83)    0.3464271621701711   (5570, 370)    0.4199910200421362<br />
(5570, 46)    0.48234172093857797   (5570, 317)   0.4171646676697801<br />
(5570, 281)   0.6456993475093024   (5572, 282)    0.25540827228532487<br />
(5572, 385)   0.36945842040023935   (5572, 448)   0.25540827228532487<br />
(5572, 931)   0.3031800542518209   (5572, 192)    0.29866989620926737<br />
(5572, 303)   0.43990016711221736   (5572, 87)    0.45211284173737176<br />
(5572, 332)   0.3924202767503492   (5573, 866)    1.0</p>
</blockquote>
<p>I would be more than happy if someone can explain about the output.</p>
",Vectorization & Embeddings,confused return result tfidfvectorizer fit transform wanted learn nlp came across piece code wa confused outcome result printed familiar tfidf could understand number mean output would happy someone explain output
Extracting features from list of strings,"<p>I have list of strings and a strings that look like this :</p>

<pre><code>    mylist = [""the yam is sweet"", ""what is the best time to come"", ""who ate my food"", ""no empty food on the table"", ""what can I do to make you happy""]  # about 20k data
    myString1 = ""Is yam a food""  # String can be longer than this
    myString2 = ""should I give you a food""
    myString3 = ""I am not happy""
</code></pre>

<p>I want to compare each of the myString to each string in my list and collect the percentage of similarity in three different lists. So the end result will look like this:</p>

<pre><code>   similar_string1 = [70, 0.5, 50, 55, 2]
   similar_string2 = [50, 0.5, 70, 85, 2]
   similar_string3 = [20, 15, 0, 5, 80]
</code></pre>

<p>So mystring1 will be compare to each string in mylist and calculate the percentage similarity. Same with myString2 and myString3. Then collect each of those percentage in a list as seen above.</p>

<p>I read that one can use TF-IDF to vectorize mylist and mystring, then use cosine similarity to compare them, but I never work on something like this before and I will love if anyone has an idea, process or code that will help me get started.</p>

<p>Thanks</p>
",Vectorization & Embeddings,extracting feature list string list string string look like want compare mystring string list collect percentage similarity three different list end result look like mystring compare string mylist calculate percentage similarity mystring mystring collect percentage list seen read one use tf idf vectorize mylist mystring use cosine similarity compare never work something like love anyone ha idea process code help get started thanks
Can you use an embedding + CNN model for both text and image classification?,"<p>I have a <code>tensorflow</code> CNN model with an embedding layer for text classification as follows:</p>
<pre><code> model = tf.keras.Sequential([
      Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding], trainable=False),
      Conv1D(128, 5, activation='relu'),
      GlobalMaxPooling1D(),
      Dense(10, activation='relu'),
      Dense(1, activation='sigmoid')
    ])
</code></pre>
<p>My colleague is adamant that this is viable but I found this <a href=""https://stackoverflow.com/questions/51696575/keras-cnn-add-text-as-additional-input-besides-image-to-cnn"">post</a> stating it is not feasible. I understand CNN as an algorithm can be used for text and image inputs, but my understanding is that you can't use the <strong>same</strong> CNN model for text input and image input: text will use <code>Conv1D</code> and image, <code>Conv2D</code>.</p>
<p>The linked post mentions:</p>
<blockquote>
<ol>
<li>Process the image using a CNN model.</li>
<li>Process the text using another model ... By CNN I mean usually a 1D CNN that runs over the words in a sentence.</li>
<li>Merge the 2 latent spaces which tells information about the image and the
text.</li>
<li>Run last few Dense layers for classification.</li>
</ol>
</blockquote>
<p>If I'm on the right track, how I can go about building two sub models (one for text, another for image classification) and merge the latent spaces. Thank you!</p>
",Vectorization & Embeddings,use embedding cnn model text image classification cnn model embedding layer text classification follows colleague adamant viable found href stating feasible understand cnn algorithm used text image input understanding use strong cnn model text input image input text use image linked post mention process image using cnn model process text using another model cnn mean usually cnn run word sentence merge latent space tell information image text run last dense layer classification right track go building two sub model one text another image classification merge latent space thank
What is negative sampling method use- sigmoid or softmax?,"<p>I am recently reading this paper: Word2Vec explained(<a href=""https://arxiv.org/pdf/1402.3722.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1402.3722.pdf</a>)</p>
<p>And there's something I can't understand..</p>
<p>In page 3, they say that p is defined using softmax</p>
<p>$p(D=1|w, c, \theta) = \frac{1}{1+e^{-v_c\dotv_w}}$</p>
<p><a href=""https://i.sstatic.net/l1m5Y.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>but i am confused because i have seen that formula in sigmoid function, not softmax function.</p>
<p>How you derive that definition from softmax?</p>
",Vectorization & Embeddings,negative sampling method use sigmoid softmax recently reading paper word vec explained something understand page say p defined using softmax p w c theta frac e v c dotv w enter image description confused seen formula sigmoid function softmax function derive definition softmax
How to improve word mover distance similarity in python and provide similarity score using weighted sentence,"<p>Word movers Distance can be used to identify similarity between text .
This similarity can be used to compare multiple text for finding nearest similar text.
However , I was unable to customise the algorithm to do the following 
1)eliminate location (GPE) - identified by spacy , in the text  to have any weightage in comparing similarity .
2)Give more weightage to features that are in first sentence of text rather than features in second sentence and second sentence over third and so on .</p>

<pre><code>instance = WmdSimilarity(wmd_corpus, loaded_model, num_best=10)
start = time()
sent = 'Abc hotel serves best in class drunken prawn in north america . ABC Hotel has branches in London, New York, Chicago and San Francisco.'
query = preprocess(sent)

sims = instance[query]  # A query is simply a ""look-up"" in the similarity class.

print('Cell took %.2f seconds to run.' % (time() - start))

print('Query:')
print(sent)
for i in range(num_best):
    print()
    print('sim = %.4f' % sims[i][1])
    print(documents[sims[i][0]])
</code></pre>

<p>In this particular example , where hotel description is passed for WMD similarity ,
The results identify descriptions such as </p>

<p>-DEF is a restaurant in Chicago serving vegan food since 1969 . 
-JKL now serving in London, New York, Chicago and San Francisco 
- Bestsellers of the hotel include drunken prawn , lasagne etc . (MNO Hotel)</p>

<p>Expected result 
Only MNO hotel from the above result is relevant accoring to the food aspect . </p>

<p>Query :
How to eliminate the other hotel which are mapped due to location ?</p>
",Vectorization & Embeddings,improve word mover distance similarity python provide similarity score using weighted sentence word mover distance used identify similarity text similarity used compare multiple text finding nearest similar text however wa unable customise algorithm following eliminate location gpe identified spacy text weightage comparing similarity give weightage feature first sentence text rather feature second sentence second sentence third particular example hotel description passed wmd similarity result identify description def restaurant chicago serving vegan food since jkl serving london new york chicago san francisco bestseller hotel include drunken prawn lasagne etc mno hotel expected result mno hotel result relevant accoring food aspect query eliminate hotel mapped due location
Combine Word Embeddings with with topic-word distribution from LDA for text summarization,"<p>Im a newbie in NLP and i was wondering if it is a good idea to summarize a document that has already been classified into a certain topic through methods such as LDA by considering the Word Embedding retrieved from Word2Vec and the topic-word distribution that has already been generated, to come up with a sentence scoring algorithm. Does this sound like a good approach for creating a summary of a document?</p>
",Vectorization & Embeddings,combine word embeddings topic word distribution lda text summarization im newbie nlp wa wondering good idea summarize document ha already classified certain topic method lda considering word embedding retrieved word vec topic word distribution ha already generated come sentence scoring algorithm doe sound like good approach creating summary document
Is there a faster way to lookup dictionary indices?,"<p>I am trying to look up dictionary indices for thousands of strings and this process is very, very slow. There are package alternatives, like <code>KeyedVectors</code> from <code>gensim.models</code>, which does what I want to do in about a minute, but I want to do what the package does more manually and to have more control over what I am doing.</p>
<p>I have two objects: (1) a dictionary that contains key : values for word embeddings, and (2) my pandas dataframe with my strings that need to be transformed into the index value found for each word in object (1). Consider the code below -- is there any obvious improvement to speed or am I relegated to external packages?</p>
<p>I would have thought that key lookups in a dictionary would be blazing fast.</p>
<h1>Object 1</h1>
<pre><code>embeddings_dictionary = dict()
glove_file = open('glove.6B.200d.txt', encoding=&quot;utf8&quot;)
for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = np.asarray(records[1:], dtype='float32')
    embeddings_dictionary [word] = vector_dimensions
</code></pre>
<h1>Object 2 (The slowdown)</h1>
<pre><code>no_matches = []
glove_tokenized_data = []
for doc in df['body'][:5]:
    doc = doc.split()
    ints = []
    for word in doc:
        try:
    # the line below is the problem
            idx = list(embeddings_dictionary.keys()).index(word)
        except:
            idx = 400000  # unknown
            no_matches.append(word)
        ints.append(idx)
    glove_tokenized_data.append(ints)

</code></pre>
",Vectorization & Embeddings,faster way lookup dictionary index trying look dictionary index thousand string process slow package alternative like doe want minute want package doe manually control two object dictionary contains key value word embeddings panda dataframe string need transformed index value found word object consider code obvious improvement speed relegated external package would thought key lookup dictionary would blazing fast object object slowdown
How to find the semantic similarity between 2 sentences?,"<p>Please tell me some methods to find semantic similarity between sentences.</p>

<pre><code>For example:

Sen1 :- Ram killed Ravan
Sen2 :- Sam was killed by Ravan
</code></pre>

<p>What is the similarity between both sentences?</p>
",Vectorization & Embeddings,find semantic similarity sentence please tell method find semantic similarity sentence similarity sentence
Is there an algorithm that tells the semantic similarity of two phrases,"<p>input: phrase 1, phrase 2</p>

<p>output: semantic similarity value (between 0 and 1), or the probability these two phrases are talking about the same thing</p>
",Vectorization & Embeddings,algorithm tell semantic similarity two phrase input phrase phrase output semantic similarity value probability two phrase talking thing
How to cluster spacy vectors (word embedding) into groups using Annoy or other similar algorithms,"<p>I have a list of words whose vector embeddings I got by using spacy's pre trained model <em>en_core_web_lg</em>.</p>
<p>My questions are two fold</p>
<p>Can these word vectors be fed into Annoy like algorithm?
Can I get say 20 groups, each group containing around 100 words?</p>
<p>I would be grateful for any resources/code.</p>
",Vectorization & Embeddings,cluster spacy vector word embedding group using annoy similar algorithm list word whose vector embeddings got using spacy pre trained model en core web lg question two fold word vector fed annoy like algorithm get say group group containing around word would grateful resource code
Pytorch word embeddings results in nan values,"<p>After training a word embedding model on a large-ish corpus, my embeddings converge to <code>nan</code> values.</p>
<p>The model is very simple (skipgram with negative sampling)</p>
<p>full model:</p>
<pre class=""lang-py prettyprint-override""><code>class NEG_loss(nn.Module):
    def __init__(self, vocab_size, embed_size, neg_sampling_table=None):
        &quot;&quot;&quot;
        :param vocab_size: An int. The number of possible classes.
        :param embed_size: An int. EmbeddingLockup size
        :param num_sampled: An int. The number of sampled from noise examples
        :param neg_sampling_table: A list of non negative floats. Class neg_sampling_table. None if
            using uniform sampling. The neg_sampling_table are calculated prior to
            estimation and can be of any form, e.g equation (5) in [1]
        &quot;&quot;&quot;

        super(NEG_loss, self).__init__()

        self.device = t.device(&quot;cuda:0&quot; if t.cuda.is_available() else &quot;cpu&quot;)

        self.vocab_size = vocab_size
        self.embed_size = embed_size

        self.out_embed.weight = nn.Parameter(
            t.cat(
                [
                    t.zeros(1, self.embed_size),
                    FT(self.vocab_size - 1, self.embed_size).uniform_(
                        -0.5 / self.embed_size, 0.5 / self.embed_size
                    ),
                ]
            )
        )


        self.in_embed.weight = nn.Parameter(
            t.cat(
                [
                    t.zeros(1, self.embed_size),
                    FT(self.vocab_size - 1, self.embed_size).uniform_(
                        -0.5 / self.embed_size, 0.5 / self.embed_size
                    ),
                ]
            )
        )

        self.neg_sampling_table = neg_sampling_table
        if self.neg_sampling_table is not None:
            assert min(self.neg_sampling_table) &gt;= 0, &quot;Each weight should be &gt;= 0&quot;

            self.neg_sampling_table = Variable(t.from_numpy(neg_sampling_table)).float()

    # TODO this is bad - find more elegant solution
    def sample(self, num_sample):
        &quot;&quot;&quot;
        draws a sample from classes based on neg_sampling_table
        &quot;&quot;&quot;

        return self.neg_sampling_table[
            t.randint(0, len(self.neg_sampling_table), (num_sample,))
        ]

    def forward(self, input_labels, out_labels, num_sampled):
        &quot;&quot;&quot;
        :param input_labels: Tensor with shape of [batch_size] of Long type
        :param out_labels: Tensor with shape of [batch_size, window_size] of Long type
        :param num_sampled: An int. The number of sampled from noise examples
        :return: Loss estimation with shape of [1]
            loss defined in Mikolov et al. Distributed Representations of Words and Phrases and their Compositionality
            papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
        &quot;&quot;&quot;

        input_labels = input_labels.to(&quot;cuda:0&quot;)
        out_labels = out_labels.to(&quot;cuda:0&quot;)

        batch_size = out_labels.size()[0]

 
        input_ = self.in_embed(input_labels.view(-1))
        output = self.out_embed(out_labels.view(-1))

        if self.neg_sampling_table is not None:
           
            noise_sample_count = batch_size * num_sampled
            draw = self.sample(noise_sample_count)
            noise = draw.view(batch_size, num_sampled).long()
            
        else:


            noise = Variable(
                t.Tensor(batch_size, num_sampled)
                .uniform_(0, self.vocab_size - 1)
                .long()
            )
        # if use_cuda:
        noise = noise.to(self.device)
        noise = self.out_embed(noise).neg()

        log_target = (input_ * output).sum(1).squeeze().sigmoid().log()

        &quot;&quot;&quot; ∑[batch_size * window_size, num_sampled, embed_size] * [batch_size * window_size, embed_size, 1] -&gt;
            ∑[batch_size, num_sampled, 1] -&gt; [batch_size] &quot;&quot;&quot;
        sum_log_sampled = (
            t.bmm(noise, input_.unsqueeze(2)).sigmoid().log().sum(1).squeeze()
        )

        loss = log_target + sum_log_sampled

        return -loss.mean()

    def input_embeddings(self):
        return self.in_embed.weight.detach().cpu().numpy()
</code></pre>
<p>and training loop and optimizer code</p>
<pre class=""lang-py prettyprint-override""><code>    # NEG loss and optim
    neg = NEG_loss(vocab_size, dim, neg_sampling_table=neg_sampling_dist)
    neg.to(&quot;cuda:0&quot;)
    optimizer = Adam(neg.parameters(), 0.01)

    sys.stdout.write(&quot;BEGINNING TRAINING\n&quot;)
    for i in range(epochs):
        sys.stdout.write(&quot;-&quot; * 35 + &quot;\n&quot;)
        sys.stdout.write(f&quot;EPOCH {i+1}\n&quot;)
        for i, batch in enumerate(tqdm(dataloader)):
            input_, output_ = batch
            optimizer.zero_grad()
            loss = neg(input_, output_, neg_samples)
            loss.backward()
            optimizer.step()
        sys.stdout.write(&quot;-&quot; * 35 + &quot;\n&quot;)
</code></pre>
<p>This model is about as simple as it gets so I am a bit surprised I am having this issue. Everything is fine on a smaller corpus (7k batches of size 128) but gets <code>nan</code>-y with a larger corpus (200k batches of size 128).</p>
<p>Anyone see anything immediately wrong or have any tips to figure out what is going?</p>
<p>Any help is appreciated. Also pretty new to pytorch - so if you see anything that is a bit dumb feel free to point it out :)</p>
",Vectorization & Embeddings,pytorch word embeddings result nan value training word embedding model large ish corpus embeddings converge value model simple skipgram negative sampling full model training loop optimizer code model simple get bit surprised issue everything fine smaller corpus k batch size get larger corpus k batch size anyone see anything immediately wrong tip figure going help appreciated also pretty new pytorch see anything bit dumb feel free point
Why computing similarity with gensim needs the size of the dictionary?,"<p>In order to use the <code>gensim.similarities.docsim.Similarity</code> class to compute similarities between words, one need to provide the corpus and the size of the dictionary.</p>
<p>In my case, the corpus are the word vectors computed using a word2vec model.</p>
<p>I wonder why <code>gensim</code> needs the size of the dictionary? And also, if it needs here the size of the dictionary used to create the word2vec model, or the size of the dictionary of the corpus, for which I want to compute the similarities.</p>
",Vectorization & Embeddings,computing similarity gensim need size dictionary order use class compute similarity word one need provide corpus size dictionary case corpus word vector computed using word vec model wonder need size dictionary also need size dictionary used create word vec model size dictionary corpus want compute similarity
Why a document less relevant to search query receives higher cosine similarity score?,"<p>Wile experimenting with TFIDF text search, oddly a document that lacks one of the search query terms receives a higher cosine similarity score.</p>
<p>I get that the term 'ASIC' is less frequent in the corpus than 'cybersecurity' which may explain the higher TF and IDF (and surely the cos. sim.) scores, so the first document outweighs the second, even with the missing 'cybersecurity' term, I'm just not sure if this is really the reason or something may be wrong with my calculation / understanding.</p>
<p>What could be done to rank the second document higher containing both search terms?</p>
<p>Also a wrorking example can be found here: <a href=""https://www.kaggle.com/edmondvarga/kernel46989d3fbc"" rel=""nofollow noreferrer"">https://www.kaggle.com/edmondvarga/kernel46989d3fbc</a></p>
<pre><code>import math, copy
from collections import OrderedDict
from sklearn.feature_extraction.text import CountVectorizer

#The way I &quot;fit&quot; the search query document to the lexicon:

query = ['cybersecurity asic']

cv2 = CountVectorizer()
query_bow = cv2.fit_transform(raw_documents=query).toarray()

vocab = OrderedDict((key, 0) for key, value in sorted(cv1.vocabulary_.items(), key=lambda tup: tup[1]))
vec = copy.copy(vocab)

for word, idx in cv2.vocabulary_.items():
  docs_containing_word = 0
  for _doc in stories['bodytext'].values:
    # if re.search(f'\\s{word}\\s', _doc.lower()):
    if word in _doc.lower():
      docs_containing_word += 1
  if docs_containing_word == 0:
    continue
  tf = query_bow[0][idx] / len(vocab)
  numOfDocs = stories.shape[0]
  idf = numOfDocs / docs_containing_word
  vec[word] = tf * idf

#function to calculate cosine similarity

def cosine_sim(vec1, vec2):
  dot_prod = 0
  for i, v in enumerate(vec1):
    dot_prod += v * vec2[i]

  mag_1 = math.sqrt(sum([x**2 for x in vec1]))
  mag_2 = math.sqrt(sum([x**2 for x in vec2]))
  return dot_prod / (mag_1 * mag_2)
</code></pre>
<p>Results:</p>
<pre><code>word &quot;cybersecurity&quot;:
present in 38 docs
tf: 5.806189397898159e-05, 
idf: 73.3157894736842
tfidf: 0.004256853595406387

word &quot;asic&quot;:
present in 17 docs
tf: 5.806189397898159e-05, 
idf: 163.88235294117646
tfidf: 0.00951531980149663

Doc 1 cosine sim: 0.45333133411924276, missing the term &quot;cybersecurity&quot;

The Australian Securities and Investments Commission (ASIC) published a report on 20 December summarising the four regulatory technology (RegTech) initiatives and events conducted by the ASIC during the financial year (FY) 2018-2019.The ASIC initiatives allowed RegTech developers to showcase the potential for RegTech in the financial services industry, which included software to detect non-compliance with mandatory disclosure requirements and a viable proof-of-concept chatbot that can advise businesses regarding licensing questions.In FY2019-2020, ASIC is planning at least four further RegTech initiatives, focusing on machine-learning and digital record-keeping amongst others.

Doc 2 cosine sim: 0.3032089236374744 with both &quot;ASIC&quot; and &quot;cybersecurity&quot; terms

The Australian Securities and Investments Commission (ASIC) published on 18 December a report on the cyber resilience of firms operating in Australia’s financial markets, following an assessment of firms’ abilities to prepare for, respond to and recover from cybersecurity events.The report also aims to identify new and emerging trends in cybercrime, such as the outsourcing of non-core functions to third parties, which has created difficulties in the management of cybersecurity risks in the supply chain.
</code></pre>
",Vectorization & Embeddings,document le relevant search query receives higher cosine similarity score wile experimenting tfidf text search oddly document lack one search query term receives higher cosine similarity score get term asic le frequent corpus cybersecurity may explain higher tf idf surely co sim score first document outweighs second even missing cybersecurity term sure really reason something may wrong calculation understanding could done rank second document higher containing search term also wrorking example found result
How do I implement (Brown) cluster represenations of texts from dicts as features for text classifier elegantly?,"<p>I'm trying to implement a version of Brown clusters for a series of review texts (SemEval 2014). I am using Owoputi et al.'s(2013) publicly available twitter clusters. They look like the following:</p>
<pre><code>0000    ijust   1446
0000    i   17071657
0000    -i  4254
000100  iyou    41
000100  #innowayshapeorform 41
</code></pre>
<p>where the bitstring indicates the cluster and there are 1000 clusters.</p>
<p>I have extracted to dictionary where I have the relevant bitstring as the key and a list of the tokens as value: e.g.</p>
<p><code>{'0000': ['ijust', 'i', '-i'], '000100': ['iyou', #innowayshapeorform] ...}</code></p>
<p>I am just missing the part of how to one hot encode the text mapping the dictionary keys to indexes in a vector (1d array):</p>
<p>such that if a word occurs in the text AND the word occurs in a cluster that the value for the cluster is changed from 0 to 1.</p>
<p>e.g.
one_hot_vector = [0]*3</p>
<ul>
<li>cluster 1 (vector index = 0): ['I','me','my','mine']</li>
<li>cluster 2 (vector index = 1): ['love', 'like', 'want','need']</li>
<li>cluster 3 (vector index = 2): ['dogs', 'pets', 'cats', 'puppies']</li>
</ul>
<p>text 1: I hate cats</p>
<p>text 1 vector representation: [1,0,1]</p>
<p>text 2: dogs love me</p>
<p>text 2 vector representation: [1,1,1]</p>
<p>text 3: I dream of sheep</p>
<p>text 3 vector representation: [1,0,0]</p>
<p>this example has 3 clusters - the clusters I have would be 1000 dimensions in length.</p>
",Vectorization & Embeddings,implement brown cluster represenations text dicts feature text classifier elegantly trying implement version brown cluster series review text semeval using owoputi et al publicly available twitter cluster look like following bitstring indicates cluster cluster extracted dictionary relevant bitstring key list token value e g missing part one hot encode text mapping dictionary key index vector array word occurs text word occurs cluster value cluster changed e g one hot vector cluster vector index mine cluster vector index love like want need cluster vector index dog pet cat puppy text hate cat text vector representation text dog love text vector representation text dream sheep text vector representation example ha cluster cluster would dimension length
CNN on tfidf as input,"<p>I am working on fake news detection using CNN, I am new to ccoding CNNs in keras and tensorflow. I need help regarding creating a CNN that takes input as statements in form of vectors each of length 100 and outputs 0 or 1 depending on its predicted value as false or true.</p>

<pre><code>X_train.shape
# 10229, 100

X_train = np.expand_dims(X_train, axis=2)
X_train.shape
# 10229,100,1

# actual cnn model here
import tensorflow as tf
from tensorflow.keras import layers

# Conv1D + global max pooling

from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Flatten, Dense
from keras.layers import Input
text_len=100
from keras.models import Model

inp = Input(batch_shape=(None, text_len, 1))
conv2 = Conv1D(filters=128, kernel_size=5, activation='relu')(inp)
drop21 = Dropout(0.5)(conv2)
conv22 = Conv1D(filters=64, kernel_size=5, activation='relu')(drop21)
drop22 = Dropout(0.5)(conv22)
pool2 = MaxPooling1D(pool_size=2)(drop22)
flat2 = Flatten()(pool2)
out = Dense(1, activation='softmax')(flat2)

model = Model(inp, out)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
model.fit(X_train, Y_train)
</code></pre>

<p>I will really appreciate if someone could give me a working code for this with a little bit of explaination</p>
",Vectorization & Embeddings,cnn tfidf input working fake news detection using cnn new ccoding cnns kera tensorflow need help regarding creating cnn take input statement form vector length output depending predicted value false true really appreciate someone could give working code little bit explaination
Using keras tokenizer for new words not in training set,"<p>I'm currently using the Keras Tokenizer to create a word index and then matching that word index to the the imported GloVe dictionary to create an embedding matrix.  However, the problem I have is that this seems to defeat one of the advantages of using a word vector embedding since when using the trained model for predictions if it runs into a new word that's not in the tokenizer's word index it removes it from the sequence.  </p>

<pre><code>#fit the tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
word_index = tokenizer.word_index

#load glove embedding into a dict
embeddings_index = {}
dims = 100
glove_data = 'glove.6B.'+str(dims)+'d.txt'
f = open(glove_data)
for line in f:
    values = line.split()
    word = values[0]
    value = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = value
f.close()

#create embedding matrix
embedding_matrix = np.zeros((len(word_index) + 1, dims))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector[:dims]

#Embedding layer:
embedding_layer = Embedding(embedding_matrix.shape[0],
                        embedding_matrix.shape[1],
                        weights=[embedding_matrix],
                        input_length=12)

#then to make a prediction
sequence = tokenizer.texts_to_sequences([""Test sentence""])
model.predict(sequence)
</code></pre>

<p>So is there a way I can still use the tokenizer to transform sentences into an array and still use as much of the words GloVe dictionary as I can instead of only the ones that show up in my training text?  </p>

<p>Edit: Upon further contemplation, I guess one option would be to add a text or texts to the texts that the tokenizer is fit on that includes a list of the keys in the glove dictionary. Though that might mess with some of the statistics if I want to use tf-idf. Is there either a preferable way to doing this or a different better approach?</p>
",Vectorization & Embeddings,using kera tokenizer new word training set currently using kera tokenizer create word index matching word index imported glove dictionary create embedding matrix however problem seems defeat one advantage using word vector embedding since using trained model prediction run new word tokenizer word index remove sequence way still use tokenizer transform sentence array still use much word glove dictionary instead one show training text edit upon contemplation guess one option would add text text text tokenizer fit includes list key glove dictionary though might mess statistic want use tf idf either preferable way different better approach
Finding Similarity between 2 sentences using word2vec of sentence with python,"<p>I want to calculate the similarity between two sentences using word2vectors, I am trying to get the vectors of a sentence so that i can calculate the average of a sentence vectors to find the cosine similarity. i have tried this code but its not working. the output it gives the sentence-vectors with ones. i want the actual vectors of sentences in sentence_1_avg_vector &amp; sentence_2_avg_vector.</p>

<p>Code:</p>

<pre><code>    #DataSet#
    sent1=[['What', 'step', 'step', 'guide', 'invest', 'share', 'market', 'india'],['What', 'story', 'Kohinoor', 'KohiNoor', 'Diamond']]
    sent2=[['What', 'step', 'step', 'guide', 'invest', 'share', 'market'],['What', 'would', 'happen', 'Indian', 'government', 'stole', 'Kohinoor', 'KohiNoor', 'diamond', 'back']]
    sentences=sent1+sent2

    #''''Applying Word2vec''''#
    word2vec_model=gensim.models.Word2Vec(sentences, size=100, min_count=5)
    bin_file=""vecmodel.csv""
    word2vec_model.wv.save_word2vec_format(bin_file,binary=False)

    #''''Making Sentence Vectors''''#
    def avg_feature_vector(words, model, num_features, index2word_set):
        #function to average all words vectors in a given paragraph
        featureVec = np.ones((num_features,), dtype=""float32"")
        #print(featureVec)
        nwords = 0
        #list containing names of words in the vocabulary
        index2word_set = set(model.wv.index2word)# this is moved as input param for performance reasons
        for word in words:
            if word in index2word_set:
                nwords = nwords+1
                featureVec = np.add(featureVec, model[word])
                print(featureVec)
        if(nwords&gt;0):
            featureVec = np.divide(featureVec, nwords)
        return featureVec

    i=0
    while i&lt;len(sent1):
        sentence_1_avg_vector = avg_feature_vector(mylist1, model=word2vec_model, num_features=300, index2word_set=set(word2vec_model.wv.index2word))
        print(sentence_1_avg_vector)

        sentence_2_avg_vector = avg_feature_vector(mylist2, model=word2vec_model, num_features=300, index2word_set=set(word2vec_model.wv.index2word))
        print(sentence_2_avg_vector)

        sen1_sen2_similarity =  1 - spatial.distance.cosine(sentence_1_avg_vector,sentence_2_avg_vector)
        print(sen1_sen2_similarity)

        i+=1
</code></pre>

<p>the output this code gives:</p>

<pre><code>[ 1.  1.  ....  1.  1.]
[ 1.  1.  ....  1.  1.]
0.999999898245
[ 1.  1.  ....  1.  1.]
[ 1.  1.  ....  1.  1.]
0.999999898245
</code></pre>
",Vectorization & Embeddings,finding similarity sentence using word vec sentence python want calculate similarity two sentence using word vector trying get vector sentence calculate average sentence vector find cosine similarity tried code working output give sentence vector one want actual vector sentence sentence avg vector sentence avg vector code output code give
Find a sentence&#39;s confidence score/similarity to a word2vec model,"<p>I created two different word2vec models for two different topics or categories.</p>

<p>Now, if I give a new sentence as an input, how can I calculate the confidence score of that sentence to both the models. I mean, how do I calculate the percentage similarity or closeness of the sentences with both the models ?</p>

<p>Is it possible to get a pooled vector for one model so that I can compare it to a new sentence's vector.</p>

<p>Example -</p>

<p>Say I created two word2vec models for SPORTS and ACADEMICS. I created both the models using various sentences, pre-processing etc.</p>

<p>Now, say I give a sentence which is a mix of both SPORTS and ACADEMICS.</p>

<p>How do I know how much my sentence is in the SPORTS context and how much in the ACADEMICS using the already present model and the vector of this new sentence ?</p>

<p>Like, can I say that my new sentence has 70% similarity to SPORTS model and 50% similarity to academics model ?</p>
",Vectorization & Embeddings,find sentence confidence score similarity word vec model created two different word vec model two different topic category give new sentence input calculate confidence score sentence model mean calculate percentage similarity closeness sentence model possible get pooled vector one model compare new sentence vector example say created two word vec model sport academic created model using various sentence pre processing etc say give sentence mix sport academic know much sentence sport context much academic using already present model vector new sentence like say new sentence ha similarity sport model similarity academic model
Using glove.6B.100d.txt embedding in spacy getting zero lex.rank,"<p>I am trying to load glove 100d emebddings in spacy nlp pipeline.  </p>

<p>I create the vocabulary in spacy format as follows: </p>

<pre><code>python -m spacy init-model en spacy.glove.model --vectors-loc glove.6B.100d.txt
</code></pre>

<p>glove.6B.100d.txt is converted to word2vec format by adding ""400000 100"" in the first line. </p>

<p>Now </p>

<pre><code>spacy.glove.model/vocab has following files: 
5468549  key2row
38430528  lexemes.bin
5485216  strings.json
160000128  vectors
</code></pre>

<p>In the code: </p>

<pre><code>import spacy 
nlp = spacy.load(""en_core_web_md"")

from spacy.vocab import Vocab
vocab = Vocab().from_disk('./spacy.glove.model/vocab')

nlp.vocab = vocab

print(len(nlp.vocab.strings)) 
print(nlp.vocab.vectors.shape) gives 
</code></pre>

<p>gives 
407174
(400000, 100)</p>

<p>However the problem is that: </p>

<pre><code>V=nlp.vocab
max_rank = max(lex.rank for lex in V if lex.has_vector)
print(max_rank) 
</code></pre>

<p>gives 0 </p>

<p>I just want to use the 100d glove embeddings within spacy in combination with ""tagger"", ""parser"", ""ner"" models from en_core_web_md. </p>

<p>Does anyone know how to go about doing this correctly (is this possible)? </p>
",Vectorization & Embeddings,using glove b txt embedding spacy getting zero lex rank trying load glove emebddings spacy nlp pipeline create vocabulary spacy format follows glove b txt converted word vec format adding first line code give however problem give want use glove embeddings within spacy combination tagger parser ner model en core web md doe anyone know go correctly possible
How to improve code to speed up word embedding with transformer models?,"<p>I need to compute words embeddings for a bunch of documents with different language models.
No problem with that, the script is doing fine, except I'm working on a notebook, without GPU and each text needs around 1.5s to be processed which is by far too long (I have thousands of texts to process).</p>

<p>Here is how I'm doing it with pytorch and transformers lib:</p>

<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import CamembertModel, CamembertTokenizer

docs = [text1, text2, ..., text20000]
tok = CamembertTokenizer.from_pretrained('camembert-base')
model = CamembertModel.from_pretrained('camembert-base', output_hidden_states=True)
# let try with a batch size of 64 documents
docids = [tok.encode(
  doc, max_length=512, return_tensors='pt', pad_to_max_length=True) for doc in docs[:64]]
ids=torch.cat(tuple(docids))
device = 'cuda' if torch.cuda.is_available() else 'cpu' # cpu in my case...
model = model.to(device)
ids = ids.to(device)
model.eval()
with torch.no_grad():
    out = model(input_ids=ids)
# 103s later...
</code></pre>

<p>Do someone has any idea or suggestions to improve speed?</p>
",Vectorization & Embeddings,improve code speed word embedding transformer model need compute word embeddings bunch document different language model problem script fine except working notebook without gpu text need around processed far long thousand text process pytorch transformer lib someone ha idea suggestion improve speed
How does gensim word2vec word embedding extract training word pair for 1 word sentence?,"<p>Refer to below image (the process of how word2vec skipgram extract training datasets-the word pair from the input sentences). </p>

<p>E.G. ""I love you."" ==> [(I,love), (I, you)]</p>

<p>May I ask what is the word pair when the sentence contains only one word? </p>

<p>Is it  ""Happy!"" ==> [(happy,happy)] ?</p>

<p>I tested the word2vec algorithm in genism, when there is just one word in the training set sentences, (and this word is not included in other sentences), the word2vec algorithm can still construct an embedding vector for this specific word. I am not sure how the algorithm is able to do so.</p>

<p><a href=""https://i.sstatic.net/zQPX6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zQPX6.png"" alt=""enter image description here""></a></p>

<p>===============UPDATE===============================</p>

<p>As the answer posted below, I think the word embedding vector created for the word in the 1-word-sentence is just the random initialization of neural network weights.</p>
",Vectorization & Embeddings,doe gensim word vec word embedding extract training word pair word sentence refer image process word vec skipgram extract training datasets word pair input sentence e g love love may ask word pair sentence contains one word happy happy happy tested word vec algorithm genism one word training set sentence word included sentence word vec algorithm still construct embedding vector specific word sure algorithm able update answer posted think word embedding vector created word word sentence random initialization neural network weight
How to reduce vocabulary using tf.keras.preprocessing.text.Tokenizer?,"<p>I'm using <code>tf.keras.preprocessing.text.Tokenizer</code> to build a vocabulary for my corpus (5 million documents). The tokenizer finds 145K tokens. The issue is that the embedding layer has far too many parameters. </p>

<p>What's a simple way to force the tokenizer to only consider the top N most common words? Then anything outside of that would get a .</p>

<h1>Solution</h1>

<p>As indicated by @MarcoCerliani in the comment, you can simply change the <code>num_words</code> parameter in the tokenizer. That won't change the tokenizer's internal data - it'll instead return <code>OOV</code> tokens for words outside the range specified by <code>num_words</code>.</p>

<pre><code>tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='&lt;OOV&gt;')
tokenizer.fit_on_texts(huge_corpus)

len(tokenizer.word_counts)
&gt;&gt;&gt; (some huge vocabulary size)

# Change vocabulary size
tokenizer.num_words = 100

tokenizer.texts_to_sequences(texts)
&gt;&gt;&gt; (any words in `texts` not within 100 most common words
&gt;&gt;&gt;  will get `1` (out of vocab).
</code></pre>
",Vectorization & Embeddings,reduce vocabulary using tf kera preprocessing text tokenizer using build vocabulary corpus million document tokenizer find k token issue embedding layer ha far many parameter simple way force tokenizer consider top n common word anything outside would get solution indicated marcocerliani comment simply change parameter tokenizer change tokenizer internal data instead return token word outside range specified
Deep learning-Flatten is one of the special form of Embedding?,"<p>In Deep Learning, </p>

<p>The definition of Embedding is making data to dense vector. </p>

<p>Flatten is a widely used concept that makes data in a line. So, we can consider Flatten also return vector. Just except changing feature numbers. </p>

<p>So, Flatten is one of the special form of Embedding? is this logically right?</p>

<p>Flatten ⊂ Embedding ? </p>
",Vectorization & Embeddings,deep learning flatten one special form embedding deep learning definition embedding making data dense vector flatten widely used concept make data line consider flatten also return vector except changing feature number flatten one special form embedding logically right flatten embedding
How to get word vectors from pre-trained word2vec model downloaded from TFHub?,"<p>So I'm using the following word2vec model from TFHub:</p>

<pre><code>embed = hub.load(""https://tfhub.dev/google/Wiki-words-250-with-normalization/2"")
</code></pre>

<p>The type of this object is:</p>

<pre><code>tensorflow.python.saved_model.load.Loader._recreate_base_user_object.&lt;locals&gt;._UserObject
</code></pre>

<p>While I can use the model to embed lists of text, it's not clear to me how I can access the word embeddings themselves. </p>
",Vectorization & Embeddings,get word vector pre trained word vec model downloaded tfhub using following word vec model tfhub type object use model embed list text clear access word embeddings
Full FastText model from KeyedVectors to infer new words in aligned space,"<p>I am working on a NLP problem with gensim that requires the use of multilingual embeddings. I have the already pretrained and aligned .txt embeddings that <a href=""https://fasttext.cc/docs/en/aligned-vectors.html"" rel=""nofollow noreferrer"">FastText provides in their web</a>. Sadly, they don't provide the full model, but these vectors are missing some important vocabulary on my problem and the per-character embedding ability of a FastText model here comes very handy for these cases.</p>

<p>My question:</p>

<ol>
<li>Is there a way to recreate the entire model so I can infer new vocabulary that is also in the vector space of aligned embeddings?</li>
<li>If not, Is there still a way to obtain those terms in that aligned embedding space? Without having to retrain a new entire FastText and then align it the already pre-trained ones? </li>
</ol>
",Vectorization & Embeddings,full fasttext model keyedvectors infer new word aligned space working nlp problem gensim requires use multilingual embeddings already pretrained aligned txt embeddings fasttext provides web sadly provide full model vector missing important vocabulary problem per character embedding ability fasttext model come handy case question way recreate entire model infer new vocabulary also vector space aligned embeddings still way obtain term aligned embedding space without retrain new entire fasttext align already pre trained one
How can I consider word dependence along with the semantic information in information retrieval?,"<p>I am working on a project that text retrieval is an important part of it. There is a reference collection (D), and users can enter queries (Q). Therefore, like a search engine, the goal is to retrieve the most related documents to each query.   </p>

<p>I used pre-trained word embeddings to extract semantic knowledge about each word within a text. I then aggregated the continuous vectors of words to represent each text as a vector (using mean/sum aggregate function). Next, I indexed the source vectors and extracted the most similar vectors to the query vector. However, the result was not acceptable. I also tested the traditional approaches like the BOW technique. While these approaches work very well in some situations, they do not consider semantic and syntactic information (that made them not good for some queries).</p>

<p>Based on my investigation, considering word dependence (for example, co-occurring the words in the same sentence) along with the semantic information (obtained using the pre-trained word embeddings) can be very useful. However, I do not know how to combine them to be applicable in IR. </p>

<p>It should be noted that:</p>

<ul>
<li><p>I'm not looking for paragraph2vec or doc2vec; those require training on a large data corpus, and I don't have a large data corpus. Instead, I want to use an existing word embeddings.</p></li>
<li><p>I'm not looking for a re-ranking technique like learning to rank approach. Instead, <strong>I'm looking for a way to take advantage of both syntactic and semantic information in the representation step i.e. mapping the text or query to a feature vector.</strong> </p></li>
</ul>

<p>Any help would be appreciated. </p>
",Vectorization & Embeddings,consider word dependence along semantic information information retrieval working project text retrieval important part reference collection user enter query q therefore like search engine goal retrieve related document query used pre trained word embeddings extract semantic knowledge word within text aggregated continuous vector word represent text vector using mean sum aggregate function next indexed source vector extracted similar vector query vector however result wa acceptable also tested traditional approach like bow technique approach work well situation consider semantic syntactic information made good query based investigation considering word dependence example co occurring word sentence along semantic information obtained using pre trained word embeddings useful however know combine applicable ir noted looking paragraph vec doc vec require training large data corpus large data corpus instead want use existing word embeddings looking ranking technique like learning rank approach instead looking way take advantage syntactic semantic information representation step e mapping text query feature vector help would appreciated
Differences in encoder - decoder models between Keras and Pytorch,"<p>There seem to be significant, fundamental differences in construction of encoder-decoder models between keras and pytorch. Here is <a href=""https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"" rel=""nofollow noreferrer"">keras' enc-dec blog</a> and here is <a href=""https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"" rel=""nofollow noreferrer"">pytorch's enc-dec blog</a>.</p>

<p>Some differences I noticed are the following: </p>

<ol>
<li>Keras' model directly feeds input to LSTM layer. Whereas Pytorch uses an embedding layer for both the encoder and decoder. </li>
<li>Pytorch uses an embedding layer with no activation in the encoder but uses relu activation for the embedding layer in the decoder.</li>
</ol>

<p>Given these observations, my questions are the following:</p>

<ol>
<li>My understanding is the following, is it correct? The embedding layer is not strictly required but it helps in finding a better and denser representation of the input. It is optional and you can still build a good model without the embedding layer (dependent on the problem). This is why Keras chose not to use it in this particular example. Is this a sound reason or is there more to the story?</li>
<li>Why use an activation for the embedding layer in the decoder but not the encoder?</li>
<li>Why use 'relu' as the activation instead of 'tanh', etc for the embedding layer? What's the intuition here? I've only seen 'relu' applied to data that has spatial relation, not temporal relation. </li>
</ol>
",Vectorization & Embeddings,difference encoder decoder model kera pytorch seem significant fundamental difference construction encoder decoder model kera pytorch kera enc dec blog pytorch enc dec blog difference noticed following kera model directly feed input lstm layer whereas pytorch us embedding layer encoder decoder pytorch us embedding layer activation encoder us relu activation embedding layer decoder given observation question following understanding following correct embedding layer strictly required help finding better denser representation input optional still build good model without embedding layer dependent problem kera chose use particular example sound reason story use activation embedding layer decoder encoder use relu activation instead tanh etc embedding layer intuition seen relu applied data ha spatial relation temporal relation
`King - Man + Woman = Queen` cannot be validated using spaCy word embedding calculations,"<p>Word embeddings are supposed to make calculations using words possible <a href=""https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html"" rel=""nofollow noreferrer"">as explained in this article</a>. However, when using spaCy's pretained word embedding, this can not be reproduced, i.e. the difference between <code>King - Man + Woman</code> and <code>Queen</code> is not close to zero.</p>

<pre><code>import spacy
import en_core_web_sm
nlp = en_core_web_sm.load()
#spacy.load('en_core_web_md')

doc = nlp('queen king woman man')
queen, king, woman, man = doc[0].vector, doc[1].vector, doc[2].vector, doc[3].vector
vec = king - man + woman
vec - queen
</code></pre>

<p>The result is:</p>

<pre><code>array([ 0.10928726,  1.5129069 ,  0.22144175, -1.0195163 , -0.88018465,
    1.0273552 , -0.42121184, -0.6132709 , -5.506116  , -1.8500991 ,
   -0.15576434, -1.1081355 ,  0.33168507, -3.3569758 , -3.671307  ,
    0.41009247,  5.0559406 ,  1.6673484 ,  1.6196246 ,  2.3392878 ,
   -1.4170032 ,  1.0845371 ,  1.1150997 ,  1.4959896 , -5.9387603 ,
    2.71976   , -5.1596265 , -2.1413157 , -2.0650306 , -0.90464056,
   -3.662921  , -1.9780679 ,  0.3792592 , -1.1127007 , -2.763383  ,
   -0.46687317, -3.3972526 , -1.0455723 ,  4.713142  , -1.3429235 ,
    1.4183658 , -1.38419   ,  3.2157912 ,  0.4593829 ,  2.57287   ,
   -5.232533  ,  2.007104  , -0.03439535, -2.5858183 ,  2.3942559 ,
   -2.2274508 ,  1.1235554 ,  1.8343859 , -3.809722  ,  2.3434563 ,
    6.6838984 , -0.79330105, -0.3786683 ,  0.5149512 , -2.567075  ,
   -4.5407395 ,  0.15355158,  0.4791546 ,  2.6068583 ,  0.06677404,
   -0.36967564, -5.109796  ,  0.45319676,  7.158951  ,  1.0552151 ,
   -0.72934663,  1.5460184 , -0.41246212, -3.068016  , -1.2780238 ,
   -2.256475  ,  0.20324552, -0.7423974 ,  2.6825244 , -1.8383589 ,
    2.2891805 ,  1.542151  , -2.3867102 ,  0.03401029, -0.70230985,
    1.4130044 , -2.416402  ,  0.6862675 , -2.270489  ,  3.9625044 ,
    2.463019  ,  1.3068041 ,  3.4472568 ,  5.8497505 ,  7.2417293 ,
   -1.8955674 ], dtype=float32)
</code></pre>

<p>What might be wrong?</p>
",Vectorization & Embeddings,validated using spacy word embedding calculation word embeddings supposed make calculation using word possible explained article however using spacy pretained word embedding reproduced e difference close zero result might wrong
I want to Train 4 more Word2vec models and average the resulting embedding matrices,"<p>I wrote the code below, I used Used spacy to restrict the words in the tweets to content words, i.e., nouns, verbs, and adjectives. Transform the words to lower case and add the POS with an underderscore. E.g.:</p>

<p>love_VERB old-fashioneds_NOUN</p>

<p>now I want to Train 4 more Word2vec models and average the resulting embedding matrices.
but I dont have any idea for it, can you help me please ?</p>

<pre><code># Tokenization of each document
from gensim.models.word2vec import FAST_VERSION
from gensim.models import Word2Vec
import spacy
import pandas as pd
from zipfile import ZipFile
import wget

url = 'https://raw.githubusercontent.com/dirkhovy/NLPclass/master/data/reviews.full.tsv.zip'
wget.download(url, 'reviews.full.tsv.zip')

with ZipFile('reviews.full.tsv.zip', 'r') as zf:
    zf.extractall()

# nrows , max amount of rows
df = pd.read_csv('reviews.full.tsv', sep='\t', nrows=100000)
documents = df.text.values.tolist()

nlp = spacy.load('en_core_web_sm')  # you can use other methods
# excluded tags
included_tags = {""NOUN"", ""VERB"", ""ADJ""}


vocab = [s for s in new_sentences]

sentences = documents[:103]  # first 10 sentences
new_sentences = []
for sentence in sentences:
    new_sentence = []
    for token in nlp(sentence):
        if token.pos_ in included_tags:
            new_sentence.append(token.text.lower()+'_'+token.pos_)
    new_sentences.append(new_sentence)


# initialize model
w2v_model = Word2Vec(
                     size=100,
                     window=15,
                     sample=0.0001,
                     iter=200,
                     negative=5,
                     min_count=1,  # &lt;-- it seems your min_count was too high
                     workers=-1,
                     hs=0
                     )


new_sentences


w2v_model.build_vocab(vocab)

w2v_model.train(vocab, 
                total_examples=w2v_model.corpus_count, 
                epochs=w2v_model.epochs)
w2v_model.wv['car_NOUN']
</code></pre>
",Vectorization & Embeddings,want train word vec model average resulting embedding matrix wrote code used used spacy restrict word tweet content word e noun verb adjective transform word lower case add po underderscore e g love verb old fashioneds noun want train word vec model average resulting embedding matrix dont idea help please
Combine tensor matrix and sparse matrix of same dataset for splitting the data,"<p>I have an array and sparse matrix in tensor type and both are to be feed into the Neural network classifier. Since the data is too large, I need to split it batch-wise. </p>

<p>How should I club my sparse matrix and array together and then split train data batch-wise? </p>

<pre><code>tensor(indices=tensor([[     0,      0,      0,  ..., 426017, 426017, 426017],
                       [   223,    310,   1045,  ...,     39,     14,    542]]),
       values=tensor([0.0730, 0.0722, 0.0911,  ..., 0.3244, 0.0883, 0.1659]),
       size=(426018, 10000), nnz=19173512, layout=torch.sparse_coo)
</code></pre>

<pre><code>torch.Size([426018, 16])
tensor([[0.0154, 0.3296, 0.7500,  ..., 0.7628, 0.1643, 0.0092],
        [0.1041, 0.6322, 1.0000,  ..., 0.7612, 0.1629, 0.1283],
        [0.4987, 0.3387, 0.7500,  ..., 0.8692, 0.1879, 0.0150],
        ...,
        [0.1158, 0.6453, 1.0000,  ..., 0.6511, 0.2130, 0.0590],
        [0.0751, 0.2653, 0.7500,  ..., 0.7232, 0.1644, 0.0208],
        [0.4051, 0.3801, 1.0000,  ..., 0.8479, 0.2427, 0.1179]],
       dtype=torch.float64)

</code></pre>

<p>Above are two parts of same dataset in which one are numerical features(16) and second are tfidf features in sparse matrix.So in order to split it batch wise I need to club them together.
I know one function <code>data.utils.data.DataLoader</code> but dont't know how to combine my dataset to feed into it.</p>
",Vectorization & Embeddings,combine tensor matrix sparse matrix dataset splitting data array sparse matrix tensor type feed neural network classifier since data large need split batch wise club sparse matrix array together split train data batch wise two part dataset one numerical feature second tfidf feature sparse matrix order split batch wise need club together know one function dont know combine dataset feed
"implement word2vec, but I got error, that word car_NOUN is in the vocabulary","<p>I wrote the code below: to implement the word2vec on it, now im testing to get embedding for w2v_model.wv['car_NOUN']   but I get error as below : <strong>""word 'car_NOUN' not in vocabulary""</strong> but im sure that word car_NOUN is in the vocabulary , what is the problem ?
can someone help me?</p>

<p>about code : I used Use spacy to restrict the words in the tweets to content words, i.e., nouns, verbs, and adjectives. Transform the words to lower case and add the POS with an underderscore. E.g.:love_VERB .then I wanted to implement word2vec on new list but I came up with that error</p>

<p>love_VERB old-fashioneds_NOUN</p>

<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-145-f6fb9c62175c&gt; in &lt;module&gt;()
----&gt; 1 w2v_model.wv['car_NOUN']

2 frames
/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py in word_vec(self, word, use_norm)
    450             return result
    451         else:
--&gt; 452             raise KeyError(""word '%s' not in vocabulary"" % word)
    453 
    454     def get_vector(self, word):

KeyError: ""word 'car_NOUN' not in vocabulary""
</code></pre>

<pre><code>! pip install wget
import wget
url = 'https://raw.githubusercontent.com/dirkhovy/NLPclass/master/data/reviews.full.tsv.zip'
wget.download(url, 'reviews.full.tsv.zip')


from zipfile import ZipFile
with ZipFile('reviews.full.tsv.zip', 'r') as zf:
    zf.extractall()


import pandas as pd
df = pd.read_csv('reviews.full.tsv', sep='\t', nrows=100000) # nrows , max amount of rows 
documents = df.text.values.tolist()
print(documents[:4])


import spacy

nlp = spacy.load('en_core_web_sm') #you can use other methods
# excluded tags
included_tags = {""NOUN"", ""VERB"", ""ADJ""}
#document = [line.strip() for line in open('moby_dick.txt', encoding='utf8').readlines()]

sentences = documents[:103] #first 10 sentences
new_sentences = []
for sentence in sentences:
    new_sentence = []
    for token in nlp(sentence):
        if token.pos_  in included_tags:
            new_sentence.append(token.text.lower()+'_'+token.pos_)
    new_sentences.append("" "".join(new_sentence))

def convert(new_sentences): 
    return ' '.join(new_sentences).split() 

x=convert(new_sentences)


from gensim.models import Word2Vec
from gensim.models.word2vec import FAST_VERSION


# initialize model
w2v_model = Word2Vec(size=100,
                     window=15,
                     sample=0.0001,
                     iter=200,
                     negative=5, 
                     min_count=100,
                     workers=-1, 
                     hs=0
)

w2v_model.build_vocab(x)

w2v_model.train(x, 
                total_examples=w2v_model.corpus_count, 
                epochs=w2v_model.epochs)


w2v_model.wv['car_NOUN']
</code></pre>
",Vectorization & Embeddings,implement word vec got error word car noun vocabulary wrote code implement word vec im testing get embedding w v model wv car noun get error word car noun vocabulary im sure word car noun vocabulary problem someone help code used use spacy restrict word tweet content word e noun verb adjective transform word lower case add po underderscore e g love verb wanted implement word vec new list came error love verb old fashioneds noun
why Lsimodel from Gensim show different output while taking the same input?,"<p>I am doing a natural language process project, when I try to use gensim'API to get LSI similarity matrix, every time I run my code, the LSImodel gives me a different similarity matrix. they are not totally different, but slightly different, like last time one of the number is -0.42562, but it change to -0.42116 next time I run my code. It makes my rest analysis change totally.</p>

<pre><code>Lsi = gensim.models.LsiModel
lsimodel = Lsi(corpus_tfidf, id2word=dictionary, num_topics=20)
lsi_similarity = similarities.MatrixSimilarity(lsimodel[corpus_tfidf])
</code></pre>

<p>I have checked that my input corpus_tfidf and dictionary is the same every time. why would this happen? is there some solution for it?</p>
",Vectorization & Embeddings,lsimodel gensim show different output taking input natural language process project try use gensim api get lsi similarity matrix every time run code lsimodel give different similarity matrix totally different slightly different like last time one number change next time run code make rest analysis change totally checked input corpus tfidf dictionary every time would happen solution
Wu-palmer Semantic Similarity for Node.js,"<p>I’ve been searching for wu-palmer Semantic Similarity implementation for node js, but no luck. Could you please point me Where I could get it.</p>
",Vectorization & Embeddings,wu palmer semantic similarity node j searching wu palmer semantic similarity implementation node j luck could please point could get
r + tfidf and inverse document frequency,"<p>I was hoping that someone can explain a specific part of an academic paper and assist in writing R code for that section: </p>

<p><strong>Name of Paper</strong></p>

<ul>
<li>Large-scale Analysis of Counseling Conversations:
An Application of Natural Language Processing to Mental Health (<a href=""https://cs.stanford.edu/people/jure/pubs/counseling-tacl16.pdf"" rel=""nofollow noreferrer"">https://cs.stanford.edu/people/jure/pubs/counseling-tacl16.pdf</a>)</li>
</ul>

<p>On page 5, we have the following snippet:</p>

<p>""
...build a TF-IDF vector of word occurrences
to represent the language of counselors within this
subset. We use the global inverse document (i.e.,
conversation) frequencies instead of the ones from
each subset to make the vectors directly comparable
and control for different counselors having different numbers of conversations by weighting conversations so all counselors have equal contributions.
""</p>

<p>What does the paper mean by ""global inverse document frequency""?
How can I code this in R with the different subsets (positive and negative counsellors for example)</p>

<p>Here is my sample code:</p>

<pre><code>corp_pos_1 &lt;- Corpus(VectorSource(positive_chats$Text1))
#corp_pos_1  &lt;- tm_map(corp_pos_1, removeWords, stopwords(""english""))

tdm_pos_1 &lt;- DocumentTermMatrix(corp_pos_1,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
ui = unique(tdm_pos_1 $i)
tdm_pos_1  = tdm_pos_1 [ui,]

cosine_tdm_pos_1 &lt;- crossprod_simple_triplet_matrix(tdm_pos_1)/(sqrt(col_sums(tdm_pos_1^2) %*% t(col_sums(tdm_pos_1^2))))
</code></pre>

<p>In the code 'pos' stands for positive, and 'neg' would stand for negative. 
The number at the end of the variable end shows the part of the chunk being calculated.</p>

<p>Now I have them chunked in 5 different parts trying to follow this paper. But how would I be able to calculate ""global inverse document frequency""? </p>

<p>I think I have found this stackoverflow question from before but I am still not understanding the paper + what I need to do in R.
<a href=""https://stackoverflow.com/questions/56365000/r-weighted-inverse-document-frequency-tfidf-similarity-between-strings"">R: weighted inverse document frequency (tfidf) similarity between strings</a></p>
",Vectorization & Embeddings,r tfidf inverse document frequency wa hoping someone explain specific part academic paper assist writing r code section name paper large scale analysis counseling conversation application natural language processing mental health page following snippet build tf idf vector word occurrence represent language counselor within subset use global inverse document e conversation frequency instead one subset make vector directly comparable control different counselor different number conversation weighting conversation counselor equal contribution doe paper mean global inverse document frequency code r different subset positive negative counsellor example sample code code po stand positive neg would stand negative number end variable end show part chunk calculated chunked different part trying follow paper would able calculate global inverse document frequency think found stackoverflow question still understanding paper need r href weighted inverse document frequency tfidf similarity string
How to use k-means algorithm to do attribute clustering after NER?,"<p>I am reading <a href=""http://aircconline.com/ijnlc/V8N5/8519ijnlc03.pdf"" rel=""nofollow noreferrer"">this paper</a> and in 3.2.1 sub-section, first paragraph last three lines,</p>

<blockquote>
  <p>To map the named entity candidates to the
  standard attribute names, we employed the k-means algorithm to cluster the identified named
  entities by computing the cosine similarities between them based on Term Frequency–Inverse
  Document Frequency (TFIDF).""</p>
</blockquote>

<p>Can anyone explain what does that mean? If possible give an example about the implementation scenario.</p>
",Vectorization & Embeddings,use k mean algorithm attribute clustering ner reading paper sub section first paragraph last three line map named entity candidate standard attribute name employed k mean algorithm cluster identified named entity computing cosine similarity based term frequency inverse document frequency tfidf anyone explain doe mean possible give example implementation scenario
Using fasttext pre-trained models as an Embedding layer in Keras,"<p>My goal is to create text generator which is going to generate non-english text based on learning set I provide to it.</p>

<p>I'm currently at the stage of figuring out how the model actually should looks like. I'm trying to implement fasttext pre-trained model as an Embedding layer in my net. But due to that I have some questions. </p>

<p>1) How to properly prepare fasttext model? Should I just download vectors, for the language that I need, and include them in the project, or I have to build it first using <code>skipgram</code> or <code>cbow</code> or in some other way?</p>

<p>2) How am I suppose to exchange Keras Embedding() with fasttext model?</p>

<p>Now I have something like this: </p>

<pre><code>    model = Sequential()
    model.add(Embedding(vocabulary_size, seq_len, input_length=seq_len, output_dim=OUTPUT_DIM))
    model.add(LSTM(50, return_sequences=True))
</code></pre>

<p>And instead of <code>model.add(Embedding())</code> I wish to put fasttext vector.</p>

<p>I hope I explained it clearly.</p>
",Vectorization & Embeddings,using fasttext pre trained model embedding layer kera goal create text generator going generate non english text based learning set provide currently stage figuring model actually look like trying implement fasttext pre trained model embedding layer net due question properly prepare fasttext model download vector language need include project build first using way suppose exchange kera embedding fasttext model something like instead wish put fasttext vector hope explained clearly
Gensim Word2Vec model floating point,"<p>I have trained a word2vec model using gensim. In the models matrix some values' floating point looks like this: ""-7.18556e-05""</p>

<p>I need to use the values on the matrix as a string. Is there a way to remove those ""e-05"",""e-04"" etc.?</p>

<pre><code>import nltk
from gensim.models import Word2Vec
from nltk.corpus import stopwords

text = ""My text is here""
sentences = nltk.sent_tokenize(text)
for i in range(len(sentences)):
    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]

model = Word2Vec(sentences, min_count=1)

words = model.wv.vocab

for word in words:
    matrix = model.wv[words.keys()]
</code></pre>
",Vectorization & Embeddings,gensim word vec model floating point trained word vec model using gensim model matrix value floating point look like e need use value matrix string way remove e e etc
Getting less than 1 score while trying to check the cosine similarities of same document,"<p>I have used doc2vec to find the similarities in multiple documents, but when i am checking the same document which i created my model, the score should be '1' right? as the used document and the to be predict document is same. Sadly, I am getting different score when trying to find the similarities. Below is the attached code. Please tell me how to make this right, I can't find what is wrong here. Pleas help me...<a href=""https://i.sstatic.net/97bk9.png"" rel=""nofollow noreferrer"">doc2vec - calculating document cosine similarity</a></p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
df['Tagged_data'] = df['sent_to_word_tokenize_text'].apply(lambda x: [TaggedDocument(d, [i]) for i, d in enumerate(x)])
sadguru_model = Doc2Vec(df['Tagged_data'][0], vector_size = 1000, window = 500, dm = 1, min_count = 1, workers = 2, epochs = 100) 
test_doc = word_tokenize(' '.join([word for word in df['Sentence_Tokenized_Text'][0]])) 
# Sadguru model document 
index0 = sadguru_model.docvecs.most_similar(positive=sadguru_model.infer_vector(test_doc)], topn =1) output: index0 = [(4014, 0.5270981788635254)] 
</code></pre>

<p>output: <code>index0 = [(4014, 0.5270981788635254)]</code></p>
",Vectorization & Embeddings,getting le score trying check cosine similarity document used doc vec find similarity multiple document checking document created model score right used document predict document sadly getting different score trying find similarity attached code please tell make right find wrong plea help doc vec calculating document cosine similarity output
Semantic Similarity across multiple languages,"<p>I am using word embeddings for finding similarity between two sentences. Using word2vec, I also get a similarity measure if one sentence is in English and the other one in Dutch (though not very good). </p>

<p>So I started wondering if it's possible to compute the similarity between two sentences in two different languages (without an explicit translation), especially if the languages have some similarities (Englis/Dutch)?</p>
",Vectorization & Embeddings,semantic similarity across multiple language using word embeddings finding similarity two sentence using word vec also get similarity measure one sentence english one dutch though good started wondering possible compute similarity two sentence two different language without explicit translation especially language similarity englis dutch
Join a few elements of the list in Python,"<p>Please have a look into the code below</p>

<pre><code>from transformers import GPT2Tokenizer, GPT2Model

text = ""Here is the sentence I want embeddings for.""
#marked_text = ""[CLS] "" + text + "" [SEP]""
# Tokenize our sentence with the GPT2 tokenizer.
tokenized_text = tokenizer.tokenize(text)
print(tokenized_text)
</code></pre>

<p>Output of the above code is shown below :-</p>

<pre><code>['Here', 'Ġis', 'Ġthe', 'Ġsentence', 'ĠI', 'Ġwant', 'Ġembed', 'd', 'ings', 'Ġfor', '.']
</code></pre>

<p>But I want an output like this:-</p>

<pre><code>['Here', 'Ġis', 'Ġthe', 'Ġsentence', 'ĠI', 'Ġwant', 'Ġembeddings', 'Ġfor', '.']
</code></pre>

<p>So, while tokenizing the text, the tokenizer has splitted the word 'embeddings' because it doesn't have this word in its dictionary. But, I don't want this happening. I want the whole word 'embedding' to be tokenized as it is.</p>

<p>I don't know how to solve this. Also kindly note that tokenized_text is a List object.
Please help.</p>

<p><strong>EDIT 1 :</strong>
I came with this solution </p>

<pre><code>tokenized_text[6:9] = [''.join(tokenized_text[6:9])]
print(tokenized_text)
</code></pre>

<p>And it gave me the desired output as well but I don't want to give the numbers here specifically. I want the machine to figure it out for itself.
Like whichever element in the list doesn't start with that 'G' special character, that element needs to be joined with the previous element and so on.</p>

<p><strong>EDIT 2 :</strong>
I came across another approach and here's the code for it, but it doesn't work probably because of wrong for loops.</p>

<pre><code>for i in range(1, len(tokenized_text)):
  if tokenized_text[i].startswith('Ġ'):
    i += 1 
  else:
    for j in range(i, len(tokenized_text)):
      if tokenized_text[j].startswith(""Ġ"") :
        pass
      else :
        j += 1


tokenized_text[i-1:j] = [''.join(tokenized_text[i-1:j])]
print(tokenized_text)
</code></pre>
",Vectorization & Embeddings,join element list python please look code output code shown want output like tokenizing text tokenizer ha splitted word embeddings word dictionary want happening want whole word embedding tokenized know solve also kindly note tokenized text list object please help edit came solution gave desired output well want give number specifically want machine figure like whichever element list start g special character element need joined previous element edit came across another approach code work probably wrong loop
Combine additional data to my TFIDF array,"<p>I'm trying to create a text classification model using scikit-learn. At first, I was using only the text's tfidf array as a feature. The structure of my dataset can be seen below (the dataset is stored in a pandas dataframe called <code>df</code>):</p>

<pre><code>&gt;&gt;&gt;df.head(2)

       id_1    id_2    id_3    target    text
       11      454     320     197       some text here
       15      440     111     205       text goes here too

&gt;&gt;&gt;df.info()

    Data columns (total 5 columns):
     #   Column    Non-Null Count   Dtype 
    ---  ------    --------------   ----- 
     0   id_1      500 non-null     uint16
     1   id_2      500 non-null     uint16
     2   id_3      500 non-null     uint16
     3   target    500 non-null     uint16
     4   text      500 non-null     object
</code></pre>

<p>So, I split the train/test datasets and proceeded with creating the tfidf vector and transforming the data for training and testing.</p>

<pre><code>X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], random_state=0)

vectorizer = TfidfVectorizer(max_features=500, decode_error=""ignore"", ngram_range=(1, 2))
vectorizer.fit(X_train)

X_train_tfidf = vectorizer.transform(X_train)
X_test_tfidf  = vectorizer.transform(X_test)
</code></pre>

<p>So far apparently the code is working ok. However, there was a need to improve the algorithm, including yet another feature. For this improvment, I want to add the <code>id_1</code> column to my features (it can be an important information to our ML model). So, in addition to my tfidf matrix, I would like to add this column (<code>id_1</code>) with my new feature, so that I can pass it as a parameter to train the model.</p>

<p>What I have tried:</p>

<pre><code>X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], random_state=0)

vectorizer = TfidfVectorizer(max_features=500, decode_error=""ignore"", ngram_range=(1, 2))
vectorizer.fit(X_train)

X_train_tfidf = vectorizer.transform(X_train)
X_test_tfidf  = vectorizer.transform(X_test)

X_train_all_features = pd.concat([pd.DataFrame(X_train_tfidf.toarray()), df['id_1']], axis = 1)
</code></pre>

<p>So, the shape of my structure is</p>

<pre><code>&gt;&gt;&gt;print(X_train_tfidf.shape)

(37, 500) # as expected (I'm loading 50 lines, so this is about 75%)

&gt;&gt;&gt;print(X_train_all_features.shape)

(50, 501) # n of columns is expected, but not the lines, because the df[id_1] was not splited in train_test_split function
</code></pre>

<p>In a nutshel, I want pass to my ML algorithm something like the image below - my tfidf vector and my <code>id_1</code> features:</p>

<p><a href=""https://i.sstatic.net/szpQR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/szpQR.png"" alt=""tfidf concat id_1""></a></p>

<p>I feel that I am missing something extremely basic, but even with all the research I have been able to solve my problem satisfactorily. I'm honestly lost in that part of the problem and I don't know how to evolve from here</p>
",Vectorization & Embeddings,combine additional data tfidf array trying create text classification model using scikit learn first wa using text tfidf array feature structure dataset seen dataset stored panda dataframe called split train test datasets proceeded creating tfidf vector transforming data training testing far apparently code working ok however wa need improve algorithm including yet another feature improvment want add column feature important information ml model addition tfidf matrix would like add column new feature pas parameter train model tried shape structure nutshel want pas ml algorithm something like image tfidf vector feature feel missing something extremely basic even research able solve problem satisfactorily honestly lost part problem know evolve
why do i get different result of cosine similarity when compare to library result,"<p>i try to compute the similarity of two words using cosine distance (<a href=""https://stackoverflow.com/questions/29484529/cosine-similarity-between-two-words-in-a-list"">source</a>).
this is the code :</p>

<pre><code>def word2vec(word):
    from collections import Counter
    from math import sqrt

    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the ""length"" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))

    # return a tuple
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]
</code></pre>

<p>the similarity is 0.1889822365046136 when i called:</p>

<pre><code>cosdis(word2vec('tahu') , word2vec('tempe'))
</code></pre>

<p>when try to compare with the result of similarity using library (gensim word2vec) the result is different (for example the difference is 0.2). why is that?
this is how i get the similarity using library:</p>

<pre><code>from gensim.models import Word2Vec
modelword2vec = Word2Vec.load(""/idwiki_word2vec_300.model"")
modelword2vec.similarity('tahu' , 'tempe')
</code></pre>

<p>the similarity is 0.21785985</p>
",Vectorization & Embeddings,get different result cosine similarity compare library result try compute similarity two word using cosine distance href code p similarity called try compare result similarity using library gensim word vec result different example difference get similarity using library similarity
NLP - Trying to find similarities between different target groups based on input dimensions,"<p>So I have a dataset which has one description column (an IT trouble ticket description) and one target column (grouping of the ticket e.g. ticket belongs to Group 0 or Group 1 - the group type e.g. access issues is not provided).</p>

<p>The thing is: I have 45 different target variables - targets are Group 0, Group 1,...... Group 45. There is a pretty long tail with some of these group having less than 0.1% of the total tickets. Now instead of just directly clubbing them together to form a single group, I wanted to see if there was any way to club these smaller groups with other groups which are 'similar' to them based on the IT trouble ticket description. For example, if a larger group has tickets describing access issues and a smaller group has tickets pertaining to login issues (depending on the text description), I would prefer to club these two groups together.</p>

<p>I thought of creating a separate Word2Vec or Glove embedding for each Group but then am unable to figure out how to find similarities between these vectors. Further, creating 45 different Word2Vec embeddings is pretty computationally painful. So I am a little stuck on this. Any ideas on how to approach this? Any help would be great</p>

<p>Thanks ! </p>
",Vectorization & Embeddings,nlp trying find similarity different target group based input dimension dataset ha one description column trouble ticket description one target column grouping ticket e g ticket belongs group group group type e g access issue provided thing different target variable target group group group pretty long tail group le total ticket instead directly clubbing together form single group wanted see wa way club smaller group group similar based trouble ticket description example larger group ha ticket describing access issue smaller group ha ticket pertaining login issue depending text description would prefer club two group together thought creating separate word vec glove embedding group unable figure find similarity vector creating different word vec embeddings pretty computationally painful little stuck idea approach help would great thanks
Cosine Similarity between Lists of Sentences using Doc2Vec,"<p>I'm new to NLP but I'm trying to match a list of sentences to another list of sentences in Python based on their semantic similarity. For example,</p>

<pre><code>list1 = ['what they ate for lunch', 'height in inches', 'subjectid']
list2 = ['food eaten two days ago', 'height in centimeters', 'id']
</code></pre>

<p>Based on previous posts and prior knowledge, it seemed the best way was to create document vectors of each sentence and compute the cosine similarity score between lists. Other posts I've found with regards to Doc2Vec, as well as the tutorial, seem focused on prediction. <a href=""https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings"">This post</a> does a good job doing the calculation by hand, but I thought it was possible for Doc2Vec to do that already. The code I'm using is</p>

<pre><code>import gensim
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

def build_model(train_docs, test_docs, comp_docs):
    '''
    Parameters
    -----------
    train_docs: list of lists - combination of known both sentence list
    test_docs: list of lists - one of the sentence lists
    comp_docs: list of lists - combined sentence lists to match the index to the sentence 
    '''
    # Train model
    model = Doc2Vec(dm = 0, dbow_words = 1, window = 2, alpha = 0.2)#, min_alpha = 0.025)
    model.build_vocab(train_docs)
    for epoch in range(10):
        model.train(train_docs, total_examples = model.corpus_count, epochs = epoch)
        #model.alpha -= 0.002
        #model.min_alpha = model.alpha


    scores = []

    for doc in test_docs:
        dd = {}
        # Calculate the cosine similarity and return top 40 matches
        score = model.docvecs.most_similar([model.infer_vector(doc)],topn=40)
        key = "" "".join(doc)
        for i in range(len(score)):
            # Get index and score
            x, y = score[i]
            #print(x)
            # Match sentence from other list
            nkey = ' '.join(comp_docs[x])
            dd[nkey] = y
        scores.append({key: dd})

    return scores
</code></pre>

<p>which works to calculate the similarity scores, but the issue here is that I have to train the model on all the sentences from both lists or one of the lists, then match. Is there a way to use Doc2Vec to just get the vectors, then compute the cosine similarity? To be clear, I'm trying to find the most similar sentences between lists. I'd expect an output like</p>

<pre><code>scores = []
for s1 in list1:
    for s2 in list2:
        scores.append((s1, s2, similarity(s1, s2)))

print(scores)
[('what they ate for lunch', 'food eaten two days ago', 0.23567),
 ('what they ate for lunch', 'height in centimeters', 0.120),
 ('what they ate for lunch', 'id', 0.01023),
 ('height in inches', 'food eaten two days ago', 0.123),
 ('height in inches', 'height in centimeters', 0.8456),
 ('height in inches', 'id', 0.145),
 ('subjectid', 'food eaten two days ago', 0.156),
 ('subjectid', 'height in centimeters', 0.1345),
 ('subjectid', 'id', 0.9567)]
</code></pre>
",Vectorization & Embeddings,cosine similarity list sentence using doc vec new nlp trying match list sentence another list sentence python based semantic similarity example based previous post prior knowledge seemed best way wa create document vector sentence compute cosine similarity score list post found regard doc vec well tutorial seem focused prediction href post doe good job calculation hand thought wa possible doc vec already code using work calculate similarity score issue train model sentence list one list match way use doc vec get vector compute cosine similarity clear trying find similar sentence list expect output like
Glove text pre-processing,"<p>I noticed in techniques, people convert text URLs, number, and dates to    . Does the glove dataset has embedding trained for these placeholders. Can I feed them directly into the dataset?</p>
",Vectorization & Embeddings,glove text pre processing noticed technique people convert text url number date doe glove dataset ha embedding trained placeholder feed directly dataset
How to compare cosine similarities across three pretrained models?,"<p>I have two corpora - one with all women leader speeches and the other with men leader speeches. I would like to test the hypothesis that cosine similarity between two words in the one corpus is significantly different than cosine similarity between the same two words in another corpus. Is such a t-test (or equivalent) logical and possible?</p>

<p>Further, if the cosine similarities are different across the two corpora, how could I examine if cosine similarity between the same two words in a third corpus is more similar to the first or the second corpus?</p>
",Vectorization & Embeddings,compare cosine similarity across three pretrained model two corpus one woman leader speech men leader speech would like test hypothesis cosine similarity two word one corpus significantly different cosine similarity two word another corpus test equivalent logical possible cosine similarity different across two corpus could examine cosine similarity two word third corpus similar first second corpus
Literary author classification from book content,"<p>I'm trying to build a model that's able to classify the author of a book (just fiction books for the moment) by looking at its text. At first, I just worked with authors that had at least 30 books. In my dataset, that was ~2000 books written by ~30 authors in total, and that went pretty great, so I decided to raise the stakes. By lowering the threshold to 10 books, the database grew to ~400 authors and ~9000 books.</p>

<p>The way I classified the books is as follows: I removed punctuation, newlines, extra spaces, and stopwords from every text, then I extracted the features with sklearn CountVectorizer and TfidfVectorizer. With this approach, the F1 score was 0.95 for the 2000 books db and 0.62 for the 9000 books db. In both cases I used the SGD model from scikit learn.</p>

<p>The training with the larger dataset was much slower, so I decided to get 5000 random words from each book and use only those. This decreased the training time from 20 minutes to only 2 minutes, and enabled me to try something different. There wasn't a big difference in the various metrics, so I decided to try and remove the tfidf features. Using only CountVectorizer I got a 0.87 F1 score.</p>

<p>I'm not sure why this is happening, my expectation was that the tfidf would make things easier and therefore better my score. In theory, tfidf should help with the sparsity of the matrix. Do you have any clue on why removing it improved the score instead?</p>

<p>Are there other strategies, maybe better suited to work with long texts? Should I consider trimming the dataset in a certain way?</p>
",Vectorization & Embeddings,literary author classification book content trying build model able classify author book fiction book moment looking text first worked author least book dataset wa book written author total went pretty great decided raise stake lowering threshold book database author book way classified book follows removed punctuation newlines extra space stopwords every text extracted feature sklearn countvectorizer tfidfvectorizer approach f score wa book db book db case used sgd model scikit learn training larger dataset wa much slower decided get random word book use decreased training time minute minute enabled try something different big difference various metric decided try remove tfidf feature using countvectorizer got f score sure happening expectation wa tfidf would make thing easier therefore better score theory tfidf help sparsity matrix clue removing improved score instead strategy maybe better suited work long text consider trimming dataset certain way
Is it possible to compare similarity scores across two word embeddings repository?,"<p>In my study, I am exploring if there is a statistically significant ideological bias in one set of media as compared to another. I was hoping to explore this using the word embeddings approach. </p>

<p>Let us take US and UK news media for example. If I build a corpora of all US media articles for a given time period and a separate corpora of all UK media articles for the same period, train them each using the same word embeddings algorithm (<code>gensim/word2vec/fasttext</code>) with the same set of parameters (e.g., window and vector size), is it possible to test if cosine similarity obtained between a pair of words in the US corpora is statistically significantly larger than cosine similarity obtained between the same pair of words in the UK corpora?</p>

<p>Many thanks for your help!</p>
",Vectorization & Embeddings,possible compare similarity score across two word embeddings repository study exploring statistically significant ideological bias one set medium compared another wa hoping explore using word embeddings approach let u take u uk news medium example build corpus u medium article given time period separate corpus uk medium article period train using word embeddings algorithm set parameter e g window vector size possible test cosine similarity obtained pair word u corpus statistically significantly larger cosine similarity obtained pair word uk corpus many thanks help
Word2Vec Giving Characters instead of Words,"<p>I have tokenized my strings and made a Pandas column out of them and if I print the column <code>df['word_splits']</code> it looks like this.</p>

<pre><code>0    ['explanation', 'why', 'the', 'edits', 'made',...
1    [""d'aww"", '!', 'he', 'matches', 'this', 'backg...
2    ['hey', 'man', ',', ""i'm"", 'really', 'not', 't...
3    ['more', 'i', ""can't"", 'make', 'any', 'real', ...
4    ['you', ',', 'sir', ',', 'are', 'my', 'hero', ...
Name: word_splits, dtype: object
</code></pre>

<p>Next, I'm running Word2Vec</p>

<pre><code>model = gensim.models.Word2Vec(sentences=df[""word_splits""])
</code></pre>

<p>When I print out the vocabulary, using</p>

<pre><code>words = list(model.wv.vocab)
print(words)
</code></pre>

<p>I'm getting characters instead of a long list of words (vocabulary).</p>

<pre><code>['[', ""'"", 'e', 'x', 'p', 'l', 'a', 'n', 't', 'i', 'o', ',', ' ', 'w', 'h', 'y', 'd', 's', 'm', 'u', 'r', 'c', 'f', 'v', '?', '""', 'j', 'g', 'k', '.', ']', '!', 'b', '-', 'q', 'z']
</code></pre>

<p>Not sure what I'm doing wrong. </p>
",Vectorization & Embeddings,word vec giving character instead word tokenized string made panda column print column look like next running word vec print vocabulary using getting character instead long list word vocabulary sure wrong
Implementation details of positional encoding in transformer model?,"<p>How exactly does this positional encoding being calculated?</p>

<p>Let's assume a machine translation scenario and these are input sentences,</p>

<pre><code>english_text = [this is good, this is bad]
german_text = [das ist gut, das ist schlecht]
</code></pre>

<p>Now our input vocabulary size is 4 and embedding dimension is 4.</p>

<pre><code>#words     #embeddings
this     - [0.5, 0.2, 0.3, 0.1]
is       - [0.1, 0.2, 0.5, 0.1]
good     - [0.9, 0.7, 0.9, 0.1]
bad      - [0.7, 0.3, 0.4, 0.1]
</code></pre>

<p>As per transformer paper we add the <strong>each word position encoding</strong> with <strong>each word embedding</strong> and then pass it to encoder like seen in the image below,</p>

<p><a href=""https://i.sstatic.net/E1aEA.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/E1aEA.jpg"" alt=""attention is all you need""></a></p>

<p>As far as the  paper is concerned they given this formula for calculating position encoding of each word,
<a href=""https://i.sstatic.net/vN3p5.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vN3p5.jpg"" alt=""attention paper""></a></p>

<p>So, this is how I think I can implement it,</p>

<pre><code>d_model = 4 # Embedding dimension

positional_embeddings = np.zeros((max_sentence_length, d_model))

max_sentence_length = 3 # as per my examples above

for position in range(maximum_sentence_length):
    for i in range(0, d_model, 2):
       positional_embeddings[position, i] = (
                                          sin(position / (10000 ** ( (2*i) / d_model) ) )
                                            )
       positional_embeddings[position, i + 1] = (
                                              cos(position / (10000 ** ( (2 * (i + 1) ) / d_model) ) )
                                                )
</code></pre>

<p>Then, the new embedding vector will be </p>

<pre><code>[[0.5, 0.2, 0.3, 0.1], 
 [0.1, 0.2, 0.5, 0.1], 
 [0.9, 0.7, 0.9, 0.1]] + positional_embeddings = NEW EMBEDDINGS

 ## shapes
  3 x 4                + 3 x 4                 = 3 x 4     
</code></pre>

<p>Is this how the calculation will be carried out in the implementation? Do correct me if there's any mistake in my above pseudo implementation.</p>

<p>If everything is correct then <strong><em>I have three doubts</em></strong> hope someone can clear them,</p>

<p>1) From the above implementation we are using sin formula for even positions and cos formula for odd positions but I couldn't understand the reason behind it? I read that it's taking use of cyclic properties but couldn't understand it.</p>

<p>2) Is there a reason behind choosing <code>10000/(2i/d)</code> or <code>10000/(2i+1/d)</code> as scaling factor in formula.</p>

<p>3) All the sentence will not be equal to max sentence length so we might have to padded the sentence so do we also calculate positional encondings to padding tokens.</p>
",Vectorization & Embeddings,implementation detail positional encoding transformer model exactly doe positional encoding calculated let assume machine translation scenario input sentence input vocabulary size embedding dimension per transformer paper add word position encoding word embedding pas encoder like seen image far paper concerned given formula calculating position encoding word think implement new embedding vector calculation carried implementation correct mistake pseudo implementation everything correct three doubt hope someone clear implementation using sin formula even position co formula odd position understand reason behind read taking use cyclic property understand reason behind choosing scaling factor formula sentence equal max sentence length might padded sentence also calculate positional encondings padding token
"How to implement network using Bert as a paragraph encoder in long text classification, in keras?","<p>I am doing a long text classification task, which has more than 10000 words in doc, I am planing to use Bert as a paragraph encoder, then feed the embeddings of paragraph to BiLSTM step by step.
The network is as below:</p>
<blockquote>
<p>Input: (batch_size, max_paragraph_len, max_tokens_per_para,embedding_size)</p>
<p>bert layer: (max_paragraph_len,paragraph_embedding_size)</p>
<p>lstm layer: ???</p>
<p>output layer: (batch_size,classification_size)</p>
</blockquote>
<p>How to implement it with keras?
I am using keras's load_trained_model_from_checkpoint to load bert model</p>
<pre><code>bert_model = load_trained_model_from_checkpoint(
        config_path,
        model_path,
        training=False,
        use_adapter=True,
        trainable=['Encoder-{}-MultiHeadSelfAttention-Adapter'.format(i + 1) for i in range(layer_num)] +
            ['Encoder-{}-FeedForward-Adapter'.format(i + 1) for i in range(layer_num)] +
            ['Encoder-{}-MultiHeadSelfAttention-Norm'.format(i + 1) for i in range(layer_num)] +
            ['Encoder-{}-FeedForward-Norm'.format(i + 1) for i in range(layer_num)],
        )
</code></pre>
",Vectorization & Embeddings,implement network using bert paragraph encoder long text classification kera long text classification task ha word doc planing use bert paragraph encoder feed embeddings paragraph bilstm step step network input batch size max paragraph len max token per para embedding size bert layer max paragraph len paragraph embedding size lstm layer output layer batch size classification size implement kera using kera load trained model checkpoint load bert model
How to save custom embedding matrix to .txt file format?,"<p>I have made a dictionary which contains word and its corresponding word vector in the following format:</p>

<pre><code>{'word1': array([ 4.530e-02, -1.170e-02, -1.201e-01,  2.439e-01,  4.670e-02d], type=float32),
'word2': array([ 4.530e-02, -1.170e-02, -1.201e-01,  2.439e-01,  4.670e-02d], type=float32)}
</code></pre>

<p>I would like to save this dictionary to custom_embeddings.txt file in the following format:</p>

<p>The format of your custom_embeddings.txt file needs to be the token followed by the values of each of the dimensions for the embedding, all separated by a single space, e.g. here's two tokens with 5 dimensional embeddings:</p>

<pre><code>word1 4.530e-02 -1.170e-02 -1.201e-01  2.439e-01  4.670e-02d
word2 4.530e-02 -1.170e-02 -1.201e-01  2.439e-01  4.670e-02d
</code></pre>

<p>It will be really helpful if you could tell me how to achieve this result?</p>

<p>Thanks in advance</p>
",Vectorization & Embeddings,save custom embedding matrix txt file format made dictionary contains word corresponding word vector following format would like save dictionary custom embeddings txt file following format format custom embeddings txt file need token followed value dimension embedding separated single space e g two token dimensional embeddings really helpful could tell achieve result thanks advance
Is it possible to fine-tune BERT to do retweet prediction?,"<p>I want to build a classifier that predicts if user <code>i</code> will retweet tweet <code>j</code>. </p>

<p>The dataset is huge, it contains 160 million tweets. Each tweet comes along with some metadata(e.g. does the retweeter follow the user of the tweet).</p>

<p>the text tokens for a single tweet is an ordered list of BERT ids. To get the embedding of the tweet, you just use the ids (So it is not text)</p>

<p>Is it possible to fine-tune BERT to do the prediction? if yes, what do courses/sources do you recommend to learn how to fine-tune? (I'm a beginner)</p>

<p>I should add that the prediction should be a probability.</p>

<p>If it's not possible, I'm thinking of converting the embeddings back to text then using some arbitrary classifier that I'm going to train. </p>
",Vectorization & Embeddings,possible fine tune bert retweet prediction want build classifier predicts user retweet tweet dataset huge contains million tweet tweet come along metadata e g doe retweeter follow user tweet text token single tweet ordered list bert id get embedding tweet use id text possible fine tune bert prediction yes course source recommend learn fine tune beginner add prediction probability possible thinking converting embeddings back text using arbitrary classifier going train
Having an issue in doing count Vectorization for Hindi Text,"<p>While Doing the count vectorization in Hindi, features names are getting automatically stemmed. </p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
test = []
test.append(""हमें फिल्म बहुत अच्छी लगी ।"")
test.append(""फिल्म में कुछ बेहतरीन गाने हैं ।"")
cv = CountVectorizer().fit(test)
print(cv.get_feature_names())
</code></pre>

<p>output: ['अच', 'बह', 'लग', 'हतर', 'हम']</p>
",Vectorization & Embeddings,issue count vectorization hindi text count vectorization hindi feature name getting automatically stemmed output
Using Bert and cosine similarity fo identify similar documents,"<p>we have a news website where we have to match news to a particular user. </p>

<p>We have to use for the matching only the user textual information, like for example the interests of the user or a brief description about them.</p>

<p>I was thinking to threat both the user textual information and the news text as document and find document similarity. </p>

<p>In this way, I hope, that if in my profile I wrote sentences like: <em>I loved the speach of the president in Chicago last year</em>, and a news talks about: <em>Trump is going to speak in Illinois</em> I can have a match (the example is purely casual). </p>

<p>I tried, first, to embed my documents using TF-IDF and then I tried a kmeans to see if there was something that makes sense, but I don't like to much the results. </p>

<p>I think the problem derives from the poor embedding that TF-IDF gives me. </p>

<p>Thus I was thinking of using BERT embedding to retrieve the embedding of my documents and then use cosine similarity to check similarity of two document (a document about the user profile and a news). </p>

<p>Is this an approach that could make sense? Bert can be used to retrieve the embedding of sentences, but there is a way to embed an entire document?</p>

<p>What would you advice me? </p>

<p>Thank you</p>
",Vectorization & Embeddings,using bert cosine similarity fo identify similar document news website match news particular user use matching user textual information like example interest user brief description wa thinking threat user textual information news text document find document similarity way hope profile wrote sentence like loved speach president chicago last year news talk trump going speak illinois match example purely casual tried first embed document using tf idf tried kmeans see wa something make sense like much result think problem derives poor embedding tf idf give thus wa thinking using bert embedding retrieve embedding document use cosine similarity check similarity two document document user profile news approach could make sense bert used retrieve embedding sentence way embed entire document would advice thank
word2vec : find words similar in a case insensitive manner,"<p>I have access to word vectors on a text corpus of my interest. Now, the issue I am faced with is that these vectors are case sensitive, i.e for example ""Him"" is different from ""him"" is different from ""HIM"". </p>

<p>I would like to find words most similar to the word ""Him"" is a case insensitive manner. I use the <code>distance.c</code> program that comes bundled with the Google <code>word2vec</code> package. Here is where I am faced with an issue.</p>

<p>Should I pass as arguments ""Him him HIM"" to the <code>distance.c</code> executable. This would return the sent of words closed to the 3 words.</p>

<p>Or should I run the <code>distance.c</code> program separately with each of the 3 arguments (""Him"" and ""him"" and ""HIM""), and then put together these lists in a sensible way to arrive at the most similar words? Please suggest.</p>
",Vectorization & Embeddings,word vec find word similar case insensitive manner access word vector text corpus interest issue faced vector case sensitive e example different different would like find word similar word case insensitive manner use program come bundled google package faced issue pas argument executable would return sent word closed word run program separately argument put together list sensible way arrive similar word please suggest
How to aggregate text sentences into a group under a common name in the sentences,"<p>I have data like this</p>

<pre><code>          Sentences
XYZ ABC Asset Management UK Limited
XYZ ABC BDDF - Informations et Etudes Comm...
XYZ ABC (Suisse) SA
XYZ ABC Comm Bank
XYZ ABC, Tokyo Branch
XYZ ABC Securities Services
XYZ ABC INVESTMENT PARTNERS BELGIUM
GIE XYZ ABC Assurance
XYZ ABC Energy Trading GP (f/k/a Fortis En...
TKB XYZ ABC Investment Partners
XYZ ABC LEASE GROUP FR
XYZ ABC
</code></pre>

<p>I want to aggregate them under the name ""XYZ ABC"" as all of them contain these two words and the word also occurs in the list of sentences (at the last).
Similarly like this, there are other names having 2 or more similar words, How do I aggregate them into something like this:</p>

<pre><code>         sentences                            grouped under
XYZ ABC Asset Management UK Limited             XYZ ABC
XYZ ABC BDDF - Informations et Etudes Comm...   XYZ ABC
GIE XYZ ABC Assurance                           XYZ ABC
XYZ ABC Energy Trading GP (f/k/a Fortis En...   XYZ ABC
TKB XYZ ABC Investment Partners                 XYZ ABC
XYZ ABC LEASE GROUP FR                          XYZ ABC
XYZ ABC                                         XYZ ABC
.......
ABC DEFGH IJKL Americas                      ABC DEFGH IJKL
ABC DEFGH IJKL Investment Grade Bonds        ABC DEFGH IJKL
ABC DEFGH IJKL Reality Television Series     ABC DEFGH IJKL
ABC DEFGH IJKL Delhi Branch                  ABC DEFGH IJKL
ABC DEFGH IJKL Suiesssee KPL ONG Ltd         ABC DEFGH IJKL
ABC DEFGH IJKL                               ABC DEFGH IJKL
</code></pre>

<p>The condition should be that the grouped name should contain 2 or more words and has to be there in all the sentences as well as should be an individual sentence by itself (the last sentences in both the example above). I tried using cosine similarity and word2vec approach but was not successful. Is there any way of doing this?</p>
",Vectorization & Embeddings,aggregate text sentence group common name sentence data like want aggregate name xyz abc contain two word word also occurs list sentence last similarly like name similar word aggregate something like condition grouped name contain word ha sentence well individual sentence last sentence example tried using cosine similarity word vec approach wa successful way
Grouping algorithm for text similarity,"<p>So i'm working on a project, based on the grouping algorithm by Chali in his ""Document Clustering with Grouping and Chaining Algorithms"" research paper. </p>

<p>I've gotten to section 4.1. I'm trying to understand in the in the context of finding articles about the same topic, what is meant by the clusters overlapping?</p>

<p>I've identified the cosine similarity score between articles. e.g. </p>

<p>So say article x is a guardian article </p>

<p>{</p>

<p>article 1 - cnn article 0.1 cosine score (no threshold) when compared to article x</p>

<p>article 2 - cnn article cosine 0.8 cosine score (inserted into high threshold cluster ) when compared to article x</p>

<p>article 3 - cnn article cosine 0.5 cosine score (inserted into low threshold cluster) when compared to article x</p>

<p>}</p>

<p>My question is, have I identified the clusters correctly? Could I e.g. mix stories from other publishers into this cluster? </p>

<p>And when the article says ""If not more than 2 texts are overlapping with the final clusters, then we take this group as a final cluster"". Interpreting this is also very difficult for me as like I said i'm not sure what 'overlapping' means. </p>

<p>Thanks for the help!</p>
",Vectorization & Embeddings,grouping algorithm text similarity working project based grouping algorithm chali document clustering grouping chaining algorithm research paper gotten section trying understand context finding article topic meant cluster overlapping identified cosine similarity score article e g say article x guardian article article cnn article cosine score threshold compared article x article cnn article cosine cosine score inserted high threshold cluster compared article x article cnn article cosine cosine score inserted low threshold cluster compared article x question identified cluster correctly could e g mix story publisher cluster article say text overlapping final cluster take group final cluster interpreting also difficult like said sure overlapping mean thanks help
Is the Gensim word2vec model same as the standard model by Mikolov?,"<p>I am implementing a paper to compare our performance. In the paper, the uathor says </p>

<blockquote>
  <p>300-dimensional pre-trained word2vec vectors (Mikolov et al., 2013)</p>
</blockquote>

<p>I am wondering whether the pretrained word2vec Gensim model <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py"" rel=""nofollow noreferrer"">here</a> is same as the pretrained embeddings on the official <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">Google site</a> (the GoogleNews-vectors-negative300.bin.gz file)</p>

<p><br>
My source of doubt arises from this line in Gensim documentation (in Word2Vec Demo section) </p>

<blockquote>
  <p>We will fetch the Word2Vec model trained on part of the Google News dataset, covering approximately 3 million words and phrases</p>
</blockquote>

<p>Does this mean the model on gensim is not fully trained? Is it different from the official embeddings by Mikolov? </p>
",Vectorization & Embeddings,gensim word vec model standard model mikolov implementing paper compare performance paper uathor say dimensional pre trained word vec vector mikolov et al wondering whether pretrained word vec gensim model pretrained embeddings official google site googlenews vector negative bin gz file source doubt arises line gensim documentation word vec demo section fetch word vec model trained part google news dataset covering approximately million word phrase doe mean model gensim fully trained different official embeddings mikolov
"InvalidArgumentError: Incompatible shapes: [10,2856,2856] vs. [10,2856]","<p>This is more of a conceptual misunderstanding, but when I go through the embedding layer, my 2-D x_train turns into a 3-D matrix. However, how can I then fit it to my model with y_train that is still 2-D? I tried flatten() to a 2-D matrix but it didn't work.</p>

<p>Here is my code:</p>

<pre><code>def biDirectRNN (vocab_size, embedding_dim, batch_size, subcat, file):
    x_train, x_test, y_train, y_test = preprocess (subcat,file)
    x,y = y_train.shape
    model = tf.keras.Sequential()
    model.add(keras.layers.Embedding(input_dim = vocab_size, output_dim = embedding_dim, batch_input_shape=[batch_size, None]))
    forward_layer = keras.layers.LSTM(64, return_sequences = True)
    backward_layer = keras.layers.LSTM(32, activation='relu', return_sequences=True,
                       go_backwards=True)
    model.add(tf.keras.layers.Bidirectional(forward_layer, backward_layer = backward_layer))

    #model.add(keras.layers.Flatten())
    model.add(keras.layers.Dense(y, activation = tf.nn.softmax))
    model.compile(optimizer = 'adam',
          loss='mean_squared_error',
        metrics=['accuracy'])
    model.fit(x_train, y_train, batch_size=batch_size, epochs = 1)
    test_loss, test_acc = model.evaluate(x_test, y_test, epochs=10, batch_size=32)
    return test_loss, test_acc
</code></pre>

<p>function call:</p>

<pre><code>biDirectRNN (2856, 100, 10, 'Crime', 'Crime14' )
</code></pre>

<p>My error is: 
InvalidArgumentError:  Incompatible shapes: [10,2856,2856] vs. [10,2856]</p>
",Vectorization & Embeddings,invalidargumenterror incompatible shape v conceptual misunderstanding go embedding layer x train turn matrix however fit model train still tried flatten matrix work code function call error invalidargumenterror incompatible shape v
Document Similarity - Multiple documents ended with same similarity score,"<p>I have been working in a business problem where i need to find a similarity of new document with existing one.
I have used various approach as below</p>

<p>1.Bag of words + Cosine similarity</p>

<p>2.TFIDF + Cosine similarity</p>

<p>3.Word2Vec + Cosine similarity</p>

<p>None of them worked as expected.
But finally i found an approach which works better its
<strong>Word2vec + Soft cosine</strong> similarity</p>

<p>But the new challenge is i ended up with <strong>multiple documents with same similarity score</strong>.
Most of them are relevant but few of them even though having some semantically similar words they are different </p>

<p>Please suggest how to over come this issue</p>
",Vectorization & Embeddings,document similarity multiple document ended similarity score working business problem need find similarity new document existing one used various approach bag word cosine similarity tfidf cosine similarity word vec cosine similarity none worked expected finally found approach work better word vec soft cosine similarity new challenge ended multiple document similarity score relevant even though semantically similar word different please suggest come issue
Cosine Similarity with word2vec not giving good documemt similarity,"<p>Why cosine similarity with word embedding is not providing good output...Its giving similarity values of new document with most of the historical documents as high..eventhough both documents are not similar</p>
",Vectorization & Embeddings,cosine similarity word vec giving good documemt similarity cosine similarity word embedding providing good output giving similarity value new document historical document high eventhough document similar
Using annoy with Torchtext for nearest neighbor search,"<p>I'm using Torchtext for some NLP tasks, specifically using the built-in embeddings.</p>

<p>I want to be able to do a inverse vector search: Generate a noisy vector, find the vector that is closest to it, then get back the word that is ""closest"" to the noisy vector.</p>

<p>From the <a href=""https://pytorch.org/text/datasets.html"" rel=""nofollow noreferrer"">torchtext docs</a>, here's how to attach embeddings to a built-in dataset:</p>

<pre class=""lang-py prettyprint-override""><code>from torchtext.vocab import GloVe
from torchtext import data

embedding = GloVe(name='6B', dim=100)

# Set up fields
TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)
LABEL = data.Field(sequential=False, is_target=True)

# make splits for data
train, test = datasets.IMDB.splits(TEXT, LABEL)

# build the vocabulary
TEXT.build_vocab(train, vectors=embedding, max_size=100000)
LABEL.build_vocab(train)

# Get an example vector
embedding.get_vecs_by_tokens(""germany"")
</code></pre>

<p>Then we can build the annoy index:</p>

<pre><code>from annoy import AnnoyIndex

num_trees = 50

ann_index = AnnoyIndex(embedding_dims, 'angular')

# Iterate through each vector in the embedding and add it to the index
for vector_num, vector in enumerate(TEXT.vocab.vectors):
    ann_index.add_item(vector_num, vector) # Here's the catch: will vector_num correspond to torchtext.vocab.Vocab.itos?

ann_index.build(num_trees)
</code></pre>

<p>Then say I want to retrieve a word using a noisy vector:</p>

<pre><code># Get an existing vector
original_vec = embedding.get_vecs_by_tokens(""germany"")
# Add some noise to it
noise = generate_noise_vector(ndims=100)
noisy_vector = original_vec + noise
# Get the vector closest to the noisy vector
closest_item_idx = ann_index.get_nns_by_vector(noisy_vector, 1)[0]
# Get word from noisy item
noisy_word = TEXT.vocab.itos[closest_item_idx]
</code></pre>

<p>My question comes in for the last two lines above: The <code>ann_index</code> was built using <code>enumerate</code> over the <code>embedding</code> object, which is a Torch tensor.</p>

<p>The <code>[vocab][2]</code> object has its own <code>itos</code> list that given an index returns a word.</p>

<p>My question is this: Can I be certain that the order in which words appear in the itos list, is the same as the order in <code>TEXT.vocab.vectors</code>? How can I map one index to the other?</p>
",Vectorization & Embeddings,using annoy torchtext nearest neighbor search using torchtext nlp task specifically using built embeddings want able inverse vector search generate noisy vector find vector closest get back word closest noisy vector torchtext doc attach embeddings built dataset build annoy index say want retrieve word using noisy vector question come last two line wa built using object torch tensor object ha list given index return word question certain order word appear itos list order map one index
meaning of in_qsize and out_qsize in gensim word2vec log files,"<p>I am running word2vec models in gensim. I don't understand 2 metrics (in_qsize/out_qsize) reported by the log file. I've spent a bit of time searching and can't find an explanation. Here is a sample from my log files:</p>

<pre><code>2020-04-17 21:04:09,032 : INFO : EPOCH 5 - PROGRESS: at 68.67% examples, 657466 words/s, in_qsize 18, out_qsize 1
2020-04-17 21:04:10,038 : INFO : EPOCH 5 - PROGRESS: at 68.92% examples, 657527 words/s, in_qsize 20, out_qsize 0
2020-04-17 21:04:11,078 : INFO : EPOCH 5 - PROGRESS: at 69.14% examples, 657513 words/s, in_qsize 20, out_qsize 1
2020-04-17 21:04:12,136 : INFO : EPOCH 5 - PROGRESS: at 69.39% examples, 657458 words/s, in_qsize 18, out_qsize 1
2020-04-17 21:04:13,139 : INFO : EPOCH 5 - PROGRESS: at 69.68% examples, 657687 words/s, in_qsize 17, out_qsize 4
</code></pre>
",Vectorization & Embeddings,meaning qsize qsize gensim word vec log file running word vec model gensim understand metric qsize qsize reported log file spent bit time searching find explanation sample log file
Does Euclidean Distance change when strings &quot;double&quot;?,"<p><strong>Problem</strong>
I'm trying to answer this question: Consider two documents A and B whose Euclidean distance is d and cosine similarity is c (using no normalization other than raw term frequencies). If we create a new document A' by appending A to itself and anotherdocument B' by appending B to itself, then: </p>

<p>a.What is the Euclidean distance between A' and B' (using raw term frequency)? </p>

<p><strong>My solution</strong></p>

<pre><code>doc1 = ""the quicker brown dogs easily jumps over the lazy dogs"" 
doc2 = ""the quicker dogs pose a serious problem for lazy dogs""

def calc_term_frequency(doc : list):

    dic = {}
    for word in doc.split():
        if word in dic:
            dic[word] = dic[word] + 1
        else:
            dic[word]= 1

    for word, frequency in dic.items():
       dic[word]= frequency / len(doc.split())

    return dic

tfs_doc1 = calc_term_frequency(doc1)
tfs_doc2 = calc_term_frequency(doc2)
print(tfs_doc1)
</code></pre>

<p>Outputs tfs_doc1 as:
{'the': 0.2, 'quicker': 0.1, 'brown': 0.1, 'dogs': 0.2, 'easily': 0.1, 'jumps': 0.1, 'over': 0.1, 'lazy': 0.1}
This seems like it works at it should. I then proceed to calculate the Euclidean Distance, first between doc1 and doc1 and then doc1 and doc2, shown below. </p>

<pre><code>import math
math.sqrt(sum((tfs_doc1.get(k, 0) - tfs_doc1.get(k, 0))**2 for k in set(tfs_doc1.keys()).union(set(tfs_doc1.keys())))) # output: 0
math.sqrt(sum((tfs_doc1.get(k, 0) - tfs_doc2.get(k, 0))**2 for k in set(tfs_doc1.keys()).union(set(tfs_doc2.keys())))) # output: 0.316227766016838
</code></pre>

<p>This gives me a score of 0.316227766016838. When I try to verify that this is correct using sklearn, like below:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import euclidean_distances

corpus_vect = CountVectorizer().fit_transform(corpus).todense() 

print(euclidean_distances(corpus_vect[0], corpus_vect[0])) # output: 0
print(euclidean_distances(corpus_vect[0], corpus_vect[1] )) # output: 3.
</code></pre>

<p>I get an output of [[0.]] [[3.]], which translates to round(, 1) of my ""manual"" result. </p>

<p><strong>The problem:</strong> when I try to answer the initial questions and ""double"" the strings, e.g.</p>

<pre><code>doc1 = ""the quicker brown dogs easily jumps over the lazy dogs the quicker brown dogs easily jumps over the lazy dogs"" 
doc2 = ""the quicker dogs pose a serious problem for lazy dogs the quicker dogs pose a serious problem for lazy dogs""
</code></pre>

<p>I get the same output for the manual technique (0.316227766016838) but [[0.]] [[6.]] when using the ""sklearn method"" / Vectorizer. So, using one method the ED stays the same and using the other it doubles!</p>

<p>What is the correct solution and what causes the difference? Really stuck here. Thanks in advance. </p>
",Vectorization & Embeddings,doe euclidean distance change string double problem trying answer question consider two document b whose euclidean distance cosine similarity c using normalization raw term frequency create new document appending anotherdocument b appending b euclidean distance b using raw term frequency solution output tfs doc quicker brown dog easily jump lazy seems like work proceed calculate euclidean distance first doc doc doc doc shown give score try verify correct using sklearn like get output translates round manual result problem try answer initial question double string e g get output manual technique using sklearn method vectorizer using one method ed stay using double correct solution cause difference really stuck thanks advance
Cosine similarity of a new text document with existing list of documents,"<p>I have a dataframe of 1000 text documents with corresponding keywords.I want to extract keywords of a new document by finding the keywords corresponding to the documents in the list which is most similar.</p>
",Vectorization & Embeddings,cosine similarity new text document existing list document dataframe text document corresponding keywords want extract keywords new document finding keywords corresponding document list similar
Which additional features to use apart from Doc2Vec embeddings for Document Similarity?,"<p>So I am doing a project on document similarity and right now my features are only the embeddings from Doc2Vec. Since that is not showing any good results, after hyperparameter optimization and word embedding before the doc embedding... What other features can I add, so as to get better results?
My dataset is 150 documents, 500-700 words each, with 10 topics(labels), each document having one topic. Documents are labeled on a document level, and that labeling is currently used only for evaluation purposes.</p>

<p>Edit: The following is answer to gojomo's questions and elaborating on my comment on his answer:</p>

<p>The evaluation of the model is done on the training set. I am comparing if the label is the same as the most similar document from the model. For this I am first getting the document vector using the model's method 'infer_vector' and then 'most_similar' to get the most similar document. The current results I am getting are 40-50% of accuracy. A satisfactory score would be of at least 65% and upwards.</p>

<p>Due to the purpose of this research and it's further use case, I am unable to get a larger dataset, that is why I was recommended by a professor, as this is a university project, to add some additional features to the document embeddings of Doc2Vec. As I had no idea what he ment, I am asking the community of stackoverflow. </p>

<p>The end goal of the model is to do clusterization of the documents, again the labels for now being used only for evaluation purposes.</p>

<p>If I don't get good results with this model, I will try out the simpler ones mentioned by @Adnan S @gojomo such as TF-IDF, Word Mover's Distance, Bag of words, just presumed I would get better results using Doc2Vec.</p>
",Vectorization & Embeddings,additional feature use apart doc vec embeddings document similarity project document similarity right feature embeddings doc vec since showing good result hyperparameter optimization word embedding doc embedding feature add get better result dataset document word topic label document one topic document labeled document level labeling currently used evaluation purpose edit following answer gojomo question elaborating comment answer evaluation model done training set comparing label similar document model first getting document vector using model method infer vector similar get similar document current result getting accuracy satisfactory score would least upwards due purpose research use case unable get larger dataset wa recommended professor university project add additional feature document embeddings doc vec idea ment asking community stackoverflow end goal model clusterization document label used evaluation purpose get good result model try simpler one mentioned adnan gojomo tf idf word mover distance bag word presumed would get better result using doc vec
How to compute cosine similarity between 2 different CORPUSES?,"<p>I'm trying to estimate the cosine similarity between <em>each</em> document <code>i</code> in a Corpus <code>A</code> and <em>all</em> documents in a Corpus <code>B</code>.</p>

<p>Any idea how I can do this efficiently? I'm working with pretty large datasets.</p>

<p>Essentially, I want to get the document(s) in Corpus <code>B</code> which is (are) <em>most similar</em> for each document within Corpus <code>A</code>.</p>
",Vectorization & Embeddings,compute cosine similarity different corpus trying estimate cosine similarity document corpus document corpus idea efficiently working pretty large datasets essentially want get document corpus similar document within corpus
Reduce the number of hidden units in hugging face transformers (BERT),"<p>I have been given a large csv each line of which is a set of BERT tokens made with hugging face BertTokenizer (<a href=""https://huggingface.co/transformers/main_classes/tokenizer.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/main_classes/tokenizer.html</a>).
1 line of this file looks as follows:</p>

<p><code>101, 108, 31278, 90939, 70325, 196, 199, 71436, 10107, 29190, 10107, 106, 16680, 68314, 10153, 17015, 15934, 10104, 108, 10233, 12396, 14945, 10107, 10858, 11405, 13600, 13597, 169, 57343, 64482, 119, 119, 119, 100, 11741, 16381, 10109, 68830, 10110, 20886, 108, 10233, 11127, 21768, 100, 14120, 131, 120, 120, 188, 119, 11170, 120, 12132, 10884, 10157, 11490, 12022, 10113, 10731, 10729, 11565, 14120, 131, 120, 120, 188, 119, 11170, 120, 162, 11211, 11703, 12022, 11211, 10240, 44466, 100886, 102</code></p>

<p>and there are 9 million lines like this</p>

<p>Now, I am trying to get embeddings from these tokens like this:</p>

<pre><code>def embedding:
    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)
    model = BertModel.from_pretrained('bert-base-multilingual-cased')
    input_ids = torch.tensor([101, 108, 31278, 90939, 70325, 196, 199, 71436, 10107, 29190, 10107, 106, 16680, 68314, 10153, 17015, 15934, 10104, 108, 10233, 12396, 14945, 10107, 10858, 11405, 13600, 13597, 169, 57343, 64482, 119, 119, 119, 100, 11741, 16381, 10109, 68830, 10110, 20886, 108, 10233, 11127, 21768, 100, 14120, 131, 120, 120, 188, 119, 11170, 120, 12132, 10884, 10157, 11490, 12022, 10113, 10731, 10729, 11565, 14120, 131, 120, 120, 188, 119, 11170, 120, 162, 11211, 11703, 12022, 11211, 10240, 44466, 100886, 102]).unsqueeze(0)  # Batch size 1
    outputs = model(input_ids)
    last_hidden_states = outputs[0][0][0]  # The last hidden-state is the first element of the output tuple
</code></pre>

<p>Output of this is embedding correspending to the line. The size is 768*1 tensor. Semantically, everything is ok. But, when I do this for the full file the output is 768 * 9,0000,000 <code>torch tensors</code>. So I get a memory error even with large machine with a 768 GB of RAM.
Here is how I call this function: 
<code>tokens['embeddings'] = tokens['text_tokens'].apply(lambda x: embedding(x))</code></p>

<p><code>tokens</code> is the pandas data frame with 9 million lines each of which contains BERT tokens.</p>

<p>Is it possible to reduce the default size of the hidden units, which is 768 here: <a href=""https://huggingface.co/transformers/main_classes/model.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/main_classes/model.html</a></p>

<p>Thank you for your help.</p>
",Vectorization & Embeddings,reduce number hidden unit hugging face transformer bert given large csv line set bert token made hugging face berttokenizer line file look follows million line like trying get embeddings token like output embedding correspending line size tensor semantically everything ok full file output get memory error even large machine gb ram call function panda data frame million line contains bert token possible reduce default size hidden unit thank help
Text Classification : LSTM vs Feedforward,"<p>I am training a text classification model. </p>

<p><strong>Task</strong> : Given a description, identify the quantifier </p>

<p><strong>For ex</strong> 
1) This field contains the total revenue amount in USD -> amount </p>

<p>2) This has city code -> code</p>

<p>3) total deposit amount is 34 -> amount </p>

<p>4) contains first name info -> name </p>

<p>5) contains last nme -> name </p>

<p>For the given task, it makes sense to model this as a text classification problem. </p>

<p>I took two approaches</p>

<p>Approach 1 : </p>

<p>a) Use glove embedding to get vector represenation </p>

<p>b) Use feedforward NN to classify data into 1 of 11 possible output classes </p>

<pre><code>
    model = Sequential()
    model.add(layers.Embedding(vocab_size, embedding_dim, 
                               weights=[embedding_matrix], 
                               input_length=maxlen, 
                               trainable=False))
    model.add(layers.GlobalMaxPool1D())
    model.add(layers.Dense(200, activation='relu'))
    model.add(layers.Dense(100, activation='relu'))
    model.add(layers.Dense(50, activation='relu'))
    model.add(layers.Dense(11, activation='softmax'))
    model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    model.summary()

</code></pre>

<p>This approach gives me 80% test accuracy </p>

<p>Approach 2 : I plan to use LSTM because they can also learn the context and from previous words </p>

<pre><code>
    model = Sequential()
    model.add(layers.Embedding(vocab_size, embedding_dim, 
                               weights=[embedding_matrix], 
                               input_length=maxlen, 


    model.add(layers.LSTM(100,dropout=0.2, recurrent_dropout=0.2, activation='tanh'))
    model.add(layers.Dense(11, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])

    epochs = 100
    batch_size = 32

    model.summary()

</code></pre>

<p>The problem is irrespective of what i do LSTM never gets above 40% accuracy mark. It gets stuck on it from start to end. </p>

<p>Morevoer, the feedforward net ( Approach 1 ) can detect simple cases like ""total amount is 6 usd"" but LSTM is unable to get even this correct and predicts it as Other</p>

<p>My question is why does LSTM ( with added power of context ) fails to improve upon feedforward. What should i do to improve it . </p>
",Vectorization & Embeddings,text classification lstm v feedforward training text classification model task given description identify quantifier ex field contains total revenue amount usd amount ha city code code total deposit amount amount contains first name info name contains last nme name given task make sense model text classification problem took two approach approach use glove embedding get vector represenation b use feedforward nn classify data possible output class approach give test accuracy approach plan use lstm also learn context previous word problem irrespective lstm never get accuracy mark get stuck start end morevoer feedforward net approach detect simple case like total amount usd lstm unable get even correct predicts question doe lstm added power context fails improve upon feedforward improve
"Pre trained vectors, nlp, word2vec, word embedding for particular topic?","<p>is there any pretrained vector for particular topic only? for example ""java"", so i want vectors related java in file. mean if i give input inheritance then cosine similarity show me polymorphism and other related stuff only! 
i am using corpus as GoogleNews-vectors-negative300.bin and Glove vectors. still not getting related words.</p>
",Vectorization & Embeddings,pre trained vector nlp word vec word embedding particular topic pretrained vector particular topic example java want vector related java file mean give input inheritance cosine similarity show polymorphism related stuff using corpus googlenews vector negative bin glove vector still getting related word
Can I use a 3D input on a Keras Dense Layer?,"<p>As an exercise I need to use only dense layers to perform text classifications. I want to leverage words embeddings, the issue is that the dataset then is 3D (samples,words of sentence,embedding dimension). Can I input a 3D dataset into a dense layer?</p>

<p>Thanks</p>
",Vectorization & Embeddings,use input kera dense layer exercise need use dense layer perform text classification want leverage word embeddings issue dataset sample word sentence embedding dimension input dataset dense layer thanks
How to use Gensim doc2vec with pre-trained word vectors?,"<p>I recently came across the doc2vec addition to Gensim. How can I use pre-trained word vectors (e.g. found in word2vec original website) with doc2vec?</p>

<p>Or is doc2vec getting the word vectors from the same sentences it uses for paragraph-vector training?</p>

<p>Thanks.</p>
",Vectorization & Embeddings,use gensim doc vec pre trained word vector recently came across doc vec addition gensim use pre trained word vector e g found word vec original website doc vec doc vec getting word vector sentence us paragraph vector training thanks
assigning custom label to spacy using pandas dataframe,"<p>I am new to NLP based programming. I am trying to compare a word to an exisiting dataset using NLP.</p>

<p>I have a dataframe as shown below</p>

<p><a href=""https://i.sstatic.net/CvQBq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CvQBq.png"" alt=""dataframe""></a></p>

<p>Now, I have an input text like "" krish Stockholm"" the program should tag each word to the column label of the trained dataset and display as shown below</p>

<p>krish: Name</p>

<p>Stockholm : City</p>

<p>Can it be done using spacy ? Appreciate guidance.</p>
",Vectorization & Embeddings,assigning custom label spacy using panda dataframe new nlp based programming trying compare word exisiting dataset using nlp dataframe shown input text like krish stockholm program tag word column label trained dataset display shown krish name stockholm city done using spacy appreciate guidance
Which nlp approach is best suited for updating model with new words without the need to train from scratch?,"<p>I (complete noob in machine learning and natural language processing) am using doc2vec approach (gensim python library) to find most similar document to a random string. Problem is that whenever I'd like to add a new document to trained model I need to retrain model from scratch. </p>

<p>Is there an approach that stands out in ability to add a new document/vocabulary to a trained model without the need to train from scratch or an approach that would train faster?</p>

<p>I'm overwhelmed by all the approaches to nlp and only started with what I found as the most popular (word2vec/doc2vec) and now I'm looking for direction where to study now. Thanks for any recommendations.</p>
",Vectorization & Embeddings,nlp approach best suited updating model new word without need train scratch complete noob machine learning natural language processing using doc vec approach gensim python library find similar document random string problem whenever like add new document trained model need retrain model scratch approach stand ability add new document vocabulary trained model without need train scratch approach would train faster overwhelmed approach nlp started found popular word vec doc vec looking direction study thanks recommendation
Vectorize list of string with Word2Vec to feed to keras sequential layer,"<p>I am trying to built a custom made word embedding model with fastText, that represents my data (list of sentences) as vectors so I can ""feed"" it to a Keras CNN for abusive language detection.</p>

<p>My tokenised data is stored in a list like this: </p>

<pre><code>data = [['is',
      'this',
      'a',
      'news',
      'if',
      'you',
      'have',
      'no',
      'news',
      'than',
      'shutdown',
      'the',
      'channel'],
     ['if',
      'interest',
      'rate',
      'will',
      'hike',
      'by',
      'fed',
      'then',
      'what',
      'is',
      'the',
      'effect',
      'on',
      'nifty']]
</code></pre>

<p>I am currently applying the fastText model like this:</p>

<pre><code>model = fastText(data, size=100, window=5, min_count=5, workers=16, sg=0, negative=5)
</code></pre>

<p>And then I perform:</p>

<pre><code>model = FastText(sentences, min_count=1)

documents = []

for document in textList:
    word_vectors = []
    for word in document: 
        word_vectors.append(model.wv[word])
    documents.append(np.concatenate(word_vectors)

document_matrix = np.concatenate(documents)
</code></pre>

<p>Obviously, the document_matrix doesn't fit as the input for my Keras model:</p>

<pre><code>from keras.models import Sequential
from keras import layers
from keras.layers import Dense, Activation

model = Sequential()
model.add(layers.Conv1D(filters=250, kernel_size = 4, padding = 'same', input_shape=( 1,))) 
model.add(layers.GlobalMaxPooling1D()) 
model.add(layers.Dense(250, activation='relu')) 
model.add(layers.Dense(3, activation='sigmoid')) 
</code></pre>

<p>I am stuck and running out of ideas how to make the output of the embedding fit the input for Keras.</p>

<p>Thank you very much in advance, you guys are the best ! </p>

<p>Lisa </p>
",Vectorization & Embeddings,vectorize list string word vec feed kera sequential layer trying built custom made word embedding model fasttext represents data list sentence vector feed kera cnn abusive language detection tokenised data stored list like currently applying fasttext model like perform obviously document matrix fit input kera model stuck running idea make output embedding fit input kera thank much advance guy best lisa
How to increase the similarity between specific words in word2vec?,"<p>I'm new to use Word2Vec.</p>

<p>I am very curious to know how to increase the similarity between the specific words in word2vec.</p>

<p>Let's say my sentences look like this.(toy example)</p>

<p>sentence1 = ['a', 'b', 'c']
sentence2 = ['a', 'b', 'a', 'b', 'a', 'b']</p>

<p>model = word2vec.Word2Vec([sentence1, sentence2])</p>

<p>What I expect in the result of word2vec is that 'a' and 'b' should be closer than 'a' and 'c'.
This is because there are many pairs of 'a' and 'b' in sentence2 and sentence2 is a training dataset in word2vec model.</p>

<p>How can I make a training dataset to have better similarity between 'a' and 'b' using word2vec?</p>

<p>If you have real applications like this example, please let me know how to use them.</p>
",Vectorization & Embeddings,increase similarity specific word word vec new use word vec curious know increase similarity specific word word vec let say sentence look like toy example sentence b c sentence b b b model word vec word vec sentence sentence expect result word vec b closer c many pair b sentence sentence training dataset word vec model make training dataset better similarity b using word vec real application like example please let know use
Training a Bert word embedding model in tensorflow,"<p>I have my own corpus of plain text. I want to train a Bert model in TensorFlow, similar to gensim's word2vec to get the embedding vectors for each word.</p>

<p>What I have found is that all the examples are related to any downstream NLP tasks like classification. But, I want to train a Bert model with my custom corpus after which I can get the embedding vectors for a given word.</p>

<p>Any lead will be helpful.</p>
",Vectorization & Embeddings,training bert word embedding model tensorflow corpus plain text want train bert model tensorflow similar gensim word vec get embedding vector word found example related downstream nlp task like classification want train bert model custom corpus get embedding vector given word lead helpful
"First time working with Word2Vec, try to cluster users based on their skill set","<p>For my thesis I have to analyse the skill of candidates. I have to cluster the users and compare their skillsets. The information is classified so I made a random database which have the same structure so I can show how my data is build. </p>

<pre><code>import random
listOfSkills = [""Dutch"",""Java OSGI"",""XML Transformation Query"",""Java Enterprise Edition"",""Functional Design"",""Scrum"",""Python"",""JavaScript"",""Ruby"",""Java"",""SQL"",""Data Analytics"",""Machine Learning"",""Deep Learning"",""English""]

rand_item = random.choice(listOfSkills)

n = 5

rand_items = random.sample(listOfSkills, n)

test_skillset = []

for i in range(5):
    result = random.sample(listOfSkills, n)
    string = "", "".join(result)
    test_skillset.append(string)

test_id_ = np.arange(0, len(test_skillset)).tolist()

test_dict = {'id' : test_id_,
             'skillset' : test_skillset
             }

test_df = pd.DataFrame(test_dict)
</code></pre>

<p>After running this code I get a DataFrame which looks like this:</p>

<pre><code>id, skillset
0, ""Java, ruby, ...""
1, ""Java, ruby, ...""
2, ""Java, ruby, ...""
</code></pre>

<p>This is the same for the database I got.</p>

<p>The list of skills are some of the skills I found in the database. Also there are more users in the database which also have more skills.</p>

<p>I am quite new to machine learning and to using Word2Vec models. I tried some stuff but almost all the time I don't get a result which gives me extra information. Some of the skills have a long name which maybe messes up the model. Or I did something wrong.</p>

<p>One of my goals is to cluster the users and find similarities between each skill set.</p>

<p>My final goal is to match the vectors of the skill set with vacancies to check how good of a match a user can be with an open vacancy. But first I need to know if I can find similarities between the users.</p>

<p>So my question are:</p>

<ul>
<li>How can I use Word2Vec to find individual similarities between skills?</li>
<li>How can I use Word2Vec cluster the users to find similar skillsets?</li>
</ul>

<p>Sorry if my question is a bit vague, English isn't my native language and Python is a bit new to me.
I am open to clarify things if needed. </p>
",Vectorization & Embeddings,first time working word vec try cluster user based skill set thesis analyse skill candidate cluster user compare skillsets information classified made random database structure show data build running code get dataframe look like database got list skill skill found database also user database also skill quite new machine learning using word vec model tried stuff almost time get result give extra information skill long name maybe mess model something wrong one goal cluster user find similarity skill set final goal match vector skill set vacancy check good match user open vacancy first need know find similarity user question use word vec find individual similarity skill use word vec cluster user find similar skillsets sorry question bit vague english native language python bit new open clarify thing needed
String similarity methods in Python - NGram? Jaro Winkler?,"<p>I am trying to design a method for analyzing logical arguments present in text. I want to be able to detect strings which are significantly similar in some given body of text to encode them into their own dictionary. Should I use Ngrams? Jaro Winkler? Some other vectorization? </p>
",Vectorization & Embeddings,string similarity method python ngram jaro winkler trying design method analyzing logical argument present text want able detect string significantly similar given body text encode dictionary use ngrams jaro winkler vectorization
Text Classification with Spacy : going beyond the basics to improve performance,"<p>I'm trying to train a text categorizer on a training dataset of texts (Reddit posts) with two exclusive classes (1 and 0) regarding <strong>a feature of the authors of the posts, and not the posts themselves</strong>. <br> 
Classes are unbalanced: approximately 75:25, which means that 75% of authors are ""0"", while 25% are ""1"".<br>
The whole dataset is composed of 3 columns: the first one representing the author of the post, the second the subreddit the post belongs to, and the third the actual post.  </p>

<h3>Data</h3>

<p>The dataset looks like this:</p>

<pre class=""lang-py prettyprint-override""><code>In [1]: train_data_full.head(5)
Out[1]: 
          author          subreddit            body
0        author1         subreddit1         post1_1 
1        author2         subreddit2         post2_1
2        author3         subreddit2         post3_1
3        author2         subreddit3         post2_2 
4        author5         subreddit4         post5_1

</code></pre>

<p>Where postI_J is the J-th post of the I-th author. Notice that in this dataset the same author may appear more than once, if she/he has posted more than once. <br></p>

<p>In a separate dataset I have the class each author belongs to. <br></p>

<p>The first thing I did was to group by author:</p>

<pre class=""lang-py prettyprint-override""><code>
def proc_subs(l):
    s = set(l)
    return "" "".join([st.lower() for st in s])

train_data_full_agg = train_data_full.groupby([""author""], as_index = False).agg({'subreddit':  proc_subs, ""body"": "" "".join}) 

train_data_full_agg.head(5)

Out[2]:
 author                          subreddit                      body     
author1              subreddit1 subreddit2           post1_1 post1_2
author2   subreddit3 subreddit2 subreddit3   post2_1 post2_2 post2_3   
author3              subreddit1 subreddit5           post3_1 post3_2 
author4                         subreddit7                   post4_1
author5              subreddit1 subreddit2           post5_1 post5_2


</code></pre>

<p>There are a total of 5000 authors, 4000 used for training and 1000 for validation (roc_auc score).
And here is the spaCy code that I'm using (<code>train_texts</code> is the subset of <code>train_data_full_agg.body.tolist()</code> I use for training, while <code>test_texts</code> the one I use for validation).</p>

<pre class=""lang-py prettyprint-override""><code># Before this line of code there are others (imports and data loading mostly) which i think are irrelevant

train_data  = list(zip(train_texts, train_labels))

nlp = spacy.blank(""en"")
if 'textcat' not in nlp.pipe_names:
    textcat = nlp.create_pipe(""textcat"", config={""exclusive_classes"": True, ""architecture"": ""ensemble""})
        nlp.add_pipe(textcat, last = True)
else:
    textcat = nlp.get_pipe('textcat')

textcat.add_label(""1"")
textcat.add_label(""0"")

def evaluate_roc(nlp,textcat):
    docs = [nlp.tokenizer(tex) for tex in test_texts]
    scores , a = textcat.predict(docs) 
    y_pred = [b[0] for b in scores]
    roc = roc_auc_score(test_labels, y_pred)
    return roc


dec = decaying(0.6 , 0.2, 1e-4)
pipe_exceptions = ['textcat']
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]
    with nlp.disable_pipes(*other_pipes): 
        optimizer = nlp.begin_training()
        for epoch in range(10):
        random.shuffle(train_data)
            batches = minibatch(train_data, size = compounding(4., 32., 1.001) )                                                             
            for batch in batches:
                texts1, labels = zip(*batch)
                nlp.update(texts1, labels, sgd=optimizer, losses=losses, drop = next(dec))
            with textcat.model.use_params(optimizer.averages):
                rocs.append(evaluate_roc(nlp, textcat))

</code></pre>

<h3>Problem</h3>

<p>I get poor performance (measured with the ROC on a test dataset i don't have the labels of) also if compared to simpler algorithms one could write using scikit learn (like tfidf, bow, word embeddings etc)</p>

<h3>Attempts</h3>

<p>I've tried to get better performance with the following procedures: </p>

<ol>
<li>Various preprocessings/lemmatizations of texts: the best one seems to be removing all punctuation, numbers, stopwords, out of vocabulary words and then lemmatize all remaining words</li>
<li>Tried textcat architectures: ensemble, bow (also with <code>ngram_size</code> and <code>attr</code> parameters): the best seems to be the ensemble, as of <a href=""https://spacy.io/api/textcategorizer#architectures"" rel=""nofollow noreferrer"">spaCy documentation</a>.</li>
<li>Tried to include subreddit information: I did it by training a separate textcat on the same 4000 authors' <code>subreddit</code> column (see point 5. to read how the information coming from this step is used). </li>
<li>Tried to include word embeddings information: using document vectors from <code>en_core_web_lg-2.2.5</code> spaCy model, I trained over the same 4000 authors' aggregated posts a scikit multi-layer perceptron.(see point 5. to read how the information coming from this step is used). </li>
<li>Then, to mix the information coming from subreddits, posts and document vectors I trained a logistic regression on the 1000 predictions of the three models (i also tried to balance classes in this last step using <a href=""https://medium.com/coinmonks/smote-and-adasyn-handling-imbalanced-data-set-34f5223e167"" rel=""nofollow noreferrer"">adasyn</a></li>
</ol>

<p>Using this logistic regression, I get ROC = 0.89 over the test dataset. If I remove any of these steps and use an intermediate model, the ROC lowers.</p>

<p>I've also tried the following steps, which again just lowered the ROC:</p>

<ol>
<li>Use pre-trained models like bert. The code I used is analogous to <a href=""https://github.com/explosion/spacy-transformers/blob/master/examples/train_textcat.py"" rel=""nofollow noreferrer"">this one</a></li>
<li>Tried to balance classes from the beginning, thus using a smaller training set.  </li>
<li>Tried to leave punctuation and put a <code>sentencizer</code> at the beginning of the <code>nlp</code> pipeline</li>
</ol>

<h3>Additional information (mainly from comments)</h3>

<ul>
<li><p>Q: <em>What is the ROC of baseline models like Multinomial Naive Bayes or SVM ?</em> <br>
I have easy access to ROCs evaluated just on texts(no subreddits or vectors). An svm set up like so: <br>
<code>svm= svm.SVC(C=1.0, kernel='poly', degree=2, gamma='scale', coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, max_iter=-1)</code>  <br>
Would give roc (using CountVectorizer bow) = 0.53 (same with rbf kernel, but rbf + class_weight = None or ""balanced"" gives 0.63 (same without the constraint on cache_size)).Anyway, an XGBregressor with parameters set with gridsearch would give roc = 0.88. The same XGB but also with CountVectorizer subreddits and scikit Doc2Vec vectors (combined with an lr like above) gives about 93. The ensemble code you see above, only on texts would give around 83. With subreddts and vectors (treated as above) gives 89 <br></p></li>
<li><p>Q: <em>Have you tried not concatenating?</em> <br>
If i don't concatenate, performance (just on not concatenated texts, so again no vectors/subreddits) is similar to the case in which I concatenate, but I wouldn't then know how to combine multiple predictions for the same author into one prediction. Because remember that I have more comments from each author, and I have to predict a binary feature regarding the authors.</p></li>
</ul>

<h3>Questions</h3>

<ol>
<li>Do you have any suggestions specifically about the spaCy code that I'm using (e.g. any other way to use subreddits and/or document vectors information)?</li>
<li>How may I improve the overall model? </li>
</ol>

<p>Any suggestion is highly appreciated. <br>
Please be as explicit as possible in terms of code / explanations / references as I am new to NLP.</p>
",Vectorization & Embeddings,text classification spacy going beyond basic improve performance trying train text categorizer training dataset text reddit post two exclusive class regarding feature author post post class unbalanced approximately mean author whole dataset composed column first one representing author post second subreddit post belongs third actual post data dataset look like posti j j th post th author notice dataset author may appear ha posted separate dataset class author belongs first thing wa group author total author used training validation roc auc score spacy code using subset use training one use validation problem get poor performance measured roc test dataset label also compared simpler algorithm one could write using scikit learn like tfidf bow word embeddings etc attempt tried get better performance following procedure various preprocessings lemmatizations text best one seems removing punctuation number stopwords vocabulary word lemmatize remaining word tried textcat architecture ensemble bow also parameter best seems ensemble spacy documentation tried include subreddit information training separate textcat author column see point read information coming step used tried include word embeddings information using document vector spacy model trained author aggregated post scikit multi layer perceptron see point read information coming step used mix information coming subreddits post document vector trained logistic regression prediction three model also tried balance class last step using adasyn using logistic regression get roc test dataset remove step use intermediate model roc lower also tried following step lowered roc use pre trained model like bert code used analogous one tried balance class beginning thus using smaller training set tried leave punctuation put beginning pipeline additional information mainly comment q roc baseline model like multinomial naive bayes svm easy access roc evaluated text subreddits vector svm set like would give roc using countvectorizer bow rbf kernel rbf class weight none balanced give without constraint cache size anyway xgbregressor parameter set gridsearch would give roc xgb also countvectorizer subreddits scikit doc vec vector combined lr like give ensemble code see text would give around subreddts vector treated give q tried concatenating concatenate performance concatenated text vector subreddits similar case concatenate know combine multiple prediction author one prediction remember comment author predict binary feature regarding author question suggestion specifically spacy code using e g way use subreddits document vector information may improve overall model suggestion highly appreciated please explicit possible term code explanation reference new nlp
How to measure similarity between words or very short text,"<p>I work on the problem of finding the nearest document in a list of documents. Each document is a word or a very short sentence (e.g. ""jeans"" or ""machine tool"" or ""biological tomatoes""). By closest I mean close in a semantical way.</p>

<p>I have tried to use word2vec embeddings (from Mikolov article) but the closest words or more contextually linked than semanticaly linked (""jeans"" is linked to ""shoes"" and not ""trousers"" as expected).</p>

<p>I have tried to use Bert encoding (<a href=""https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#32-understanding-the-output"" rel=""nofollow noreferrer"">https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#32-understanding-the-output</a>) using last layers but it faces the same issues.</p>

<p>I have tried elastic search, but it doesn't find semantical similarities.
(The task needs to be solved in French but maybe solving it in English is a good first step)</p>
",Vectorization & Embeddings,measure similarity word short text work problem finding nearest document list document document word short sentence e g jean machine tool biological tomato closest mean close semantical way tried use word vec embeddings mikolov article closest word contextually linked semanticaly linked jean linked shoe trouser expected tried use bert encoding using last layer face issue tried elastic search find semantical similarity task need solved french maybe solving english good first step
How to initialize BOW or Skipgrams in Word2vec embeedings?,"<p>I am not able to find the good source where i can able to find out how to initialize BOW or SKipgrams.</p>
",Vectorization & Embeddings,initialize bow skipgrams word vec embeedings able find good source able find initialize bow skipgrams
Subword vectors to a word vector tokenized by Sentencepiece,"<p>There are some embedding models that have used the <strong>Sentencepiece</strong> model for tokenization. So they give subword vectors for unknown words that are not in the vocabulary. But I want to get word vector for each word like Word2vec, fastText.
<strong>Should I average subword vectors to represent a word vector?</strong> </p>
",Vectorization & Embeddings,subword vector word vector tokenized sentencepiece embedding model used sentencepiece model tokenization give subword vector unknown word vocabulary want get word vector word like word vec fasttext average subword vector represent word vector
Purpose of pooling layer after text embedding layer,"<p>I'm following the tutorial on the tensorflow site (<a href=""https://www.tensorflow.org/tutorials/text/word_embeddings#create_a_simple_model"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/word_embeddings#create_a_simple_model</a>) to learn word embeddings, and a confusion that I have is about the purpose of having a Globalaveragepooling layer right after the embedding layer as follows:</p>

<pre><code>model = keras.Sequential([
  layers.Embedding(encoder.vocab_size, embedding_dim),
  layers.GlobalAveragePooling1D(),
  layers.Dense(16, activation='relu'),
  layers.Dense(1)
])
</code></pre>

<p>I understand what pooling means and how it's done. If someone can explain why we need a pooling layer, and what would change if we didn't use it, I'd appreciate it.</p>
",Vectorization & Embeddings,purpose pooling layer text embedding layer following tutorial tensorflow site learn word embeddings confusion purpose globalaveragepooling layer right embedding layer follows understand pooling mean done someone explain need pooling layer would change use appreciate
combining structured and text data in classification problem using keras,"<p>The following code is a very simple example of using word embedding to predict the labels (see below). The example is taken from <a href=""https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"" rel=""nofollow noreferrer"">here</a>.</p>

<pre><code>from numpy import array
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.embeddings import Embedding

# define documents
docs = ['Well done!',
        'Good work',
        'Great effort',
        'nice work',
        'Excellent!',
        'Weak',
        'Poor effort!',
        'not good',
        'poor work',
        'Could have done better.']

# define class labels
labels = array([1,1,1,1,1,0,0,0,0,0])

# integer encode the documents
vocab_size = 50
encoded_docs = [one_hot(d, vocab_size) for d in docs]
print(encoded_docs)

# pad documents to a max length of 4 words
max_length = 4
padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
print(padded_docs)

# define the model
model = Sequential()
model.add(Embedding(vocab_size, 8, input_length=max_length))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))

# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

# summarize the model
print(model.summary())

# fit the model
model.fit(padded_docs, labels, epochs=50, verbose=0)

# evaluate the model
loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)
print('Accuracy: %f' % (accuracy*100))
</code></pre>

<p>Let us say we have structured data like this:</p>

<pre><code>hours_of_revision = [10, 5, 7, 3, 100, 0, 1, 0.5, 4, 0.75]
</code></pre>

<p>Here every entry aligns with each row showing nicely that one should really spend more time to revise to achieve good marks (-:</p>

<p>Just wondering, could one incorporate this into the model to use the text and structured data?</p>
",Vectorization & Embeddings,combining structured text data classification problem using kera following code simple example using word embedding predict label see example taken let u say structured data like every entry aligns row showing nicely one really spend time revise achieve good mark wondering could one incorporate model use text structured data
How to find the closest word to a vector using BERT,"<p>I am trying to get textual representation(or the closest word) of given word embedding using BERT. Basically I am trying to get similar functionality as in gensim:</p>

<pre><code>&gt;&gt;&gt; your_word_vector = array([-0.00449447, -0.00310097, 0.02421786, ...], dtype=float32)
&gt;&gt;&gt; model.most_similar(positive=[your_word_vector], topn=1))
</code></pre>

<p>So far, I have been able to generate contextual word embedding using <a href=""https://github.com/hanxiao/bert-as-service#getting-elmo-like-contextual-word-embedding"" rel=""noreferrer"">bert-as-service</a> but can't figure out how to get closest words to this embedding. I have used pre-trained bert model (uncased_L-12_H-768_A-12) and haven't done any fine tuning.</p>
",Vectorization & Embeddings,find closest word vector using bert trying get textual representation closest word given word embedding using bert basically trying get similar functionality gensim far able generate contextual word embedding using bert service figure get closest word embedding used pre trained bert model uncased l h done fine tuning
How does word2vec predicts the word correctly but the actual dataset does not contain it?,"<p>I'm trying to understand how word2vec predicts a word, given a list of words.
Specifically, I trained my skip-gram model on twitter data of 500k tweets with the following parameters:</p>

<pre><code>model = gensim.models.Word2Vec(data, window=5, workers=7, sg=1, min_count=10, size=200)
</code></pre>

<p>Given the words <code>discrimination</code> and <code>uberx</code>, I get the following output:</p>

<pre><code>model.wv.most_similar(positive=[PorterStemmer().stem(WordNetLemmatizer().lemmatize(""discrimination"", pos='v')), WordNetLemmatizer().lemmatize(""uberx"", pos='v')], topn=30)
[('discret', 0.7425585985183716),
 ('fold_wheelchair', 0.7286415696144104),
 ('illeg_deni', 0.7280288338661194),
 ('tradit_cab', 0.7262350916862488),
 ('mobil_aid', 0.7252357602119446),
 ('accommod_disabl', 0.724936842918396),
 ('uberwav', 0.720955491065979),
 ('discrimin_disabl', 0.7206833958625793),
 ('deni_access', 0.7202375531196594),...]
</code></pre>

<p>However, when I search the dataset <code>data</code> which I dumped on my hard drive, for the words ""discrimination"", ""uberx"", and any other word from the output list, I never find a single instance of a datapoint which contained all 3 words. So my question is, how does the model know that, say, word ""accommodation disabled"" is the right word for the context ""discrimination"" and ""uberx"" if it has never seen those 3 words together in a single tweet?</p>
",Vectorization & Embeddings,doe word vec predicts word correctly actual dataset doe contain trying understand word vec predicts word given list word specifically trained skip gram model twitter data k tweet following parameter given word get following output however search dataset dumped hard drive word discrimination uberx word output list never find single instance datapoint contained word question doe model know say word accommodation disabled right word context discrimination uberx ha never seen word together single tweet
"What does input_shape,input_dim and units indicate or mean while adding layers in a Keras?","<p>Im new to keras and i was wondering if I could do some work regarding text classification using neural networks.
So i went ahead and got a data set regarding spam or ham and I vectorized the data using tfidf and converted the labels to a numpy array using the to_categorical() and managed to split my data into train and test each of which is a numpy array having around 7k columns.
This the code i used. </p>

<pre><code>model.add(Dense(8,input_dim=7082,activation='relu'))
model.add(Dense(8,input_dim=7082))
model.add(Dense(2,activation='softmax'))
model.compile(loss=""categorical_crossentropy"",optimizer=""adam"",metrics=['accuracy'])
</code></pre>

<p>I dont know if im doing something totally wrong. Could someone point me in the right direction as to what i should change. 
The error thrown:
<code>Error when checking input: expected dense_35_input to have 2 dimensions, but got array with shape ()</code></p>
",Vectorization & Embeddings,doe input shape input dim unit indicate mean adding layer kera im new kera wa wondering could work regarding text classification using neural network went ahead got data set regarding spam ham vectorized data using tfidf converted label numpy array using categorical managed split data train test numpy array around k column code used dont know im something totally wrong could someone point right direction change error thrown
Access the output of several layers of pretrained DistilBERT model,"<p>I am trying to access the output embeddings from several different layers of the pretrained ""DistilBERT"" model. (""distilbert-base-uncased"")</p>

<pre><code>bert_output = model(input_ids, attention_mask=attention_mask)
</code></pre>

<p>The bert_output seems to return only the embedding values of the last layer for the input tokens.</p>
",Vectorization & Embeddings,access output several layer pretrained distilbert model trying access output embeddings several different layer pretrained distilbert model distilbert base uncased bert output seems return embedding value last layer input token
Doc2Vec Pre training and Inferring vectors,"<p>Suppose I have trained the doc2vec model with 50000 documents and I want to infer vectors for a separate dataset containing 36000 documents. In this case will the inferred vectors be effective for the downstream task of classification, becasue my assumption is that the inferred vectors depend on the size of documents with which the model is trained. </p>

<p>Note: Both dataset i.e one used for training doc2vec and other for inferring vectors are unique but from the same domain of US supreme court.</p>

<p>Please correct me if I am wrong with valid reason.</p>
",Vectorization & Embeddings,doc vec pre training inferring vector suppose trained doc vec model document want infer vector separate dataset containing document case inferred vector effective downstream task classification becasue assumption inferred vector depend size document model trained note dataset e one used training doc vec inferring vector unique domain u supreme court please correct wrong valid reason
What kind of model/technique should I use to compare supermarket product names,"<p>I have a database with supermarket product items(it contains name, descriptions, price, stock, etc). </p>

<p>I want to make a price comparison between those supermarkets, but, for that i need to know if supermarket A and B refers to the same product. </p>

<p>For example I found out that supermarket <strong>A</strong> has a product called <code>Leche Evaporada GLORIA Azul Paquete 6un Lata 400g</code> and supermarket <strong>B</strong> has a product named <code>Leche Evaporada Gloria Azul Pack 6 Unid x 400 g</code> and those refers to the same product.</p>

<p>I pointed out that I will need to have semantic comparison for those cases. I'm new in this problems so I don't really know what is the best solution to not underestimate the problem or overkill it.</p>

<p>What I'm doing right now with not so great results:</p>

<ol>
<li>I'm only using product names.</li>
<li>Remove stop words from those product names.</li>
<li>Convert the sentence in an array of words.</li>
<li>Get frequency for every word.</li>
<li>If a word has frequency &lt;= 1, then delete it.</li>
<li>With that words I create a dictionary(bag of words) that i will use to map an array of words(a sentence converted) to a feature vector.</li>
<li>Then I ""train"" a TFIDF model with all feature vectors.</li>
<li>Make comparisons(with no great results).</li>
</ol>

<p>I'm using python as LP and <a href=""https://radimrehurek.com/gensim/"" rel=""nofollow noreferrer"">gensim</a> to create models, dictionaries(bag of word) and to make comparisons.</p>

<p>EDIT:
Another examples:</p>

<pre><code>Leche Fresca UHT GLORIA Entera Bolsa 946ml == Leche Entera UHT Gloria Bolsa 946 ml
Yogurt Griego Gloria con Miel y Granola Vaso 115 g == Yogurt Griego GLORIA Batido con Miel Vaso 115g
Leche sin Lactosa GLORIA Mocaccino Botella 330ml == Shake Mocaccino UHT Gloria Frasco 330 ml.
</code></pre>
",Vectorization & Embeddings,kind model technique use compare supermarket product name database supermarket product item contains name description price stock etc want make price comparison supermarket need know supermarket b refers product example found supermarket ha product called supermarket b ha product named refers product pointed need semantic comparison case new problem really know best solution underestimate problem overkill right great result using product name remove stop word product name convert sentence array word get frequency every word word ha frequency delete word create dictionary bag word use map array word sentence converted feature vector train tfidf model feature vector make comparison great result using python lp gensim create model dictionary bag word make comparison edit another example
TFIDF separate for each label,"<p>Using TFIDFvectorizor(SKlearn), how to obtain word ranking based on tfidf score for each label separately. I want the word frequency for each label (positive and negative).</p>

<p>relevant code:</p>

<pre><code>vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,stop_words='english',use_idf=True, ngram_range =(1,1))
features_train = vectorizer.fit_transform(features_train).todense()
features_test = vectorizer.transform(features_test).todense()


for i in range(len(features_test)):
    first_document_vector=features_test[i]
    df_t = pd.DataFrame(first_document_vector.T, index=feature_names, columns=[""tfidf""])


df_t.sort_values(by=[""tfidf""],ascending=False).head(50)
</code></pre>
",Vectorization & Embeddings,tfidf separate label using tfidfvectorizor sklearn obtain word ranking based tfidf score label separately want word frequency label positive negative relevant code
Can word2vec deal with sequence of number?,"<p>I am very new to network embedding, especially for the attributed network embedding. Currently, I am studying the node2vec algorithm. I think the process is </p>

<pre><code>RandomWalk with p and q
Fed the walks to Word2Vec
</code></pre>

<p>For the second step, I see the algorithm takes every node as a string.</p>

<p>But my problem is that the nodes of my network are values. Maybe some nodes have the same value. I think this strategy will take the nodes with the same value as 'one' node. </p>

<p>Then what should I do if I want to embed such a network? My network is an attributed graph, each node has n dimensional attributes.</p>

<p>Thanks so much!</p>
",Vectorization & Embeddings,word vec deal sequence number new network embedding especially attributed network embedding currently studying node vec algorithm think process second step see algorithm take every node string problem node network value maybe node value think strategy take node value one node want embed network network attributed graph node ha n dimensional attribute thanks much
How to save sklearn pipeline/feature-transformer,"<p>I have a pipeline contains only a feature union that has three different sets of feature, including tfidf:</p>

<pre><code>A_vec = AVectorizer()
B_vec = BVectorizer()
tfidf_vec = TfidfVectorizer(ngram_range=(1,2), analyzer='word', binary=False, stop_words=stopWords, min_df=0.01, use_idf=True)
all_features = FeatureUnion([('A_feature', A_vec), ('V_feature', B_vec), ('tfidf_feature', tfidf_vec)])
pipeline = Pipeline([('all_feature', all_features)])
</code></pre>

<p>I want to save this pipelined feature transformer for my test data (I am using LibSVM for classification), and this is what I have tried:</p>

<ul>
<li><p>I have used joblib.dump to save this pipeline but it generated toooo many .npy files so I had to stop the writing process. It was a rather stupid attempt!</p></li>
<li><p>I have saved tfidf_vec.vocabulary_ and thus</p>

<p>tfidf_vec2 = TfidfVectorizer(ngram_range=(1,3), analyzer='word', binary=False, stop_words=stopWords, min_df=0.01, use_idf=True,vocabulary=pickle.load(open(""../vocab.pkl"", ""rb""))</p>

<p>... ...</p>

<p>feat_test = pipeline2.transform(X_test)</p></li>
</ul>

<p>It says ""NotFittedError: idf vector is not fitted"". I then used fit_transform rather than transform but it generates a feature vector that contains different values (comparing to the correct feature vector). Then I followed <a href=""http://thiagomarzagao.com/2015/12/08/saving-TfidfVectorizer-without-pickles/"" rel=""nofollow"">http://thiagomarzagao.com/2015/12/08/saving-TfidfVectorizer-without-pickles/</a> and still struggling to get it work. </p>

<p>Is there a simpler way to achieve this? Thanks!</p>
",Vectorization & Embeddings,save sklearn pipeline feature transformer pipeline contains feature union ha three different set feature including tfidf want save pipelined feature transformer test data using libsvm classification tried used joblib dump save pipeline generated toooo many npy file stop writing process wa rather stupid attempt saved tfidf vec vocabulary thus tfidf vec tfidfvectorizer ngram range analyzer word binary false stop word stopwords min df use idf true vocabulary pickle load open vocab pkl rb feat test pipeline transform x test say notfittederror idf vector fitted used fit transform rather transform generates feature vector contains different value comparing correct feature vector followed still struggling get work simpler way achieve thanks
GPT-2 language model: multiplying decoder-transformer output with token embedding or another weight matrix,"<p>I was reading the code of GPT2 language model. The transformation of hidden states to the probability distribution over the vocabulary has done in the following line:</p>

<pre><code>lm_logits = self.lm_head(hidden_states)
</code></pre>

<p>Here,</p>

<pre><code>self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
</code></pre>

<p>However, 
In the <a href=""https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf"" rel=""nofollow noreferrer"">original paper</a>, they suggested multiplying hidden states with the token embedding matrix whereas <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">huggingface</a>  implementation used another matrix.</p>

<p>Is there any advantage of this? Am I missing something?  </p>
",Vectorization & Embeddings,gpt language model multiplying decoder transformer output token embedding another weight matrix wa reading code gpt language model transformation hidden state probability distribution vocabulary ha done following line however original paper suggested multiplying hidden state token embedding matrix whereas huggingface implementation used another matrix advantage missing something
Why can Bert&#39;s three embeddings be added?,"<p>I already know the meaning of Token Embedding, Segment Embedding, and Position Embedding. But why can these three vectors be added together? The Size and direction of vectors will change after the addition, and the semantics of the word will also change. (It's the same question for the Transformer model which has two Embeddings named Input Embedding and Position Embedding.)</p>
",Vectorization & Embeddings,bert three embeddings added already know meaning token embedding segment embedding position embedding three vector added together size direction vector change addition semantics word also change question transformer model ha two embeddings named input embedding position embedding
how to use the muse to get aligned embedding,"<p>Resently, I used the MUSE to align en-zh embeddings. By this way, I get 5 files as the result of the MUSE, three of which are vectors-en.txt, vectors-zh.txt and best_mapping.pth. How do I use the three files to get the alignment file? Or how to represent chinese embeddings and english embeddings in a common share space?</p>
",Vectorization & Embeddings,use muse get aligned embedding resently used muse align en zh embeddings way get file result muse three vector en txt vector zh txt best mapping pth use three file get alignment file represent chinese embeddings english embeddings common share space
Dataframe Rows are matching with each other in TF-IDF Cosine similarity i,"<p>I am trying to learn data science and found this great article online.</p>

<p><a href=""https://bergvca.github.io/2017/10/14/super-fast-string-matching.html"" rel=""nofollow noreferrer"">https://bergvca.github.io/2017/10/14/super-fast-string-matching.html</a></p>

<p>I have this database full of company names, but am finding that the results where the similarity is equal to 1, they are in fact literally the same exact row. I obviously want to catch duplicates, but I do not want the same row to match itself. </p>

<p>On a side note, this has opened my eyes to pandas and NLP. Super fascinating field - Hopefully, somebody can help me out here.</p>

<pre><code>import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from scipy.sparse import csr_matrix
import sparse_dot_topn.sparse_dot_topn as ct

pd.set_option('display.max_colwidth', -1)
df = pd.read_csv('CSV/Contacts.csv',  dtype=str)
print(df.shape)
df.head(2)
</code></pre>

<p>Shape: (72489, 3)</p>

<pre><code>    Id  Name    Email
0   0031J00001bvXFTQA2 FRESHPOINT ATLANTA, INC  dotcomp@sysco.com
1   0031J00001aJtFaQAK  VIRGIL  dotcom@corp.sysco.com
</code></pre>

<p>Then I clean the data</p>

<pre><code># Clean the data
df.dropna()
# df['Email'] = df['Email'].str.replace('[^a-zA-Z]', '')
# df['Email'] = df['Email'].str.replace(r'[^\w\s]+', '')

contact_emails = df['Email']
</code></pre>

<p>Then I implement the N-Grams function</p>

<pre><code>def ngrams(string, n=3):
    string = re.sub(r'[,-./]|\sBD',r'', string)
    ngrams = zip(*[string[i:] for i in range(n)])
    return [''.join(ngram) for ngram in ngrams]
</code></pre>

<p>Then I get the TF-IDF Matrix</p>

<pre><code># get Tf-IDF Matrix
vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)
tf_idf_matrix = vectorizer.fit_transform(contact_emails.apply(lambda x: np.str_(x)))
</code></pre>

<p>Then I implement the Cosine Similarity function - Which I am still not quite sure what each parameter does.</p>

<pre><code>def awesome_cossim_top(A, B, ntop, lower_bound=0):
    # force A and B as a CSR matrix.
    # If they have already been CSR, there is no overhead
    A = A.tocsr()
    B = B.tocsr()
    M, _ = A.shape
    _, N = B.shape

    idx_dtype = np.int32

    nnz_max = M*ntop

    indptr = np.zeros(M+1, dtype=idx_dtype)
    indices = np.zeros(nnz_max, dtype=idx_dtype)
    data = np.zeros(nnz_max, dtype=A.dtype)

    ct.sparse_dot_topn(
        M, N, np.asarray(A.indptr, dtype=idx_dtype),
        np.asarray(A.indices, dtype=idx_dtype),
        A.data,
        np.asarray(B.indptr, dtype=idx_dtype),
        np.asarray(B.indices, dtype=idx_dtype),
        B.data,
        ntop,
        lower_bound,
        indptr, indices, data)

    return csr_matrix((data,indices,indptr),shape=(M,N))
</code></pre>

<p>Then we actually find the matches. I am not sure what the transpose does in this case and how that finds matches.</p>

<pre><code>matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.7)
</code></pre>

<p>Then here is the function for extracting the matches.</p>

<pre><code>def get_matches_df(sparse_matrix, email_vector,email_ids, top=5):
    non_zeros = sparse_matrix.nonzero()

    sparserows = non_zeros[0]
    sparsecols = non_zeros[1]


    if top:
        nr_matches = top
    else:
        nr_matches = sparsecols.size
    left_name_Ids = np.empty([nr_matches], dtype=object)
    right_name_Ids = np.empty([nr_matches], dtype=object)

    left_side = np.empty([nr_matches], dtype=object)
    right_side = np.empty([nr_matches], dtype=object)
    similairity = np.zeros(nr_matches)

    for index in range(nr_matches):        
        left_name_Ids[index] = email_ids[sparserows[index]]
        left_side[index] = email_vector[sparserows[index]]

        right_name_Ids[index] = email_ids[sparsecols[index]]
        right_side[index] = email_vector[sparsecols[index]]
        similairity[index] = sparse_matrix.data[index]

    return pd.DataFrame({
                        'SFDC_ID':  left_name_Ids,
                        'left_side': left_side,
                        'right_SFDC_ID':right_name_Ids,
                          'right_side': right_side,
                           'similairity': similairity})
</code></pre>

<p>Then I call the function and pass in the params</p>

<pre><code>name_Ids = df['Id']
matches_df = get_matches_df(matches, contact_emails,name_Ids, top=72489)
</code></pre>

<p>Now I only want to extract matches that are 90% similar or more.</p>

<pre><code>matches_df = matches_df[matches_df['similairity'] &gt; 0.9] 
</code></pre>

<p>Then I sort the values by similarity</p>

<pre><code>matches_df.sort_values('similairity' )
</code></pre>

<p><a href=""https://i.sstatic.net/igI2B.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/igI2B.png"" alt=""enter image description here""></a> </p>

<p>So what I am finding is that the same rows are being matched with each other. I know this because the SFDC ids are exactly the same - Why is this happening? How can I avoid this in the future? I obviously do not want the row to asses itself when finding similarities.</p>
",Vectorization & Embeddings,dataframe row matching tf idf cosine similarity trying learn data science found great article online database full company name finding result similarity equal fact literally exact row obviously want catch duplicate want row match side note ha opened eye panda nlp super fascinating field hopefully somebody help shape clean data implement n gram function get tf idf matrix implement cosine similarity function still quite sure parameter doe actually find match sure transpose doe case find match function extracting match call function pas params want extract match similar sort value similarity finding row matched know sfdc id exactly happening avoid future obviously want row ass finding similarity
SVR model keeps running forever when trying to fit this on sparse csr matrix,"<p>I have a dataframe with text column , to fit a model on this I convert the text using TFIDF Vectorizer, once that is done I have a Sparse CSR Matrix as output of shape - (1353655, 318304), when I try to fir a support vector regressor model on this , the code keeps running forever </p>

<pre><code>from sklearn.svm import SVR
svr_model = SVR(C=0.1).fit(X_train,y_train)
print(svr_model.score(X_test,y_test))
</code></pre>

<p>Any idea what could be going wrong , is the volume to much to be handled? </p>
",Vectorization & Embeddings,svr model keep running forever trying fit sparse csr matrix dataframe text column fit model convert text using tfidf vectorizer done sparse csr matrix output shape try fir support vector regressor model code keep running forever idea could going wrong volume much handled
"What is the difference between word2vec, glove, and elmo?","<p>What is the difference between word2vec, glove, and elmo? According to my understanding all of them are used for training word embedding, am I correct?</p>
",Vectorization & Embeddings,difference word vec glove elmo difference word vec glove elmo according understanding used training word embedding correct
How to Embed your Dataframe using already trained model with Gensim (GoogleNews-vectors-negative300.bin),"<p>I am following this <a href=""https://medium.springboard.com/identifying-duplicate-questions-a-machine-learning-case-study-37117723844"" rel=""nofollow noreferrer"">tutorial</a> in which i have a following Dataset from Quora:</p>

<p><a href=""https://i.sstatic.net/dBzId.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dBzId.jpg"" alt=""enter image description here""></a></p>

<p>Here i have already cleaned and tokenize the data in column <strong>q1_clean</strong> &amp; <strong>q1_clean</strong>.</p>

<p>Now i have trained the <strong>W2vModel</strong> by using GoogleNews pretrained model with the following code. </p>

<pre><code># We are concating the two columns of Question1 and Question2

nData = pd.Series(pd.concat([data['q1_clean'], data['q2_clean']]))
model_w2v = Word2Vec(nData, size=300) 

# step 2: intersect the initialized word2vec model with the pre-trained fasttext model
model_w2v.intersect_word2vec_format('GoogleNews-vectors-negative300.bin',lockf=1.0,binary=True)

# step 3: improve model with transfer-learning using the training data
model_w2v.train(nData, total_examples=model_w2v.corpus_count, epochs= 10)

</code></pre>

<p>Now i have to do the feature analysis, for that i have following function to get the average computed distance.</p>

<pre><code>def get_pairwise_distance(word1, word2, weight1, weight2, method = 'euclidean'):
    if(word1.size==0 or word2.size==0):
        return np.nan
    dist_matrix = pairwise_distances(word1, word2, metric=method)
    return np.average(dist_matrix, weights=np.matmul(weight1.reshape(-1,1),weight2.reshape(-1,1).T))
</code></pre>

<p>Here i have computed the tfidf to use as a weights: </p>

<pre><code>X_train_tokens = get_tokenized_questions(data=X_train)

from sklearn.feature_extraction.text import TfidfVectorizer
pass_through = lambda x:x
tfidf = TfidfVectorizer(analyzer=pass_through)
# compute tf-idf weights for the words in the training set questions
X_tfidf = tfidf.fit_transform(X_train_tokens)

# split into two
# X1_tfidf -&gt; tf-idf weights of first question in question pair and 
# X2_tfidf -&gt; tf-idf weights of second question in question pair
X1_tfidf = X_tfidf[:len(X_train)]
X2_tfidf = X_tfidf[len(X_train):]
</code></pre>

<p>and i am calling this <strong>get_pairwise_distance</strong> function like in the <a href=""https://medium.springboard.com/identifying-duplicate-questions-a-machine-learning-case-study-37117723844"" rel=""nofollow noreferrer"">tutorial</a>.</p>

<pre><code>#cosine similarities
# here X1 and X2 are the embedded versions of the first and second questions in the question-pair data
# and X1_tfidf and X2_tfidf are the tf-idf weights of the first and second questions in the question-pair data

cosine = compute_pairwise_dist(X1, X2, X1_tfidf, X2_tfidf)
</code></pre>

<p><strong>For this function i need to pass the embedded version of <em>q1_clean</em> and <em>q2_clean</em> as X1 and X2 where weights are already computed using TFIDF. and i am getting no clue how to embed these two columns into vectors using pretrained model and pass it to the given function?</strong> </p>
",Vectorization & Embeddings,embed dataframe using already trained model gensim googlenews vector negative bin following tutorial following dataset quora already cleaned tokenize data column q clean q clean trained w vmodel using googlenews pretrained model following code feature analysis following function get average computed distance computed tfidf use weight calling get pairwise distance function like tutorial function need pas embedded version q clean q clean x x weight already computed using tfidf getting clue embed two column vector using pretrained model pas given function
compare similarity of multiple texts using python,"<p>So i have about 300-500 text articles that i would like to compare the similarity of and figure which are related / duplicates some articles might be addressing the same topics but not identical. so to tackle this i started experimenting with spaCy and the similarity function .. now the problem is similarity only compares two documents at a time and I think i would need to loop every single text and to compare it to the other one which is a very slow and memory consuming process is there a way around this ?</p>
",Vectorization & Embeddings,compare similarity multiple text using python text article would like compare similarity figure related duplicate article might addressing topic identical tackle started experimenting spacy similarity function problem similarity compare two document time think would need loop every single text compare one slow memory consuming process way around
Gensim and Annoy for finding similar sentences,"<p>I have a large number of sentences in a database and I want to find the most similar of those sentences to a single sentence that the user types in.  </p>

<p>It looks like I may be able to do this with <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_annoy.html"" rel=""nofollow noreferrer"">annoy and gensim</a>, but all the examples I can see are using word2vec which I believe is good for finding single similar words, but not for sentences.  However, I note that the AnnoyIndexer() can take a word2vec OR a doc2vec model.</p>

<p>Am I correct that the process is the same, but swapping the word2vec model with a doc2vec model and using a doc2vec vector of the search sentence?</p>

<p>Do I need to use pre-trained word embeddings in any way, or do I literally just train the doc2vec model with the corpus of sentences that I have in my database?</p>

<p>Thank you!</p>
",Vectorization & Embeddings,gensim annoy finding similar sentence large number sentence database want find similar sentence single sentence user type look like may able annoy gensim example see using word vec believe good finding single similar word sentence however note annoyindexer take word vec doc vec model correct process swapping word vec model doc vec model using doc vec vector search sentence need use pre trained word embeddings way literally train doc vec model corpus sentence database thank
Word2Vec compare vectors from different models with different sizes,"<p>I have trained several word2vec models using gensim for different languages, but the <code>size</code> is different for each of them.</p>

<p>vectors are obtained like this:</p>

<pre><code>vec_sp = word_vectors_sp.get_vector(""uno"")
</code></pre>

<p>How to use <code>vec_sp</code> as input for different model with different vector size:</p>

<pre><code>word_vectors_en.most_similar(positive=[vec_sp], topn=1)
</code></pre>

<p>to obtain the corresponding word in the second model</p>
",Vectorization & Embeddings,word vec compare vector different model different size trained several word vec model using gensim different language different vector obtained like use input different model different vector size obtain corresponding word second model
how tfidf value is used in k-means clustering,"<p>I am using K-means clustering with TF-IDF using sckit-learn library. I understand that K-means uses distance to create clusters and the distance is represented in (x axis value, y axis value) but the tf-idf is a single numerical value. My question is how is this tf-idf value converted into (x,y) value by K-means clustering.</p>
",Vectorization & Embeddings,tfidf value used k mean clustering using k mean clustering tf idf using sckit learn library understand k mean us distance create cluster distance represented x axis value axis value tf idf single numerical value question tf idf value converted x value k mean clustering
Glove: training with single text file. Does GLoVE try to read it into memory? Or is it streamed?,"<p>I need to train some glove models to compare them with word2vec and fasttext output.  It's implemented in C, and I can't read C code.  The github is <a href=""https://github.com/stanfordnlp/GloVe/tree/master/src"" rel=""nofollow noreferrer"">here</a>.</p>

<p>The training corpus needs to be formatted into a single text file.  For me, this would be >>100G -- way too big for memory.  Before I waste time constructing such a thing, I'd be grateful if someone could tell me whether the glove algo tries to read the thing into memory, or whether it streams it from disk.  </p>

<p>If the former, then glove's current implementation wouldn't be compatible with my data (I think).  If the latter, I'd have at it.</p>
",Vectorization & Embeddings,glove training single text file doe glove try read memory streamed need train glove model compare word vec fasttext output implemented c read c code github training corpus need formatted single text file would g way big memory waste time constructing thing grateful someone could tell whether glove algo try read thing memory whether stream disk former glove current implementation compatible data think latter
Gensim&#39;s Doc2Vec - How to use pre-trained word2vec (word similarities),"<p>I don't have large corpus of data to train word similarities e.g. 'hot' is more similar to 'warm' than to 'cold'. However, I like to train doc2vec on a relatively small corpus ~100 docs so that it can classify my domain specific documents. </p>

<p><strong>To elaborate let me use this toy example.</strong> Assume I've only 4 training docs given  by 4 sentences - ""I love hot chocolate."", ""I hate hot chocolate."", ""I love hot tea."", and ""I love hot cake."".
Given a test document ""I adore hot chocolate"", I would expect, doc2vec will invariably return ""I love hot chocolate."" as the closest document. This expectation will be true if word2vec already supplies the knowledge that ""adore"" is very similar to ""love"". However, I'm getting most similar document as ""I hate hot chocolate"" -- which is a bizarre!!</p>

<p>Any suggestion on how to circumvent this, i.e. be able to use pre-trained word embeddings so that I don't need to venture into training ""adore"" is close to ""love"", ""hate"" is close to ""detest"", and so on.</p>

<p><strong>Code (Jupyter Nodebook. Python 3.7. Jensim 3.8.1)</strong></p>

<pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
data = [""I love hot chocolate."",
        ""I hate hot chocolate"",
       ""I love hot tea."",
       ""I love hot cake.""]

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]
print(tagged_data)
#Train and save
max_epochs = 10
vec_size = 5
alpha = 0.025


model = Doc2Vec(vector_size=vec_size, #it was size earlier
                alpha=alpha, 
                min_alpha=0.00025,
                min_count=1,
                dm =1)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    if epoch % 10 == 0:
        print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.epochs) #It was model.iter earlier
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

print(""Model Ready"")

test_sentence=""I adore hot chocolate""
test_data = word_tokenize(test_sentence.lower())
v1 = model.infer_vector(test_data)
#print(""V1_infer"", v1)

# to find most similar doc using tags
sims = model.docvecs.most_similar([v1])
print(""\nTest: %s\n"" %(test_sentence))
for indx, score in sims:
    print(""\t(score: %.4f) %s"" %(score, data[int(indx)]))
</code></pre>
",Vectorization & Embeddings,gensim doc vec use pre trained word vec word similarity large corpus data train word similarity e g hot similar warm cold however like train doc vec relatively small corpus doc classify domain specific document elaborate let use toy example assume training doc given sentence love hot chocolate hate hot chocolate love hot tea love hot cake given test document adore hot chocolate would expect doc vec invariably return love hot chocolate closest document expectation true word vec already supply knowledge adore similar love however getting similar document hate hot chocolate bizarre suggestion circumvent e able use pre trained word embeddings need training adore close love hate close detest code jupyter nodebook python jensim
Parsing natural language ingredient quantities for recipes,"<p>I'm building a ruby recipe management application, and as part of it, I want to be able to parse ingredient quantities into a form I can compare and scale. I'm wondering what the best tools are for doing this.</p>

<p>I originally planned on a complex regex, then on some other code that converts human readable numbers like <code>two</code> or <code>five</code> into integers, and finally code that will convert say <code>1 cup</code> and <code>3 teaspoons</code> into some base measurement. I control the input, so I kept the actual ingredient separate. However, I noticed users inputting abstract measurements like <code>to taste</code> and <code>1 package</code>. At least with the abstract measurements, I think I could just ignore them and scale and just scrape any number preceding them.</p>

<p>Here are some more examples</p>

<pre class=""lang-none prettyprint-override""><code>1 tall can
1/4 cup
2 Leaves
1 packet
To Taste
One
Two slices
3-4 fillets
Half-bunch
2 to 3 pinches (optional)
</code></pre>

<p>Are there any tricks to this? I have noticed users seem somewhat confused of what constitutes a quantity. I could try to enforce stricter rules and push things like <code>tall can</code> and <code>leaves</code> into the ingredient part. However, in order to enforce that, I need to be able to convey what's invalid.</p>

<p>I'm also not sure what the ""base"" measurement I should convert quantities into.</p>

<p>These are my goals.</p>

<ol>
<li><p><strong>To be able to scale recipes.</strong>  Arbitrary units of measurement like
<code>packages</code> don't have to be scaled but precise ones like <code>cups</code> or
<code>ounces</code> need to be.</p></li>
<li><p><strong>Figure out the ""main"" ingredients.</strong>  In the context of this question, this will be done largely by figuring out what the largest ingredient is in the recipe. In production, there will have to be some sort of modifier based on the type of ingredient because, obviously, <code>flour</code> is almost never considered the ""main"" ingredient. However, <code>chocolate</code> can be used sparingly, and it can still be said a <code>chocolate cake</code>.</p></li>
<li><p><strong>Normalize input.</strong>  To keep some consistency on the site, I want to keep consistent abbreviations. For example, instead of <code>pounds</code>, it should be <code>lbs</code>.</p></li>
</ol>
",Vectorization & Embeddings,parsing natural language ingredient quantity recipe building ruby recipe management application part want able parse ingredient quantity form compare scale wondering best tool originally planned complex regex code convert human readable number like integer finally code convert say base measurement control input kept actual ingredient separate however noticed user inputting abstract measurement like least abstract measurement think could ignore scale scrape number preceding example trick noticed user seem somewhat confused constitutes quantity could try enforce stricter rule push thing like ingredient part however order enforce need able convey invalid also sure base measurement convert quantity goal able scale recipe arbitrary unit measurement like scaled precise one like need figure main ingredient context question done largely figuring largest ingredient recipe production sort modifier based type ingredient obviously almost never considered main ingredient however used sparingly still said normalize input keep consistency site want keep consistent abbreviation example instead
Spacy: retrieve words/keys associated with a particular index,"<p>Given an index corresponding to a row in <code>&lt;some_model&gt;.vocab.vectors.data</code>, how can I retrieve the corresponding words / keys?</p>

<pre><code>import spacy

nlp = spacy.load('en_core_web_md')

nlp.vocab.vectors.data[6, :]   # gives me the 6th embedding 
</code></pre>

<p>What words/keys map to this 6th embedding? I could do some sort of brute force search by iterating through <code>*.vocab.vectors.items()</code> like</p>

<pre><code>for key, vector in nlp.vocab.vectors.items():
    # check if vector at my index matches this vector
         # print(nlp.vocab.strings[key])
</code></pre>

<p>but I'm hoping there's a better approach.</p>
",Vectorization & Embeddings,spacy retrieve word key associated particular index given index corresponding row retrieve corresponding word key word key map th embedding could sort brute force search iterating like hoping better approach
How to make part of the embedding matrix trainable and rest part as not trainable in pytorch?,"<p><strong>Codes are in Pytorch.</strong></p>

<p>I want part of my embedding matrix to be trainable and I want rest part to freeze weights as they are pretrained vectors.</p>

<p>My src_weight_matrix is the pretrained embedding matrix of dimension : [vocab_size + 4 special_tokens] <strong>X</strong> [emb_dim] = [49996 + 4] <strong>X</strong> [300]</p>

<p>first 49996 rows are for word in vocab and last 4 words in the vocabulary are special token i.e. [UNK], [PAD], [START], [STOP]. I have randomly initialized the embedding vectors for these 4 words.</p>

<p>So I want to <strong><em>train these 4 embedding weights</em></strong> and let other embedding have their own weights.</p>

<p>The code is as follow where all embedding weights are frozen except last 4 which is correct or not I don't know.</p>

<pre><code>class Encoder(nn.Module):
    def __init__(self, src_weights_matrix):
        super(Encoder, self).__init__()
        self.embedding = nn.Embedding(config.vocab_size, config.emb_dim)
        self.embedding.load_state_dict({'weight': src_weights_matrix})
        self.embedding.weight.requires_grad = False
        self.embedding.weight[-4:, :].requires_grad = True
        init_wt_normal(self.embedding.weight)
        self.lstm = nn.LSTM(config.emb_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        init_lstm_wt(self.lstm)
        self.W_h = nn.Linear(config.hidden_dim * 2, config.hidden_dim * 2, bias=False)
</code></pre>

<p><em>Any lead is greatly appreciated. Thanks in advance</em></p>
",Vectorization & Embeddings,make part embedding matrix trainable rest part trainable pytorch code pytorch want part embedding matrix trainable want rest part freeze weight pretrained vector src weight matrix pretrained embedding matrix dimension vocab size special token x emb dim x first row word vocab last word vocabulary special token e unk pad start stop randomly initialized embedding vector word want train embedding weight let embedding weight code follow embedding weight frozen except last correct know lead greatly appreciated thanks advance
How to use custom embeddings with keras LSTM?,"<p>I want to use pre-trained word embeddings with an LSTM.</p>

<p>That is I already have a model of form:</p>

<pre><code>embedding_for_word = model[word]
</code></pre>

<p>I have data of the following form:</p>

<pre><code>1. ""word1 word2 word3"" label 0
2. ""word4 word5 word6 word7"" label 1
3. ""word8 word9"" label 1
...
..
.
</code></pre>

<p>I know that for a standard LSTM (if timesteps are fixed) we can have:</p>

<pre><code>model = Sequential()
model.add(LSTM(N, input_shape=(n_timesteps, 1), return_sequences=True))
model.add(TimeDistributed(Dense(1, activation='sigmoid')))
model.compile(loss='binary_crossentropy', optimizer='adam')
</code></pre>

<p>How do I give sequential input of the form:</p>

<pre><code>batch_1[embedding_word1,
embedding_word2,embedding_word3 .. some_end_of_sequence_character] --&gt; label 0
batch_2[embedding_word4,
embedding_word5,embedding_word,embedding_word7,some_end_of_sequence_character] --&gt; label 1
...
..
.
</code></pre>

<p>How do I engineer the data and create the model (For the model, I'm only asking what the input layer would look like) for the example above?</p>

<p>Assume:</p>

<p>size_of_embeddings = K
batch_size = B</p>
",Vectorization & Embeddings,use custom embeddings kera lstm want use pre trained word embeddings lstm already model form data following form know standard lstm timesteps fixed give sequential input form engineer data create model model asking input layer would look like example assume size embeddings k batch size b
Comparison between two lists of words,"<p>I want to compare two lists (result, ground-truth). Output should contain 1 if both are match, if not '0' and output positive sensitive.  For example:</p>

<pre><code> result= [1,2,3,4,5]
 ground-truth=[2,4]
 Output= [0,1,0,1,0]
</code></pre>

<p>I implemented python code for this:</p>

<pre><code>def comparedkeground(dke,grd):
    correct=np.zeros(len(dke))
    try:
        for i in range(len(grd)):
            a=dke.index(grd[i])
            correct[a]=1
    except:
        'ValueError'
    return correct
</code></pre>

<p>This code give perfect result for some cases: for example : </p>

<pre><code>d=[1,2,30,4,6, 8, 50, 90, 121]
e=[30, 2, 50, 90]
print(comparedkeground(d,e))
[0. 1. 1. 0. 0. 0. 1. 1. 0.]

cc=['word', 'flags', 'tv', 'nanjo', 'panjo']
ccc=['panjo', 'tv']
print(comparedkeground(cc,ccc))
[0. 0. 1. 0. 1.]
</code></pre>

<p>But same code not working:</p>

<pre><code>u=['Lyme-disease vaccine', 'United States', 'Lyme disease', 'Allen Steere']
u1= ['drugs', 'Lyme-disease vaccine', 'Lyme disease']
print(comparedkeground(u,u1))
[0. 0. 0. 0.]
</code></pre>
",Vectorization & Embeddings,comparison two list word want compare two list result ground truth output contain match output positive sensitive example implemented python code code give perfect result case example code working
How to create Embedding matrix by using TFIDF?,"<p>I had taken a Text dataset to predict the sentiment of review is positive or negative. By using TFIDF I had converted word to vectors. Next, I had loaded Glove Embedding pre-trained file. Now how to create Embedding Matrix using TFIDF and glove word embeddings? I want to use Embedding Matrix in my Recurrent Neural Network.</p>

<p>I am facing Index Error while creating Embedding matrix, please correct me if I did wrong anything in the coding part.</p>

<pre><code>**TFIDF Vectorizer**
''' from sklearn.feature_extraction.text import TfidfVectorizer
    vectorizer_1 = TfidfVectorizer( max_features=10000,sublinear_tf=True, 
    use_idf=True,stop_words='english')
    X_vt = vectorizer_1.fit_transform(X_train)
    X_vt.shape
    (426340, 10000)'''
**Glove Embedding**
''' embedding_index = {}
    f = open(os.path.join(' ', 
    'C:/Users/User/glove.6B/glove.6B.100d.txt'),encoding=""utf-8"")
    for line in f:
      values=line.split()
      word = values[0]
      coefs = np.asarray(values[1:])
      embedding_index[word] = coefs
    f.close() '''

enter code here
''' emdedding_matrix = zeros((vocab_size,100))
 for feature, names in vectorizer_1.get_feature_items():
     embedding_vector = embedding_index.get(feature)
     if embedding_vector is not None:
        emdedding_matrix[names] = embedding_vector '''
</code></pre>

<p><a href=""https://i.sstatic.net/Wf6UX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Wf6UX.png"" alt=""Error""></a></p>
",Vectorization & Embeddings,create embedding matrix using tfidf taken text dataset predict sentiment review positive negative using tfidf converted word vector next loaded glove embedding pre trained file create embedding matrix using tfidf glove word embeddings want use embedding matrix recurrent neural network facing index error creating embedding matrix please correct wrong anything coding part
How to do supervised learning with Gensim/Word2Vec/Doc2Vec having large corpus of text documents?,"<p>I have a set of text documents(2000+) with labels (Liked/Disliked).Each document consists of 200+ words.
I am trying to do a supervised learning with these documents.
<strong>My approach would be:</strong></p>

<ol>
<li>Vectorize each document in the corpus. Say we have 2347 docs.</li>
<li>I can have 2347 rows with labels viz. Like as 1 and Dislike as 0.</li>
<li>Using any ML classification supervised model train above dataset with 2347 rows.</li>
</ol>

<p><strong>How to vectorize and create such dataset?</strong></p>
",Vectorization & Embeddings,supervised learning gensim word vec doc vec large corpus text document set text document label liked disliked document consists word trying supervised learning document approach would vectorize document corpus say doc row label viz like dislike using ml classification supervised model train dataset row vectorize create dataset
doc2vec: How is PV-DBOW implemented,"<p>I know that there exists already an implementation of PV-DBOW (paragraph vector) in python (gensim).
But I'm interested in knowing how to implement it myself.
The explanation from the <a href=""https://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""nofollow noreferrer"">official paper</a> for PV-DBOW is as follows:</p>

<blockquote>
  <p>Another way is to ignore the context words in the input, but force the model to predict words randomly sampled from the paragraph in the output. In reality, what this means is that at each iteration of stochastic gradient descent, we sample a text window, then sample a random word from the text window and form a classification task given the Paragraph Vector.</p>
</blockquote>

<p>According to the paper the word vectors are not stored
and PV-DBOW is said to work similar to skip gram in word2vec.</p>

<p>Skip-gram is explained in <a href=""http://arxiv.org/pdf/1411.2738v3.pdf"" rel=""nofollow noreferrer"">word2vec Parameter Learning</a>.
In the skip gram model the word vectors are mapped to the hidden layer.
The matrix that performs this mapping is updated during the training.
In PV-DBOW the dimension of the hidden layer should be the dimension of one paragraph vector. When I want to multiply the word vector of a sampled example with the paragraph vector they should have the same size.
The original representation of a word is of size (vocabulary size x 1). Which mapping is performed to get the right size (paragraph dimension x 1)
in the hidden layer. And how is this mapping performed when the word vectors are not stored?
I assume that word and paragraph representation should have the same size in the hidden layer because of equation 26 in <a href=""http://arxiv.org/pdf/1411.2738v3.pdf"" rel=""nofollow noreferrer"">word2vec Parameter Learning</a></p>
",Vectorization & Embeddings,doc vec pv dbow implemented know exists already implementation pv dbow paragraph vector python gensim interested knowing implement explanation official paper pv dbow follows another way ignore context word input force model predict word randomly sampled paragraph output reality mean iteration stochastic gradient descent sample text window sample random word text window form classification task given paragraph vector according paper word vector stored pv dbow said work similar skip gram word vec skip gram explained word vec parameter learning skip gram model word vector mapped hidden layer matrix performs mapping updated training pv dbow dimension hidden layer dimension one paragraph vector want multiply word vector sampled example paragraph vector size original representation word size vocabulary size x mapping performed get right size paragraph dimension x hidden layer mapping performed word vector stored assume word paragraph representation size hidden layer equation word vec parameter learning
Tfidf Vectorizer,"<p>I had taken a dataset for prediction of sentiment from the text review, initially, I clean the data (removal of punctuations, removal of stopwords, Tokenization). When I try to give Tokenized data as input to TFIDF vectorizer I am getting 
AttributeError: 'list' object has no attribute 'lower'. Please help me to get over this mistake.
<a href=""https://i.sstatic.net/u7QUR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/u7QUR.png"" alt=""This is my Clean Text ""></a></p>

<p><a href=""https://i.sstatic.net/kyjvQ.png"" rel=""nofollow noreferrer"">TFIDF Vectorizer</a></p>

<p><a href=""https://i.sstatic.net/kyjvQ.png"" rel=""nofollow noreferrer"">2</a>[<img src=""https://i.sstatic.net/kyjvQ.png"" alt=""Attribute Error"">]<a href=""https://i.sstatic.net/kyjvQ.png"" rel=""nofollow noreferrer"">2</a>: <a href=""https://i.sstatic.net/VSsHu.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/VSsHu.png</a></p>

<p><a href=""https://i.sstatic.net/dWdh4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dWdh4.png"" alt=""Error""></a></p>
",Vectorization & Embeddings,tfidf vectorizer taken dataset prediction sentiment text review initially clean data removal punctuation removal stopwords tokenization try give tokenized data input tfidf vectorizer getting attributeerror list object ha attribute lower please help get mistake tfidf vectorizer
using BERT for article summarization where label or expected output summary is not present for the article,"<p>I am working on a project where I have constraints that I can not use the Extractive methods to summarize an article and have to use BERT for this. If if this would have been a labelling problem (summarizing tweets, comments, questions) where I have the respective labels for the train data, I would have used the vectors from BERT as an input to <code>Keras</code> embedding layer with <code>LSTM</code> and build a model that with input and output labels. But the problem is that I have to summarize the text rather thhose labelled tweets and comments. Is there any I way (I am sure there is because I have been asked so specifically) that I can use BERT when I have the vectors corresponding the vocabulary?</p>
",Vectorization & Embeddings,using bert article summarization label expected output summary present article working project constraint use extractive method summarize article use bert would labelling problem summarizing tweet comment question respective label train data would used vector bert input embedding layer build model input output label problem summarize text rather thhose labelled tweet comment way sure asked specifically use bert vector corresponding vocabulary
Is there a way to create comparison and commonality wordclouds in python (as in r),"<p>i'm trying to compare 3 sets of documents using a wordcloud with python.
in R there is a simple comparison wordcloud which shows the common words (or weighted by tfidf) per category with different color in the same wordcloud.
also there is a commonality wordcloud that shows the most common words from all categories.</p>

<p>I didnt find a way to do that in python. the closest i got was Venn diagram but i couldn't make the word-sizes relate to the tfidf score.</p>

<p>is there a way to create those wordclouds in python?</p>
",Vectorization & Embeddings,way create comparison commonality wordclouds python r trying compare set document using wordcloud python r simple comparison wordcloud show common word weighted tfidf per category different color wordcloud also commonality wordcloud show common word category didnt find way python closest got wa venn diagram make word size relate tfidf score way create wordclouds python
Trying to simplify BERT architecture,"<p>I have an interesting question about BERT.</p>

<p>Can I simplify the architecture of the model by saying that the similarity of two words in different context will depend on the similarity of input embeddings making up different contexts? For example, can I say that the similarity of the embeddings of GLASS in the context DRINK_GLASS and WINE in the context LOVE_WINE will depend on the similarity of the input embeddings GLASS and WINE (last position) and DRINK and LOVE (first position)? Or should I also take into account the similarity between DRINK (first context, first position) and WINE (second context, second position) and LOVE and GLASS (viceversa)?</p>

<p>Thanks for your help, for now it is really difficult for me to understand exactly the architecture of Bert, but I'm trying to make experiments so I need to understand some basics. </p>
",Vectorization & Embeddings,trying simplify bert architecture interesting question bert simplify architecture model saying similarity two word different context depend similarity input embeddings making different context example say similarity embeddings glass context drink glass wine context love wine depend similarity input embeddings glass wine last position drink love first position also take account similarity drink first context first position wine second context second position love glass viceversa thanks help really difficult understand exactly architecture bert trying make experiment need understand basic
How to do prune in &quot;A Structured Self-Attentive Sentence Embedding&quot; paper,"<p>This paper ""A Structured Self-Attentive Sentence Embedding"" <a href=""https://arxiv.org/abs/1703.03130"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1703.03130</a> mentioned pruning in Appendix A.  I looked at all available implementation in GitHub, none does this.</p>

<p>I don't understand how it's done by reading the paper.  Is there implementation or explanation of how this can be done?</p>

<p>Thanks.</p>
",Vectorization & Embeddings,prune structured self attentive sentence embedding paper paper structured self attentive sentence embedding mentioned pruning appendix looked available implementation github none doe understand done reading paper implementation explanation done thanks
adding a new document to the term document matrix for similarity calculations,"<p>So I am aware there are several methods for finding a most similar or say three most similar documents in a corpus of documents. I know there can be scaling issues, for now I have around ten thousand documents and have been running tests on a subset of around thirty. This is what I've got for now but am considering looking into elasticsearch or doc2vec if this proves to be impossible or inefficient.</p>

<p>The scripts work very nicely so far, they use spaCy to tokenise the text and Sklearn TfidfVectorizer to fit accross all the documents, and very similar documents are found. I notice that shape of my NumPy object coming out of the pipeline is (33, 104354) which probably implies 104354 vocab excluding stopwords across all the 33 documents. That step takes a good twenty minutes to run, but the next step being a matrix multiplication which computes all the cosine similarities is very quick, but I know it might slow down as that matrix gets thousands rather than thirty rows.</p>

<p>If you could efficiently add a new document to the matrix, it wouldn't matter if the initial compute took ten hours or even days if you saved the result of that compute. </p>

<ol>
<li>When I press tab after the . there seems to be a method on the vectorizer called <code>vectorizer.fixed_vocabulary_</code> . I can't find this method on google or in SKlearn. Anyway, when I run the method, it returns <code>False</code>. Does anyone know what this is? Am thinking it might be useful to fix the vocabulary if possible otherwise it might be troublesome to add a new document to the term document matrix, although am not sure how to do that.</li>
</ol>

<p>Someone asked a <a href=""https://stackoverflow.com/questions/13986518/how-to-efficiently-compute-similarity-between-documents-in-a-stream-of-documents"">similar question</a> here which got voted up but nobody ever answered. </p>

<p>He wrote:</p>

<blockquote>
  <p>For new documents 
  what do I do when I get a new document doc(k)? Well, I have to compute the similarity of this document with all the previous ones, which doesn't require to build a whole matrix. I can just take the inner-product of doc(k) dot doc(j) for all previous j, and that result in S(k, j), which is great.</p>
</blockquote>

<ol start=""2"">
<li>Does anyone understand exactly what he means here or have any good links where this rather obscure topic is explained? Is he right? I somehow think that the ability to add new documents with this inner-product, if he is right, will depend on fixing the vocabulary as mentioned above.    </li>
</ol>
",Vectorization & Embeddings,adding new document term document matrix similarity calculation aware several method finding similar say three similar document corpus document know scaling issue around ten thousand document running test subset around thirty got considering looking elasticsearch doc vec prof impossible inefficient script work nicely far use spacy tokenise text sklearn tfidfvectorizer fit accross document similar document found notice shape numpy object coming pipeline probably implies vocab excluding stopwords across document step take good twenty minute run next step matrix multiplication computes cosine similarity quick know might slow matrix get thousand rather thirty row could efficiently add new document matrix matter initial compute took ten hour even day saved result compute press tab seems method vectorizer called find method google sklearn anyway run method return doe anyone know thinking might useful fix vocabulary possible otherwise might troublesome add new document term document matrix although sure someone asked doe anyone understand exactly mean good link rather obscure topic explained right somehow think ability add new document inner product right depend fixing vocabulary mentioned
Does summing up word embedding vectors in ML destroy their meaning?,"<p>For example, I have a paragraph which I want to classify in a binary manner. But because the inputs have to have a fixed length, I need to ensure that every paragraph is represented by a uniform quantity. </p>

<p>One thing I've done is taken every word in the paragraph, vectorized it using GloVe word2vec and then summed up all of the vectors to create a ""paragraph"" vector, which I've then fed in as an input for my model. In doing so, have I destroyed any meaning the words might have possessed? Considering these two sentences would have the same vector:
""My dog bit Dave"" &amp; ""Dave bit my dog"", how do I get around this? Am I approaching this wrong?</p>

<p>What other way can I train my model? If I take every word and feed that into my model, how do I know how many words I should take? How do I input these words? In the form of a 2D array, where each word vector is a column?</p>

<p>I want to be able to train a model that can classify text accurately. 
Surprisingly, I'm getting a high (>90%) for a relatively simple model like RandomForestClassifier just by using this summing up method. Any insights?</p>

<p>Edit: One suggestion I have received is to instead featurize my data as a 2D array where each word is a column, on which a CNN could work. Another suggestion I received was to use transfer learning through the huggingface transformer to get a vector for the whole paragraph. Which one is more feasible?</p>
",Vectorization & Embeddings,doe summing word embedding vector ml destroy meaning example paragraph want classify binary manner input fixed length need ensure every paragraph represented uniform quantity one thing done taken every word paragraph vectorized using glove word vec summed vector create paragraph vector fed input model destroyed meaning word might possessed considering two sentence would vector dog bit dave dave bit dog get around approaching wrong way train model take every word feed model know many word take input word form array word vector column want able train model classify text accurately surprisingly getting high relatively simple model like randomforestclassifier using summing method insight edit one suggestion received instead featurize data array word column cnn could work another suggestion received wa use transfer learning huggingface transformer get vector whole paragraph one feasible
Understanding gensim Word2Vec most_similar results for 3 words,"<p>I construct sentences using 3 words ""1"", ""2"", ""3"", in different ways, and observe that the word vectors are unchanged for each of these words.</p>

<p>Following are the different sentences</p>

<p>Type 1: [[""1"", ""2""], [""1"", ""3""]]</p>

<p>Type 2: [[""1"", ""2"", ""3""]]</p>

<p>Type 3: [[""1"", ""2""], [""3""]]</p>

<p>I am training <code>Word2Vec</code> model as follows</p>

<pre><code>model = Word2Vec(sentences,min_count=1,size=2)
print (model.wv.most_similar(""1""))
print (model.wv.most_similar(""2""))
print (model.wv.most_similar(""3""))
print (model.wv['1'])
print (model.wv['2'])
print (model.wv['3'])
</code></pre>

<p>And results are same on changing the sentence type</p>

<pre><code>[('3', 0.5377859473228455), ('2', -0.5831003785133362)]
[('1', -0.5831003189086914), ('3', -0.9985027313232422)]
[('1', 0.5377858281135559), ('2', -0.9985026717185974)]
[-0.24893647 -0.24495095]
[ 0.19231372 -0.03319569]
[-0.22207274  0.05098101]
</code></pre>

<p>Also when I change word ""1"" to suppose ""101"", the result changes</p>

<pre><code>[('3', 0.5407046675682068), ('2', -0.5859125256538391)]
[('101', -0.5859125256538391), ('3', -0.9985027313232422)]
[('101', 0.540704607963562), ('2', -0.9985026717185974)]
[-0.05898098 -0.0576357 ]
[ 0.19231372 -0.03319569]
[-0.22207274  0.05098101]
</code></pre>

<p>I wanted to know </p>

<ol>
<li><p>Why the results didn't change when I changed the sentences?</p></li>
<li><p>Why results changed when I just updated the value?</p></li>
</ol>
",Vectorization & Embeddings,understanding gensim word vec similar result word construct sentence using word different way observe word vector unchanged word following different sentence type type type training model follows result changing sentence type also change word suppose result change wanted know result change changed sentence result changed updated value
how to download co-occurance matrix for glove pre-trained word vectors,"<p>I am trying to train glove embeddings on a domain corpus, trying to experiment with incremental training with existing glove vectors, but unable to get the co-occurrence matrix for glove pre-trained word vectors. how do I get the <strong>co-occurrence matrix</strong>?</p>
",Vectorization & Embeddings,download co occurance matrix glove pre trained word vector trying train glove embeddings domain corpus trying experiment incremental training existing glove vector unable get co occurrence matrix glove pre trained word vector get co occurrence matrix
NLP - Separate punctuations only at the start and end of the words,"<p>I'm new at NLP and I'm trying basic preprocessing steps while learning. I'm trying to separate punctuations at the start and end of words for embeddings. While doing that, I don't want to damage words like <code>can't</code>, <code>I'm</code>, etc. because I'm handling them separately.</p>

<pre><code>s = 'This is what I'm trying to do, but I can't figure out how.'
</code></pre>

<p>Desired output:</p>

<pre><code>s_separated = 'This is what I'm trying to do , but I can't figure out how .'
</code></pre>
",Vectorization & Embeddings,nlp separate punctuation start end word new nlp trying basic preprocessing step learning trying separate punctuation start end word embeddings want damage word like etc handling separately desired output
Word Mover&#39;s Distance vs Cosine Similarity,"<p>I wonder which algorithm is the best for semantic similarity? Can anyone explain why? </p>

<p>Thank you!</p>
",Vectorization & Embeddings,word mover distance v cosine similarity wonder algorithm best semantic similarity anyone explain thank
Should I remove duplicates from a column(Embedding layer input)?,"<p>So say my dataset has two columns: movies watched by a user and genres of these movies.</p>

<p>Example:</p>

<p>User| Movies watched | genres</p>

<p>Movies watched is a list of movieIDs. Ex [1,4,6...] And genres is a string of all the genres of all those movies separated by a space. Ex. ""action adventure ..."". </p>

<p>Example of the dataset:</p>

<p>User1 | [1,2,5...] | ""Adventure comedy adventure action...""</p>

<p>User2 | [1,3,23,24...] | ""Adventure documentary action action thriller...""</p>

<p>User3 | [11,21,77...] | ""comedy scifi scifi adventure...""</p>

<p>Totally 610 users. That is, 610 rows.</p>

<p>So my input would be, the moviesId list and the genres list. </p>

<p>Note: Total number of movies 9742. Maximum movies watched by any user is 2600. And there are 24 different genres. And a single movie can have multiple genres. </p>

<p>Say I pass these two to an embedding layer. Do I pass the tokenized list after removing duplicate/repeating genres or the tokenized list as it is?</p>

<p>That is, the input would be movieId list of length 2600(padded) and genres list(tokenized) of length(if duplicates are removed) 24, both to two separate embedding layers whose results are concatenated and the next layer would be lstm. </p>

<p>Thankyou! </p>
",Vectorization & Embeddings,remove duplicate column embedding layer input say dataset ha two column movie watched user genre movie example user movie watched genre movie watched list movieids ex genre string genre movie separated space ex action adventure example dataset user adventure comedy adventure action user adventure documentary action action thriller user comedy scifi scifi adventure totally user row input would moviesid list genre list note total number movie maximum movie watched user different genre single movie multiple genre say pas two embedding layer pas tokenized list removing duplicate repeating genre tokenized list input would movieid list length padded genre list tokenized length duplicate removed two separate embedding layer whose result concatenated next layer would lstm thankyou
Calculating cosine-similarity for non English text strings,"<p>Want to create a Python script that can compare two text strings and decide how simular they are. Best method I found for comparison is cosine-similarity. </p>

<p>To understand the words in each string I started locking at Word2vec but read that numberbatch method is better. </p>

<p>Did find an older post with some good implementations in the answers. But this only supports the English language and I need Swedish. 
<a href=""https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings/59517730#59517730"">Calculate cosine similarity given 2 sentence strings</a></p>

<p>Is there any better way of comparing similarities between two short strings?</p>

<p>And how to do it for Swedish language? </p>
",Vectorization & Embeddings,calculating cosine similarity non english text string want create python script compare two text string decide simular best method found comparison cosine similarity understand word string started locking word vec read numberbatch method better find older post good implementation answer support english language need swedish href cosine similarity given sentence string better way comparing similarity two short string swedish language
Remove synonyms of TFIDF results in python,"<p>I am currently working on a project where get the top 10 most relevant words of set of document using tfidf in python. However, there are results where are get the same word and its plurial or adverb or so. To go around this problem, I decided to use stemming, but this leads to a problem where words and their antonyms can have the same root or by reducing a word to its root does not enable to go back and find that specific word in the document if a user was to search for it. Is there a nlp that might be better in this context than nlp? Any hint or link will be useful. I working on something that is very similar to youtube.</p>
",Vectorization & Embeddings,remove synonym tfidf result python currently working project get top relevant word set document using tfidf python however result get word plurial adverb go around problem decided use stemming lead problem word antonym root reducing word root doe enable go back find specific word document user wa search nlp might better context nlp hint link useful working something similar youtube
Sentiment Classification using Doc2Vec,"<p>I am confused as to how I can use <strong>Doc2Vec(using Gensim)</strong> for IMDB sentiment classification dataset. I have got the Doc2Vec embeddings after training on my corpus and built my Logistic Regression model using it. How do I use it to make predictions for new reviews? sklearn TF-IDF has a <em>transform</em> method that can be used on test data after training on training data, what is its equivalent in Gensim Doc2Vec?</p>
",Vectorization & Embeddings,sentiment classification using doc vec confused use doc vec using gensim imdb sentiment classification dataset got doc vec embeddings training corpus built logistic regression model using use make prediction new review sklearn tf idf ha transform method used test data training training data equivalent gensim doc vec
Keras - Embedding Layer Input Error and corresponding input_length error,"<p>I am facing an error while taking <code>Input</code> where <code>Embedding</code> is my first layer. It is unable to find the tensor of shape <code>(,9)</code> although I have clearly mentioned the shape in <code>Input()</code>. Can someone help me out of this?</p>

<p>Code is as follows:</p>

<pre><code>def model_3(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):

    _nput = Input(shape=[src_timesteps], dtype='int32')
    embedding = Embedding(input_dim = src_vocab, output_dim = n_units, input_length=src_timesteps, mask_zero=False)(_nput)
    activations = LSTM(n_units, return_sequences=True)(embedding)
    attention = Dense(1, activation='tanh')(activations)
    attention = Flatten()(attention)
    attention = Activation('softmax')(attention)
    attention = RepeatVector(tar_timesteps)(attention)
    activations = Permute([2,1])(activations)
    sent_representation = dot([attention,activations], axes=-1)
    sent_representation = LSTM(n_units, return_sequences=True)(sent_representation)
    sent_representation = TimeDistributed(Dense(tar_vocab, activation='softmax'))(sent_representation)
    model = Model(input=_nput,output=sent) 
    model.compile(optimizer='adam', loss='categorical_crossentropy')
    print(model.summary())
    plot_model(model, to_file='model.png', show_shapes=True)
</code></pre>
",Vectorization & Embeddings,kera embedding layer input error corresponding input length error facing error taking first layer unable find tensor shape although clearly mentioned shape someone help code follows
What to do when Seq2Seq network repeats words over and over in output?,"<p>So, I've been working on a project for a while, we have <em>very</em> little data, I know it would become much better if we were able to put together a much much larger dataset. That aside, my issue at the moment is when I have a sentence input, my outputs look like this right now:</p>

<blockquote>
  <p>contactid contactid contactid contactid</p>
</blockquote>

<p>A single word is focused on and repeated over and over again. What can I do to overcome this hurdle?</p>

<p>Things I've tried:</p>

<ol>
<li>Double checked I was appending start/stop tokens and make sure the tokens were properly placed in the top of their vocab files, I am sharing vocab.</li>
<li>I found something saying it could be due to poor word embeddings. To that end I checked with tensorboard and sure enough PCA showed a very dense cluster of points. Seeing that I grabbed Facebook's public pre trained word vectors and loaded them in as the embedding. Trained again and this time tensorboard PCA showed a much better picture.</li>
<li>Switched my training scheduler from basic to SampledScheduling to occasionally replace a training output with the ground truth.</li>
<li>Switched my decoder to use the beam search decoder I figured this may give more robust responses if the word choices were close together in the intermediary feature space.</li>
</ol>

<p>For certain my perplexity is steadily decreasing.</p>

<p>Here is my dataset preperation code:</p>

<pre><code>class ModelInputs(object):
""""""Factory to construct various input hooks and functions depending on mode """"""

def __init__(
    self, vocab_files, batch_size,
    share_vocab=True, src_eos_id=1, tgt_eos_id=2
):
    self.batch_size = batch_size
    self.vocab_files = vocab_files
    self.share_vocab = share_vocab
    self.src_eos_id = src_eos_id
    self.tgt_eos_id = tgt_eos_id

def get_inputs(self, file_path, num_infer=None, mode=tf.estimator.ModeKeys.TRAIN):
    self.mode = mode
    if self.mode == tf.estimator.ModeKeys.TRAIN:
        return self._training_input_hook(file_path)
    if self.mode == tf.estimator.ModeKeys.EVAL:
        return self._validation_input_hook(file_path)
    if self.mode == tf.estimator.ModeKeys.PREDICT:
        if num_infer is None:
            raise ValueError('If performing inference must supply number of predictions to be made.')
        return self._infer_input_hook(file_path, num_infer)

def _prepare_data(self, dataset, out=False):
    prep_set = dataset.map(lambda string: tf.string_split([string]).values)
    prep_set = prep_set.map(lambda words: (words, tf.size(words)))
    if out == True:
        return prep_set.map(lambda words, size: (self.vocab_tables[1].lookup(words), size))
    return prep_set.map(lambda words, size: (self.vocab_tables[0].lookup(words), size))

def _batch_data(self, dataset, src_eos_id, tgt_eos_id):
    batched_set = dataset.padded_batch(
            self.batch_size,
            padded_shapes=((tf.TensorShape([None]), tf.TensorShape([])), (tf.TensorShape([None]), tf.TensorShape([]))),
            padding_values=((src_eos_id, 0), (tgt_eos_id, 0))
    )
    return batched_set

def _batch_infer_data(self, dataset, src_eos_id):
    batched_set = dataset.padded_batch(
        self.batch_size,
        padded_shapes=(tf.TensorShape([None]), tf.TensorShape([])),
        padding_values=(src_eos_id, 0)
    )
    return batched_set

def _create_vocab_tables(self, vocab_files, share_vocab=False):
    if vocab_files[1] is None and share_vocab == False:
        raise ValueError('If share_vocab is set to false must provide target vocab. (src_vocab_file, \
                target_vocab_file)')

    src_vocab_table = lookup_ops.index_table_from_file(
        vocab_files[0],
        default_value=UNK_ID
    )

    if share_vocab:
        tgt_vocab_table = src_vocab_table
    else:
        tgt_vocab_table = lookup_ops.index_table_from_file(
            vocab_files[1],
            default_value=UNK_ID
        )

    return src_vocab_table, tgt_vocab_table

def _prepare_iterator_hook(self, hook, scope_name, iterator, file_path, name_placeholder):
    if self.mode == tf.estimator.ModeKeys.TRAIN or self.mode == tf.estimator.ModeKeys.EVAL:
        feed_dict = {
                name_placeholder[0]: file_path[0],
                name_placeholder[1]: file_path[1]
        }
    else:
        feed_dict = {name_placeholder: file_path}

    with tf.name_scope(scope_name):
        hook.iterator_initializer_func = \
                lambda sess: sess.run(
                    iterator.initializer,
                    feed_dict=feed_dict,
                )

def _set_up_train_or_eval(self, scope_name, file_path):
    hook = IteratorInitializerHook()
    def input_fn():
        with tf.name_scope(scope_name):
            with tf.name_scope('sentence_markers'):
                src_eos_id = tf.constant(self.src_eos_id, dtype=tf.int64)
                tgt_eos_id = tf.constant(self.tgt_eos_id, dtype=tf.int64)
            self.vocab_tables = self._create_vocab_tables(self.vocab_files, self.share_vocab)
            in_file = tf.placeholder(tf.string, shape=())
            in_dataset = self._prepare_data(tf.contrib.data.TextLineDataset(in_file).repeat(None))
            out_file = tf.placeholder(tf.string, shape=())
            out_dataset = self._prepare_data(tf.contrib.data.TextLineDataset(out_file).repeat(None))
            dataset = tf.contrib.data.Dataset.zip((in_dataset, out_dataset))
            dataset = self._batch_data(dataset, src_eos_id, tgt_eos_id)
            iterator = dataset.make_initializable_iterator()
            next_example, next_label = iterator.get_next()
            self._prepare_iterator_hook(hook, scope_name, iterator, file_path, (in_file, out_file))
            return next_example, next_label

    return (input_fn, hook)

def _training_input_hook(self, file_path):
    input_fn, hook = self._set_up_train_or_eval('train_inputs', file_path)

    return (input_fn, hook)

def _validation_input_hook(self, file_path):
    input_fn, hook = self._set_up_train_or_eval('eval_inputs', file_path)

    return (input_fn, hook)

def _infer_input_hook(self, file_path, num_infer):
    hook = IteratorInitializerHook()

    def input_fn():
        with tf.name_scope('infer_inputs'):
            with tf.name_scope('sentence_markers'):
                src_eos_id = tf.constant(self.src_eos_id, dtype=tf.int64)
            self.vocab_tables = self._create_vocab_tables(self.vocab_files, self.share_vocab)
            infer_file = tf.placeholder(tf.string, shape=())
            dataset = tf.contrib.data.TextLineDataset(infer_file)
            dataset = self._prepare_data(dataset)
            dataset = self._batch_infer_data(dataset, src_eos_id)
            iterator = dataset.make_initializable_iterator()
            next_example, seq_len = iterator.get_next()
            self._prepare_iterator_hook(hook, 'infer_inputs', iterator, file_path, infer_file)
            return ((next_example, seq_len), None)

    return (input_fn, hook)
</code></pre>

<p>And here is my model:</p>

<pre><code>class Seq2Seq():

def __init__(
    self, batch_size, inputs,
    outputs, inp_vocab_size, tgt_vocab_size,
    embed_dim, mode, time_major=False,
    enc_embedding=None, dec_embedding=None, average_across_batch=True,
    average_across_timesteps=True, vocab_path=None, embedding_path='./data_files/wiki.simple.vec'
):
    embed_np = self._get_embedding(embedding_path)
    if not enc_embedding:
        self.enc_embedding = tf.contrib.layers.embed_sequence(
            inputs,
            inp_vocab_size,
            embed_dim,
            trainable=True,
            scope='embed',
            initializer=tf.constant_initializer(value=embed_np, dtype=tf.float32)
        )
    else:
        self.enc_embedding = enc_embedding
    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:
        if not dec_embedding:
            embed_outputs = tf.contrib.layers.embed_sequence(
                outputs,
                tgt_vocab_size,
                embed_dim,
                trainable=True,
                scope='embed',
                reuse=True
            )
            with tf.variable_scope('embed', reuse=True):
                dec_embedding = tf.get_variable('embeddings')
            self.embed_outputs = embed_outputs
            self.dec_embedding = dec_embedding

        else:
            self.dec_embedding = dec_embedding
    else:
        with tf.variable_scope('embed', reuse=True):
            self.dec_embedding = tf.get_variable('embeddings')

    if mode == tf.estimator.ModeKeys.PREDICT and vocab_path is None:
        raise ValueError('If mode is predict, must supply vocab_path')
    self.vocab_path = vocab_path
    self.inp_vocab_size = inp_vocab_size
    self.tgt_vocab_size = tgt_vocab_size
    self.average_across_batch = average_across_batch
    self.average_across_timesteps = average_across_timesteps
    self.time_major = time_major
    self.batch_size = batch_size
    self.mode = mode

def _get_embedding(self, embedding_path):
    model = KeyedVectors.load_word2vec_format(embedding_path)
    vocab = model.vocab
    vocab_len = len(vocab)
    return np.array([model.word_vec(k) for k in vocab.keys()])

def _get_lstm(self, num_units):
    return tf.nn.rnn_cell.BasicLSTMCell(num_units)

def encode(self, num_units, num_layers, seq_len, cell_fw=None, cell_bw=None):
    if cell_fw and cell_bw:
        fw_cell = cell_fw
        bw_cell = cell_bw
    else:
        fw_cell = self._get_lstm(num_units)
        bw_cell = self._get_lstm(num_units)
    encoder_outputs, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(
        fw_cell,
        bw_cell,
        self.enc_embedding,
        sequence_length=seq_len,
        time_major=self.time_major,
        dtype=tf.float32
    )
    c_state = tf.concat([bi_encoder_state[0].c, bi_encoder_state[1].c], axis=1)
    h_state = tf.concat([bi_encoder_state[0].h, bi_encoder_state[1].h], axis=1)
    encoder_state = tf.contrib.rnn.LSTMStateTuple(c=c_state, h=h_state)
    return tf.concat(encoder_outputs, -1), encoder_state

def _train_decoder(self, decoder_cell, out_seq_len, encoder_state, helper):
    if not helper:
        helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(
            self.embed_outputs,
            out_seq_len,
            self.dec_embedding,
            0.3,
        )
        # helper = tf.contrib.seq2seq.TrainingHelper(
        #     self.dec_embedding,
        #     out_seq_len,
        # )
    projection_layer = layers_core.Dense(self.tgt_vocab_size, use_bias=False)
    decoder = tf.contrib.seq2seq.BasicDecoder(
        decoder_cell,
        helper,
        encoder_state,
        output_layer=projection_layer
    )
    return decoder

def _predict_decoder(self, cell, encoder_state, beam_width, length_penalty_weight):
    tiled_encoder_state = tf.contrib.seq2seq.tile_batch(
        encoder_state, multiplier=beam_width
    )
    with tf.name_scope('sentence_markers'):
        sos_id = tf.constant(1, dtype=tf.int32)
        eos_id = tf.constant(2, dtype=tf.int32)
    start_tokens = tf.fill([self.batch_size], sos_id)
    end_token = eos_id
    projection_layer = layers_core.Dense(self.tgt_vocab_size, use_bias=False)
    emb = tf.squeeze(self.dec_embedding)
    decoder = tf.contrib.seq2seq.BeamSearchDecoder(
        cell=cell,
        embedding=self.dec_embedding,
        start_tokens=start_tokens,
        end_token=end_token,
        initial_state=tiled_encoder_state,
        beam_width=beam_width,
        output_layer=projection_layer,
        length_penalty_weight=length_penalty_weight
    )
    return decoder

def decode(
    self, num_units, out_seq_len,
    encoder_state, cell=None, helper=None,
    beam_width=None, length_penalty_weight=None
):
    with tf.name_scope('Decode'):
        if cell:
            decoder_cell = cell
        else:
            decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(2*num_units)
        if self.mode != estimator.ModeKeys.PREDICT:
            decoder = self._train_decoder(decoder_cell, out_seq_len, encoder_state, helper)
        else:
            decoder = self._predict_decoder(decoder_cell, encoder_state, beam_width, length_penalty_weight)
        outputs = tf.contrib.seq2seq.dynamic_decode(
            decoder,
            maximum_iterations=20,
            swap_memory=True,
        )
        outputs = outputs[0]
        if self.mode != estimator.ModeKeys.PREDICT:
            return outputs.rnn_output, outputs.sample_id
        else:
            return outputs.beam_search_decoder_output, outputs.predicted_ids

def prepare_predict(self, sample_id):
    rev_table = lookup_ops.index_to_string_table_from_file(
        self.vocab_path, default_value=UNK)
    predictions = rev_table.lookup(tf.to_int64(sample_id))
    return tf.estimator.EstimatorSpec(
        predictions=predictions,
        mode=tf.estimator.ModeKeys.PREDICT
    )

def prepare_train_eval(
    self, t_out,
    out_seq_len, labels, lr,
    train_op=None, loss=None
):
    if not loss:
        weights = tf.sequence_mask(
            out_seq_len,
            dtype=t_out.dtype
        )
        loss = tf.contrib.seq2seq.sequence_loss(
            t_out,
            labels,
            weights,
            average_across_batch=self.average_across_batch,
        )

    if not train_op:
        train_op = tf.contrib.layers.optimize_loss(
            loss,
            tf.train.get_global_step(),
            optimizer='SGD',
            learning_rate=lr,
            summaries=['loss', 'learning_rate']
        )

    return tf.estimator.EstimatorSpec(
        mode=self.mode,
        loss=loss,
        train_op=train_op,
    )
</code></pre>
",Vectorization & Embeddings,seq seq network repeat word output working project little data know would become much better able put together much much larger dataset aside issue moment sentence input output look like right contactid contactid contactid contactid single word focused repeated overcome hurdle thing tried double checked wa appending start stop token make sure token properly placed top vocab file sharing vocab found something saying could due poor word embeddings end checked tensorboard sure enough pca showed dense cluster point seeing grabbed facebook public pre trained word vector loaded embedding trained time tensorboard pca showed much better picture switched training scheduler basic sampledscheduling occasionally replace training output ground truth switched decoder use beam search decoder figured may give robust response word choice close together intermediary feature space certain perplexity decreasing dataset preperation code model
Natural Language Processing techniques for understanding contextual words,"<p>Take the following sentence:</p>

<pre><code>I'm going to change the light bulb
</code></pre>

<p>The meaning of <code>change</code> means <code>replace</code>, as in someone is going to replace the light bulb. This could easily be solved by using a dictionary api or something similar. However, the following sentences</p>

<pre><code>I need to go the bank to change some currency

You need to change your screen brightness
</code></pre>

<p>The first sentence does not mean <code>replace</code> anymore, it means <code>Exchange</code>and the second sentence, <code>change</code> means <code>adjust</code>.</p>

<p>If you were trying to understand the meaning of <code>change</code> in this situation, what techniques would someone use to extract the correct definition based off of the context of the sentence? What is what I'm trying to do called?</p>

<p>Keep in mind, the input would only be one sentence. So something like:</p>

<pre><code>Screen brightness is typically too bright on most peoples computers.
People need to change the brightness to have healthier eyes.
</code></pre>

<p>Is not what I'm trying to solve, because you can use the previous sentence to set the context. Also this would be for lots of different words, not just the word <code>change</code>.</p>

<p>Appreciate the suggestions.</p>

<p><strong>Edit:</strong> I'm aware that various embedding models can help gain insight on this problem. If this is your answer, how do you interpret the word embedding that is returned? These arrays can be upwards of 500+ in length which isn't practical to interpret. </p>
",Vectorization & Embeddings,natural language processing technique understanding contextual word take following sentence meaning mean someone going replace light bulb could easily solved using dictionary api something similar however following sentence first sentence doe mean anymore mean second sentence mean trying understand meaning situation technique would someone use extract correct definition based context sentence trying called keep mind input would one sentence something like trying solve use previous sentence set context also would lot different word word appreciate suggestion edit aware various embedding model help gain insight problem answer interpret word embedding returned array upwards length practical interpret
How to use pretrained word2vec vectors in doc2vec model?,"<p>I am trying to implement doc2vec, but I am not sure how the input for the model should look like if I have pretrained word2vec vectors.</p>

<p>The problem is, that I am not sure how to theoretically use pretrained word2vec vectors for doc2vec. I imagine, that I could prefill the hidden layer with the vectors and the rest of the hidden layer fill with random numbers</p>

<p>Another idea is to use the vector as input for word instead of a one-hot-encoding but I am not sure if the output vectors for docs would make sense.</p>

<p>Thank you for your answer!</p>
",Vectorization & Embeddings,use pretrained word vec vector doc vec model trying implement doc vec sure input model look like pretrained word vec vector problem sure theoretically use pretrained word vec vector doc vec imagine could prefill hidden layer vector rest hidden layer fill random number another idea use vector input word instead one hot encoding sure output vector doc would make sense thank answer
TFIDF vectorizer: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(),"<p>I've been trying to apply TFIDF vectorizer on a gensim LDA model with no success. It looks like I have to use any() or all() but I'm not sure what is going on with the vectorizer. The data has been cleaned and pre-processed.</p>

<p>Data:</p>

<pre><code>text_data=
0         [new, leaked, treasury, document, full, sugges...
1         [tommy, robinson, endorsing, boris, johnson's,...
2         [thanks, already, watched, corbyn, catch, tv, ...
3         [treasury, document, check, boris, johnson, to...
</code></pre>

<p>Code:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

# Create dummy function to initialize

def dummy_fun(doc):
    return doc

tfidf = TfidfVectorizer(
    analyzer='word',
    tokenizer=dummy_fun,
    preprocessor=dummy_fun,
    token_pattern=None)  

# Fit and transform on text_data
tfidf_corpus = tfidf.fit_transform(text_data)

# Use LDA model to find 10 topics in the data, based on TFIDF vectorizer

ldamodel_tfidf = gensim.models.ldamodel.LdaModel(corpus=tfidf_corpus,
                                       #id2word=id2word,
                                       num_topics=10, 
                                       random_state=42,
                                       update_every=1,
                                       chunksize=100,
                                       passes=10,
                                       alpha='auto',
                                       per_word_topics=True)
</code></pre>

<p>The error is asking that I use any() or all(), but why is it asking for this?</p>

<pre><code>Traceback (most recent call last)
    &lt;ipython-input-25-57489833d281&gt; in &lt;module&gt;
          9                                            passes=10,
         10                                            alpha='auto',
    ---&gt; 11                                            per_word_topics=True)

    /opt/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py in __init__(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)
        431         if self.id2word is None:
        432             logger.warning(""no word id mapping provided; initializing from corpus, assuming identity"")
    --&gt; 433             self.id2word = utils.dict_from_corpus(corpus)
        434             self.num_terms = len(self.id2word)
        435         elif len(self.id2word) &gt; 0:

    /opt/anaconda3/lib/python3.7/site-packages/gensim/utils.py in dict_from_corpus(corpus)
        824 
        825     """"""
    --&gt; 826     num_terms = 1 + get_max_id(corpus)
        827     id2word = FakeDict(num_terms)
        828     return id2word

    /opt/anaconda3/lib/python3.7/site-packages/gensim/utils.py in get_max_id(corpus)
        733     maxid = -1
        734     for document in corpus:
    --&gt; 735         if document:
        736             maxid = max(maxid, max(fieldid for fieldid, _ in document))
        737     return maxid

    /opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py in __bool__(self)
        285             return self.nnz != 0
        286         else:
    --&gt; 287             raise ValueError(""The truth value of an array with more than one ""
        288                              ""element is ambiguous. Use a.any() or a.all()."")
        289     __nonzero__ = __bool__

    ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().
</code></pre>
",Vectorization & Embeddings,tfidf vectorizer truth value array one element ambiguous use trying apply tfidf vectorizer gensim lda model success look like use sure going vectorizer data ha cleaned pre processed data code error asking use asking
Text classification with torchnlp,"<p>i'm trying to build a neural network using <strong>pytorch-nlp</strong> (<a href=""https://pytorchnlp.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">https://pytorchnlp.readthedocs.io/en/latest/</a>).
My intent is to build a network like this:</p>
<ul>
<li>Embedding layer (uses pytorch standard layer and <code>from_pretrained</code> method)</li>
<li>Encoder with LSTM (also uses standard <code>nn.LSTM</code>)</li>
<li>Attention mechanism (uses <code>torchnlp.nn.Attention</code>)</li>
<li>Decoder siwth LSTM (as encoder)</li>
<li>Linear layer standard</li>
</ul>
<p>I'm encountering a major p<em>roblem with the dimensions</em> of the input sentences (each word is a vector) but most importantly with the <em>attention layer</em> : I don't know how to declare it because i need the exact dimensions of the output from the encoder, but the sequences have varying dimensions (corresponding to the fact that sentences have different number of words).</p>
<p>I've tried to look at <code>torch.nn.utils.rnn.pad_packed_sequence</code> and <code>torch.nn.utils.rnn.pack_padded_sequence</code> since they're supported by LSTM, but i cannot find the solution.</p>
<p>Can anyone help me?</p>
<h2>EDIT</h2>
<p>I thought about padding all sequences to a specific dimension, but I don't want to truncate longer sequences because I want to keep all the information.</p>
",Vectorization & Embeddings,text classification torchnlp trying build neural network using pytorch nlp intent build network like embedding layer us pytorch standard layer method encoder lstm also us standard attention mechanism us decoder siwth lstm encoder linear layer standard encountering major problem dimension input sentence word vector importantly attention layer know declare need exact dimension output encoder sequence varying dimension corresponding fact sentence different number word tried look since supported lstm find solution anyone help edit thought padding sequence specific dimension want truncate longer sequence want keep information
Training a Neural Network for Word Embedding,"<p><a href=""https://github.com/siddharth-agrawal/Neural-Tensor-Network/blob/master/entities.txt"" rel=""nofollow noreferrer"">Attached</a> is the link file for Entities. I want to train a Neural Network to represent each entity into a vector. Attach is my code for training</p>

<pre><code>import pandas as pd
import numpy as np

from numpy import array
from keras.preprocessing.text import one_hot

from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.models import Model
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Input


from keras.layers.embeddings import Embedding
from sklearn.model_selection import train_test_split 

file_path = '/content/drive/My Drive/Colab Notebooks/Deep Learning/NLP/Data/entities.txt'
df = pd.read_csv(file_path, delimiter = '\t', engine='python', quoting = 3, header = None)
df.columns = ['Entity']
Entity = df['Entity']

X_train, X_test = train_test_split(Entity, test_size = 0.10)
print('Total Entities: {}'.format(len(Entity)))
print('Training Entities: {}'.format(len(X_train)))
print('Test Entities: {}'.format(len(X_test)))
vocab_size = len(Entity)
X_train_encode = [one_hot(d, vocab_size,lower=True, split=' ') for d in X_train]
X_test_encode = [one_hot(d, vocab_size,lower=True, split=' ') for d in X_test]
model = Sequential()
model.add(Embedding(input_length=1,input_dim=vocab_size, output_dim=100))
model.add(Flatten())
model.add(Dense(vocab_size, activation='softmax'))

model.compile(optimizer='adam', loss='mse', metrics=['acc'])
print(model.summary())

model.fit(X_train_encode, X_train_encode, epochs=20, batch_size=1000, verbose=1)
</code></pre>

<p>The following error encountered when I am trying to execute the code.</p>

<blockquote>
  <p>Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 34826 arrays:</p>
</blockquote>
",Vectorization & Embeddings,training neural network word embedding attached link file entity want train neural network represent entity vector attach code training following error encountered trying execute code error checking model input list numpy array passing model size model expected expected see array instead got following list array
Python: list object has no attribute &#39;lower&#39; - but corpus is already in lower case,"<p>My corpus is a series of documents with twitter data, and has been cleaned and pre-processed to be best of my knowledge (even including emoji)- example below:</p>

<pre><code>    0         [national, interest, think, worth, holding, ta...
    1         [must, accurate, diane, abbott, done, calculat...
</code></pre>

<p>I then instantiate TFIDF:</p>

<pre><code>    # Instantiate vectoriser
    vect = TfidfVectorizer()

    # Fit
    vect = TfidfVectorizer(min_df=10, ngram_range = (1,3)).fit(text)
</code></pre>

<p>When I try to fit this, I get: </p>

<pre><code>   AttributeError: 'list' object has no attribute 'lower' 
</code></pre>

<p>But I've already converted everything to lower case. Is this something to do with the fact that it's a series?</p>
",Vectorization & Embeddings,python list object ha attribute lower corpus already lower case corpus series document twitter data ha cleaned pre processed best knowledge even including emoji example instantiate tfidf try fit get already converted everything lower case something fact series
Keras Multitask learning with two different input sample size,"<p>I am implementing multitask regression model using code from the Keras <a href=""https://keras.io/getting-started/functional-api-guide/"" rel=""noreferrer"">API</a> under the shared layers section.</p>

<p>There are two data sets, Let's call them <code>data_1</code> and <code>data_2</code> as follows.</p>

<pre><code>data_1 : shape(1434, 185, 37)
data_2 : shape(283, 185, 37)
</code></pre>

<p><code>data_1</code> is consists of 1434 samples, each sample is 185 characters long and 37 shows total number of unique characters is 37 or in another words the <code>vocab_size</code>. Comparatively <code>data_2</code> consists of 283 characters.</p>

<p>I convert the <code>data_1</code> and <code>data_2</code> into two dimensional numpy array as follows before giving it to the Embedding layer.</p>

<pre><code>data_1=np.argmax(data_1, axis=2)
data_2=np.argmax(data_2, axis=2)
</code></pre>

<p>That makes the shape of the data as follows.</p>

<pre><code>print(np.shape(data_1)) 
(1434, 185)

print(np.shape(data_2)) 
(283, 185)
</code></pre>

<p>Each number in the matrix represents index integer.</p>

<p>The multitask model is as under.</p>

<pre><code>user_input = keras.layers.Input(shape=((185, )), name='Input_1')
products_input =  keras.layers.Input(shape=((185, )), name='Input_2')

shared_embed=(keras.layers.Embedding(vocab_size, 50, input_length=185))

user_vec_1 = shared_embed(user_input )
user_vec_2 = shared_embed(products_input )


input_vecs = keras.layers.concatenate([user_vec_1, user_vec_2], name='concat')

input_vecs_1=keras.layers.Flatten()(input_vecs)
input_vecs_2=keras.layers.Flatten()(input_vecs)

# Task 1 FC layers
nn = keras.layers.Dense(90, activation='relu',name='layer_1')(input_vecs_1)
result_a = keras.layers.Dense(1, activation='linear', name='output_1')(nn)



# Task 2 FC layers
nn1 = keras.layers.Dense(90, activation='relu', name='layer_2')(input_vecs_2)
result_b = keras.layers.Dense(1, activation='linear',name='output_2')(nn1) 

model = Model(inputs=[user_input , products_input], outputs=[result_a, result_b])

model.compile(optimizer='rmsprop',
              loss='mse',
              metrics=['accuracy'])
</code></pre>

<p>The model is visualized as follows.
<a href=""https://i.sstatic.net/ni7nK.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/ni7nK.png"" alt=""enter image description here""></a></p>

<p>Then I fit the model as follows.</p>

<pre><code>model.fit([data_1, data_2], [Y_1,Y_2], epochs=10)
</code></pre>

<p><strong>Error:</strong></p>

<pre><code>ValueError: All input arrays (x) should have the same number of samples. Got array shapes: [(1434, 185), (283, 185)]
</code></pre>

<p>Is there any way in Keras where I can use two different sample size inputs or to some trick to avoid this error to achieve my goal of multitasking regression.</p>

<p><strong>Here is the minimum working code for testing</strong>.</p>

<pre><code>data_1=np.array([[25,  5, 11, 24,  6],
       [25,  5, 11, 24,  6],
       [25,  0, 11, 24,  6],
       [25, 11, 28, 11, 24],
       [25, 11,  6, 11, 11]])

data_2=np.array([[25, 11, 31,  6, 11],
       [25, 11, 28, 11, 31],
       [25, 11, 11, 11, 31]])

Y_1=np.array([[2.33],
       [2.59],
       [2.59],
       [2.54],
       [4.06]])


Y_2=np.array([[2.9],
       [2.54],
       [4.06]])



user_input = keras.layers.Input(shape=((5, )), name='Input_1')
products_input =  keras.layers.Input(shape=((5, )), name='Input_2')

shared_embed=(keras.layers.Embedding(37, 3, input_length=5))
user_vec_1 = shared_embed(user_input )
user_vec_2 = shared_embed(products_input )

input_vecs = keras.layers.concatenate([user_vec_1, user_vec_2], name='concat')


input_vecs_1=keras.layers.Flatten()(input_vecs) 
input_vecs_2=keras.layers.Flatten()(input_vecs)

    nn = keras.layers.Dense(90, activation='relu',name='layer_1')(input_vecs_1)
    result_a = keras.layers.Dense(1, activation='linear', name='output_1')(nn)

    # Task 2 FC layers
    nn1 = keras.layers.Dense(90, activation='relu', name='layer_2')(input_vecs_2)

    result_b = keras.layers.Dense(1, activation='linear',name='output_2')(nn1)

model = Model(inputs=[user_input , products_input], outputs=[result_a, result_b])

model.compile(optimizer='rmsprop',
              loss='mse',
              metrics=['accuracy'])

model.fit([data_1, data_2], [Y_1,Y_2], epochs=10)
</code></pre>
",Vectorization & Embeddings,kera multitask learning two different input sample size implementing multitask regression model using code kera api shared layer section two data set let call follows consists sample sample character long show total number unique character another word comparatively consists character convert two dimensional numpy array follows giving embedding layer make shape data follows number matrix represents index integer multitask model model visualized follows fit model follows error way kera use two different sample size input trick avoid error achieve goal multitasking regression minimum working code testing
Shape of weights in the Softmax layer in Word2vec(skip-gram),"<p>I have a question about a shape of the weights for the Softmax Layer.</p>

<p>Suppose our vocabulary is 10000 words and our Embedding layer will reduce the dimensionality to 300.</p>

<p>So an Input is a one-hot-vector of length 10000 and Embedding layer has 300 neurons. It means, that weight matrix from Input layer to the Embedding layer has shape 10000*300(number of words in vocabulary* neurons in Embedding layer).</p>

<p>According to this tutorial(<a href=""https://www.kaggle.com/christofer/word2vec-skipgram-model-with-tensorflow"" rel=""nofollow noreferrer"">https://www.kaggle.com/christofer/word2vec-skipgram-model-with-tensorflow</a>) and many others the next weight matrix(that connects Embedding layer and Softmax classifier) has the same shape(number of words in vocabulary* neurons in Embedding layer or in our case 10000 * 300). I don't understand why? Shouldnt it be 300 * 10000(because we have to predict 10000 probabilities for each class)?</p>

<p>Can you explain me this?</p>
",Vectorization & Embeddings,shape weight softmax layer word vec skip gram question shape weight softmax layer suppose vocabulary word embedding layer reduce dimensionality input one hot vector length embedding layer ha neuron mean weight matrix input layer embedding layer ha shape number word vocabulary neuron embedding layer according tutorial many others next weight matrix connects embedding layer softmax classifier ha shape number word vocabulary neuron embedding layer case understand shouldnt predict probability class explain
How to Compare two sentences semantically?,"<p>I am trying to build an app for college so the students can have their exams on it 
but I am having problem the article questions which I can't compare the answer of the student and the answer of the model answer to check if it was right or wrong 
so can anyone help me with that or tell me where to start ?
I have read about a lot of algorithms but I don't know where to begin.</p>

<p>I have found codes and function but I don't know how to use them like the following link <a href=""https://stackoverflow.com/questions/2037832/semantic-similarity-between-sentences"">click here</a></p>
",Vectorization & Embeddings,compare two sentence semantically trying build app college student exam problem article question compare answer student answer model answer check wa right wrong anyone help tell start read lot algorithm know begin found code function know use like following link href
"Keras Prediction result (getting score,use of argmax)","<p>I am trying to use the elmo model for text classification for my own dataset. The training is completed and the number of classes is 4(used keras model and elmo embedding).In the prediction, I got a numpy array. I am attaching the sample code and the result below...</p>

<pre><code>import tensorflow as tf
import keras.backend as K
new_text_pr = np.array(data, dtype=object)[:, np.newaxis]
with tf.Session() as session:
    K.set_session(session)
    session.run(tf.global_variables_initializer())
    session.run(tf.tables_initializer())
    model_elmo = build_model(classes)
    model_elmo.load_weights(model+""/""+elmo_model)
    import time
    t = time.time()
    predicted = model_elmo.predict(new_text_pr)
    print(""time: "", time.time() - t)
    print(predicted)
    # print(predicted[0][0])
    print(""result:"",np.argmax(predicted[0]))
    return np.argmax(predicted[0])
</code></pre>

<p>when I print the predicts variable I got this.</p>

<pre><code>time:  1.561854362487793
 [[0.17483692 0.21439584 0.24001297 0.3707543 ]
 [0.15607062 0.24448264 0.4398888  0.15955798]
 [0.06494818 0.3439018  0.42254424 0.16860574]
 [0.08343349 0.37218323 0.32528472 0.2190985 ]
 [0.14868192 0.25948635 0.32722548 0.2646063 ]
 [0.0365712  0.4194748  0.3321385  0.21181548]
 [0.05350104 0.18225929 0.56712115 0.19711846]
 [0.08343349 0.37218323 0.32528472 0.2190985 ]
 [0.09541835 0.19085276 0.41069734 0.30303153]
 [0.03930932 0.40526104 0.45785302 0.09757669]
 [0.06377257 0.33980298 0.32396355 0.27246094]
 [0.09784496 0.2292052  0.44426462 0.22868524]
 [0.06089798 0.31685832 0.47317514 0.14906852]
 [0.03956613 0.46605557 0.3502095  0.14416872]
 [0.10513227 0.26166025 0.36598155 0.26722598]
 [0.15165758 0.22900137 0.50939053 0.10995051]
 [0.06377257 0.33980298 0.32396355 0.27246094]
 [0.11404029 0.21311268 0.46880838 0.2040386 ]
 [0.07556026 0.20502563 0.52019936 0.19921473]
 [0.11096822 0.23295449 0.36192006 0.29415724]
 [0.05018891 0.16656907 0.60114646 0.18209551]
 [0.08880813 0.2893545  0.44374797 0.1780894 ]
 [0.14868192 0.25948635 0.32722548 0.2646063 ]
 [0.09596984 0.18282187 0.5053091  0.2158991 ]
 [0.09428936 0.13995855 0.62395805 0.14179407]
 [0.10513227 0.26166025 0.36598155 0.26722598]
 [0.08244281 0.15743142 0.5462735  0.21385226]
 [0.07199708 0.2446867  0.44568574 0.23763043]
 [0.1339082  0.27288827 0.43478844 0.15841508]
 [0.07354636 0.24499843 0.44873005 0.23272514]
 [0.08880813 0.2893545  0.44374797 0.1780894 ]
 [0.14868192 0.25948635 0.32722548 0.2646063 ]
 [0.08924995 0.36547357 0.40014726 0.14512917]
 [0.05132649 0.28190497 0.5224545  0.14431408]
 [0.06377257 0.33980292 0.32396355 0.27246094]
 [0.04849219 0.36724472 0.39698333 0.1872797 ]
 [0.07206573 0.31368822 0.4667826  0.14746341]
 [0.05948553 0.28048623 0.41831577 0.2417125 ]
 [0.07582933 0.18771031 0.54879296 0.18766735]
 [0.03858965 0.20433436 0.5596278  0.19744818]
 [0.07443814 0.20681688 0.3933627  0.32538226]
 [0.0639974  0.23687115 0.5357675  0.16336392]
 [0.11005415 0.22901568 0.4279426  0.23298755]
 [0.12625505 0.22987585 0.31619486 0.32767424]
 [0.08893713 0.14554602 0.45740074 0.30811617]
 [0.07906891 0.18683094 0.5214609  0.21263924]
 [0.06316617 0.30398315 0.4475617  0.185289  ]
 [0.07060979 0.17987429 0.4829593  0.26655656]
 [0.0720717  0.27058697 0.41439256 0.24294883]
 [0.06377257 0.33980292 0.32396355 0.27246094]
 [0.04745338 0.25831962 0.46751252 0.22671448]
 [0.06624557 0.20708969 0.54820716 0.17845756]]
 result:3
</code></pre>

<p>Anyone have any idea about what is the use of taking the 0th index value only. Considering this as a list of lists 0th index means first list and the argmax returns index the maximum value from the list. Then what is the use of other values in the lists?. Why isn't it considered?. Also is it possible to get the score from this? I hope the question is clear. Is it the correct way or is it wrong?</p>

<p><strong>I have found the issue. just posting it others who met the same problem.</strong> </p>

<p><strong><em>Answer:</em></strong> When predicting with Elmo model, it expects a list of strings. In code, the prediction data were split and the model predicted for each word. That's why I got this huge array. I have used a temporary fix. The data is appended to a list then an empty string is also appended with the list. The model will predict the both list values but I took only the first predicted data. This is not the correct way but I have done this as a quick fix and hoping to find a fix in the future</p>
",Vectorization & Embeddings,kera prediction result getting score use argmax trying use elmo model text classification dataset training completed number class used kera model elmo embedding prediction got numpy array attaching sample code result print predicts variable got anyone idea use taking th index value considering list list th index mean first list argmax return index maximum value list use value list considered also possible get score hope question clear correct way wrong found issue posting others met problem answer predicting elmo model expects list string code prediction data split model predicted word got huge array used temporary fix data appended list empty string also appended list model predict list value took first predicted data correct way done quick fix hoping find fix future
keras add features after embedding,"<p>I want to add part of speech features into my word vector after embedding in Keras. I would like to add them as one hot and concat them after embedding. But the part of speech of a word is dynamic so I can't use another embedding layer for part of speech one hot look up and combine two embedding layers.</p>
",Vectorization & Embeddings,kera add feature embedding want add part speech feature word vector embedding kera would like add one hot concat embedding part speech word dynamic use another embedding layer part speech one hot look combine two embedding layer
How can we implement word sense disambiguation using word2vec representation?,"<p>I know how word2vec works, but I am having trouble with finding out how to implement word sense disambiguation using word2vec. Can you help with the process?</p>
",Vectorization & Embeddings,implement word sense disambiguation using word vec representation know word vec work trouble finding implement word sense disambiguation using word vec help process
How to do language representation on huge documents of 3000-4000 word for query-based retrieval?,"<p>I am trying to implement a semantic search to retrieve similar documents from a dataset of unstructured French documents.</p>

<ul>
<li>These documents are not categorized and are templates with 300 - 3000 words per document.</li>
<li>I am using doc2vec using gensim to find the paragraph embeddings with 300 dimensions and a window of 5 of the dataset.</li>
<li>I am then converting the search query which is a maximum of 5 words to the vector with 300 dimensions and comparing the cosine distance to find the document close to the search queries.</li>
</ul>

<p>I am not getting good results. Please suggest some strategies to do the semantic search. I was trying to reduce the number of words in my dataset by doing rake keyword extraction.</p>
",Vectorization & Embeddings,language representation huge document word query based retrieval trying implement semantic search retrieve similar document dataset unstructured french document document categorized template word per document using doc vec using gensim find paragraph embeddings dimension window dataset converting search query maximum word vector dimension comparing cosine distance find document close search query getting good result please suggest strategy semantic search wa trying reduce number word dataset rake keyword extraction
What type of Autoencoder for text similarity?,"<p>I don't have any experience working on neural networks before,so any help would be highly appreciated. I am solving the following task: I want to find the similarity score between sentence pairs. My idea to solve this is to generate the embeddings for each word in the sentence and feed the vectors to an encoder which will learn to aggregate these many inputs to one lower representation sequence vector. And use cosine similarity between this sequence vectors to find the similarity score. 
My question is do you have any suggestions on which type of autoencoder or neural network architecture would work better for my case.   </p>
",Vectorization & Embeddings,type autoencoder text similarity experience working neural network help would highly appreciated solving following task want find similarity score sentence pair idea solve generate embeddings word sentence feed vector encoder learn aggregate many input one lower representation sequence vector use cosine similarity sequence vector find similarity score question suggestion type autoencoder neural network architecture would work better case
cosine similarity preprocesing task,"<p>I have recently started with NLP. As part of cosine similarities calculation I have to complete the following task:</p>

<pre><code># Convert the sentences into bag-of-words vectors.
sent_1 = dictionary.doc2bow(sent_1)
sent_2 = dictionary.doc2bow(sent_2)
sent_3 = dictionary.doc2bow(sent_3)
</code></pre>

<p>I have more than 10000 different sentences (documents), so I want to generete a code which iterates automatically over documents.
I have tried the following but it does not work:</p>

<pre><code>sent_X = []
for i in documents:
    sent_X .append(dictionary.doc2bow(simple_preprocess(i)))
</code></pre>

<p>Thanks</p>
",Vectorization & Embeddings,cosine similarity preprocesing task recently started nlp part cosine similarity calculation complete following task different sentence document want generete code iterates automatically document tried following doe work thanks
NLP - Best document embedding library,"<p>Good day, fellow humans (?).</p>

<p>I have a methodological question that is confused by a deep research in a tiny amount of time.</p>

<p>The question arises from the following problem(s): I need to apply semi-supervised or unsupervised clustering on documents. I have ~300 documents classified with multi-labels and approximately 3400 documents not classified. The number of unsupervised documents could become ~10'000 in the next days.</p>

<p>The main idea is that of applying semi-supervised clustering based on the labels at hands. Alternatively, that of going fully unsupervised for soft clustering.</p>

<p>We thought of creating embeddings for the whole documents, but here lies the confusion: which library is the best for such a task? </p>

<p>I guess the utmost importance needs to lie in the context of the whole document. As far as I know, BERT and FastText provide context-dependent word embedding, but not whole document embedding. On the other hand, Gensim's Doc2Vec is context-agnostic, right?</p>

<p>I think I saw a way to train sentence embeddings with BERT, via the HuggingFace API, and was wondering whether it could be useful to consider the whole document as a single sentence.</p>

<p>Do you have any suggestion? I'm probably exposing my utter ignorance and confusion on the matter, but my brain is melted.</p>

<p>Thank you very much for your time.</p>

<p>Viva!</p>

<p>Edit to answer to @gojomo:</p>

<p>My documents are on average ~180 words. The original task was that of multi-label text classification, i.e. each document can have from 1 to N labels, with the number of labels now being N=18.  They are highly imbalanced.
Having only 330 labeled documents so far due to several issues, we asked the documents' provider to give also unlabeled data, that should reach the order of the 10k.
I used FastText classification mode, but the result is obviously atrocious. I also run a K-NN with Doc2Vec document embedding, but the result is obviously still atrocious.
I was going to use biomedical BERT-based models (like BioBERT and SciBERT) to produce a NER tagging (trained on domain-specific datasets) on the documents to later apply a classifier.
Now that we have unlabeled documents at disposal, we wanted to adventure into semi-supervised classification or unsupervised clustering, just to explore possibilities. I have to say that this is just a master thesis.</p>
",Vectorization & Embeddings,nlp best document embedding library good day fellow human methodological question confused deep research tiny amount time question arises following problem need apply semi supervised unsupervised clustering document document classified multi label approximately document classified number unsupervised document could become next day main idea applying semi supervised clustering based label hand alternatively going fully unsupervised soft clustering thought creating embeddings whole document lie confusion library best task guess utmost importance need lie context whole document far know bert fasttext provide context dependent word embedding whole document embedding hand gensim doc vec context agnostic right think saw way train sentence embeddings bert via huggingface api wa wondering whether could useful consider whole document single sentence suggestion probably exposing utter ignorance confusion matter brain melted thank much time viva edit answer gojomo document average word original task wa multi label text classification e document n label number label n highly imbalanced labeled document far due several issue asked document provider give also unlabeled data reach order k used fasttext classification mode result obviously atrocious also run k nn doc vec document embedding result obviously still atrocious wa going use biomedical bert based model like biobert scibert produce ner tagging trained domain specific datasets document later apply classifier unlabeled document disposal wanted adventure semi supervised classification unsupervised clustering explore possibility say master thesis
How is the TFIDFVectorizer in scikit-learn supposed to work?,"<p>I'm trying to get words that are distinctive of certain documents using the TfIDFVectorizer class in scikit-learn. It creates a tfidf matrix with all the words and their scores in all the documents, but then it seems to count common words, as well. This is some of the code I'm running: </p>

<pre><code>vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(contents)
feature_names = vectorizer.get_feature_names()
dense = tfidf_matrix.todense()
denselist = dense.tolist()
df = pd.DataFrame(denselist, columns=feature_names, index=characters)
s = pd.Series(df.loc['Adam'])
s[s &gt; 0].sort_values(ascending=False)[:10]
</code></pre>

<p>I expected this to return a list of distinctive words for the document 'Adam', but what it does it return a list of common words: </p>

<pre><code>and     0.497077
to      0.387147
the     0.316648
of      0.298724
in      0.186404
with    0.144583
his     0.140998
</code></pre>

<p>I might not understand it perfectly, but as I understand it, tf-idf is supposed to find words that are distinctive of one document in a corpus, finding words that appear frequently in one document, but not in other documents. Here, <code>and</code> appears frequently in other documents, so I don't know why it's returning a high value here. </p>

<p>The complete code I'm using to generate this is <a href=""https://github.com/JonathanReeve/milton-analysis/blob/v0.1/tfidf-scikit.ipynb"" rel=""noreferrer"">in this Jupyter notebook</a>. </p>

<p>When I compute tf/idfs semi-manually, using the NLTK and computing scores for each word, I get the appropriate results. For the 'Adam' document: </p>

<pre><code>fresh        0.000813
prime        0.000813
bone         0.000677
relate       0.000677
blame        0.000677
enough       0.000677
</code></pre>

<p>That looks about right, since these are words that appear in the 'Adam' document, but not as much in other documents in the corpus. The complete code used to generate this is in <a href=""https://github.com/JonathanReeve/milton-analysis/blob/v0.1/tfidf-nltk.ipynb"" rel=""noreferrer"">this Jupyter notebook</a>. </p>

<p>Am I doing something wrong with the scikit code? Is there another way to initialize this class where it returns the right results? Of course, I can ignore stopwords by passing <code>stop_words = 'english'</code>, but that doesn't really solve the problem, since common words of any sort shouldn't have high scores here. </p>
",Vectorization & Embeddings,tfidfvectorizer scikit learn supposed work trying get word distinctive certain document using tfidfvectorizer class scikit learn creates tfidf matrix word score document seems count common word well code running expected return list distinctive word document adam doe return list common word might understand perfectly understand tf idf supposed find word distinctive one document corpus finding word appear frequently one document document appears frequently document know returning high value complete code using generate jupyter notebook compute tf idf semi manually using nltk computing score word get appropriate result adam document look right since word appear adam document much document corpus complete code used generate jupyter notebook something wrong scikit code another way initialize class return right result course ignore stopwords passing really solve problem since common word sort high score
Ranking words across multiple text files by TFIDF,"<p>I have crawled a wikipedia article and extracted several <strong>climate change</strong> related url's and saved their content with their url's as the name of file. Now i wanna find out which are the most popular words amongst all this corpus by using tfidf. This is my piece of code:</p>

<pre><code>from nltk.corpus import stopwords as stop

def termFrequency(term, doc):  

    """""" 
    Input: term: Term in the Document, doc: Document 
    Return: Normalized tf: Number of times term occurs 
      in document/Total number of terms in the document 
    """"""
    # Splitting the document into individual terms 
    normalizeTermFreq = doc.lower().split()  

    # Number of times the term occurs in the document 
    term_in_document = normalizeTermFreq.count(term.lower())  

    # Total number of terms in the document 
    len_of_document = float(len(normalizeTermFreq ))  

    # Normalized Term Frequency 
    normalized_tf = term_in_document / len_of_document
    print(""TFVAL"",normalized_tf)  

    return normalized_tf 


def inverseDocumentFrequency(term, allDocs): 
    num_docs_with_given_term = 0

    """""" 
    Input: term: Term in the Document, 
           allDocs: List of all documents 
    Return: Inverse Document Frequency (idf) for term 
            = Logarithm ((Total Number of Documents) /  
            (Number of documents containing the term)) 
    """"""
    # Iterate through all the documents 
    for doc in allDocs: 

        """""" 
        Putting a check if a term appears in a document. 
        If term is present in the document, then  
        increment ""num_docs_with_given_term"" variable 
        """"""
        with open(""2009_United_Nations_Climate_Change_Conference.txt"",""r+"",encoding=""utf-8"") as f:
            doc=f.read().split()
            for word in doc:
                print(word)
                result=re.match(r'^[a-zA-Z]+$',word)
                if result is not None:
                    document.append(word)

        document = ' '.join([i for i in document if i not in stop.words()])
        if term.lower() in document.lower().split(): 
            num_docs_with_given_term += 1
        f.close()
    if num_docs_with_given_term &gt; 0: 
        # Total number of documents 
        total_num_docs = len(allDocs)  

        # Calculating the IDF  
        idf_val = log(float(total_num_docs) / num_docs_with_given_term)
        print(""IDF_VALUE:"",idf_val) 
        return idf_val 
    else: 
        return 0

def start(file,alldocs): 
    #my_function()
    import nltk,re
    document=[]

    # print(count)
    with open(file,""r+"",encoding=""utf-8"") as f:
        doc=f.read().split()
        for word in doc:
            try:
                print(word)
                result=re.match(r'^[a-zA-Z]+$',word)
                if result is not None:
                    document.append(word)
            except Exception:
                pass

        document = ' '.join([i for i in document if i not in stop.words()])
        print(""xxxxxxxxxxxxxx"",document,""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"")
        words = nltk.tokenize.word_tokenize(document)
        print(""xxxxxxxxxxxxxx"",words,""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###################"")
        words = [word for word in words if word not in stop.words()]
        print(""xxxxxxxxxxxxxx"",words,""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###################33"")
        fdist = nltk.FreqDist(words)
    for term in fdist:            
        tf-idf(term, file) = tf(term, file)* idf(term, alldocs)```


</code></pre>

<blockquote>
  <p>My code shows issue in this portion of code:</p>
</blockquote>

<pre><code>for term in fdist:            
        tf-idf(term, file) = tf(term, file)* idf(term, alldocs)
</code></pre>

<p>here ""file"" refers to as filename and alldocs contains list of all the climate change related text file  present.</p>
",Vectorization & Embeddings,ranking word across multiple text file tfidf crawled wikipedia article extracted several climate change related url saved content url name file wan na find popular word amongst corpus using tfidf piece code code show issue portion code file refers filename alldocs contains list climate change related text file present
Getting F-Score of 0 when combining RoBERTa and BiLSTM,"<p>I am trying to stack an LSTM on top of RoBERTa model for binary classification problem
I've tried to configurations:
    - Freeze RoBERTa embedding 
    - Fine-tune embedding </p>

<p>In Freezing case I get around 57% F-score , this is relatively low compared to regular RoBERTa for sequence classification which got the same data around 81% </p>

<p>In fine-tune case I get 0% F-score and validation loss isn't converging </p>

<p>Most probably I am doing something wrong but I can't really spot it.</p>

<p>Here is the model part</p>

<pre><code>
class RoBERTaLSTMClassifier(nn.Module):
    def __init__(self, bert_config, num_classes, hidden_size=None, dropout=0.5):
        """"""
        bert: pretrained bert model
        num_classes: the number of num_classes
        hidden_size: the number of hiddens which will be used by LSTM layer
        dropout: dropout rate
        """"""
        super(RoBERTaLSTMClassifier, self).__init__()
        self.num_classes = num_classes
        self.model = RobertaModel(bert_config)
        if hidden_size is None: self.hidden_size = bert_config.hidden_size
        else: self.hidden_size = hidden_size
        self.lstm = nn.LSTM(bert_config.hidden_size, self.hidden_size, bidirectional=True,batch_first=True)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(self.hidden_size * 2, 1)
        self.softmax = nn.Softmax()
        ## add sigmoid non linearity for binary classification
        self.sig           = nn.Sigmoid()

    def forward(self, input_ids, attention_mask, current_batch_size, hidden):
        """"""
        all_layers: whether or not to return all encoded_layers
        return: logits in the following format (batch_size, num_classes)
        """"""
        with torch.no_grad():
             ## freeze embedding from BERT
             outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
       # last hidden state is input to the LSTM
        output, (hidden_h, hidden_c) = self.lstm(outputs[0], hidden)

        output_hidden = torch.cat((hidden_h[0], hidden_h[1]), dim=1) #[B, H*2]

        logits = self.classifier(self.dropout(output_hidden)) #[B, C]
        sig_out = self.sig(logits).view(current_batch_size, -1)

        ## get the last batch output
        sig_out = sig_out[:, -1] # get last batch of labels
        hidden = (hidden_h, hidden_c)

        return sig_out, hidden

    def init_bilstm_hidden(self, batch_size):
        h0 = torch.zeros(2, batch_size, self.hidden_size).to(device) # 2 for bidirection 
        c0 = torch.zeros(2, batch_size, self.hidden_size).to(device)

        return (h0, c0)
</code></pre>

<p>** Here is the Training Loop Part** </p>

<pre><code>from sklearn.metrics import f1_score
from tqdm import tqdm, trange
import numpy as np
lr=0.0001
roberta_conf = RobertaConfig.from_pretrained('roberta-base')

num_classes = 2
hidden_size = 256

LSTMRoBERTaModel =  RoBERTaLSTMClassifier(roberta_conf, num_classes=num_classes,hidden_size= hidden_size,dropout=0.5)
criterion = nn.BCELoss() ## binary cross entropy
optimizer = torch.optim.Adam(LSTMRoBERTaModel.parameters(), lr=lr)

epochs = 5
counter = 0
max_grad_norm = 1.0

nb_tr_examples, nb_tr_steps = 0, 0
for _ in trange(epochs, desc=""Epoch""):
    LSTMRoBERTaModel.cuda()
    LSTMRoBERTaModel.train()
    tr_loss = 0
    y_preds = []
    y_true = []
    hidden_init = LSTMRoBERTaModel.init_bilstm_hidden(batch_size=bs)
    h = hidden_init
    for step, batch in enumerate(train_dataloader):
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        current_batch_size = b_input_ids.size()[0]
        ##

        h = tuple([each.data for each in h])

        ## forward pass
        preds, h = LSTMRoBERTaModel.forward(b_input_ids, b_input_mask, current_batch_size,h)

        loss = criterion(preds.squeeze(),b_labels.float())

        # track train loss
        tr_loss += loss.item()
        nb_tr_examples += b_input_ids.size(0)
        nb_tr_steps += 1

        # gradient clipping
        torch.nn.utils.clip_grad_norm_(parameters=LSTMRoBERTaModel.parameters(), max_norm=max_grad_norm)

        loss.backward()
        optimizer.step()
        LSTMRoBERTaModel.zero_grad()
    # print train loss per epoch
    print(""\nTrain loss: {}"".format(tr_loss/nb_tr_steps))     

    LSTMRoBERTaModel.eval()
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0


    val_h = LSTMRoBERTaModel.init_bilstm_hidden(bs)

    for batch in dev_dataloader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        current_batch_size = b_input_ids.size()[0]
        with torch.no_grad():

             preds, val_h = LSTMRoBERTaModel.forward(b_input_ids, b_input_mask, current_batch_size, val_h)
             loss = criterion(preds.squeeze(),b_labels.float())

        eval_loss += loss
        y_preds.extend(np.round(preds.data.cpu()))
        y_true.extend(b_labels.data.cpu())
        #print(preds[2], b_labels[2] )
        #eval_accuracy += f1_score(torch.tensor.numpy(b_labels.float), toch.tensor.numpy(preds))
        nb_eval_examples += b_input_ids.size(0)
        nb_eval_steps += 1
    eval_loss = eval_loss/nb_eval_steps    
    print(""Validation loss: {}"".format(eval_loss))
    print(""F1 - Score: {}"".format(f1_score(y_true,y_preds)))
    #print(""F1- Score: {}"".format(eval_accuracy/nb_eval_steps))     


</code></pre>
",Vectorization & Embeddings,getting f score combining roberta bilstm trying stack lstm top roberta model binary classification problem tried configuration freeze roberta embedding fine tune embedding freezing case get around f score relatively low compared regular roberta sequence classification got data around fine tune case get f score validation loss converging probably something wrong really spot model part training loop part
Gensim most_similar method coefficients are very low,"<p>I trained word embedding word2vec model using gensim and then used most_similar method to find the most associated words.</p>

<pre><code>Word to search:  forest 
</code></pre>

<p>The result is below:</p>

<pre><code>Most similar words:  [('wood', 0.2495424747467041), ('trees', 0.24147865176200867), ('distant', 0.2403097301721573), ('island', 0.2402323037)]
</code></pre>

<p>I wonder why the coefficient is very low, even the top word is less than 0.25.</p>

<p>Thank you!</p>
",Vectorization & Embeddings,gensim similar method coefficient low trained word embedding word vec model using gensim used similar method find associated word result wonder coefficient low even top word le thank
How to evaluate Word2Vec model,"<p>Hi have my own corpus and I train several Word2Vec models on it.
What is the best way to evaluate them one against each-other and choose the best one? (Not manually obviously - I am looking for various measures).</p>

<p>It worth noting that the embedding is for items and not word, therefore I can't use any existing benchmarks.</p>

<p>Thanks!</p>
",Vectorization & Embeddings,evaluate word vec model hi corpus train several word vec model best way evaluate one choose best one manually obviously looking various measure worth noting embedding item word therefore use existing benchmark thanks
Explanation of build_vocab in torch and it&#39;s association with pre-trained embeddings,"<p>Can some explain me what is build_vocab in torch, it is not clear from online documentation? Why do we need it and it's relation to pre-trained embeddings? </p>
",Vectorization & Embeddings,explanation build vocab torch association pre trained embeddings explain build vocab torch clear online documentation need relation pre trained embeddings
Is it a good idea to use word2vec for encoding of categorical features?,"<p>I am facing a binary prediction task and have a set of features of which all are categorical. A key challenge is therefore to encode those categorical features to numbers and I was looking for smart ways to do so. 
I stumbled over word2vec, which is mostly used for NLP, but I was wondering whether I could use it to encode my variables, i.e. simply take the weights of the neural net as the encoded features.</p>

<p>However, I am not sure, whether it is a good idea since, the context words, which serve as the input features in word2vec are in my case more or less random, in contrast to real sentences which word2vec was originially made for. </p>

<p>Do you guys have any advice, thoughts, recommendations on this?</p>
",Vectorization & Embeddings,good idea use word vec encoding categorical feature facing binary prediction task set feature categorical key challenge therefore encode categorical feature number wa looking smart way stumbled word vec mostly used nlp wa wondering whether could use encode variable e simply take weight neural net encoded feature however sure whether good idea since context word serve input feature word vec case le random contrast real sentence word vec wa originially made guy advice thought recommendation
Question pairs (ground truth) datasets for Word2Vec model testing?,"<p>I'm looking for test datasets to optimize my Word2Vec model. I have found a good one from gensim:</p>

<p>gensim/test/test_data/questions-words.txt </p>

<p>Does anyone know other similar datasets?</p>

<p>Thank you!</p>
",Vectorization & Embeddings,question pair ground truth datasets word vec model testing looking test datasets optimize word vec model found good one gensim gensim test test data question word txt doe anyone know similar datasets thank
Is it possible to retrain Google&#39;s Universal Sentence Encoder such that it takes keywords into account when encoding sentences?,"<p>I am a bit confused on what it means to set <code>trainable = True</code> when loading the Universal Sentence Encoder 3. I have a small corpus (3000 different sentences), given a sentence I want to find the 10 most similar sentences.
My current method is:</p>

<p>1) Load the module
<code>embed = hub.Module(""path"", trainable =False)</code></p>

<p>2) Encode all sentences:
 <code>session.run(embed(sentences))</code></p>

<p>3) Find the closest sentences using cosine similarity.</p>

<p>It performs decent, but I would want the model to be finetuned to my own dictionary, becuase there are certain keywords which are more important than others. This is thus not a classification problem. When looking at the existing examples for retrainin the module (<a href=""https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub"" rel=""nofollow noreferrer"">https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub</a>) it is for classification. </p>

<p>Is it possible to make the Universal Sentence Encoder retrain on my keywords and output different embeddings (for instance by setting <code>trainable = True</code>)?</p>
",Vectorization & Embeddings,possible retrain google universal sentence encoder take keywords account encoding sentence bit confused mean set loading universal sentence encoder small corpus different sentence given sentence want find similar sentence current method load module encode sentence find closest sentence using cosine similarity performs decent would want model finetuned dictionary becuase certain keywords important others thus classification problem looking existing example retrainin module classification possible make universal sentence encoder retrain keywords output different embeddings instance setting
Requirement to compare the BDD Test cases to the crawled page objects and map the bdd steps to page objects and pages automatically,"<p>I am creating a Keras based nueral networking on top of Tensorflow. My requirement is to take 2 data sets as below and compare</p>

<ol>
<li>Take the test steps as first input as a sentence for a website</li>
<li>Get the crawled objects and pages for a web site</li>
</ol>

<p>Then using Keras and Tensor flow pass both inputs. </p>

<p>Convert </p>

<p>a) to NLP based output after cleansing and objects also cleanse it. Both need to be passed to the neuron to find the similairty. Outcome should be similarity. </p>

<p>Pls advise how do we do it. Basic idea is based on test case it should tell which page and object is taking about</p>

<ol>
<li>Moved page and objects to the pandas data frame cleansed it and it became as below</li>
<li>Take the test cases which is as below in pandas data frame
now both the inputs are passed to Keras Embedding as Tensors.</li>
</ol>

<p>Next done cosine similarity.</p>

<p>Next fit that model in loop for each of records cosinesimilarity.Words like personal details etc I made my own corpus as personal details firstname ,personal details lastname etc</p>

<p>Pls advise when fitting it is giving shape issue if it do directly and hence has to pass it in loop for each record. is this correct or what is correct approach</p>
",Vectorization & Embeddings,requirement compare bdd test case crawled page object map bdd step page object page automatically creating kera based nueral networking top tensorflow requirement take data set compare take test step first input sentence website get crawled object page web site using kera tensor flow pas input convert nlp based output cleansing object also cleanse need passed neuron find similairty outcome similarity pls advise basic idea based test case tell page object taking moved page object panda data frame cleansed became take test case panda data frame input passed kera embedding tensor next done cosine similarity next fit model loop record cosinesimilarity word like personal detail etc made corpus personal detail firstname personal detail lastname etc pls advise fitting giving shape issue directly hence ha pas loop record correct correct approach
How to save self-trained word2vec to a txt file with format like &#39;word2vec-google-news&#39; or &#39;glove.6b.50d&#39;,"<p>I wonder that how can I save a self-trained word2vec to txt file with the format like 'word2vec-google-news' or 'glove.6b.50d' which has the tokens followed by matched vectors.<a href=""https://i.sstatic.net/YdCbv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YdCbv.png"" alt="".""></a></p>

<p>I export my self-trained vectors to txt file which only has vectors but no tokens in the front of those vectors.
<a href=""https://i.sstatic.net/gu2yr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gu2yr.png"" alt=""enter image description here""></a></p>

<p>My code for training my own word2vec:</p>

<pre><code>from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import math
import random
import numpy as np
from six.moves import xrange
import zipfile
import tensorflow as tf
import pandas as pd

filename = ('data/data.zip')

# Step 1: Read the data into a list of strings.
def read_data(filename):
  with zipfile.ZipFile(filename) as f:
    data = tf.compat.as_str(f.read(f.namelist()[0])).split()
    return data

words = read_data(filename)
#print('Data size', len(words))


# Step 2: Build the dictionary and replace rare words with UNK token.
vocabulary_size = 50000
def build_dataset(words):
    count = [['UNK', -1]]
    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
    #print(""count"",len(count))
    dictionary = dict()
    for word, _ in count:
        dictionary[word] = len(dictionary)
    data = list()
    unk_count = 0
    for word in words:
        if word in dictionary:
            index = dictionary[word]
        else:
            index = 0
            unk_count += 1
        data.append(index)
    count[0][1] = unk_count
    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))
    return data, count, dictionary, reverse_dictionary

data, count, dictionary, reverse_dictionary = build_dataset(words)

#del words  # Hint to reduce memory.
#print('Most common words (+UNK)', count[:5])
#print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])



data_index = 0

# Step 3: Function to generate a training batch for the skip-gram model.
def generate_batch(batch_size, num_skips, skip_window):
    global data_index
    assert batch_size % num_skips == 0
    assert num_skips &lt;= 2 * skip_window
    batch = np.ndarray(shape=(batch_size), dtype=np.int32)
    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
    span = 2 * skip_window + 1  # [ skip_window target skip_window ]
    buffer = collections.deque(maxlen=span)
    for _ in range(span):
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    for i in range(batch_size // num_skips):
        target = skip_window  # target label at the center of the buffer
        targets_to_avoid = [skip_window]
        for j in range(num_skips):
            while target in targets_to_avoid:
                target = random.randint(0, span - 1)
            targets_to_avoid.append(target)
            batch[i * num_skips + j] = buffer[skip_window]
            labels[i * num_skips + j, 0] = buffer[target]
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    return batch, labels

batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)
#for i in range(8):
 #print(batch[i], reverse_dictionary[batch[i]],'-&gt;', labels[i, 0], reverse_dictionary[labels[i, 0]])

# Step 4: Build and train a skip-gram model.
batch_size = 128
embedding_size = 128
skip_window = 2
num_skips = 2
valid_size = 9
valid_window = 100
num_sampled = 64    # Number of negative examples to sample.
valid_examples = np.random.choice(valid_window, valid_size, replace=False)

graph = tf.Graph()
with graph.as_default():
    # Input data.
    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

    # Ops and variables pinned to the CPU because of missing GPU implementation
    with tf.device('/cpu:0'):
        # Look up embeddings for inputs.
        embeddings = tf.Variable(
            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
        embed = tf.nn.embedding_lookup(embeddings, train_inputs)

        # Construct the variables for the NCE loss
        nce_weights = tf.Variable(
            tf.truncated_normal([vocabulary_size, embedding_size],
                                stddev=1.0 / math.sqrt(embedding_size)))
        nce_biases = tf.Variable(tf.zeros([vocabulary_size]),dtype=tf.float32)

    # Compute the average NCE loss for the batch.
    # tf.nce_loss automatically draws a new sample of the negative labels each
    # time we evaluate the loss.
    loss = tf.reduce_mean(
            tf.nn.nce_loss(weights=nce_weights,biases=nce_biases, inputs=embed, labels=train_labels,
                 num_sampled=num_sampled, num_classes=vocabulary_size))

    # Construct the SGD optimizer using a learning rate of 1.0.
    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)

    # Compute the cosine similarity between minibatch examples and all embeddings.
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)

    # Add variable initializer.
    init = tf.global_variables_initializer()

# Step 5: Begin training.
num_steps = 20000

with tf.Session(graph=graph) as session:
    # We must initialize all variables before we use them.
    init.run()
    #print(""Initialized"")

    average_loss = 0
    for step in xrange(num_steps):
        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)
        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}

        # We perform one update step by evaluating the optimizer op (including it
        # in the list of returned values for session.run()
        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)
        average_loss += loss_val

        #if step % 2000 == 0:
         #   if step &gt; 0:
          #      average_loss /= 2000
            # The average loss is an estimate of the loss over the last 2000 batches.
           # print(""Average loss at step "", step, "": "", average_loss)
            #average_loss = 0

    final_embeddings = normalized_embeddings.eval()


np.savetxt('data/w2v.txt', final_embeddings)
</code></pre>
",Vectorization & Embeddings,save self trained word vec txt file format like word vec google news glove b wonder save self trained word vec txt file format like word vec google news glove b ha token followed matched vector export self trained vector txt file ha vector token front vector code training word vec
How to combine my python nlp solution for document ranking with solr for indexing?,"<p>I'm new to NLP and Solr. I'm trying to build an application to rank past court case documents based on a newly given case document. I confused about how to use Solr for document indexing to speed up my processing time.</p>

<p>I have built a python solution including Preprocessing with NLTK library and lemmatization with Spacy library. And I have calculated document similarity with Word2Vec Word Embedding Model. </p>

<p>I'm stuck in the middle and am confused about whether I have done correctly. Can someone help me out?</p>

<p>Thanks in advance.</p>
",Vectorization & Embeddings,combine python nlp solution document ranking solr indexing new nlp solr trying build application rank past court case document based newly given case document confused use solr document indexing speed processing time built python solution including preprocessing nltk library lemmatization spacy library calculated document similarity word vec word embedding model stuck middle confused whether done correctly someone help thanks advance
Visualize the cosine similarity scores calculated using pretrained word embeddding in SpaCy,"<p>I have used SpaCy's pretrained model 'en_core_web_lg' to find the cosine distance between a group of values and attributes. I wanted to visualize the relationship of how close a word is from the other word, very much similar to clustering.</p>

<p><a href=""https://i.sstatic.net/An012.png"" rel=""nofollow noreferrer"">Here is the link to the table which contains similarity scores for each value vs attribute </a></p>

<p>Here the columns are the attributes for which i am trying to find the similarity score, while the row are the values for which i am trying to find what attribute it is most likely to be classified</p>

<p><a href=""https://i.sstatic.net/UfQLZ.png"" rel=""nofollow noreferrer"">This is the output i am trying to achieve. Please take a look at it</a></p>
",Vectorization & Embeddings,visualize cosine similarity score calculated using pretrained word embeddding spacy used spacy pretrained model en core web lg find cosine distance group value attribute wanted visualize relationship close word word much similar clustering link table contains similarity score value v attribute column attribute trying find similarity score row value trying find attribute likely classified output trying achieve please take look
Normalization of Term-frequency and Inverse Document Frequency of varying documents lengths to calculate cosine similarity,"<p>I have been trying to find thousands of textual document's similarity against one single query. And every document size is majorly varying (from 20 words to 2000 words)</p>

<p>I did refer the question: <a href=""https://stackoverflow.com/questions/39704220/tf-idf-documents-of-different-length"">tf-idf documents of different length</a></p>

<p>But that doesn't help me because a fraction of cosine value matters too when comparing with a pool of documents to maintain order.</p>

<p>I then came across a wonderful normalization blog: <a href=""https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/"" rel=""nofollow noreferrer"">Tf-Idf and Cosine similarity</a>. But the problem here is to tweak in the TermFreq of every document. </p>

<p>I am using <code>sklearn</code> to calculate tf-idf. But now I am looking for some utility similar to sklearn's tf-idf performance. An iterative approach over all the documents to calculate TF and then modify it is not only time consuming but also not efficient.</p>

<p>Any knowledge/suggestions are appreciated.</p>
",Vectorization & Embeddings,normalization term frequency inverse document frequency varying document length calculate cosine similarity trying find thousand textual document similarity one single query every document size majorly varying word word refer question tf idf cosine similarity problem tweak termfreq every document using calculate tf idf looking utility similar sklearn tf idf performance iterative approach document calculate tf modify time consuming also efficient knowledge suggestion appreciated
Extracting fixed vectors from BioBERT without using terminal command?,"<p>If we want to use weights from pretrained <a href=""https://github.com/dmis-lab/biobert"" rel=""nofollow noreferrer"">BioBERT</a> model, we can execute following terminal command after downloading all the required BioBERT files.</p>

<pre><code>os.system('python3 extract_features.py \
      --input_file=trial.txt \
      --vocab_file=vocab.txt \
      --bert_config_file=bert_config.json \
      --init_checkpoint=biobert_model.ckpt \
      --output_file=output.json')
</code></pre>

<p>The above command actually reads individual file containing the text, reads the textual content from it, and then writes the extracted vectors to another file. So, the problem with this is that it could not be scaled easily for very large data-sets containing thousands of sentences/paragraphs. </p>

<p>Is there is a way to extract these features on the go (using an embedding layer) like it could be done for the word2vec vectors in PyTorch or TF1.3?</p>

<p>Note: BioBERT checkpoints do not exist for TF2.0, so I guess there is no way it could be done with TF2.0 unless someone generates TF2.0 compatible checkpoint files.</p>

<p>I will be grateful for any hint or help.</p>
",Vectorization & Embeddings,extracting fixed vector biobert without using terminal command want use weight pretrained biobert model execute following terminal command downloading required biobert file command actually read individual file containing text read textual content writes extracted vector another file problem could scaled easily large data set containing thousand sentence paragraph way extract feature go using embedding layer like could done word vec vector pytorch tf note biobert checkpoint exist tf guess way could done tf unless someone generates tf compatible checkpoint file grateful hint help
How to vectorize words?,"<p>From Keras's documentation:</p>

<blockquote>
  <p>The main input will receive the headline, as a sequence of integers (each integer encodes a word). The integers will be between 1
  and 10,000 (a vocabulary of 10,000 words) and the sequences will be
  100 words long.</p>
</blockquote>

<p>Here the way to encode words seem to be simply using word's index in the constructed vocabulary to represent the word. If there are 'n' words in the training data, then the integer <em>i</em> to vectorization would be from [0, n-1]. </p>

<p>But in the scikit-learn's CountVectorizer:</p>

<blockquote>
  <p>This implementation produces a sparse representation of the counts
  using scipy.sparse.csr_matrix.</p>
</blockquote>

<p>Is there any connection between these two ways of vectorizations of words? Is the count vector representation would be more effective than index based vectorization? The CountVectorizer takes into account of the word frequency. </p>
",Vectorization & Embeddings,vectorize word kera documentation main input receive headline sequence integer integer encodes word integer vocabulary word sequence word long way encode word seem simply using word index constructed vocabulary represent word n word training data integer vectorization would n scikit learn countvectorizer implementation produce sparse representation count using scipy sparse csr matrix connection two way vectorizations word count vector representation would effective index based vectorization countvectorizer take account word frequency
how to find the semantic meaning similarity of two string in python,"<p>rather than finding the similarity between two string ,i just want find the similarity of the meaning of the two strings for ex. </p>

<ol>
<li>what are the types of hyper threading  </li>
<li>is there any categoriesin hyper threading</li>
</ol>

<p>should have similarity .Till now i tried cosine similarity and word mover distance but i am not getting accurate result for some of the strings</p>
",Vectorization & Embeddings,find semantic meaning similarity two string python rather finding similarity two string want find similarity meaning two string ex type hyper threading categoriesin hyper threading similarity till tried cosine similarity word mover distance getting accurate result string
how to load a vector of certrain word form word2vec saved model?,"<p>how can i find a respective words vector from previous trained word2vec model?</p>

<pre><code>data = {'one': array([-0.06590105,  0.01573388,  0.00682817,  0.53970253, -0.20303348,
   -0.24792041,  0.08682659, -0.45504045,  0.89248925,  0.0655603 ,
   ......
   -0.8175681 ,  0.27659689,  0.22305458,  0.39095637,  0.43375066,
    0.36215973,  0.4040089 , -0.72396156,  0.3385369 , -0.600869  ],
  dtype=float32),
 'two': array([ 0.04694849,  0.13303463, -0.12208422,  0.02010536,  0.05969441,
   -0.04734801, -0.08465996,  0.10344813,  0.03990637,  0.07126121,
    ......
    0.31673026,  0.22282903, -0.18084198, -0.07555179,  0.22873943,
   -0.72985399, -0.05103955, -0.10911274, -0.27275378,  0.01439812],
  dtype=float32),
 'three': array([-0.21048863,  0.4945509 , -0.15050395, -0.29089224, -0.29454648,
    0.3420335 , -0.3419629 ,  0.87303966,  0.21656844, -0.07530259,
    ......
   -0.80034876,  0.02006451,  0.5299498 , -0.6286509 , -0.6182588 ,
   -1.0569025 ,  0.4557548 ,  0.4697938 ,  0.8928275 , -0.7877308 ],
  dtype=float32),
  'four': ......
}
</code></pre>

<p>now i want to obtain like </p>

<pre><code>word = ""one""
wordvector = data.get_vector(word)
</code></pre>

<p>and returns </p>

<pre><code>[-0.06590105,  0.01573388,  0.00682817,  0.53970253, -0.20303348,
   -0.24792041,  0.08682659, -0.45504045,  0.89248925,  0.0655603 ,
   ......
   -0.8175681 ,  0.27659689,  0.22305458,  0.39095637,  0.43375066,
    0.36215973,  0.4040089 , -0.72396156,  0.3385369 , -0.600869  ]
</code></pre>
",Vectorization & Embeddings,load vector certrain word form word vec saved model find respective word vector previous trained word vec model want obtain like return
Python Find Similarities between requests,"<p>I am working on a project to analyse the previous requests and if a new request comes, I need to match the earlier request and use the solution provided for the same.</p>

<p>For Example: if these are previous requests ""Risk rating for Microsoft Inc"", ""Report for the month of September"", etc and if new request is ""Report for the month of September"", I need to find the similarities and use the solution provided for one of the matching previous requests.</p>

<p>I am planing to implement in Python. I came across this algorithm for implementation - Topic Modelling and word2vec. Am I going in the right direction?</p>
",Vectorization & Embeddings,python find similarity request working project analyse previous request new request come need match earlier request use solution provided example previous request risk rating microsoft inc report month september etc new request report month september need find similarity use solution provided one matching previous request planing implement python came across algorithm implementation topic modelling word vec going right direction
How to apply the word2vec model to text file?,"<p>I trained word2vec model from my corpus.</p>

<pre><code>corpus = ""fewdata.txt""
f = io.open(corpus, mode =""r"", encoding = ""utf-8"")
#corpus1 = list(f) 
lines = f.readlines()
sentences =[]
for line in lines:
    mqul= line.split()
    #print(mqul)
    sentences.append(mqul)
model = Word2Vec(sentences = sentences, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = Pool()._processes)
model.init_sims(replace = True)
model.save('model.bin')
model = Word2Vec.load('model.bin')
print(model)
</code></pre>

<p>then </p>

<pre><code>model['aImIroawi']
array([-0.06561889, -0.15222837,  0.00912119, -0.11638119, -0.03242991,
       -0.13457145, -0.09813376,  0.07011288,  0.0711898 ,  0.10069774,
       -0.01028561,  0.11995316,  0.03737569, -0.01811702, -0.12935248],
      dtype=float32)

</code></pre>

<p>but i want to use this model for the txt file with 5333 vocabulary and save it to txt file in the form of </p>

<pre><code>{ 'Aimurawi : array([-0.04728228,  0.13645388,  0.13822217,  0.13086553, -0.0963688 ],dtype= float32),
 Tiona : array([-0.04728228,  0.13645388,  0.13822217,  0.13086553, -0.0963688 ], dype =float32)}
</code></pre>

<p>for all the vocabulary in my text file, can anybody help me how i can do it?</p>
",Vectorization & Embeddings,apply word vec model text file trained word vec model corpus want use model txt file vocabulary save txt file form vocabulary text file anybody help
Unknown token with sentencepiece,"<p>I have a huge corpus of text that I have trained sentencepiece on. I want to tokenize this text and pass it to word2vec for learning word embeddings.</p>

<p>However, when I run sp.EncodeAsPieces(text), if there exists a word that is not in the sp vocab, it does not return &lt; unk > . It instead keeps the word as is. This is a problem for running word2vec because these tokens will not be treated as &lt; unk >.</p>

<p>The only solution I have found so far is to use sp.EncodeAsIds, which tokenizes the text to numbers, and unknown words are tokenized to 0. But I lose the readability that comes with EncodeAsPieces. Any way to use EncodeAsPieces and tokenize words not in vocab to &lt; unk >?</p>
",Vectorization & Embeddings,unknown token sentencepiece huge corpus text trained sentencepiece want tokenize text pas word vec learning word embeddings however run sp encodeaspieces text exists word sp vocab doe return unk instead keep word problem running word vec token treated unk solution found far use sp encodeasids tokenizes text number unknown word tokenized lose readability come encodeaspieces way use encodeaspieces tokenize word vocab unk
How to access/use Google&#39;s pre-trained Word2Vec model without manually downloading the model?,"<p>I want to analyse some text on a Google Compute server on Google Cloud Platform (GCP) using the Word2Vec model.</p>

<p>However, the un-compressed word2vec model from <a href=""https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/"" rel=""noreferrer"">https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/</a> is over 3.5GB and it will take time to download it manually and upload it to a cloud instance. </p>

<p>Is there any way to access this (or any other) pre-trained Word2Vec model on a Google Compute server without uploading it myself?</p>
",Vectorization & Embeddings,access use google pre trained word vec model without manually downloading model want analyse text google compute server google cloud platform gcp using word vec model however un compressed word vec model gb take time download manually upload cloud instance way access pre trained word vec model google compute server without uploading
BERT( Bidirectional Encoder Representations from Transformers) with numbers,"<p><em>Hi!</em></p>

<p>I am trying to understand how BERT is dealing with text that has number within. 
More concretely I'm trying to find the most similar line in document(text+numbers) and specific line(text+numbers).</p>

<p>I tried an example with BERT of 30 characters and cosine similarity:</p>

<pre><code>sentence2 = ""I have 2 apple""; score(between sentence1 &amp; sentence2): 0.99000436
sentence3 = ""I have 3 apple""; score(between sentence1 &amp; sentence3): 0.98602057
sentence4 = ""I have 0 apple""; score(between sentence1 &amp; sentence4): 0.97923964
sentence5 = ""I have 2.1 apple""; score(between sentence1 &amp; sentence5): 0.95482975
</code></pre>

<p>I do not understand why sentence4 has smaller score than sentence3(0 closer to 1 than 3), and 2.1 is closer to 1 than 3...</p>
",Vectorization & Embeddings,bert bidirectional encoder representation transformer number hi trying understand bert dealing text ha number within concretely trying find similar line document text number specific line text number tried example bert character cosine similarity understand sentence ha smaller score sentence closer closer
What is the significance of the magnitude/norm of BERT word embeddings?,"<p>We generally compare similarity between word embeddings with cosine similarity, but this only takes into account the angle between the vectors, not the norm. With word2vec, the norm of the vector decreases as the word is used in more varied contexts. So, stopwords are close to 0 and very unique, high meaning words tend to be large vectors. BERT is context sensitive, so this explanation doesn't entirely cover BERT embeddings. Does anyone have any idea what the significance of vector magnitude could be with BERT?</p>
",Vectorization & Embeddings,significance magnitude norm bert word embeddings generally compare similarity word embeddings cosine similarity take account angle vector norm word vec norm vector decrease word used varied context stopwords close unique high meaning word tend large vector bert context sensitive explanation entirely cover bert embeddings doe anyone idea significance vector magnitude could bert
Distill bert and svm for text classification,"<p>I am using DistillBert from hugging face to get the vector embedding of the words. The output for the example in hugging face will be a torch tensor with three dimensions. how can I feed these embeddings to an SVM?
I did detach the vectors and then reshape them to be 2 dimensional as an input for SVM. I was wondering if it is right?
So here, I have a list of sentences. I will make a for loop and get the vectors.
Code:
'''</p>

<pre><code>embeddings=[]
embedding_np_array=[]
for i in my_list:
   tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
   model = DistilBertModel.from_pretrained('distilbert-base-uncased')
   input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)
   outputs = model(input_ids)
   last_hidden_states = outputs[0]
   embeddings.append(last_hidden_states)

for i in embeddings:
     embedding_np_array.append(i.detach().numpy())
input_svm=embedding_np_array.reshape(embedding_np_array.reshape[0],-1)
</code></pre>

<p>'''</p>
",Vectorization & Embeddings,distill bert svm text classification using distillbert hugging face get vector embedding word output example hugging face torch tensor three dimension feed embeddings svm detach vector reshape dimensional input svm wa wondering right list sentence make loop get vector code
How to use Google&#39;s Universal Sentence Encoder to find the most similar document based on several documents?,"<p>I'm trying to build a simple recommendation system which uses Google's Universal Sentence Encoder to transform the description of different products into vector space. I am pre-computing the embeddings for all the different products. Currently I can give the top N recommendations based upon a single product by calculating the cosine distance between the chosen product and all the other products.</p>

<pre><code>   chosen_product_idx = product_df.index[product_df['Name']== chosen_product][0] 
   ranking_list = []
      for i in products['names']
         ranking_list.append((products['names'][i],cosine_distance(products['product_vector'][chosen_product_idx],products['product_vector'][i]))
       ... 
       ...
</code></pre>

<p>But if a user would say that ""I like these 10 products"", what is the best way to take that into consideration when making a recommendation? I do not have any data regarding which companies the user does NOT like. Is it to take the average vector of all those 10 products and then find the closest vectors to that? </p>

<p>Or treat them as a cluster and find the centroid and make a recommendation based on that?</p>

<p>Does anyone know any good practices for this? </p>
",Vectorization & Embeddings,use google universal sentence encoder find similar document based several document trying build simple recommendation system us google universal sentence encoder transform description different product vector space pre computing embeddings different product currently give top n recommendation based upon single product calculating cosine distance chosen product product user would say like product best way take consideration making recommendation data regarding company user doe like take average vector product find closest vector treat cluster find centroid make recommendation based doe anyone know good practice
Reducing runtime for cosine similarity calculations between 2 lists in Python,"<p>I'm assembling a twitter hashtag dictionary using Python. The keys are the hashtag itself and the corresponding entry is a large collection of tweets that contain this hashtag appended end-to-end. I've got a separate list of all hashtagless tweets and am adding them to dictionary entries according to cosine similarity. Everything is working but is VERY slow (a few hours for 4000 tweets). The nested for loops are giving me O(N^2) runtime. Does anyone have any ideas on how I could improve my runtime? Any suggestions will be greatly appreciated!</p>

<pre><code>taglessVects = normalize(vectorizer.transform(needTags))
    dictVects = normalize(vectorizer.transform(newDict))

   #newDict contains: newDict[hashtag]: ""tweets that used that hashtag""
   #needTags is a list of all the tweets that didn;t use a hashtag
    for dVect, entry in zip(dictVects, newDict):
        for taglessVect, tweet in zip(taglessVects, needTags):
            if cosine_similarity(taglessVect, dVect) &gt; .9:
                newDict[entry] = newDict[entry] + ' ' + tweet


    return newDict

</code></pre>
",Vectorization & Embeddings,reducing runtime cosine similarity calculation list python assembling twitter hashtag dictionary using python key hashtag corresponding entry large collection tweet contain hashtag appended end end got separate list hashtagless tweet adding dictionary entry according cosine similarity everything working slow hour tweet nested loop giving n runtime doe anyone idea could improve runtime suggestion greatly appreciated
error while loading semantic similarity with BERT model,"<p>I want to calculate semantic similarity between sentences using BERT. I found this code on github for an already fine-tuned BERT for semantic similarity: </p>

<pre><code>    from semantic_text_similarity.models import WebBertSimilarity
    from semantic_text_similarity.models import      ClinicalBertSimilarity 
    web_model = WebBertSimilarity(device='cpu', batch_size=10)
</code></pre>

<p>It downloads 100% and gives me the following error (this is the final line):</p>

<pre><code>    TypeError: init_weights() takes 1 positional argument but 2 were given
</code></pre>

<p>I tried to read about this error, but I don't understand where is the 2 positional arguments that where given instead of one. </p>

<ul>
<li>the model I downloaded is from <a href=""https://github.com/AndriyMulyar/semantic-text-similarity"" rel=""nofollow noreferrer"">https://github.com/AndriyMulyar/semantic-text-similarity</a> </li>
</ul>

<p>I'd appreciate any hint on where to look. </p>

<p>Thanks!</p>

<p>-------------------------EDIT QUESTION------------------------------ </p>

<p>This is the whole error:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-73-97be4030b59e&gt; in &lt;module&gt;()
----&gt; 1 web_model = WebBertSimilarity(device='cpu', batch_size=10) #defaults to GPU prediction

/anaconda3/lib/python3.6/site-packages/semantic_text_similarity/models/bert/web_similarity.py in __init__(self, device, batch_size, model_name)
      6     def __init__(self, device='cuda', batch_size=10, model_name=""web-bert-similarity""):
      7         model_path = get_model_path(model_name)
----&gt; 8         super().__init__(device=device, batch_size=batch_size, bert_model_path=model_path)

/anaconda3/lib/python3.6/site-packages/semantic_text_similarity/models/bert/similarity.py in __init__(self, args, device, bert_model_path, batch_size, learning_rate, weight_decay, additional_features)
     80         config.pretrained_config_archive_map['additional_features'] = additional_features
     81 
---&gt; 82         self.regressor_net = BertSimilarityRegressor.from_pretrained(self.args['bert_model_path'], config=config)
     83         self.optimizer = torch.optim.Adam(
     84             self.regressor_net.parameters(),

/anaconda3/lib/python3.6/site-packages/pytorch_transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    534         if hasattr(model, 'tie_weights'):
    535             model.tie_weights()  # make sure word embedding weights are still tied
--&gt; 536 
    537         # Set model in evaluation mode to desactivate DropOut modules by default
    538         model.eval()

/anaconda3/lib/python3.6/site-packages/semantic_text_similarity/models/bert/similarity.py in __init__(self, bert_model_config)
     25         )
     26 
---&gt; 27         self.apply(self.init_weights)
     28 
     29 

/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in apply(self, fn)
    291         """"""
    292         for module in self.children():
--&gt; 293             module.apply(fn)
    294         fn(self)
    295         return self

/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in apply(self, fn)
    291         """"""
    292         for module in self.children():
--&gt; 293             module.apply(fn)
    294         fn(self)
    295         return self

/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in apply(self, fn)
    291         """"""
    292         for module in self.children():
--&gt; 293             module.apply(fn)
    294         fn(self)
    295         return self

/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in apply(self, fn)
    292         for module in self.children():
    293             module.apply(fn)
--&gt; 294         fn(self)
    295         return self
    296 

TypeError: init_weights() takes 1 positional argument but 2 were given
</code></pre>
",Vectorization & Embeddings,error loading semantic similarity bert model want calculate semantic similarity sentence using bert found code github already fine tuned bert semantic similarity downloads give following error final line tried read error understand positional argument given instead one model downloaded appreciate hint look thanks edit question whole error
Create a matrix from a dict of dicts for calculating similarities between docs,"<p>Here is my problem:</p>

<p>I have a dataframe like this:</p>

<pre><code>id   tfidf_weights   
1    {word1: 0.01, word2: 0.01, word3: 0.01, ...}
2    {word4: 0.01, word5: 0.01, word6: 0.01, ...}
3    {word7: 0.01, word8: 0.01, word9: 0.01, ...}
4    {word10: 0.01, word11: 0.01, word12: 0.01, ...}
5    {word13: 0.01, word14: 0.01, word15: 0.01, ...}    
.
.
.
</code></pre>

<p>column 'id' represent the ids of the docs and 'tfidf_weights' the tfidf weight for each word of each docs.</p>

<p>from this dataframe, i can obtain a dict with the following structure:</p>

<pre><code>mydict = {1:{word1: 0.01, word2: 0.01, word3: 0.01, ...}, 2:{word4: 0.01, word5: 0.01, word6: 0.01, ...}, 3:{word7: 0.01, word8: 0.01, word9: 0.01, ...}, 4:{word10: 0.01, word11: 0.01, word12: 0.01, ...}, 5:{word13: 0.01, word14: 0.01, word15: 0.01, ...}, ...}
</code></pre>

<p>what i want to do is, from this dictionary, obtain a matrix like this:</p>

<pre><code>      word1     word2     word3     word4   ...
1     0.01      0.01      0.01      0.01     
2     0.01      0.01      0.01      0.01
3     0.01      0.01      0.01      0.01
4     0.01      0.01      0.01      0.01
5     0.01      0.01      0.01      0.01
.
.
.
</code></pre>

<p>Thank you for your help !</p>
",Vectorization & Embeddings,create matrix dict dicts calculating similarity doc problem dataframe like column id represent id doc tfidf weight tfidf weight word doc dataframe obtain dict following structure want dictionary obtain matrix like thank help
How to compare contents of two large text files in Python?,"<p>Datasets: Two Large text files for train and test that all words of them are tokenized. a part of data is like the following: "" the fulton county grand jury said friday an investigation of atlanta's recent primary election produced `` no evidence '' that any irregularities took place . ""</p>

<p>Question: How can I replace every word in the test data not seen in training with the word ""unk"" in Python?</p>

<p>So far, I made the dictionary by the following codes to count the frequency of each word in the file:</p>

<pre><code>#open text file and assign it to varible with the name ""readfile""
readfile= open('C:/Users/amtol/Desktop/NLP/Homework_1/brown-train.txt','r')

writefile=open('C:/Users/amtol/Desktop/NLP/Homework_1/brown-trainReplaced.txt','w')

# Create an empty dictionary 
d = dict()

# Loop through each line of the file
for line in readfile:

    # Split the line into words 
    words = line.split("" "") 

    # Iterate over each word in line 
    for word in words: 
        # Check if the word is already in dictionary 
        if word in d:

        # Increment count of word by 1 
            d[word] = d[word] + 1
        else: 
            # Add the word to dictionary with count 1 
            d[word] = 1

#replace all words occurring in the training data once with the token&lt;unk&gt;.

for key in list(d.keys()): 
    line= d[key] 
    if (line==1):
        line=""&lt;unk&gt;""
        writefile.write(str(d))
    else:
        writefile.write(str(d))

#close the file that we have created and we wrote the new data in that
writefile.close()
</code></pre>

<p>Honestly the above code doesn't work with writefile.write(str(d)) which I want to write the result in the new textfile, but by print(key, "":"", line) it works and shows the frequency of each word but in the console which doesn't create the new file. if you also know the reason for this, please let me know.</p>
",Vectorization & Embeddings,compare content two large text file python datasets two large text file train test word tokenized part data like following fulton county grand jury said friday investigation atlanta recent primary election produced evidence irregularity took place question replace every word test data seen training word unk python far made dictionary following code count frequency word file honestly code work writefile write str want write result new textfile print key line work show frequency word console create new file also know reason please let know
how to fine-tune word2vec when training our CNN for text classification?,"<p>I have 3 Questions about fine-tuning word vectors. Please, help me out. I will really appreciate it! Many thanks in advance!</p>

<ol>
<li><p>When I train my own CNN for text classification, I use Word2vec to initialize the words, then I just employ these pre-trained vectors as my input features to train CNN, so if I never had a embedding layer, it surely can not do any fine-tunes through back-propagation. my question is if I want to do fine-tuning, does it means to create a Embedding layer?and how to create it? </p></li>
<li><p>When we train Word2vec, we use unsupervised training right? as in my case, I use the skip-gram model to get my pre-trained word2vec; But when I had the vec.bin and use it in the text classification model (CNN) as my words initialiser, if I could fine-tune the word-to-vector map in vec.bin, does it means that I have to have a CNN net structure exactly same as the one when training my Word2vec? and does the fine-tunes stuff would change the vec.bin or just fine-tune in computer memory?</p></li>
<li><p>Are the skip-gram model and CBOW model are only used for unsupervised Word2vec training? Or they could also apply for other general text classification tasks? and what's the different of the network between Word2vec unsupervised training supervised fine-tuning? </p></li>
</ol>

<p>@Franck Dernoncourt thank you for reminding me. I'm green here, and hope to learn something from the powerful community. Please have a look at my questions when you have time, thank you again!</p>
",Vectorization & Embeddings,fine tune word vec training cnn text classification question fine tuning word vector please help really appreciate many thanks advance train cnn text classification use word vec initialize word employ pre trained vector input feature train cnn never embedding layer surely fine tune back propagation question want fine tuning doe mean create embedding layer create train word vec use unsupervised training right case use skip gram model get pre trained word vec vec bin use text classification model cnn word initialiser could fine tune word vector map vec bin doe mean cnn net structure exactly one training word vec doe fine tune stuff would change vec bin fine tune computer memory skip gram model cbow model used unsupervised word vec training could also apply general text classification task different network word vec unsupervised training supervised fine tuning franck dernoncourt thank reminding green hope learn something powerful community please look question time thank
A multi-input (text and numeric) model for regression giving same output,"<p>I have a problem where I need to predict the number of clicks based on Date, CPC(cost per click), Market(US or UK) and keywords(average length 3-4 words, max 6). So the features that I decided to input the model were:</p>

<pre><code>Day(1-31)  WeekDay(0-6)  Month(1-12)   US-Market(0 or 1)  UK-Market(0 or 1)   CPC(continuous numeric)
</code></pre>

<p>And the output is the continuous numeric Number of Clicks.</p>

<p>For Keywords I used keras tokenizer to convert to sequences and the padded those sequences. The I used the glove word embeddings and created an embedding matrix and fed to the nueral Network model as described <a href=""https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"" rel=""nofollow noreferrer"">here</a> in pretrained glove embeddings section.</p>

<p>The model that I used is:</p>

<p><a href=""https://i.sstatic.net/QY1kF.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QY1kF.jpg"" alt=""enter image description here""></a></p>

<p>The last Dense layer has linear activation. The model has two inputs (nlp_input for text data) and meta_input for (numerical,categorical data). Both models are concatenated after the Bidirectional LSTM to the nlp_input
The loss is :</p>

<pre><code>model.compile(loss=""mean_absolute_percentage_error"", optimizer=opt,metrics=['acc'])
</code></pre>

<p>where <code>opt = Adam(lr=1e-3, decay=1e-3 / 200)</code></p>

<p>I trained the model for 100 epochs and the loss was close to 8000 at that point.
But when I apply prediction to the test they result in the same number for all test inputs and that number is even negative <code>-4.5 * e^-5</code>. Could someone guide me as to how should I approach this problem and what improvements could I do to the model.</p>
",Vectorization & Embeddings,multi input text numeric model regression giving output problem need predict number click based date cpc cost per click market u uk keywords average length word max feature decided input model output continuous numeric number click keywords used kera tokenizer convert sequence padded sequence used glove word embeddings created embedding matrix fed nueral network model described pretrained glove embeddings section model used last dense layer ha linear activation model ha two input nlp input text data meta input numerical categorical data model concatenated bidirectional lstm nlp input loss trained model epoch loss wa close point apply prediction test result number test input number even negative could someone guide approach problem improvement could model
How to get &quot;Word&quot; Importance in NLP (TFIDF + Logistic Regression),"<p>I have a function to get tfidf feature like this:</p>

<pre><code>def get_tfidf_features(data, tfidf_vectorizer=None, ngram_range=(1,2)):
    """""" Creates tfidf features and returns them as sparse matrix. If no tfidf_vectorizer is given, 
    the function will train one.""""""

    if tfidf_vectorizer is not None:
        tfidf = tfidf_vectorizer.transform(data.Comment_text)
    else:
        # only add words to the vocabulary that appear at least 200 times
        tfidf_vectorizer = TfidfVectorizer(min_df=700, ngram_range=ngram_range, stop_words='english')
        tfidf = tfidf_vectorizer.fit_transform(data.Comment_text)        

    tfidf = pd.SparseDataFrame(tfidf.toarray()).to_sparse()
    tfidf.applymap(lambda x: round(x, 4))
    tfidf_features = ['tfidf_' + word for word in tfidf_vectorizer.get_feature_names()]
    tfidf.columns = tfidf_features
    data = data.reset_index().join(tfidf).set_index('index')

    return data, tfidf_vectorizer, tfidf_features    

X_train, tfidf_vectorizer, tfidf_features = get_tfidf_features(X_train)
</code></pre>

<p>I applied a simple logistic regression like this: </p>

<pre><code>logit = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr')
logit.fit(X_train.loc[:, features].fillna(0), X_train['Hateful_or_not'])
preds = logit.predict(X_test.loc[:, features].fillna(0))
</code></pre>

<p>I am getting feature importance like this:</p>

<pre><code> logit.coef_
</code></pre>

<p>But this is giving me feature importance of ""columns"" not words</p>
",Vectorization & Embeddings,get word importance nlp tfidf logistic regression function get tfidf feature like applied simple logistic regression like getting feature importance like giving feature importance column word
How to cluster keywords or get keywords similarity when I have their vectors,"<p>I have got a python-dictionary stored as Vector file with Pickle method (through Bert-as-Service and Google's pretrained model) like:</p>

<p>(key)Phrase : (value)Phrase_Vector_from_Bert = 
woman cloth : 1.3237 -2.6354 1.7458 ....</p>

<p>But I have no idea to get phrases' similarity with the vector files from Bert-as-Service model as I do with Gensim Word2Vec, since the later is equipped with .similarity method.</p>

<p>Would you please give an advice to get phrases/keywords similarity or to cluster them with my python-Pickle-dictionary vector file?</p>

<p>Or maybe is there an better idea to cluster keywords with Bert-as-Service?</p>

<p>The following codes show how I get the vectors for phrases/keywords:</p>

<pre><code>import Myutility
# the file Myutility includes the function save_model and load_model

import BertCommand
# the file Bertcommand includes the function to start Bert-as-service 
  client

WORD_PATH = 'E:/Works/testwords.txt'
WORD_FEATURE = 'E:/Works/word.google.vector'

word_vectors = {}

with open(WORD_PATH) as f:
    lines = f.readlines()
    for line in lines:
        line = line.strip('\n')
        if line:                
            word = line
            print(line)
            word_vectors[word]=None

for word in word_vectors:
    try:
        v = bc.encode([word])
        word_vectors[word] = v
    except:
        pass

save_model(word_vectors,WORD_FEATURE)
</code></pre>
",Vectorization & Embeddings,cluster keywords get keywords similarity vector got python dictionary stored vector file pickle method bert service google pretrained model like key phrase value phrase vector bert woman cloth idea get phrase similarity vector file bert service model gensim word vec since later equipped similarity method would please give advice get phrase keywords similarity cluster python pickle dictionary vector file maybe better idea cluster keywords bert service following code show get vector phrase keywords
embedding and clustering a specific text (using GloVe),"<p>Edit 2: I thought better on my question and realized it was way to generalized and it is only a matter of something basic;</p>

<p>creating a new array from the Glove file (glove.6B.300d.txt) that contains ONLY the list of words that I have in my document.</p>

<p>I'm aware that this actually has nothing to do with this specific GloVe file and I should learn how to do it for any two lists of words...</p>

<h2>I assume that I just don't know how properly look for this in order to learn how to execute this part. i.e what library to use/functions/buuzzwords I should look for.</h2>

<p>Edit 1: I'm adding the code I used that works for the whole GloVe library;</p>

<pre><code>from __future__ import division
from sklearn.cluster import KMeans
from numbers import Number
from pandas import DataFrame
import sys, codecs, numpy
class autovivify_list(dict):
  def __missing__(self, key):
     value = self[key] = []
     return value
  def __add__(self, x):

    if not self and isinstance(x, Number):
       return x
    raise ValueError
  def __sub__(self, x):

    if not self and isinstance(x, Number):
       return -1 * x
    raise ValueError
 def build_word_vector_matrix(vector_file, n_words):
   numpy_arrays = []
   labels_array = []
   with codecs.open(vector_file, 'r', 'utf-8') as f:
      for c, r in enumerate(f):
         sr = r.split()
         labels_array.append(sr[0])
         numpy_arrays.append( numpy.array([float(i) for i in sr[1:]]) )

         if c == n_words:
           return numpy.array( numpy_arrays ), labels_array

return numpy.array( numpy_arrays ), labels_array
def find_word_clusters(labels_array, cluster_labels):
  cluster_to_words = autovivify_list()
     for c, i in enumerate(cluster_labels):
     cluster_to_words[ i ].append( labels_array[c] )
  return cluster_to_words
if __name__ == ""__main__"":
   input_vector_file = 
   '/Users/.../Documents/GloVe/glove.6B/glove.6B.300d.txt'
   n_words = 1000 
   reduction_factor = 0.5
   n_clusters = int( n_words * reduction_factor ) 
   df, labels_array = build_word_vector_matrix(input_vector_file, 
   n_words)
   kmeans_model = KMeans(init='k-means++', n_clusters=n_clusters, 
   n_init=10)
   kmeans_model.fit(df)

   cluster_labels  = kmeans_model.labels_
   cluster_inertia   = kmeans_model.inertia_
   cluster_to_words  = find_word_clusters(labels_array, 
   cluster_labels)

   for c in cluster_to_words:
      print cluster_to_words[c]
      print ""\n""
</code></pre>

<hr>

<p>Original question: </p>

<p>Let's say I have a specific text (say of 500 words). 
I want to do the following: </p>

<ol>
<li>Create an embedding of all the words in this text (i.e have a list of the GloVe vectors only of this 500 words)</li>
<li>Cluster it (*this I know how to do)</li>
</ol>

<p>How do I do such a thing?</p>
",Vectorization & Embeddings,embedding clustering specific text using glove edit thought better question realized wa way generalized matter something basic creating new array glove file glove b txt contains list word document aware actually ha nothing specific glove file learn two list word assume know properly look order learn execute part e library use function buuzzwords look edit adding code used work whole glove library original question let say specific text say word want following create embedding word text e list glove vector word cluster know thing
how to find similarity between two question even though the words are differentiate,"<p>is there is any way to find the meaning of the string is similar or not,,, even though the words in the string are differentiated </p>

<p>Till now i tried fuzzy-wuzzy,levenstein distance,cosine similarity to match the string but all are matches the words not the meaning of the words</p>

<pre><code>Str1 = ""what are types of negotiation""
Str2 = ""what are advantages of negotiation""
Str3 = ""what are categories of negotiation""
Ratio = fuzz.ratio(Str1.lower(),Str2.lower())
Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())
Token_Sort_Ratio = fuzz.token_sort_ratio(Str1,Str2)
Ratio1 = fuzz.ratio(Str1.lower(),Str3.lower())
Partial_Ratio1 = fuzz.partial_ratio(Str1.lower(),Str3.lower())
Token_Sort_Ratio1 = fuzz.token_sort_ratio(Str1,Str3)
print(""fuzzywuzzy"")
print(Str1,"" "",Str2,"" "",Ratio)
print(Str1,"" "",Str2,"" "",Partial_Ratio)
print(Str1,"" "",Str2,"" "",Token_Sort_Ratio)
print(Str1,"" "",Str3,"" "",Ratio1)
print(Str1,"" "",Str3,"" "",Partial_Ratio1)
print(Str1,"" "",Str3,"" "",Token_Sort_Ratio1)
print(""levenshtein ratio"")
Ratio = levenshtein_ratio_and_distance(Str1,Str2,ratio_calc = True)
Ratio1 = levenshtein_ratio_and_distance(Str1,Str3,ratio_calc = True)
print(Str1,"" "",Str2,"" "",Ratio)
print(Str1,"" "",Str3,"" "",Ratio)

output:
fuzzywuzzy
what are types of negotiation   what are advantages of negotiation   86
what are types of negotiation   what are advantages of negotiation   76
what are types of negotiation   what are advantages of negotiation   73
what are types of negotiation   what are categories of negotiation   86
what are types of negotiation   what are categories of negotiation   76
what are types of negotiation   what are categories of negotiation   73
levenshtein ratio
what are types of negotiation   what are advantages of negotiation               
0.8571428571428571
what are types of negotiation   what are categories of negotiation       
0.8571428571428571



expected output:
""what are the types of negotiation skill?""
""what are the categories in negotiation skill?""
output:similar
""what are the types of negotiation skill?""
""what are the advantages of negotiation skill?""
output:not similar
</code></pre>
",Vectorization & Embeddings,find similarity two question even though word differentiate way find meaning string similar even though word string differentiated till tried fuzzy wuzzy levenstein distance cosine similarity match string match word meaning word
How can I recover the likelihood of a certain word appearing in a given context from word embeddings?,"<p>I know that some methods of generating word embeddings (e.g. CBOW) are based on predicting the likelihood of a given word appearing in a given context. I'm working with polish language, which is sometimes ambiguous with respect to segmentation, e.g. 'Coś' can be either treated as one word, or two words which have been conjoined ('Co' + '-ś') depending on the context. What I want to do, is create a tokenizer which is context sensitive. Assuming that I have the vector representation of the preceding context, and all possible segmentations, could I somehow calculate, or approximate the likelihood of particular words appearing in this context?</p>
",Vectorization & Embeddings,recover likelihood certain word appearing given context word embeddings know method generating word embeddings e g cbow based predicting likelihood given word appearing given context working polish language sometimes ambiguous respect segmentation e g co either treated one word two word conjoined co depending context want create tokenizer context sensitive assuming vector representation preceding context possible segmentation could somehow calculate approximate likelihood particular word appearing context
Is there a reason to not normalize the document output vectors of Doc2Vec for clustering?,"<p>I know that in Word2Vec the length of word vectors could encode properties like term frequency. In that case, we can see two word vectors, say synonyms, with a similar meaning but with a different length given their usage in our corpus.</p>

<p>However, if we normalize the word vectors, we keep their ""directions of meaning"" and we could clusterize them according that: meaning.</p>

<p>Following that train of thought, the same would be applicable to document vectors in Doc2Vec.</p>

<p>But my question is, is there a reason to <strong>NOT</strong> normalize document vectors if we want to cluster them? In Word2Vec we can say we want to keep the frequency property of the words, is there a similar thing for documents?</p>
",Vectorization & Embeddings,reason normalize document output vector doc vec clustering know word vec length word vector could encode property like term frequency case see two word vector say synonym similar meaning different length given usage corpus however normalize word vector keep direction meaning could clusterize according meaning following train thought would applicable document vector doc vec question reason normalize document vector want cluster word vec say want keep frequency property word similar thing document
What would be the most efficient way to embed sentences in a distributed Spark system?,"<p>I have a file with word embeddings (defining word embedding as the vector representation of a word), with the following format:</p>

<pre><code>a | [0.23, 0.04, ..., -0.22]
aaron | [0.21, 0.08, ..., -0.41]
... | ...
zebra | [0.97, 0.01, ..., -0.34]
</code></pre>

<p>This file is about 2.5 GB. I also have a large amount of sentences I want to convert into vectors, for example:</p>

<pre><code>Yes sir, today is a great day.
Would you want to buy that blue shirt?
...
Is there anything else I can help you with?
</code></pre>

<p>My sentence embedding strategy is simple for now: </p>

<pre><code>For each sentence:
  For each word:
    Obtain the vector representation of the word using the word embedding file.
  End
  Calculate the average of the word vectors of the sentence.
End
</code></pre>

<p>I figured that since I have a large amount of sentences I want to embed, I could use Spark for this task; storing the word embeddings as a file in the HDFS and using Spark SQL to query the sentences from a Hive table, but since each node would likely need to have access to the entire word embedding file that would imply collecting the entire word embedding RDD in each node, making communication between nodes very expensive.</p>

<p>Anyone has any ideas on how could this problem be efficiently solved? Please also let me know if the problem is not clear or you think I've misunderstood something about the way Spark works. I'm still learning and would really appreciate your help!</p>

<p>Thanks in advance. </p>
",Vectorization & Embeddings,would efficient way embed sentence distributed spark system file word embeddings defining word embedding vector representation word following format file gb also large amount sentence want convert vector example sentence embedding strategy simple figured since large amount sentence want embed could use spark task storing word embeddings file hdfs using spark sql query sentence hive table since node would likely need access entire word embedding file would imply collecting entire word embedding rdd node making communication node expensive anyone ha idea could problem efficiently solved please also let know problem clear think misunderstood something way spark work still learning would really appreciate help thanks advance
"Get similarity score between 2 words using Pre trained Bert, Elmo","<p>I am trying to compare Glove, Fasttext, Bert ,Elmo on basis on similarity between 2 words using pre-trained models of Wiki. Glove and Fasttext had pretrained models which could easily be used with gensim word2vec in python. Does Elmo and Bert have any such models ?</p>
",Vectorization & Embeddings,get similarity score word using pre trained bert elmo trying compare glove fasttext bert elmo basis similarity word using pre trained model wiki glove fasttext pretrained model could easily used gensim word vec python doe elmo bert model
Why does word2Vec use cosine similarity?,"<p>I have been reading the papers on Word2Vec (e.g. <a href=""https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"" rel=""noreferrer"">this one</a>), and I think I understand training the vectors to maximize the probability of other words found in the same contexts.</p>

<p>However, I do not understand why cosine is the correct measure of word similarity.  Cosine similarity says that two vectors point in the same direction, but they could have different magnitudes.</p>

<p>For example, cosine similarity makes sense comparing bag-of-words for documents.  Two documents might be of different length, but have similar distributions of words.</p>

<p>Why not, say, Euclidean distance?</p>

<p>Can anyone one explain why cosine similarity works for word2Vec?</p>
",Vectorization & Embeddings,doe word vec use cosine similarity reading paper word vec e g one think understand training vector maximize probability word found context however understand cosine correct measure word similarity cosine similarity say two vector point direction could different magnitude example cosine similarity make sense comparing bag word document two document might different length similar distribution word say euclidean distance anyone one explain cosine similarity work word vec
Calculate word embeddings using fasttext,"<p>I am trying to calculate the word embeddings using fasttext for the following sentence.</p>

<pre><code>a = 'We are pencil in the hands'
</code></pre>

<p>I dont have any pretrained model, so how do i go about it?</p>
",Vectorization & Embeddings,calculate word embeddings using fasttext trying calculate word embeddings using fasttext following sentence dont pretrained model go
Amazon SageMaker BlazingText,"<p>I am working on a word embedding project. I am using Amazon SageMaker for this purpose. The BlazingText algorithm in the Amazon SageMaker produced fast result than the other options. But I don't see any facility to get the prediction model or the weights. The output consists only the vectors file from which I cannot generate the model.
Is there any way by which I can get the model with the vector file? I need this to predict new words. Thanks in advance.</p>
",Vectorization & Embeddings,amazon sagemaker blazingtext working word embedding project using amazon sagemaker purpose blazingtext algorithm amazon sagemaker produced fast result option see facility get prediction model weight output consists vector file generate model way get model vector file need predict new word thanks advance
spacy similarity bigger than 1,"<p>The spaCy similarity works strange sometimes.
If we compare the completely equal texts, we got a score of 1.0.
but the texts are almost equal we can get a score > 1.
This behavior could harm our code.
Why we got this > 1.0 score and can we predict it?</p>

<pre><code>def calc_score(text_source, text_target):
    return nlp(text_source).similarity(nlp(text_target))

# nlp = spacy.load('en_core_web_md')
calc_score('software development', 'Software development')
# 1.0000000155153665
</code></pre>
",Vectorization & Embeddings,spacy similarity bigger spacy similarity work strange sometimes compare completely equal text got score text almost equal get score behavior could harm code got score predict
keras pad_sequence for string data type,"<p>I have a list of sentences. I want to add padding to them; but when I use keras pad_sequence like this:</p>

<pre><code>from keras.preprocessing.sequence import pad_sequences
s = [[""this"", ""is"", ""a"", ""book""], [""this"", ""is"", ""not""]]
g = pad_sequences(s, dtype='str', maxlen=10, value='_PAD_')
</code></pre>

<p>the result is:</p>

<pre><code>array([['_', '_', '_', '_', '_', '_', 't', 'i', 'a', 'b'],
       ['_', '_', '_', '_', '_', '_', '_', 't', 'i', 'n']], dtype='&lt;U1')
</code></pre>

<p>Why it is not working properly?</p>

<p>I want to use this result as the input to the ELMO embedding and I need string sentences not integer encoding.</p>
",Vectorization & Embeddings,kera pad sequence string data type list sentence want add padding use kera pad sequence like result working properly want use result input elmo embedding need string sentence integer encoding
SageMaker AWS Binary Text Classification,"<p>Can AWS SageMaker handle binary classification using TFidf vectorized text as prediction base?</p>
",Vectorization & Embeddings,sagemaker aws binary text classification aws sagemaker handle binary classification using tfidf vectorized text prediction base
why my neural network sequential model gets low accuracy ( below than 0.0011)?,"<p>I am building a hashtag recommendation model for twitter media posts, which takes tweet text as input and does 300-dimensional word embedding on it and classifies it among 198 hashtags as classes. When I run my model I get lower than 0.0011 accuracy which does not change later! What is wrong in my model? </p>

<pre><code>import pickle

import numpy as np

from keras import initializers, regularizers
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from keras.layers import LSTM, Activation, Dense, Dropout, Embedding
from keras.layers.normalization import BatchNormalization
from keras.models import Sequential, load_model

package = ""2018_pickle""
with open(path1, ""rb"") as f:
    maxLen, l_h2i, l_w2i = pickle.load(f)
with open(path2, ""rb"") as f:
    X_train, X_test, X_train_indices, X_test_indices = pickle.load(f)
with open(path3, ""rb"") as f:
    Y_train, Y_test, Y_train_oh, Y_test_oh = pickle.load(f)
with open(path4, ""rb"") as f:
    emd_matrix = pickle.load(f)


if __name__ == ""__main__"":
    modelname = ""model_1""
    train = False
    vocab_size = len(emd_matrix)
    emd_dim = emd_matrix.shape[1]
    if train:
        model = Sequential()
        model.add(
            Embedding(
                vocab_size,
                emd_dim,
                weights=[emd_matrix],
                input_length=maxLen,
                trainable=False,
            )
        )
        model.add(
            LSTM(
                256,
                return_sequences=True,
                activation=""relu"",
                kernel_regularizer=regularizers.l2(0.01),
                kernel_initializer=initializers.glorot_normal(seed=None),
            )
        )
        model.add(
            LSTM(
                256,
                return_sequences=True,
                activation=""relu"",
                kernel_regularizer=regularizers.l2(0.01),
                kernel_initializer=initializers.glorot_normal(seed=None),
            )
        )
        model.add(
            LSTM(
                256,
                return_sequences=False,
                activation=""relu"",
                kernel_regularizer=regularizers.l2(0.01),
                kernel_initializer=initializers.glorot_normal(seed=None),
            )
        )
        model.add(Dense(198, activation=""softmax""))
        model.compile(
            loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""]
        )
        checkpoint = ModelCheckpoint(
            filepath, monitor=""loss"", verbose=1, save_best_only=True, mode=""min""
        )
        reduce_lr = ReduceLROnPlateau(
            monitor=""val_loss"", factor=0.5, patience=2, min_lr=0.000001
        )
        history = model.fit(
            X_train_indices,
            Y_train_oh,
            batch_size=2048,
            epochs=5,
            validation_split=0.1,
            shuffle=True,
            callbacks=[checkpoint, reduce_lr],
        )

</code></pre>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_10 (Embedding)     (None, 54, 300)           22592100  
_________________________________________________________________
lstm_18 (LSTM)               (None, 54, 256)           570368    
_________________________________________________________________
lstm_19 (LSTM)               (None, 54, 256)           525312    
_________________________________________________________________
lstm_20 (LSTM)               (None, 256)               525312    
_________________________________________________________________
dense_7 (Dense)              (None, 198)               50886     
=================================================================
Total params: 24,263,978
Trainable params: 1,671,878
Non-trainable params: 22,592,100
_________________________________________________________________
None
Train on 177278 samples, validate on 19698 samples
Epoch 1/5
177278/177278 [==============================] - 70s 396us/step - loss:     3.3672 - acc: 8.7433e-04 - val_loss: 0.3103 - val_acc: 0.0000e+00

Epoch 00001: loss improved from inf to 3.36719, saving model to     ./checkpoints/model_1/lstm-01-3.367-0.001-0.310-0.000.hdf5
Epoch 2/5
177278/177278 [==============================] - 66s 371us/step - loss: 0.1950 - acc: 2.4820e-04 - val_loss: 0.1616 - val_acc: 0.0016

Epoch 00002: loss improved from 3.36719 to 0.19496, saving model to ./checkpoints/model_1/lstm-02-0.195-0.000-0.162-0.002.hdf5
Epoch 3/5
177278/177278 [==============================] - 66s 370us/step - loss: 0.1583 - acc: 0.0011 - val_loss: 0.1570 - val_acc: 0.0016

Epoch 00003: loss improved from 0.19496 to 0.15826, saving model to     ./checkpoints/model_1/lstm-03-0.158-0.001-0.157-0.002.hdf5
Epoch 4/5
177278/177278 [==============================] - 65s 369us/step - loss: 0.1566 - acc: 0.0011 - val_loss: 0.1573 - val_acc: 0.0016

Epoch 00004: loss improved from 0.15826 to 0.15660, saving model to     ./checkpoints/model_1/lstm-04-0.157-0.001-0.157-0.002.hdf5
Epoch 5/5
177278/177278 [==============================] - 66s 374us/step - loss: 0.1561 - acc: 0.0011 - val_loss: 0.1607 - val_acc: 0.0016

Epoch 00005: loss improved from 0.15660 to 0.15610, saving model to     ./checkpoints/model_1/lstm-05-0.156-0.001-0.161-0.002.hdf5
</code></pre>
",Vectorization & Embeddings,neural network sequential model get low accuracy building hashtag recommendation model twitter medium post take tweet text input doe dimensional word embedding classifies among hashtags class run model get lower accuracy doe change later wrong model
why my neural network sequential model reaches 0.9998 accuracy form the beginning?,"<p>I am building a hashtag recommendation model for twitter media posts, which takes tweet text as input and does 300-dimensional word embedding on it and classifies it among 198 hashtags as classes. When I run my model I get 0.9998 accuracy from the beginning which does not change later! What is wrong in my model? </p>

<pre><code>import numpy as np
import pickle
from keras.layers.normalization import BatchNormalization
from keras.models import Sequential, load_model
from keras.layers import Dense, Dropout, Activation,LSTM, Embedding
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from keras import regularizers, initializers
package=""2018_pickle""
with open(path1,""rb"") as f:
    maxLen,l_h2i,l_w2i=pickle.load(f)
with open(path2,""rb"") as f:
    X_train,X_test,X_train_indices,X_test_indices=pickle.load(f)
with open(path3,""rb"") as f:
    Y_train,Y_test,Y_train_oh,Y_test_oh=pickle.load(f)
with open(path4,""rb"") as f:
    emd_matrix=pickle.load(f)


if __name__ == '__main__':
modelname=""model_1""
train=False
vocab_size = len(emd_matrix)
emd_dim=emd_matrix.shape[1]
if train:
    model = Sequential()
    model.add(Embedding(vocab_size , emd_dim, weights=[emd_matrix]
                        ,input_length=maxLen,trainable=False))
    model.add(LSTM(256,return_sequences=True,activation=""relu"",
                   kernel_regularizer=regularizers.l2(0.01),
                   kernel_initializer=initializers.glorot_normal(seed=None)))
    model.add(LSTM(256,return_sequences=True,activation=""relu"",
                   kernel_regularizer=regularizers.l2(0.01),
                   kernel_initializer=initializers.glorot_normal(seed=None)))
    model.add(LSTM(256,return_sequences=False,activation=""relu"",
                   kernel_regularizer=regularizers.l2(0.01),
                   kernel_initializer=initializers.glorot_normal(seed=None)))
    model.add(Dense(198,activation='softmax'))
    model.compile(loss='binary_crossentropy', optimizer='adam',
                  metrics=['accuracy'])
    checkpoint = ModelCheckpoint(filepath, monitor=""loss"",
                                 verbose=1, save_best_only=True, mode='min')
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,
                                  patience=2, min_lr=0.000001)
    history=model.fit(X_train_indices, Y_train_oh, batch_size=2048,
                      epochs=5, validation_split=0.1, shuffle=True,
                      callbacks=[checkpoint, reduce_lr])


_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_10 (Embedding)     (None, 54, 300)           22592100  
_________________________________________________________________
lstm_18 (LSTM)               (None, 54, 256)           570368    
_________________________________________________________________
lstm_19 (LSTM)               (None, 54, 256)           525312    
_________________________________________________________________
lstm_20 (LSTM)               (None, 256)               525312    
_________________________________________________________________
dense_7 (Dense)              (None, 198)               50886     
=================================================================
Total params: 24,263,978
Trainable params: 1,671,878
Non-trainable params: 22,592,100
_________________________________________________________________
</code></pre>
",Vectorization & Embeddings,neural network sequential model reach accuracy form beginning building hashtag recommendation model twitter medium post take tweet text input doe dimensional word embedding classifies among hashtags class run model get accuracy beginning doe change later wrong model
How find the most decisive sentences or words in a document via Doc2Vec?,"<p>I've trained a Doc2Vec model in order to do a simple binary classification task, but I would also love to see which words or sentences weigh more in terms of contributing to the meaning of a given text. So far I had no luck finding anything relevant or helpful. Any ideas how could I implement this feature? Should I switch from Doc2Vec to more conventional methods like tf-idf?</p>
",Vectorization & Embeddings,find decisive sentence word document via doc vec trained doc vec model order simple binary classification task would also love see word sentence weigh term contributing meaning given text far luck finding anything relevant helpful idea could implement feature switch doc vec conventional method like tf idf
Does doc2vec work with multi-class problem with only 1 sample per class?,"<p>I have a test sentence (which define a skill, such as ""Perform equipment maintenance"") and a set of diplomas (10000 different diplomas) with description of the needed skills (=1 paragraph per diploma). My problem consists in finding the diploma closest to the test sentence in terms of semantic similarity.</p>

<p>I thought about creating a doc2vec model (multi-class, 1 class per diploma) in order to transform each diploma in feature vector, then infer vector for the test sentence and calculate cosine similarity with each feature vector. Yet, I only have one sample for each diploma. Will it still work? 
Or do I have to split the sentences of each diploma text in order to obtain several samples for a diploma ?</p>
",Vectorization & Embeddings,doe doc vec work multi class problem sample per class test sentence define skill perform equipment maintenance set diploma different diploma description needed skill paragraph per diploma problem consists finding diploma closest test sentence term semantic similarity thought creating doc vec model multi class class per diploma order transform diploma feature vector infer vector test sentence calculate cosine similarity feature vector yet one sample diploma still work split sentence diploma text order obtain several sample diploma
Ignore out-of-vocabulary words when averaging vectors in Spacy,"<p>I would like to use a pre-trained word2vec model in Spacy to encode titles by (1) mapping words to their vector embeddings and (2) perform the mean of word embeddings. </p>

<p>To do this I use the following code: </p>

<pre><code>import spacy
nlp = spacy.load('myspacy.bioword2vec.model')
sentence = ""I love Stack Overflow butitsalsodistractive""
avg_vector = nlp(sentence).vector
</code></pre>

<p>Where <code>nlp(sentence).vector</code> (1) tokenizes my sentence with white-space splitting, (2) vectorizes each word according to the dictionary provided and (3) averages the word vectors within a sentence to provide a single output vector. That's fast and cool. </p>

<p>However, in this process, out-of-vocabulary (OOV) terms are mapped to n-dimensional 0 vectors, which affects the resulting mean. Instead, I would like OOV terms to be ignored when performing the average. In my example, '<em>butitsalsodistractive</em>' is the only term not present in my dictionary, so I would like  <code>nlp(""I love Stack Overflow butitsalsodistractive"").vector = nlp(""I love Stack Overflow"").vector</code>. </p>

<p>I have been able to do this with a post-processing step (see code below), but this becomes too slow for my purposes, so I was wondering if there is a way to tell the <code>nlp</code> pipeline to ignore OOV terms beforehand? So when calling <code>nlp(sentence).vector</code> it does not include OOV-term vectors when computing the mean</p>

<pre><code>import numpy as np
avg_vector = np.asarray([word.vector for word in nlp(sentence) if word.has_vector]).mean(axis=0)
</code></pre>

<p><strong>Approaches tried</strong></p>

<p>In both cases <code>documents</code> is a list with 200 string elements with ≈ 400 words each.  </p>

<ol>
<li>Without dealing with OOV terms: </li>
</ol>

<pre><code>import spacy
import time
nlp = spacy.load('myspacy.bioword2vec.model')
times = []
for i in range(0, 100):
    init = time.time()
    documents_vec = [document.vector for document in list(nlp.pipe(documents))]
    fin = time.time()
    times.append(fin-init)
print(""Mean time after 100 rounds:"", sum(times)/len(times), ""s"")
# Mean time after 100 rounds: 2.0850741124153136 s
</code></pre>

<ol start=""2"">
<li>Ignoring OOV terms in output vector. Note that in this case we need to add an extra 'if' statment for those cases in which all words are OOV (if this happens the output vector is r_vec):</li>
</ol>

<pre><code>r_vec = np.random.rand(200) # Random vector for empty text
# Define function to obtain average vector given a document
def get_vector(text):
    vectors = np.asarray([word.vector for word in nlp(text) if word.has_vector])
    if vectors.size == 0:
        # Case in which none of the words in text were in vocabulary
        avg_vector = r_vec
    else:
        avg_vector = vectors.mean(axis=0)
    return avg_vector

times = []
for i in range(0, 100):
    init = time.time()
    documents_vec = [get_vector(document) for document in documents]
    fin = time.time()
    times.append(fin-init)
print(""Mean time after 100 rounds:"", sum(times)/len(times), ""s"")
# Mean time after 100 rounds: 2.4214172649383543 s
</code></pre>

<p>In this example the mean difference time in vectorizing 200 documents was 0.34s. However, when processing 200M documents this becomes critical. I am aware that the second approach needs an extra 'if' condition to deal with documents full of OOV terms, which might slightly increase computational time. In addition, in the first case I am able to use <code>nlp.pipe(documents)</code> to process all documents in one go, which I guess must optimize the process. </p>

<p>I could always look for extra computational resources to apply the second piece of code, but I was wondering if there is any way of applying the <code>nlp.pipe(documents)</code> ignoring the OOV terms in the output. Any suggestion will be very much welcome.  </p>
",Vectorization & Embeddings,ignore vocabulary word averaging vector spacy would like use pre trained word vec model spacy encode title mapping word vector embeddings perform mean word embeddings use following code tokenizes sentence white space splitting vectorizes word according dictionary provided average word vector within sentence provide single output vector fast cool however process vocabulary oov term mapped n dimensional vector affect resulting mean instead would like oov term ignored performing average example butitsalsodistractive term present dictionary would like able post processing step see code becomes slow purpose wa wondering way tell pipeline ignore oov term beforehand calling doe include oov term vector computing mean approach tried case list string element word without dealing oov term ignoring oov term output vector note case need add extra statment case word oov happens output vector r vec example mean difference time vectorizing document wa however processing document becomes critical aware second approach need extra condition deal document full oov term might slightly increase computational time addition first case able use process document one go guess must optimize process could always look extra computational resource apply second piece code wa wondering way applying ignoring oov term output suggestion much welcome
I get &#39;single&#39; characters as learned vocabulary on word2vec genism as an output,"<p>I am new for word2vec and I have trained a text file via word2vec for feature extraction than when I look at the words that are trained I found that it is single characters instead of words, what did I miss here? anyone help</p>

<p>I try to feed tokens instead of the raw text into the models</p>

<pre><code>import nltk

from pathlib import Path
data_folder = Path("""")
file_to_open = data_folder / ""test.txt""
#read the file
file = open(file_to_open , ""rt"")
raw_text = file.read()
file.close()

#tokenization
token_list = nltk.word_tokenize(raw_text)

#Remove Punctuation
from nltk.tokenize import punkt
token_list2 = list(filter(lambda token : punkt.PunktToken(token).is_non_punct,token_list))
#upper to lower case
token_list3 = [word.lower() for word in token_list2]
#remove stopwords
from nltk.corpus import stopwords
token_list4 = list(filter(lambda token: token not in stopwords.words(""english""),token_list3))

#lemmatization
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
token_list5 = [lemmatizer.lemmatize(word) for word in token_list4]
print(""Final Tokens are :"")
print(token_list5,""\n"")
print(""Total tokens : "", len(token_list5))

#word Embedding
from gensim.models import Word2Vec
# train model
model = Word2Vec(token_list5, min_count=2)
# summarize the loaded model

    print(""The model is :"")
    print(model,""\n"")`enter code here`

# summarize vocabulary

    words = list(model.wv`enter code here`.vocab)
    print(""The learned vocabulary words are : \n"",words)

Output- ['p', 'o', 't', 'e', 'n', 'i', 'a', 'l', 'r', 'b', 'u', 'm', 'h', 'd', 'c', 's', 'g', 'q', 'f', 'w', '-']
Expected -[ 'potenial', 'xyz','etc']
</code></pre>
",Vectorization & Embeddings,get single character learned vocabulary word vec genism output new word vec trained text file via word vec feature extraction look word trained found single character instead word miss anyone help try feed token instead raw text model
Cluster similar words using word2vec,"<p>I have various restaurant labels with me and i have some words that are unrelated to restaurants as well. like below:</p>

<p><code>vegan
vegetarian
pizza
burger
transportation
coffee
Bookstores
Oil and Lube</code></p>

<p>I have such mix of around 500 labels. I want to know is there a way pick the similar labels that are related to food choices and leave out words like oil and lube, transportation.</p>

<p>I tried using word2vec but, some of them have more than one word and could not figure out a right way. </p>

<p>Brute-force approach is to tag them manually. But, i want to know is there a way using NLP or Word2Vec to cluster all related labels together.</p>
",Vectorization & Embeddings,cluster similar word using word vec various restaurant label word unrelated restaurant well like mix around label want know way pick similar label related food choice leave word like oil lube transportation tried using word vec one word could figure right way brute force approach tag manually want know way using nlp word vec cluster related label together
Calling tf.session.run gets slower,"<p>I am having trouble with my performance doing nlp tasks. I want to use <a href=""https://tfhub.dev/google/nnlm-en-dim128-with-normalization/1"" rel=""nofollow noreferrer"">this</a> module for word embeddings and it produces output, but its runtime increases with each iterative call. I have already read about different solutions, but i cant get them to work. I suspect using <a href=""https://www.tensorflow.org/api_docs/python/tf/placeholder"" rel=""nofollow noreferrer"">tf.placeholders</a> would be the a good solution, but i dont know how to use them in this instance.</p>

<p>Example code for my problem:</p>

<pre class=""lang-py prettyprint-override""><code>embedder = hub.Module(""https://tfhub.dev/google/nnlm-en-dim128-with-normalization/1"")
session = tf.Session()
session.run(tf.global_variables_initializer())
session.run(tf.tables_initializer())

doc = [[""Example1"", ""Example2"", ""Example3"", ""Example4"", ...], [...], ...]

for paragraph in doc:
   vectors = session.run(embedder(paragraph))
   #do something with vectors
</code></pre>

<p>Note, that doc cant be fed to the embedder all at once.</p>

<p>Thank you in advance.</p>
",Vectorization & Embeddings,calling tf session run get slower trouble performance nlp task want use module word embeddings produce output runtime increase iterative call already read different solution cant get work suspect using tf placeholder would good solution dont know use instance example code problem note doc cant fed embedder thank advance
Adding vocabulary and improve word embedding with another model that was built on bigger corpus,"<p>I'm new to NLP. I'm currently building a NLP system in a specific domain. After training a word2vec and fasttext model on my documents, I found that the embedding is not really good because I didn't feed enough number of documents (e.g. the embedding can't see that ""bar"" and ""pub"" is strongly correlated to each other because ""pub"" only appears a few in the documents). Later, I found a word2vec model online built on that domain-specific corpus which definitely has a way better embedding (so ""pub"" is more related to ""bar""). Is there any way to improve my word embedding using the model I found? Thanks!</p>
",Vectorization & Embeddings,adding vocabulary improve word embedding another model wa built bigger corpus new nlp currently building nlp system specific domain training word vec fasttext model document found embedding really good feed enough number document e g embedding see bar pub strongly correlated pub appears document later found word vec model online built domain specific corpus definitely ha way better embedding pub related bar way improve word embedding using model found thanks
How to use Elmo word embedding with the original pre-trained model (5.5B) in interactive mode,"<p>I am trying to learn how to use Elmo embeddings via this tutorial:</p>

<p><a href=""https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md</a></p>

<p>I am specifically trying to use the interactive mode as described like this:</p>

<pre><code>$ ipython
&gt; from allennlp.commands.elmo import ElmoEmbedder
&gt; elmo = ElmoEmbedder()
&gt; tokens = [""I"", ""ate"", ""an"", ""apple"", ""for"", ""breakfast""]
&gt; vectors = elmo.embed_sentence(tokens)

&gt; assert(len(vectors) == 3) # one for each layer in the ELMo output
&gt; assert(len(vectors[0]) == len(tokens)) # the vector elements 
correspond with the input tokens

&gt; import scipy
&gt; vectors2 = elmo.embed_sentence([""I"", ""ate"", ""a"", ""carrot"", ""for"", 
""breakfast""])
&gt; scipy.spatial.distance.cosine(vectors[2][3], vectors2[2][3]) # cosine 
distance between ""apple"" and ""carrot"" in the last layer
0.18020617961883545
</code></pre>

<p>My overall question is how do I make sure to use the pre-trained elmo model on the original 5.5B set (described here: <a href=""https://allennlp.org/elmo"" rel=""nofollow noreferrer"">https://allennlp.org/elmo</a>)?</p>

<p>I don't quite understand why we have to call ""assert"" or why we use the [2][3] indexing on the vector output. </p>

<p>My ultimate purpose is to average the all the word embeddings in order to get a sentence embedding, so I want to make sure I do it right! </p>

<p>Thanks for your patience as I am pretty new in all this.</p>
",Vectorization & Embeddings,use elmo word embedding original pre trained model b interactive mode trying learn use elmo embeddings via tutorial specifically trying use interactive mode described like overall question make sure use pre trained elmo model original b set described quite understand call assert use indexing vector output ultimate purpose average word embeddings order get sentence embedding want make sure right thanks patience pretty new
How to use sklearn TfidfVectorizer on new data,"<p>I have a fairly simple NLTK and sklearn classifier (I'm a complete noob at this). </p>

<p>I do the usual imports</p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt

from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer

from sklearn.model_selection import train_test_split

from sklearn.naive_bayes import MultinomialNB

from sklearn import metrics

from sklearn.feature_extraction.text import TfidfVectorizer
</code></pre>

<p>I load the data (I already cleaned it). It is a very simple dataframe with two columns. The first is <code>'post_clean'</code> which contains the cleaned text, the second is <code>'uk'</code> which is either <code>True</code> or <code>False</code></p>

<pre><code>data = pd.read_pickle('us_uk_posts.pkl')
</code></pre>

<p>Then I Vectorize with tfidf and split the dataset, followed by creating the model</p>

<pre><code>tf = TfidfVectorizer()
text_tf = tf.fit_transform(data['post_clean'])
X_train, X_test, y_train, y_test = train_test_split(text_tf, data['uk'], test_size=0.3, random_state=123)


clf = MultinomialNB().fit(X_train, y_train)
predicted = clf.predict(X_test)
print(""MultinomialNB Accuracy:"" , metrics.accuracy_score(y_test,predicted))
</code></pre>

<p>Apparently, unless I'm completely missing something here, I have Accuracy of 93%</p>

<p>My two questions are:</p>

<p>1) How do I now use this model to actually classify some items that don't have a known <code>UK</code> value? </p>

<p>2) How do I test this model using a completely separate test set (that I haven't split)?</p>

<p>I have tried</p>

<p><code>new_data = pd.read_pickle('new_posts.pkl')</code></p>

<p>Where the new_posts data is in the same format</p>

<pre><code>new_text_tf = tf.fit_transform(new_data['post_clean'])

predicted = clf.predict(new_X_train)
predicted
</code></pre>

<p>and</p>

<pre><code>new_text_tf = tf.fit_transform(new_data['post_clean'])

new_X_train, new_X_test, new_y_train, new_y_test = train_test_split(new_text_tf, new_data['uk'], test_size=1)

predicted = clf.predict(new_text_tf)
predicted
</code></pre>

<p>but both  return ""ValueError: dimension mismatch""</p>
",Vectorization & Embeddings,use sklearn tfidfvectorizer new data fairly simple nltk sklearn classifier complete noob usual import load data already cleaned simple dataframe two column first contains cleaned text second either vectorize tfidf split dataset followed creating model apparently unless completely missing something accuracy two question use model actually classify item known value test model using completely separate test set split tried new post data format return valueerror dimension mismatch
"Should I train embeddings using data from both training,validating and testing corpus?","<p>I am in a case that I don't have any pre-trained words embedding for my domain (Vietnamese food reviews). so I got a though of embedding from the general and specific corpus.</p>

<p>And the point here is can I use the dataset of training, test and validating (did preprocess) as a source for creating my own word embeddings. If don't, hope you can give your experience.</p>

<p>Based on my intuition, and some experiments a wide corpus appears to be better, but I'd like to know if there's relevant research or other relevant results.</p>
",Vectorization & Embeddings,train embeddings using data training validating testing corpus case pre trained word embedding domain vietnamese food review got though embedding general specific corpus point use dataset training test validating preprocess source creating word embeddings hope give experience based intuition experiment wide corpus appears better like know relevant research relevant result
Word shows up more than once in TSNE plot,"<p>When plotting word embedding TSNE results, words show up more than once.</p>

<p>I am reducing dimensionality of a Word2Vec word embedding, but when I plot the results for a subset of the most similar words (manually enter several words for which I want the most similar ones), the same words show up more than once:</p>

<pre><code>from sklearn.manifold import TSNE

words = sum([[k] + v for k, v in similar_words.items()], [])
wvs = model.wv[words]

tsne = TSNE(n_components=3, random_state=0, n_iter=10000, perplexity=29)
np.set_printoptions(suppress=True)
T = tsne.fit_transform(wvs)
labels = words

plt.figure(figsize=(16, 12))
plt.scatter(T[:, 0], T[:, 1], c='purple', edgecolors='purple')
for label, x, y in zip(labels, T[:, 0], T[:, 1]):
    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')
</code></pre>

<p>Is this a normal behavior for PCA and TSNE word similarity dimensionality reduction, or is there something off with my code? Is it possible that the plot is treating each of the similar words subsets as independent from each other?</p>
",Vectorization & Embeddings,word show tsne plot plotting word embedding tsne result word show reducing dimensionality word vec word embedding plot result subset similar word manually enter several word want similar one word show normal behavior pca tsne word similarity dimensionality reduction something code possible plot treating similar word subset independent
Universal sentence encoding embedding digits very similar,"<p>I have task of sentence similarity where i calculate the cosine of two sentence to decide how similar they are . It seems that for sentence with digits the similarity is not affected no matter how ""far"" the numbers are . For an example:</p>

<p>a = generate_embedding('issue 845')</p>

<p>b = generate_embedding('issue 11')</p>

<p>cosine_sim(a,b) = 0.9307</p>

<p>is there a way to distance the hashing of numbers or any other hack to handle that issue?</p>
",Vectorization & Embeddings,universal sentence encoding embedding digit similar task sentence similarity calculate cosine two sentence decide similar seems sentence digit similarity affected matter far number example generate embedding issue b generate embedding issue cosine sim b way distance hashing number hack handle issue
Does the size of a database affect the predictions speed of a model,"<p>I am building a chatbot model in keras, and I am planning on using it on a raspberry pi. I have a huge database with the size of (1000000, 15, 100) which means there are 1 million sample with a maximum of 15 words and the embedding dimensions are 100d using GloVe. I build a simple model consists of 1 embedding layer, 1 bidirectional lstm layer, 1 droput layer and 2 dense layer with output shape of (25,).</p>

<p>I know that because of the huge database the training process is going to take long, but does the size of the database going to affect the speed of model.predict or does the speed only influenced by the structure of the model and the shape of the input?</p>
",Vectorization & Embeddings,doe size database affect prediction speed model building chatbot model kera planning using raspberry pi huge database size mean million sample maximum word embedding dimension using glove build simple model consists embedding layer bidirectional lstm layer droput layer dense layer output shape know huge database training process going take long doe size database going affect speed model predict doe speed influenced structure model shape input
dl4j - what&#39;s the label mechanism in paragraph2vec?,"<p>I just read the paper <a href=""https://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""nofollow noreferrer"">Distributed Representations of Sentences and Documents</a>. In the sentiment analysis experiment section, it says, ""After learning the vector representations for training sentences and their subphrases, we feed them to a logistic regression to learn a predictor of the movie rating."" So it uses logistic regression algorithm as a classifier to determine what the label is. </p>

<p>Then I moved on to dl4j, I read the example ""ParagraphVectorsClassifierExample"" the code shows as below:</p>

<pre><code>       void makeParagraphVectors()  throws Exception {
         ClassPathResource resource = new ClassPathResource(""paravec/labeled"");

         // build a iterator for our dataset
         iterator = new FileLabelAwareIterator.Builder()
                 .addSourceFolder(resource.getFile())
                 .build();

         tokenizerFactory = new DefaultTokenizerFactory();
         tokenizerFactory.setTokenPreProcessor(new CommonPreprocessor());

         // ParagraphVectors training configuration
         paragraphVectors = new ParagraphVectors.Builder()
                 .learningRate(0.025)
                 .minLearningRate(0.001)
                 .batchSize(1000)
                 .epochs(20)
                 .iterate(iterator)
                 .trainWordVectors(true)
                 .tokenizerFactory(tokenizerFactory)
                 .build();

         // Start model training
         paragraphVectors.fit();
       }

       void checkUnlabeledData() throws IOException {
         /*
         At this point we assume that we have model built and we can check
         which categories our unlabeled document falls into.
         So we'll start loading our unlabeled documents and checking them
        */
        ClassPathResource unClassifiedResource = new ClassPathResource(""paravec/unlabeled"");
        FileLabelAwareIterator unClassifiedIterator = new FileLabelAwareIterator.Builder()
                .addSourceFolder(unClassifiedResource.getFile())
                .build();

        /*
         Now we'll iterate over unlabeled data, and check which label it could be assigned to
         Please note: for many domains it's normal to have 1 document fall into few labels at once,
         with different ""weight"" for each.
        */
        MeansBuilder meansBuilder = new MeansBuilder(
            (InMemoryLookupTable&lt;VocabWord&gt;)paragraphVectors.getLookupTable(),
              tokenizerFactory);
        LabelSeeker seeker = new LabelSeeker(iterator.getLabelsSource().getLabels(),
            (InMemoryLookupTable&lt;VocabWord&gt;) paragraphVectors.getLookupTable());

        while (unClassifiedIterator.hasNextDocument()) {
            LabelledDocument document = unClassifiedIterator.nextDocument();
            INDArray documentAsCentroid = meansBuilder.documentAsVector(document);
            List&lt;Pair&lt;String, Double&gt;&gt; scores = seeker.getScores(documentAsCentroid);

            /*
             please note, document.getLabel() is used just to show which document we're looking at now,
             as a substitute for printing out the whole document name.
             So, labels on these two documents are used like titles,
             just to visualize our classification done properly
            */
            log.info(""Document '"" + document.getLabels() + ""' falls into the following categories: "");
            for (Pair&lt;String, Double&gt; score: scores) {
                log.info(""        "" + score.getFirst() + "": "" + score.getSecond());
            }
        }

       }
</code></pre>

<p>It demonstrates how does doc2vec associate arbitrary documents with labels, but it hides the implementations behind the scenes. My question is: is it does so also by logistic regression? if not, what is it? And how can I do it by logistic regression?</p>
",Vectorization & Embeddings,dl j label mechanism paragraph vec read paper distributed representation sentence document sentiment analysis experiment section say learning vector representation training sentence subphrases feed logistic regression learn predictor movie rating us logistic regression algorithm classifier determine label moved dl j read example paragraphvectorsclassifierexample code show demonstrates doe doc vec associate arbitrary document label hide implementation behind scene question doe also logistic regression logistic regression
"How to compare frequency of unigrams with frequencies of bigrams, trigrams, etc?","<p>I want to build a word cloud containing multiple word structures (not just one word). In any given text we will have bigger frequencies for unigrams than bigrams. Actually, the n-gram <strong>frequency</strong> decreases when <strong>n</strong> increases for the same text.</p>

<p>I want to find a magic number or a method to obtain comparative results between unigrams and bigrams, trigrams, n-grams. </p>

<p><strong>There is any magic number as a multiplier for n-gram frequency in order to be comparable with a unigram?</strong> </p>

<p>A solution that I have now in mind is to make a top for any n-gram (1, 2, 3, ...) and use the first z positions for any category of n-grams.</p>
",Vectorization & Embeddings,compare frequency unigrams frequency bigram trigram etc want build word cloud containing multiple word structure one word given text bigger frequency unigrams bigram actually n gram frequency decrease n increase text want find magic number method obtain comparative result unigrams bigram trigram n gram magic number multiplier n gram frequency order comparable unigram solution mind make top n gram use first z position category n gram
Can we compare word vectors from different models using transfer learning?,"<p>I want to train two word2vec/GLoVe models on different corpora and then compare the vectors of a single word. I know that it makes no sense to do so as different models start at different random states, but what if we use pre-trained word vectors as the starting point. Can we assume that the two models will continue to build upon the pre-trained vectors by incorporating the respective domain-specific knowledge, and not go into completely different states?</p>

<p>Tried to find some research papers which discuss this problem, but couldn't find any.</p>
",Vectorization & Embeddings,compare word vector different model using transfer learning want train two word vec glove model different corpus compare vector single word know make sense different model start different random state use pre trained word vector starting point assume two model continue build upon pre trained vector incorporating respective domain specific knowledge go completely different state tried find research paper discus problem find
Wordembeddings for local languages,"<p>Word embeddings visualization for local languages like Telugu 
While developing word embeddings for local languages we give input file in bytes under utf-8 encoding scheme. Then how to decode and visualize the proper words in [vectorspace] local languages using tsne?In my tsne plot how to get words in telugu?</p>

<p><a href=""https://i.sstatic.net/DNYF6.png"" rel=""nofollow noreferrer"">images show encoded bytes to utf-8 words</a></p>
",Vectorization & Embeddings,wordembeddings local language word embeddings visualization local language like telugu developing word embeddings local language give input file byte utf encoding scheme decode visualize proper word vectorspace local language using tsne tsne plot get word telugu image show encoded byte utf word
How to combine vectors generated by PV-DM and PV-DBOW methods of doc2vec?,"<p>I have around 20k documents with 60 - 150 words. Out of these 20K documents, there are 400 documents for which the similar document are known. These 400 documents serve as my test data.</p>

<p>I am trying to find similar documents for these 400 datasets using gensim doc2vec. The paper ""Distributed Representations of Sentences and Documents"" says that ""The combination of PV-DM and PV-DBOW often work consistently better (7.42% in IMDB) and therefore recommended.""</p>

<p>So I would like to combine the vectors of these two methods and find cosine similarity with all the train documents and select the top 5 with the least cosine distance.</p>

<p>So what's the effective method to combine the vectors of these 2 methods: adding or averaging or any other method ???</p>

<p>After combining these 2 vectors I can normalise each vector and then find the cosine distance.</p>
",Vectorization & Embeddings,combine vector generated pv dm pv dbow method doc vec around k document word k document document similar document known document serve test data trying find similar document datasets using gensim doc vec paper distributed representation sentence document say combination pv dm pv dbow often work consistently better imdb therefore recommended would like combine vector two method find cosine similarity train document select top least cosine distance effective method combine vector method adding averaging method combining vector normalise vector find cosine distance
How to fine tune universal sentence encoder 3 embeddings to own corpus,"<p>I would like the fine tune the embeddings produced by googles universal sentence encoder large 3(<a href=""https://tfhub.dev/google/universal-sentence-encoder-large/3"" rel=""nofollow noreferrer"">https://tfhub.dev/google/universal-sentence-encoder-large/3</a>) to my own corpus.  Any suggestions on how to do this would be greatly appreciated. My current idea is to feed sentence pairs from my corpus to the encoder and then use an extra layer to classify if they are the same semantically.  My trouble is that I am not sure how to set this up as this requires setting up two USE models that share weights, I believe it is called a siamese network.  Any help on how to do this would be greatly appreciated</p>

<pre><code>def train_and_evaluate_with_module(hub_module, train_module=False):
    embedded_text_feature_column1 = hub.text_embedding_column(
      key=""sentence1"", module_spec=hub_module, trainable=train_module)

    embedded_text_feature_column2 = hub.text_embedding_column(
      key=""sentence2"", module_spec=hub_module, trainable=train_module)


    estimator = tf.estimator.DNNClassifier(
      hidden_units=[500, 100],
      feature_columns=[embedded_text_feature_column1,embedded_text_feature_column2],
      n_classes=2,
      optimizer=tf.train.AdagradOptimizer(learning_rate=0.003))

    estimator.train(input_fn=train_input_fn, steps=1000)

    train_eval_result = estimator.evaluate(input_fn=predict_train_input_fn)
    test_eval_result = estimator.evaluate(input_fn=predict_test_input_fn)

    training_set_accuracy = train_eval_result[""accuracy""]
    test_set_accuracy = test_eval_result[""accuracy""]

    return {
      ""Training accuracy"": training_set_accuracy,
      ""Test accuracy"": test_set_accuracy
    }
</code></pre>
",Vectorization & Embeddings,fine tune universal sentence encoder embeddings corpus would like fine tune embeddings produced google universal sentence encoder large corpus suggestion would greatly appreciated current idea feed sentence pair corpus encoder use extra layer classify semantically trouble sure set requires setting two use model share weight believe called siamese network help would greatly appreciated
Improve accuracy of Multi-Label Text Classification,"<p>I'm trying to do a multi-label text classification problem where i am stuck at  a certain accuracy score and not able to improve the accuracy.</p>

<p>So in multi-label classification, I have 35 Categories(Classes) and 192 Sub-Categories(Classes), till now i've tried:</p>

<ol>
<li><p>Preprocessing: replaced contractions, lowered strings, removed punctuations, lemmatized the data, created some custom regex based on data, ran spell checker and improved the spelling mistakes.</p></li>
<li><p>TfidfVectorization: I have also tried various combinations of values in tfidf matrix such as having bigrams and trigrams increases my accuracy a little bit.</p></li>
<li><p>Tried many Classifiers (LinearSVC performing best till now) such as RandomForest, Logistic Regression, LinearSVC, XtraTreeClassifier, MultinomialNB, XGBoost and many more.. </p></li>
<li><p>Also applied GridSearchCV and RandomizedSearchCV on almost all the classifiers.</p></li>
</ol>

<p>Now from running a base LinearSVC classifier on the actual data without any customized preprocessing i have came from 45% exact match accuracy to 57% exact match accuracy.</p>

<p>I'm kinda stuck here and not being able to think of any solution to improve the accuracy. My target is to achieve 70% Exact Match accuracy  </p>

<p>My vectorizer and parameters looks like this:</p>

<pre class=""lang-py prettyprint-override""><code>classifier = Pipeline([
    ('tfidf', TfidfVectorizer(analyzer='word',ngram_range=(1, 3),sublinear_tf =True,max_features=100000)),
    ('clf', OneVsRestClassifier(LinearSVC(dual=False,multi_class='crammer_singer', max_iter = 1000)))]) ## Creating a pipeline to chain &amp; perform feature engineering and Modelling
</code></pre>

<p>I have achieved 57% Exact match accuracy, However my target is to achieve 70% 
Would be helpful to know how can i increase the accuracy more efficiently.</p>
",Vectorization & Embeddings,improve accuracy multi label text classification trying multi label text classification problem stuck certain accuracy score able improve accuracy multi label classification category class sub category class till tried preprocessing replaced contraction lowered string removed punctuation lemmatized data created custom regex based data ran spell checker improved spelling mistake tfidfvectorization also tried various combination value tfidf matrix bigram trigram increase accuracy little bit tried many classifier linearsvc performing best till randomforest logistic regression linearsvc xtratreeclassifier multinomialnb xgboost many also applied gridsearchcv randomizedsearchcv almost classifier running base linearsvc classifier actual data without customized preprocessing came exact match accuracy exact match accuracy kinda stuck able think solution improve accuracy target achieve exact match accuracy vectorizer parameter look like achieved exact match accuracy however target achieve would helpful know increase accuracy efficiently
How to improve the accuracy on google analogy task while training word2vec models?,"<p>I have implemented a word2vec model(skipgram+negative sampling) only using Numpy, and I trained it on 300M English copera, and the model achieved an mediocre wordsim353 score about 0.5, but the accuracy on google analogy task is very bad, smaller that 0.1,how can I improve that?</p>

<p>As I observed, with more and more epochs, the wordsim353 score will improve abount 0.05 to 0.1 for each epoch, but the google analogy score only improves abount 0.01, I think it's weired.....</p>

<p>Is the embedding initialization presented in the paper matter? I initialized the embedding matrix by filling the matrix with the data sampled from a gassian(mean 0,variance 1) distribution.... </p>

<p>The codes are written with Numpy.</p>

<pre><code>model=[np.random.rand(vocab_size,embed_dim),np.random.rand(vocab_size,embed_dim)]
</code></pre>

<pre class=""lang-py prettyprint-override""><code>def eval_analogy(inVecs,ques):
    analogy_a = ques[:,0] #(N,)
    analogy_b = ques[:,1]
    analogy_c = ques[:,2]
    analogy_d = ques[:,3]   
    nemb = normalize(inVecs,axis=1,norm='l2') 

    a_emb=nemb[analogy_a]  #(N,emb_dim)
    b_emb=nemb[analogy_b]
    c_emb=nemb[analogy_c]

    target=c_emb + (b_emb - a_emb) #(N,emb_dim)

    dist=np.matmul(target,nemb.T) #(N,vocab_size)
    pred_idx=np.argmax(dist,axis=1)  #(N,1)
    acc = (analogy_d == pred_idx).mean() 
    return acc

filepath='questions-words.txt'
token_to_idx=readfile('token_to_idx')
ques=read_analogies(filepath,token_to_idx)
print('input Vectors analogy:',eval_analogy(model[0],ques))
print('output Vectors analogy:',eval_analogy(model[1],ques))
</code></pre>

<p>I expected a high score but the actual output is <code>0.0864  0.0758</code></p>
",Vectorization & Embeddings,improve accuracy google analogy task training word vec model implemented word vec model skipgram negative sampling using numpy trained english copera model achieved wordsim score accuracy google analogy task bad smaller improve observed epoch wordsim score improve abount epoch google analogy score improves abount think weired embedding initialization presented paper matter initialized embedding matrix filling matrix data sampled gassian mean variance distribution code written numpy expected high score actual output
Working on google collab with python : garbage collector is not working?,"<p>I am working on google collab using python and I have a 12Gb Ram.
I am trying to use word2vec pre-trained by google to represent sentences by vectors. 
I should have same length vectors even if they do not have the same number of words so I used padding (the maximum length of a sentence here is my variable max)
The problem is that every time I want to create a matrix containing all of my vectors i run out of RAM memory quickly (on 20k th / 128k vector)</p>

<p>This is my code :</p>

<pre><code>final_x_train = []
l=np.zeros((max,300)) # The legnth of a google pretained model is 300 
for i in new_X_train: 
    buildWordVector(final_x_train, i, model, l)
    gc.collect() #doesn't do anything except slowing the run time


def buildWordVector(new_X, sent, model, l):    
    for x in range(len(sent)):
        try:
            l[x]= list(model[sent[x]])
            gc.collect() #doesn't do anything except slowing the run time
    except KeyError:
        continue
    new_X.append([list(x) for x in l])
</code></pre>

<p>all the variable that i have :</p>

<pre><code>     df:  16.8MiB
     new_X_train: 1019.1KiB
     X_train: 975.5KiB
     y_train: 975.5KiB
     new_X_test: 247.7KiB
     X_test: 243.9KiB
     y_test: 243.9KiB
     l: 124.3KiB
     final_x_train:  76.0KiB
     stop_words:   8.2KiB
</code></pre>

<p>But I am at 12Gb/12Gb (RAM) and the session has expired</p>

<p>As you can see the garbage collector is not doing anything because apperently is cannot see the variables but I really need a solution to solve this problem can anyone help me please?</p>
",Vectorization & Embeddings,working google collab python garbage collector working working google collab using python gb ram trying use word vec pre trained google represent sentence vector length vector even number word used padding maximum length sentence variable max problem every time want create matrix containing vector run ram memory quickly k th k vector code variable gb gb ram session ha expired see garbage collector anything apperently see variable really need solution solve problem anyone help please
NLP: Why use two vectorizers (Bag of Words/TFIDF) in sklearn Pipeline?,"<p>I am trying to solve a text classification problem using <code>SVC</code> on <code>sklearn</code>. I also wanted to check which vectorizer would work best for my data: Bag of Words <code>CountVectorizer()</code> or TF-IDF <code>TfidfVectorizer()</code></p>

<p>What I've been doing so far is using these two vectorizers separately, one after the other, then comparing their results.</p>

<pre class=""lang-py prettyprint-override""><code># Bag of Words (BoW)
from sklearn.feature_extraction.text import CountVectorizer
count_vectorizer = CountVectorizer()
features_train_cv = count_vectorizer.fit_transform(features_train)

# TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vec = TfidfVectorizer()
features_train_tfidf = tfidf_vec.fit_transform(features_train)

# Instantiate SVC
classifier_linear =  SVC(random_state=1, class_weight='balanced', kernel = ""linear"", C=1000)

# Fit SVC with BoW features
classifier_linear.fit(features_train_cv,target_train)
features_test_cv = count_vectorizer.transform(features_test)
target_test_pred_cv = classifier_linear.predict(features_test_cv)


# Confusion matrix: SVC with BoW features
from sklearn.metrics import confusion_matrix
print(confusion_matrix(target_test, target_test_pred_cv))
[[ 689  517]
 [ 697 4890]]

# Fit SVC with TF-IDF features
classifier_linear.fit(features_train_tfidf,target_train)
features_test_tfidf = tfidf_vec.transform(features_test)
target_test_pred_tfidf = classifier_linear.predict(features_test_tfidf)

# Confusion matrix: SVC with TF-IDF features

[[ 701  505]
 [ 673 4914]]

</code></pre>

<p>I thought that maybe using <code>Pipeline</code> would make my code look more organized. But I noticed that in the suggested <code>Pipeline</code> code in <a href=""https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow noreferrer"">sklearn tutorial from the module official page</a> includes two vectorizers: <em>both</em> <code>CountVectorizer()</code> (Bag of Words) <em>and</em> <code>TfidfVectorizer()</code></p>

<pre class=""lang-py prettyprint-override""><code># from sklearn official tutorial
from sklearn.pipeline import Pipeline
&gt;&gt;&gt; text_clf = Pipeline([
...     ('vect', CountVectorizer()),
...     ('tfidf', TfidfTransformer()),
...     ('clf', MultinomialNB()),
</code></pre>

<p>My impression was, you only need to do choose one vectorizer for your features. Wouldn't that mean that the data gets vectorized <em>twice</em>, once with simple term frequency then next with TF-IDF?</p>

<p>How would this code work?</p>
",Vectorization & Embeddings,nlp use two vectorizers bag word tfidf sklearn pipeline trying solve text classification problem using also wanted check vectorizer would work best data bag word tf idf far using two vectorizers separately one comparing result thought maybe using would make code look organized noticed suggested code sklearn tutorial module official page includes two vectorizers bag word impression wa need choose one vectorizer feature mean data get vectorized twice simple term frequency next tf idf would code work
Check if a substring of a string is in a list of strings in python,"<p>I have a dictionary of foods: </p>

<pre><code>foods={
  ""chicken masala"" : ""curry"",
  ""chicken burger"" : ""burger"",
  ""beef burger"" : ""burger"",
  ""chicken soup"" : ""appetizer"",
  ""vegetable"" : ""curry""
}
</code></pre>

<p>Now I have a list of strings: </p>

<pre><code>queries = [""best burger"", ""something else""]
</code></pre>

<p>I have to find out if there is any string in <code>queries</code> that has and entry in our <code>food</code> dictionary. 
Like in the above example it should return True for <code>best burger</code>. 
Currently, I am calculating cosine similarity between each string in the list for all the entries in the <code>foods.keys()</code>. 
It works but it's very time inefficient. The <code>food</code> dictionary has almost 1000 entries. Is there any efficient way to do so?  </p>

<p><strong>Edit:</strong></p>

<p>Here the best burger should be returned because there is <code>burger</code> in it and <code>burger</code> is also present in <code>chicken burger</code> in <code>foods.keys()</code>. I am basically trying to find out if there is any query which is a food type.</p>

<p>This is how I am calculating :</p>

<pre><code>import re, math
from collections import Counter

WORD = re.compile(r'\w+')

def get_cosine(text1, text2):
     vec1 = text_to_vector(text1.lower())
     vec2 = text_to_vector(text2.lower())
     intersection = set(vec1.keys()) &amp; set(vec2.keys())
     numerator = sum([vec1[x] * vec2[x] for x in intersection])

     sum1 = sum([vec1[x]**2 for x in vec1.keys()])
     sum2 = sum([vec2[x]**2 for x in vec2.keys()])
     denominator = math.sqrt(sum1) * math.sqrt(sum2)

     if not denominator:
        return 0.0
     else:
        return (float(numerator) / denominator) * 100

foods={
  ""chicken masala"" : ""curry"",
  ""chicken burger"" : ""burger"",
  ""beef burger"" : ""burger"",
  ""chicken soup"" : ""appetizer"",
  ""vegetable"" : ""curry""
}
queries = [""best burger"", ""something else""]
flag = False
food = []
for phrase in queries:
   for k in foods.keys():
      cosine = get_cosine(phrase, k)
      if int(cosine) &gt; 40:
         flag = True
         food.append(phrase)
         break

print('Foods:', food)

</code></pre>

<p>OUTPUT:</p>

<pre><code>Foods: ['best burger']
</code></pre>

<p>Solution:
Though @Black Thunder's solution works for the example I have provided in the example but it doesn't work for queries like <code>best burgers</code>. But this solution works in that case. Which is a major concern for me. Thanks @Andrej Kesely. This was the reason I went for the cosine similarity in my solution. But i think SequenceMatcher works better here. </p>
",Vectorization & Embeddings,check substring string list string python dictionary food list string find string ha entry dictionary like example return true currently calculating cosine similarity string list entry work time inefficient dictionary ha almost entry efficient way edit best burger returned also present basically trying find query food type calculating output solution though black thunder solution work example provided example work query like solution work case major concern thanks andrej kesely wa reason went cosine similarity solution think sequencematcher work better
gensim word2vec entry greater than 1,"<p>I'm new to NLP and gensim, currently trying to solve some NLP problems with gensim word2vec module. I my current understanding of word2vec, the result vectors/matrix should have all entries between -1 and 1. However, trying a simple one results into a vector which has entries greater than 1. I'm not sure which part is wrong, could anyone give some suggestions, please?</p>

<p>I've used gensim utils.simple_preprocess to generate a list of list of token. The list looks like: </p>

<pre><code>[['buffer', 'overflow', 'in', 'client', 'mysql', 'cc', 'in', 'oracle', 'mysql', 'and', 'mariadb', 'before', 'allows', 'remote', 'database', 'servers', 'to', 'cause', 'denial', 'of', 'service', 'crash', 'and', 'possibly', 'execute', 'arbitrary', 'code', 'via', 'long', 'server', 'version', 'string'], ['the', 'xslt', 'component', 'in', 'apache', 'camel', 'before', 'and', 'before', 'allows', 'remote', 'attackers', 'to', 'read', 'arbitrary', 'files', 'and', 'possibly', 'have', 'other', 'unspecified', 'impact', 'via', 'an', 'xml', 'document', 'containing', 'an', 'external', 'entity', 'declaration', 'in', 'conjunction', 'with', 'an', 'entity', 'reference', 'related', 'to', 'an', 'xml', 'external', 'entity', 'xxe', 'issue']]
</code></pre>

<p>I believe this is the correct input format for gensim word2vec.</p>

<pre><code>word2vec = models.word2vec.Word2Vec(sentences, size=50, window=5, min_count=1, workers=3, sg=1)
vector = word2vec['overflow']
print(vector)
</code></pre>

<p>I expect the output to be a vector containing probabilities (i.e., all between -1 and 1), but it actually turned out to be the following:</p>

<pre><code>[ 0.12800379 -0.7405527  -0.85575     0.25480416 -0.2535793   0.142656
 -0.6361196  -0.13117172  1.1251501   0.5350017   0.05962601 -0.58876884
  0.02858278  0.46106443 -0.22623934  1.6473309   0.5096218  -0.06609935
 -0.70007527  1.0663376  -0.5668168   0.96070313 -1.180383   -0.58649933
 -0.09380565 -0.22683378  0.71361005  0.01779896  0.19778453  0.74370056
 -0.62354785  0.11807996 -0.54997736  0.10106519  0.23364201 -0.11299669
 -0.28960565 -0.54400533  0.10737313  0.3354464  -0.5992898   0.57183135
 -0.67273194  0.6867607   0.2173506   0.15364875  0.7696457  -0.24330224
  0.46414775  0.98163396]
</code></pre>

<p>You can see there are <code>1.6473309</code> and <code>-1.180383</code> in the above vector.</p>
",Vectorization & Embeddings,gensim word vec entry greater new nlp gensim currently trying solve nlp problem gensim word vec module current understanding word vec result vector matrix entry however trying simple one result vector ha entry greater sure part wrong could anyone give suggestion please used gensim utils simple preprocess generate list list token list look like believe correct input format gensim word vec expect output vector containing probability e actually turned following see vector
I want to know how can we give a categorical variable as an input to an embedding layer in keras and train that embedding layer?,"<p>let's say we have a data frame where we have a categorical column which has 7 categories - Monday, Tuesday, Wednesday, Thursday, Friday, Saturday and Sunday. Let's say we have 100 data points and we want to give the categorical data as an input to the embedding layer and train the embedding layer using Keras. How do we actually achieve it? Can you share some intuition with code examples? </p>

<p>I have tried this code but it gives me an error which says ""ValueError: ""input_length"" is 1, but received input has shape (None, 26)"". I have referred to this blog <a href=""https://medium.com/@satnalikamayank12/on-learning-embeddings-for-categorical-data-using-keras-165ff2773fc9"" rel=""nofollow noreferrer"">https://medium.com/@satnalikamayank12/on-learning-embeddings-for-categorical-data-using-keras-165ff2773fc9</a>, but I didn't get how to use it for my particular case.</p>

<pre><code>from sklearn.preprocessing import LabelEncoder
l_encoder=LabelEncoder()
l_encoder.fit(X_train[""Weekdays""])

encoded_weekdays_train=l_encoder.transform(X_train[""Weekdays""])
encoded_weekdays_test=l_encoder.transform(X_test[""Weekdays""])

no_of_unique_cat=len(X_train.school_state.unique())
embedding_size = min(np.ceil((no_of_unique_cat)/2),50)
embedding_size = int(embedding_size)
vocab  = no_of_unique_cat+1

#Get the flattened LSTM output for categorical text
input_layer2 = Input(shape=(embedding_size,))
embedding = Embedding(input_dim=vocab, output_dim=embedding_size, input_length=1, trainable=True)(input_layer2)
flatten_school_state = Flatten()(embedding)
</code></pre>

<p>I want to know in case of 7 categories, what will be the shape of input_layer2? What should be the vocab size, output dim and input_length? Can anyone explain, or correct my code? Your insights will be really helpful.</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-46-e28d41acae85&gt; in &lt;module&gt;
      1 #Get the flattened LSTM output for input text
      2 input_layer2 = Input(shape=(embedding_size,))
----&gt; 3 embedding = Embedding(input_dim=vocab, output_dim=embedding_size, input_length=1, trainable=True)(input_layer2)
      4 flatten_school_state = Flatten()(embedding)

~/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)
    472             if all([s is not None
    473                     for s in to_list(input_shape)]):
--&gt; 474                 output_shape = self.compute_output_shape(input_shape)
    475             else:
    476                 if isinstance(input_shape, list):

~/anaconda3/lib/python3.7/site-packages/keras/layers/embeddings.py in compute_output_shape(self, input_shape)
    131                         raise ValueError(
    132                             '""input_length"" is %s, but received input has shape %s' %
--&gt; 133                             (str(self.input_length), str(input_shape)))
    134                     elif s1 is None:
    135                         in_lens[i] = s2

ValueError: ""input_length"" is 1, but received input has shape (None, 26)


</code></pre>
",Vectorization & Embeddings,want know give categorical variable input embedding layer kera train embedding layer let say data frame categorical column ha category monday tuesday wednesday thursday friday saturday sunday let say data point want give categorical data input embedding layer train embedding layer using kera actually achieve share intuition code example tried code give error say valueerror input length received input ha shape none referred blog get use particular case want know case category shape input layer vocab size output dim input length anyone explain correct code insight really helpful
Python gensim create word2vec model from vectors (in ndarray),"<p>I have a ndarray with words and their corresponding vector (with the size of 100 per word).
For example:</p>

<pre><code>Computer 0.11 0.41 ... 0.56
Ball     0.31 0.87 ... 0.32
</code></pre>

<p>And so on.</p>

<p>I want to create a word2vec model from it:</p>

<pre><code>model = load_from_ndarray(arr)
</code></pre>

<p>How can it be done? I saw </p>

<blockquote>
  <p>KeyedVectors</p>
</blockquote>

<p>but it only takes file and not array</p>
",Vectorization & Embeddings,python gensim create word vec model vector ndarray ndarray word corresponding vector size per word example want create word vec model done saw keyedvectors take file array
How can I see TF-IDF values from tfidf_vectorizer?,"<p>I am using Python</p>

<p>I have this code that analyse Text documents</p>

<pre><code>tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)


# split dataset into training and validation set
xtrain, xval, ytrain, yval = train_test_split(movies_new['clean_plot'], y, test_size=0.2, random_state=9)


# create TF-IDF features
xtrain_tfidf = tfidf_vectorizer.fit_transform(xtrain)
xval_tfidf = tfidf_vectorizer.transform(xval)
</code></pre>

<p>I know that TF-IDF assigns a value to each word.</p>

<p>Is there a way that let me see what are the values of inside <code>xtrain_tfidf</code> ?</p>
",Vectorization & Embeddings,see tf idf value tfidf vectorizer using python code analyse text document know tf idf assigns value word way let see value inside
Creating Self Learning Sentiment Dictionary,"<p>I am planning to <strong>build a self-learning dictionary</strong> of sentiment word with their sentiment label.</p>

<p>I am able to identify the sentiment words by using POS tags but not able to label those words as positive, negative or neutral.</p>

<p>For example: ""The food was not good"" is the sentence, and I have extracted ""not good"" from the sentence as sentiment word by using the POS tag. Now I want to label this as negative and add it to my new dictionary for future use.</p>

<p>my preference to do this project/task is by not using any pre-defined dictionary/word bank/any pre-defined sentiment analysis package.</p>

<p><strong>I am seeking your views to know the way to label it without using any pre-defined dictionary or with pre-defined dictionary.</strong></p>

<p>Currently, I have explored Word embedding, Skip through n-gram model for this. I have also used a pre-defined dictionary to train the model by using some supervised learning model like Xgboost, KNN,  Naive Bayes classifier. I have used some unsupervised model like k-mean to predict the label by using the words. 
Still not able to get the results.</p>

<p>If You know any other way or some input to apply with any of above-used models to label word as positive, negative or neutral then please suggest.</p>
",Vectorization & Embeddings,creating self learning sentiment dictionary planning build self learning dictionary sentiment word sentiment label able identify sentiment word using po tag able label word positive negative neutral example food wa good sentence extracted good sentence sentiment word using po tag want label negative add new dictionary future use preference project task using pre defined dictionary word bank pre defined sentiment analysis package seeking view know way label without using pre defined dictionary pre defined dictionary currently explored word embedding skip n gram model also used pre defined dictionary train model using supervised learning model like xgboost knn naive bayes classifier used unsupervised model like k mean predict label using word still able get result know way input apply used model label word positive negative neutral please suggest
Is there any method in fasttext that takes two words as inputs and returns their similarity,"<p>I have followed <a href=""https://fasttext.cc/docs/en/unsupervised-tutorial.html"" rel=""nofollow noreferrer"">this</a> documentation to get fasttext running, fairly straightforward. Commands such as  <code>$ ./fasttext nn result/fil9.bin</code> can be used to find nearest neighbour of a word. However, is there any such simple command in fasttext that takes two words such as ""Vehicle"" and ""Car"" as input and returns their (cosine) similarity i.e. a number like 0.777.  </p>
",Vectorization & Embeddings,method fasttext take two word input return similarity followed documentation get fasttext running fairly straightforward command used find nearest neighbour word however simple command fasttext take two word vehicle car input return cosine similarity e number like
How to create one hot vectors to find similarity of texts after gensim doc2bow implementation?,"<p>I have two lists (say  list A and list B) of texts. I want to find most similar texts in list B for each text in list A.
This I want to do using bag of words and later cosine similarity.</p>

<p>I created a dictionary using gensim</p>

<pre><code>tokensA = [preprocess_string(''.join(doc),CUSTOM_FILTERS) for doc in 
listA]
tokensB = [preprocess_string(''.join(doc),CUSTOM_FILTERS) for doc in 
listB]
# Create dictionary
dictionary = corpora.Dictionary(tokensA)
</code></pre>

<p>Then I obtained the occurences of words for each text in list B,</p>

<pre><code>mycorpus = [dictionary.doc2bow(doc, allow_update=True) for doc in tokensB]

[[(9, 1), (25, 1), (611, 1), (627, 1), (1917, 1), (1918, 1)],
[(9, 1), (25, 1), (627, 1), (1918, 1), (1919, 1), (1920, 1)],...
</code></pre>

<p>How can I convert this to vectors so that I can do a cosine_similarity?</p>
",Vectorization & Embeddings,create one hot vector find similarity text gensim doc bow implementation two list say list list b text want find similar text list b text list want using bag word later cosine similarity created dictionary using gensim obtained occurences word text list b convert vector cosine similarity
Where should I pass pre trained word embedding in a encoder-decoder architecture?,"<p>I have  pre-trained word embeddings from two different languages using MUSE. Now suppose I have a encoder-decoder architecture. And I created a embedding layer from one of this embedding. But where do I pass it in the model?</p>

<p>The model is trying to translate from one language to another. I have created a embedding_layer. Where do I pass it in in the below code?</p>

<pre><code>"""""" 
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

# Run training
model.compile(optimizer='rmsprop', loss='categorical_crossentropy') 
model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
          batch_size=batch_size,
          epochs=epochs,
          validation_split=0.2)
""""""
</code></pre>
",Vectorization & Embeddings,pas pre trained word embedding encoder decoder architecture pre trained word embeddings two different language using muse suppose encoder decoder architecture created embedding layer one embedding pas model model trying translate one language another created embedding layer pas code
"After training word embedding with gensim&#39;s fasttext&#39;s wrapper, how to embed new sentences?","<p>After reading the tutorial at gensim's <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb"" rel=""nofollow noreferrer"">docs</a>, I do not understand what is the correct way of generating new embeddings from a trained model. So far I have trained gensim's fast text embeddings like this:</p>

<pre><code>from gensim.models.fasttext import FastText as FT_gensim

model_gensim = FT_gensim(size=100)

# build the vocabulary
model_gensim.build_vocab(corpus_file=corpus_file)

# train the model
model_gensim.train(
    corpus_file=corpus_file, epochs=model_gensim.epochs,
    total_examples=model_gensim.corpus_count, total_words=model_gensim.corpus_total_words
)
</code></pre>

<p>Then, let's say I want to get the embeddings vectors associated with this sentences:</p>

<pre><code>sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()
sentence_president = 'The president greets the press in Chicago'.lower().split()
</code></pre>

<p>How can I get them with <code>model_gensim</code> that I trained previously?</p>
",Vectorization & Embeddings,training word embedding gensim fasttext wrapper embed new sentence reading tutorial gensim doc understand correct way generating new embeddings trained model far trained gensim fast text embeddings like let say want get embeddings vector associated sentence get trained previously
"After creating an embedding layer using a tensorflow placeholder, how is the tf.nn.embedding_lookup() function used with it?","<p>I'm trying to add a pre-trained word2vec word embedding to my tensorflow code. Now after creating the embedding matrix, one way to add this layer is by creating a tensorflow variable, but this leads to repetition of calculation and hence, not efficient. 
The other way is to create a placeholder and pass data through that. </p>

<pre><code>W = tf.Variable(tf.constant(0.0, shape=[vsize, 200]),
                trainable=False, name=""W"")
embedding_init = W.assign(self._embedding)
#self._embedding is the placeholder

sess = tf.Session()
sess.run(embedding_init,feed_dict={self._embedding:emb_matrix})
</code></pre>

<p>Now I want to be able to pass data to that embedding by calling the tf.nn.embedding_lookup() function, what tensor exactly do I pass into the lookup function?</p>

<pre><code>tf.nn.embedding_lookup(?,list(word2id),input_data)

</code></pre>
",Vectorization & Embeddings,creating embedding layer using tensorflow placeholder tf nn embedding lookup function used trying add pre trained word vec word embedding tensorflow code creating embedding matrix one way add layer creating tensorflow variable lead repetition calculation hence efficient way create placeholder pas data want able pas data embedding calling tf nn embedding lookup function tensor exactly pas lookup function
"Python, Keras - Binary text classifier prediction results in array of values instead of single probability","<p>I am building a very simple DNN binary model which I define as:</p>

<pre><code>def __build_model(self, vocabulary_size):
    model = Sequential()
    model.add(Embedding(vocabulary_size, 12, input_length=vocabulary_size))
    model.add(Flatten())
    model.add(Dense(16, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
    return model
</code></pre>

<p>with training like:</p>

<pre><code>def __train_model(self, model, model_data, training_data, labels):
    hist = model.fit(training_data, labels, epochs=20, verbose=True, validation_split=0.2)

    model.save('models/' + model_data['Key'] + '.h5')

    return model
</code></pre>

<p>The idea is to feed tfidf vectorized text after training and predict whenever it belongs to class 1 or 0. Sadly when I run predict against it, I get an array of predictions instead of expected 1 probability for the article belonging to class 1. The array values seem very uniform. I assume this comes from some mistake in the model. I try popping prediction like so:</p>

<pre><code>            self._tokenizer.fit_on_texts(asset_article_data.content)

            predicted_post_vector = self._tokenizer.texts_to_matrix(post, mode='tfidf')

            return model.predict(predicted_post_vector) &gt; 0.60 // here return array instead of true/false
</code></pre>

<p>The training data is vectorized text itself. What might be off?</p>
",Vectorization & Embeddings,python kera binary text classifier prediction result array value instead single probability building simple dnn binary model define training like idea feed tfidf vectorized text training predict whenever belongs class sadly run predict get array prediction instead expected probability article belonging class array value seem uniform assume come mistake model try popping prediction like training data vectorized text might
Using pre-trained sentence embedding on recurrent networks,"<p>I want to use <a href=""https://arxiv.org/abs/1803.11175"" rel=""nofollow noreferrer"">Universal Sentence Embedding</a> on recurrent networks.</p>
<p>So a traditional word embedding with RNN encodes each word into a vector and the RNN's time_step would be the number of words in a sentence.</p>
<p>What I want to do is to use the sentence embedding to encode each sentence into a 512-dim vector. The RNN's time_step would be the number of sentences in a text, or IMDB review in my case.</p>
<p>I am trying this on IMDB binary classification. The problem is that no matter how I tune the hyperparamters, the model will not learn. The training and test accuracies stay at 50%, which means that the model predicts only 1 out of the 2 classes.</p>
<p>I would appreciate any help!</p>
<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_1 (LSTM)                (None, 128)               131584
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 258
=================================================================
Total params: 131,842
Trainable params: 131,842
Non-trainable params: 0
_________________________________________________________________
WARNING:tensorflow:From C:\Users\shaggyday\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
W0709 14:26:44.883890  9716 deprecation.py:323] From C:\Users\shaggyday\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Epoch 1/10
249/249 [==============================] - 55s 220ms/step - loss: 0.6937 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5061
Epoch 2/10
249/249 [==============================] - 68s 274ms/step - loss: 0.6970 - acc: 0.5002 - val_loss: 0.6942 - val_acc: 0.5009
Epoch 3/10
249/249 [==============================] - 71s 285ms/step - loss: 0.6947 - acc: 0.4961 - val_loss: 0.6980 - val_acc: 0.5009
Epoch 4/10
249/249 [==============================] - 70s 279ms/step - loss: 0.6938 - acc: 0.4998 - val_loss: 0.6956 - val_acc: 0.5033
Epoch 5/10
249/249 [==============================] - 66s 267ms/step - loss: 0.6936 - acc: 0.5018 - val_loss: 0.6939 - val_acc: 0.5046
Epoch 6/10
249/249 [==============================] - 63s 251ms/step - loss: 0.6931 - acc: 0.5003 - val_loss: 0.6933 - val_acc: 0.5058
</code></pre>
<p>The code for pre embedding the text is</p>
<pre><code>file = 'train.csv'
df = pd.read_csv(file)
# df['sentiment'] = [1 if sentiment == 'positive' else 0 for sentiment in df['sentiment'].values]
x = df['review'].values
y = df['sentiment'].values
x_sent = []
for review in x:
    x_sent.append(sent_tokenize( review ) )


num_sample = len(x)
val_split = int(num_sample*0.5)
x_train, y_train = x_sent, y
x_test, y_test = x_sent[val_split:], y[val_split:]

module_url = &quot;https://tfhub.dev/google/universal-sentence-encoder-large/2&quot; 
out_dir = 'use(dan)'
embed = hub.Module(module_url)

num_files = 10
n_file = num_sample // num_files

for n in range( num_files ):

    def batch_embed( batch, labels, lens, set_ ):
        &quot;&quot;&quot;
        batch:   1-D array of sentences
        labels:  labels for each reviews
        lens:    offsets for the reviews
        set_:    'train' | 'test'
        &quot;&quot;&quot;
        with tf.Session( config=config ) as session:
          session.run([tf.global_variables_initializer(), tf.tables_initializer()])
          
          print( 'Getting embeddings for the {} data'.format( set_ ) )
          path = os.path.join( out_dir, 'embed_{}_{}.bin'.format( set_ , n ) )
          if not os.path.exists( path ):
            embeddings = session.run( embed( batch ) )
            offset = 0
            review_embeddings = []
            for l in lens:
                review_embeddings.append( embeddings[ offset : offset + l ] )
                offset += l
            with open( path, 'wb' ) as f:
                pickle.dump( (review_embeddings, labels), f )
            
            for i, re in enumerate(embeddings):
                if re.shape[0]==0:
                    print( i, batch[i] )
    
    train_batch = x_train[ n * n_file : min( len( x_train ),  ( n + 1 ) * n_file )  ]
    labels = y_train[ n * n_file : min( len( x_train ),  ( n + 1 ) * n_file )  ]
    lens = [ len( x ) for x in train_batch ]
    sent_batch = [ sent for review in train_batch for sent in review ]
    print( len( sent_batch ) )
    batch_embed(sent_batch, labels, lens, 'train')

    test_batch = x_test[ n * n_file : min( len( x_test ),  ( n + 1 ) * n_file )  ]
    labels = y_test[ n * n_file : min( len( x_test ),  ( n + 1 ) * n_file )  ]
    lens = [ len( x ) for x in test_batch ]
    sent_batch = [ sent for review in test_batch for sent in review ]
    print( len( sent_batch ) )
    batch_embed(sent_batch, labels, lens, 'test')
</code></pre>
<p>The model is a very simple lstm with one layer and 256 neurons. Each batch is padded because each IMDB review has different number of sentences</p>
",Vectorization & Embeddings,using pre trained sentence embedding recurrent network want use universal sentence embedding recurrent network traditional word embedding rnn encodes word vector rnn time step would number word sentence want use sentence embedding encode sentence dim vector rnn time step would number sentence text imdb review case trying imdb binary classification problem matter tune hyperparamters model learn training test accuracy stay mean model predicts class would appreciate help code pre embedding text model simple lstm one layer neuron batch padded imdb review ha different number sentence
GloVe Word Vectors Cosine Similarity - &quot;Ally&quot; closer to &quot;powerful&quot; than &#39;friend&quot;,"<p>When I tested the two words ""ally"" and ""friend"" for cosine similarity (using a function verified to  be a correct implementation) in python 3.6 with GloVe word vectors, the cosine similarity was <code>
0.6274969008615137</code>. When I tested ""ally"" and ""friend"" however, the result was <code>0.4700224263147646</code>. </p>

<p>It seems that ""ally"" and ""friend"", two nouns given as synonyms, should have a larger cosine similarity than ""ally"" and ""powerful"", a noun and a barely related word. </p>

<p>Am I misunderstanding the idea of word vectors or cosine similarity?</p>
",Vectorization & Embeddings,glove word vector cosine similarity ally closer powerful friend tested two word ally friend cosine similarity using function verified correct implementation python glove word vector cosine similarity wa tested ally friend however result wa seems ally friend two noun given synonym larger cosine similarity ally powerful noun barely related word misunderstanding idea word vector cosine similarity
Is there a way to assign unique values to the names that have cosine similarity higher than 0.8?,"<p>I am able to calculate the cosine similarity between different names.
After that i want to assign unique values to those names that have similarity > 0.8</p>

<p>I have tried creating a dictionary containing names as key and assigned unique value as values</p>

<pre><code>dict1 = {}

ui = 0

def word2vec(word):
    from collections import Counter
    from math import sqrt
    cw = Counter(word)
    sw = set(cw)
    lw = sqrt(sum(c*c for c in cw.values()))
    return cw, sw, lw
def cosdis(v1, v2):
    common = v1[1].intersection(v2[1])
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]
list1 = list(df['Name'].unique())
for m in list1:
    for i in range(0, len(list1)):   
        vec1 = word2vec(m)
        vec2 = word2vec(list1[i])
        x = cosdis(vec1,vec2)
        if(x &gt;=.80):
            dict1[m, list[i]] = 'A' + str(ui)                       
        else:
            pass
        ui = ui + 1                            
</code></pre>

<p>I would like all similar names to have same value in the dictionary.</p>
",Vectorization & Embeddings,way assign unique value name cosine similarity higher able calculate cosine similarity different name want assign unique value name similarity tried creating dictionary containing name key assigned unique value value would like similar name value dictionary
How to turn featurized text back to actual text in ML.NET (for chatbot)?,"<p>I am trying to create an FAQ bot with ML.NET (cannot use QNA Maker). I want to compare the questions in the FAQ KB to an input and then return the most relevant answer. Most FAQ bots I found online worked like this: featurize the FAQ questions, featurize the input, do a cosine similarity, and then return the most relevant answer. I don't really understand Microsoft's featurization but I can't even test it because I can't find how to relate the feature vector to the original text.</p>

<p>This is what I have so far (in Main):</p>

<pre><code>mlContext = new MLContext(seed: 0);
IDataView dataview = mlContext.Data.LoadFromTextFile&lt;SampleData&gt;(""Data/training_data.tsv"", hasHeader: true);
var textPipeline = mlContext.Transforms.Text.FeaturizeText(""Features"", ""Question"");
var textTransformer = textPipeline.Fit(dataview);
var predictionEngine = mlContext.Model.CreatePredictionEngine&lt;SampleData, TransformedTextData&gt;(textTransformer);
SampleData sampleData = new SampleData()
    {
        Question = ""Setting Up Data Exchange"" //would be changed to user input
    };
var prediction = predictionEngine.Predict(sampleData);
Console.WriteLine($""Number of Features: {prediction.Features.Length}"");
Console.Write(""Features: "");
    for (int i = 0; i &lt; 1000; i++)
        Console.Write($""{prediction.Features[i]:F4}  "");
</code></pre>

<p>SampleData class:</p>

<pre><code>public class SampleData
{
        [LoadColumn(0)]
        public string Question { get; set; }

        [LoadColumn(1)]
        public string Answer { get; set; }
}

public class TransformedTextData : SampleData
{
        public float[] Features { get; set; }
}
</code></pre>

<p>It returns the feature vector but almost all of the values are zero so hopefully that's normal, but I just don't know how I can turn this into readable output. Also I don't understand why I can't just featurize and model just the FAQ text, why do I need a sample question, I feel like that's inefficient and probably I'm not going about it right. Thanks for any help!</p>
",Vectorization & Embeddings,turn featurized text back actual text ml net chatbot trying create faq bot ml net use qna maker want compare question faq kb input return relevant answer faq bot found online worked like featurize faq question featurize input cosine similarity return relevant answer really understand microsoft featurization even test find relate feature vector original text far main sampledata class return feature vector almost value zero hopefully normal know turn readable output also understand featurize model faq text need sample question feel like inefficient probably going right thanks help
How to normalize TF*IDF or counts in scikit-learn?,"<p>I want to check the cosine similarity of two documents having varying length (say one is a one or two liner while other is of 100-200 lines).</p>

<p>I need a way to normalize tfidf or count vectorizer in scikit-learn for this.</p>
",Vectorization & Embeddings,normalize tf idf count scikit learn want check cosine similarity two document varying length say one one two liner line need way normalize tfidf count vectorizer scikit learn
Pytorch - How to achieve higher accuracy with imdb review dataset using LSTM?,"<p>I'm trying to practice with LSTM and Pytorch. I took <a href=""https://www.kaggle.com/iarunava/imdb-movie-reviews-dataset/kernels"" rel=""nofollow noreferrer"">IMDB movie review dataset</a> to predict whether the review is positive or negative. I use 80% of the dataset for my training, remove punctuations, use <code>GloVe</code> (with 200 dims) as an embedding layer. </p>

<p>Before training, I also exclude too short (reviews with length smaller than 50 symbols) and too long (reviews with longer than 1000 symbols) reviews.</p>

<p>For the <code>LSTM</code> layer I use <code>hidden dimension 256</code>, <code>num_layers 2</code> and <code>one directional</code> parameters with <code>0.5 dropout</code>. Afterwards, I have fully connected layer.
For the training I used nn.BCELoss function with Adam optimizer (<code>lr=0.001</code>).</p>

<p>Currently I get 85% validation accuracy with 98% training accuracy after 7 epochs. I did following steps for preventing overfitting and getting higher accuracy:</p>

<ul>
<li>used weight_decay for Adam optimizer,</li>
<li>tried SGD (lr=0.1, 0.001) instead of Adam,</li>
<li>tried to increase num_layers of LSTM,</li>
</ul>

<p>In all of these cases model didn't learn at all, giving 50% of accuracy for both training and validation sets.</p>

<pre class=""lang-py prettyprint-override""><code>class CustomLSTM(nn.Module):
    def __init__(self, vocab_size, use_embed=False, embed=None, embedding_size=200, hidden_size=256,
                 num_lstm_layers=2, bidirectional=False, dropout=0.5, output_dims=2):
        super().__init__()

        self.vocab_size = vocab_size
        self.embedding_size = embedding_size
        self.hidden_size = hidden_size
        self.num_lstm_layers = num_lstm_layers
        self.bidirectional = bidirectional
        self.dropout = dropout

        self.embedding = nn.Embedding(vocab_size, embedding_size)
        if use_embed:
            self.embedding.weight.data.copy_(torch.from_numpy(embed))
#             self.embedding.requires_grad = False
        self.lstm = nn.LSTM(input_size=embedding_size,
                           hidden_size=hidden_size,
                           num_layers=num_lstm_layers,
                           batch_first=True,
                           dropout=dropout,
                           bidirectional=bidirectional)
#         print('output dims value ', output_dims)
        self.drop_fc = nn.Dropout(0.5)
        self.fc = nn.Linear(hidden_size, output_dims)
        self.sig = nn.Sigmoid()
</code></pre>

<p>I want to understand:</p>

<ol>
<li>Why the model doesn't learn at all with that changes applied? </li>
<li>How to increase the accuracy?</li>
</ol>
",Vectorization & Embeddings,pytorch achieve higher accuracy imdb review dataset using lstm trying practice lstm pytorch took imdb movie review dataset predict whether review positive negative use dataset training remove punctuation use dims embedding layer training also exclude short review length smaller symbol long review longer symbol review layer use parameter afterwards fully connected layer training used nn bceloss function adam optimizer currently get validation accuracy training accuracy epoch following step preventing overfitting getting higher accuracy used weight decay adam optimizer tried sgd lr instead adam tried increase num layer lstm case model learn giving accuracy training validation set want understand model learn change applied increase accuracy
How and why we use CNN layer wrapped with time distributed layer?,"<p>I need to know how this code works.  It's taking Embedding then it sends it into this model. model1 is CNN and moel2 is Time distributed layer. Why wrapping is done in this code, i didn't find article on this. </p>

<pre class=""lang-py prettyprint-override""><code>model1 = Sequential()
model1.add(Embedding(nb_words + 1,
                     embedding_dim,
                     weights = [word_embedding_matrix],
                     input_length = max_sentence_len,
                     trainable = False))

model1.add(Convolution1D(filters = nb_filter, 
                         kernel_size = filter_length, 
                         padding = 'same'))
model1.add(BatchNormalization())
model1.add(Activation('relu'))
model1.add(Dropout(dropout))

model1.add(Convolution1D(filters = nb_filter, 
                         kernel_size = filter_length, 
                         padding = 'same'))
model1.add(BatchNormalization())
model1.add(Activation('relu'))
model1.add(Dropout(dropout))

model1.add(Flatten())



model2 = Sequential()
model2.add(Embedding(nb_words + 1,
                     embedding_dim,
                     weights = [word_embedding_matrix],
                     input_length = max_sentence_len,
                     trainable = False))

model2.add(Convolution1D(filters = nb_filter, 
                         kernel_size = filter_length, 
                         padding = 'same'))
model2.add(BatchNormalization())
model2.add(Activation('relu'))
model2.add(Dropout(dropout))

model2.add(Convolution1D(filters = nb_filter, 
                         kernel_size = filter_length, 
                         padding = 'same'))
model2.add(BatchNormalization())
model2.add(Activation('relu'))
model2.add(Dropout(dropout))

model2.add(Flatten())



</code></pre>

<p>then  it merges and getting the output. I don't understand the computation behind this.</p>
",Vectorization & Embeddings,use cnn layer wrapped time distributed layer need know code work taking embedding sends model model cnn moel time distributed layer wrapping done code find article merges getting output understand computation behind
NLP Structure Question (best way for doing feature extraction),"<p>I am building an NLP pipeline and I am trying to get my head around in regards to the optimal structure. My understanding at the moment is the following: </p>

<ul>
<li>Step1 - Text Pre-processing [a. Lowercasing, b. Stopwords removal, c. stemming, d. lemmatisation,] </li>
<li>Step 2 - Feature extraction </li>
<li>Step 3 - Classification - using the different types of classifier(linearSvC etc) </li>
</ul>

<p>From what I read online there are several approaches in regard to feature extraction but there isn't a solid example/answer. </p>

<ul>
<li>a. Is there a solid strategy for feature extraction ? 
I read online that you can do [a. Vectorising usin ScikitLearn b. TF-IDF] 
but also I read that you can use Part of Speech or word2Vec or other embedding and Name entity recognition. </li>
<li>b. What is the optimal process/structure of using these? </li>
<li>c. On the text pre-processing I am ding the processing on a text column on a df and the last modified version of it is what I use as an input in my classifier. If you do feature extraction do you do that in the same column or you create a new one and you only send to the classifier the features from that column? </li>
</ul>

<p>Thanks so much in advance</p>
",Vectorization & Embeddings,nlp structure question best way feature extraction building nlp pipeline trying get head around regard optimal structure understanding moment following step text pre processing lowercasing b stopwords removal c stemming lemmatisation step feature extraction step classification using different type classifier linearsvc etc read online several approach regard feature extraction solid example answer solid strategy feature extraction read online vectorising usin scikitlearn b tf idf also read use part speech word vec embedding name entity recognition b optimal process structure using c text pre processing ding processing text column df last modified version use input classifier feature extraction column create new one send classifier feature column thanks much advance
How to find the score for sentence Similarity using Word2Vec,"<p>I am new to NLP, how to find the similarity between 2 sentences and also how to print scores of each word. And also how to implement the gensim word2Vec model.</p>

<p>Try this code:
here my two sentences :</p>

<pre><code>sentence1=""I am going to India""
sentence2="" I am going to Bharat""
from gensim.models import word2vec
import numpy as np


words1 = sentence1.split(' ')
words2 = sentence2.split(' ')

#The meaning of the sentence can be interpreted as the average of its words
sentence1_meaning = word2vec(words1[0])
count = 1
for w in words1[1:]:
    sentence1_meaning = np.add(sentence1_meaning, word2vec(w))
    count += 1
sentence1_meaning /= count

sentence2_meaning = word2vec(words2[0])
count = 1
for w in words2[1:]:
    sentence2_meaning = np.add(sentence2_meaning, word2vec(w))
    count += 1
sentence2_meaning /= count

#Similarity is the cosine between the vectors
similarity = np.dot(sentence1_meaning, sentence2_meaning)/(np.linalg.norm(sentence1_meaning)*np.linalg.norm(sentence2_meaning))
</code></pre>
",Vectorization & Embeddings,find score sentence similarity using word vec new nlp find similarity sentence also print score word also implement gensim word vec model try code two sentence
How to get similar words in pre trained ELMO embedding?,"<p>How to get similar word for a given word in the pre trained ELMO Embedding? For Example: In Glove, we have glove_model.most_similar() to find the most similar word and its embedding for any given word. 
Similarly do we have anything in ELMO?</p>
",Vectorization & Embeddings,get similar word pre trained elmo embedding get similar word given word pre trained elmo embedding example glove glove model similar find similar word embedding given word similarly anything elmo
Paragraph embedding with ELMo,"<p>I'm trying to understand how to prepare paragraphs for ELMo vectorization.</p>

<p>The <a href=""https://tfhub.dev/google/elmo/2"" rel=""nofollow noreferrer"">docs</a> only show how to embed multiple sentences/words at the time.</p>

<p>eg.</p>

<pre><code>sentences = [[""the"", ""cat"", ""is"", ""on"", ""the"", ""mat""],
         [""dogs"", ""are"", ""in"", ""the"", ""fog"", """"]]
elmo(
     inputs={
          ""tokens"": sentences,
          ""sequence_len"": [6, 5]
            },
     signature=""tokens"",
     as_dict=True
    )[""elmo""]
</code></pre>

<p>As I understand, this will return 2 vectors each representing a given sentence.
How would I go about preparing input data to vectorize a whole paragraph containing multiple sentences. Note that I would like to use my own preprocessing.</p>

<p>Can this be done like so?</p>

<pre><code>sentences = [[""&lt;s&gt;"" ""the"", ""cat"", ""is"", ""on"", ""the"", ""mat"", ""."", ""&lt;/s&gt;"", 
              ""&lt;s&gt;"", ""dogs"", ""are"", ""in"", ""the"", ""fog"", ""."", ""&lt;/s&gt;""]]
</code></pre>

<p>or maybe like so?</p>

<pre><code>sentences = [[""the"", ""cat"", ""is"", ""on"", ""the"", ""mat"", ""."", 
              ""dogs"", ""are"", ""in"", ""the"", ""fog"", "".""]]
</code></pre>
",Vectorization & Embeddings,paragraph embedding elmo trying understand prepare paragraph elmo vectorization doc show embed multiple sentence word time eg understand return vector representing given sentence would go preparing input data vectorize whole paragraph containing multiple sentence note would like use preprocessing done like maybe like
Can I train Word2vec using a Stacked Autoencoder with non-linearities?,"<p>Every time I read about Word2vec, the embedding is obtained with a very simple Autoencoder: just one hidden layer, linear activation for the initial layer, and softmax for the output layer.</p>

<p>My question is: why can't I train some Word2vec model using a stacked Autoencoder, with several hidden layers with fancier activation functions? (The softmax at the output would be kept, of course.)</p>

<p>I never found any explanation about this, therefore any hint is welcome.</p>
",Vectorization & Embeddings,train word vec using stacked autoencoder non linearity every time read word vec embedding obtained simple autoencoder one hidden layer linear activation initial layer softmax output layer question train word vec model using stacked autoencoder several hidden layer fancier activation function softmax output would kept course never found explanation therefore hint welcome
Extract key words by topic,"<p>I have a structured dataset with columns 'text' and 'topic'. Someone has already conducted a word embedding/topic modeling so each row in 'text' is assigned a topic number (1-200). I would like to create a new data frame with the topic number and the top 5-10 key words that represent that topic.</p>

<p>I've done this before, but I usually start from scratch and run an LDA model. Then use the objects created by the LDA to find keywords per topic. That said, I'm starting from a mid-point that my supervisor gave me, and it's throwing me off.</p>

<p>The data structure looks like below:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'text': ['foo bar baz', 'blah bling', 'foo'], 
               'topic': [1, 2, 1]})
</code></pre>

<p>So would the plan be to create a bag of words, groupby 'topic,' and count the words? Or is there a keywords function and group by a column option that I don't know about in gensim or nltk?</p>
",Vectorization & Embeddings,extract key word topic structured dataset column text topic someone ha already conducted word embedding topic modeling row text assigned topic number would like create new data frame topic number top key word represent topic done usually start scratch run lda model use object created lda find keywords per topic said starting mid point supervisor gave throwing data structure look like would plan create bag word groupby topic count word keywords function group column option know gensim nltk
What are some common ways to get sentence vector from corresponding word vectors?,"<p>I have successfully implemented word2vec model to generate word embedding or word vectors now I require to generate sentence vectors from generated word vectors so that I can feed a neural network  to summarize a text corpus. What are the common approaches to generate sentence vectors from word vectors?</p>
",Vectorization & Embeddings,common way get sentence vector corresponding word vector successfully implemented word vec model generate word embedding word vector require generate sentence vector generated word vector feed neural network summarize text corpus common approach generate sentence vector word vector
Computing the similarity between two document using cosine similarity (neo4j),"<p>I have two documents represented by graphs in neo4j. The basic version of the graph describing of each document is as follows:
- DocumentNode (represents each document name)
- TokenNode (represents token mentioned in the document)
These two label nodes are related throughout  the relationship ""HAS_token""  which contains the property frequency (frequency of a token in a document).
I wrote the following query but I'm not sure that it allows to compute efficiency the similarity:</p>

<pre><code>MATCH (p1:DocumentNode {name: 'doc1'})-[r1:HAS_token]-&gt;(tag)
MATCH (p2:DocumentNode {name: ""doc2""})-[r2:HAS_token]-&gt;(tag)
RETURN p1.name AS from,
       p2.name AS to,
       algo.similarity.cosine(collect(r1.score), collect(r2.score)) AS similarity
</code></pre>

<p>In addition, the Tag are related to Wikipedia Category (represented by the label Category) via the relationship HAS_category. Is there any way to include the category of each tag when computing the cosine similarity?</p>
",Vectorization & Embeddings,computing similarity two document using cosine similarity neo j two document represented graph neo j basic version graph describing document follows documentnode represents document name tokennode represents token mentioned document two label node related throughout relationship ha token contains property frequency frequency token document wrote following query sure allows compute efficiency similarity addition tag related wikipedia category represented label category via relationship ha category way include category tag computing cosine similarity
How do I apply Tfidf_vectorizer to the whole pandas column?,"<p>First of all I'm kinda new to NLP, so I might have understood a concept in a wrong way or something</p>

<p>I am trying to find a way to vectorize the whole column as 1 text, and then after getting the results, I would like to fit the model I'm using to my target set.</p>

<p>I am currently using a pipeline to vectorize my dataframe columns, but I believe that they are being vectorized 1 by 1, instead of joining all the columns together and then doing it.</p>

<p>Here's an exaggerated example of my dataset:</p>

<pre><code>   data                                                 target
1 ""conventional normal breast cancer test""              breast cancer test
2 ""regular and conventional normal lung cancer test""    lung cancer test
</code></pre>

<p>Basically, I would like to give the terms ""breast"" and ""lung"" a high tfidf score because it is unique, and I don't want my model to mistake these 2 types of data because they seem similar</p>

<p>My Current Code:</p>

<pre><code>vectorizer = feature_extraction.text.TfidfVectorizer(ngram_range=(1, 3),
                         analyzer='word',)

pipe = pipeline.Pipeline([
('vectorizer', vectorizer),
('clf', linear_model.LogisticRegression())])

pipe.fit(X_train, y_train)

y_predicted = pipe.predict(X_test)
</code></pre>
",Vectorization & Embeddings,apply tfidf vectorizer whole panda column first kinda new nlp might understood concept wrong way something trying find way vectorize whole column text getting result would like fit model using target set currently using pipeline vectorize dataframe column believe vectorized instead joining column together exaggerated example dataset basically would like give term breast lung high tfidf score unique want model mistake type data seem similar current code
calculate cosine similarity of two words in R?,"<p>I have a text file and would like to create semantic vectors for each word in the file. I would then like to extract the cosine similarity for about 500 pairs of words. What is the best package in R for doing this?</p>
",Vectorization & Embeddings,calculate cosine similarity two word r text file would like create semantic vector word file would like extract cosine similarity pair word best package r
What are the purposes of each step in train-evaluate-predict in tensorflow?,"<p>What do each of the stages do? I understand that for neural nets in nlp, the train will find the best parameters for the word embedding. But what is the purpose of the evaluation step? What is it supposed to do? How is that different from the prediction phase?</p>
",Vectorization & Embeddings,purpose step train evaluate predict tensorflow stage understand neural net nlp train find best parameter word embedding purpose evaluation step supposed different prediction phase
How to categorize positive and negative features from top features,"<p>I have trained user reviews thru average tfidf wor2vec model and got top features. Would like to tag top features as positive &amp; negative.</p>

<p>Could you please suggest. </p>

<pre><code>def top_tfidf_feats(row, features, top_n=1):
    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''
    topn_ids = np.argsort(row)[::-1][:top_n]
    top_feats = [(features[i], row[i]) for i in topn_ids]
    df = pd.DataFrame(top_feats)
    df.columns = ['feature', 'tfidf']
    return df

top_tfidf = top_tfidf_feats(final_tf_idf[1,:].toarray()[0],tfidf_feat,10)
</code></pre>

<pre><code>Top 10 features...
feature     tfidf
-------     ------
0   urgent  0.513783
1   tells   0.501945
2   says    0.490708
3   clear   0.424756
4   care    0.206723
5   not 0.141886
6   flanum  0.000000
7   flap    0.000000
8   flare   0.000000
9   flared  0.000000
10  flares  0.000000
</code></pre>
",Vectorization & Embeddings,categorize positive negative feature top feature trained user review thru average tfidf wor vec model got top feature would like tag top feature positive negative could please suggest
Pytorch: How to implement nested transformers: a character-level transformer for words and a word-level transformer for sentences?,"<p>I have a model in mind, but I'm having a hard time figuring out how to actually implement it in Pytorch, especially when it comes to training the model (e.g. how to define mini-batches, etc.). First of all let me quickly introduce the context:</p>

<p>I'm working on VQA (visual question answering), in which the task is to answer questions about images, for example:</p>

<p><a href=""https://i.sstatic.net/UW8hE.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/UW8hE.png"" alt=""enter image description here""></a></p>

<p>So, letting aside many details, I just want to focus here on the NLP aspect/branch of the model. In order to process the natural language question, I want to  use <em>character-level</em> embeddings (instead of traditional <em>word-level</em> embeddings) because they are more robust in the sense that they can easily accommodate for morphological variations in words (e.g. prefixes, suffixes, plurals, verb conjugations, hyphens, etc.). But at the same time I don't want to lose the inductive bias of reasoning at the word level. Therefore, I came up with the following design:</p>

<p><a href=""https://i.sstatic.net/Tl7bu.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/Tl7bu.png"" alt=""enter image description here""></a></p>

<p>As you can see in the picture above, I want to use <a href=""http://papers.nips.cc/paper/7181-attention-is-all-you-need"" rel=""noreferrer"">transformers</a> (or even better, <a href=""https://arxiv.org/abs/1807.03819"" rel=""noreferrer"">universal transformers</a>), but with a little twist. I want to use 2 transformers: the first one will process each word characters in isolation (<em>character-level</em> transformer) to produce an initial word-level embedding for each word in the question. Once we have all these initial word-level embeddings, a second <em>word-level</em> transformer will refine these embeddings to enrich their representation with context, thus obtaining <em>context-aware word-level</em> embeddings.</p>

<p>The full model for the whole VQA task obviously is more complex, but I just want to focus here on this NLP part. So my question is basically about which Pytorch functions should I pay attention to when implementing this. For example, since I'll be using <em>character-level</em> embeddings I have to define a <em>character-level</em> embedding matrix, but then I have to perform lookups on this matrix to generate the inputs for the <em>character-level</em> transformer, repeat this for each word in the question and then feed all these vectors into the <em>word-level</em> transformer. Moreover, words in a single question can have different lengths, and questions within a single mini-batch can have different lengths too. So in my code I have to somehow account for different lengths at the word and the question level simultaneously in a single mini-batch (during training), and I've got no idea how to do that in Pytorch or whether it's even possible at all.</p>

<p>Any tips on how to go about implementing this in Pytorch that could lead me in the right direction will be deeply appreciated.</p>
",Vectorization & Embeddings,pytorch implement nested transformer character level transformer word word level transformer sentence model mind hard time figuring actually implement pytorch especially come training model e g define mini batch etc first let quickly introduce context working vqa visual question answering task answer question image example letting aside many detail want focus nlp aspect branch model order process natural language question want use character level embeddings instead traditional word level embeddings robust sense easily accommodate morphological variation word e g prefix suffix plural verb conjugation hyphen etc time want lose inductive bias reasoning word level therefore came following design see picture want use transformer even better universal transformer little twist want use transformer first one process word character isolation character level transformer produce initial word level embedding word question initial word level embeddings second word level transformer refine embeddings enrich representation context thus obtaining context aware word level embeddings full model whole vqa task obviously complex want focus nlp part question basically pytorch function pay attention implementing example since using character level embeddings define character level embedding matrix perform lookup matrix generate input character level transformer repeat word question feed vector word level transformer moreover word single question different length question within single mini batch different length code somehow account different length word question level simultaneously single mini batch training got idea pytorch whether even possible tip go implementing pytorch could lead right direction deeply appreciated
Keras - Issues using pre-trained word embeddings,"<p>I'm following Keras tutorials on word embeddings and replicated the code (with a few modifications) from this particular one:</p>

<p><a href=""https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"" rel=""nofollow noreferrer"">Using pre-trained word embeddings in a Keras model</a></p>

<p>It's a topic classification problem in which they are loading pre-trained word vectors and use them via a fixed embedding layer.</p>

<p>When using the pre-trained embedding vectors I can, in fact, achieve their 95% accuracy. This is the code:</p>



<pre class=""lang-python prettyprint-override""><code>embedding_layer = Embedding(len(embed_matrix), len(embed_matrix.columns), weights=[embed_matrix],
                           input_length=data.shape[1:], trainable=False)

sequence_input = Input(shape=(MAXLEN,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)

x = Conv1D(128, 5, activation='relu')(embedded_sequences)
x = MaxPooling1D(5)(x)
x = Conv1D(128, 5, activation='relu')(x)
x = MaxPooling1D(5)(x)
x = Dropout(0.2)(x)
x = Conv1D(128, 5, activation='relu')(x)
x = MaxPooling1D(35)(x)  # global max pooling
x = Flatten()(x)
x = Dense(128, activation='relu')(x)
output = Dense(target.shape[1], activation='softmax')(x)

model = Model(sequence_input, output)
model.compile(loss='categorical_crossentropy', optimizer='adam', 
metrics=['acc'])
model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, 
batch_size=128)
</code></pre>

<p>The issue happens when I remove the embedding vectors and use completely random vectors, surprisingly achieving <strong>higher</strong> accuracy: 96.5%. </p>

<p>The code is the same, with one modification: <em>weighs</em>=[<em>random_matrix</em>]. That's a matrix with the same shape of <em>embed_matrix</em>, but using random values. So this is the embedding layer now:</p>

<pre class=""lang-python prettyprint-override""><code>embedding_layer = Embedding(len(embed_matrix), 
len(embed_matrix.columns), weights=[random_matrix],
                        input_length=data.shape[1:], trainable=False)
</code></pre>

<p>I experimented many times with random weights and the result is always similar. Notice that even though those weights are random, the <em>trainable</em> parameter is still <em>False</em>, so the NN is not updating them.</p>

<p>After that, I fully removed the embedding layer and used words sequences as the input, expecting that those weights were not contributing to the model's accuracy. With that, I got nothing more than 16% accuracy. </p>

<p>So, what is going on? How could random embeddings achieve the same or better performance than pre-trained ones?</p>

<p>And why using word indexes (normalized, of course) as inputs result in such a poor accuracy?</p>
",Vectorization & Embeddings,kera issue using pre trained word embeddings following kera tutorial word embeddings replicated code modification particular one using pre trained word embeddings kera model topic classification problem loading pre trained word vector use via fixed embedding layer using pre trained embedding vector fact achieve accuracy code issue happens remove embedding vector use completely random vector surprisingly achieving higher accuracy code one modification weighs random matrix matrix shape embed matrix using random value embedding layer experimented many time random weight result always similar notice even though weight random trainable parameter still false nn updating fully removed embedding layer used word sequence input expecting weight contributing model accuracy got nothing accuracy going could random embeddings achieve better performance pre trained one using word index normalized course input result poor accuracy
Questions regarding the input dimensionality of Embeddings in Keras based on official documentation,"<p>I have been reading the Keras tutorial published in Jonathan Hui Blog (<a href=""https://jhui.github.io/2018/02/11/Keras-tutorial/"" rel=""nofollow noreferrer"">https://jhui.github.io/2018/02/11/Keras-tutorial/</a>) which as he states draws directly from the official Keras documentation.</p>

<p>There are parts of the code I don't quite understand and these relate to the dimensions of the Embedding layers.</p>

<p>Let's look at two examples:</p>

<p><strong>First Example:</strong></p>

<pre><code>from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Embedding
from keras.layers import LSTM

import numpy as np

max_features = 10
x_train = np.random.random((1000, max_features))
y_train = np.random.randint(2, size=(1000, 1))
x_test = np.random.random((100, max_features))
y_test = np.random.randint(2, size=(100, 1))

model = Sequential()
model.add(Embedding(max_features, output_dim=256))
model.add(LSTM(128))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.fit(x_train, y_train, batch_size=16, epochs=10)
score = model.evaluate(x_test, y_test, batch_size=16)
</code></pre>

<p>Here X is a matrix (1000 x 10) containing random numbers, each of one of them potentially unique.  However the Embedding layers receives as argument for input dimensions max_features (i.e. 10 in this example).  But when we pass input dimensions to an Embedding layers are n't we counting the unique possible values of the variable we are encoding?  To put it in other words, is n't the number of columns that would result from dummifying the variable on the basis of an assumption regarding the Dimensionality of the Space from which its values originate (e.g. a Vocabulary)?</p>

<p><strong>Second Example:</strong></p>

<pre><code>import keras
import numpy as np

from keras.layers import Input, Embedding, LSTM, Dense
from keras.models import Model

# The first input
main_input = Input(shape=(100,), dtype='int32', name='main_input')

# This embedding layer will encode the input sequence
# into a sequence of dense 512-dimensional vectors.
x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)

# A LSTM will transform the vector sequence into a single vector,
# containing information about the entire sequence
lstm_out = LSTM(32)(x)

auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)

# Second input
auxiliary_input = Input(shape=(5,), name='aux_input')
x = keras.layers.concatenate([lstm_out, auxiliary_input])

# We stack a deep densely-connected network on top
x = Dense(64, activation='relu')(x)
x = Dense(64, activation='relu')(x)
x = Dense(64, activation='relu')(x)

# And finally we add the main logistic regression layer
main_output = Dense(1, activation='sigmoid', name='main_output')(x)

model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])

model.compile(optimizer='rmsprop', loss='binary_crossentropy',
              loss_weights=[1., 0.2])

headline_data = np.random.random((1000, 100))
additional_data = np.random.random((1000, 5))
labels = np.random.random((1000, 1))

model.fit([headline_data, additional_data], [labels, labels],
          epochs=50, batch_size=32)
</code></pre>

<p>Here the input dimension is set to be 10,000 but again we are dealing with an input (headline_data) which has potentially 1000 x 100 = 100,000 unique values.  How can we represent these 100,000 values using a 10,000 dimensional input space?</p>
",Vectorization & Embeddings,question regarding input dimensionality embeddings kera based official documentation reading kera tutorial published jonathan hui blog state draw directly official kera documentation part code quite understand relate dimension embedding layer let look two example first example x matrix x containing random number one potentially unique however embedding layer receives argument input dimension max feature e example pas input dimension embedding layer n counting unique possible value variable encoding put word n number column would result dummifying variable basis assumption regarding dimensionality space value originate e g vocabulary second example input dimension set dealing input headline data ha potentially x unique value represent value using dimensional input space
java.lang.RuntimeException: java.lang.UnsupportedOperationExceptionat error while training GloVe with DL4j,"<p>I want to work with GloVe embedding on English sentences. While training it is showing error  ""java.lang.RuntimeException: java.lang.UnsupportedOperationExceptionat""</p>

<p>Glove embedding is unsupervised model to encode English text into vectors. Dl4j is a java library. While training Glove model this error is showing.</p>

<pre><code>File inputFile = new ClassPathResource(""raw_sentences.txt"").getFile();

    // creating SentenceIterator wrapping our training corpus
    SentenceIterator iter = new BasicLineIterator(inputFile.getAbsolutePath());

    // Split on white spaces in the line to get words
    TokenizerFactory t = new DefaultTokenizerFactory();
    t.setTokenPreProcessor(new CommonPreprocessor());

    Glove glove = new Glove.Builder()
            .iterate(iter)
            .tokenizerFactory(t)


            .alpha(0.75)
            .learningRate(0.1)

            // number of epochs for training
            .epochs(25)

            // cutoff for weighting function
            .xMax(100)

            // training is done in batches taken from training corpus
            .batchSize(1000)

            // if set to true, batches will be shuffled before training
            .shuffle(true)

            // if set to true word pairs will be built in both directions, LTR and RTL
            .symmetric(true)
            .build();
    log.info(""Up here fine"");
    glove.fit();  // Getting error here

o.d.m.s.SequenceVectors - Starting learning process...
Exception in thread ""VectorCalculationsThread 0"" java.lang.RuntimeException: java.lang.UnsupportedOperationException
    at org.deeplearning4j.models.sequencevectors.SequenceVectors$VectorCalculationsThread.run(SequenceVectors.java:1341)
Caused by: java.lang.UnsupportedOperationException
    at org.deeplearning4j.models.embeddings.learning.impl.elements.GloVe.learnSequence(GloVe.java:137)
    at org.deeplearning4j.models.sequencevectors.SequenceVectors.trainSequence(SequenceVectors.java:399)
    at org.deeplearning4j.models.sequencevectors.SequenceVectors$VectorCalculationsThread.run(SequenceVectors.java:1274)
</code></pre>
",Vectorization & Embeddings,java lang runtimeexception java lang unsupportedoperationexceptionat error training glove dl j want work glove embedding english sentence training showing error java lang runtimeexception java lang unsupportedoperationexceptionat glove embedding unsupervised model encode english text vector dl j java library training glove model error showing
Python3 - Doc2Vec: Get document by vector/ID,"<p>I've already built my Doc2Vec model, using around 20.000 files. I'm looking for a way to find the string representation of a given vector/ID, which might be similar to Word2Vec's index2entity. I'm able to get the vector itself, using model['n'], but now I'm wondering whether there's a way to get some sort of string representation of it as well.</p>
",Vectorization & Embeddings,python doc vec get document vector id already built doc vec model using around file looking way find string representation given vector id might similar word vec index entity able get vector using model n wondering whether way get sort string representation well
Using cosine similarity for classifying documents,"<p>I have a set of files for five different categories and most of them are not labelled correctly.Objective is to predict the correct category of the file whenever the same is uploaded.I used cosine similarity along with tf -idf to predict the class of the document with which cosine similarity is the maximum as of now i am getting good results but really not sure how well this will work down the road. Also why isnt cosine similarity used in building document classifiers instead of machine learning models when the categories of files are labelled correctly?Would really appreciate your feedback on my approach as well as your answer to the question.</p>
",Vectorization & Embeddings,using cosine similarity classifying document set file five different category labelled correctly objective predict correct category file whenever uploaded used cosine similarity along tf idf predict class document cosine similarity maximum getting good result really sure well work road also isnt cosine similarity used building document classifier instead machine learning model category file labelled correctly would really appreciate feedback approach well answer question
get ngrams with positional information,"<p>I'm trying to group similar short descriptions together and currently using ngrams to extract text features. Here's the ngrams function that I'm using:</p>

<pre><code>def generate_ngrams(text, n):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)
    tokens = [token for token in text.split("" "") if token != """"]
    ngrams = zip(*[token[i:] for i in range(n)])
    return ["" "".join(ngram) for ngram in ngrams]
</code></pre>

<p>However, I'm experiencing some undesired results after clustering. Suppose I have the following two texts:</p>

<pre><code>00011122abc
00111224abc
</code></pre>

<p>By using ngrams(n=3), my clustering model grouped these together, which is not what I want. So I think I need to pass a new function into tfidf vectorizer instead of ngrams. I think I need to anchor the first char and create substrings as my features for tfidf, so for the first text it will be something like this:</p>

<pre><code>[000, 0001, 00011, 0001111, 0001112 ...]
</code></pre>

<p>Has anyone else experienced similar problems or is there a better way to approach this? Thanks!</p>
",Vectorization & Embeddings,get ngrams positional information trying group similar short description together currently using ngrams extract text feature ngrams function using however experiencing undesired result clustering suppose following two text using ngrams n clustering model grouped together want think need pas new function tfidf vectorizer instead ngrams think need anchor first char create substring feature tfidf first text something like ha anyone else experienced similar problem better way approach thanks
scikit cosine_similarity vs pairwise_distances,"<p>What is the difference between Scikit-learn's sklearn.metrics.pairwise.cosine_similarity and sklearn.metrics.pairwise.pairwise_distances(.. metric=""cosine"")?</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

documents = (
    ""Macbook Pro 15' Silver Gray with Nvidia GPU"",
    ""Macbook GPU""    
)

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(documents)

from sklearn.metrics.pairwise import cosine_similarity
print(cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)[0,1])
</code></pre>

<p>0.37997836</p>

<pre><code>from sklearn.metrics.pairwise import pairwise_distances
print(pairwise_distances(tfidf_matrix[0:1], tfidf_matrix, metric='cosine')[0,1])
</code></pre>

<p>0.62002164</p>

<p>Why are these different?</p>
",Vectorization & Embeddings,scikit cosine similarity v pairwise distance difference scikit learn sklearn metric pairwise cosine similarity sklearn metric pairwise pairwise distance metric cosine different
Word embeddings perform poorly for text classification,"<p>I am working on a text classification use case. The text is basically contents of legal documents, for example, companies annual reports, W9 etc. So there are 10 different categories and 500 documents in total. Therefore 50 documents per category. So the dataset consists of 500 rows and 2 columns, 1st column consisting of text and 2nd column is the Target. </p>

<p>I have built a basic model using TF-IDF for my textual features. I have used Multinomial Naive Bayes, SVC, Linear SGD, Multilayer Perceptron, Random Forest. These models are giving me an F1-score of approx 70-75%.</p>

<p>I wanted to see if creating word-embedding will help me improve the accuracy. I trained the word vectors using gensim Word2vec, and fit the word vectors through the same ML models as above, but I am getting a score of about 30-35%. I have a very small dataset and lot of categories, is that the problem? Is it the only reason, or there is something I am missing out? </p>
",Vectorization & Embeddings,word embeddings perform poorly text classification working text classification use case text basically content legal document example company annual report w etc different category document total therefore document per category dataset consists row column st column consisting text nd column target built basic model using tf idf textual feature used multinomial naive bayes svc linear sgd multilayer perceptron random forest model giving f score approx wanted see creating word embedding help improve accuracy trained word vector using gensim word vec fit word vector ml model getting score small dataset lot category problem reason something missing
Word2Vec word not found with Gensim but shows up on TensorFlow embedding projector?,"<p>I've recently started experimenting with pre-trained word embeddings to enhance the performance of my LSTM model on a NLP task. In this case, I looked into Google's Word2Vec. Based on online tutorials, I first downloaded Word2Vec with <code>wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz</code> and used python's <code>gensim</code> package to query the embeddings, using the following code.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import KeyedVectors

if __name__ == ""__main__"":
    model = KeyedVectors.load_word2vec_format(""./data/word2vec/GoogleNews-vectors-negative300.bin"", binary=True)
    print(model[""bosnia""])
</code></pre>

<p>However, after noticing that many common words weren't found in the model, I started to wonder if something was awry. I tried searching for <code>bosnia</code> in the embedding repo, as shown above, but it wasn't found. So, I went on the <a href=""https://projector.tensorflow.org/"" rel=""nofollow noreferrer"">TensorFlow embedding projector</a>, loaded the Word2Vec model, and searched for <code>bosnia</code> - it was there.</p>

<p>So, my question is: why is this happening? Was the version of Word2Vec I downloaded not complete? Or is gensim unable to load all words into memory and therefore omitting some?</p>
",Vectorization & Embeddings,word vec word found gensim show tensorflow embedding projector recently started experimenting pre trained word embeddings enhance performance lstm model nlp task case looked google word vec based online tutorial first downloaded word vec used python package query embeddings using following code however noticing many common word found model started wonder something wa awry tried searching embedding repo shown found went tensorflow embedding projector loaded word vec model searched wa question happening wa version word vec downloaded complete gensim unable load word memory therefore omitting
Understanding ELMo&#39;s number of presentations,"<p>I am trying my hand at ELMo by simply using it as part of a larger PyTorch model. A basic example is given <a href=""https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md#using-elmo-as-a-pytorch-module-to-train-a-new-model"" rel=""nofollow noreferrer"">here</a>.</p>

<blockquote>
  <p>This is a torch.nn.Module subclass that computes any number of ELMo
  representations and introduces trainable scalar weights for each. For
  example, this code snippet computes two layers of representations (as
  in the SNLI and SQuAD models from our paper):</p>
</blockquote>

<pre><code>from allennlp.modules.elmo import Elmo, batch_to_ids

options_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json""
weight_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5""

# Compute two different representation for each token.
# Each representation is a linear weighted combination for the
# 3 layers in ELMo (i.e., charcnn, the outputs of the two BiLSTM))
elmo = Elmo(options_file, weight_file, 2, dropout=0)

# use batch_to_ids to convert sentences to character ids
sentences = [['First', 'sentence', '.'], ['Another', '.']]
character_ids = batch_to_ids(sentences)

embeddings = elmo(character_ids)

# embeddings['elmo_representations'] is length two list of tensors.
# Each element contains one layer of ELMo representations with shape
# (2, 3, 1024).
#   2    - the batch size
#   3    - the sequence length of the batch
#   1024 - the length of each ELMo vector
</code></pre>

<p>My question concerns the 'representations'. Can you compare them to normal word2vec output layers? You can choose how <em>many</em> ELMo will give back (increasing an n-th dimension), but what is the difference between these generated representations and what is their typical use? </p>

<p>To give you an idea, for the above code, <code>embeddings['elmo_representations']</code> returns a list of two items (the two representation layers) but they are identical.</p>

<p>In short, how can one define the 'representations' in ELMo? </p>
",Vectorization & Embeddings,understanding elmo number presentation trying hand elmo simply using part larger pytorch model basic example given torch nn module subclass computes number elmo representation introduces trainable scalar weight example code snippet computes two layer representation snli squad model paper question concern representation compare normal word vec output layer choose many elmo give back increasing n th dimension difference generated representation typical use give idea code return list two item two representation layer identical short one define representation elmo
Quick way to get document vector using GloVe,"<h2>Problem</h2>

<p>I am trying to use GloVe to represent entire document. However, GloVe is initially designed to get word embedding. One way to get the document embedding is to take the average of all word embeddings in the document.</p>

<p>I am following the solution posted <a href=""https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python"">here</a> to load the GloVe look-up table. However, when I tried to get the document embedding, the runtime is extremely slow (about 1s per document for more than 1 million documents). </p>

<p>I am wondering if there is any way I could accelerate this process.</p>

<p>The GloVe look-up table could be downloaded <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">here</a> and the following is the code I use to get the document embedding. The data is stored in a <code>pd.DataFrame()</code>, where there is a <code>review</code> column.</p>

<p>Note there might be some words in the <code>text_processed_list</code> not present in the look-up table, that is why <code>try...catch...</code> comes into play.</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd
import string
import csv

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

remove_list = stopwords.words('english') + list(string.punctuation)

X = np.zeros((dataset_size, 300))
glove_model = pd.read_table(""glove.42B.300d.txt"", sep="" "", index_col=0, header=None, quoting=csv.QUOTE_NONE)
for iter in range(dataset_size):
    text = data.loc[iter, ""review""]
    text_processed_list = [word for word in word_tokenize(text.lower()) if word not in remove_list]
    for word in text_processed_list:
        try:
            X[iter] += glove_model.loc[word].values
        except KeyError:
            pass
        X[iter] /= len(text_processed_list)

</code></pre>
",Vectorization & Embeddings,quick way get document vector using glove problem trying use glove represent entire document however glove initially designed get word embedding one way get document embedding take average word embeddings document following solution posted following code use get document embedding data stored column note might word present look table come play
How does word2vec or skip-gram model convert words to vector?,"<p>I have been reading a lot of papers on NLP, and came across many models. I got the SVD Model and representing it in 2-D, but I still did not get how do we make a word vector by giving a corpus to the word2vec/skip-gram model? Is it also co-occurrence matrix representation for each word? Can you explain it by taking an example corpus:</p>

<pre><code>Hello, my name is John.
John works in Google.
Google has the best search engine. 
</code></pre>

<p>Basically, how does skip gram convert <code>John</code> to a vector?</p>
",Vectorization & Embeddings,doe word vec skip gram model convert word vector reading lot paper nlp came across many model got svd model representing still get make word vector giving corpus word vec skip gram model also co occurrence matrix representation word explain taking example corpus basically doe skip gram convert vector
Correct gradients for word2vec negative sampling skip gram model,"<p>I am trying to implement skip-gram word2vec in python using negative sampling.
From my understanding, I should be maximizing the equation (4) from the paper by <a href=""https://arxiv.org/pdf/1310.4546.pdf"" rel=""nofollow noreferrer"">Mikolov Et al</a>. </p>

<p>I have taken the gradients of this equation with respect to Vc, U, and U_rand. Where Vc is the center vector corresponding to the center word, U is the context vector corresponding to a word in the context of the center word and U_rand is the context vector of a randomly sampled word.</p>

<p>I am then calculating the cost function for each combination of word and context word adding them up and printing out the total sum of the whole corpus. I am running this a few times and I do not see an improvement on the whole corpus sum of costs. The cost goes up and then down repeatedly.  </p>

<p>I got the following gradients</p>

<blockquote>
  <p>grad J with respect to Vc = (1-sigma(V•U))*U - Summation over random
  vectors (1-sigma(-V•U_rand))*U_rand</p>
  
  <p>grad J with respect to U = (1-sigma(V•U))*V</p>
  
  <p>grad J with respect to U_rand = (1-sigma(-V•U_rand))*-V</p>
</blockquote>

<p>So with that being said, I have a few questions: </p>

<ol>
<li>Are these gradients correct?</li>
<li>Should I be taking a step in the direction of the gradient? (as opposed to the negative of the gradient) To me, I should be as we are maximizing the cost function</li>
<li>for the randomly sampled word are we using its center word representation or context work representation. From the Stanford lecture I watched on <a href=""https://www.youtube.com/watch?v=ASn7ExxLZws&amp;t=3582s"" rel=""nofollow noreferrer"">youtube</a> it seems to be its context vector. But this <a href=""https://web.stanford.edu/~jurafsky/slp3/16.pdf"" rel=""nofollow noreferrer"">source</a> seems to differ. </li>
<li>Is adding all the cost function results for the whole corpus a valid way to see improvement? (I do not see why not)</li>
</ol>
",Vectorization & Embeddings,correct gradient word vec negative sampling skip gram model trying implement skip gram word vec python using negative sampling understanding maximizing equation paper mikolov et al taken gradient equation respect vc u u rand vc center vector corresponding center word u context vector corresponding word context center word u rand context vector randomly sampled word calculating cost function combination word context word adding printing total sum whole corpus running time see improvement whole corpus sum cost cost go repeatedly got following gradient grad j respect vc sigma v u u summation random vector sigma v u rand u rand grad j respect u sigma v u v grad j respect u rand sigma v u rand v said question gradient correct taking step direction gradient opposed negative gradient maximizing cost function randomly sampled word using center word representation context work representation stanford lecture watched youtube seems context vector source seems differ adding cost function result whole corpus valid way see improvement see
Can i use one-hot vector for named entity tags to concate with word embedding to improve neural machine translation?,"<p>i'm using word features such as additional input for my nmt model. I used gensim to create pretrained embedding for each features and concated them with my word embedding vectors. So the question is can i use named entity tags as word feature and concate named entity tag one-hot vector to word embedding vector ? Main idea come from this <a href=""https://www.aclweb.org/anthology/U16-1001"" rel=""nofollow noreferrer"">paper</a></p>

<p>Example:</p>

<p>word embedding vector: [1,2,3,4]</p>

<p>pos tag: [5,6,7]</p>

<p>named entity tag: [0,1,0]</p>

<p>=> final embedding vector: [1,2,3,4,5,6,7,0,1,0]</p>
",Vectorization & Embeddings,use one hot vector named entity tag concate word embedding improve neural machine translation using word feature additional input nmt model used gensim create pretrained embedding feature concated word embedding vector question use named entity tag word feature concate named entity tag one hot vector word embedding vector main idea come paper example word embedding vector po tag named entity tag final embedding vector
Word2Vec with POS not producing expected results?,"<p>I am trying to gauge the impact of part of speech information with Word2Vec embeddings but am not obtaining expected results.</p>

<p>I expected POS included word2vec embeddings to perform better in a machine translation task but it is actually performing worse.</p>

<p>I am creating two sets of embedding off of the same corpus using Gensim, one is normal Word2Vec, the other, I am changing tokens to ""[WORD]__[POS]"".</p>

<p>I am gauging differences in performance by using the embeddings in a Seq2Seq machine translation task. I am evaluating the two approaches with BLEU</p>

<p>This is how I am training the word2vec + POS embeddings with SpaCy:</p>

<pre><code>sentences = []
    for sent in doc.sents:
        tokens = []
        for t in sent:
            tokens += [""{}__{}"".format(t.text, t.pos_)]
        sentences += tokens
    pos_train += [sentences]
</code></pre>

<p>This is my benchmark machine translation model with Keras + Tensorflow:</p>

<pre><code>encoder_inputs = Input(shape=(None, num_encoder_tokens))
encoder = LSTM(LATENT_DIM, return_state=True)
_, state_h, state_c = encoder(encoder_inputs)
encoder_states = [state_h, state_c]

decoder_inputs = Input(shape=(None, num_decoder_tokens))
decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
decoder_dense = Dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
</code></pre>

<p>With BLEU, the Word2Vec+POS approach consistently scores the same as Word2Vec or 0.01-0.02 points below the normal Word2Vec embeddings.</p>

<p>Does anyone know why this might be happening? Is there a gap in my reasoning or expectations?</p>
",Vectorization & Embeddings,word vec po producing expected result trying gauge impact part speech information word vec embeddings obtaining expected result expected po included word vec embeddings perform better machine translation task actually performing worse creating two set embedding corpus using gensim one normal word vec changing token word po gauging difference performance using embeddings seq seq machine translation task evaluating two approach bleu training word vec po embeddings spacy benchmark machine translation model kera tensorflow bleu word vec po approach consistently score word vec point normal word vec embeddings doe anyone know might happening gap reasoning expectation
How to find similarity between two list of strings using doc2vec?,"<p>I have a list of strings like below. I would like to see similarity between list1 and list2 using Doc2Vec.</p>

<pre><code>list1 = [['i','love','machine','learning','its','awesome'],['i', 'love', 'coding', 'in', 'python'],['i', 'love', 'building', 'chatbots']]
list2 = ['i', 'love', 'chatbots']
</code></pre>
",Vectorization & Embeddings,find similarity two list string using doc vec list string like would like see similarity list list using doc vec
Similarity of LDA &amp; Word2vec to cluster words,"<p>recently I am working on a project of text mining. My goal is to cluster comments based on their topics(contents). </p>

<p>I tried a model myself based on:<a href=""https://www.kaggle.com/liananapalkova/simply-about-word2vec"" rel=""nofollow noreferrer"">https://www.kaggle.com/liananapalkova/simply-about-word2vec</a>. It seems that creating word-vectors can definitely take me to my goal, I was thinking to use doc2vec and create vectors that represent each of the comments and perform k-means and other unsupervised learning clustering techniques(i.e. treat it just as numeric).</p>

<p>Then I look something up about LDA, I found that use a lot of sub-topics to describe a document(the comment in my case) also makes sense, eventually, I will let the LDA assign some topics to each of my comments.  </p>

<p>My question is: It seems word2vec/doc2vec is using some tokens to represent the word, while LDA uses some topics to represent those documents, they sound really similar! Can I combine the technique of word2vec and LDA together, to get my clusters of comments, at the same assign each cluster some topics? </p>

<p>How does it work? I am looking on Kaggle but cannot find some similar projects...</p>
",Vectorization & Embeddings,similarity lda word vec cluster word recently working project text mining goal cluster comment based topic content tried model based seems creating word vector definitely take goal wa thinking use doc vec create vector represent comment perform k mean unsupervised learning clustering technique e treat numeric look something lda found use lot sub topic describe document comment case also make sense eventually let lda assign topic comment question seems word vec doc vec using token represent word lda us topic represent document sound really similar combine technique word vec lda together get cluster comment assign cluster topic doe work looking kaggle find similar project
word2vec limit similar_by_vector() result to re-trained corpus,"<p>Assume you have a (wikipedia) pre-trained word2vec model, and train it on an additional corpus (very small, 1000 scentences).</p>

<p>Can you imagine a way to limit a vector-search to the ""re-trained"" corpus only?</p>

<p>For example</p>

<pre><code>model.wv.similar_by_vector() 
</code></pre>

<p>will simply find the closest word for a given vector, no matter if it is part of the Wikipedia corpus, or the re-trained vocabulary.</p>

<p>On the other hand, for 'word' search the concept exists:</p>

<pre><code>most_similar_to_given('house',['garden','boat'])
</code></pre>

<p>I have tried to train based on the small corpus from scratch, and it somewhat works as expected. But of course could be much more powerful if the assigned vectors come from a pre-trained set.</p>
",Vectorization & Embeddings,word vec limit similar vector result trained corpus assume wikipedia pre trained word vec model train additional corpus small scentences imagine way limit vector search trained corpus example simply find closest word given vector matter part wikipedia corpus trained vocabulary hand word search concept exists tried train based small corpus scratch somewhat work expected course could much powerful assigned vector come pre trained set
What&#39;s the difference between vector of indices vs. one-hot vector when inputting to embedding layer in NLP?,"<p>Lots of NLP tasks require putting inputs into an embedding layer. The output of the embedding layer is then fed to a model such as LSTM.</p>

<p>Say you had a sentence like ""I like cats"" and ""I like dogs"".</p>

<p>The vocabulary dictionary would be something like:</p>

<pre><code>{'&lt;pad&gt;': 0, 'I': 1, 'like': 2, 'cats': 3, 'dogs': 4}
</code></pre>

<p>My question is, what would the <strong>input to the embedding layer</strong> look like? Would it be:</p>

<ol>
<li><p>[1,2,3,0] for ""I like cats"" and [1,2,4,0] for ""I like dogs"" ?</p></li>
<li><p>[1,1,1,0] for ""I like cats"" and [1,1,0,1] for ""I like dogs""?</p></li>
</ol>

<p>It seems like it doesn't matter either way, but I have seen #1 used quite often, but I don't know why #2 wouldn't work either.</p>
",Vectorization & Embeddings,difference vector index v one hot vector inputting embedding layer nlp lot nlp task require putting input embedding layer output embedding layer fed model lstm say sentence like like cat like dog vocabulary dictionary would something like question would input embedding layer look like would like cat like dog like cat like dog seems like matter either way seen used quite often know work either
How to get penultimate layer output of fastai text model?,"<pre><code>learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.7)
learn.fit_one_cycle(1, 1e-2)
</code></pre>

<p>I have trained fastai model as above. I can get prediction as below</p>

<pre><code>preds, targets = learn.get_preds()
</code></pre>

<p>But instead I want penultimate layer embeddings of model <code>learn</code> (This practise is common for CNN models). Could you help me how to do it? </p>
",Vectorization & Embeddings,get penultimate layer output fastai text model trained fastai model get prediction instead want penultimate layer embeddings model practise common cnn model could help
word mapping for 2D word embedding,"<p>For my Masters Thesis, I created a Word2Vec model. I wanted to show this image to clarify the result. But how does the mapping works to display the words in this 2D space?</p>

<p>All words are represented by a vector of 300 dim. How are they mapped on this 2D image? What are the x &amp; y axes?</p>

<p>Code:</p>

<pre><code>w2v_model.build_vocab(documents)

words = w2v_model.wv.vocab.keys()
vocab_size = len(words)
print(""Vocab size"", vocab_size)

w2v_model.train(documents, total_examples=len(documents), 

epochs=W2V_EPOCH)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(df_train.text)

vocab_size = len(tokenizer.word_index) + 1
print(""Total words"", vocab_size)

x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH)
x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH)

labels = df_train.target.unique().tolist()
labels.append(NEUTRAL)

encoder = LabelEncoder()
encoder.fit(df_train.target.tolist())

y_train = encoder.transform(df_train.target.tolist())
y_test = encoder.transform(df_test.target.tolist())

y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)

embedding_matrix = np.zeros((vocab_size, W2V_SIZE))
for word, i in tokenizer.word_index.items():
  if word in w2v_model.wv:
    embedding_matrix[i] = w2v_model.wv[word]
print(embedding_matrix.shape)
embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)
</code></pre>

<p><a href=""https://i.sstatic.net/bjXX3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bjXX3.png"" alt=""enter image description here""></a></p>
",Vectorization & Embeddings,word mapping word embedding master thesis created word vec model wanted show image clarify result doe mapping work display word space word represented vector dim mapped image x ax code
"Keras ValueError: Shapes (?, ?, ?) and (6, 1) must have the same rank &amp; logits and labels must have the same shape ((6, 1) vs (?, ?, ?)) when compile","<p>I'm trying to build a NLP CNN model for multiclass-classification(6 classes). The first part of structure is: </p>

<p>Input --> Embedding --> Conv --> GlobalMaxPool --> Dropout --> Dense</p>

<p>And after the dense layer, each input sentence is converted to a 100 dimension embeddings.</p>

<p>After this, I'm passing in a constant matrix(6,100) which is a word embedding matrix of six different labels (each row represents a 100-dimensional word embedding) and I calculate the cosine similarity between the sentence embedding and each of the label word embedding as scoring function, and it gives me a result of (6,100).</p>

<p>Next, I pass the result of that to a dense layer to get output, using 1 neuron and sigmoid as activation which gives a result of (6, 1) but it's giving me that error in title when I compile it. </p>

<p>Below is all the code and I appreciate all the help!</p>

<p>MAX_SEQUENCE_LENGTH = 250</p>

<pre><code>jdes_sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32')
word_embedding_layer = embedding_layer(jdes_sequence_input)
jdes = word_embedding_layer
jdes = Conv1D(filters=1000, kernel_size=5, strides=1, activation='tanh')(jdes)
jdes = GlobalMaxPooling1D()(jdes)
jdes = Dense(1000, activation='tanh')(jdes)
jdes = Dropout(0.3)(jdes)
jdes = Dense(100, activation='relu')(jdes)

def cosine_distance(input): # label_embedding is the constant matrix
    jd = K.l2_normalize(input, axis=-1)
    jt_six = K.l2_normalize(label_embedding, axis=-1)
    return jd * jt_six # return a 6*100 result

distance = Lambda(cosine_distance, output_shape=(6,100))(jdes)
result = Dense(1, activation='sigmoid')(distance)

model = Model(inputs=jdes_sequence_input, outputs = result)
sgd = optimizers.SGD(lr=0.05)
model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])
model.fit(pad_data, labels, validation_split=0.2, batch_size=64, nb_epoch=1)
</code></pre>

<p>pad_data has shape: (18722, 250)
labels has shape: (18722, 6)</p>
",Vectorization & Embeddings,kera valueerror shape must rank logits label must shape v compile trying build nlp cnn model multiclass classification class first part structure input embedding conv globalmaxpool dropout dense dense layer input sentence converted dimension embeddings passing constant matrix word embedding matrix six different label row represents dimensional word embedding calculate cosine similarity sentence embedding label word embedding scoring function give result next pas result dense layer get output using neuron sigmoid activation give result giving error title compile code appreciate help max sequence length pad data ha shape label ha shape
How to pad a n dimensional array,"<p>I have a matrix of word embedding it goes on like -</p>

<pre><code>
([[""word1"",""word2""...],[""word6"",""word5""....],[...],[....]......])

</code></pre>

<p>Here the array are sentences and the words are embeddings ,
embeddings have shape (100,)</p>

<p>Not all sentences have the same length I want all the sentences to have the same length I want to pad and trim how can I do it ?</p>
",Vectorization & Embeddings,pad n dimensional array matrix word embedding go like array sentence word embeddings embeddings shape sentence length want sentence length want pad trim
training word2vec from a webinar subtitles,"<p>I am building a model to start playback of video from a time frame , depending on the context. For example play climax of the movie, starts playing from 59th minute.</p>

<p>I am using subtitles of the video, and match the text in the particular sequence and identify the time frame. I am trying find exact words with real expressions $$not so effective$$. I came across word2vec which can find simialar words with cosine similarities</p>
",Vectorization & Embeddings,training word vec webinar subtitle building model start playback video time frame depending context example play climax movie start playing th minute using subtitle video match text particular sequence identify time frame trying find exact word real expression effective came across word vec find simialar word cosine similarity
How do I make a cluster with similar type of skills together?,"<p>Suppose that I have a file which has thousands of skills starting from A-Z. Now, I would like to create a model that can group similar skills together (example neural network and SVM can group together). I know that I can use NLP for this problem, but I'm not sure about the algorithm that I can use to get the best result.</p>

<p>I'm new to NLP so any help is greatly appreciated.</p>

<p>I was thinking at first to use semantic similarity. So I can use pre-trained word embeddings to map the words to a new vector space where I can calculate the distance between the word embeddings, e.g. with word2vec or other implementations. But I'm not sure about this. Can you give me some link or show me how do I do it so I can get a best result? Take a look at the data[1]: <a href=""https://i.sstatic.net/jGRI0.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/jGRI0.png</a></p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 36943 entries, 0 to 36942
Data columns (total 1 columns):
Skills    36942 non-null object
dtypes: object(1)
memory usage: 288.7+ KB
None
                   Skills
0                    .NET
1                .NET CLR
2  .NET Compact Framework
3          .NET Framework
4           .NET Remoting
</code></pre>
",Vectorization & Embeddings,make cluster similar type skill together suppose file ha thousand skill starting z would like create model group similar skill together example neural network svm group together know use nlp problem sure algorithm use get best result new nlp help greatly appreciated wa thinking first use semantic similarity use pre trained word embeddings map word new vector space calculate distance word embeddings e g word vec implementation sure give link show get best result take look data
Understanding embedding vectors dimension,"<p>In deep learning, in particularly NLP, words are transformed into a vector representation to be fed into a neural network such as an RNN. By referring to the link:</p>

<p><a href=""http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/#Word%20Embeddings"" rel=""nofollow noreferrer"">http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/#Word%20Embeddings</a></p>

<p>In the section of Word Embeddings, it is said that: </p>

<blockquote>
  <p>A word embedding W:words→Rn is a paramaterized function mapping words
  in 
      some language to high-dimensional vectors (perhaps 200 to 500 dimensions)</p>
</blockquote>

<p>I do not understand the <strong>purpose of the dimension of the vectors</strong>. What does it mean to have a <strong>vector of 200 dimensions</strong> compared to a <strong>vector of 20 dimensions</strong>?</p>

<p>Does it improve the overall accuracy of the model? Could anyone give me a simple example regarding the choice of dimension of the vectors.</p>
",Vectorization & Embeddings,understanding embedding vector dimension deep learning particularly nlp word transformed vector representation fed neural network rnn referring link section word embeddings said word embedding w word rn paramaterized function mapping word language high dimensional vector perhaps dimension understand purpose dimension vector doe mean vector dimension compared vector dimension doe improve overall accuracy model could anyone give simple example regarding choice dimension vector
Extract word/sentence probabilities from lm_1b trained model,"<p>I have successfully downloaded the 1B word language model trained using a CNN-LSTM (<a href=""https://github.com/tensorflow/models/tree/master/research/lm_1b"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/tree/master/research/lm_1b</a>), and I would like to be able to input sentences or partial sentences to get the probability of each subsequent word in the sentence.</p>

<p>For example, if I have a sentence like, ""An animal that says "", I'd like to know the probability of the next word being ""woof"" vs. ""meow"".</p>

<p>I understand that running the following produces the LSTM embeddings:</p>

<pre><code>bazel-bin/lm_1b/lm_1b_eval --mode dump_lstm_emb \
                           --pbtxt data/graph-2016-09-10.pbtxt \
                           --vocab_file data/vocab-2016-09-10.txt \
                           --ckpt 'data/ckpt-*' \
                           --sentence ""An animal that says woof"" \                             
                           --save_dir output
</code></pre>

<p>That will produce files <code>lstm_emb_step_*.npy</code> where each file is the LSTM embedding for each word in the sentence. How can I transform these into probabilities over the trained model to be able to compare <code>P(woof|An animal that says)</code> vs. <code>P(meow|An animal that says)</code>?</p>

<p>Thanks in advance.</p>
",Vectorization & Embeddings,extract word sentence probability lm b trained model successfully downloaded b word language model trained using cnn lstm would like able input sentence partial sentence get probability subsequent word sentence example sentence like animal say like know probability next word woof v meow understand running following produce lstm embeddings produce file file lstm embedding word sentence transform probability trained model able compare v thanks advance
Word embedding for OOV words,"<p>I have generated word vectors from a corpus, but I am facing out of vocabulary issues for many words. How can I generate word vectors for OOV words on the fly using existing word embedding?</p>
",Vectorization & Embeddings,word embedding oov word generated word vector corpus facing vocabulary issue many word generate word vector oov word fly using existing word embedding
Visualize a Word2Vec model using Embedding Projector,"<p>What is the best way to visualize a Word2Vec model using TensorFlow's Embedding Projector?
is there a way to export the Word2Vec model's vectors to the format that Embedding Projector expects? or is there a built in function in tensorflow for that?</p>

<p>Thanks!</p>
",Vectorization & Embeddings,visualize word vec model using embedding projector best way visualize word vec model using tensorflow embedding projector way export word vec model vector format embedding projector expects built function tensorflow thanks
What is exactly novel object captioning? And why 8 classes in MS-COCO are excluded in this task?,"<p>I am working with image captioning but I am having a hard time to understand precisely the term <code>novel object captioning</code>. Is this something that we trained a model on sentence-description pairs, and then apply to a dataset with objects that have never appeared in training process? I read this from <a href=""https://arxiv.org/pdf/1803.09845.pdf"" rel=""nofollow noreferrer"">Neural Baby Talk</a>. <a href=""https://arxiv.org/pdf/1511.05284.pdf"" rel=""nofollow noreferrer"">DCC</a> also states that in Introduction:</p>

<blockquote>
  <p>Existing state-of-the-art caption models lack the ability to
  form compositional structures which integrate new objects
  with known concepts without explicit examples of imagesentence pairs. To address this limitation, we propose
  the Deep Compositional Captioner (DCC) which can combine visual groundings of lexical units to generate descriptions about objects which are not present in caption corpora (paired image-sentence data), but are present in object
  recognition datasets (unpaired image data) and text corpora
  (unpaired text data).</p>
</blockquote>

<p>And they mentioned about 8 classes of MS-COCO are excluded for <code>novel object captioning</code> but I do not really understand the meaning of this step.</p>

<blockquote>
  <p>To ensure that excluded objects
  are at least similar to some included ones, we cluster the 80
  objects annotated in the MSCOCO segmentation challenge
  using the vectors from the word2vec embedding described
  in Section 3.4 and exclude one object from each cluster. The
  following words are chosen: “bottle”, “bus”, “couch”, “microwave”, “pizza”, “racket”, “suitcase”, and “zebra”.</p>
</blockquote>

<p>For the second question, I add here but expect deeper explanation: </p>

<blockquote>
  <p>To evaluate the ability of our approach to perform out-of-domain image
  captioning, we replicate an existing experimental design (Hendricks et
  al., 2016) using MSCOCO. Following this approach, all images with
  captions that mention one of eight selected objects (or their
  synonyms) are excluded from the image caption training set. This
  reduces the size of the caption training set from 82,783 images to
  70,194 images. However, the complete caption training set is tokenized
  as a bag of words per image, and made available as image tag training
  data. As such, the selected objects are unseen in the image caption
  training data, but not the image tag training data. The excluded
  objects, selected by Hendricks et. al. (2016) from the 80 main object
  categories in MSCOCO, are: ‘bottle’, ‘bus’, ‘couch’, ‘microwave’,
  ‘pizza’, ‘racket’,‘suitcase’ and ‘zebra’.</p>
</blockquote>

<p>Then now, how should I understand <code>out of domain image captioning</code>? Is it the same to <code>novel image captioning</code>?</p>

<p>Could someone experienced in image captioning help me clarify those questions? I would appreciate your help.</p>
",Vectorization & Embeddings,exactly novel object captioning class coco excluded task working image captioning hard time understand precisely term something trained model sentence description pair apply dataset object never appeared training process read neural baby talk dcc also state introduction existing state art caption model lack ability form compositional structure integrate new object known concept without explicit example imagesentence pair address limitation propose deep compositional captioner dcc combine visual grounding lexical unit generate description object present caption corpus paired image sentence data present object recognition datasets unpaired image data text corpus unpaired text data mentioned class coco excluded really understand meaning step ensure excluded object least similar included one cluster object annotated mscoco segmentation challenge using vector word vec embedding described section exclude one object cluster following word chosen bottle bus couch microwave pizza racket suitcase zebra second question add expect deeper explanation evaluate ability approach perform domain image captioning replicate existing experimental design hendricks et al using mscoco following approach image caption mention one eight selected object synonym excluded image caption training set reduces size caption training set image image however complete caption training set tokenized bag word per image made available image tag training data selected object unseen image caption training data image tag training data excluded object selected hendricks et al main object category mscoco bottle bus couch microwave pizza racket suitcase zebra understand could someone experienced image captioning help clarify question would appreciate help
determine most similar phrase using word2vec,"<p>I try create a model which determine the most similar sentence for another sentence using word2vec.</p>

<p>The idea is to determine the most similar for a sentence, I created an average vector for the words composed this sentence.</p>

<p>Then, I should to predict the most similar sentence using embedding words.
My question is: How can I determine the best similar target sentence after created an average vector of source sentence?</p>

<p>Here the code : </p>

<pre><code>import gensim
from gensim import utils
import numpy as np
import sys
from sklearn.datasets import fetch_20newsgroups
from nltk import word_tokenize
from nltk import download
from nltk.corpus import stopwords
import matplotlib.pyplot as plt

model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)


download('punkt') #tokenizer, run once
download('stopwords') #stopwords dictionary, run once
stop_words = stopwords.words('english')

def preprocess(text):
    text = text.lower()
    doc = word_tokenize(text)
    doc = [word for word in doc if word not in stop_words]
    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only
    return doc
############  doc content  -&gt; num label     -&gt; string label
#note to self: texts[XXXX] -&gt; y[XXXX] = ZZZ -&gt; ng20.target_names[ZZZ]

# Fetch ng20 dataset
ng20 = fetch_20newsgroups(subset='all',
                          remove=('headers', 'footers', 'quotes'))
# text and ground truth labels
texts, y = ng20.data, ng20.target

corpus = [preprocess(text) for text in texts]

def filter_docs(corpus, texts, labels, condition_on_doc):
    """"""
    Filter corpus, texts and labels given the function condition_on_doc which takes
    a doc.
    The document doc is kept if condition_on_doc(doc) is true.
    """"""
    number_of_docs = len(corpus)
    print(number_of_docs)

    if texts is not None:
        texts = [text for (text, doc) in zip(texts, corpus)
                 if condition_on_doc(doc)]

    labels = [i for (i, doc) in zip(labels, corpus) if condition_on_doc(doc)]
    corpus = [doc for doc in corpus if condition_on_doc(doc)]

    print(""{} docs removed"".format(number_of_docs - len(corpus)))

    return (corpus, texts, labels)

corpus, texts, y = filter_docs(corpus, texts, y, lambda doc: (len(doc) != 0))

def document_vector(word2vec_model, doc):
    # remove out-of-vocabulary words
    #print(""doc:"")
    #print(doc)
    doc = [word for word in doc if word in word2vec_model.vocab]
    return np.mean(word2vec_model[doc], axis=0)

def has_vector_representation(word2vec_model, doc):
    """"""check if at least one word of the document is in the
    word2vec dictionary""""""
    return not all(word not in word2vec_model.vocab for word in doc)

corpus, texts, y = filter_docs(corpus, texts, y, lambda doc: has_vector_representation(model, doc))

x =[]
for doc in corpus: #look up each doc in model

    x.append(document_vector(model, doc))


X = np.array(x) #list to array

model.most_similar(positive=X, topn=1)
</code></pre>
",Vectorization & Embeddings,determine similar phrase using word vec try create model determine similar sentence another sentence using word vec idea determine similar sentence created average vector word composed sentence predict similar sentence using embedding word question determine best similar target sentence created average vector source sentence code
ELMO embedding start session,"<p>I have an error when I apply the Elmo embedding to my data. I have 7255 sentences.</p>

<pre><code>embeddings = embed(
    sentences,
    signature=""default"",
    as_dict=True)['default']

#Start a session and run ELMo to return the embeddings in variable x
with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  sess.run(tf.tables_initializer())
  x = sess.run(embeddings)
</code></pre>

<p>The error is:</p>

<blockquote>
  <p>ResourceExhaustedError: OOM when allocating tensor with shape[36021075,50] and type int32 on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
       [[node module_apply_default/map/TensorArrayStack/TensorArrayGatherV3 (defined at C:\Users...\envs\tf_36\lib\site-packages\tensorflow_hub\native_module.py:547) ]]
  Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.</p>
</blockquote>
",Vectorization & Embeddings,elmo embedding start session error apply elmo embedding data sentence error resourceexhaustederror oom allocating tensor shape type int job localhost replica task device cpu allocator cpu node module apply default map tensorarraystack tensorarraygatherv defined c user envs tf lib site package tensorflow hub native module py hint want see list allocated tensor oom happens add report tensor allocation upon oom runoptions current allocation info
Is Word2Vec itself a discriminative model or a generative model?,"<p>I am wondering whether Word2Vec itself is a discriminative model or generative model? </p>

<p>Both CBOW and Skip-gram aim at maximizing likelihood function corresponding to conditional probabilities between tokens and their contexts. Only focusing on the network and the training process, I suppose it follows a discriminative approach. </p>

<p>However, the word embedding is kind of a by-product, which depicts the relationships between tokens in the training corpus. Considering a Word2Vec model, which is trained with a segmented corpus, takes in a token and outputs its embedding, we often say that 'it generates a word embedding'. Is a Word2Vec model a discriminative one or a generative one? </p>

<p>I have met with some trouble formulating it.</p>
",Vectorization & Embeddings,word vec discriminative model generative model wondering whether word vec discriminative model generative model cbow skip gram aim maximizing likelihood function corresponding conditional probability token context focusing network training process suppose follows discriminative approach however word embedding kind product depicts relationship token training corpus considering word vec model trained segmented corpus take token output embedding often say generates word embedding word vec model discriminative one generative one met trouble formulating
Replacing empty texts - text embedding,"<p>I am trying to embed texts, using pre-trained fastText models. Some are empty. How would one replace them to make embedding possible? I was thinking about replacing them with dummy words, like that (docs being a pandas DataFrame object):
        <code>docs = docs.replace(np.nan, 'unknown', regex=True)</code></p>

<p>However it doesn't really make sense as the choice of this word is arbitrary and it is not equivalent to having an empty string.</p>

<p>Otherwise, I could associate the 0 vector embedding to empty strings, or the average vector, but I am not convinced either would make sense, as the embedding operation is non-linear.</p>
",Vectorization & Embeddings,replacing empty text text embedding trying embed text using pre trained fasttext model empty would one replace make embedding possible wa thinking replacing dummy word like doc panda dataframe object however really make sense choice word arbitrary equivalent empty string otherwise could associate vector embedding empty string average vector convinced either would make sense embedding operation non linear
How do you create Bag-of-Words feature vector after applying GloVe embedding?,"<p>If I have two movie reviews:</p>

<pre><code>""this was a really good movie"" and ""i did not like this movie at all""
</code></pre>

<p>And I apply GloVe embedding to them I will get two vectors, with multiple word vectors inside them that look like this:</p>

<pre><code>1st Review: [[300 Floats],[300 Floats],[300 Floats],[300 Floats],[300 Floats],[300 Floats]] 
2nd Review: [[300 Floats],[300 Floats],[300 Floats],[300 Floats],[300 Floats],[300 Floats],[300 Floats],[300 Floats]]
</code></pre>

<p>Basically each word in the review will be converted into a 300 element array of float point numbers. Since the arrays are of variable length I can't just plug them into a classifier.</p>

<p>I thought about doing some kind of Bag-of-Words representation, but I am not sure how I would implement that now that the words have become numbers.</p>
",Vectorization & Embeddings,create bag word feature vector applying glove embedding two movie review apply glove embedding get two vector multiple word vector inside look like basically word review converted element array float point number since array variable length plug classifier thought kind bag word representation sure would implement word become number
Character level CNN - 1D or 2D,"<p>I want to implement a character-level CNN in Pytorch. </p>

<p>My input has <strong>4</strong> dimensions:</p>

<blockquote>
  <p><em>(batch_size, seq_length, padded_character_length, embedding_dim)</em></p>
</blockquote>

<p>I'm wondering whether I should merge two dimensions and use a <code>Conv1D</code>-layer or instead use a <code>Conv2D</code>-layer on the existing dimensions.</p>

<p>Given the dimensions of the input both would <em>technically</em> work fine, I also have seen implementations for both versions. So I'm wondering which method to prefer.</p>

<p>Does one of the two methods have particular advantages over the other?</p>
",Vectorization & Embeddings,character level cnn want implement character level cnn pytorch input ha dimension batch size seq length padded character length embedding dim wondering whether merge two dimension use layer instead use layer existing dimension given dimension input would technically work fine also seen implementation version wondering method prefer doe one two method particular advantage
How to pass word2vec embedding as a Keras Embedding layer?,"<p>I am solving a multi-class classification problem using Keras. But I am assuming the accuracy is bad due to poor word embedding of my data (domain-specific data).</p>

<p>Keras has its own Embedding layer, which is a supervised learning method.</p>

<p>So I have 2 questions regarding this :</p>

<ol>
<li><p>Can I use word2vec embedding in Embedding layer of Keras, because word2vec is a form of unsupervised learning/self-supervised?</p></li>
<li><p>If yes, then can I use transfer learning on word2vec pre-train model to put extra knowledge of my domain specific features.</p></li>
</ol>
",Vectorization & Embeddings,pas word vec embedding kera embedding layer solving multi class classification problem using kera assuming accuracy bad due poor word embedding data domain specific data kera ha embedding layer supervised learning method question regarding use word vec embedding embedding layer kera word vec form unsupervised learning self supervised yes use transfer learning word vec pre train model put extra knowledge domain specific feature
How to interpret TfidfVectorizer output,"<p>I am doing sentiment analysis and for feature generation from text, I am using TF-IDF method but I am not able interpret the output.</p>

<p>I have used the TfidfVectorizer function from Sklearn.</p>

<p>I have used the below code:</p>

<p>from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')
tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])</p>

<p>The output is below:</p>

<p>(0, 302)  0.46871135687055143
  (0, 463)  0.5896490179849546
  (0, 738)  0.6577413621857342
  (1, 879)  0.3938403468675415
  (1, 131)  0.6145629375807904
  (1, 600)  0.6835218920644196
  (2, 79)   1.0
  (3, 557)  0.7040384885805177
  (3, 518)  0.44016705593507854
  (3, 888)  0.5572995329862621
  (4, 566)  1.0
  (5, 423)  0.586120951905663
  (5, 858)  0.4743403266916206
  (5, 69)   0.4637175931713698
  (5, 485)  0.4652198168550412
  (6, 121)  0.809676118019697
  (6, 894)  0.5868769751051355
  (7, 749)  0.47546741144240784
  (7, 992)  0.40382612331421974
  (7, 283)  0.6221668428341786
  (7, 883)  0.20713435439054187
  (7, 393)  0.22953868678391207
  (7, 432)  0.29836739781603</p>

<p>I can understand that the last column is TF-IDF value but what are the other columns .</p>
",Vectorization & Embeddings,interpret tfidfvectorizer output sentiment analysis feature generation text using tf idf method able interpret output used tfidfvectorizer function sklearn used code sklearn feature extraction text import tfidfvectorizer tfidf vectorizer tfidfvectorizer max df min df max feature stop word english tfidf tfidf vectorizer fit transform combi tidy tweet output understand last column tf idf value column
How to evaluate performance of Word2Vec?,"<p>I want to know an effective way to evaluate my word2Vec model performance so that I can properly tune my hyper-parameters. </p>

<p>For example, if I were to do document classification with supervised learning, the model performance evaluation is easy, because I can just compare the predicted label to the pre-defined label for test data set. </p>

<p>But I'm not sure how to do so with Word2Vec. Can anyone explain how it can be done, with codes, or provide a link to a page that does this?</p>

<p>Please don't put a link to a paper... I'm really tired of reading difficult papers.</p>
",Vectorization & Embeddings,evaluate performance word vec want know effective way evaluate word vec model performance properly tune hyper parameter example document classification supervised learning model performance evaluation easy compare predicted label pre defined label test data set sure word vec anyone explain done code provide link page doe please put link paper really tired reading difficult paper
LDA vs Word2Vec. Which is the right solution for predicting recipients of a message?,"<p>I'm investigating various NLP algorithms and tools to solve the following problem; NLP newbie here, so pardon my question if it's too basic.</p>

<p>Let's say, I have a messaging app where users can send text messages to one or more people. When the user types a message, I want the app to suggest to the user who the potential recipients of the message are?</p>

<p>If user ""A"" sends a lot of text messages regarding ""cats"" to user ""B"" and some messages to user ""C"" and sends a lot of messages regarding ""politics"" to user ""D"", then next time user types the message about ""cats"" then the app should suggest ""B"" and ""C"" instead of ""D"".</p>

<p>So I'm doing some research on topic modeling and word embeddings and see that LDA and Word2Vec are the 2 probable algorithms I can use.</p>

<p>Wanted to pick your brain on which one you think is more suitable for this scenario.</p>

<p>One idea I have is, extract topics using LDA from the previous messages and rank the recipients of the messages based on the # of times a topic has been discussed (ie, the message sent) in the past. If I have this mapping of the topic and a sorted list of users who you talk about it (ranked based on frequency), then when the user types a message, I can again run topic extraction on the message, predict what the message is about and then lookup the mapping to see who can be the possible recipients and show to user.</p>

<p>Is this a good approach? Or else, Word2Vec (or doc2vec or lda2vec) is better suited for this problem where we can predict similar messages using vector representation of words aka word embeddings? Do we really need to extract topics from the messages to predict the recipients or is that not necessary here? Any other algorithms or techniques you think will work the best?</p>

<p>What are your thoughts and suggestions?</p>

<p>Thanks for the help.</p>
",Vectorization & Embeddings,lda v word vec right solution predicting recipient message investigating various nlp algorithm tool solve following problem nlp newbie pardon question basic let say messaging app user send text message one people user type message want app suggest user potential recipient message user sends lot text message regarding cat user b message user c sends lot message regarding politics user next time user type message cat app suggest b c instead research topic modeling word embeddings see lda word vec probable algorithm use wanted pick brain one think suitable scenario one idea extract topic using lda previous message rank recipient message based time topic ha discussed ie message sent past mapping topic sorted list user talk ranked based frequency user type message run topic extraction message predict message lookup mapping see possible recipient show user good approach else word vec doc vec lda vec better suited problem predict similar message using vector representation word aka word embeddings really need extract topic message predict recipient necessary algorithm technique think work best thought suggestion thanks help
Casing removable from word embeddings?,"<p>Many words have multiple vectors because they are used with multiple casings. It seems to be quite unusual, though, that a word's casing alters its meaning. Can I combine vectors such that there's only one vector per word?</p>

<p>I was thinking of using a weighted geometric midpoint. The weighting would be a simple linear transformation that depends on the different word frequencies. How could I show that the new vector is valid, or that it is a good representation? I'm sure there must be other approaches to this.</p>
",Vectorization & Embeddings,casing removable word embeddings many word multiple vector used multiple casing seems quite unusual though word casing alters meaning combine vector one vector per word wa thinking using weighted geometric midpoint weighting would simple linear transformation depends different word frequency could show new vector valid good representation sure must approach
What&#39;s the different between fasttext skipgram and word2vec skipgram?,"<p>Given a sentence 'hello world', the vocabulary is </p>

<p>{hello, world} + {&lt;hel, hell, ello, llo>, &lt;wor, worl, orld, rld>}, </p>

<p>for convenience, just list all 4-gram.</p>

<p>In my comprehension, the word2vec skipgram will maximize</p>

<p><img src=""https://latex.codecogs.com/gif.latex?%5Cdpi%7B200%7D&space;P(hello%5Cvert&space;world)&space;&plus;&space;P(world%5Cvert&space;hello)"" title=""P(hello\vert world) + P(world\vert hello)"" /></p>

<p>What will fasttext skipgram do?</p>
",Vectorization & Embeddings,different fasttext skipgram word vec skipgram given sentence hello world vocabulary hello world hel hell ello llo wor worl orld rld convenience list gram comprehension word vec skipgram maximize fasttext skipgram
word2vec lemmatization of corpus before training,"<p>Word2vec seems to be mostly trained on raw corpus data. However, lemmatization is a standard preprocessing for many semantic similarity tasks. I was wondering if anybody had experience in lemmatizing the corpus before training word2vec and if this is a useful preprocessing step to do.</p>
",Vectorization & Embeddings,word vec lemmatization corpus training word vec seems mostly trained raw corpus data however lemmatization standard preprocessing many semantic similarity task wa wondering anybody experience lemmatizing corpus training word vec useful preprocessing step
Manage KeyError with gensim and pretrained word2vec model,"<p>I pretrained a word embedding using wang2vec (<a href=""https://github.com/wlin12/wang2vec"" rel=""nofollow noreferrer"">https://github.com/wlin12/wang2vec</a>), and i loaded it in python through gensim. When i tried to get the vector of some words not in vocabulary, i obviously get:</p>

<pre class=""lang-py prettyprint-override""><code>KeyError: ""word 'kjklk' not in vocabulary""
</code></pre>

<p>So, i thought about adding an item to the vocabulary to map oov (out of vocabulary) words, let's say <code>&lt;OOV&gt;</code>. Since the vocabulary is in <code>Dict</code> format, i would simply add the item <code>{""&lt;OOV&gt;"":0}</code>. </p>

<p>But, i searched an item of the vocabulary, with</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.KeyedVectors.load_word2vec_format(w2v_ext, binary=False, unicode_errors='ignore')
dict(list(model.vocab.items())[5:6])
</code></pre>

<p>The output was something like</p>

<pre class=""lang-py prettyprint-override""><code>{'word': &lt;gensim.models.keyedvectors.Vocab at 0x7fc5aa6007b8&gt;}
</code></pre>

<p>So, is there a way to add the <code>&lt;OOV&gt;</code> token to the vocabulary of a pretrained word embedding loaded through gensim, and avoid the KeyError? I looked at gensim doc and i found this: <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.build_vocab"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.build_vocab</a>
but it seems not work with the update parameter.</p>
",Vectorization & Embeddings,manage keyerror gensim pretrained word vec model pretrained word embedding using wang vec loaded python gensim tried get vector word vocabulary obviously get thought adding item vocabulary map oov vocabulary word let say since vocabulary format would simply add item searched item vocabulary output wa something like way add token vocabulary pretrained word embedding loaded gensim avoid keyerror looked gensim doc found seems work update parameter
How to evaluate performance of Co-occurrence matrix,"<p>I am learning the application of co-occurrence matrix as an alternative to Word2Vec. This <a href=""https://pdfs.semanticscholar.org/73e6/351a8fb61afc810a8bb3feaa44c41e5c5d7b.pdf"" rel=""nofollow noreferrer"">paper</a> talks about how to improve the performance of co-occurrence matrix word embedding, and shows us the tables and graphs to visualize the improved performance of his model.</p>

<p>But I don't understand how the evaluation is done. His evaluation methods are explained in page 10 and 11. Can anyone explain how the values for Table 8, Table 9 and Figure 2 in this paper are obtained?</p>
",Vectorization & Embeddings,evaluate performance co occurrence matrix learning application co occurrence matrix alternative word vec paper talk improve performance co occurrence matrix word embedding show u table graph visualize improved performance model understand evaluation done evaluation method explained page anyone explain value table table figure paper obtained
LDA Topic Model Performance - Topic Coherence Implementation for scikit-learn,"<p>I have a question around measuring/calculating topic coherence for LDA models built in scikit-learn. </p>

<p>Topic Coherence is a useful metric for measuring the human interpretability of a given LDA topic model. Gensim's <a href=""https://radimrehurek.com/gensim/models/coherencemodel.html"" rel=""noreferrer"">CoherenceModel</a> allows Topic Coherence to be calculated for a given LDA model (several variants are included). </p>

<p>I am interested in leveraging <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"" rel=""noreferrer"">scikit-learn's LDA</a> rather than <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""noreferrer"">gensim's LDA</a> for ease of use and documentation (<em>note: I would like to avoid using the gensim to scikit-learn wrapper i.e. actually leverage sklearn’s LDA</em>). From my research, there is seemingly no scikit-learn equivalent to Gensim’s CoherenceModel. </p>

<p>Is there a way to either:</p>

<p><strong>1</strong> - Feed scikit-learn’s LDA model into gensim’s CoherenceModel pipeline, either through manually converting the scikit-learn model into gensim format or through a scikit-learn to gensim wrapper (I have seen the wrapper the other way around) to generate Topic Coherence?</p>

<p>Or</p>

<p><strong>2</strong> - Manually calculate topic coherence from scikit-learn’s LDA model and CountVectorizer/Tfidf matrices?</p>

<p>I have done quite a bit of research on implementations for this use case online but haven’t seen any solutions. The only leads I have are the documented equations from scientific literature.</p>

<p>If anyone has any knowledge on any similar implementations, or if you could point me in the right direction for creating a manual method for this, that would be great. Thank you!</p>

<p>*Side note: I understand that perplexity and log-likelihood are available in scikit-learn for performance measurements, but these are not as predictive from what I have read.</p>
",Vectorization & Embeddings,lda topic model performance topic coherence implementation scikit learn question around measuring calculating topic coherence lda model built scikit learn topic coherence useful metric measuring human interpretability given lda topic model gensim coherencemodel allows topic coherence calculated given lda model several variant included interested leveraging scikit learn lda rather gensim lda ease use documentation note would like avoid using gensim scikit learn wrapper e actually leverage sklearn lda research seemingly scikit learn equivalent gensim coherencemodel way either feed scikit learn lda model gensim coherencemodel pipeline either manually converting scikit learn model gensim format scikit learn gensim wrapper seen wrapper way around generate topic coherence manually calculate topic coherence scikit learn lda model countvectorizer tfidf matrix done quite bit research implementation use case online seen solution lead documented equation scientific literature anyone ha knowledge similar implementation could point right direction creating manual method would great thank side note understand perplexity log likelihood available scikit learn performance measurement predictive read
Using word2vec in a sentence,"<p>I'm trying to generate the probability of a given sentence to be be correct. </p>

<p>I have word2vec for each token in the language and I want to predict the probability of the sentence to be correct. I'm unable to create a suitable model. How can I proceed ?</p>
",Vectorization & Embeddings,using word vec sentence trying generate probability given sentence correct word vec token language want predict probability sentence correct unable create suitable model proceed
May I run only half of a neural network model when doing prediction?,"<p>I am trying to use the Deep-Semantic-Similarity-Model to calculate query and document similarity in document search. I want to compute all the documents' embedding vectors offline/beforehand, to accomplish that means after training, I have to run only half of the model to predict/calculate the documents' embedding vectors. But I don't know how to do that? Is it possible in <code>keras</code> or <code>tensorflow</code>?
e.g. using <a href=""https://github.com/airalcorn2/Deep-Semantic-Similarity-Model"" rel=""nofollow noreferrer"">this</a> project </p>
",Vectorization & Embeddings,may run half neural network model prediction trying use deep semantic similarity model calculate query document similarity document search want compute document embedding vector offline beforehand accomplish mean training run half model predict calculate document embedding vector know possible e g using project
How can I count word frequencies in Word2Vec&#39;s training model?,"<p>I need to count the frequency of each word in <code>word2vec</code>'s training model. I want to have output that looks like this: </p>

<pre><code>term    count
apple   123004
country 4432180
runs    620102
...
</code></pre>

<p>Is it possible to do that? How would I get that data out of word2vec? </p>
",Vectorization & Embeddings,count word frequency word vec training model need count frequency word training model want output look like possible would get data word vec
How do i input doc2vec vectors combined with categorical features in a CNN to predict a class?,"<p>I have a dataset with 2 features , one is a text feature and the other is a categorical feature. I want to predict the category based on these two features. I am using doc2vec to convert the text column into vectors and one hot encoding the categorical feature . After converting both the column, I don't know how to combine both of these features and feed into a neural network, say a CNN . Please guide me what approach should I take? I am new to this, please don't mind the shortcomings.</p>

<p>My dataset and some of the code:</p>

<p><a href=""https://i.sstatic.net/YYC3h.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YYC3h.png"" alt=""enter image description here""></a></p>
",Vectorization & Embeddings,input doc vec vector combined categorical feature cnn predict class dataset feature one text feature categorical feature want predict category based two feature using doc vec convert text column vector one hot encoding categorical feature converting column know combine feature feed neural network say cnn please guide approach take new please mind shortcoming dataset code
How to predict whether the given sentence is grammatically correct or not?,"<p>I am trying to create a predictive model where the model tells whether the give sentence is correct or not by checking the order of the words in the sentence. The model checks weather the particular sequence of words as already occurred in a huge corpus and makes sense or no. </p>

<p>I tried doing this with the word2vec model and removed the cosine similarity or WMD distance of the two sentences but that only gives the similarity based on the word vector similarity and not the sequence of the words.</p>

<p>So if we give the input as 2 sentences:</p>

<p>Sentence 1- ""I am going to the shop""</p>

<p>Sentence 2- ""going I am the shop to""</p>

<p>output should indicate that the sentence is invalid or with a similarity of 20% or less</p>

<p>Whereas the word2vec model shows 100% similarity as the words entered are same irrespective of the order. So i guess it cannot be used for comparing the word order. Any other suggestions could also be very helpful.</p>
",Vectorization & Embeddings,predict whether given sentence grammatically correct trying create predictive model model tell whether give sentence correct checking order word sentence model check weather particular sequence word already occurred huge corpus make sense tried word vec model removed cosine similarity wmd distance two sentence give similarity based word vector similarity sequence word give input sentence sentence going shop sentence going shop output indicate sentence invalid similarity le whereas word vec model show similarity word entered irrespective order guess used comparing word order suggestion could also helpful
What are the state-of-art algorithms for resolving words polysemy/homonymy?,"<p>I tried to solve the word-polysemy problem (fix WordNet-synsets for polysemy words in the text) via word2vec-like neural networks (<a href=""https://stackoverflow.com/questions/51330549/using-word2vec-for-polysemy-solving-problems"">Using Word2Vec for polysemy solving problems</a>), but it give too poor results.
What are other state-of-art algorithms for resolving words polysemy/homonymy? Can you give me some articles?</p>
",Vectorization & Embeddings,state art algorithm resolving word polysemy homonymy tried solve word polysemy problem fix wordnet synset polysemy word text via word vec like neural network href word vec polysemy solving problem give poor result state art algorithm resolving word polysemy homonymy give article
Why use the first token of a line as the sentence vector,"<p>In Tomas Mikolov's doc2vec implementation, the first token of the sentence is used as the sentence vector. But I don't know if this will take up the first token and affect its word vector. 
I consider setting a sentence vector separately for each sentence, which is equivalent to a word, just like a word in the context window with an offset of -1 or at the end.And it is also initialized like other words.
I am not sure if this is correct. Maybe I have not understood the method of Tomas Mikolov? Hope to hear from you。</p>
",Vectorization & Embeddings,use first token line sentence vector tomas mikolov doc vec implementation first token sentence used sentence vector know take first token affect word vector consider setting sentence vector separately sentence equivalent word like word context window offset end also initialized like word sure correct maybe understood method tomas mikolov hope hear
Which order matters comparing text sequences?,"<p>I want to compare the similarity in some texts to detect duplicates, but if i use difflib, it returns different ratios depending on the order i give the data.</p>

<p>Some random example ....</p>

<p>Thanks </p>

<pre><code>import difflib


a='josephpFRANCES'
b='ABswazdfsadSASAASASASAS'

seq=difflib.SequenceMatcher(None,a,b)
d=seq.ratio()*100
print(d)

seq2=difflib.SequenceMatcher(None,b,a)
d2=seq2.ratio()*100
print(d2)

</code></pre>

<p>d  = 16.216216216216218</p>

<p>d2 = 10.81081081081081</p>
",Vectorization & Embeddings,order matter comparing text sequence want compare similarity text detect duplicate use difflib return different ratio depending order give data random example thanks
What is wrong with a naive implementation of cosine similarity?,"<p>In a <a href=""https://masongallo.github.io/machine/learning,/python/2016/07/29/cosine-similarity.html"" rel=""nofollow noreferrer"">blog post</a> I read that the following ""naive implementation"" of cosine similarity should never be used in production, the blog post didn't explain why and I am really curious, can anyone give an explanation?</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np

def cos_sim(a, b):
    """"""Takes 2 vectors a, b and returns the cosine similarity according 
    to the definition of the dot product
    """"""
    dot_product = np.dot(a, b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    return dot_product / (norm_a * norm_b)

# the counts we computed above
sentence_m = np.array([1, 1, 1, 1, 0, 0, 0, 0, 0]) 
sentence_h = np.array([0, 0, 1, 1, 1, 1, 0, 0, 0])
sentence_w = np.array([0, 0, 0, 1, 0, 0, 1, 1, 1])

# We should expect sentence_m and sentence_h to be more similar
print(cos_sim(sentence_m, sentence_h)) # 0.5
print(cos_sim(sentence_m, sentence_w)) # 0.25
</code></pre>
",Vectorization & Embeddings,wrong naive implementation cosine similarity blog post read following naive implementation cosine similarity never used production blog post explain really curious anyone give explanation
What do input layers represent in a Hierarchical Attention Network,"<p>I'm trying to grasp the idea of a Hierarchical Attention Network (HAN), most of the code i find online is more or less similar to the one here: <a href=""https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f"" rel=""nofollow noreferrer"">https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f</a> :</p>

<pre><code>embedding_layer=Embedding(len(word_index)+1,EMBEDDING_DIM,weights=[embedding_matrix],
input_length=MAX_SENT_LENGTH,trainable=True)
sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32', name='input1')
embedded_sequences = embedding_layer(sentence_input)
l_lstm = Bidirectional(LSTM(100))(embedded_sequences)
sentEncoder = Model(sentence_input, l_lstm)

review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32',  name='input2')
review_encoder = TimeDistributed(sentEncoder)(review_input)
l_lstm_sent = Bidirectional(LSTM(100))(review_encoder)
preds = Dense(len(macronum), activation='softmax')(l_lstm_sent)
model = Model(review_input, preds)
</code></pre>

<p>My question is: What do the input layers here represent? I'm guessing that input1 represents the sentences wrapped with the embedding layer, but in that case what is input2? Is it the output of the sentEncoder? In that case it should be a float, or if it's another layer of embedded words, then it should be wrapped with an embedding layer as well.</p>
",Vectorization & Embeddings,input layer represent hierarchical attention network trying grasp idea hierarchical attention network han code find online le similar one question input layer represent guessing input represents sentence wrapped embedding layer case input output sentencoder case float another layer embedded word wrapped embedding layer well
Doc2vec predictions - do we average the words or what is the paragraph ID for a new paragraph?,"<p>I understand that you treat the paragraph ID as a new word in doc2vec (DM approach, left on the figure) during training. The training output is the context word. After a model is trained, suppose I want to get 1 embedding given a new document. </p>

<p>Do I feed each word to the network and then average it to get the embedding? Or is there another way? </p>

<p>I can feed this to gensim, but I am trying to understand how it works. </p>

<p><a href=""https://i.sstatic.net/t7slV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/t7slV.png"" alt=""enter image description here""></a></p>
",Vectorization & Embeddings,doc vec prediction average word paragraph id new paragraph understand treat paragraph id new word doc vec dm approach left figure training training output context word model trained suppose want get embedding given new document feed word network average get embedding another way feed gensim trying understand work
Semantic similarity between words A and B : Dependency on frequency of A and B in corpus?,"<p><strong>Background :</strong></p>

<p>Given a corpus I want to train it with an implementation of word2wec (Gensim). </p>

<p>Want to understand if the final similarity between 2 tokens is dependent on the frequency of A and B in the corpus <em>(all contexts preserved)</em>, or agnostic of it.</p>

<p><em>Example</em>: 
(May not be ideal, but using it to elaborate the problem statement) </p>

<p>Suppose word 'A' is being used in 3 different contexts within the corpus : </p>

<pre><code>Context 1 : 1000 times
Context 2 : 50000 times
Context 3 : 50000 times
</code></pre>

<p>'B' is being used in 2 different contexts :</p>

<pre><code>Context 1 : 300 times 
Context 5 : 1000 time
</code></pre>

<p><strong>Question :</strong> </p>

<p>If I change the frequency of 'A' in my corpus (ensuring no context is lost, i.e. 'A' is still being used at least once in all the contexts as in the original corpus), is the similarity between A snd B going to be the same ?</p>

<p>New distribution of 'A' across contexts</p>

<pre><code> Context 1 : 5 times
 Context 2 : 10 times
 Context 3 : 5000 times
</code></pre>

<p>Any leads appreciated</p>
",Vectorization & Embeddings,semantic similarity word b dependency frequency b corpus background given corpus want train implementation word wec gensim want understand final similarity token dependent frequency b corpus context preserved agnostic example may ideal using elaborate problem statement suppose word used different context within corpus b used different context question change frequency corpus ensuring context lost e still used least context original corpus similarity snd b going new distribution across context lead appreciated
"I used pack_padded_sequence() and put in lstm layer, but I got start () + length () exceeds dimension size (). error","<p>In my model, there are a embedding layer, a conv1d layer, a lstm layer. And I'm using Pytorch.
My question is: 
When I put <code>pack = pack_padded_sequence(conv)</code> in the lstm layer, I got <code>RuntimeError: start (pack[0].size(0)) + length (1) exceeds dimension size (pack[0].size(0)).</code> error. Why I got this error? And What does mean?? Please help me..</p>

<pre class=""lang-py prettyprint-override""><code>class CNNModel(nn.Module):
    def __init__(self, vocab_size: int, embed_size: int, out_size: int, filter_size: List[int], hidden_size: int,
                 layer_num: int, pad: int = 1):
        super().__init__()
        self.vocab_size = vocab_size
        self.embed_size = embed_size
        self.pad = pad
        self.out_size = out_size
        self.filter_size = filter_size
        self.hidden_size = hidden_size
        self.layer_num = layer_num

        self.embed = nn.Embedding(
            num_embeddings=self.vocab_size,
            embedding_dim=self.embed_size,
            padding_idx=self.pad,
        )

        self.cnn = nn.Conv1d(
            in_channels=self.embed_size,
            out_channels=self.out_size,
            kernel_size=self.filter_size[0],
            bias=True,
        )

        self.lstm = nn.LSTM(
            input_size=self.out_size,
            hidden_size=self.hidden_size,
            num_layers=self.layer_num,
            bidirectional=True,
            batch_first=True,
        )

    def forward(self, batch):
        embed = self.embed(batch.word[0])  # [B(64), L(465), F(256)]
        embed = embed.transpose(1, 2)  # [B(64), L(256), F(465)]

        conv = self.cnn(embed)  # [B(64), F(64), L(465)]
        conv = conv.transpose(1, 2)  # [B(64), L(465), F(64)]

        encoding = pack_padded_sequence(conv,
                                        [tensor.item() for tensor in batch.word[1]],
                                        batch_first=True)
        print(f'encoding =&gt; {encoding[0].size()}')  # [8093, 64]
                print(f'encoding =&gt; {encoding[1].size()}')  # [465]
        _, (h, _) = self.lstm(encoding)  # [bsz, sln, dim3]
        print(f'h =&gt; {h}')
</code></pre>

<p>And result like below.</p>

<pre><code> File ""/Users/user_name/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py"", line 182, in forward
    self.num_layers, self.dropout, self.training, self.bidirectional)
RuntimeError: start (8093) + length (1) exceeds dimension size (8093).
</code></pre>
",Vectorization & Embeddings,used pack padded sequence put lstm layer got start length exceeds dimension size error model embedding layer conv layer lstm layer using pytorch question put lstm layer got error got error doe mean please help result like
Sequence Labelling at paragraph/sentence embedding level using Bi-LSTM + CRF with Keras,"<p>I am working on a Sequence tagging task where the element to be tagged is  sentences (or paragraph). Most of implementations I found present solution at the token level (NER, POS-Tagging, etc.), whereas here, I first need to build a paragraph embedding before doing sequence labelling.</p>

<p>There is actually two steps: 
the first model is building a dense representation (with attention) from the word embeddings 
First model takes the input (#batch size, #words in a paragraph)</p>

<p>the second model takes has input the following shape: 
- (#batch size, #paragraphs, #words)
is taking the output of the first model and apply the Bi-LSTM + CRF architecture on top of it to tag every paragraph of a given document. </p>

<pre class=""lang-py prettyprint-override""><code>import keras 
import keras.backend as K
from keras.layers import *
from keras.activations import *
from keras.regularizers import *
from keras.initializers import *

from keras.models import Model
from keras.layers import Embedding, Input, Bidirectional, GRU, LSTM, TimeDistributed, Layer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.text import Tokenizer, text_to_word_sequence
from keras_contrib.layers import CRF
from keras_contrib.metrics import crf_accuracy
from keras_contrib.losses import crf_loss
from keras_contrib.metrics import crf_viterbi_accuracy

MAX_PARAGRAPHS = 200 # number of max paragraphs (ie. sequences)
MAX_PARAGRAPH_LENGTH = 60 # number of max token in a given sequences 
EMBEDDING_DIM = 100 # word embedding dimension 
PARAGRAPH_DIM = 100 # paragraph/sentence embedding dimension
MAX_NB_WORDS = 5000 # max nb words in the vocabulary 

n_tags = 5 #number of classes

class AttLayer(Layer):
    def __init__(self, attention_dim):
        self.init = initializers.get('normal')
        self.supports_masking = True
        self.attention_dim = attention_dim
        super(AttLayer, self).__init__()

    def build(self, input_shape):
        assert len(input_shape) == 3
        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))
        self.b = K.variable(self.init((self.attention_dim, )))
        self.u = K.variable(self.init((self.attention_dim, 1)))
        self.trainable_weights = [self.W, self.b, self.u]
        super(AttLayer, self).build(input_shape)

    def compute_mask(self, inputs, mask=None):
        return mask

    def call(self, x, mask=None):
        # size of x :[batch_size, sel_len, attention_dim]
        # size of u :[batch_size, attention_dim]
        # uit = tanh(xW+b)
        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))
        ait = K.dot(uit, self.u)
        ait = K.squeeze(ait, -1)

        ait = K.exp(ait)

        if mask is not None:
            # Cast the mask to floatX to avoid float64 upcasting in theano
            ait *= K.cast(mask, K.floatx())
        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())
        ait = K.expand_dims(ait)
        weighted_input = x * ait
        output = K.sum(weighted_input, axis=1)

        return output

    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[-1])



tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(flatten_texts)

vocab_size = len(tokenizer.word_index) + 1
embedding_layer = Embedding(vocab_size,
                            EMBEDDING_DIM,
                            #weights=[embedding_matrix],
                            input_length=MAX_PARAGRAPH_LENGTH,
                            trainable=True,
                            mask_zero=True)

paragraph_input = Input(shape=(MAX_PARAGRAPH_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(paragraph_input)
l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)
l_att = AttLayer(60)(l_lstm)
paragraph_encoder = Model(paragraph_input, l_att)

sequence_input = Input(shape=(MAX_PARAGRAPHS, MAX_PARAGRAPH_LENGTH), dtype='int32')

parag_encoder = TimeDistributed(paragraph_encoder)(sequence_input)
bi_lstm = Bidirectional(LSTM(units=128, return_sequences=True,recurrent_dropout=0.2))(parag_encoder)  # variational biLSTM
final_dense = TimeDistributed(Dense(128, activation=""relu""))(bi_lstm)
crf = CRF(n_tags, sparse_target=True)  # CRF layer
out = crf(final_dense)  # output
model = Model(sequence_input, out)
model.compile(optimizer=""rmsprop"", loss=crf_loss, metrics=[crf_accuracy])
</code></pre>

<p>compiling the model leads to following error: </p>

<p>AssertionError: Input mask to CRF must have dim 2 if not None</p>

<p>I am using CRF layer from keras_contrib package</p>

<p>Could you please tell me what I'm doing wrong ?</p>
",Vectorization & Embeddings,sequence labelling paragraph sentence embedding level using bi lstm crf kera working sequence tagging task element tagged sentence paragraph implementation found present solution token level ner po tagging etc whereas first need build paragraph embedding sequence labelling actually two step first model building dense representation attention word embeddings first model take input batch size word paragraph second model take ha input following shape batch size paragraph word taking output first model apply bi lstm crf architecture top tag every paragraph given document compiling model lead following error assertionerror input mask crf must dim none using crf layer kera contrib package could please tell wrong
In Keras elmo embedding layer has 0 parameters? is this normal?,"<p>So I was using GloVe with my model and it worked, but now I changed to Elmo (reference that Keras code available on GitHub <a href=""https://github.com/JHart96/keras_elmo_embedding_layer/blob/master/example_word_level.py"" rel=""nofollow noreferrer"">Elmo Keras Github</a>, <a href=""https://github.com/JHart96/keras_elmo_embedding_layer/blob/master/utils.py"" rel=""nofollow noreferrer"">utils.py</a></p>

<p>however, when I print model.summary I get 0 parameters in the ELMo Embedding layer unlike when I was using Glove <strong>is that normal ? If not can you please tell me what am I doing wrong</strong> 
Using glove I Got over 20Million parameters</p>

<p><a href=""https://i.sstatic.net/SOgAv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SOgAv.png"" alt=""Model Summary""></a></p>

<pre><code>##--------&gt;  When I was using Glove  Embedding Layer
word_embedding_layer = emb.get_keras_embedding(#dropout = emb_dropout,
                                            trainable = True,
                                            input_length = sent_maxlen, 
                                            name='word_embedding_layer') 


## --------&gt; Deep layers
pos_embedding_layer = Embedding(output_dim =pos_tag_embedding_size, #5
                         input_dim = len(SPACY_POS_TAGS),
                         input_length = sent_maxlen, #20
                         name='pos_embedding_layer')
latent_layers = stack_latent_layers(num_of_latent_layers)



##--------&gt; 6] Dropout 
dropout = Dropout(0.1) 

## --------&gt; 7]Prediction
predict_layer = predict_classes()


## --------&gt; 8] Prepare input features, and indicate how to embed them
inputs = [Input((sent_maxlen,), dtype='int32', name='word_inputs'),
            Input((sent_maxlen,), dtype='int32', name='predicate_inputs'),
            Input((sent_maxlen,), dtype='int32', name='postags_inputs')]


## --------&gt; 9] ELMo Embedding and Concat all inputs and run on deep network
from elmo import ELMoEmbedding
import utils 

idx2word = utils.get_idx2word()
ELmoembedding1 = ELMoEmbedding(idx2word=idx2word, output_mode=""elmo"", trainable=True)(inputs[0]) # These two are interchangeable
ELmoembedding2 = ELMoEmbedding(idx2word=idx2word, output_mode=""elmo"", trainable=True)(inputs[1]) # These two are interchangeable


embeddings = [ELmoembedding1, 
               ELmoembedding2,
               pos_embedding_layer(inputs[3])]

con1 = keras.layers.concatenate(embeddings)

## --------&gt; 10]Build model 

outputI = predict_layer(dropout(latent_layers(con1)))
model = Model(inputs, outputI)

model.compile(optimizer='adam',
                           loss='categorical_crossentropy',
                           metrics=['categorical_accuracy'])
model.summary()
</code></pre>

<h1>Trials:</h1>

<p>note: I tried using the TF-Hub Elmo with Keras code, but the output was always a 2D tensor [even when I changed it to 'Elmo' setting and 'LSTM' instead of default']so I couldn't Concatenate with POS_embedding_layer. I tried reshaping but eventually I got the same issue total Parameters 0.</p>
",Vectorization & Embeddings,kera elmo embedding layer ha parameter normal wa using glove model worked changed elmo reference kera code available github elmo kera github utils py however print model summary get parameter elmo embedding layer unlike wa using glove normal please tell wrong using glove got million parameter trial note tried using tf hub elmo kera code output wa always tensor even changed elmo setting lstm instead default concatenate po embedding layer tried reshaping eventually got issue total parameter
Character-Word Embeddings from lm_1b in Keras,"<p>I would like to use some pre-trained word embeddings in a Keras NN model, which have been published by Google in a <a href=""https://arxiv.org/pdf/1602.02410.pdf"" rel=""noreferrer"">very well known article</a>.   They have provided the code to train a new model, as well as the embeddings <a href=""https://github.com/tensorflow/models/tree/master/lm_1b"" rel=""noreferrer"">here</a>.</p>

<p>However, it is not clear from the documentation how to retrieve an embedding vector from a given string of characters (word) from a simple python function call.  Much of the documentation seems to center on dumping vectors to a <em>file</em> for an entire sentence presumably for sentimental analysis.  </p>

<p>So far, I have seen that you can feed in pretrained embeddings with the following syntax:</p>

<pre><code>embedding_layer = Embedding(number_of_words??,
                            out_dim=128??,
                            weights=[pre_trained_matrix_here],
                            input_length=60??,
                            trainable=False)
</code></pre>

<p>However, converting the different files and their structures to <code>pre_trained_matrix_here</code> is not quite clear to me.</p>

<p>They have several softmax outputs, so I am uncertain which one would belong - and furthermore how to align the words in my input to the dictionary of words for which they have.</p>

<p>Is there a simple manner to use these word/char embeddings in keras and/or to construct the character/word embedding portion of the model in keras such that further layers may be added for other NLP tasks?</p>
",Vectorization & Embeddings,character word embeddings lm b kera would like use pre trained word embeddings kera nn model published google well known article provided code train new model well embeddings however clear documentation retrieve embedding vector given string character word simple python function call much documentation seems center dumping vector file entire sentence presumably sentimental analysis far seen feed pretrained embeddings following syntax however converting different file structure quite clear several softmax output uncertain one would belong furthermore align word input dictionary word simple manner use word char embeddings kera construct character word embedding portion model kera layer may added nlp task
Concepts to measure text &quot;relevancy&quot; to a subject?,"<p>I do side work writing/improving a research project web application for some political scientists. This application collects articles pertaining to the U.S. Supreme Court and runs analysis on them, and after nearly a year and half, we have a database of around 10,000 articles (and growing) to work with.</p>

<p>One of the primary challenges of the project is being able to determine the ""relevancy"" of an article - that is, the primary focus is the federal U.S. Supreme Court (and/or its justices), and not a local or foreign supreme court. Since its inception, the way we've addressed it is to primarily parse the title for various explicit references to the federal court, as well as to verify that ""supreme"" and ""court"" are keywords collected from the article text. Basic and sloppy, but it actually works fairly well. That being said, irrelevant articles can find their way into the database - usually ones with headlines that don't explicitly mention a state or foreign country (the Indian Supreme Court is the usual offender).</p>

<p>I've reached a point in development where I can focus on this aspect of the project more, but I'm not quite sure where to start. All I know is that I'm looking for a method of analyzing article text to determine its relevance to the federal court, and nothing else. I imagine this will entail some machine learning, but I've basically got no experience in the field. I've done a little reading into things like tf-idf weighting, vector space modeling, and word2vec (+ CBOW and Skip-Gram models), but I'm not quite seeing a ""big picture"" yet that shows me how just how applicable these concepts can be to my problem. Can anyone point me in the right direction?</p>
",Vectorization & Embeddings,concept measure text relevancy subject side work writing improving research project web application political scientist application collect article pertaining u supreme court run analysis nearly year half database around article growing work one primary challenge project able determine relevancy article primary focus u supreme court local foreign supreme court since inception way addressed primarily parse title various explicit reference court well verify supreme court keywords collected article text basic actually work fairly well said irrelevant article find way database usually one headline explicitly mention state foreign country indian supreme court usual offender reached point development focus aspect project quite sure start know looking method analyzing article text determine relevance court nothing else imagine entail machine learning basically got experience field done little reading thing like tf idf weighting vector space modeling word vec cbow skip gram model quite seeing big picture yet show applicable concept problem anyone point right direction
How does Pyspark Calculate Doc2Vec from word2vec word embeddings?,"<p>I have a pyspark dataframe with a corpus of ~300k unique rows each with a ""doc"" that contains a few sentences of text in each.</p>

<p>After processing, I have a 200 dimension vectorized representation of each row/doc. My NLP Process: </p>

<ol>
<li>Remove Punctuation with regex udf </li>
<li>Word Stemming with nltk snowball udf)</li>
<li>Pyspark Tokenizer</li>
<li>Word2Vec (ml.feature.Word2Vec, vectorSize=200, windowSize=5)</li>
</ol>

<p>I understand how this implementation uses the skipgram model to create embeddings for each word based on the full corpus used. My question is: <strong>How does this implementation go from a vector for each word in the corpus to a vector for each document/row?</strong></p>

<p>Is it the same processes as in the gensim doc2vec implementation where it simply concatenates the word vectors in each doc together?: <a href=""https://stackoverflow.com/questions/40413866/how-does-gensim-calculate-doc2vec-paragraph-vectors"">How does gensim calculate doc2vec paragraph vectors</a>. If so, how does it cut the vector down to the specified size of 200 (Does it use just the first 200 words? Average?)? </p>

<p>I was unable to find the information from the sourcecode: <a href=""https://spark.apache.org/docs/2.2.0/api/python/_modules/pyspark/ml/feature.html#Word2Vec"" rel=""noreferrer"">https://spark.apache.org/docs/2.2.0/api/python/_modules/pyspark/ml/feature.html#Word2Vec</a> </p>

<p>Any help or reference material to look at is super appreciated!</p>
",Vectorization & Embeddings,doe pyspark calculate doc vec word vec word embeddings pyspark dataframe corpus k unique row doc contains sentence text processing dimension vectorized representation row doc nlp process remove punctuation regex udf word stemming nltk snowball udf pyspark tokenizer word vec ml feature word vec vectorsize windowsize understand implementation us skipgram model create embeddings word based full corpus used question doe implementation go vector word corpus vector document row process gensim doc vec implementation simply concatenates word vector doc together help reference material look super appreciated
How do I input doc2vec vectors of multiple text columns?,"<p>I have a dataset which has 3 different columns of relevant text information which I want to convert into doc2vec vectors and subsequently classify using a neural net. My question is how do I convert these three columns into vectors and input into a neural net?</p>

<p>How do I input the concatenated vectors into a neural network?</p>
",Vectorization & Embeddings,input doc vec vector multiple text column dataset ha different column relevant text information want convert doc vec vector subsequently classify using neural net question convert three column vector input neural net input concatenated vector neural network
Convert list of words in Text file to Word Vectors,"<p>I have a text file with million of rows which I wanted to convert into word vectors and later on I can compare these vectors with a search keyword and see which all texts are closer to the search keyword.</p>

<p>My Dilemma is all the training files that I have seen for the Word2vec are in the form of paragraphs so that each word has some contextual meaning within that file. Now my file here is independent and contains different keywords in each row.</p>

<p>My question is whether is it possible to create word embedding using this text file or not, if not then what's the best approach for searching a matching search keyword in this million of texts</p>

<p>**My File Structure: **</p>

<pre><code>Walmart
Home Depot
Home Depot
Sears
Walmart
Sams Club
GreenMile
Walgreen
</code></pre>

<p><strong>Expected</strong></p>

<pre><code>search Text : 'WAL'
</code></pre>

<p><strong>Result from My File:</strong></p>

<pre><code>WALGREEN
WALMART
WALMART
</code></pre>
",Vectorization & Embeddings,convert list word text file word vector text file million row wanted convert word vector later compare vector search keyword see text closer search keyword dilemma training file seen word vec form paragraph word ha contextual meaning within file file independent contains different keywords row question whether possible create word embedding using text file best approach searching matching search keyword million text file structure expected result file
Mapping entity embeddings back to the original categorical values,"<p>I am using Keras embedding layers to create entity embeddings made popular on the Kaggle Rossmann Store Sales <a href=""https://www.kaggle.com/c/rossmann-store-sales/discussion/17974"" rel=""nofollow noreferrer"">3rd place entry.</a> However, I am unsure about how to map back the embeddings back to the actual categorical values. Let's take a look at a very basic example:</p>

<p>In the code below, I create a dataset with two numeric and one categorical feature.</p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from keras.models import Model
from keras.layers import Input, Dense, Concatenate, Reshape, Dropout
from keras.layers.embeddings import Embedding

# create some fake data
data, labels = make_classification(n_classes=2, class_sep=2, n_informative=2,
                                   n_redundant=0, flip_y=0, n_features=2,
                                   n_clusters_per_class=1, n_samples=100,
                                   random_state=10)

cat_col = np.random.choice(a=[0,1,2,3,4], size=100)

data = pd.DataFrame(data)
data[2] = cat_col
embed_cols = [2]

# converting data to list of lists, as the network expects to
# see the data in this format
def preproc(df):
    data_list = []

    # convert cols to list of lists
    for c in embed_cols:
        vals = np.unique(df[c])
        val_map = {}
        for i in range(len(vals)):
            val_map[vals[i]] = vals[i]
        data_list.append(df[c].map(val_map).values)

    # the rest of the columns
    other_cols = [c for c in df.columns if (not c in embed_cols)]
    data_list.append(df[other_cols].values)
    return data_list

data = preproc(data)
</code></pre>

<p>There are 5 unique values for the categorical column: </p>

<pre><code>print(""Unique Values: "", np.unique(data[0]))
Out[01]: array([0, 1, 2, 3, 4])
</code></pre>

<p>which then get fed into a Keras model with an embedding layer: </p>

<pre><code>inputs = []
embeddings = []

input_cat_col = Input(shape=(1,))
embedding = Embedding(5, 3, input_length=1, name='cat_col')(input_cat_col)
embedding = Reshape(target_shape=(3,))(embedding)
inputs.append(input_cat_col)
embeddings.append(embedding)


# add the remaining two numeric columns from the 'data array' to the network
input_numeric = Input(shape=(2,))
embedding_numeric = Dense(8)(input_numeric)
inputs.append(input_numeric)
embeddings.append(embedding_numeric)

x = Concatenate()(embeddings)
output = Dense(1, activation='sigmoid')(x)

model = Model(inputs, output)
model.compile(loss='binary_crossentropy', optimizer='adam')

history = model.fit(data, labels,
                    epochs=10,
                    batch_size=32,
                    verbose=1,
                    validation_split=0.2)
</code></pre>

<p>I can get the actual embeddings by getting the weight for the embedding layer:</p>

<pre><code>embeddings = model.get_layer('cat_col').get_weights()[0]
print(""Unique Values: "", np.unique(data[0]))
print(""3 Dimensional Embedding: \n"", embeddings)

Unique Values:  [0 1 2 3 4]
3 Dimensional Embedding: 
 [[ 0.02749949  0.04238378  0.0080842 ]
 [-0.00083209  0.01848664  0.0130044 ]
 [-0.02784528 -0.00713446 -0.01167112]
 [ 0.00265562  0.03886909  0.0138318 ]
 [-0.01526615  0.01284053 -0.0403452 ]]
</code></pre>

<p>However, I am unsure how to map these back. Is it safe to assume that the weights are ordered? For example, <code>0=[ 0.02749949  0.04238378  0.0080842 ]</code>?</p>
",Vectorization & Embeddings,mapping entity embeddings back original categorical value using kera embedding layer create entity embeddings made popular kaggle rossmann store sale rd place entry however unsure map back embeddings back actual categorical value let take look basic example code create dataset two numeric one categorical feature unique value categorical column get fed kera model embedding layer get actual embeddings getting weight embedding layer however unsure map back safe assume weight ordered example
Text classification with own word embeddings using Neural Networks in R,"<p>This is a rather lengthy one, so please bear with me, unfortunately enough the error occurs right at the very end...I cannot predict on the unseen test set!</p>

<p>I would like to perform text classification with word embeddings (that I have trained on my data set) that are embedded into neural networks.
I simply have column with textual descriptions = input and four different price classes = target.</p>

<p>For a reproducible example, here are the necessary data set and the word embedding:</p>

<p>DF: <a href=""https://www.dropbox.com/s/it0jsbv8e7nkryt/DF.csv?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/it0jsbv8e7nkryt/DF.csv?dl=0</a></p>

<p>WordEmb: <a href=""https://www.dropbox.com/s/ia5fmio2e0plwkr/WordEmb.txt?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/ia5fmio2e0plwkr/WordEmb.txt?dl=0</a></p>

<p>And here my code:</p>

<pre><code>set.seed(2077)
DF = read.delim(""DF.csv"", header = TRUE, sep = "","",
                             dec = ""."", stringsAsFactors = FALSE)
DF &lt;- DF[,-1]

# parameters
max_num_words = 9000         # simply see number of observations
validation_split = 0.3
embedding_dim = 300

##### Data Preparation #####

# split into training and test set

set.seed(2077)
n &lt;- nrow(DF)
shuffled &lt;- DF[sample(n),]

# Split the data in train and test
train &lt;- shuffled[1:round(0.7 * n),]
test &lt;- shuffled[(round(0.7 * n) + 1):n,]
rm(n, shuffled)

# predictor/target variable
x_train &lt;- train$Description
x_test &lt;- test$Description

y_train &lt;- train$Price_class
y_test &lt;- test$Price_class

### encode target variable ###

# One hot encode training target values
trainLabels &lt;- to_categorical(y_train) 
trainLabels &lt;- trainLabels[, 2:5]

# One hot encode test target values
testLabels &lt;- keras::to_categorical(y_test)
testLabels &lt;- testLabels[, 2:5]

### encode predictor variable ###

# pad sequences
tokenizer &lt;- text_tokenizer(num_words = max_num_words)

# finally, vectorize the text samples into a 2D integer tensor

set.seed(2077)
tokenizer %&gt;% fit_text_tokenizer(x_train)
train_data &lt;- texts_to_sequences(tokenizer, x_train)
tokenizer %&gt;% fit_text_tokenizer(x_test)
test_data &lt;- texts_to_sequences(tokenizer, x_test)

# determine average length of document -&gt; set as maximal sequence length
seq_mean &lt;- stri_count(train_data, regex=""\\S+"")
mean((seq_mean))

max_sequence_length = 70

# This turns our lists of integers into a 2D integer tensor of shape`(samples, maxlen)`

x_train &lt;- keras::pad_sequences(train_data, maxlen = max_sequence_length)
x_test &lt;- keras::pad_sequences(test_data, maxlen = max_sequence_length)

word_index &lt;- tokenizer$word_index
Encoding(names(word_index)) &lt;- ""UTF-8""

#### PREPARE EMBEDDING MATRIX ####

embeddings_index &lt;- new.env(parent = emptyenv())
lines &lt;- readLines(""WordEmb.txt"")
for (line in lines) {
  values &lt;- strsplit(line, ' ', fixed = TRUE)[[1]]    
  word &lt;- values[[1]]
  coefs &lt;- as.numeric(values[-1])
  embeddings_index[[word]] &lt;- coefs
}

embedding_dim &lt;- 300
embedding_matrix &lt;- array(0,c(max_num_words, embedding_dim))
for(word in names(word_index)){
  index &lt;- word_index[[word]]
  if(index &lt; max_num_words){
    embedding_vector &lt;- embeddings_index[[word]]
    if(!is.null(embedding_vector)){
      embedding_matrix[index+1,] &lt;- embedding_vector  
    }
  }
}

##### Convolutional Neural Network #####

# load pre-trained word embeddings into an Embedding layer
# note that we set trainable = False so as to keep the embeddings fixed
num_words &lt;- min(max_num_words, length(word_index) + 1)

embedding_layer &lt;- keras::layer_embedding(
  input_dim = num_words,
  output_dim = embedding_dim,
  weights = list(embedding_matrix), 
  input_length = max_sequence_length,
  trainable = FALSE
)

# train a 1D convnet with global maxpooling
sequence_input &lt;- layer_input(shape = list(max_sequence_length), dtype='int32')

preds &lt;- sequence_input %&gt;%
  embedding_layer %&gt;% 
  layer_conv_1d(filters = 128, kernel_size = 1, activation = 'relu') %&gt;% 
  layer_max_pooling_1d(pool_size = 5) %&gt;% 
  layer_conv_1d(filters = 128, kernel_size = 1, activation = 'relu') %&gt;% 
  layer_max_pooling_1d(pool_size = 5) %&gt;% 
  layer_conv_1d(filters = 128, kernel_size = 1, activation = 'relu') %&gt;% 
  layer_max_pooling_1d(pool_size = 2) %&gt;% 
  layer_flatten() %&gt;% 
  layer_dense(units = 128, activation = 'relu') %&gt;% 
  layer_dense(units = 4, activation = 'softmax')

model &lt;- keras_model(sequence_input, preds)

model %&gt;% compile(
  loss = 'categorical_crossentropy',
  optimizer = 'adam',
  metrics = c('acc')  
)

model %&gt;% keras::fit(
  x_train,                              
  trainLabels,                          
  batch_size = 1024,
  epochs = 20,
  validation_split = 0.3
)
</code></pre>

<p>Now here is where I get stuck:
I cannot use the results of the NN to predict on the unseen test data set:</p>

<pre><code># Predict the classes for the test data
classes &lt;- model %&gt;% predict_classes(x_test, batch_size = 128)

I get this error: 
Error in py_get_attr_impl(x, name, silent) : 
  AttributeError: 'Model' object has no attribute 'predict_classes'

Afterwards, I'd proceed like this:

# Confusion matrix
table(y_test, classes)

# Evaluate on test data and labels
score &lt;- model %&gt;% evaluate(x_val, testLabels, batch_size = 128) 

# Print the score
print(score)
</code></pre>

<p>For now the actual accuracy does not really matter since this is only a small example of my data set.</p>

<p>I know this is a long one but AAANNY help would be very muuuch appreciated.</p>
",Vectorization & Embeddings,text classification word embeddings using neural network r rather lengthy one please bear unfortunately enough error occurs right end predict unseen test set would like perform text classification word embeddings trained data set embedded neural network simply column textual description input four different price class target reproducible example necessary data set word embedding df wordemb code get stuck use result nn predict unseen test data set actual accuracy doe really matter since small example data set know long one aaanny help would muuuch appreciated
Predicting probability score of each classification bin for a given document,"<p>I am creating a python model that will classify a given document based on the text. Because each document still needs to be manually reviewed by a human, I am creating a suggestion platform that will give the user the top n-classes that a given document belongs too. Additionally each document can belong to more than one class. I have a training set of documents filled with rich text and their tags.</p>

<p>What I would like to do is perform a regression on each document to get a probabilistic score of each classification and return the top 5 highest scored classes.</p>

<p>I have looked into Bayes classification models, and recommendation systems and I think a logistic regression will help be better as it returns a score. I am new to machine learning and would appreciate any advice or examples that is modeled after this kind of problem. Thank you.</p>

<p>EDIT: Specifically, my problem is how should I parse my text data for ML modeling with logistic regression? Do I need to represent my text in a vector format using Word2Vec/Doc2Vec or a Bag-of-words model?</p>
",Vectorization & Embeddings,predicting probability score classification bin given document creating python model classify given document based text document still need manually reviewed human creating suggestion platform give user top n class given document belongs additionally document belong one class training set document filled rich text tag would like perform regression document get probabilistic score classification return top highest scored class looked bayes classification model recommendation system think logistic regression help better return score new machine learning would appreciate advice example modeled kind problem thank edit specifically problem parse text data ml modeling logistic regression need represent text vector format using word vec doc vec bag word model
Start token in window word embeddings,"<p>I'm using the pre-trained SENNA embeddings and feeding a 3 word window into a Dense neural net. </p>

<p>Does senna have a start or end token embedding? 
Or do I create a random vector?    </p>

<pre><code>Sentence:  'McDonalds sells fries'
input 1:  ['&lt;s&gt;', 'McDonalds', 'sells']
</code></pre>

<p>But there is no embedding for <code>&lt;s&gt;</code>... 
Do I create my own? (all -1 for example)?</p>
",Vectorization & Embeddings,start token window word embeddings using pre trained senna embeddings feeding word window dense neural net doe senna start end token embedding create random vector embedding create example
Is there an alternative to fully loading pre-trained word embeddings in memory?,"<p>I want to use pre-trained word embeddings in my machine learning model. The word embedings file I have is about 4GB. I currently read the entire file into memory in a dictionary and whenever I want to map a word to its vector representation I perform a lookup in that dictionary.</p>

<p>The memory usage is very high and I would like to know if there is another way of using word embeddings without loading the entire data into memory.</p>

<p>I have recently come across generators in Python. Could they help me reduce the memory usage?</p>

<p>Thank you!</p>
",Vectorization & Embeddings,alternative fully loading pre trained word embeddings memory want use pre trained word embeddings machine learning model word embedings file gb currently read entire file memory dictionary whenever want map word vector representation perform lookup dictionary memory usage high would like know another way using word embeddings without loading entire data memory recently come across generator python could help reduce memory usage thank
Load a plain text file into PyTorch,"<p>I have two separate files, one is a text file, with each line being a single text. The other file contains the class label of that corresponding line. How do I load this into PyTorch and carry out further tokenization, embedding, etc? </p>
",Vectorization & Embeddings,load plain text file pytorch two separate file one text file line single text file contains class label corresponding line load pytorch carry tokenization embedding etc
How cosine similarity differs from Okapi BM25?,"<p>I'm conducting a research using elasticsearch. I was planning to use cosine similarity but I noted that it is unavailable and instead we have BM25 as default scoring function.</p>

<p>Is there a reason for that? Is cosine similarity improper for querying documents? Why was BM25 chosen as default?
Thanks</p>
",Vectorization & Embeddings,cosine similarity differs okapi bm conducting research using elasticsearch wa planning use cosine similarity noted unavailable instead bm default scoring function reason cosine similarity improper querying document wa bm chosen default thanks
Document similarity with Word Mover Distance and Bert-Embedding,"<p>I am trying to calculate the document similarity (nearest neighbor) for two arbitrary documents using word embeddings based on <a href=""https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html"" rel=""noreferrer"">Google's BERT</a>.
In order to obtain word embeddings from Bert, I use <a href=""https://github.com/hanxiao/bert-as-service"" rel=""noreferrer"">bert-as-a-service</a>.
Document similarity should be based on Word-Mover-Distance with the python <a href=""https://github.com/src-d/wmd-relax"" rel=""noreferrer"">wmd-relax</a> package.</p>

<p>My previous tries are orientated along this tutorial from the <code>wmd-relax</code> github repo: <a href=""https://github.com/src-d/wmd-relax/blob/master/spacy_example.py"" rel=""noreferrer"">https://github.com/src-d/wmd-relax/blob/master/spacy_example.py</a></p>

<pre><code>import numpy as np
import spacy
import requests
from wmd import WMD
from collections import Counter
from bert_serving.client import BertClient

# Wikipedia titles
titles = [""Germany"", ""Spain"", ""Google"", ""Apple""]

# Standard model from spacy
nlp = spacy.load(""en_vectors_web_lg"")

# Fetch wiki articles and prepare as specy document
documents_spacy = {}
print('Create spacy document')
for title in titles:
    print(""... fetching"", title)
    pages = requests.get(
        ""https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=%s""
        ""&amp;prop=extracts&amp;explaintext"" % title).json()[""query""][""pages""]
    text = nlp(next(iter(pages.values()))[""extract""])
    tokens = [t for t in text if t.is_alpha and not t.is_stop]
    words = Counter(t.text for t in tokens)
    orths = {t.text: t.orth for t in tokens}
    sorted_words = sorted(words)
    documents_spacy[title] = (title, [orths[t] for t in sorted_words],
                              np.array([words[t] for t in sorted_words],
                                       dtype=np.float32))


# This is the original embedding class with the model from spacy
class SpacyEmbeddings(object):
    def __getitem__(self, item):
        return nlp.vocab[item].vector


# Bert Embeddings using bert-as-as-service
class BertEmbeddings:
    def __init__(self, ip='localhost', port=5555, port_out=5556):
        self.server = BertClient(ip=ip, port=port, port_out=port_out)

    def __getitem__(self, item):
        text = nlp.vocab[item].text
        emb = self.server.encode([text])
        return emb


# Get the nearest neighbor of one of the atricles
calc_bert = WMD(BertEmbeddings(), documents_spacy)
calc_bert.nearest_neighbors(titles[0])
</code></pre>

<p>Unfortunately, the calculations fails with a dimensions mismatch in the distance calculation:
<code>ValueError: shapes (812,1,768) and (768,1,812) not aligned: 768 (dim 2) != 1 (dim 1)</code></p>
",Vectorization & Embeddings,document similarity word mover distance bert embedding trying calculate document similarity nearest neighbor two arbitrary document using word embeddings based google bert order obtain word embeddings bert use bert service document similarity based word mover distance python wmd relax package previous try orientated along tutorial github repo unfortunately calculation fails dimension mismatch distance calculation
Text classification beyond the keyword dependency and inferring the actual meaning,"<p>I am trying to develop a text classifier that will classify a piece of text as <strong>Private</strong> or <strong>Public</strong>. Take medical or health information as an example domain. A typical classifier that I can think of considers keywords as the main distinguisher, right? What about a scenario like bellow? What if both of the pieces of text contains similar keywords but carry a different meaning. </p>

<p><strong>Following piece of text is revealing someone's private (health) situation (the patient has cancer):</strong>  </p>

<p>I've been to two <code>clinics</code> and my <code>pcp</code>. I've had an <code>ultrasound</code> only to be told it's a resolving <code>cyst</code> or a <code>hematoma</code>, but it's getting larger and starting to make my leg <code>ache</code>. The <code>PCP</code> said it can't be a <code>cyst</code> because it started out way too big and I swear I have NEVER <code>injured</code> my leg, not even a <code>bump</code>. I am now scared and afraid of <code>cancer</code>. I noticed a slightly uncomfortable sensation only when squatting down about 9 months ago. 3 months ago I went to squat down to put away laundry and it kinda <code>hurt</code>. The <code>pain</code> prompted me to examine my <code>leg</code> and that is when I noticed a <code>lump</code> at the bottom of my calf <code>muscle</code> and flexing only made it more noticeable. Eventually after four <code>clinic</code> visits, an <code>ultrasound</code> and one <code>pcp</code> the result seems to be positive and the mass is getting larger.<br>
<strong>[Private] (Correct Classification)</strong></p>

<p><strong>Following piece of text is a comment from a doctor which is definitely not revealing is health situation. It introduces the weaknesses of a typical classifier model:</strong>  </p>

<p>Don’t be scared and do not assume anything bad as <code>cancer</code>. I have gone through several cases in my <code>clinic</code> and it seems familiar to me. As you mentioned it might be a <code>cyst</code> or a <code>hematoma</code> and it's getting larger, it must need some additional <code>diagnosis</code> such as <code>biopsy</code>. Having an <code>ache</code> in that area or the size of the <code>lump</code> does not really tells anything <code>bad</code>. You should visit specialized <code>clinics</code> few more times and go under some specific tests such as <code>biopsy</code>, <code>CT scan</code>, <code>pcp</code> and <code>ultrasound</code> before that <code>lump</code> become more larger.<br>
<strong>[Private] (Which is the Wrong Classification. It should be [Public])</strong>  </p>

<p>The second paragraph was classified as private by all of my current classifiers, for obvious reason. Similar keywords, valid word sequences, the presence of subjects seemed to make the classifier very confused. Even, both of the content contains subjects like <code>I</code>, <code>You</code> (Noun, Pronouns) etc. I thought about from Word2Vec to Doc2Vec, from Inferring meaning to semantic embeddings but can't think about a solution approach that best suits this problem.</p>

<p>Any idea, which way I should handle the classification problem? Thanks in advance. </p>

<p><strong>Progress so Far:</strong><br>
The data, I have collected from a public source where patients/victims usually post their own situation and doctors/well-wishers reply to those. I assumed while crawling is that - posts belongs to my private class and comments belongs to public class. All to gether I started with 5K+5K posts/comments and got around 60% with a naive bayes classifier without any major preprocessing. I will try Neural Network soon. But before feeding into any classifier, I just want to know how I can preprocess better to put reasonable weights to either class for better distinction.</p>
",Vectorization & Embeddings,text classification beyond keyword dependency inferring actual meaning trying develop text classifier classify piece text private public take medical health information example domain typical classifier think considers keywords main distinguisher right scenario like bellow piece text contains similar keywords carry different meaning following piece text revealing someone private health situation patient ha cancer two told resolving getting larger starting make leg said started way big swear never leg even scared afraid noticed slightly uncomfortable sensation squatting month ago month ago went squat put away laundry kinda prompted examine noticed bottom calf flexing made noticeable eventually four visit one result seems positive mass getting larger private correct classification following piece text comment doctor definitely revealing health situation introduces weakness typical classifier model scared assume anything bad gone several case seems familiar mentioned might getting larger must need additional area size doe really tell anything visit specialized time go specific test become larger private wrong classification public second paragraph wa classified private current classifier obvious reason similar keywords valid word sequence presence subject seemed make classifier confused even content contains subject like noun pronoun etc thought word vec doc vec inferring meaning semantic embeddings think solution approach best suit problem idea way handle classification problem thanks advance progress far data collected public source patient victim usually post situation doctor well wishers reply assumed crawling post belongs private class comment belongs public class gether started k k post comment got around naive bayes classifier without major preprocessing try neural network soon feeding classifier want know preprocess better put reasonable weight either class better distinction
"Why no word embeddings (Glove, word2vecetc) used in first attention paper?","<p>In the paper <a href=""https://arxiv.org/abs/1409.0473"" rel=""nofollow noreferrer"">Neural Machine Translation by Jointly Learning to Align and Translate Bahdanau et. al.</a> why are there no word embeddings such as Glove or word2vec used? </p>

<p>I understand that this was a 2014 paper, but the current implementations of the paper on github don't use any word embeddings as well?</p>

<p>For trying to code the paper is using word embeddings reasonable?</p>
",Vectorization & Embeddings,word embeddings glove word vecetc used first attention paper paper neural machine translation jointly learning align translate bahdanau et al word embeddings glove word vec used understand wa paper current implementation paper github use word embeddings well trying code paper using word embeddings reasonable
Compare vectors of a doc and just a word,"<p>So, I have to compare vector of article and vector of single word. And I don't have any idea how to do it. Looks like that BERT and Doc2wec good work with long text, Word2vec works with single words. But how to compare long text with just a word?</p>
",Vectorization & Embeddings,compare vector doc word compare vector article vector single word idea look like bert doc wec good work long text word vec work single word compare long text word
how to convert Word to vector using embedding layer in Keras,"<p>I am having a word embedding file as shown below <a href=""https://github.com/hit-computer/SC-LSTM/blob/master/Data/vec5.txt"" rel=""nofollow noreferrer"">click here to see the complete file in github</a>.I would like to know the procedure for generating word embeddings So that i can generate word embedding for my personal dataset   </p>

<pre><code>in -0.051625 -0.063918 -0.132715 -0.122302 -0.265347 
to 0.052796 0.076153 0.014475 0.096910 -0.045046 
for 0.051237 -0.102637 0.049363 0.096058 -0.010658 
of 0.073245 -0.061590 -0.079189 -0.095731 -0.026899 
the -0.063727 -0.070157 -0.014622 -0.022271 -0.078383 
on -0.035222 0.008236 -0.044824 0.075308 0.076621 
and 0.038209 0.012271 0.063058 0.042883 -0.124830 
a -0.060385 -0.018999 -0.034195 -0.086732 -0.025636 
The 0.007047 -0.091152 -0.042944 -0.068369 -0.072737 
after -0.015879 0.062852 0.015722 0.061325 -0.099242 
as 0.009263 0.037517 0.028697 -0.010072 -0.013621 
Google -0.028538 0.055254 -0.005006 -0.052552 -0.045671 
New 0.002533 0.063183 0.070852 0.042174 0.077393 
with 0.087201 -0.038249 -0.041059 0.086816 0.068579 
at 0.082778 0.043505 -0.087001 0.044570 0.037580 
over 0.022163 -0.033666 0.039190 0.053745 -0.035787 
new 0.043216 0.015423 -0.062604 0.080569 -0.048067 
</code></pre>
",Vectorization & Embeddings,convert word vector using embedding layer kera word embedding file shown click see complete file github would like know procedure generating word embeddings generate word embedding personal dataset
Replacing positional embedding with pre-calculated results in BERT leads to poor prediction result,"<p>I'm trying to use <code>BERT</code> for a <code>NER</code> task. To achieve better prediction results, I'm trying to replace the positional embedding in the <code>embedding_postprocessor()</code> function with some pre-calculated results, based on the principle of <code>sinusoidal embedding</code>, as presented in paper <code>""Attention is all you need""</code>. </p>

<p>Although after about <code>20 hours training</code>, the model seems achieving good convergence (loss drops to about 10^-2 or 10^-3), the tested results were pretty bad, with the accuracy rate around 20%-30%.  </p>

<p>Has anyone tried to replace the <code>positional embedding</code> of <code>BERT</code> with other implementation methods? Will the idea of using <code>sinusoidal embedding</code> work in <code>BERT</code>? or we could only stick to the learned <code>positional embedding</code> in <code>BERT</code>?</p>
",Vectorization & Embeddings,replacing positional embedding pre calculated result bert lead poor prediction result trying use task achieve better prediction result trying replace positional embedding function pre calculated result based principle presented paper although model seems achieving good convergence loss drop tested result pretty bad accuracy rate around ha anyone tried replace implementation method idea using work could stick learned
Why the FastText word embedding could generate the representation of a word from another language?,"<p>Recently, I trained a FastText word embedding from <a href=""https://www.kaggle.com/kazanova/sentiment140"" rel=""nofollow noreferrer"">sentiment140</a> to get the representation for English words. However, today just for a trial, I run the FastText module on a couple of Chinese words, for instance:</p>

<pre><code>import gensim.models as gs

path = r'\data\word2vec'

w2v = gs.FastText.load(os.path.join(path, 'fasttext_model'))

w2v.wv['哈哈哈哈']
</code></pre>

<p>It outputs:</p>

<pre><code>array([ 0.00303676,  0.02088235, -0.00815559,  0.00484574, -0.03576371,
       -0.02178247, -0.05090654,  0.03063928, -0.05999983,  0.04547168,
       -0.01778449, -0.02716631, -0.03326027, -0.00078981,  0.0168153 ,
        0.00773436,  0.01966593, -0.00756055,  0.02175765, -0.0050137 ,
        0.00241255, -0.03810823, -0.03386266,  0.01231019, -0.00621936,
       -0.00252419,  0.02280569,  0.00992453,  0.02770403,  0.00233192,
        0.0008545 , -0.01462698,  0.00454278,  0.0381292 , -0.02945416,
       -0.00305543, -0.00690968,  0.00144188,  0.00424266,  0.00391074,
        0.01969502,  0.02517333,  0.00875261,  0.02937791,  0.03234404,
       -0.01116276, -0.00362578,  0.00483239, -0.02257918,  0.00123061,
        0.00324584,  0.00432153,  0.01332884,  0.03186348, -0.04119627,
        0.01329033,  0.01382102, -0.01637722,  0.01464139,  0.02203292,
        0.0312229 ,  0.00636201, -0.00044287, -0.00489291,  0.0210293 ,
       -0.00379244, -0.01577058,  0.02185207,  0.02576622, -0.0054543 ,
       -0.03115215, -0.00337738, -0.01589811, -0.01608399, -0.0141606 ,
        0.0508234 ,  0.00775024,  0.00352813,  0.00573649, -0.02131752,
        0.01166397,  0.00940598,  0.04075769, -0.04704212,  0.0101376 ,
        0.01208556,  0.00402935,  0.0093914 ,  0.00136144,  0.03284211,
        0.01000613, -0.00563702,  0.00847146,  0.03236216, -0.01626745,
        0.04095127,  0.02858841,  0.0248084 ,  0.00455458,  0.01467448],
      dtype=float32)
</code></pre>

<p>Hence, I really want to know why the FastText module trained from <a href=""https://www.kaggle.com/kazanova/sentiment140"" rel=""nofollow noreferrer"">sentiment140</a> could do this. Thank you!</p>
",Vectorization & Embeddings,fasttext word embedding could generate representation word another language recently trained fasttext word embedding sentiment get representation english word however today trial run fasttext module couple chinese word instance output hence really want know fasttext module trained sentiment could thank
Create word embeddings without keeping fastText Vector file in the repository,"<p>I am trying to embed a sentence with the help of <a href=""https://github.com/facebookresearch/InferSent"" rel=""nofollow noreferrer"">Infersent</a>, and Infersent uses <a href=""https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip"" rel=""nofollow noreferrer"">fastText</a> vectors for word embedding. The fastText vector file is close to 5 GiB.</p>

<p>When we keep the fastText vector file along with the code repository it makes the repository size huge, and makes the code difficult to share/deploy (even creating a docker container).</p>

<p>Is there any method to avoid keeping the vector file along with the repository, but reuse it for embedding new sentences?</p>
",Vectorization & Embeddings,create word embeddings without keeping fasttext vector file repository trying embed sentence help infersent infersent us fasttext vector word embedding fasttext vector file close gib keep fasttext vector file along code repository make repository size huge make code difficult share deploy even creating docker container method avoid keeping vector file along repository reuse embedding new sentence
BERT multilingual model - For classification,"<p>I am trying to build multilingual classification model with BERT.</p>

<p>I'm using a feature-based approach (concatenating the features from top-4 hidden layers) and building a CNN classifier on top of that.</p>

<p>After that I'm using different language (say chinese) from the same domain for testing, but accuracy for these languages is near zero.</p>

<p>I am not sure that I understand paper well, so here is my question:</p>

<p>Is it possible to fine-tune BERT multilingual model on one language
(e.g. English) or use feature-based approach to extract the features and build classifer, and after that use this model for different languages (other
languages from the list of supported languages in documentation of
BERT)?</p>

<p>Also, is my hypothesis, ""regarding BERT that it maps I think that it's embedding layer maps words from different languages with same context to similar clusters"", correct?</p>
",Vectorization & Embeddings,bert multilingual model classification trying build multilingual classification model bert using feature based approach concatenating feature top hidden layer building cnn classifier top using different language say chinese domain testing accuracy language near zero sure understand paper well question possible fine tune bert multilingual model one language e g english use feature based approach extract feature build classifer use model different language language list supported language documentation bert also hypothesis regarding bert map think embedding layer map word different language context similar cluster correct
Desired distribution of weights in word embedding vectors,"<p>I am training my own embedding vectors as I'm focused on an academic dataset (WOS); whether the vectors are generated via word2vec or fasttext doesn't particularly matter. Say my vectors are 150 dimensions each. I'm wondering what the desired distribution of weights within a vector ought to be, if you averaged across an entire corpus's vectors?  </p>

<p>I did a few experiments while looking at the distributions of a sample of my vectors and came to these conclusions (uncertain as to how absolutely they hold): </p>

<p>If one trains their model with too few epochs then the vectors don't change significantly from their initiated values (easy to see if you start you vectors as weight 0 in every category). Thus if my weight distribution is centered around some point (typically 0) then I've under-trained my corpus. </p>

<p>If one trains their model with too few documents/over-trains then the vectors show significant correlation with each other (I typically visualize a random set of vectors and you can see stripes where all the vectors have weights that are either positive or negative). </p>

<p>What I imagine is a single ""good"" vector has various weights across the entire range of -1 to 1. For any single vector it may have significantly more dimensions near -1 or 1. However, the weight distribution of an entire corpus would balance out vectors that randomly have more values towards one end of the spectrum or another, so that the weight distribution of the entire corpus is approximately evenly distributed across the entire corpus. Is this intuition correct? </p>
",Vectorization & Embeddings,desired distribution weight word embedding vector training embedding vector focused academic dataset wos whether vector generated via word vec fasttext particularly matter say vector dimension wondering desired distribution weight within vector ought averaged across entire corpus vector experiment looking distribution sample vector came conclusion uncertain absolutely hold one train model epoch vector change significantly initiated value easy see start vector weight every category thus weight distribution centered around point typically trained corpus one train model document train vector show significant correlation typically visualize random set vector see stripe vector weight either positive negative imagine single good vector ha various weight across entire range single vector may significantly dimension near however weight distribution entire corpus would balance vector randomly value towards one end spectrum another weight distribution entire corpus approximately evenly distributed across entire corpus intuition correct
Data augmentation for text classification,"<p>What is the current state of the art data augmentation technic about text classification?</p>

<p>I made some research online about how can I extend my training set by doing some data transformation, the same we do on image classification.
I found some interesting ideas such as:</p>

<ul>
<li><p>Synonym Replacement: Randomly choose n words from the sentence that does not stop words. Replace each of these words with one of its synonyms chosen at random.</p></li>
<li><p>Random Insertion: Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random place in the sentence. Do this n times.</p></li>
<li><p>Random Swap: Randomly choose two words in the sentence and swap their positions. Do this n times.</p></li>
<li><p>Random Deletion: Randomly remove each word in the sentence with probability p.</p></li>
</ul>

<p>But nothing about using pre-trained word vector representation model such as word2vec. Is there a reason?</p>

<p>Data augmentation using a word2vec might help the model to get more data based on external information. For instance, replacing a toxic comment token randomly in the sentence by its closer token in a pre-trained vector space trained specifically on external online comments.</p>

<p>Is it a good method or do I miss some important drawbacks of this technic? </p>
",Vectorization & Embeddings,data augmentation text classification current state art data augmentation technic text classification made research online extend training set data transformation image classification found interesting idea synonym replacement randomly choose n word sentence doe stop word replace word one synonym chosen random random insertion find random synonym random word sentence stop word insert synonym random place sentence n time random swap randomly choose two word sentence swap position n time random deletion randomly remove word sentence probability p nothing using pre trained word vector representation model word vec reason data augmentation using word vec might help model get data based external information instance replacing toxic comment token randomly sentence closer token pre trained vector space trained specifically external online comment good method miss important drawback technic
Need help while building my doc2vec embedding model for Holy Quran verses retrieval system based on verse&#39;s topic,"<p>I have an important Module in my Graduation Project, It is about to learn a model that helps me to give it an input/query which expected to be an ""Islamic"" topic and the model's outputs are some Verses from the ""Holy Quran"" that related to my topic.</p>

<h2>An Example (in Arabic)</h2>

<p>Query may be something like this: بر الوالدين, or الإحسان بالوالين</p>

<p>The Expected output some Quran verses related to the previous query like this:</p>

<ul>
<li>وَاعْبُدُوا اللَّهَ وَلَا تُشْرِكُوا بِهِ شَيْئًا وَبِالْوَالِدَيْنِ إِحْسَانً</li>
<li>وَقُلْ لَهُمَا قَوْلًا كَرِيمًا</li>
<li>وَوَصَّيْنَا الْإِنْسَانَ بِوَالِدَيْهِ حُسْنًا</li>
</ul>

<p>This is a simple example, but note that the topic may be not be mentioned itself in the verse's text, so it may be more difficult than the above example.</p>

<h2>My Data Set</h2>

<p>Till now. I have collected an <strong>Arabic, Islamic</strong> data such as:</p>

<ul>
<li>Sayings of the Prophet Mohammed: about 35000 Arabic Documents, this is a sample from the data file:</li>
</ul>

<p>حدثنا سعيد بن يحيى بن سعيد القرشي، قال حدثنا أبي قال، حدثنا أبو بردة بن عبد الله بن أبي بردة، عن أبي بردة، عن أبي موسى  رضى الله عنه  قال قالوا يا رسول الله أى الإسلام أفضل قال ‏""‏ من سلم المسلمون من لسانه ويده ‏""‏‏.
‏</p>

<p>حدثنا عمرو بن خالد، قال حدثنا الليث، عن يزيد، عن أبي الخير، عن عبد الله بن عمرو  رضى الله عنهما  أن رجلا، سأل النبي صلى الله عليه وسلم أى الإسلام خير قال ‏""‏ تطعم الطعام، وتقرأ السلام على من عرفت ومن لم تعرف ‏""‏‏.
‏</p>

<p>حدثنا مسدد، قال حدثنا يحيى، عن شعبة، عن قتادة، عن أنس  رضى الله عنه  عن النبي صلى الله عليه وسلم‏.‏وعن حسين المعلم، قال حدثنا قتادة، عن أنس، عن النبي صلى الله عليه وسلم قال ‏""‏ لا يؤمن أحدكم حتى يحب لأخيه ما يحب لنفسه ‏""‏‏.
‏</p>

<p>حدثنا أبو اليمان، قال أخبرنا شعيب، قال حدثنا أبو الزناد، عن الأعرج، عن أبي هريرة  رضى الله عنه  أن رسول الله صلى الله عليه وسلم قال ‏""‏ فوالذي نفسي بيده لا يؤمن 
أحدكم حتى أكون أحب إليه من والده وولده ‏""‏‏.‏</p>

<ul>
<li>I have also the Holy Quran Meanings data set, about 7000 documents, and here is a sample of this documents:</li>
</ul>

<p>الحمد لله جملة خبرية قصد بها الثناء على الله بمضمونها من انه تعالى مالك لجميع الحمد من الخلق او مستحق لان يحمدوه والله علم على المعبود بحق رب العالمين اي مالك جميع الخلق من الانس والجن والملائكة والدواب وغيرهم وكل منها يطلق عليه عالم يقال عالم الانس وعالم الجن الى غير ذلك وغلب في جمعه بالياء والنون 
اولي العلم على غيرهم وهو من العلامة لانه علامة على موجده</p>

<p>الرحمن الرحيم اي ذي الرحمة وهي ارادة الخير لاهله</p>

<p>اي الجزاء وهو يوم القيامة وخص بالذكر لانه لا ملك ظاهرا فيه لاحد الا الله تعالى بدليل لمن الملك اليوم؟ لله ومن قرا مالك فمعناه الامر كله في يوم القيامة او هو موصوف بذلك دائما كغافر الذنب فصح وقوعه صفة لمعرفة</p>

<p>اياك نعبد واياك نستعين اي نخصك بالعبادة من توحيد وغيره ونطلب المعونة على العبادة وغيرها</p>

<p>اهدنا الصراط المستقيم اي ارشدنا اليه ويبدل منه</p>

<p>صراط الذين انعمت عليهم بالهداية ويبدل من الذين بصلته غير المغضوب عليهم وهم اليهود ولا وغير الضالين وهم النصارى ونكتة البدل افادة ان المهتدين ليسوا يهودا ولا نصارى والله اعلم بالصواب واليه المرجع والماب وصلى الله على سيدنا محمد وعلى اله وصحبه وسلم تسليما كثيرا دائما ابدا وحسبنا الله ونعم الوكيل ولا حول ولا قوة الا بالله العلي العظيم</p>

<ul>
<li>I collected also manual annotated topics for each verse from an Islamic expert who classified the whole Quran verses to 11 main sections, each section my have subsections and subsubsections, ... so I have a 2-column data set, the first is the verse itself and the second is it's annotated topic. this is a sample:
<a href=""https://drive.google.com/open?id=1tEYs2QV9AmGQFqSzpvscNtlZfO3fVgCE"" rel=""nofollow noreferrer"">https://drive.google.com/open?id=1tEYs2QV9AmGQFqSzpvscNtlZfO3fVgCE</a></li>
</ul>

<p>note that, in the <code>ManualKeyword</code> column the topic which is after <code>-</code> mark is a subsection of the topic which is before <code>-</code></p>

<h2>Doc2Vec Model</h2>

<p>After many tries to achieve my goal, I read about <code>doc2vec</code> model and read it's paper and saw some implementations to it on Arabic tasks. I think my problem will be solved if I train a <code>doc2vec</code> model on the whole Arabic, Islamic data sets which I collected for now and any other data sets related to this field.</p>

<p>My Idea after training my model is to use it to embed each Manual annotated topic <strong>individuality</strong> (For clarification: I will separate each row like this <code>أركان الاسلام-التوحيد-الكافرون-افتراؤهم علي الله و تكذيبهم و جدالهم</code> to single phrases</p>

<pre><code>أركان الاسلام
التوحيد
الكافرون
افتراؤهم علي الله وتكذيبهم وجدالهم
</code></pre>

<p>and embed/represent each single topic with a vector)
then embed user's query (which is expected to be more than one word) to it's vector using my trained model. Then calculate the <strong>Cosine Similarity</strong> between Query's Vector and each topics' vector. so I can map the topic which gets the highest similarity to it's related verses and retrieve them to the user.</p>

<h2>My Code</h2>

<p><strong>After reading quran meanings and the Sayings of the Prophet data in</strong> <code>hadithsDocumentsList</code>:</p>

<pre><code>#Converting docuemnts list into tagged documents (each document is splited)
tagged_data = [gensim.models.doc2vec.TaggedDocument(words=_d.split(),
                              tags=[str(i)]) for i, _d in enumerate(hadithsDocumentsList)]

cores = multiprocessing.cpu_count()     #Getting number of cores

model = Doc2Vec(dm=1, size=200, window=10, workers=6)    #Initialize the model
model.build_vocab(tagged_data)      #Bulding vocabulary
print(""model.corpus_count"" , model.corpus_count)

#Training the model
model.train(tagged_data, total_examples=model.corpus_count, epochs=10)


#Saving Model
model.save(""HadithAndTafserModel"")
print(""Model Saved"")

model= Doc2Vec.load(""HadithAndTafserModel"")

testData = "" رسول الله محمد"".split()
testDataVector = model.infer_vector(testData)
print(""Query Vector: "", testDataVector)

mostSemilarDocs = model.wv.most_similar([testDataVector])
print(""MOST SIMILAR DOCUMENTS: \n"")
print(mostSemilarDocs)
</code></pre>

<p>After training the model and getting a vector for a query from my choice and using <code>most_similar</code> function to get similar documents from my model. the output is definitely not what I expected. and model's accuracy is very bad.</p>

<p>I don't know what are the correct parameters like <code>dm</code>, <code>dbow_words</code>, <code>size</code>, <code>window</code>, <code>alpha</code> that should be passed to the model to achieve the highest accuracy. I little understanding each parameter's functionality and need help to tune each of them. I also want to know if my dataset is enough to build this model or I need to increase it? is there any inaccuracy or mistakes while collecting or passing them to the model?</p>

<p>What are your suggestions or opinions?</p>
",Vectorization & Embeddings,need help building doc vec embedding model holy quran verse retrieval system based verse topic important module graduation project learn model help give input query expected islamic topic model output verse holy quran related topic example arabic query may something like expected output quran verse related previous query like simple example note topic may mentioned verse text may difficult example data set till collected arabic islamic data saying prophet mohammed arabic document sample data file also holy quran meaning data set document sample document collected also manual annotated topic verse islamic expert classified whole quran verse main section section subsection subsubsections column data set first verse second annotated topic sample note column topic mark subsection topic doc vec model many try achieve goal read model read paper saw implementation arabic task think problem solved train model whole arabic islamic data set collected data set related field idea training model use embed manual annotated topic individuality clarification separate row like single phrase embed represent single topic vector embed user query expected one word vector using trained model calculate cosine similarity query vector topic vector map topic get highest similarity related verse retrieve user code reading quran meaning saying prophet data training model getting vector query choice using function get similar document model output definitely expected model accuracy bad know correct parameter like passed model achieve highest accuracy little understanding parameter functionality need help tune also want know dataset enough build model need increase inaccuracy mistake collecting passing model suggestion opinion
Query data dimension must match training data dimension,"<p>I'm developing a tweet classifier. I trained a knn clasiffier with a a tfidf dataset in which each row has a length of 3.173, after training the model a load it into a file so that I can classify new tweets. </p>

<p>The problem is that every time I extract new tweets and try to classify them the tfidf lenths varies dependending on the vocabulary of new extracted tweets, so it is impossible for the model to classify those new tweets. </p>

<p>I've been searching and trying to solve this for two days but did not find an efficient solution. How can I adapt the dimension of the querying data to the dimension of the training data efficiently???</p>

<p>Here is my code:</p>

<pre><code> #CLASIFICA TWEETS TASS TEST
    clf = joblib.load('files/model_knn_pos.sav')

    #Carga los tweets
    dfNew = pd.read_csv(f'files/tweetsTASStestCaract.csv', encoding='UTF-8',sep='|')

    #Preprocesa 
    prepro = Preprocesado()
    dfNew['clean_text'] = prepro.procesa(dfNew['tweet'])

    #Tercer excluso
    dfNew['type'].replace(['NEU','N','NONE'], 'NoPos', inplace=True)

    #Funcion auxiliar para crear los vectores
    def tokenize(s):
        return s.split()

    #Creo un vector por cada tweet, tendré en cuenta las palabras q aparezcan al menos 3 veces
    vect = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 2), max_df=0.75, min_df=3, sublinear_tf=True)
    muestra = vect.fit_transform(dfNew['clean_text']).toarray().tolist()

    #Caracterizo los tweets a clasificar
    for i in range(len(muestra)):
            caract=dfNew.drop(columns=['tweet','clean_text','type']).values[i]
            muestra[i].extend(caract)

    #Clasifica pos
    y_train=dfNew['type'].values
    resultsPos = clf.predict(muestra)
    print(Counter(resultsPos))  
</code></pre>

<p>And this is the error I get:</p>

<blockquote>
  <p>File ""sklearn/neighbors/binary_tree.pxi"", line 1294, in
  sklearn.neighbors.kd_tree.BinaryTree.query</p>
  
  <p>ValueError: query data dimension must match training data dimension</p>
</blockquote>
",Vectorization & Embeddings,query data dimension must match training data dimension developing tweet classifier trained knn clasiffier tfidf dataset row ha length training model load file classify new tweet problem every time extract new tweet try classify tfidf lenths varies dependending vocabulary new extracted tweet impossible model classify new tweet searching trying solve two day find efficient solution adapt dimension querying data dimension training data efficiently code error get file sklearn neighbor binary tree pxi line sklearn neighbor kd tree binarytree query valueerror query data dimension must match training data dimension
How to perform clustering on Word2Vec,"<p>I have a semi-structured dataset, each row pertains to a single user:</p>

<pre><code>id, skills
0,""java, python, sql""
1,""java, python, spark, html""
2, ""business management, communication""
</code></pre>

<p>Why semi-structured is because the followings skills can only be selected from a list of 580 unique values.</p>

<p>My goal is to cluster users, or find similar users based on similar skillsets. I have tried using a Word2Vec model, which gives me very good results to identify similar skillsets - For eg. </p>

<pre><code>model.most_similar([""Data Science""])
</code></pre>

<p>gives me -</p>

<pre><code>[('Data Mining', 0.9249375462532043),
 ('Data Visualization', 0.9111810922622681),
 ('Big Data', 0.8253220319747925),...
</code></pre>

<p>This gives me a very good model for identifying <strong>individual</strong> skills and not group of skills. how do I make use of the vector provided from the Word2Vec model to successfully cluster groups of similar users?</p>
",Vectorization & Embeddings,perform clustering word vec semi structured dataset row pertains single user semi structured following skill selected list unique value goal cluster user find similar user based similar skillsets tried using word vec model give good result identify similar skillsets eg give give good model identifying individual skill group skill make use vector provided word vec model successfully cluster group similar user
How to calculate cosine similarity between scalar and vector?,"<p>How to calculate cosine similarity between scalar and vector in Python? </p>

<p>I am trying to multiply the output of a ngram model's probability with the output of a pretrained word2vec model to rerank the next possible word using word prediction. (Natural language processing)</p>

<p>Is there a library for this? I tried sklearn's here:<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html</a></p>

<p>But it only takes in two vectors to calculate pairwise cosine similarity.</p>
",Vectorization & Embeddings,calculate cosine similarity scalar vector calculate cosine similarity scalar vector python trying multiply output ngram model probability output pretrained word vec model rerank next possible word using word prediction natural language processing library tried sklearn take two vector calculate pairwise cosine similarity
Optimizing word2vec model comparisons,"<p>I have a word2vec model for every user, so I understand what two words look like on different models. Is there a more optimized way to compare the trained models than this? </p>

<pre><code>userAvec = Word2Vec.load(userAvec.w2v)  
userBvec = Word2Vec.load(userBvec.w2v)  

#for word in vocab, perform dot product:

cosine_similarity = np.dot(userAvec['president'], userBvec['president'])/(np.linalg.norm(userAvec['president'])* np.linalg.norm(userBvec['president']))
</code></pre>

<p>Is this the best way to compare two models? Is there a stronger way to see how two models compare rather than word by word? Picture 1000 users/models, each with similar number of words in the vocab. </p>
",Vectorization & Embeddings,optimizing word vec model comparison word vec model every user understand two word look like different model optimized way compare trained model best way compare two model stronger way see two model compare rather word word picture user model similar number word vocab
"Using featuretools for text data (word count, tfidf)","<p>Featuretools is best for relational categorical and numerical data. </p>

<p>Regarding text it seems that it only counts text length and some other very basic stats. </p>

<p>What would be the best pipeline for preparing textual data for featuretools?</p>

<p>Should it be done with <code>make_trans_primitive</code> or it would be better to prepare data some other way?</p>
",Vectorization & Embeddings,using featuretools text data word count tfidf featuretools best relational categorical numerical data regarding text seems count text length basic stats would best pipeline preparing textual data featuretools done would better prepare data way
Find all potential similar documents out of a list of documents using clustering,"<p>I'm working with the quora question pairs csv file which I loaded into a pd dataframe and isolated the qid and question so my questions are in this form : </p>

<pre><code>0        What is the step by step guide to invest in sh...
1        What is the step by step guide to invest in sh...
2        What is the story of Kohinoor (Koh-i-Noor) Dia...
3        What would happen if the Indian government sto...
.....
19408    What are the steps to solve this equation: [ma...
19409                           Is IMS noida good for BCA?
19410              How good is IMS Noida for studying BCA?
</code></pre>

<p>My dataset is actually bigger (500k questions) but I will use these questions to showcase my problem.</p>

<p>I want to identify pairs of questions that have a high probability of asking the same thing. I thought about the naive way, which is to turn each sentence into a vector using doc2vec and then for each sentence calculate the cosine similarity with every other sentence. Then, keep the one with the highest similarity and in the end print all those that have a high enough cosine similarity. The problem is this would take ages to finish so I need another approach.</p>

<p>Then I found an answer in another question that suggests to use clustering to solve a similar problem. So following is the code I implemented based on that answer.</p>

<pre><code>""Load and transform the dataframe to a new one with only question ids and questions""
train_df = pd.read_csv(""test.csv"", encoding='utf-8')

questions_df=pd.wide_to_long(train_df,['qid','question'],i=['id'],j='drop')
questions_df=questions_df.drop_duplicates(['qid','question'])[['qid','question']]
questions_df.sort_values(""qid"", inplace=True)
questions_df=questions_df.reset_index(drop=True)

print(questions_df['question'])

# vectorization of the texts
vectorizer = TfidfVectorizer(stop_words=""english"")
X = vectorizer.fit_transform(questions_df['question'].values.astype('U'))
# used words (axis in our multi-dimensional space)
words = vectorizer.get_feature_names()
print(""words"", words)


n_clusters=30
number_of_seeds_to_try=10
max_iter = 300
number_of_process=2 # seads are distributed
model = KMeans(n_clusters=n_clusters, max_iter=max_iter, n_init=number_of_seeds_to_try, n_jobs=number_of_process).fit(X)

labels = model.labels_
# indices of preferable words in each cluster
ordered_words = model.cluster_centers_.argsort()[:, ::-1]

print(""centers:"", model.cluster_centers_)
print(""labels"", labels)
print(""intertia:"", model.inertia_)

texts_per_cluster = numpy.zeros(n_clusters)
for i_cluster in range(n_clusters):
    for label in labels:
        if label==i_cluster:
            texts_per_cluster[i_cluster] +=1

print(""Top words per cluster:"")
for i_cluster in range(n_clusters):
    print(""Cluster:"", i_cluster, ""texts:"", int(texts_per_cluster[i_cluster])),
    for term in ordered_words[i_cluster, :10]:
        print(""\t""+words[term])

print(""\n"")
print(""Prediction"")

text_to_predict = ""Why did Donald Trump win the elections?""
Y = vectorizer.transform([text_to_predict])
predicted_cluster = model.predict(Y)[0]
texts_per_cluster[predicted_cluster]+=1

print(text_to_predict)
print(""Cluster:"", predicted_cluster, ""texts:"", int(texts_per_cluster[predicted_cluster])),
for term in ordered_words[predicted_cluster, :10]:
    print(""\t""+words[term])
</code></pre>

<p>I thought that this way I could find for each sentence the cluster that it most likely belongs in and then calculate the cosine similarity between all other questions of that cluster. This way instead of doing it on all the dataset I will be doing it on far fewer documents. However using the code for an example sentence ""Why did Donald Trump win the elections?"" I have the following results.</p>

<pre><code>Prediction
Why did Donald Trump win the elections?
Cluster: 25 texts: 244
    trump
    donald
    clinton
    hillary
    president
    vote
    win
    election
    did
    think
</code></pre>

<p>I know that my sentence belongs to cluster 25 and I can see the top words for that cluster. However how could I access the sentences that are in this cluster. Is there any way to do it?</p>
",Vectorization & Embeddings,find potential similar document list document using clustering working quora question pair csv file loaded pd dataframe isolated qid question question form dataset actually bigger k question use question showcase problem want identify pair question high probability asking thing thought naive way turn sentence vector using doc vec sentence calculate cosine similarity every sentence keep one highest similarity end print high enough cosine similarity problem would take age finish need another approach found answer another question suggests use clustering solve similar problem following code implemented based answer thought way could find sentence cluster likely belongs calculate cosine similarity question cluster way instead dataset far fewer document however using code example sentence donald trump win election following result know sentence belongs cluster see top word cluster however could access sentence cluster way
"NLP, spaCy: Strategy for improving document similarity","<p><strong>One sentence backdrop</strong>: I have text data from auto-transcribed talks, and I want to compare their similarity of their content (e.g. what they are talking about) to do clustering and recommendation. I am quite new to NLP.</p>
<hr />
<p><strong>Data</strong>: The data I am using is available <a href=""https://github.com/TMorville/transcribed_data"" rel=""nofollow noreferrer"">here</a>. For all the lazy ones</p>
<p><code>clone https://github.com/TMorville/transcribed_data</code></p>
<p>and here is a snippet of code to put it in a df:</p>
<pre><code>import os, json
import pandas as pd

from pandas.io.json import json_normalize 

def td_to_df():
    
    path_to_json = '#FILL OUT PATH'
    json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('td.json')]

    tddata = pd.DataFrame(columns=['trans', 'confidence'])

    for index, js in enumerate(json_files):
        with open(os.path.join(path_to_json, js)) as json_file:
            json_text = json_normalize(json.load(json_file))

            tddata['trans'].loc[index] = str(json_text['trans'][0])
            tddata['confidence'].loc[index] = str(json_text['confidence'][0])

    return tddata
</code></pre>
<hr />
<p><strong>Approach</strong>: So far, I have only used the spaCy package to do &quot;out of the box&quot; similarity. I simply apply the nlp model on the entire corpus of text, and compare it to all others.</p>
<pre><code>def similarity_get():
    
    tddata = td_to_df()
    
    nlp = spacy.load('en_core_web_lg')
    
    baseline = nlp(tddata.trans[0])
    
    for text in tddata.trans:
        print (baseline.similarity(nlp(text)))
</code></pre>
<hr />
<p><strong>Problem</strong>: <em>Practically all similarities comes out as &gt; 0.95</em>. This is more or less independent of the baseline.  Now, this may not come a major surprise given the lack of preprocessing.</p>
<hr />
<p><strong>Solution strategy</strong>: Following the advice in <a href=""https://stackoverflow.com/questions/49767270/document-similarity-in-spacy-vs-word2vec"">this post</a>, I would like to do the following (using spaCy where possible): 1) Remove stop words. 2) Remove most frequent words. 3) Merge word pairs. 4) Possibly use Doc2Vec outside of spaCy.</p>
<hr />
<p><strong>Questions</strong>: Does the above seem like a sound strategy? If no, what's missing? If yes, how much of this already happening under the hood by using the pre-trained model loaded in <code>nlp = spacy.load('en_core_web_lg')</code>?</p>
<p>I can't seem find the documentation that demonstrates what exactly these models are doing, or how to configure it. A <a href=""https://www.google.dk/search?ei=Xj8ZW8i2FKGB6QSx4qX4Cg&amp;q=spacy.load%20config%20api&amp;oq=spacy.load%20config%20api&amp;gs_l=psy-ab.3...8259.8938.0.9041.4.4.0.0.0.0.73.281.4.4.0....0...1c.1.64.psy-ab..0.3.218...33i160k1.0.0yzROYBDp-g"" rel=""nofollow noreferrer"">quick google search yields nothing</a> and even the, very neat, <a href=""https://spacy.io/api/"" rel=""nofollow noreferrer"">api documentation</a> does not seem to help. Perhaps I am looking in the wrong place?</p>
",Vectorization & Embeddings,nlp spacy strategy improving document similarity one sentence backdrop text data auto transcribed talk want compare similarity content e g talking clustering recommendation quite new nlp data data using available lazy one snippet code put df approach far used spacy package box similarity simply apply nlp model entire corpus text compare others problem practically similarity come le independent baseline may come major surprise given lack preprocessing solution strategy following advice quick google search yield nothing even neat api documentation doe seem help perhaps looking wrong place
Am I applying embedding layer in seq2seq correctly in inference model?,"<p>I am new to NLP and Keras and am still learning.</p>

<p>I tried to follow this guide: <a href=""https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"" rel=""nofollow noreferrer"">https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html</a> and have added an embedding layer. I am using fra2eng dataset.</p>

<p>However, I am not sure if my inference model and generation of output code are correct. Basically, my decoder input is an array of an index (a single number) feed into my inference model. </p>

<p>I am not very sure if this is correct. Do let me know if more background info or code is needed.</p>

<p>input_seq is an array of index of word in vocabulary.</p>

<pre><code>array([36., 64., 57.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.], dtype=float32)
</code></pre>

<p>0 is my index of start token
so my first target seq is np.array([0]) --> not sure if its correct</p>

<pre><code>def decode_sequence(input_seq):
    # Encode the input as state vectors.
    states_value = encoder_model.predict(input_seq)
    target_seq = np.array([0])
    stop_condition = False
    decoded_sentence = ''
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = target_idx2char[sampled_token_index]
        decoded_sentence += sampled_char
        if (sampled_char == '\n' or len(decoded_sentence) &gt; 20):
            stop_condition = True
            # Update the target sequence (of length 1).
        target_seq = np.array([sampled_token_index])
            # Update states
        states_value = [h, c]
    return decoded_sentence
</code></pre>

<p>Here is my output, was wondering if the output is due to any error above.</p>

<pre><code>Input sentence: Go.
Decoded sentence: tréjous?!

-
Input sentence: Run!
Decoded sentence: ï

-
Input sentence: Run!
Decoded sentence: ï

-
Input sentence: Wow!
Decoded sentence: u te les fois.

-
Input sentence: Fire!
Decoded sentence: ïï

-
Input sentence: Help!
Decoded sentence: ez joi de l'argent.

</code></pre>
",Vectorization & Embeddings,applying embedding layer seq seq correctly inference model new nlp kera still learning tried follow guide added embedding layer using fra eng dataset however sure inference model generation output code correct basically decoder input array index single number feed inference model sure correct let know background info code needed input seq array index word vocabulary index start token first target seq np array sure correct output wa wondering output due error
How does word2vec work to find sentence similarity?,"<p>I am using word2vec/doc2vec to find text similarities of two documents.
I studied that word2vec works on two approaches :</p>

<ul>
<li>CBOW : which predicts words on the basis of its context</li>
<li>Skipgram : which predicts context on the basis of the word</li>
</ul>

<p>But I am stuck at understanding how these two approaches works in calculating the text similarities. 
Also which one is the better approach for the current task.</p>
",Vectorization & Embeddings,doe word vec work find sentence similarity using word vec doc vec find text similarity two document studied word vec work two approach cbow predicts word basis context skipgram predicts context basis word stuck understanding two approach work calculating text similarity also one better approach current task
How to train my own custom word embedding on web pages?,"<p>I have tons of text data on multiple web pages about the product I am interested to sell to customers. I tried using pre-trained fasttext word embedding trained on Wikipedia and it didn't give me good results for the classification task. Probably because the text data on the website contains lots of technical details and its different from text data in wikipedia. So I would like to do some kind of transfer learning of word embedding keeping the pretrained fasttext word embedding as base.  </p>

<ol>
<li>How can I train my own custom word embedding on these web pages using Keras?</li>
<li>How can I initialize the custom word embedding with fasttext pre-trained embedding and train? Will this initialization really help in giving better word embedding?</li>
</ol>

<p>I would prefer a solution using Keras for training the word embedding.<br>
I know Embedding has trainable=True option not sure how I can use it.</p>

<pre><code>Embedding(voc_size, emb_dim, weights=[embedding_matrix], input_length, trainable=True)
</code></pre>

<p>Which framework due to recommend for this Keras or Gensim and why?</p>
",Vectorization & Embeddings,train custom word embedding web page ton text data multiple web page product interested sell customer tried using pre trained fasttext word embedding trained wikipedia give good result classification task probably text data website contains lot technical detail different text data wikipedia would like kind transfer learning word embedding keeping pretrained fasttext word embedding base train custom word embedding web page using kera initialize custom word embedding fasttext pre trained embedding train initialization really help giving better word embedding would prefer solution using kera training word embedding know embedding ha trainable true option sure use framework due recommend kera gensim
How to optimize preprocess all text documents without using for loop to preprocess a single text document in each iteration?,"<p>I want to optimize the below code so that it can process the 3000 text data efficiently and that data then will be fed to TFIDF Vectorizer and linkage() for clustering.</p>

<p>So far, I have read the excel using pandas and saved the dataframe into list variable. Then I iterated on the list by each text element in the list into tokens and then filtering out the stopwords from the element. The filtered element is stored into another variable and that variable is stored in the list. So at the end, I created a list of processed text elements(from list).</p>

<p>I think that optimization can be performed when a list is created and when stopwords are filtered out and also while saving the data into two different variables:  documents_no_stopwords and processed_words.</p>

<p>It would be great if someone can help me on this or suggest me a direction to follow.</p>

<pre><code>temp=0
df=pandas.read_excel('File.xlsx')

for text in df['text'].tolist():
    temp=temp+1
    preprocessing(text)
    print temp


def preprocessing(word):

    tokens = tokenizer.tokenize(word)

    processed_words = []
    for w in tokens:
        if w in stop_words:
            continue
        else:
    ## a new list is created with only the nouns in them for each text document
            processed_words.append(w)
    ## This step creates a list of text documents with only the nouns in them
    documents_no_stopwords.append(' '.join(processed_words))
    processed_words=[]
</code></pre>
",Vectorization & Embeddings,optimize preprocess text document without using loop preprocess single text document iteration want optimize code process text data efficiently data fed tfidf vectorizer linkage clustering far read excel using panda saved dataframe list variable iterated list text element list token filtering stopwords element filtered element stored another variable variable stored list end created list processed text element list think optimization performed list created stopwords filtered also saving data two different variable document stopwords processed word would great someone help suggest direction follow
How to use tfidf in text classification?,"<p>I have a dataset which has 300000 lines, each line of which is an article title, I want to find features like <code>tf</code> or <code>tfidf</code> of this dataset.
I am able to count the words(tf) in this dataset, such as:<br>
WORD FREQUENCE<br>
<code>must 10000</code><br>
<code>amazing 9999</code></p>

<p>or <code>word percentage</code>:<br>
<code>must 0.2</code><br>
<code>amazing 0.19</code></p>

<p>but how to caculate <code>idf</code>, I mean I need to find some features to discriminate this dataset from the others? or HOW DOES <code>tfidf</code> used in text classification?</p>
",Vectorization & Embeddings,use tfidf text classification dataset ha line line article title want find feature like dataset able count word tf dataset word frequence caculate mean need find feature discriminate dataset others doe used text classification
Classify movies based on their ratings using their subtitles-accuracy very bad,"<p>I have a data set of 130 movies and their subtitles.I have to classify them based on their ratings (R,NR,PG,PG-13,G).(language used python)
I did the following:
1)tokenized the data using treebank whitespace and wordpunc tokenizers.
2)lemmatized the data.(lemmatization gave more accuracy when pos tags were included. )
3)removed stop words and punctuation.
4)for movies belonging to each class, performed tfidf vectorization and picked the top 1000 words using max_features and constructed a data frame of size 125 *5000.
5)I applied several classification and clustering algorithms and they gave me the following accuracy:
SVC: test accuracy :0.325 and train accuracy :0.63
Naive Bayes: test acc:0.25 and train accuracy :0.33
knn: test accuracy : 0.41
kmeans:test accuracy:0.162
Logistic regression: test accuracy:0.53 and training accuracy of : 0.96</p>

<p>What should I do to improve my accuracy?
Am I making any mistakes or missing out on something important?</p>
",Vectorization & Embeddings,classify movie based rating using subtitle accuracy bad data set movie subtitle classify based rating r nr pg pg g language used python following tokenized data using treebank whitespace wordpunc tokenizers lemmatized data lemmatization gave accuracy po tag included removed stop word punctuation movie belonging class performed tfidf vectorization picked top word using max feature constructed data frame size applied several classification clustering algorithm gave following accuracy svc test accuracy train accuracy naive bayes test acc train accuracy knn test accuracy kmeans test accuracy logistic regression test accuracy training accuracy improve accuracy making mistake missing something important
What does &#39;discourse vector&#39; mean in word/sentence embedding?,"<p>While I read the paper below, I got a question what the discourse vector is. and how this vector is made up.</p>

<ul>
<li>S.Arora (TACL 2016): A Latent variable model approach to PMI-based Word Embeddings</li>
<li>S.Arora (ICLR 2017): A simple but tough-to-beat baseline for sentence embeddings</li>
</ul>

<p>In this paper, it says,
""discourse vector represents what is being talked about""</p>

<p>But it is not clear to me.</p>

<p>In summary, my question is</p>

<ol>
<li>what the discourse vector means; is it topic or context or something else?</li>
<li>Then if so, how do we create this vector?</li>
<li>how this vector learn? or is it fixed?</li>
</ol>
",Vectorization & Embeddings,doe discourse vector mean word sentence embedding read paper got question discourse vector vector made arora tacl latent variable model approach pmi based word embeddings arora iclr simple tough beat baseline sentence embeddings paper say discourse vector represents talked clear summary question discourse vector mean topic context something else create vector vector learn fixed
fix misspelled words in a corpus without dictionary,"<p>We have a history of conversations between humans (any language, any vocabulary), so with a lof of spelling errors:</p>

<pre><code>""hellobb do u hav skip?"" =&gt; ""hello baby, do you have skype?""
</code></pre>

<p>Before running a deep learning task against this data set (find synonyms etc..), I would like to fix these errors.</p>

<p>Is it a good idea? I've never worked with such bad quality data. Wondering if there is a ""magic solution"" to achieve this.</p>

<p>Else I plan to use:</p>

<ul>
<li>word embeddings (word2vec) to check if good and bad words are similar</li>
<li>distance function between words</li>
<li>if wordA is less famous wordB then fix(wordA) = wordB</li>
</ul>
",Vectorization & Embeddings,fix misspelled word corpus without dictionary history conversation human language vocabulary lof spelling error running deep learning task data set find synonym etc would like fix error good idea never worked bad quality data wondering magic solution achieve else plan use word embeddings word vec check good bad word similar distance function word worda le famous wordb fix worda wordb
Add Features to An Sklearn Classifier,"<p>I'm building a SGDClassifier, and using a tf<em>idf transformer. Aside from the features created from tf</em>idf, I'd also like to add additional features like document length or other ratings. How can I add these features to the feature-set? Here is how the classifier is constructed in a pipeline:</p>

<pre><code>data = fetch_20newsgroups(subset='train', categories=None)
pipeline = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', SGDClassifier()),
])
parameters = {
    'vect__max_df': (0.5, 0.75, 1.0),
    'vect__max_features': (None, 5000, 10000, 50000),
    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams
    'tfidf__use_idf': (True, False),
}

grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)
grid_search.fit(data.data, data.target)
print(grid_search.best_score_)
</code></pre>
",Vectorization & Embeddings,add feature sklearn classifier building sgdclassifier using tfidf transformer aside feature created tfidf also like add additional feature like document length rating add feature feature set classifier constructed pipeline
Natural language Processing using TfidfVectorizer,"<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
filename='train1.txt'
dataset=[]
with open(filename) as f:
    for line in f:
        dataset.append([str(n) for n in line.strip().split(',')])
print (dataset)
tfidf=TfidfVectorizer()
tfidf.fit(dataset)
dict1=tfidf.vocabulary_
print 'Using tfidfVectorizer'
for key in dict1.keys():
    print key+"" ""+ str(dict1[key])
</code></pre>

<p>I'm reading strings in file train1.txt. But when trying to execute the statement tfidf.fit(dataset),its resulting in an error. I'm unable to fix the error completely.Looking for help.</p>

<p>Error Log:</p>

<pre><code>Traceback (most recent call last):
  File ""Q1.py"", line 52, in &lt;module&gt;
    tfidf.fit(dataset)
  File ""/opt/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.py"", line 1361, in fit
    X = super(TfidfVectorizer, self).fit_transform(raw_documents)
  File ""/opt/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.py"", line 869, in fit_transform
    self.fixed_vocabulary_)
  File ""/opt/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.py"", line 792, in _count_vocab
    for feature in analyze(doc):
  File ""/opt/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.py"", line 266, in &lt;lambda&gt;
    tokenize(preprocess(self.decode(doc))), stop_words)
  File ""/opt/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.py"", line 232, in &lt;lambda&gt;
    return lambda x: strip_accents(x.lower())
AttributeError: 'list' object has no attribute 'lower'
</code></pre>
",Vectorization & Embeddings,natural language processing using tfidfvectorizer reading string file train txt trying execute statement tfidf fit dataset resulting error unable fix error completely looking help error log
How to train millions of doc2vec embeddings using GPU?,"<p>I am trying to train a doc2vec based on user browsing history (urls tagged to user_id). I use chainer deep learning framework. </p>

<p>There are more than 20 millions (user_id and urls) of embeddings to initialize which doesn’t fit in a GPU internal memory (maximum available 12 GB). Training on CPU is very slow.</p>

<p>I am giving an attempt using code written in chainer given here
<a href=""https://github.com/monthly-hack/chainer-doc2vec"" rel=""nofollow noreferrer"">https://github.com/monthly-hack/chainer-doc2vec</a></p>

<p>Please advise options to try if any.</p>
",Vectorization & Embeddings,train million doc vec embeddings using gpu trying train doc vec based user browsing history url tagged user id use chainer deep learning framework million user id url embeddings initialize fit gpu internal memory maximum available gb training cpu slow giving attempt using code written chainer given please advise option try
Feature extraction NLP,"<p>I'm working on a reviews dataset. The problem is to fetch the important(number of times the same feature reviewed) positive and negative features of that specific product from the reviews.</p>

<p>Ex: <code>some xyz car</code></p>

<p><strong>positive:</strong> Great mileage, good looking, spacious etc</p>

<p><strong>Negative:</strong> Poor power, bad performance, software problems etc</p>

<p>Thing is to extract the best and worst things about the product!</p>

<p>Until now I've used gensim's doc2vec to find the top positive and negative sentence. The results are not so good and because it gets similar sentences with structure, not similar feathers it holds.</p>
",Vectorization & Embeddings,feature extraction nlp working review dataset problem fetch important number time feature reviewed positive negative feature specific product review ex positive great mileage good looking spacious etc negative poor power bad performance software problem etc thing extract best worst thing product used gensim doc vec find top positive negative sentence result good get similar sentence structure similar feather hold
Save gensim Word2vec model in binary format .bin with save_word2vec_format,"<p>I'm training my own word2vec model using different data. To implement the resulting model into my classifier and compare the results with the original pre-trained Word2vec model I need to save the model in binary extension .bin. Here is my code, <em>sentences</em> is a list of short messages.</p>

<pre><code>import gensim, logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
sentences = gensim.models.word2vec.LineSentence('dati.txt')
model = gensim.models.Word2Vec(
sentences, size=300, window=5, min_count=5, workers=5,
sg=1, hs=1, negative=0
)
model.save_word2vec_format('model.bin', binary=True)
</code></pre>

<p>The last method, save_word2vec_format, gives me this error:</p>

<p><code>
AttributeError: 'Word2Vec' object has no attribute 'save_word2vec_format'
</code></p>

<p>What am I missing here? I've read the documentation of gensim and other forums. This <a href=""https://github.com/devmount/GermanWordEmbeddings/blob/c2b603a07d968146995ee9dde54a25fd0aa8586a/training.py#L56"" rel=""noreferrer"">repo on github</a> uses almost the same configuration so I cannot understand what's wrong. I've tried to switch from skipgram to cbow and from hierarchical softmax to negative sampling with no results.</p>

<p>Thank you in advance!</p>
",Vectorization & Embeddings,save gensim word vec model binary format bin save word vec format training word vec model using different data implement resulting model classifier compare result original pre trained word vec model need save model binary extension bin code sentence list short message last method save word vec format give error missing read documentation gensim forum repo github us almost configuration understand wrong tried switch skipgram cbow hierarchical softmax negative sampling result thank advance
Doc2Vec vs Avg Word Vectors : Which is better for Sentiment Analysis?,"<p>I was performing Sentiment Analysis on the IMdb dataset on Kaggle. I used the BOW approach with bigrams and that gave me a decent accuracy of ~89%. But I dont know how to approach the same using word embeddings: Should i go for averaged word vectors or doc2vec?</p>

<p>Someone please help. Thanks in advance.</p>
",Vectorization & Embeddings,doc vec v avg word vector better sentiment analysis wa performing sentiment analysis imdb dataset kaggle used bow approach bigram gave decent accuracy dont know approach using word embeddings go averaged word vector doc vec someone please help thanks advance
Pretrained (Word2Vec) embedding in Neural Networks,"<p>If I have to use pretrained word vectors as embedding layer in Neural Networks (eg. say CNN), How do I deal with index 0?</p>

<p><strong>Detail:</strong> </p>

<p>We usually start with creating a zero numpy 2D array. Later we fill in the indices of words from the vocabulary. 
The problem is, 0 is already the index of another word in our vocabulary (say, 'i' is index at 0). Hence, we are basically initializing the whole matrix filled with 'i' instead of empty words. So, how do we deal with padding all the sentences of equal length?</p>

<p>One easy pop-up in mind is we can use the another digit=numberOfWordsInVocab+1 to pad. But wouldn't that take more size? [Help me!]</p>
",Vectorization & Embeddings,pretrained word vec embedding neural network use pretrained word vector embedding layer neural network eg say cnn deal index detail usually start creating zero numpy array later fill index word vocabulary problem already index another word vocabulary say index hence basically initializing whole matrix filled instead empty word deal padding sentence equal length one easy pop mind use another digit numberofwordsinvocab pad take size help
how to reduce the dimension of the document embedding?,"<p>Let us assume that I have a set of document embeddings. (D) 
Each of document embedding is consisting of N number of word vectors where each of these pre-trained vector has 300 dimensions.</p>

<p>The corpus would be represented as [D,N,300].</p>

<p>My question is that, what would be the best way to reduce [D,N,300] to [D,1, 300]. How should I represent the document in a single vector instead of N vectors?</p>

<p>Thank you in advance.</p>
",Vectorization & Embeddings,reduce dimension document embedding let u assume set document embeddings document embedding consisting n number word vector pre trained vector ha dimension corpus would represented n question would best way reduce n represent document single vector instead n vector thank advance
How does Word2Vec ensure that antonyms will be far apart in the vector space,"<p>Broadly speaking the training of word2vec is a process in which words that are often in the same context are clustered together in the vector space.
We start by randomly shuffling the words on the plane and then with each iteration more and more clusters form. 
I think I understood this but how can we assure that the words that are antonyms or rarely appear in the same context don't end up in clusters that are close by?  Also how can we know that words that are more irrelevant are farther away than word that are less irrelevant.</p>
",Vectorization & Embeddings,doe word vec ensure antonym far apart vector space broadly speaking training word vec process word often context clustered together vector space start randomly shuffling word plane iteration cluster form think understood assure word antonym rarely appear context end cluster close also know word irrelevant farther away word le irrelevant
Embedding vs inserting word vectors directly to input layer,"<p>I used gensim to build a word2vec embedding of my corpus.
Currently I'm converting my (padded) input sentences to the word vectors using the gensim model.
This vectors are used as input for the model.</p>

<pre><code>model = Sequential()
model.add(Masking(mask_value=0.0, input_shape=(MAX_SEQUENCE_LENGTH, dim)))
model.add(Bidirectional(
    LSTM(num_lstm, dropout=0.5, recurrent_dropout=0.4, return_sequences=True))
)
...
model.fit(training_sentences_vectors, training_labels, validation_data=validation_data)
</code></pre>

<p>Are there any drawbacks using the word vectors directly without a keras embedding layer?</p>

<p>I'm also currently adding additional (one-hot encoded) tags to the input tokens by concatenating them to each word vector, does this approach make sense?</p>
",Vectorization & Embeddings,embedding v inserting word vector directly input layer used gensim build word vec embedding corpus currently converting padded input sentence word vector using gensim model vector used input model drawback using word vector directly without kera embedding layer also currently adding additional one hot encoded tag input token concatenating word vector doe approach make sense
How to improve the reproducibility of Doc2vec cosine similarity,"<p>I am using Gensim's Doc2vec to train a model, and I use the infer_vector to infer the vector of a new document to compare the similarity document of the model. However, reusing the same document can have very different results. This way there is no way to accurately evaluate similar documents.<br>
The search network mentions that infer_vector has random characteristics, so each time a new text vector is produced, it will be different.<br>
Is there any way to solve this problem?</p>

<pre><code>model_dm =pickle.load(model_pickle)

inferred_vector_dm = model_dm.infer_vector(i)  

simsinput =model_dm.docvecs.most_similar([inferred_vector_dm],topn=10)
</code></pre>
",Vectorization & Embeddings,improve reproducibility doc vec cosine similarity using gensim doc vec train model use infer vector infer vector new document compare similarity document model however reusing document different result way way accurately evaluate similar document search network mention infer vector ha random characteristic time new text vector produced different way solve problem
Concatenate char embeddings and word embeddings,"<p>I want to use char sequences and word sequences as inputs. Each of them will be embedded its related vocabulary and then resulted embeddings will be concatenated. I write following code to concatenate two embeddings:</p>

<pre><code>char_model = Sequential()
char_model.add(Embedding(vocab_size, char_emnedding_dim,input_length=char_size,embeddings_initializer='random_uniform',trainable=False, input_shape=(char_size, )))

word_model = Sequential()
word_model.add(Embedding(word_vocab_size,word_embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False,input_shape=(max_length, )))

model = Sequential()
model.add(Concatenate([char_model, word_model]))
model.add(Dropout(drop_prob))
model.add(Conv1D(filters=250, kernel_size=3, padding='valid', activation='relu', strides = 1))
model.add(GlobalMaxPooling1D())
model.add(Dense(hidden_dims)) # fully connected layer
model.add(Dropout(drop_prob)) 
model.add(Activation('relu'))
model.add(Dense(num_classes)) 
model.add(Activation('softmax'))
print(model.summary())
</code></pre>

<p>When I execute the code, I have the following error:</p>

<pre><code>ValueError: This model has not yet been built. Build the model first by calling build() or calling fit() with some data. Or specify input_shape or batch_input_shape in the first layer for automatic build. 
</code></pre>

<p>I defined <code>input_shape</code> for each embedding, but I still have same error. How can I concatenate two sequential model?</p>
",Vectorization & Embeddings,concatenate char embeddings word embeddings want use char sequence word sequence input embedded related vocabulary resulted embeddings concatenated write following code concatenate two embeddings execute code following error defined embedding still error concatenate two sequential model
Is Elmo a word embedding or a sentence embedding?,"<p>Supposedly, Elmo is a word embedding.
So if the input is a sentence or a sequence of words, the output should be a sequence of vectors. Apparently, this is not the case.</p>

<p>The code below uses keras and tensorflow_hub.</p>

<pre><code>a = ['aaa bbbb cccc uuuu vvvv wrwr', 'ddd ee fffff ppppp']
a = np.array(a, dtype=object)[:, np.newaxis]
#a.shape==(2,1)

input_text = layers.Input(shape=(1,), dtype=""string"")
embedding = ElmoEmbeddingLayer()(input_text)
model = Model(inputs=[input_text], outputs=embedding)

model.summary()
</code></pre>

<p>The class ElmoEmbedding is from <a href=""https://github.com/strongio/keras-elmo/blob/master/Elmo%20Keras.ipynb"" rel=""nofollow noreferrer"">https://github.com/strongio/keras-elmo/blob/master/Elmo%20Keras.ipynb</a>.</p>

<pre><code>b = model.predict(a)
#b.shape == (2, 1024)
</code></pre>

<p>Apparently, the embedding assigns a 1024-dimensional vector to each sentence. This is confusing.</p>

<p>Thank you.</p>
",Vectorization & Embeddings,elmo word embedding sentence embedding supposedly elmo word embedding input sentence sequence word output sequence vector apparently case code us kera tensorflow hub class elmoembedding apparently embedding assigns dimensional vector sentence confusing thank
Cosine Similarity between keywords,"<p>I'm new to document similarity in python and I'm confused about how to go about working with some data. Basically, I want to get the cosine similarity between dicts containing keywords. </p>

<p>I have dicts like so, which I am getting straight from a database:</p>

<pre><code>{'hat': 0.12, 'cat': 0.33, 'sat': 0.45}
{'rat': 0.22, 'bat':0.98, 'cat': 0.01}
</code></pre>

<p>I query the database and I get back data in this format. These are each lists of keywords and their respective tf-idf scores/weights. </p>

<pre><code>{'keyword': tfidf_score}
</code></pre>

<p>All I want to do is get the cosine similarity between these two dicts, weighted by the tfidf score. Looking online, I was pretty overwhelmed by all the different python libraries/modules when it comes to document similarity. I have no idea if there is some built-in function out there that I can just pass these sorts of json objects to, if I should be writing my own function that uses the weights, or what. </p>

<p>Any help is appreciated!</p>

<p>Thank you!</p>
",Vectorization & Embeddings,cosine similarity keywords new document similarity python confused go working data basically want get cosine similarity dicts containing keywords dicts like getting straight database query database get back data format list keywords respective tf idf score weight want get cosine similarity two dicts weighted tfidf score looking online wa pretty overwhelmed different python library module come document similarity idea built function pas sort json object writing function us weight help appreciated thank
How to use doc2vec embeddings as an input to a neural network,"<p>I'm trying to slowly begin working on a Twitter recommender system as part of a project, which requires me to use some form of deep learning. My goal is to recommend other tweets based on the topical content of a tweet with unlabelled data.</p>

<p>I have pre-processed my data and trained a few variations of models in doc2vec to get both word embeddings and document embeddings. But my issue is that I feel a little lost with where to go from here. I've read that doc2vec can be used as an input to a deeper neural network for training such as an LSTM or even a CNN.</p>

<p>Could anyone help me understand how these document embeddings (and word embeddings, I trained the model on DM mode) are used as input and what the purpose of the neural net would be in this case, is it for clustering? I understand the question is a little open-ended but I'm quite new to all this, any help would be appreciated.</p>
",Vectorization & Embeddings,use doc vec embeddings input neural network trying slowly begin working twitter recommender system part project requires use form deep learning goal recommend tweet based topical content tweet unlabelled data pre processed data trained variation model doc vec get word embeddings document embeddings issue feel little lost go read doc vec used input deeper neural network training lstm even cnn could anyone help understand document embeddings word embeddings trained model dm mode used input purpose neural net would case clustering understand question little open ended quite new help would appreciated
Calculate perplexity of word2vec model,"<p>I trained Gensim W2V model on 500K sentences (around 60K) words and I want to calculate the perplexity.</p>

<ol>
<li>What will be the best way to do so?</li>
<li>for 60K words, how can I check what will be a proper amount of data?</li>
</ol>

<p>Thanks</p>
",Vectorization & Embeddings,calculate perplexity word vec model trained gensim w v model k sentence around k word want calculate perplexity best way k word check proper amount data thanks
What does the embedding layer for a network looks like?,"<p>I just start with text classification, and I got stuck in the embedding layer. If I have a batch of sequences encoded as integer corresponding to each word, what does the embedding layer looks like? Is there neurons like normal neural layer? </p>

<p>I've seen the <code>keras.layers.Embedding</code>, but after looking for the document I'm really confused about how does it works. I can understand <code>input_dim</code>, but why is <code>output_dim</code> a 2D matrix? How many weights do I have in this embedding layer?</p>

<p>I'm sorry if my question is not explained clearly, I've no experience in NLP, if this problem about word embedding is common basics in NLP, please tell me and I will check for it.</p>
",Vectorization & Embeddings,doe embedding layer network look like start text classification got stuck embedding layer batch sequence encoded integer corresponding word doe embedding layer look like neuron like normal neural layer seen looking document really confused doe work understand matrix many weight embedding layer sorry question explained clearly experience nlp problem word embedding common basic nlp please tell check
"Understanding word embeddings, convolutional layer and max pooling layer in LSTMs and RNNs for NLP Text Classification","<p>Here is my input data:</p>

<pre><code>data['text'].head()

0    process however afforded means ascertaining di...
1          never occurred fumbling might mere mistake 
2    left hand gold snuff box which capered hill cu...
3    lovely spring looked windsor terrace sixteen f...
4    finding nothing else even gold superintendent ...
Name: text, dtype: object
</code></pre>

<p>And here is the one hot encoded label (multi-class classification where the number of classes = 3)</p>

<pre><code>[[1 0 0]
 [0 1 0]
 [1 0 0]
 ...
 [1 0 0]
 [1 0 0]
 [0 1 0]]
</code></pre>

<p>Here is what I think happens step by step, please correct me if I'm wrong:</p>

<ol>
<li><p>Converting my input text <code>data['text']</code> to a bag of indices (sequences)</p>

<pre><code>vocabulary_size = 20000

tokenizer = Tokenizer(num_words = vocabulary_size)
tokenizer.fit_on_texts(data['text'])
sequences = tokenizer.texts_to_sequences(data['text'])

data = pad_sequences(sequences, maxlen=50)
</code></pre></li>
</ol>

<p>What is happening is my <code>data['text'].shape</code> which is of shape <code>(19579, )</code> is being converted into an array of indices of shape <code>(19579, 50)</code>, where each word is being replaced by the index found in <code>tokenizer.word_index.items()</code></p>

<ol start=""2"">
<li><p>Loading the <code>glove 100d</code> word vector</p>

<pre><code>embeddings_index = dict()
f = open('/Users/abhishekbabuji/Downloads/glove.6B/glove.6B.100d.txt')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print(embedding_index)
    {'the': array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,
    -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,
     0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,
    -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,
     0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,
    -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,
     0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,
     0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,
    -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,
    -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,
    -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,
    -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,
    -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,
    -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,
    -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,
     0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,
    -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32),
</code></pre></li>
</ol>

<p>So what we have now are the word vectors for every word of 100 dimensions. </p>

<ol start=""3"">
<li><p>Creating the embedding matrix using the glove word vector </p>

<pre><code>vocabulary_size = 20000
embedding_matrix = np.zeros((vocabulary_size, 100))

for word, index in tokenizer.word_index.items():
    if index &gt; vocabulary_size - 1:
        break
    else:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[index] = embedding_vector
</code></pre></li>
</ol>

<p>So we now have the a <code>vector</code> of 100 dimensions for EACH of the 20000 words. The </p>

<p>And here is the architecture:</p>

<pre><code>model_glove = Sequential()
model_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))
model_glove.add(Dropout(0.5))
model_glove.add(Conv1D(64, 5, activation='relu')) 
model_glove.add(MaxPooling1D(pool_size=4))
model_glove.add(LSTM(100))
model_glove.add(Dense(3, activation='softmax'))
model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model_glove.summary())
</code></pre>

<p>I get </p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_7 (Embedding)      (None, 50, 100)           2000000   
_________________________________________________________________
dropout_7 (Dropout)          (None, 50, 100)           0         
_________________________________________________________________
conv1d_7 (Conv1D)            (None, 46, 64)            32064     
_________________________________________________________________
max_pooling1d_7 (MaxPooling1 (None, 11, 64)            0         
_________________________________________________________________
lstm_7 (LSTM)                (None, 100)               66000     
_________________________________________________________________
dense_7 (Dense)              (None, 3)                 303       
=================================================================
Total params: 2,098,367
Trainable params: 98,367
Non-trainable params: 2,000,000
_________________________________________________________________
</code></pre>

<p>The input to the above architecture will be the training data</p>

<pre><code>array([[    0,     0,     0, ...,  4867,    22,   340],
       [    0,     0,     0, ...,    12,   327,  2301],
       [    0,     0,     0, ...,   255,   388,  2640],
       ...,
       [    0,     0,     0, ...,    17, 15609, 15242],
       [    0,     0,     0, ...,  9517,  9266,   442],
       [    0,     0,     0, ...,  3399,   379,  5927]], dtype=int32)
</code></pre>

<p>of shape <code>(19579, 50)</code></p>

<p>and labels as one hot encodings..</p>

<p>My trouble is understanding the following what exactly is happening to my <code>(19579, 50)</code> as it goes through each of the following lines:</p>

<pre><code>model_glove = Sequential()
model_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))
model_glove.add(Dropout(0.5))
model_glove.add(Conv1D(64, 5, activation='relu')) 
model_glove.add(MaxPooling1D(pool_size=4))
</code></pre>

<p>I understand why we need <code>model_glove.add(Dropout(0.5))</code>, this is to shut down some hidden units with a probability of 0.5 to avoid the model from being overly complex. But I have no idea why we need the <code>Conv1D(64, 5, activation='relu')</code>, the <code>MaxPooling1D(pool_size=4)</code> and how this goes into my <code>model_glove.add(LSTM(100))</code> unit..</p>
",Vectorization & Embeddings,understanding word embeddings convolutional layer max pooling layer lstms rnns nlp text classification input data one hot encoded label multi class classification number class think happens step step please correct wrong converting input text bag index sequence happening shape converted array index shape word replaced index found loading word vector word vector every word dimension creating embedding matrix using glove word vector dimension word architecture get input architecture training data shape label one hot encoding trouble understanding following exactly happening go following line understand need shut hidden unit probability avoid model overly complex idea need go unit
Best way to handle OOV words when using pretrained embeddings in PyTorch,"<p>I am using word2vec pretrained embedding in PyTorch (following code <a href=""https://stackoverflow.com/questions/49710537/pytorch-gensim-how-to-load-pre-trained-word-embeddings"">here</a>). However, it does not seem to handle unseen words. Is there any good way to solve it?</p>
",Vectorization & Embeddings,best way handle oov word using pretrained embeddings pytorch using word vec pretrained embedding pytorch following code href however doe seem handle unseen word good way solve p
How to change the tensor shape in middle layers?,"<p>Saying I have a 2000x100 matrix, I put it into 10 dimension embedding layer, which gives me 2000x100x10 tensor. so it's 2000 examples and each example has a 100x10 matrix. and then, I pass it to a conv1d and KMaxpolling to get 2000x24 matrix, which is 2000 examples and each example has a 24 dimension vector. and now, I would like to recombine those examples before I apply another layer. I would like to combine the first 10 examples together, and such and such, so I get a tuple. and then I pass that tuple to the next layer.
My question is, Can I do that with Keras? and any idea on how to do it?   </p>
",Vectorization & Embeddings,change tensor shape middle layer saying x matrix put dimension embedding layer give x x tensor example example ha x matrix pas conv kmaxpolling get x matrix example example ha dimension vector would like recombine example apply another layer would like combine first example together get tuple pas tuple next layer question kera idea
Find similarity with doc2vec like word2vec,"<p>Is there a way to find similar docs like we do in word2vec</p>

<p>Like:</p>

<pre><code>  model2.most_similar(positive=['good','nice','best'],
    negative=['bad','poor'],
    topn=10)
</code></pre>

<p>I know we can use infer_vector,feed them to have similar ones, but I want to feed many positive and negative examples as we do in word2vec.</p>

<p>is there any way we can do that! thanks !</p>
",Vectorization & Embeddings,find similarity doc vec like word vec way find similar doc like word vec like know use infer vector feed similar one want feed many positive negative example word vec way thanks
"I&#39;ve computed TF AND IDF, but how to get TF-IDF?","<p>From my code below:</p>

<pre><code>def dot(docA,docB):
    the_sum=0
    for (key,value) in docA.items():
        the_sum+=value*docB.get(key,0)
    return the_sum

def cos_sim(docA,docB):
    sim=dot(docA,docB)/(math.sqrt(dot(docA,docA)*dot(docB,docB)))
    return sim

def doc_freq(doclist):
    df={}
    for doc in doclist:
        for feat in doc.keys():
            df[feat]=df.get(feat,0)+1
    return df

def idf(doclist):
    N=len(doclist)
    return {feat:math.log(N/v) for feat,v in doc_freq(doclist).items()} 


tf_med=doc_freq(bow_collections[""medline""])
tf_wsj=doc_freq(bow_collections[""wsj""])

idf_med=idf(bow_collections[""medline""])
idf_wsj=idf(bow_collections[""wsj""])

print(tf_med)
print(idf_med)
</code></pre>

<p>So I've managed to finally get this far, though I can't seem to find information on what I have to do next in terms of Python, sure the maths is there but I don't feel it necessary to spend hours trying to understand what it means. Just a quick reassurance this is what I get from tf_med:</p>

<pre><code>{'NUM': 37, 'early': 3, 'case': 3, 'organ': 1, 'transplantation': 1, 'section': 1, 
'healthy': 1, 'ovary': 1, 'fertile': 1, 'woman': 1, 'unintentionally': 1, 
'unknowingly': 1, 'subjected': 1, 'oophorectomy': 1, 'described': 4, .... , }
</code></pre>

<p>And here is what I get from idf_med:</p>

<pre><code>{'NUM': 0.3011050927839216, 'early': 2.8134107167600364, 'case': 2.8134107167600364, 
'organ': 3.912023005428146, 'transplantation': 3.912023005428146, 'section': 
3.912023005428146, 'healthy': 3.912023005428146, 'ovary': 3.912023005428146, 'fertile': 
3.912023005428146, .... , }
</code></pre>

<p>Though now I don't know how to compute these two together to get me my TF-IDF and from there my average cosine similarities. I understand they need to be multiplied but how on earth do I go about doing that!</p>
",Vectorization & Embeddings,computed tf idf get tf idf code managed finally get far though seem find information next term python sure math feel necessary spend hour trying understand mean quick reassurance get tf med get idf med though know compute two together get tf idf average cosine similarity understand need multiplied earth go
How to obtain embedded representation of single test instance after training,"<p>The first layer of my RNN is embedded layer as follows.</p>

<pre><code>visible = Input(shape=(250,)) 
embed=Embedding(vocab_size,50)(visible)     
x2=keras.layers.GRU(224, return_sequences=False)(embed)
predictions=Dense(1, activation='sigmoid')(x2)
</code></pre>

<p>I train this network and predict some output. </p>

<p>Now after training, I test it on my test data which is very straight forward. </p>

<p>I want to know the exact embeddings of my test data or for the sake of implementation, embeddings of any of my test sequence after the model has been trained. I want to use those embedding in some other application. Is there any way to extract those embedding for my test sequence data after the training?</p>

<p>Try: Let's say my model is m, the I obtain the embedding weights as follows.</p>

<pre><code>embeddings = m.layers[1].get_weights()
</code></pre>

<p>I also have a reverse dictionary <code>reverse_char_map</code> as follows.</p>

<pre><code>{1: 'c',
 2: 'C',
 3: '(',
 4: ')',
 5: 'O',
 6: '=',
 7: '1',
 8: '2',
 9: 'N',
 10: '3',
 11: 'n',
 12: '[',
 13: ']',
 14: 'S',
 15: '4',
 16: '-',
 17: 'l',
 18: '+',
 19: 'H',
 20: '5',
 21: 'F',
 22: '.',
 23: '#',
 24: 's',
 25: 'o',
 26: '6',
 27: 'P',
 28: 'B',
 29: 'r',
 30: 'a',
 31: '7',
 32: 'e',
 33: 'I',
 34: 'i',
 35: '8',
 36: 'u',
 37: 'K',
 38: '9',
 39: 'R',
 40: '%',
 41: '0',
 42: 'Z',
 43: 'h',
 44: 'L',
 45: 'A',
 46: 't',
 47: 'd',
 48: 'G',
 49: 'M',
 50: 'g',
 51: 'U',
 52: 'b',
 53: 'T',
 54: 'W',
 55: 'p',
 56: 'V'}
</code></pre>

<p>I don't know how to use <code>embeddings</code> with <code>reverse_char_map</code> to obtain embeddings for one of the test input sequence.</p>
",Vectorization & Embeddings,obtain embedded representation single test instance training first layer rnn embedded layer follows train network predict output training test test data straight forward want know exact embeddings test data sake implementation embeddings test sequence model ha trained want use embedding application way extract embedding test sequence data training try let say model obtain embedding weight follows also reverse dictionary follows know use obtain embeddings one test input sequence
Effective way to compute cosine similarity for sparse tensors in python?,"<p>I have a list of unit <strong>tensors(length = 1)</strong>. This list contains <strong>~20 000</strong> such tensors. Tensors have <strong>~3 000</strong>  dimensions but are very sparse. Only <strong>x (0 &lt; x &lt; 1)</strong> dimensions are <strong>not 0</strong>. And I need to compute cosine similarity between all these tensors. What is the most effective way to do this? <em>(This is not an NLP task, but my solution looks similar to <strong>word2Vect</strong> approach, that's why I have added NLP tag. My tensor has more dimensions than <strong>word2vec</strong> and it is more sparse)</em></p>
",Vectorization & Embeddings,effective way compute cosine similarity sparse tensor python list unit tensor length list contains tensor tensor dimension sparse x x dimension need compute cosine similarity tensor effective way nlp task solution look similar word vect approach added nlp tag tensor ha dimension word vec sparse
"word2vec: user-level, document-level embeddings with pre-trained model","<p>I am currently developing a Twitter content-based recommender system and have a word2vec model pre-trained on 400 million tweets.</p>

<p>How would I go about using those word embeddings to create a document/tweet-level embedding and then get the user embedding based on the tweets they had posted? </p>

<p>I was initially intending on averaging those words in a tweet that had a word vector representation and then averaging the document/tweet vectors to get a user vector but I wasn't sure if this was optimal or even correct. Any help is much appreciated.</p>
",Vectorization & Embeddings,word vec user level document level embeddings pre trained model currently developing twitter content based recommender system word vec model pre trained million tweet would go using word embeddings create document tweet level embedding get user embedding based tweet posted wa initially intending averaging word tweet word vector representation averaging document tweet vector get user vector sure wa optimal even correct help much appreciated
Online updating Word2Vec,"<p>I've got a problem with online updating my Word2Vec model.</p>

<p>I have a document and build model by it. But this document can update with new words, and I need to update vocabulary and model in general.</p>

<p>I know that in gensim 0.13.4.1 we can do this</p>

<p>My code:</p>

<pre><code>model = gensim.models.Word2Vec(size=100, window=10, min_count=5, workers=11, alpha=0.025, min_alpha=0.025, iter=20)
model.build_vocab(sentences, update=False)

model.train(sentences, epochs=model.iter, total_examples=model.corpus_count)

model.save('model.bin')
</code></pre>

<p>And after this I have new words. For e.x.:</p>

<pre><code>sen2 = [['absd', 'jadoih', 'sdohf'], ['asdihf', 'oisdh', 'oiswhefo'], ['a', 'v', 'b', 'c'], ['q', 'q', 'q']]

model.build_vocab(sen2, update=True)
model.train(sen2, epochs=model.iter, total_examples=model.corpus_count)
</code></pre>

<p>What's wrong and how can I solve my problem?</p>
",Vectorization & Embeddings,online updating word vec got problem online updating word vec model document build model document update new word need update vocabulary model general know gensim code new word e x wrong solve problem
Interpreting and using principal components of word embeddings,"<p>Imagine you have a set of semantically related words (e.g. restaurant, food, dish, waiter), along with a few relatively unrelated words (e.g. sad, angry, iphone). How would you go about finding these ""anomalous"" words? </p>

<p>I'm using word vectors (e.g. fasttext, glove) to represent these words and one simple way that works to some extent is to sort the vectors based on their distance from their mean. But this isn't perfect...</p>

<p>I've considered using PCA, but not sure if that is a good approach or how exactly to find the anomalous words using it. </p>

<p>Thanks a lot!</p>
",Vectorization & Embeddings,interpreting using principal component word embeddings imagine set semantically related word e g restaurant food dish waiter along relatively unrelated word e g sad angry iphone would go finding anomalous word using word vector e g fasttext glove represent word one simple way work extent sort vector based distance mean perfect considered using pca sure good approach exactly find anomalous word using thanks lot
Find an match similar elements in two columns in Python,"<p>I have a data set like:</p>

<pre><code>Column1                Column2
 a bc                    cdr
 cd r                    ab c
 bose                    beats
 bea ts                  bo se
 i phone                 sam sung
 samsung                 iphone
</code></pre>

<p>If you notice both columns contains almost similar words, but are different in terms of format and have spaces in them. I want techniques such as Cosine Similarity or sequence matcher to match these to columns such that the results becomes like this:</p>

<pre><code>column 1                 column 2 
a bc                      ab c
cd r                      cdr 
bose                      bo se
bea ts                    beats
i phone                   iphone
samsung                   sam sung
</code></pre>

<p>Please not, this is just a sample data, the strings are more complex than these. 
How can I leverage packages such as Cosine Similarity and Sequence Matcher to make this happen?</p>
",Vectorization & Embeddings,find match similar element two column python data set like notice column contains almost similar word different term format space want technique cosine similarity sequence matcher match column result becomes like please sample data string complex leverage package cosine similarity sequence matcher make happen
Making Predictions on single review from input text using saved CNN model,"<p>I am making a classifier based on a CNN model in Keras.</p>

<p>I will use it in an application, where the user can load the application and enter input text and the model will be loaded from the weights and make predictions.</p>

<p>The thing is I am using GloVe embeddings as well and the CNN model uses padded text sequences as well.</p>

<p>I used Keras tokenizer as following:</p>

<pre><code>tokenizer = text.Tokenizer(num_words=max_features, lower=True, char_level=False)
tokenizer.fit_on_texts(list(train_x))

train_x = tokenizer.texts_to_sequences(train_x)
test_x = tokenizer.texts_to_sequences(test_x)

train_x = sequence.pad_sequences(train_x, maxlen=maxlen)
test_x = sequence.pad_sequences(test_x, maxlen=maxlen)
</code></pre>

<p>I trained the model and predicted on test data, but now I want to test the same with loaded model which I loaded and working.</p>

<p>But my problem here is If I provide a single review, it has to be passed through the <code>tokeniser.text_to_sequences()</code> which is returning 2D array, with a shape of <code>(num_chars, maxlength)</code> and hence followed by a <code>num_chars</code> predictions, but I need it in <code>(1, max_length)</code> shape.</p>

<p>I am using the following code for prediction:</p>

<pre><code>review = 'well free phone cingular broke stuck not abl offer kind deal number year contract up realli want razr so went look cheapest one could find so went came euro charger small adpat made fit american outlet, gillett fusion power replac cartridg number count packagemay not greatest valu out have agillett fusion power razor'
xtest = tokenizer.texts_to_sequences(review)
xtest = sequence.pad_sequences(xtest, maxlen=maxlen)

model.predict(xtest)
</code></pre>

<p>Output is:</p>

<pre><code>array([[0.29289   , 0.36136267, 0.6205081 ],
       [0.362869  , 0.31441122, 0.539749  ],
       [0.32059124, 0.3231736 , 0.5552745 ],
       ...,
       [0.34428033, 0.3363668 , 0.57663095],
       [0.43134686, 0.33979046, 0.48991954],
       [0.22115968, 0.27314988, 0.6188136 ]], dtype=float32)
</code></pre>

<p>I need a single prediction here <code>array([0.29289   , 0.36136267, 0.6205081 ])</code> as I have a single review.</p>
",Vectorization & Embeddings,making prediction single review input text using saved cnn model making classifier based cnn model kera use application user load application enter input text model loaded weight make prediction thing using glove embeddings well cnn model us padded text sequence well used kera tokenizer following trained model predicted test data want test loaded model loaded working problem provide single review ha passed returning array shape hence followed prediction need shape using following code prediction output need single prediction single review
Semantic similarity to compare two columns in data frames using sklearn,"<p>i face an issue to pass a function to compare between two column </p>

<pre><code>import nltk, string
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')

def cosine_sim1(text1, text2):
    tfidf = vectorizer.fit_transform([text1, text2])
    return ((tfidf * tfidf.T).A)[0,1]
</code></pre>

<p>after i apply the function </p>

<pre><code>cosine_sim1('like football', 'football')
</code></pre>

<p>The results is:
        0.5797386715376657</p>

<p>I face a little issue to pass that function between two column in dataframe to calculate the score. here is a small sample of the data </p>

<pre><code> d = pd.DataFrame({'A': ['my name is', 'i live in', 'i like football'], 'B': ['london is nice city', 'london city', 'football']})
</code></pre>

<p>i have tried to do like that. However there are some errors appears. </p>

<pre><code>def cosine_sim1(text1, text2):
    tfidf = vectorizer.fit_transform([text1(d['A']), text2(d['B'])])
    return ((tfidf * tfidf.T).A)[0,1]
d.apply(cosine_sim1, axis=1)   
</code></pre>

<p>The error is: 
TypeError: (""cosine_sim1() missing 1 required positional argument: 'text2'"", 'occurred at index 0')</p>
",Vectorization & Embeddings,semantic similarity compare two column data frame using sklearn face issue pas function compare two column apply function result face little issue pas function two column dataframe calculate score small sample data tried like however error appears error typeerror cosine sim missing required positional argument text occurred index
What is the best way to handle missing words when using word embeddings?,"<p>I have a set of pre-trained word2vec word vectors and a corpus. I want to use the word vectors to represent words in the corpus. The corpus has some words in it that I don't have trained word vectors for. What's the best way to handle those words for which there is no pre-trained vector?</p>

<p>I've heard several suggestions. </p>

<ol>
<li><p>use a vector of zeros for every missing word</p></li>
<li><p>use a vector of random numbers for every missing word (with a bunch of suggestions on how to bound those randoms)</p></li>
<li><p>an idea I had: take a vector whose values are the mean of all values in that position from all pre-trained vectors</p></li>
</ol>

<p>Anyone with experience with the problem have thoughts on how to handle this?</p>
",Vectorization & Embeddings,best way handle missing word using word embeddings set pre trained word vec word vector corpus want use word vector represent word corpus corpus ha word trained word vector best way handle word pre trained vector heard several suggestion use vector zero every missing word use vector random number every missing word bunch suggestion bound randoms idea take vector whose value mean value position pre trained vector anyone experience problem thought handle
NLP // Create vector representation of words using &#39;count-based&#39; model,"<p>So I know most people use prediction/regression models now like GloVe or word2vec, and creating vectors using the count-based model should actually be simpler, but I'm having a difficult time figuring it out (I think because I don't have much background in programming).</p>

<p>Basically, I want to create vector representations of English words, and I want to be able to set the dimensions for each vector (which will probably be something like the 2,000 most frequent words in whatever corpus I end up using). I also want to be able to set the window (for instance, look at x number of words before and after the target word).</p>

<p>What would be the best way to get started doing this (preferably using python)?</p>
",Vectorization & Embeddings,nlp create vector representation word using count based model know people use prediction regression model like glove word vec creating vector using count based model actually simpler difficult time figuring think much background programming basically want create vector representation english word want able set dimension vector probably something like frequent word whatever corpus end using also want able set window instance look x number word target word would best way get started preferably using python
How to use keras embedding layer with 3D tensor input?,"<p>I am facing difficulty in using Keras embedding layer with one hot encoding of my input data.</p>

<p>Following is the toy code.</p>

<p><strong>Import packages</strong></p>

<pre><code>from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.embeddings import Embedding
from keras.optimizers import Adam
import matplotlib.pyplot as plt
import numpy as np
import openpyxl
import pandas as pd
from keras.callbacks import ModelCheckpoint
from keras.callbacks import ReduceLROnPlateau
</code></pre>

<p>The input data is text based as follows.</p>

<p><strong>Train and Test data</strong></p>

<pre><code>X_train_orignal= np.array(['OC(=O)C1=C(Cl)C=CC=C1Cl', 'OC(=O)C1=C(Cl)C=C(Cl)C=C1Cl',
       'OC(=O)C1=CC=CC(=C1Cl)Cl', 'OC(=O)C1=CC(=CC=C1Cl)Cl',
       'OC1=C(C=C(C=C1)[N+]([O-])=O)[N+]([O-])=O'])

X_test_orignal=np.array(['OC(=O)C1=CC=C(Cl)C=C1Cl', 'CCOC(N)=O',
       'OC1=C(Cl)C(=C(Cl)C=C1Cl)Cl'])

Y_train=np.array(([[2.33],
       [2.59],
       [2.59],
       [2.54],
       [4.06]]))

Y_test=np.array([[2.20],
   [2.81],
   [2.00]])
</code></pre>

<p><strong>Creating dictionaries</strong></p>

<p>Now i create two dictionaries, characters to index vice. The unique character number is stored in <code>len(charset)</code> and maximum length of the string along with 5 additional characters is stored in <code>embed</code>. The start of each string will be padded with <code>!</code> and end will be <code>E</code>.</p>

<pre><code>charset = set("""".join(list(X_train_orignal))+""!E"")
char_to_int = dict((c,i) for i,c in enumerate(charset))
int_to_char = dict((i,c) for i,c in enumerate(charset))
embed = max([len(smile) for smile in X_train_orignal]) + 5
print (str(charset))
print(len(charset), embed)
</code></pre>

<p><strong>One hot encoding</strong> </p>

<p>I convert all the train data into one hot encoding as follows. </p>

<pre><code>def vectorize(smiles):
        one_hot =  np.zeros((smiles.shape[0], embed , len(charset)),dtype=np.int8)
        for i,smile in enumerate(smiles):
            #encode the startchar
            one_hot[i,0,char_to_int[""!""]] = 1
            #encode the rest of the chars
            for j,c in enumerate(smile):
                one_hot[i,j+1,char_to_int[c]] = 1
            #Encode endchar
            one_hot[i,len(smile)+1:,char_to_int[""E""]] = 1

        return one_hot[:,0:-1,:]

X_train = vectorize(X_train_orignal)
print(X_train.shape)
X_test = vectorize(X_test_orignal)
print(X_test.shape)
</code></pre>

<p>When it converts the input train data into one hot encoding, the shape of the one hot encoded data becomes <code>(5, 44, 14)</code> for train and <code>(3, 44, 14)</code> for test. For train, there are 5 example, 0-44 is the maximum length and 14 are the unique characters. The examples for which there are less number of characters, are padded with <code>E</code> till the maximum length.</p>

<p><strong>Verifying the correct padding</strong>
Following is the code to verify if we have done the padding rightly.</p>

<pre><code>mol_str_train=[]
mol_str_test=[]
for x in range(5):

    mol_str_train.append("""".join([int_to_char[idx] for idx in np.argmax(X_train[x,:,:], axis=1)]))

for x in range(3):
    mol_str_test.append("""".join([int_to_char[idx] for idx in np.argmax(X_test[x,:,:], axis=1)]))
</code></pre>

<p>and let's see, how the train set looks like.</p>

<pre><code>mol_str_train

['!OC(=O)C1=C(Cl)C=CC=C1ClEEEEEEEEEEEEEEEEEEEE',
 '!OC(=O)C1=C(Cl)C=C(Cl)C=C1ClEEEEEEEEEEEEEEEE',
 '!OC(=O)C1=CC=CC(=C1Cl)ClEEEEEEEEEEEEEEEEEEEE',
 '!OC(=O)C1=CC(=CC=C1Cl)ClEEEEEEEEEEEEEEEEEEEE',
 '!OC1=C(C=C(C=C1)[N+]([O-])=O)[N+]([O-])=OEEE']
</code></pre>

<p>Now is the time to build model.</p>

<p><strong>Model</strong></p>

<pre><code>model = Sequential()
model.add(Embedding(len(charset), 10, input_length=embed))
model.add(Flatten())
model.add(Dense(1, activation='linear'))

def coeff_determination(y_true, y_pred):
    from keras import backend as K
    SS_res =  K.sum(K.square( y_true-y_pred ))
    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )
    return ( 1 - SS_res/(SS_tot + K.epsilon()) )

def get_lr_metric(optimizer):
    def lr(y_true, y_pred):
        return optimizer.lr
    return lr


optimizer = Adam(lr=0.00025)
lr_metric = get_lr_metric(optimizer)
model.compile(loss=""mse"", optimizer=optimizer, metrics=[coeff_determination, lr_metric])



callbacks_list = [
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-15, verbose=1, mode='auto',cooldown=0),
    ModelCheckpoint(filepath=""weights.best.hdf5"", monitor='val_loss', save_best_only=True, verbose=1, mode='auto')]


history =model.fit(x=X_train, y=Y_train,
                              batch_size=1,
                              epochs=10,
                              validation_data=(X_test,Y_test),
                              callbacks=callbacks_list)
</code></pre>

<p><strong>Error</strong></p>

<pre><code>ValueError: Error when checking input: expected embedding_3_input to have 2 dimensions, but got array with shape (5, 44, 14)
</code></pre>

<p>The embedding layer expects two dimensional array. How can I deal with this issue so that it can accept the one hot vector encoded data.</p>

<p>All the above code can be run. </p>
",Vectorization & Embeddings,use kera embedding layer tensor input facing difficulty using kera embedding layer one hot encoding input data following toy code import package input data text based follows train test data creating dictionary create two dictionary character index vice unique character number stored maximum length string along additional character stored start string padded end one hot encoding convert train data one hot encoding follows convert input train data one hot encoding shape one hot encoded data becomes train test train example maximum length unique character example le number character padded till maximum length verifying correct padding following code verify done padding rightly let see train set look like time build model model error embedding layer expects two dimensional array deal issue accept one hot vector encoded data code run
Test data giving prediction error in Keras in the model with Embedding layer,"<p>I have trained a Bi-LSTM model to find NER on a set of sentences. For this I took the different words present and I did a mapping between a word and a number and then created the Bi-LSTM model using those numbers. I then create and pickle that model object.</p>

<p>Now I get a set of new sentences containing certain words that the training model has not seen. Thus these words do not have a numeric value till now. Thus when I test it on my previously existing model, it would give an error. It is not able to find the words or features as the numeric values for those do not exist.</p>

<p>To circumvent this error I gave a new integer value to all the new words that I see.</p>

<p>However, when I load the model and test it, it gives the error that: </p>

<pre><code>InvalidArgumentError: indices[0,24] = 5444 is not in [0, 5442)   [[Node: embedding_14_16/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true,
_device=""/job:localhost/replica:0/task:0/device:CPU:0""](embedding_14_16/embeddings/read, embedding_14_16/Cast)]]
</code></pre>

<p>The training data contains 5445 words including the padding word. Thus = [0, 5444]</p>

<p>5444 is the index value I have given to the paddings in the test sentences. Not clear why it is assuming the index values to range between [0, 5442).</p>

<p>I have used the base code available on the following link: <a href=""https://www.kaggle.com/gagandeep16/ner-using-bidirectional-lstm"" rel=""nofollow noreferrer"">https://www.kaggle.com/gagandeep16/ner-using-bidirectional-lstm</a></p>

<p>The code:</p>

<pre><code>input = Input(shape=(max_len,))
model = Embedding(input_dim=n_words, output_dim=50
                  , input_length=max_len)(input)

model = Dropout(0.1)(model)
model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)
out = TimeDistributed(Dense(n_tags, activation=""softmax""))(model)  # softmax output layer

model = Model(input, out)
model.compile(optimizer=""rmsprop"", loss=""categorical_crossentropy"", metrics=[""accuracy""])

#number of  epochs - Also for output file naming
epoch_num=20
domain=""../data/Laptop_Prediction_Corrected""
output_file_name=domain+""_E""+str(epoch_num)+"".xlsx""

model_name=""../models/Laptop_Prediction_Corrected""
output_model_filename=model_name+""_E""+str(epoch_num)+"".sav""


history = model.fit(X_tr, np.array(y_tr), batch_size=32, epochs=epoch_num, validation_split=0.1, verbose=1)
</code></pre>

<p><code>max_len</code> is the total number of words in a sentence and <code>n_words</code> is the vocab size. In the model the padding has been done using the following code where <code>n_words=5441</code>:</p>

<pre><code>X = pad_sequences(maxlen=max_len, sequences=X, padding=""post"", value=n_words)
</code></pre>

<p>The padding in the new dataset:</p>

<pre><code>max_len = 50
# this is to pad sentences to the maximum length possible
#-&gt; so all records of X will be of the same length

#X = pad_sequences(maxlen=max_len, sequences=X, padding=""post"", value=res_new_word2idx[""pad_blank""])

#X = pad_sequences(maxlen=max_len, sequences=X, padding=""post"", value=5441)
</code></pre>

<p>Not sure which of these paddings is correct?</p>

<p>However, the vocab only includes the words in the training data. When I say:</p>

<pre><code>p = loaded_model.predict(X)
</code></pre>

<p>How to use <code>predict</code> for text sentences which contain words that are not present in the initial vocab?</p>
",Vectorization & Embeddings,test data giving prediction error kera model embedding layer trained bi lstm model find ner set sentence took different word present mapping word number created bi lstm model using number create pickle model object get set new sentence containing certain word training model ha seen thus word numeric value till thus test previously existing model would give error able find word feature numeric value exist circumvent error gave new integer value new word see however load model test give error training data contains word including padding word thus index value given padding test sentence clear assuming index value range used base code available following link code total number word sentence vocab size model padding ha done using following code padding new dataset sure padding correct however vocab includes word training data say use text sentence contain word present initial vocab
Gensim Doc2vec model: how to compute similarity on a corpus obtained using a pre-trained doc2vec model?,"<p>I have a model based on <code>doc2vec</code> trained on multiple documents. I would like to use that model to infer the vectors of another document, which I want to use as the corpus for comparison. So, when I look for the most similar sentence to one I introduce, it uses this new document vectors instead of the trained corpus.
Currently, I am using the <code>infer_vector()</code> to compute the vector for each one of the sentences of the new document, but I can't use the <code>most_similar()</code> function with the list of vectors I obtain, it has to be <code>KeyedVectors</code>.</p>

<p>I would like to know if there's any way that I can compute these vectors for the new document that will allow the use of the <code>most_similar()</code> function, or if I have to compute the similarity between each one of the sentences of the new document and the sentence I introduce individually (in this case, is there any implementation in Gensim that allows me to compute the cosine similarity between 2 vectors?).</p>

<p>I am new to Gensim and NLP, and I'm open to your suggestions.</p>

<p>I can not provide the complete code, since it is a project for the university, but here are the main parts in which I'm having problems.</p>

<p>After doing some pre-processing of the data, this is how I train my model:</p>

<pre><code>documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(train_data)]
assert gensim.models.doc2vec.FAST_VERSION &gt; -1

cores = multiprocessing.cpu_count()

doc2vec_model = Doc2Vec(vector_size=200, window=5, workers=cores)
doc2vec_model.build_vocab(documents)
doc2vec_model.train(documents, total_examples=doc2vec_model.corpus_count, epochs=30)
</code></pre>

<p>I try to compute the vectors for the new document this way:</p>

<pre><code>questions = [doc2vec_model.infer_vector(line) for line in lines_4]
</code></pre>

<p>And then I try to compute the similarity between the new document vectors and an input phrase:</p>

<pre><code>text = str(input('Me: '))

tokens = text.split()

new_vector = doc2vec_model.infer_vector(tokens)

index = questions[i].most_similar([new_vector])
</code></pre>
",Vectorization & Embeddings,gensim doc vec model compute similarity corpus obtained using pre trained doc vec model model based trained multiple document would like use model infer vector another document want use corpus comparison look similar sentence one introduce us new document vector instead trained corpus currently using compute vector one sentence new document use function list vector obtain ha would like know way compute vector new document allow use function compute similarity one sentence new document sentence introduce individually case implementation gensim allows compute cosine similarity vector new gensim nlp open suggestion provide complete code since project university main part problem pre processing data train model try compute vector new document way try compute similarity new document vector input phrase
Genisim doc2vec: how is short doc processed?,"<p>In each tiny step of doc2vec training process, it takes a word and its neighbors within certain length(called window size). The neighbors are summed up, averaged, or concated, and so on and so on.</p>

<p>My question is, what if the window exceed the boundary of a certain doc, like 
<a href=""https://i.sstatic.net/Iy5e0.png"" rel=""nofollow noreferrer"">this</a></p>

<p>Then how are the neighbors summed up, averaged, or concated? Or they are just simply discarded? </p>

<p>I am doing some nlp work and most doc in my dataset are quite short. Appeciate for any idea.</p>
",Vectorization & Embeddings,genisim doc vec short doc processed tiny step doc vec training process take word neighbor within certain length called window size neighbor summed averaged concated question window exceed boundary certain doc like neighbor summed averaged concated simply discarded nlp work doc dataset quite short appeciate idea
How to transform the data and calculate the TFIDF value?,"<p>My data format is：
<code>datas = {[1,2,4,6,7],[2,3],[5,6,8,3,5],[2],[93,23,4,5,11,3,5,2],...}</code>
Each element in datas is a sentence ,and each number is a word.I want to get the TFIDF value for each number. How to do it with sklearn or other ways?</p>

<p>My code:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfTransformer  
from sklearn.feature_extraction.text import CountVectorizer  
datas = {[1,2,4,6,7],[2,3],[5,6,8,3,5],[2],[93,23,4,5,11,3,5,2]}
vectorizer=CountVectorizer()

transformer = TfidfTransformer()
tfidf = transformer.fit_transform(vectorizer.fit_transform(datas))  
print(tfidf)
</code></pre>

<p>My code doesn't work.Error:</p>

<pre><code>Traceback (most recent call last):   File
""C:/Users/zhuowei/Desktop/OpenNE-master/OpenNE-
master/src/openne/buildTree.py"", line 103, in &lt;module&gt;
    X = vectorizer.fit_transform(datas)   File
""C:\Users\zhuowei\Anaconda3\lib\site-
packages\sklearn\feature_extraction\text.py"", line 869, in fit_transform
    self.fixed_vocabulary_)   File ""C:\Users\zhuowei\Anaconda3\lib\site-
packages\sklearn\feature_extraction\text.py"", line 792, in _count_vocab
    for feature in analyze(doc):   File 
""C:\Users\zhuowei\Anaconda3\lib\site-
packages\sklearn\feature_extraction\text.py"", line 266, in &lt;lambda&gt;
    tokenize(preprocess(self.decode(doc))), stop_words)   File 
""C:\Users\zhuowei\Anaconda3\lib\site-
packages\sklearn\feature_extraction\text.py"", line 232, in &lt;lambda&gt;
    return lambda x: strip_accents(x.lower()) 
AttributeError: 'int' object has no attribute 'lower'
</code></pre>
",Vectorization & Embeddings,transform data calculate tfidf value data format element data sentence number word want get tfidf value number sklearn way code code work error
GloVe embeddings - unknown / out-of-vocabulary token,"<p>I would like to know if there is a general (default) <em>out-of-vocabulary (OOV)</em> token for <em>GloVe</em> embeddings. In particular for the pre-trained ones from Stanford:
<a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a></p>

<p>I found this on SO: <a href=""https://stackoverflow.com/questions/49239941/what-is-unk-in-glove-6b-50d-txt"">What is &quot;unk&quot; in glove.6B.50d.txt?</a></p>

<p>The given answer suggests that the token <code>""unk""</code> represents the <em>OOV-Token</em> and <a href=""https://github.com/stanfordnlp/GloVe/search?utf8=%E2%9C%93&amp;q=unk&amp;type="" rel=""nofollow noreferrer"">shared a link to the Glove project on Github</a> as evidence. <br>However this doesn't seem very conclusive to me as the link only refers to <code>""&lt;unk&gt;""</code> tokens in the code (not <code>""unk""</code>), but <code>""&lt;unk&gt;""</code> does existent in the vocabulary!</p>

<p>So I would like to know, if there is any (default) <em>OOV-token</em> for GloVe (what can be used for unknown/unseen words) and if so what is it? </p>
",Vectorization & Embeddings,glove embeddings unknown vocabulary token would like know general default vocabulary oov token glove embeddings particular pre trained one stanford found shared link glove project github evidence however seem conclusive link refers token code doe existent vocabulary would like know default oov token glove used unknown unseen word
How do I find a synonym of a word or multi-word paraphrase using the gensim toolkit,"<p>Having loaded a pre-trained word2vec model with the gensim toolkit, I would like to find a synonym of a word given a context such as intelligent for 'she is a bright person'.</p>
",Vectorization & Embeddings,find synonym word multi word paraphrase using gensim toolkit loaded pre trained word vec model gensim toolkit would like find synonym word given context intelligent bright person
High accuracy measures when using pretrained embedding layer in Python,"<p>I am trying to implement a pretrained embedding layer into my generative model using GloVe. </p>

<p>Into the model I feed sequences of 50 (X) items pulled from a text, and it is to predict the 51. word (y) in the text. </p>

<p>I reach an accuracy of 0.99 already when the model only has trained for 1/100 iterations. What can be the issue?</p>

<pre><code># create a weight matrix for words in training docs
embedding_matrix = zeros((vocab_size, 100))
for word, i in tokenizer.word_index.items():
embedding_vector = embeddings_index.get(word)
if embedding_vector is not None:
    embedding_matrix[i] = embedding_vector

 # define model
 model = Sequential() #assigning the sequential function to a model
model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=seq_length, trainable = False)) #defining embedding layer size
model.add(LSTM(100, return_sequences=True)) #adding layer of nodes
model.add(LSTM(100))  #adding layer of nodes
model.add(Dense(100, activation='relu')) #specifying the structure of the hidden layer, recu is an argument of a rectified linear unit. 
model.add(Dense(vocab_size, activation='softmax')) #using the softmax function to creating probabilities
print(model.summary())
# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# fit the model
model.fit(X, y, batch_size=128, epochs=100, verbose=1)
</code></pre>

<p>Link to github: <a href=""https://github.com/KiriKoppelgaard/Generative_model"" rel=""nofollow noreferrer"">https://github.com/KiriKoppelgaard/Generative_model</a> 
commit from  Nov 14, 2018 </p>
",Vectorization & Embeddings,high accuracy measure using pretrained embedding layer python trying implement pretrained embedding layer generative model using glove model feed sequence x item pulled text predict word text reach accuracy already model ha trained iteration issue link github commit nov
Why can&#39;t spacy differentiate between two homograph tokens in the following code?,"<p>A homograph is a word that has the same spelling as another word but has a different sound and a different meaning, for example,<strong>lead</strong> <em>(to go in front of)</em> / <strong>lead</strong> <em>(a metal)</em> .</p>
<p>I was trying to use spacy word vectors to compare documents with each other by summing each word vector for each document and then finally finding cosine similarity. If for example spacy vectors have the same vector for the two <strong>'lead'</strong> listed above , the results will be probably bad.</p>
<p>In the code below , why does the similarity between the two <strong>'bank'</strong>
tokens come out as <strong>1.00</strong> ?</p>
<pre><code>import spacy
nlp = spacy.load('en')

str1 = 'The guy went inside the bank to take out some money'
str2 = 'The house by the river bank.'

str1_tokenized = nlp(str1.decode('utf8'))
str2_tokenized = nlp(str2.decode('utf8'))

token1 = str1_tokenized[-6]
token2 = str2_tokenized[-2]

print 'token1 =  {}  token2 = {}'.format(token1,token2)

print token1.similarity(token2)
</code></pre>
<p>The output for given program is</p>
<blockquote>
<p>token1 =  bank  token2 = bank</p>
<p>1.0</p>
</blockquote>
",Vectorization & Embeddings,spacy differentiate two homograph token following code homograph word ha spelling another word ha different sound different meaning example lead go front lead metal wa trying use spacy word vector compare document summing word vector document finally finding cosine similarity example spacy vector vector two lead listed result probably bad code doe similarity two bank token come output given program token bank token bank
Unsupervised sentiment Analysis using doc2vec,"<p>Folks,</p>

<p>I have searched Google for different type of papers/blogs/tutorials etc but haven't found anything helpful. I would appreciate if anyone can help me. <strong>Please note that I am not asking for code step-by-step but rather an idea/blog/paper or some tutorial.</strong>   </p>

<p>Here's my problem statement:  </p>

<blockquote>
  <p>Just like sentiment analysis is used for identifying positive and
  negative tone of a sentence, I want to find whether a sentence is
  forward-looking (future outlook) statement or not.</p>
</blockquote>

<p>I do not want to use bag of words approach to sum up the number of forward-looking words/phrases such as <em>""going forward""</em>, ""<em>in near future</em>"" or ""<em>In 5 years from now</em>"" etc. I am not sure if word2vec or doc2vec can be used. Please enlighten me.  </p>

<p>Thanks. </p>
",Vectorization & Embeddings,unsupervised sentiment analysis using doc vec folk searched google different type paper blog tutorial etc found anything helpful would appreciate anyone help please note asking code step step rather idea blog paper tutorial problem statement like sentiment analysis used identifying positive negative tone sentence want find whether sentence forward looking future outlook statement want use bag word approach sum number forward looking word phrase going forward near future year etc sure word vec doc vec used please enlighten thanks
PCA on word2vec embeddings,"<p>I am trying to reproduce the results of this paper: <a href=""https://arxiv.org/pdf/1607.06520.pdf"" rel=""noreferrer"">https://arxiv.org/pdf/1607.06520.pdf</a></p>

<p>Specifically this part:</p>

<blockquote>
  <p>To identify the gender subspace, we took the ten gender pair difference vectors and computed its principal components (PCs). As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors. The first eigenvalue is significantly larger than the rest.</p>
</blockquote>

<p><a href=""https://i.sstatic.net/EOJJK.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/EOJJK.png"" alt=""enter image description here""></a></p>

<p>I am using the same set of word vectors as the authors (Google News Corpus, 300 dimensions), which I load into word2vec. </p>

<p>The 'ten gender pair difference vectors' the authors refer to are computed from the following word pairs:</p>

<p><a href=""https://i.sstatic.net/7b6Dj.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/7b6Dj.png"" alt=""enter image description here""></a></p>

<p>I've computed the differences between each normalized vector in the following way:</p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-
negative300.bin', binary = True)
model.init_sims()

pairs = [('she', 'he'),
('her', 'his'),
('woman', 'man'),
('Mary', 'John'),
('herself', 'himself'),
('daughter', 'son'),
('mother', 'father'),
('gal', 'guy'),
('girl', 'boy'),
('female', 'male')]

difference_matrix = np.array([model.word_vec(a[0], use_norm=True) - model.word_vec(a[1], use_norm=True) for a in pairs])
</code></pre>

<p>I then perform PCA on the resulting matrix, with 10 components, as per the paper:</p>

<pre><code>from sklearn.decomposition import PCA
pca = PCA(n_components=10)
pca.fit(difference_matrix)
</code></pre>

<p>However I get very different results when I look at <code>pca.explained_variance_ratio_</code> :</p>

<pre><code>array([  2.83391436e-01,   2.48616155e-01,   1.90642492e-01,
         9.98411858e-02,   5.61260498e-02,   5.29706681e-02,
         2.75670634e-02,   2.21957722e-02,   1.86491774e-02,
         1.99108478e-32])
</code></pre>

<p>or with a chart:</p>

<p><a href=""https://i.sstatic.net/RuNEi.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/RuNEi.png"" alt=""enter image description here""></a></p>

<p>The first component accounts for less than 30% of the variance when it should be above 60%! </p>

<p>The results I get are similar to what I get when I try to do the PCA on randomly selected vectors, so I must be doing something wrong, but I can't figure out what.</p>

<p>Note: I've tried without normalizing the vectors, but I get the same results.</p>
",Vectorization & Embeddings,pca word vec embeddings trying reproduce result paper specifically part identify gender subspace took ten gender pair difference vector computed principal component pc figure show single direction explains majority variance vector first eigenvalue significantly larger rest using set word vector author google news corpus dimension load word vec ten gender pair difference vector author refer computed following word pair computed difference normalized vector following way perform pca resulting matrix component per paper however get different result look chart first component account le variance result get similar get try pca randomly selected vector must something wrong figure note tried without normalizing vector get result
Can a matrix be given as input to Keras&#39;s embedding layer?,"<p>I am using Keras to capture semantic information for a dataset. And I already tokenize the data to integer vectors. It has a form like this:</p>

<pre><code>texts=[[1,2,3,2,1],
       [2,3,4,2,2],
       [3,33,2,1,3]]

labels=[1,0,1]
</code></pre>

<p>And the labels only contains 0 or 1, each list contain one label.
I want to use Keras's embedding layer to embed this. But the examples on the Internet only contain a list:</p>

<pre><code>texts=[1,2,3,4,2,1]
</code></pre>

<p>I am wondering can I input a matrix to the embedding layer?</p>
",Vectorization & Embeddings,matrix given input kera embedding layer using kera capture semantic information dataset already tokenize data integer vector ha form like label contains list contain one label want use kera embedding layer embed example internet contain list wondering input matrix embedding layer
How to implement Latent Dirichlet Allocation to give bigrams/trigrams in topics instead of unigrams,"<p>I used the gensim LDAModel for topic extraction for customer reviews as follows:</p>

<pre><code>dictionary = corpora.Dictionary(clean_reviews)
dictionary.filter_extremes(keep_n=11000) #change filters
dictionary.compactify()
dictionary_path = ""dictionary.dict""
corpora.Dictionary.save(dictionary, dictionary_path)

# convert tokenized documents to vectors

corpus = [dictionary.doc2bow(doc) for doc in clean_reviews]
vocab = lda.datasets.load_reuters_vocab()  

# Training lda using number of topics set = 10 (which can be changed)

lda = gensim.models.LdaModel(corpus, id2word = dictionary,
                        num_topics = 20,
                        passes = 20,
                        random_state=1,
                        alpha = ""auto"")
</code></pre>

<p>This returns unigrams in topics like:</p>

<pre><code>topic1 -delivery,parcel,location

topic2 -app, login, access
</code></pre>

<p>But I am looking for ngrams. I came across sklearn's LatentDirichletAllocation which uses Tfidf vectorizer as follows:</p>

<pre><code>vectorizer = TfidfVectorizer(analyzer='word', ngram_range=[2,5], stop_words='english', min_df=2)    
X = vectorizer.fit_transform(new_review_list)
clf = decomposition.LatentDirichletAllocation(n_topics=20, random_state=3, doc_topic_prior = .1).fit(X)
</code></pre>

<p>where we can specify range for ngrams in the vectorizer. Is it possible to do so in the gensim LDA Model as well.</p>

<p>Sorry, I'm very new to using all these models, so don't know much about them.</p>
",Vectorization & Embeddings,implement latent dirichlet allocation give bigram trigram topic instead unigrams used gensim ldamodel topic extraction customer review follows return unigrams topic like looking ngrams came across sklearn latentdirichletallocation us tfidf vectorizer follows specify range ngrams vectorizer possible gensim lda model well sorry new using model know much
What is the difference between TfidfVectorizer.fit_transfrom and tfidf.transform?,"<p>In Tfidf.fit_transform we are only using the parameters X and have not used y for fitting the data set.
Is this right?
We are generating the tfidf matrix for only parameters of the training set.We are not using ytrain in fitting the model.
Then how do we make predictions for the test data set</p>
",Vectorization & Embeddings,difference tfidfvectorizer fit transfrom tfidf transform tfidf fit transform using parameter x used fitting data set right generating tfidf matrix parameter training set using ytrain fitting model make prediction test data set
Dimension Problem in Keras Multilabel Classification with Word Embeddings,"<p>I am currently solving an exercise which involves reading in TED talks, labelling them according to the topics they are about, and training a Feed Forward NN in Keras that can label new talks accordingly, using pre-trained word embeddings.</p>

<p>Depending on what the talk is about (technology, education or design or multiple of those topics), it can have one of the following labels:</p>

<pre><code>labels_dict = {
'txx': 0, 'xex': 1, 'xxd': 2, 'tex': 3, 'txd': 4, 'xed': 5, 'ted': 6, 'xxx': 7
}
</code></pre>

<p>I load the talks like this:</p>

<pre><code>def load_talks(path):
tree = et.parse(path)
root = tree.getroot()
for file in root:
    label = ''
    keywords = file.find('head').find('keywords').text.lower()
    if 'technology' in keywords:
        label += 't'
    else:
        label += 'x'

    if 'education' in keywords:
        label += 'e'
    else:
        label += 'x'

    if 'design' in keywords:
        label += 'd'
    else:
        label += 'x'

    talk = file.find('content').text
    talk = process_text(talk)
    texts.append(talk)
    labels.append(labels_dict[label])
</code></pre>

<p>I then calculate TF-IDF scores for the tokens in the texts:</p>

<pre><code>tf_idf_vect = TfidfVectorizer()
tf_idf_vect.fit_transform(texts)
tf_idf_vectorizer_tokens = tf_idf_vect.get_feature_names()
</code></pre>

<p>Then I use a tokenizer to assign the tokens in the texts to indexes:</p>

<pre><code>t = Tokenizer()
t.fit_on_texts(texts)

vocab_size = len(t.word_index) + 1
encoded_texts = t.texts_to_sequences(texts)

print('Padding the docs')
# pad documents to a max length of 4 words
max_length = max(len(d) for d in encoded_texts)
padded_docs = pad_sequences(encoded_texts, maxlen=max_length, padding='post')
</code></pre>

<p>Next, I compute the embedding matrix:</p>

<pre><code>def compute_embedding_matrix(word_index, embedding_dim):
embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))
for word, i in word_index.items():
    embedding_vector = word_embeddings.get(word)
    if embedding_vector is not None and get_tf_idf_score(word) &gt; TF_IDF_THRESHOLD:
        # words not found in embedding index and with a too low tf-idf score will be all-zeros.
        embedding_matrix[i] = embedding_vector
return embedding_matrix

embedding_dim = load_word_embeddings('word_embeddings/de/de.tsv') + 1
embedding_matrix = compute_embedding_matrix(t.word_index, embedding_dim)
</code></pre>

<p>I then prepare the labels and split the data in training and testing:</p>

<pre><code>labels = to_categorical(np.array(labels))

X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.1, random_state=0)
</code></pre>

<p>The following prints output this:</p>

<pre><code>print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

(1647, 6204)
(184, 6204)
(1647, 8)
(184, 8)
</code></pre>

<p>I then prepare my model like this:</p>

<pre><code>e = Embedding(input_dim=vocab_size,
              weights=[embedding_matrix],
              input_length=max_length,
              output_dim=embedding_dim,
              trainable=False)

print('Preparing the network')
model = models.Sequential()
model.add(e)
model.add(layers.Flatten())
model.add(layers.Dense(units=100, input_dim=embedding_dim, activation='relu'))
model.add(layers.Dense(len(labels), activation='softmax'))
# compile the model
model.compile(loss='binary_crossentropy', metrics=['acc'])
# summarize the model
print(model.summary())
# fit the model
print('Fitting the model')
model.fit(X_train, y_train, epochs=10, verbose=0, batch_size=500)
# evaluate the model
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print('Accuracy: %f' % (accuracy*100))
</code></pre>

<p>This outputs the following error:</p>

<pre><code>embedding_1 (Embedding)      (None, 6204, 301)         47951106  
_________________________________________________________________
flatten_1 (Flatten)          (None, 1867404)           0         
_________________________________________________________________
dense_1 (Dense)              (None, 100)               186740500 
_________________________________________________________________
dense_2 (Dense)              (None, 1831)              184931    
=================================================================
Total params: 234,876,537
Trainable params: 186,925,431
Non-trainable params: 47,951,106
_________________________________________________________________
None
Fitting the model
    batch_size=batch_size)
  File ""/Users/tim/anaconda3/lib/python3.6/site-packages/keras/engine/training.py"", line 789, in _standardize_user_data
    exception_prefix='target')
  File ""/Users/tim/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py"", line 138, in standardize_input_data
    str(data_shape))
ValueError: Error when checking target: expected dense_2 to have shape (1831,) but got array with shape (8,)

Process finished with exit code 1
</code></pre>

<p>Can someone point me in the right direction about how to go about fitting the dimensions of this model?</p>
",Vectorization & Embeddings,dimension problem kera multilabel classification word embeddings currently solving exercise involves reading ted talk labelling according topic training feed forward nn kera label new talk accordingly using pre trained word embeddings depending talk technology education design multiple topic one following label load talk like calculate tf idf score token text use tokenizer assign token text index next compute embedding matrix prepare label split data training testing following print output prepare model like output following error someone point right direction go fitting dimension model
Doc2Vec: Similarity Between Coded Documents and Unseen Documents,"<p>I have a sample of ~60,000 documents.  We've hand coded 700 of them as having a certain type of content.  Now we'd like to find the ""most similar"" documents to the 700 we already hand-coded.  We're using gensim doc2vec and I can't quite figure out the best way to do this.</p>

<p>Here's what my code looks like:</p>

<pre><code>cores = multiprocessing.cpu_count()

model = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, 
        epochs=10, workers=cores, dbow_words=1, train_lbls=False)

all_docs = load_all_files() # this function returns a named tuple
random.shuffle(all_docs)
print(""Docs loaded!"")
model.build_vocab(all_docs)
model.train(all_docs, total_examples=model.corpus_count, epochs=5)
</code></pre>

<p>I can't figure out the right way to go forward.  Is this something that doc2vec can do?  In the end, I'd like to have a ranked list of the 60,000 documents, where the first one is the ""most similar"" document.</p>

<p>Thanks for any help you might have!  I've spent a lot of time reading the gensim help documents and the various tutorials floating around and haven't been able to figure it out.</p>

<p>EDIT: I can use this code to get the documents most similar to a short sentence:</p>

<pre><code>token = ""words associated with my research questions"".split()
new_vector = model.infer_vector(token)
sims = model.docvecs.most_similar([new_vector])
for x in sims:
    print(' '.join(all_docs[x[0]][0]))
</code></pre>

<p>If there's a way to modify this to instead get the documents most similar to the 700 coded documents, I'd love to learn how to do it!</p>
",Vectorization & Embeddings,doc vec similarity coded document unseen document sample document hand coded certain type content like find similar document already hand coded using gensim doc vec quite figure best way code look like figure right way go forward something doc vec end like ranked list document first one similar document thanks help might spent lot time reading gensim help document various tutorial floating around able figure edit use code get document similar short sentence way modify instead get document similar coded document love learn
Sequence Models Word2vec,"<p>I am working on data-set with more than 100,000 records.
This is how the data looks like:</p>

<pre><code>email_id    cust_id campaign_name
123         4567     World of Zoro
123         4567     Boho XYz
123         4567     Guess ABC
234         5678     Anniversary X
234         5678     World of Zoro
234         5678     Fathers day
234         5678     Mothers day
345         7890     Clearance event
345         7890     Fathers day
345         7890     Mothers day
345         7890     Boho XYZ
345         7890     Guess ABC
345         7890     Sale
</code></pre>

<p>I am trying to understand the campaign sequence and predict the next possible campaign for the customers.</p>

<p>Assume I have processed my data and stored it in 'camp'.</p>

<p>With Word2Vec-</p>

<pre><code>from gensim.models import Word2Vec

model = Word2Vec(sentences=camp, size=100, window=4, min_count=5, workers=4, sg=0)
</code></pre>

<p>The problem with this model is that it accepts tokens and spits out text-tokens with probabilities in return when looking for similarities.</p>

<p>Word2Vec accepts this form of input- </p>

<pre><code>['World','of','Zoro','Boho','XYZ','Guess','ABC','Anniversary','X'...]
</code></pre>

<p>And gives this form of output -</p>

<pre><code> model.wv.most_similar('Zoro')
[Guess,0.98],[XYZ,0.97]
</code></pre>

<p>Since I want to predict campaign sequence, I was wondering if there is anyway I can give below input to the model and get the campaign name in the output</p>

<p>My input to be as - </p>

<pre><code>[['World of Zoro','Boho XYZ','Guess ABC'],['Anniversary X','World of 
Zoro','Fathers day','Mothers day'],['Clearance event','Fathers day','Mothers 
day','Boho XYZ','Guess ABC','Sale']]
</code></pre>

<p>Output - </p>

<pre><code>model.wv.most_similar('World of Zoro')
[Sale,0.98],[Mothers day,0.97]
</code></pre>

<p>I am also not sure if there is any functionality within the Word2Vec or any similar algorithms which can help predicting campaigns for individual users.</p>

<p>Thank you for your help.</p>
",Vectorization & Embeddings,sequence model word vec working data set record data look like trying understand campaign sequence predict next possible campaign customer assume processed data stored camp word vec problem model accepts token spit text token probability return looking similarity word vec accepts form input give form output since want predict campaign sequence wa wondering anyway give input model get campaign name output input output also sure functionality within word vec similar algorithm help predicting campaign individual user thank help
Non English Word Embedding from English Word Embedding,"<p>How can i generate non-english (french , spanish , italian ) word embedding from english word embedding ?</p>

<p>What are the best ways to generate high quality word embedding for non - english words .</p>

<p>Words may include (samsung-galaxy-s9)</p>
",Vectorization & Embeddings,non english word embedding english word embedding generate non english french spanish italian word embedding english word embedding best way generate high quality word embedding non english word word may include samsung galaxy
Document similarity with doc2vec,"<p>With this Gensim example in github, <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a> it provides examples at the end to find simalarities with phrases or keywords, like 'lady gaga' or 'machine learning'. However am looking to find similarity with actual document in plain text file, could this be done? and how can I do it? suppose text file is located on my local laptop in txt format.</p>
",Vectorization & Embeddings,document similarity doc vec gensim example github provides example end find simalarities phrase keywords like lady gaga machine learning however looking find similarity actual document plain text file could done suppose text file located local laptop txt format
Finding the sum or mean of a 3d matrix with variable length in tensorflow,"<p>I have to do a averaging of 3d Tensor, where first dimension represents batch_size , second dimension reporesents max_length of sentence ( time axis ) in the batch and last dimension represents the embedding dimension. Those who are familiar with lstm, it is obtained by <code>tf.nn.emebedding_lookup</code></p>

<p>For example:</p>

<pre><code>Assume I have 3 sentences
[ [i, love, you,], [i, don't, love, you,], [i, always, love, you, so, much ]]
</code></pre>

<p>Here <code>batch_size = 3</code>, <code>max_length = 6</code> (3rd sentence ) and assume <code>embedding dimension = 100</code>. Normally, we will pad the first 2 sentences to match the max_length. Now, I need to average the word embeddings of each word. But, if I am using tf.reduce_sum, it will consider those padded vectors into consideration for the first two sentences, which is wrong. Is there an efficient way to do this in tensorflow.  </p>
",Vectorization & Embeddings,finding sum mean matrix variable length tensorflow averaging tensor first dimension represents batch size second dimension reporesents max length sentence time axis batch last dimension represents embedding dimension familiar lstm obtained example rd sentence assume normally pad first sentence match max length need average word embeddings word using tf reduce sum consider padded vector consideration first two sentence wrong efficient way tensorflow
Supervised Text Similarity,"<p>I am trying to calculate text similarity between sentences. I have standardized medical services list containing text of service ( for e.g. consultation of neurologist). Every time hospital/clinic comes with their own service list so I need to map hospital's service list with standardized service list. I calculate TF-IDF cosine similarity between hospital's service with standardized service list using skip-gram tokens. I have been doing this for long time so I also have correct mapping of services of some 15 hospitals. By 'correct mapping', I mean medical experts from my organization provided correct mapping of services which are wrongly labelled or mapped using tf-idf cosine similarity algorithm. I want to use 'correct mapping' as text classification problem but no. of labels in this case is more than 10K. Is there a way to perform 'Supervised text similarity'? I tried word2vec algorithm but it does not incorporate supervised element (i.e. target variable (correct mapping of previous results)). Currently I am using R. I am open for Python as well.</p>

<p>See the example of my datasets below ( consider <code>A as 'standardized service list', B as 'hospital's service list', C as 'correct mapping'</code>) .</p>

<pre><code>A &lt;- data.frame(name= c(""Patient had X-ray right leg arteries."",
                         ""Subject was administered Rgraphy left shoulder"",
                         ""Exam consisted of x-ray leg arteries"",
                         ""Patient administered x-ray leg with 20km distance.""),
                row.names = paste0(""A"", 1:4), stringsAsFactors = FALSE)
B &lt;- data.frame(name= c(B = ""Patient had X-ray left leg arteries"",
                         ""Rgraphy right shoulder given to patient"",
                         ""X-ray left shoulder revealed nothing sinister"",
                         ""Rgraphy right leg arteries tested""), 
                row.names = paste0(""A"", 1:4), stringsAsFactors = FALSE)

C &lt;- data.frame(name= c(""Patient had X-ray right leg arteries."",
                         ""Subject was administered Rgraphy left shoulder"",
                         ""Exam consisted of x-ray leg arteries"",
                         ""Patient administered x-ray leg with 20km distance.""),
                mapping = c(""Radiography right leg artery."",
                            ""Radiography left shoulder"",
                            ""Radiography leg arteries"",
                            ""Radiography leg with more than 10km distance.""),
                row.names = paste0(""A"", 1:4), stringsAsFactors = FALSE)
</code></pre>
",Vectorization & Embeddings,supervised text similarity trying calculate text similarity sentence standardized medical service list containing text service e g consultation neurologist every time hospital clinic come service list need map hospital service list standardized service list calculate tf idf cosine similarity hospital service standardized service list using skip gram token long time also correct mapping service hospital correct mapping mean medical expert organization provided correct mapping service wrongly labelled mapped using tf idf cosine similarity algorithm want use correct mapping text classification problem label case k way perform supervised text similarity tried word vec algorithm doe incorporate supervised element e target variable correct mapping previous result currently using r open python well see example datasets consider
Use Glove vectors without Embedding layers in LSTM,"<p>I want to use the Glove vectors in Language Modeling. But the problem is if I use Embedding layer in the model, I can't predict the output vector and match the word. What I mean here is, I want to give the glove vector representation for my sentences as the Input. and get them out from the Lstm layer and get the vectors and match it with Glove vectors I want to use the glove vectors without embedding layer.  can someone propose a method to do it? I am using the keras and python3</p>

<p>What I want is to use the embedding layer as one model1 and return the output vector and give it to another LSTM model2 as the input. where it gives the index of the word vector.</p>

<p><a href=""https://i.sstatic.net/MlLz9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MlLz9.jpg"" alt=""enter image description here""></a></p>
",Vectorization & Embeddings,use glove vector without embedding layer lstm want use glove vector language modeling problem use embedding layer model predict output vector match word mean want give glove vector representation sentence input get lstm layer get vector match glove vector want use glove vector without embedding layer someone propose method using kera python want use embedding layer one model return output vector give another lstm model input give index word vector
MST on Complete graph to cluster them (for cosine similarity),"<p>I need to cluster (let's say given as parameter k), words (that I 
store in array List) according to their cosine similarity. I have stored my all words as vertexes in list in a complete ,weighed, and undirected graph (that uses adjacency list), and put their cosine similarity values on edges. As I understand I need to use MST (Kruskals Algorithm) for clustering process.</p>

<p>However since, my graph is complete graph and MST used for connected graphs, I am kind of confused how to use it on complete graph? Or I am doing wrong by using complete graph?</p>

<p>This is my wordList:</p>

<pre><code> [directors, producers, film, movie, black, white, man, woman, person, man, young, woman, science, fiction, thrilling, realistic, lovely, stunning, criminals, zombies, father, son, girlfriend, boyfriend, nurse, soldier, professor, college] 
</code></pre>

<p>And I need to cluster them by MST so that if k (number of clusters) is 2 it will be like this (2 clusters according to their similarities):</p>

<pre><code>boyfriend,college,father,girlfriend,man,nurse,person,professor,son,woman,young
criminals,directors,fiction,film,lovely,movie,producers,science,stunning,thrilling,zombies
</code></pre>
",Vectorization & Embeddings,mst complete graph cluster cosine similarity need cluster let say given parameter k word store array list according cosine similarity stored word vertex list complete weighed undirected graph us adjacency list put cosine similarity value edge understand need use mst kruskals algorithm clustering process however since graph complete graph mst used connected graph kind confused use complete graph wrong using complete graph wordlist need cluster mst k number cluster like cluster according similarity
What is vector for specific word in CBOW word2vec?,"<p>Classical CBOW word2vec looks like:</p>

<p><a href=""https://i.sstatic.net/29d91.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/29d91.png"" alt=""CBOW word2vec scheme""></a></p>

<p>What is vector for specific word in this scheme? How it obtains from WI and WO matrixes? Or useful word-vectors obtains only from Skip-gram word2vec?</p>
",Vectorization & Embeddings,vector specific word cbow word vec classical cbow word vec look like vector specific word scheme obtains wi wo matrix useful word vector obtains skip gram word vec
Word2Vec: Number of Dimensions,"<p>I am using Word2Vec with a dataset of roughly 11,000,000 tokens looking to do both word similarity (as part of synonym extraction for a downstream task) but I don't have a good sense of how many dimensions I should use with Word2Vec. Does anyone have a good heuristic for the range of dimensions to consider based on the number of tokens/sentences?</p>
",Vectorization & Embeddings,word vec number dimension using word vec dataset roughly token looking word similarity part synonym extraction downstream task good sense many dimension use word vec doe anyone good heuristic range dimension consider based number token sentence
sentence embeddings for alternatives of a word in a sentence,"<p>Looking for Embeddings which can throw the alternatives of a given word in a sentence looking at the context info.</p>
<p>Ex:</p>
<blockquote>
<p>I read a great book at lunch.</p>
</blockquote>
<p>Generated sentences:</p>
<blockquote>
<p>I read a good book at lunch.</p>
<p>I read a good novel at lunch.</p>
<p>I read an excellent book at lunch.</p>
<p>I read an excellent novel at lunch.</p>
</blockquote>
<p>Any help would be greatly appreciated.</p>
",Vectorization & Embeddings,sentence embeddings alternative word sentence looking embeddings throw alternative given word sentence looking context info ex read great book lunch generated sentence read good book lunch read good novel lunch read excellent book lunch read excellent novel lunch help would greatly appreciated
how to use both word2vec and RNN for NLP together?,"<p>I recently studied and understood how <code>word2vec</code> works, it is responsible to convert words into numerical form so when we plot them or put them in the world space they will be spread and reveal the relationship between every word and the other.</p>

<p>my question here, I found also <code>RNNs</code> and suddenly I became confused. Is <code>word2vec</code> an alternative to <code>RNNs</code> or I can use <code>word2vec</code> to transfer the words to numeric form and then use them on <code>RNNs</code> ? </p>

<p>I mean both of them predict the next word, so I want to know if they are the different approaches for the same problem or I can use them both together ?</p>

<p><strong>NOTE:</strong> I finished computer vision and started in NLP so please don't judge my question I am just starting, thanks in advance.</p>
",Vectorization & Embeddings,use word vec rnn nlp together recently studied understood work responsible convert word numerical form plot put world space spread reveal relationship every word question found also suddenly became confused alternative use transfer word numeric form use mean predict next word want know different approach problem use together note finished computer vision started nlp please judge question starting thanks advance
How to use own word embedding with pre-trained embedding like word2vec in Keras,"<p>I have a co-occurrence matrix stored in a CSV file which contains the relationship between words and emojis like this:</p>

<pre><code>word emo1 emo2 emo3
w1   0.5   0.3  0.2
w2   0.8   0    0
w3   0.2   0.5  0.2
</code></pre>

<p>This co-occurrence matrix is huge which has <code>1584755</code> rows and <code>621</code> columns. I have a <code>Sequential() LSTM</code> model in <code>Keras</code> where I use pre-trained (word2vec) word-embedding. Now I would like to use the co-occurrence matrix as another embedding layer. How can I do that? My current code is something like this:</p>

<pre><code>model = Sequential()
model.add(Embedding(max_features, embeddings_dim, input_length=max_sent_len, weights=[embedding_weights]))
model.add(Dropout(0.25))
model.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length, border_mode='valid', activation='relu', subsample_length=1))
model.add(MaxPooling1D(pool_length=pool_length))
model.add(LSTM(embeddings_dim))
model.add(Dense(reg_dimensions))
model.add(Activation('sigmoid'))
model.compile(loss='mean_absolute_error', optimizer='adam')
model.fit( train_sequences , train_labels , nb_epoch=30, batch_size=16) 
</code></pre>

<p>Also, if the co-occurrence matrix is sparse then what would be the best way to use it in the embedding layer? </p>
",Vectorization & Embeddings,use word embedding pre trained embedding like word vec kera co occurrence matrix stored csv file contains relationship word emojis like co occurrence matrix huge ha row column model use pre trained word vec word embedding would like use co occurrence matrix another embedding layer current code something like also co occurrence matrix sparse would best way use embedding layer
How to reduce semantically similar words?,"<p>I have a large corpus of words extracted from the documents. In the corpus are words which might mean the same.
For eg: ""command"" and ""order"" means the same, ""apple"" and ""apply"" which does not mean the same.</p>

<p>I would like to merge the similar words, say ""command"" and ""order"" to ""command"".
I have tried to use word2vec but it doesn't check for semantic similarity of words(it ouputs good similarity for apple and apply since four characters in the words are the same). And when I try using wup similarity, it gives good similarity score if the words have matching synonyms whose results are not that impressive.</p>

<p>What could be the best approach to reduce semantically similar words to get rid of redundant data and merge similar data?</p>
",Vectorization & Embeddings,reduce semantically similar word large corpus word extracted document corpus word might mean eg command order mean apple apply doe mean would like merge similar word say command order command tried use word vec check semantic similarity word ouputs good similarity apple apply since four character word try using wup similarity give good similarity score word matching synonym whose result impressive could best approach reduce semantically similar word get rid redundant data merge similar data
Retraining of word2vec on the pretrained Google news vector,"<p>I am running an experiment and I want to know if it is possible to retrain the word2vec Google news vectors using only hyperparameter that i want. And also assign the values that I want to these hyperparameters.
Thanks</p>
",Vectorization & Embeddings,retraining word vec pretrained google news vector running experiment want know possible retrain word vec google news vector using hyperparameter want also assign value want hyperparameters thanks
Is skip-gram model in word2vec an expanded version of N-Gram model? skip-gram vs. skip-grams?,"<p>The skip-gram model of word2vec uses a shallow neural network to learn the word embedding with (input-word, context-word) data. When I read the tutorials for the skip-gram model there was not any mentioning regarding the N-gram. However I came across several online discussions in which people claim --- skip-gram model in word2vec is an expanded version of <strong>N-Gram</strong> model. Also I don't really understand this ""<strong>k-skip-n-gram</strong>"" in the following Wikipedia page.</p>

<p>Wikipedia cited a paper from 1992 for ""<strong>skip-grams</strong>"", so I guess this is not the word2vec's skip-gram model, right? Another paper regarding this ""skip-grams"" is <a href=""https://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf"" rel=""nofollow noreferrer"">https://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf</a>. This is very confusing. Why there's no one clear this up.</p>

<p>The wikipedia source and the online discussion are as follows:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/N-gram#Skip-gram"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/N-gram#Skip-gram</a></li>
<li><p><a href=""https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures"" rel=""nofollow noreferrer"">https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures</a></p></li>
<li><p><a href=""https://www.quora.com/What-is-a-skip-gram-model-Why-is-it-better-than-other-language-models-And-how-does-it-work"" rel=""nofollow noreferrer"">https://www.quora.com/What-is-a-skip-gram-model-Why-is-it-better-than-other-language-models-And-how-does-it-work</a></p></li>
</ul>
",Vectorization & Embeddings,skip gram model word vec expanded version n gram model skip gram v skip gram skip gram model word vec us shallow neural network learn word embedding input word context word data read tutorial skip gram model wa mentioning regarding n gram however came across several online discussion people claim skip gram model word vec expanded version n gram model also really understand k skip n gram following wikipedia page wikipedia cited paper skip gram guess word vec skip gram model right another paper regarding skip gram confusing one clear wikipedia source online discussion follows
how &#39;negative sampling&#39; improve word representation quality in word2vec?,"<p>negative sampling in '<strong>word2vec</strong>' improves the training speed, that's obviously!  </p>

<p>but why '<strong>makes the word representations significantly more accurate.</strong>'?  </p>

<p>I didn't find the relevant discussion or details. can u help me?</p>
",Vectorization & Embeddings,negative sampling improve word representation quality word vec negative sampling word vec improves training speed obviously make word representation significantly accurate find relevant discussion detail u help
What features are good for Sentence Classification apart from using vector representation like Bag-of-words?,"<p>I am trying to find whether a given sentence is a ""question request"", ""call for action"", etc. I am using supervised multilabel classification for that.</p>

<p>What will be a good set of features to use? I am currently using Bag-of-words with trigrams, modal verbs, question words, etc. but the result is not that good.</p>

<p>Input example: ""Can you get this today? I need following items.""  </p>
",Vectorization & Embeddings,feature good sentence classification apart using vector representation like bag word trying find whether given sentence question request call action etc using supervised multilabel classification good set feature use currently using bag word trigram modal verb question word etc result good input example get today need following item
gensim doc2vec - How to infer label,"<p>I am using gensim's doc2vec implementation and I have a few thousand documents tagged with four labels.</p>

<pre><code>yield TaggedDocument(text_tokens, [labels])
</code></pre>

<p>I'm training a Doc2Vec model with a list of these <strong>TaggedDocument</strong>s. However, I'm not sure how to infer the tag for a document that was not seen during training. I see that there is a infer_vector method which returns the embedding vector. But how can I get the most likely label from that?</p>

<p>An idea would be to infer the vectors for every label that I have and then calculate the cosine similarity between these vectors and the vector for the new document I want to classify. Is this the way to go? If so, how can I get the vectors for each of my four labels?</p>
",Vectorization & Embeddings,gensim doc vec infer label using gensim doc vec implementation thousand document tagged four label training doc vec model list taggeddocuments however sure infer tag document wa seen training see infer vector method return embedding vector get likely label idea would infer vector every label calculate cosine similarity vector vector new document want classify way go get vector four label
Word Embedding to word,"<p>I am using a GloVe based pre-trained embedded vectors for words in my I/P sentences to a NMT-like model. The model then generates a series of word embeddings as its output for each sentence.</p>

<p>How can I convert these output word embeddings to respective words? One way I tried is using cosine similarity between each output embedding vector and all the i/p embedding vectors. Is there a better way than this?</p>

<p>Also, is there a better way to approach this than using embedding vectors?</p>
",Vectorization & Embeddings,word embedding word using glove based pre trained embedded vector word p sentence nmt like model model generates series word embeddings output sentence convert output word embeddings respective word one way tried using cosine similarity output embedding vector p embedding vector better way also better way approach using embedding vector
what is best solution for setting &#39;unknown&#39; or &#39;unk&#39; word vector in word2vec?,"<p>I usually set unk'value as random distribution vector or 0-vector.<br>
It performed not bad,but most situation it's also not best for many task,i think.<br>
But i'm curious about the best method to process 'unk' word vector,thank for any helpful advice.  </p>
",Vectorization & Embeddings,best solution setting unknown unk word vector word vec usually set unk value random distribution vector vector performed bad situation also best many task think curious best method process unk word vector thank helpful advice
Input to Bidirectional LSTM in tensorflow,"<p>Normally all inputs fed to BiLSTM are of shape <code>[batch_size, time_steps, input_size]</code>.</p>

<p>However, I'm working on a problem of Automatic Grading of an Essay in which there's an extra dimension called number of sentences in each essay. So in my case, a typical batch after embedding using <code>word2vec</code>, is of shape <code>[2,16,25,300]</code>.</p>

<p>Here, there are 2 essays in each batch <code>(batch_size=2)</code>, each essay has 16 sentences, each sentence is 25 words long<code>(time_step=25)</code> and I'm using word2vec of 300 dimensions <code>(input_size=300)</code>.</p>

<p>So clearly I need to loop this batch over dimension 16 somehow such that the shape of input becomes <code>[2,25,300]</code> in each iteration. I have tried for a long time but I haven't been able to find a way to do it. For example, if you make a loop over <code>tf.nn.bidirectional_dynamic_rnn()</code>, it'll give error in second iteration saying that <code>tf.nn.bidirectional_dynamic_rnn()</code> kernel already exists. I can't directly make a <code>for</code> loop over <code>sentence_dimension</code> because those are tensors of shape <code>[None,None,None,300]</code> and I gave values just for the sake of simplicity. If there any other way to do it? Thanks. Please note that I am not using <code>Keras</code> or any other framework.</p>

<p>Here's a sample encoding layer for reference.</p>

<pre><code>def bidirectional_lstm(input_data):
    cell = tf.nn.rnn_cell.LSTMCell(num_units=200, state_is_tuple=True)
    outputs, states  = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell,
                                                       cell_bw=cell,
                                                       dtype=tf.float32,
                                                       inputs=input_data)
    return tf.concat(outputs,2)
</code></pre>

<p><code>embedded</code> is of shape <code>[2,16,25,300]</code>.</p>

<p>And here's a sample input</p>

<pre><code>for batch_i, texts_batch in enumerate(get_batches(X_train, batch_size)): ## get_batches() is yielding batches one by one
  ## X_train is of shape [2,16,25] ([batch_size,sentence_length,num_words])

  ## word2vec
  embeddings = tf.nn.embedding_lookup(word_embedding_matrix, texts_batch)
  ## embeddings shape is now [2,16,25,300] ([batch_size,sentence_length,num_words,word2vec_dim])

  ## Now I need some kind of loop here to loop it over sentence dimension. Can't use for loop since these are all placeholders with dimensions None
  ## ??? output = bidirectional_lstm(embeddings,3,200,0.7) ??? This would be correct if there was no sentence dimension.
</code></pre>
",Vectorization & Embeddings,input bidirectional lstm tensorflow normally input fed bilstm shape however working problem automatic essay extra dimension called number sentence essay case typical batch embedding using shape essay batch essay ha sentence sentence word long using word vec dimension clearly need loop batch dimension somehow shape input becomes iteration tried long time able find way example make loop give error second iteration saying kernel already exists directly make loop tensor shape gave value sake simplicity way thanks please note using framework sample encoding layer reference shape sample input
Retrieving vocabulary from HashingVectorizer,"<p>I was using <code>tfidfVectorizer</code> to use tfidf values as weights to convert my word vectors to sentence vectors. Since I ran into memory error , I decided to use <code>HashingVectorizer</code> instead. Is there a way to get the vector for a given word in this set-up like with <code>tfidf_vectorizer.vocabulary_[word]</code> ? </p>
",Vectorization & Embeddings,retrieving vocabulary hashingvectorizer wa using use tfidf value weight convert word vector sentence vector since ran memory error decided use instead way get vector given word set like
"What is the parameter &quot;size&quot; means in gensim.model.Word2Vec(sentence, size)?","<p>I have just been started learning about word embeddings and gensim and I tried this <a href=""https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"" rel=""nofollow noreferrer"">code</a> 
. In this article during the visualisation it says we need PCA to convert high-dimensional vectors into low-dimensions. Now we have a parameter ""size"" in Word2Vec method, so why can't we set that size equals to 2 rather using PCA. 
So, I tried to do this and compare both graphs (one with 100 size and other with 2 as size) and got very different result. Now I am confused that what this ""size"" depicts? How the size of vectors affect this?</p>

<p><a href=""https://i.sstatic.net/B2AkA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/B2AkA.png"" alt=""enter image description here""></a></p>

<p>This is what I got when I used 100 as size.</p>

<p><a href=""https://i.sstatic.net/Wf4vM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Wf4vM.png"" alt=""enter image description here""></a></p>

<p>This is what I got when I used 2 as size.</p>
",Vectorization & Embeddings,parameter size mean gensim model word vec sentence size started learning word embeddings gensim tried code article visualisation say need pca convert high dimensional vector low dimension parameter size word vec method set size equal rather using pca tried compare graph one size size got different result confused size depicts size vector affect got used size got used size
add more vocabulary to pretrained word2vec model,"<p>help me to correct this or are there any other way to accomplish this task?</p>

<p><img src=""https://i.sstatic.net/OVvoK.png"" alt=""Snapshot of code snippet""></p>
",Vectorization & Embeddings,add vocabulary pretrained word vec model help correct way accomplish task
NLP: is there any model that generates sentence embedding with self-defined length?,"<p>I found universal sentence encoder <a href=""https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder/1"" rel=""nofollow noreferrer"">https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder/1</a></p>

<p>but seems like it can only output sentence vector with 512 dim. I'm new to NLP and like to know whether there exists a model that generates sentence representation with user-defined length.</p>

<p>thanks!</p>
",Vectorization & Embeddings,nlp model generates sentence embedding self defined length found universal sentence encoder seems like output sentence vector dim new nlp like know whether exists model generates sentence representation user defined length thanks
Semantic similarity among documents to do clustering in Python,"<p>I have around 1000 documents(text like paragraphs). I want to find similarities among the documents to cluster the documents. Finally, I want to do hierarchical clustering. I want to implement in Python. How to proceed for this.</p>
",Vectorization & Embeddings,semantic similarity among document clustering python around document text like paragraph want find similarity among document cluster document finally want hierarchical clustering want implement python proceed
Word2vec - get rank of similarity,"<p>Given I got a word2vec model (by gensim), I want to get the rank similarity between to words.
For example, let's say I have the word ""desk"" and the most similar words to ""desk"" are:</p>

<blockquote>
  <ol>
  <li>table 0.64</li>
  <li>chair 0.61</li>
  <li>book 0.59</li>
  <li>pencil 0.52</li>
  </ol>
</blockquote>

<p>I want to create a function such that:</p>

<blockquote>
  <p>f(desk,book) = 3
  Since book is the 3rd most similar word to desk.
  Does it exists? what is the most efficient way to do this?</p>
</blockquote>
",Vectorization & Embeddings,word vec get rank similarity given got word vec model gensim want get rank similarity word example let say word desk similar word desk table chair book pencil want create function f desk book since book rd similar word desk doe exists efficient way
Can I interpret doc2vec components?,"<p>I am solving a binary text classification problem with corporate filings. Using Doc2Vec embeddings of length 100 with LightGBM is producing great results. However, for this project it would be very valuable to approximate a thematic meaning for at least one of the components. Ideally, this would be a feature ranked with high importance by LightGBM explained anecdotally with a few examples.</p>

<p>Has anyone attempted this, or should interpretation be off the table for a high-dimensional model with this level of complexity?</p>
",Vectorization & Embeddings,interpret doc vec component solving binary text classification problem corporate filing using doc vec embeddings length lightgbm producing great result however project would valuable approximate thematic meaning least one component ideally would feature ranked high importance lightgbm explained anecdotally example ha anyone attempted interpretation table high dimensional model level complexity
What is the semantic relationship expected between word vectors which are scalar multiples of each other in word2vec?,"<p>Let's say you have a word vector for the word <code>queen</code>. Some of its scalar multiples would be <code>x = queen + queen</code> , <code>y = queen + queen + queen</code> and <code>n * queen</code> for any real value of n ( so we're also considering non-integer values of n such as in <code>0.83 * queen</code> ).</p>

<p>Consider x to be the word most similar to the vector queen + queen according to the cosine similarity between a simple mean of the projection weight vectors of the most similar word and the vector queen + queen.</p>

<p>Consider y to be the word most similar to the vector queen + queen + queen by the same method.</p>

<p>Then what is the semantic relationship expected between the words <code>x</code>, <code>y</code> and <code>queen</code>? I know these vectors will all have the same ratio between the dimensional values within the vector, but I'm having a hard time figuring out how to read this in terms of word meaning. </p>

<p>My intution says that I'll get something in another context that has a position on that context similar to queen. For instance, a queen's ""wealth"" may be significantly larger than a queen's ""beauty"". So I'll get another word in another context that has the same wealth/beauty balance as ""queen"".</p>

<p>So let's say I'm moving from Royal titles ( queen, king, princess... ) to the Forbes list ( Jeff Bezos, Bill Gates, Warren Buffet... ) when I multiply queen by n.</p>

<p>queen * n = someone on the Forbes list that has the same wealth/beauty balance as a queen ( very wealthy, but not very pretty )</p>

<p>princess * n = someone on the Forbes list that has the same wealth/beauty balance as a princess ( moderately wealthy, but very pretty )</p>

<p>However this is just a wild theory, I have no clue about how to systematically prove this is real. </p>
",Vectorization & Embeddings,semantic relationship expected word vector scalar multiple word vec let say word vector word scalar multiple would real value n also considering non integer value n consider x word similar vector queen queen according cosine similarity simple mean projection weight vector similar word vector queen queen consider word similar vector queen queen queen method semantic relationship expected word know vector ratio dimensional value within vector hard time figuring read term word meaning intution say get something another context ha position context similar queen instance queen wealth may significantly larger queen beauty get another word another context ha wealth beauty balance queen let say moving royal title queen king princess forbes list bezos bill gate warren buffet multiply queen n queen n someone forbes list ha wealth beauty balance queen wealthy pretty princess n someone forbes list ha wealth beauty balance princess moderately wealthy pretty however wild theory clue prove real
Adding additional words in word2vec or Glove (maybe using gensim),"<p>I have two pretrained word embeddings: <code>Glove.840b.300.txt</code> and <code>custom_glove.300.txt</code></p>

<p>One is pretrained by Stanford and the other is trained by me.
Both have different sets of vocabulary. To reduce oov, I'd like to add words that don't appear in file1 but do appear in file2 to file1.
How do I do that easily?</p>

<p>This is how I load and save the files in gensim 3.4.0.</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors

model = KeyedVectors.load_word2vec_format('path/to/thefile')
model.save_word2vec_format('path/to/GoogleNews-vectors-negative300.txt', binary=False)
</code></pre>
",Vectorization & Embeddings,adding additional word word vec glove maybe using gensim two pretrained word embeddings one pretrained stanford trained different set vocabulary reduce oov like add word appear file appear file file easily load save file gensim
Saved tensorflow NLP model outputs nothing after restoring saved variables for training,"<p>I built a seq2seq model for a chatbot after getting inspired by a github repo. To train the chatbot I used my facebook chat history. Since most of my chat is like hindi words written in english language. I had to train word embedding from scratch. I knew that the model will take about 30-40 hours(500000 iterations of batch size 24) of training on cpu. So, I learned to use </p>

<blockquote>
  <p>tf.train.saver()</p>
</blockquote>

<p>method to save the variables and restore them in future.</p>

<p>To see the progress of my model I made model to output the replies of five input text sequences at every 250th iteration.
In the beginning of training I was getting blank output(since  token is most common). But after few thousand iterations it started to give most common words as output. After 90,000 iterations it was giving some illogical but different kind of outputs.So, I stopped training there.</p>

<p>Now when I restore variables from latest checkpoint. I am again getting blank lines as output. Is this normal behavior or is there some kind of mistake in my code. </p>

<p>CODE:
<a href=""https://codeshare.io/5vNrwD"" rel=""nofollow noreferrer"">Full code</a></p>

<p>code snippets:
(code to restore from latest checkpoint)</p>

<pre><code>sess = tf.Session()
saver = tf.train.Saver()
saver.restore(sess, tf.train.latest_checkpoint('models/'))
sess.run(tf.global_variables_initializer())
</code></pre>

<p>(code to save variables in iteration loop)</p>

<pre><code>    if (i % 10000 == 0 and i != 0):
        savePath = saver.save(sess, ""models/pretrained_seq2seq.ckpt"", global_step=i)
</code></pre>
",Vectorization & Embeddings,saved tensorflow nlp model output nothing restoring saved variable training built seq seq model chatbot getting inspired github repo train chatbot used facebook chat history since chat like hindi word written english language train word embedding scratch knew model take hour iteration batch size training cpu learned use tf train saver method save variable restore future see progress model made model output reply five input text sequence every th iteration beginning training wa getting blank output since token common thousand iteration started give common word output iteration wa giving illogical different kind output stopped training restore variable latest checkpoint getting blank line output normal behavior kind mistake code code full code code snippet code restore latest checkpoint code save variable iteration loop
what does the vector of a word in word2vec represents?,"<p><a href=""https://code.google.com/p/word2vec/"" rel=""noreferrer"">word2vec</a> is a open source tool by Google: </p>

<ul>
<li><p>For each word it provides a vector of float values, what exactly do they represent?</p></li>
<li><p>There is also a paper on <a href=""http://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""noreferrer"">paragraph vector</a> can anyone explain how they are using word2vec in order to obtain fixed length vector for a paragraph.</p></li>
</ul>
",Vectorization & Embeddings,doe vector word word vec represents word vec open source tool google word provides vector float value exactly represent also paper paragraph vector anyone explain using word vec order obtain fixed length vector paragraph
Embedding in Keras,"<p>Which algorithm is used for embedding in Keras built-in function?
Word2vec? Glove? Other?</p>

<p><a href=""https://keras.io/layers/embeddings/"" rel=""nofollow noreferrer"">https://keras.io/layers/embeddings/</a></p>
",Vectorization & Embeddings,embedding kera algorithm used embedding kera built function word vec glove
How can a sentence or a document be converted to a vector?,"<p>We have models for converting words to vectors (for example the word2vec model). Do similar models exist which convert sentences/documents into vectors, using perhaps the vectors learnt for the individual words?</p>
",Vectorization & Embeddings,sentence document converted vector model converting word vector example word vec model similar model exist convert sentence document vector using perhaps vector learnt individual word
Problems while TFIDF vectorizing tokenized documents?,"<p>I am vectorizing a text blob with tokens that have the following style:</p>

<pre><code>hi__(how are you), 908__(number code), the__(POS)
</code></pre>

<p>As you can see the tokens have attached some information with <code>__(info)</code>, I am extracting key words using tfidf, as follows:</p>

<pre><code>vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(doc)
indices = np.argsort(vectorizer.idf_)[::-1]
features = vectorizer.get_feature_names()
</code></pre>

<p>The problem is that when I do the above procedure for extracting keywords, I am suspecting that the vectorizer object is removing the parenthesis from my textblob. Thus, which parameter from the tfidf vectorizer object can I use in order to preserve such information in the parenthesis?</p>

<p><strong>UPDATE</strong></p>

<p>I also tried to:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

def dummy_fun(doc):
    return doc

tfidf = TfidfVectorizer(
    analyzer='word',
    tokenizer=dummy_fun,
    preprocessor=dummy_fun,
    token_pattern=None)  
</code></pre>

<p>and</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

def dummy_fun(doc):
    return doc

tfidf = TfidfVectorizer(
    tokenizer=dummy_fun,
    preprocessor=dummy_fun,
    token_pattern=None) 
</code></pre>

<p>However, this returns me a sequence of characters instead of tokens that I already tokenize:</p>

<pre><code>['e', 's', '_', 'a', 't', 'o', 'c', 'r', 'i', 'n']
</code></pre>
",Vectorization & Embeddings,problem tfidf vectorizing tokenized document vectorizing text blob token following style see token attached information extracting key word using tfidf follows problem procedure extracting keywords suspecting vectorizer object removing parenthesis textblob thus parameter tfidf vectorizer object use order preserve information parenthesis update also tried however return sequence character instead token already tokenize
"What should be the word vectors of token &lt;pad&gt;, &lt;unknown&gt;, &lt;go&gt;, &lt;EOS&gt; before sent into RNN?","<p>In word embedding, what should be a good vector representation for the start_tokens _PAD, _UNKNOWN, _GO, _EOS? </p>
",Vectorization & Embeddings,word vector token pad unknown go eos sent rnn word embedding good vector representation start token pad unknown go eos
Get word-embedding dictionary with glove-python model,"<p>I trained a Glove model in python using Maciejkula's implementation (<a href=""http://github.com/maciejkula/glove-python"" rel=""nofollow noreferrer"">github repo</a>).
For the next step I need a word-to-embedding dictionary.
However I can't seem to find an easy way to extract such a dictionary from the glove model I trained.</p>

<p>I can extract the embeddings by accessing <code>model.word_vectors</code> but this only returns an array containing the vectors without a mapping to the corresponding words.
There is also the <code>model.dictionary</code> attribute containing word-to-index pairs.
I thought that these indexes might correspond to the embedding-indexes in the <code>model.word_vectors</code> array, but I'm not sure that this is correct.</p>

<p>Do the indexes correspond or is there another easy way to get a word-to-embedding dictionary from a glove-python model? </p>

<p>I realize that <a href=""https://stackoverflow.com/questions/47974626/getting-word-embeddings-for-your-dataset-using-training-data-in-glove"">Sanj</a> asked I similar although wider question, but since there is no response yet I thought I'd ask this more specific question.</p>
",Vectorization & Embeddings,get word embedding dictionary glove python model trained glove model python using maciejkula implementation github repo next step need word embedding dictionary however seem find easy way extract dictionary glove model trained extract embeddings accessing return array containing vector without mapping corresponding word also attribute containing word index pair thought index might correspond embedding index array sure correct index correspond another easy way get word embedding dictionary glove python model realize href asked similar although wider question since response yet thought ask specific question p
Word alignment task vs dictionary induction,"<p>I've been reading up on multi-lingual word embedding methods, and couldn't quite grasp the difference between two of the evaluation methods used - word alignment and dictionary induction.<br>
My curiosity was heightened by looking at Table 1 from <a href=""https://levyomer.files.wordpress.com/2017/01/a-strong-baseline-for-learning-cross-lingual-word-embeddings-eacl-20171.pdf"" rel=""nofollow noreferrer"">this paper</a>, where the Bilingual Autoencoders methods overperforms Inverted index for the Word Alignment task, but it's the other way around for Dictionary Induction.<br>
Thanks for the help!  </p>
",Vectorization & Embeddings,word alignment task v dictionary induction reading multi lingual word embedding method quite grasp difference two evaluation method used word alignment dictionary induction curiosity wa heightened looking table paper bilingual autoencoders method overperforms inverted index word alignment task way around dictionary induction thanks help
Using Word Embeddings to find similarity between documents with certain words having more weight,"<p>Using Word embeddings ,I am calculating the similarity distance between 2 paragraphs where distance between 2 paragraphs is the sum of euclidean distances between vectors of 2 words ,1 from each paragraph.
The more the value of this sum, the less similar 2 documents are-</p>

<p>How can I assign prefernce/weights to certain words while calculating this similarity distance.</p>
",Vectorization & Embeddings,using word embeddings find similarity document certain word weight using word embeddings calculating similarity distance paragraph distance paragraph sum euclidean distance vector word paragraph value sum le similar document assign prefernce weight certain word calculating similarity distance
How do I get words from an embedded vector?,"<p>How can I convert them into their original words when I generate word vectors in the generator?
I used the nn.Embedding module built into pytorch to embed words.</p>
",Vectorization & Embeddings,get word embedded vector convert original word generate word vector generator used nn embedding module built pytorch embed word
Word embedding decreasing classification precision,"<p><strong>I am currently trying to classify text into 7 classes.</strong> Up to now, I have been able to reach a 90% precision score using Majority Voting (with SVM, Multinomial NB, Random Forest and KNN).</p>

<p><strong>I wanted to try to increase a little more this precision by using word embeddings</strong> and thus getting less dimensions for my samples. I use gensim word2vec to create my model and the NLTK list of stop-words and tokenizer:</p>

<pre><code>with open('data.pkl','r') as f:
    corpus=pickle.load(f)

with open('targets.pkl','r') as f:
    targets=pickle.load(f)

tokenized_corpus=[word_tokenize(anomaly) for anomaly in corpus]
stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', ""you're"", ""you've"", ""you'll"", ""you'd"", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', ""she's"", 'her', 'hers', 'herself', 'it', ""it's"", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', ""that'll"", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', ""don't"", 'should', ""should've"", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', ""aren't"", 'couldn', ""couldn't"", 'didn', ""didn't"", 'doesn', ""doesn't"", 'hadn', ""hadn't"", 'hasn', ""hasn't"", 'haven', ""haven't"", 'isn', ""isn't"", 'ma', 'mightn', ""mightn't"", 'mustn', ""mustn't"", 'needn', ""needn't"", 'shan', ""shan't"", 'shouldn', ""shouldn't"", 'wasn', ""wasn't"", 'weren', ""weren't"", 'won', ""won't"", 'wouldn', ""wouldn't""]
for i,anomaly in enumerate(tokenized_corpus):
    for w in anomaly:
        if w in stop_words:
            tokenized_corpus[i].remove(w)

model = gensim.models.Word2Vec(
        tokenized_corpus,
        window=5,
        size=100)

model.train(tokenized_corpus, total_examples=len(corpus), epochs=10)
</code></pre>

<p>The model seems fine, I get satisfying results when I use similarity between words.</p>

<p>I use this class to get a mean representation of my samples:</p>

<pre><code>class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        if len(word2vec)&gt;0:
            self.dim=len(word2vec[next(iter(word2vec))])
        else:
            self.dim=0

    def fit(self, X, y):
        return self 

    def transform(self, X):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec] 
                    or [np.zeros(self.dim)], axis=0) #moyenne des vecteurs des mots (a.word2vec[w])[ou 0 si il connait pas le mot] composant un élément de X
            for words in X
        ])
</code></pre>

<p>and </p>

<pre><code>w2v = dict(zip(model.wv.index2word, model.wv.syn0))
a=MeanEmbeddingVectorizer(w2v)    
X_transformed=a.transform(tokenized_corpus)
</code></pre>

<p>Finally, I build a common sklearn pipeline and let sklearn perform a GridSearchCV on my data:</p>

<pre><code>test_param_n_estimators=[i for i in range(1,50)]
parameters = {'clf2__n_estimators': test_param_n_estimators}

pip=Pipeline([['clf2',RandomForestClassifier()]])

gs_clf2 = GridSearchCV(pip, parameters,verbose=10,n_jobs=2)
gs_clf2 = gs_clf2.fit(X_transformed, targets)

print(gs_clf2.best_score_)
print(gs_clf2.best_params_)
</code></pre>

<p><strong>The issue is that I get random precision scores (around 0.5).</strong></p>

<p>Maybe dimensionality reduction is not always able to increase precision but I think I don't understand something or I did something wrong, <strong>do you have an idea of what is going wrong ?</strong></p>

<p>Thank you in advance</p>
",Vectorization & Embeddings,word embedding decreasing classification precision currently trying classify text class able reach precision score using majority voting svm multinomial nb random forest knn wanted try increase little precision using word embeddings thus getting le dimension sample use gensim word vec create model nltk list stop word tokenizer model seems fine get satisfying result use similarity word use class get mean representation sample finally build common sklearn pipeline let sklearn perform gridsearchcv data issue get random precision score around maybe dimensionality reduction always able increase precision think understand something something wrong idea going wrong thank advance
BRNN implementation issue,"<p>I am trying to implement bidirectional RNN from scratch and have the issue with it.
Let's assume we have implemented RNN cell with given number of hidden units, in this case the forward pass for BRNN will be the next (pseudocode):</p>

<pre><code>def brnn_forward(input):
    hiddden_state = RNN.forward(input)
    reversed_input = reverse(input)
    hiddden_state_reversed = RNN.forward(reversed_input)
    output = concatenate(hiddden_state, hiddden_state_reversed)
    return output
</code></pre>

<p>But then I don't know how to implement backward pass. I get derivative error dA (shape = (hidden_units, batch_size, times)) from the next layer with the shape of forward pass output (assuming of course we didn't have concatenation of outputs which doubled amount of hidden units after forward pass). 
However the standard RNN cell backward function takes dA with the shape of forward input, so I've tried:</p>

<pre><code>def brnn_backward(dA):
    h = number_of_hidden_units
    d_hiddden_state = RNN.backward(dA[:h,:,:])
    d_hiddden_state_reversed = RNN.backward(dA[h:,:,:])
    dA_for_previous_layer = d_hiddden_state+d_hiddden_state_reversed
    return dA_for_previous_layer
</code></pre>

<p>But it didnt work and gave me results worse then with unidirectional RNN. 
Also I am not sure how to find derivative error for previous layer (in case we have embedding layer before for instance). Could someone help with backward pass?</p>
",Vectorization & Embeddings,brnn implementation issue trying implement bidirectional rnn scratch issue let assume implemented rnn cell given number hidden unit case forward pas brnn next pseudocode know implement backward pas get derivative error da shape hidden unit batch size time next layer shape forward pas output assuming course concatenation output doubled amount hidden unit forward pas however standard rnn cell backward function take da shape forward input tried didnt work gave result worse unidirectional rnn also sure find derivative error previous layer case embedding layer instance could someone help backward pas
Using Word2Vec for polysemy solving problems,"<p>I have some questions about Word2Vec:</p>

<ol>
<li><p>What determines the dimension of the result model vectors?</p></li>
<li><p>What is elements of this vectors?</p></li>
<li><p>Can I use Word2Vec for polysemy solving problems (state = administrative unit vs state = condition), if I already have texts for every meaning of words?</p></li>
</ol>
",Vectorization & Embeddings,using word vec polysemy solving problem question word vec determines dimension result model vector element vector use word vec polysemy solving problem state administrative unit v state condition already text every meaning word
Why are word embedding actually vectors?,"<p>I am sorry for my naivety, but I don't understand why word embeddings that are the result of NN training process (word2vec) are actually vectors.</p>

<p>Embedding is the process of dimension reduction, during the training process NN reduces the 1/0 arrays of words into smaller size arrays, the process does nothing that applies vector arithmetic.</p>

<p>So as result we got just arrays and not the vectors. Why should I think of these arrays as vectors?</p>

<p>Even though, we got vectors, why does everyone depict them as vectors coming from the origin (0,0)?</p>

<p>Again, I am sorry if my question looks stupid.</p>
",Vectorization & Embeddings,word embedding actually vector sorry naivety understand word embeddings result nn training process word vec actually vector embedding process dimension reduction training process nn reduces array word smaller size array process doe nothing applies vector arithmetic result got array vector think array vector even though got vector doe everyone depict vector coming origin sorry question look stupid
How to interpret CBOW word embeddings?,"<p>In context of word2vec, it is said that ""words occurring in similar contexts have similar word embeddings""; for example, ""love"" and ""hate"" may have similar embeddings because they appear in contextual words such as ""I"" and ""movie"", just for an example. </p>

<p>I get the intuition with skip-gram: both embeddings of ""love"" and ""hate"" should predict the context words ""I"" and ""movie"", thus the embeddings should be similar. However, I can't get it with CBOW: it says that the average embeddings of ""I"" and ""movie"" should predict ""love"" and ""hate""; does that necessarily lead to that the embeddings of ""love"" and ""hate"" should be similar? Or do we interpret word embeddings of SG and CBOW in different ways?</p>
",Vectorization & Embeddings,interpret cbow word embeddings context word vec said word occurring similar context similar word embeddings example love hate may similar embeddings appear contextual word movie example get intuition skip gram embeddings love hate predict context word movie thus embeddings similar however get cbow say average embeddings movie predict love hate doe necessarily lead embeddings love hate similar interpret word embeddings sg cbow different way
What&#39;s the maximum euclidean distance between 2 hyperpoints in word2vec algorithm?,"<p>I have been considering to use Word2vec for a problem.
I know that you can use cosine distance which means the minimum distance can be 0 if the hyperpoints are identical or 1 because cosine spans from [-1,1] in case of maximum.
The same applies for minimum in euclidean distance.
My question is in practice what is the maximum euclidean distance two said words can achieve while using word2vec to project them in the same hyperspace ? Can it be estimated mathematically ? is it theoretically infinite ?</p>
",Vectorization & Embeddings,maximum euclidean distance hyperpoints word vec algorithm considering use word vec problem know use cosine distance mean minimum distance hyperpoints identical cosine span case maximum applies minimum euclidean distance question practice maximum euclidean distance two said word achieve using word vec project hyperspace estimated mathematically theoretically infinite
How to classify one specific word in sentence?,"<p>How to classify one specific word in the sentence?</p>

<p>For example, I have:</p>

<blockquote>
  <p>""Thus, SENP2 may be a promising therapeutic target for BC""</p>
</blockquote>

<p>and </p>

<blockquote>
  <p>""The JNK Signaling Pathway in Renal Fibrosis""</p>
</blockquote>

<p>In the first sentence ""SENP2"" is a gene, and in second sentence ""JNK"" isn't a gene but is on the list of genes.</p>

<p>I want to train a neural network (I think CNN + RNN) which will only classify is one word per sentence. At the input, I have Embedding layer with pre-trained word2vec from PubMed. I also have the positions of these words in sentences in the CSV file, as another column for a particular sentence. Can I train my network normally, that is, whole sentences and label them as 0 and 1? How to predict in new sentences using pre-trained model?</p>
",Vectorization & Embeddings,classify one specific word sentence classify one specific word sentence example thus senp may promising therapeutic target bc jnk signaling pathway renal fibrosis first sentence senp gene second sentence jnk gene list gene want train neural network think cnn rnn classify one word per sentence input embedding layer pre trained word vec pubmed also position word sentence csv file another column particular sentence train network normally whole sentence label predict new sentence using pre trained model
Use proxy sentences from cleaned data,"<p>Gensim's <code>Word2Vec</code> model takes as an  input a list of lists with the inner list containing individual tokens/words of a sentence. As I understand <code>Word2Vec</code> is used to ""quantify"" the context of words within a text using vectors. 
I am currently dealing with a corpus of text that has already been split into individual tokens and no longer contains an obvious sentence format (punctuation has been removed). I was wondering how should I input this into the <code>Word2Vec</code> model? </p>

<p>Say if I simply split the corpus into ""sentences"" of uniform length (10 tokens per sentence for example), would this be a good way of inputting the data into the model? </p>

<p>Essentially, <strong>I am wondering how the format of the input sentences (list of lists) affects the output of Word2Vec?</strong> </p>
",Vectorization & Embeddings,use proxy sentence cleaned data gensim model take input list list inner list containing individual token word sentence understand used quantify context word within text using vector currently dealing corpus text ha already split individual token longer contains obvious sentence format punctuation ha removed wa wondering input model say simply split corpus sentence uniform length token per sentence example would good way inputting data model essentially wondering format input sentence list list affect output word vec
Comparing strings in huge lists but cannot use set in Python,"<p>I have a text file with 11965 entries that looks like:</p>

<pre><code>AAA
BBB
CCC
DDD

Which I transformed into:
list_1 = ['AAA', 'BBB', 'CCC', ...]
</code></pre>

<p>And I need to compare it with another text file with 2221545 entries that looks like:</p>

<pre><code>AAA,.ADJ UK
AAA,.N UK
AAA,.N ES
B,.ADV UK
BB,.ADV UK
BBB,.N IT

Which I transformed into:
list_2 = ['AAA\tADJ\tUK', 'AAA\tN\tUK', 'AAA\tN\tES', 'B\tADV\UK', 'BB\tADV\tUK', ...]
</code></pre>

<p>So I have to get a dict that looks like this:</p>

<pre><code>result_dict = {'AAA':[[UK, ADJ, N], [ES,N]], 'BBB':[[IT,N]], ...}
</code></pre>

<p>Due to the size of the second list, if we compare the entries one by one the time complexity will be <code>O(11965*2221545)</code>. (Am I getting in right?)</p>

<p>And because I have to get the entire entry, I cannot use set to compare them. Is there any efficient way to get the job done?</p>
",Vectorization & Embeddings,comparing string huge list use set python text file entry look like need compare another text file entry look like get dict look like due size second list compare entry one one time complexity getting right get entire entry use set compare efficient way get job done
how to cal tfidf in different way,"<p>I have two type document, one is labeled, and other is not. 
I want use the labeled document to calculate tf, and use other unlabeled document to calculate idf.</p>

<pre><code>from gensim import corpora, models, similarities

dictionary = corpora.Dictionary(line.lower().split(',') for line in open('data/unannotated.csv',encoding='utf-8'))
dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000, keep_tokens=None) 
</code></pre>

<p>This unannotated is unlabeled data, one line is one document.
Based on this data, I got a dict:</p>

<pre><code>{'get': 353867,
 'http': 351618,
 'u': 324711,
 'one': 291526,
 'go': 279237,
 'know': 265001,
 'good': 249368,
 'say': 236003,
 'like': 225619,
 'qatar': 223010,
 'time': 191195,
 'would': 188766,
 'think': 187128,
 'people': 182873,
 'make': 180120,
 'need': 166287,
 'take': 157106,
...
}
</code></pre>

<p>I use next piece of code to calculate tfidf, and it gives all same answer:0.9999999999999946</p>

<pre><code>def TFCalculator(question, word):
    if word not in question:
        return 0
    count = dict(Counter(question))
    q_len = len(question)
    return float(count[word]) / float(q_len)

def n_containing(unannotated, word):
    return float(unannotated.get(word,0))

def IDFCalculator(unannotated, word):
    return math.log(float(len(unannotated.keys())) / (1.0 + n_containing(unannotated, word)))

def tfidf(stem, unannotated):
    tfidfVector_ques = range(len(unannotated))
    for word in stem:
        tf= TFCalculator(stem,word)
        idf = IDFCalculator(unannotated, word)
        tfidf = tf * idf
        try:
            tfidfVector_ques[list(unannotated).index(word)] = tfidf
        except:
            pass
    return sparse.csr_matrix(tfidfVector_ques)

def cosine_similarities(question,comment,unannotated):
    X_tfidf = tfidf(question,unannotated)
    Y_tfidf = tfidf(comment,unannotated)
    similarities = cosine_similarity(X_tfidf,Y_tfidf)
    return similarities
</code></pre>
",Vectorization & Embeddings,cal tfidf different way two type document one labeled want use labeled document calculate tf use unlabeled document calculate idf unannotated unlabeled data one line one document based data got dict use next piece code calculate tfidf give answer
Keras how to save specified variable?,"<p>I want to save my trained model in <code>keras</code> with 10 fold cv, and I use the word embedding in my model, so <code>keras</code> saved model file is too big, about 550M. and 10 fold trained model is about 5GB.</p>

<p>If I could delete the embedding variable in the saved model,  and just save another variable, I think I could save most file size, because the word embedding array file is about 500M. And then total file size will be reduced to 1GB. </p>

<p>But I can not make it with <code>keras</code>.</p>

<p>Also when I try using tf.train.saver in <code>keras</code>, something strange happens.</p>

<p>Does anyone have any idea?</p>
",Vectorization & Embeddings,kera save specified variable want save trained model fold cv use word embedding model saved model file big fold trained model gb could delete embedding variable saved model save another variable think could save file size word embedding array file total file size reduced gb make also try using tf train saver something strange happens doe anyone idea
Index of the matched words of the given two text,"<p>I have been working on finding the lower level clinical terms in given document either in the same exact words or in different words but the same meaning. I used cosine similarity matching for the given text with every terms I have to match with and I do get the value of how much it has matched the given text highest cos value gives me the exact value.</p>

<pre><code>sent_list = process.SBD(""The patient has been given paracetamol for fever in interval of every two hour. There has been sever headache and abnorm of the labor. Continuation of these medicine might lead to abdomen has been crushing."")
</code></pre>

<p>output:
<code>[['Arenaviral haemorrhagic fever'], ['Abnormal labor'], ['Abdomen crushing']]</code></p>

<p>but I also need to get the index of the words which has matched in the text
Any algorithm to get the index of words matched in the given text.</p>
",Vectorization & Embeddings,index matched word given two text working finding lower level clinical term given document either exact word different word meaning used cosine similarity matching given text every term match get value much ha matched given text highest co value give exact value output also need get index word ha matched text algorithm get index word matched given text
Custom loss in Keras with softmax to one-hot,"<p>I have a model that outputs a Softmax, and I would like to develop a custom loss function. The desired behaviour would be:</p>

<p>1) Softmax to one-hot (normally I do numpy.argmax(softmax_vector) and set that index to 1 in a null vector, but this is not allowed in a loss function).</p>

<p>2) Multiply the resulting one-hot vector by my embedding matrix to get an embedding vector (in my context: the word-vector that is associated to a given word, where words have been tokenized and assigned to indices, or classes for the Softmax output).</p>

<p>3) Compare this vector with the target (this could be a normal Keras loss function).</p>

<p>I know how to write a custom loss function in general, but not to do this. I found this <a href=""https://stackoverflow.com/questions/50082781/how-to-produce-a-one-hot-output-in-keras"">closely related question</a> (unanswered), but my case is a bit different, since I would like to preserve my softmax output.</p>
",Vectorization & Embeddings,custom loss kera softmax one hot model output softmax would like develop custom loss function desired behaviour would softmax one hot normally numpy argmax softmax vector set index null vector allowed loss function multiply resulting one hot vector embedding matrix get embedding vector context word vector associated given word word tokenized assigned index class softmax output compare vector target could normal kera loss function know write custom loss function general found href related question unanswered case bit different since would like preserve softmax output
Gradient Descent &amp; Backpropagation difference b/w Embedding Layer in Keras and Word2Vec of Gensim,"<p>I was working on a Amazon Sentiment Classification dataset, where I have to predict the sentiment based on the reviews given. However I was experimenting with 2 methods, one with the normal <code>Embedding</code> layer from Keras, and this is my architecture:</p>

<pre><code>model = Sequential()
model.add(Embedding(MAX_NB_WORDS, 128))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2,input_shape=(1,)))
model.add(Dense(1, activation='sigmoid'))
</code></pre>

<p>And the next one I was trying with <code>Word2Vec</code> from <code>gensim.models</code>. Here is my code:</p>

<pre><code>from gensim.models import Word2Vec
word_model = Word2Vec(df['reviewText'], size=200, min_count = 1, window = 5,sg=0, negative=5)
WV_DIM = 100
nb_words = min(MAX_NB_WORDS, len(word_vectors.vocab))
# we initialize the matrix with random numbers
wv_matrix = (np.random.rand(nb_words, WV_DIM) - 0.5) / 5.0
for word, i in word_index.items():
    if i &gt;= MAX_NB_WORDS:
        continue
    try:
        embedding_vector = word_vectors[word]
        # words not found in embedding index will be all-zeros.
        wv_matrix[i] = embedding_vector
    except:
        pass    
model = Sequential()
model.add(Embedding(nb_words,
                     WV_DIM,
                     mask_zero=False,
                     weights=[wv_matrix],
                     input_length=MAX_SEQUENCE_LENGTH,
                     trainable=False))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2,input_shape=(1,)))
model.add(Dense(1, activation='sigmoid'))
</code></pre>

<p>But I'm unable to understand what is the difference b/w just using the <code>model.add(Embedding(..))</code> and using the <code>word2vec</code> along with <code>Embedding</code>. I want to know the Maths behind the working of Keras's <code>Embedding</code> layer and how <code>word2vec</code>+<code>Embedding</code> is working out. </p>

<p>I've gone through <a href=""https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work"">this post</a> to understand how Keras embedding layer work out but I want to understand the gradient descent and backpropagation for it and also for the combination of <code>word2vec</code> and <code>Embedding</code>.</p>

<p>P.S. For the both the codes I've given, I have skipped the <code>tokenize</code> and <code>pad_sequence</code> part.</p>
",Vectorization & Embeddings,gradient descent backpropagation difference b w embedding layer kera word vec gensim wa working amazon sentiment classification dataset predict sentiment based review given however wa experimenting method one normal layer kera architecture next one wa trying code unable understand difference b w using using along want know math behind working kera layer working gone href post understand kera embedding layer work want understand gradient descent backpropagation also combination p code given skipped part
Doc2Vec Get most similar documents,"<p>I am trying to build a document retrieval model that returns most documents ordered by their relevancy with respect to a query or a search string. For this I trained a doc2vec model using the <code>Doc2Vec</code> model in gensim. My dataset is in the form of a pandas dataset which has each document stored as a string on each line. This is the code I have so far</p>

<pre><code>import gensim, re
import pandas as pd

# TOKENIZER
def tokenizer(input_string):
    return re.findall(r""[\w']+"", input_string)

# IMPORT DATA
data = pd.read_csv('mp_1002_prepd.txt')
data.columns = ['merged']
data.loc[:, 'tokens'] = data.merged.apply(tokenizer)
sentences= []
for item_no, line in enumerate(data['tokens'].values.tolist()):
    sentences.append(LabeledSentence(line,[item_no]))

# MODEL PARAMETERS
dm = 1 # 1 for distributed memory(default); 0 for dbow 
cores = multiprocessing.cpu_count()
size = 300
context_window = 50
seed = 42
min_count = 1
alpha = 0.5
max_iter = 200

# BUILD MODEL
model = gensim.models.doc2vec.Doc2Vec(documents = sentences,
dm = dm,
alpha = alpha, # initial learning rate
seed = seed,
min_count = min_count, # ignore words with freq less than min_count
max_vocab_size = None, # 
window = context_window, # the number of words before and after to be used as context
size = size, # is the dimensionality of the feature vector
sample = 1e-4, # ?
negative = 5, # ?
workers = cores, # number of cores
iter = max_iter # number of iterations (epochs) over the corpus)

# QUERY BASED DOC RANKING ??
</code></pre>

<p>The part where I am struggling is in finding documents that are most similar/relevant to the query. I used the <code>infer_vector</code> but then realised that it considers the query as a document, updates the model and returns the results. I tried using the <code>most_similar</code> and <code>most_similar_cosmul</code> methods but I get words along with a similarity score(I guess) in return. What I want to do is when I enter a search string(a query), I should get the documents (ids) that are most relevant along with a similarity score(cosine etc). How do I get this part done?</p>
",Vectorization & Embeddings,doc vec get similar document trying build document retrieval model return document ordered relevancy respect query search string trained doc vec model using model gensim dataset form panda dataset ha document stored string line code far part struggling finding document similar relevant query used realised considers query document update model return result tried using method get word along similarity score guess return want enter search string query get document id relevant along similarity score cosine etc get part done
Finding most similar sentence match,"<p>I have a large dataset containing a mix of words and short phrases, such as:</p>

<pre><code>dataset = [
    ""car"",
    ""red-car"",
    ""lorry"",
    ""broken lorry"",
    ""truck owner"",
    ""train"",
    ...
]
</code></pre>

<p>I am trying to find a way to determine the most similar word from a short sentence, such as:</p>

<pre><code>input = ""I love my car that is red""   # should map to ""red-car""
input = ""I purchased a new lorry""     # should map to ""lorry""
input = ""I hate my redcar""            # should map to ""red-car""
input = ""I will use my truck""         # should map to ""truck owner""
input = ""Look at that yellow lorri""   # should map to ""lorry""
</code></pre>

<p>I have tried a number of approaches to this with no avail, including:</p>

<p>Vectoring the <code>dataset</code> and the <code>input</code> using TfidfVectorizer, then calculating the Cosine similarity of the vectorized <code>input</code> value against each individual, vectorized item value from the <code>dataset</code>.</p>

<p>The problem is, this only really works if the <code>input</code> contains the exact word(s) that are in the dataset - so for example, in the case where the <code>input = ""trai""</code> then it would have a cosine value of 0, whereas I am trying to get it to map to the value <code>""train""</code> in the dataset.</p>

<p>The most obvious solution would be to perform a simple spell check, but that may not be a valid option, because I still want to choose the most similar result, even when the words are slightly different, i.e.:</p>

<pre><code>input = ""broke""    # should map to ""broken lorry"" given the above dataset
</code></pre>

<p>If someone could suggest other potential approach I could try, that would be much appreciated.</p>
",Vectorization & Embeddings,finding similar sentence match large dataset containing mix word short phrase trying find way determine similar word short sentence tried number approach avail including vectoring using tfidfvectorizer calculating cosine similarity vectorized value individual vectorized item value problem really work contains exact word dataset example case would cosine value whereas trying get map value dataset obvious solution would perform simple spell check may valid option still want choose similar result even word slightly different e someone could suggest potential approach could try would much appreciated
loading of fasttext pre trained german word embedding&#39;s .vec file throwing out of memory error,"<p>I am using gensim to load the fasttext's pre-trained word embedding</p>

<p><code>de_model = KeyedVectors.load_word2vec_format('wiki.de\wiki.de.vec')</code></p>

<p>But this gives me a memory error.</p>

<p>Is there any way I can load it?</p>
",Vectorization & Embeddings,loading fasttext pre trained german word embedding vec file throwing memory error using gensim load fasttext pre trained word embedding give memory error way load
Assign categories to words based on similarities,"<p>I have corpus of about 1 million documents of sentences. Let's say:-</p>

<blockquote>
  <p>sentence 1: ""Thrilling contest between manchester city and manchester united ends in draw""</p>
</blockquote>

<p>I want to assign sentences based on categories like above message belongs to category of ""sports"" and again assigned it to further categories like ""football"" categories in sports. 
I want to categorize texts into four categories ""entertainment"",""sports"",""politics"",""technology"". 
I have used an approach of Word2vec but it can only tells the relationship between two sentences. How exactly I can do this? I don't have any predefined labelled data just only million of records.
what I really want to do something like <a href=""https://www.paralleldots.com/text-classification"" rel=""nofollow noreferrer"">this</a></p>
",Vectorization & Embeddings,assign category word based similarity corpus million document sentence let say sentence thrilling contest manchester city manchester united end draw want assign sentence based category like message belongs category sport assigned category like football category sport want categorize text four category entertainment sport politics technology used approach word vec tell relationship two sentence exactly predefined labelled data million record really want something like
word2Vec and abbreviations,"<p>I am working on text classification task where my dataset contains a lot of abbreviations and proper nouns. For instance: <strong>Milka choc. bar</strong>.<br>
My idea is to use bidirectional LSTM model with word2vec embedding.<br>
And here is my problem how to code words, that not appears in the dictionary? 
I partially solved this problem by merging pre-trained vectors with randomly initialized. Here is my implementation:</p>

<pre><code>import gensim
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess

from gensim.models.keyedvectors import KeyedVectors

word_vectors = KeyedVectors.load_word2vec_format('ru.vec', binary=False, unicode_errors='ignore')

EMBEDDING_DIM=300
vocabulary_size=min(len(word_index)+1,num_words)
embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))
for word, i in word_index.items():
    if i&gt;=num_words:
        continue
    try:
        embedding_vector = word_vectors[word]
        embedding_matrix[i] = embedding_vector
    except KeyError:
        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)

def LSTMModel(X,words_nb, embed_dim, num_classes):
    _input = Input(shape=(X.shape[1],))
    X = embedding_layer = Embedding(words_nb,
                            embed_dim,
                            weights=[embedding_matrix],
                            trainable=True)(_input)
   X = The_rest_of__the_LSTM_model()(X)
</code></pre>

<p>Do you think, that allowing the model to adjust the embedding weights is a good idea? 
Could you please tell me, how can I encode words like <strong>choc</strong>? Obviously, this abbreviation stands for <strong>chocolate</strong>.   </p>
",Vectorization & Embeddings,word vec abbreviation working text classification task dataset contains lot abbreviation proper noun instance milka choc bar idea use bidirectional lstm model word vec embedding problem code word appears dictionary partially solved problem merging pre trained vector randomly initialized implementation think allowing model adjust embedding weight good idea could please tell encode word like choc obviously abbreviation stand chocolate
Can we use char rnn to create embeddings for out of vocabulary words?,"<p>I have word embeddings for 10 million words which were trained on a huge corpus. Now I want to produce word embeddings for out of vocabulary words.  Can I design some char RNN to use these word embeddings and generate embeddings for out of vocab words? Or is there any other I can get embeddings for OOV words?</p>

<p>FastText is capable of producing word embeddings for OOV but it does not have distributed way of training or GPU implementation, so in my case, it could take almost 3 months to finish training. Any suggestions on this?</p>
",Vectorization & Embeddings,use char rnn create embeddings vocabulary word word embeddings million word trained huge corpus want produce word embeddings vocabulary word design char rnn use word embeddings generate embeddings vocab word get embeddings oov word fasttext capable producing word embeddings oov doe distributed way training gpu implementation case could take almost month finish training suggestion
How to predict word using trained skipgram model?,"<p>I'm using Google's Word2vec and I'm wondering how to get the top words that are predicted by a skipgram model that is trained using hierarchical softmax, given an input word?</p>

<p>For instance, when using negative sampling, one can simply multiply an input word's embedding (from the input matrix) with each of the vectors in the output matrix and take the one with the top value. However, in hierarchical softmax, there are multiple output vectors that correspond to each input word, due to the use of the Huffman tree. </p>

<p>How do we compute the likelihood value/probability of an output word given an input word in this case?</p>
",Vectorization & Embeddings,predict word using trained skipgram model using google word vec wondering get top word predicted skipgram model trained using hierarchical softmax given input word instance using negative sampling one simply multiply input word embedding input matrix vector output matrix take one top value however hierarchical softmax multiple output vector correspond input word due use huffman tree compute likelihood value probability output word given input word case
Get most frequent contexts between two words in word2vec,"<p>I've already built my own Skip-Gram model by using gensim word2vec. I know that I can get the similarity score between two words, e.g. <code>model.wv.similarity('car', 'truck') -&gt; 0.75</code>. Now I want to know why they are such ""similars"".</p>

<p>Since Skip-Gram has been trained with the context words, I suppose that there is a way to get the most frequent context words shared between <code>car</code> and <code>truck</code>. Another example: if I have the following sentences, I'd like to get the word <code>slow</code> as ""most frequent context"":</p>

<ul>
<li><code>the car is slow</code></li>
<li><code>the truck is slow</code></li>
<li><code>the car is red</code></li>
</ul>

<p>Notice that <code>red</code> isn't appear with <code>truck</code>, so it shouldn't be returned as ""most frequent context"".</p>

<p>Is there any way to do this?</p>
",Vectorization & Embeddings,get frequent context two word word vec already built skip gram model using gensim word vec know get similarity score two word e g want know similars since skip gram ha trained context word suppose way get frequent context word shared another example following sentence like get word frequent context notice appear returned frequent context way
Word embedding as features for classification,"<p>W.r.t text classification, now a common approach is to combine (often sum or average) word embeddings and use the result vector as features.</p>

<p>Are there any reference document(s) that establish the comparison of this approach for text classification over traditional feature engineering approaches? [Comparison based on accuracy] [could be on popular datasets like IMDB, sentiment-140 etc]</p>
",Vectorization & Embeddings,word embedding feature classification w r text classification common approach combine often sum average word embeddings use result vector feature reference document establish comparison approach text classification traditional feature engineering approach comparison based accuracy could popular datasets like imdb sentiment etc
Got a smaller vocabulary after using tf.contrib.learn.preprocessing.VocabularyProcessor,"<pre><code>#!/usr/bin/env python
# -*- coding: utf-8  -*-    
import warnings

warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')  

import logging
import os.path
import sys
import multiprocessing

# from gensim.corpora import WikiCorpus
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence

if __name__ == '__main__':    
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s', level=logging.INFO)
    logger.info(""running %s"" % ' '.join(sys.argv))

    min_count=100
    data_dir='/opt/mengyuguang/word2vec/'
    inp = data_dir + 'wiki.zh.simp.seg.txt'
    outp1 = data_dir + 'wiki.zh.min_count{}.model'.format(str(min_count))
    outp2 = data_dir + 'wiki.zh.min_count{}.vector'.format(str(min_count))

    # train cbow
    model = Word2Vec(LineSentence(inp), size=300,
                     workers=multiprocessing.cpu_count(),min_count=min_count)

    # save
    model.save(outp1)
    model.wv.save_word2vec_format(outp2, binary=False)
</code></pre>

<p>Firstly,I trained word embedding with the code above, I don't think there is anything wrong with it. And I created a list <code>vocab</code> to store the words in the vector file. Then </p>

<pre><code>vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document_length) 
pretrain = vocab_processor.fit(vocab)
</code></pre>

<p>Vocab is a list of  415657 words. And I got a vocabulary of 412722. I know that <code>vocab_processor.fit</code> won't take upper and lower case as two words. This is really strange. How is this happening?<br>
I checked the vector file again. There are no overlapping words at all.</p>
",Vectorization & Embeddings,got smaller vocabulary using tf contrib learn preprocessing vocabularyprocessor firstly trained word embedding code think anything wrong created list store word vector file vocab list word got vocabulary know take upper lower case two word really strange happening checked vector file overlapping word
How to make word2vec model&#39;s loading time and memory use more efficient?,"<p>I want to use Word2vec in a web server (production) in two different variants where I fetch two sentences from the web and compare it in real-time. For now, I am testing it on a local machine which has 16GB RAM. </p>

<p>Scenario:
w2v = load w2v model </p>

<pre><code>If condition 1 is true:
   if normalize:
      reverse normalize by w2v.init_sims(replace=False) (not sure if it will work)
   Loop through some items:
   calculate their vectors using w2v
else if condition 2 is true:
   if not normalized:
       w2v.init_sims(replace=True)
   Loop through some items:
   calculate their vectors using w2v
</code></pre>

<p>I have already read the solution about reducing the vocabulary size to a small size but I would like to use all the vocabulary. </p>

<p>Are there new workarounds on how to handle this? Is there a way to initially load a small portion of the vocabulary for first 1-2 minutes and in parallel keep loading the whole vocabulary? </p>
",Vectorization & Embeddings,make word vec model loading time memory use efficient want use word vec web server production two different variant fetch two sentence web compare real time testing local machine ha gb ram scenario w v load w v model already read solution reducing vocabulary size small size would like use vocabulary new workarounds handle way initially load small portion vocabulary first minute parallel keep loading whole vocabulary
Explicit Semantic Analysis implementation,"<p>I would like to compute the semantic similarity between texts based on explicit semantic similarity methods ESA , can i found an implementation with tutorial  of this methods?</p>
",Vectorization & Embeddings,explicit semantic analysis implementation would like compute semantic similarity text based explicit semantic similarity method esa found implementation tutorial method
Gensim doc2vec most_similar equivalent to get full documents,"<p>In Gensim's doc2vec implementation, <code>gensim.models.keyedvectors.Doc2VecKeyedVectors.most_similar</code> returns the tags and cosine similarity of the documents most similar to the query document. What if I want the actual documents themselves and not the tags? Is there a way to do that directly without searching for the document associated with the tag returned by <code>most_similar</code>?</p>

<p>Also, is there documentation on this? I can't seem to find the documentation for half of Gensim's classes.</p>
",Vectorization & Embeddings,gensim doc vec similar equivalent get full document gensim doc vec implementation return tag cosine similarity document similar query document want actual document tag way directly without searching document associated tag returned also documentation seem find documentation half gensim class
Word Embeddings with neural networks in keras,"<p>Can someone give me an intuitive explanation for why we use word embeddings and how neural networks make this process easier and better? </p>
",Vectorization & Embeddings,word embeddings neural network kera someone give intuitive explanation use word embeddings neural network make process easier better
How to account for variation in spelling (especially for slang) for Word Embeddings/Word2Vec generation using song lyrics?,"<p>So I am working on a artist classification project that utilizes hip hop lyrics from genius.com.  The problem is these lyrics are user generated, so the same word can be spelled in various different ways, especially if it is slang which is a very common case in hip hop. </p>

<p>I looked into spell correction using hunspell/pyhunspell, but the problem with that is it doesn't fix slang misspellings.  I technically could make a mini dictionary with a bunch of misspelled variations but that is effectively useless because there could be a dozen variations of the same word over my (growing) 6000 song corpus.  </p>

<p>Any suggestions?</p>
",Vectorization & Embeddings,account variation spelling especially slang word embeddings word vec generation using song lyric working artist classification project utilizes hip hop lyric genius com problem lyric user generated word spelled various different way especially slang common case hip hop looked spell correction using hunspell pyhunspell problem fix slang misspelling technically could make mini dictionary bunch misspelled variation effectively useless could dozen variation word growing song corpus suggestion
How can I get the word2vec.bin file,"<p>I want to build a chatbot using python and deeplearning methodology.Iam referring the below link
<a href=""https://github.com/shreyans29/thesemicolon/blob/master/chatbotPreprocessing.py"" rel=""nofollow noreferrer"">chatbot code</a>
But I troubled in the word2vec.bin file as describing in the code.Where should I get the bin file?</p>
",Vectorization & Embeddings,get word vec bin file want build chatbot using python deeplearning methodology iam referring link chatbot code troubled word vec bin file describing code get bin file
Calculating the similarity between two vectors,"<p>I did LDA over a corpus of documents with topic_number=5. As a result, I have five vectors of words, each word associates with a weight or degree of importance, like this:</p>
<pre><code>Topic_A = {(word_A1,weight_A1), (word_A2, weight_A2), ... ,(word_Ak, weight_Ak)}
Topic_B = {(word_B1,weight_B1), (word_B2, weight_B2), ... ,(word_Bk, weight_Bk)}
.
.
Topic_E = {(word_E1,weight_E1), (word_E2, weight_E2), ... ,(word_Ek, weight_Ek)}
</code></pre>
<p>Some of the words are common between documents. Now, I want to know, how I can calculate the similarity between these vectors. I can calculate cosine similarity (and other similarity measures) by programming from scratch, but I was thinking, there might be an easier way to do it. Any help would be appreciated. Thank you in advance for spending time on this.</p>
<blockquote>
<ul>
<li><p>I am programming with Python 3.6 and gensim library (but I am open to any other library)</p>
</li>
<li><p>I know someone else has asked similar question (<a href=""https://stackoverflow.com/questions/48115965/cosine-similarity-and-lda-topics"">Cosine Similarity and LDA topics</a>) but becasue he didn't get the answer, I ask it again</p>
</li>
</ul>
</blockquote>
",Vectorization & Embeddings,calculating similarity two vector lda corpus document topic number result five vector word word associate weight degree importance like word common document want know calculate similarity vector calculate cosine similarity similarity measure programming scratch wa thinking might easier way help would appreciated thank advance spending time programming python gensim library open library know someone else ha asked similar question href similarity lda topic becasue get answer ask
Alternatives to TF-IDF and Cosine Similarity (comparing documents with different formats),"<p>I've been working on a small, personal project which takes a user's job skills and suggests the most ideal career for them based on those skills.  I use a database of job listings to achieve this.  At the moment, the code works as follows:</p>

<p>1) Process the text of each job listing to extract skills that are mentioned in the listing</p>

<p>2) For each career (e.g. ""Data Analyst""), combine the processed text of the job listings for that career into one document</p>

<p>3) Calculate the TF-IDF of each skill within the career documents</p>

<p>After this, I'm not sure which method I should use to rank careers based on a list of a user's skills.  The most popular method that I've seen would be to treat the user's skills as a document as well, then to calculate the TF-IDF for the skill document, and use something like cosine similarity to calculate the similarity between the skill document and each career document.</p>

<p>This doesn't seem like the ideal solution to me, since cosine similarity is best used when comparing two documents of the same format.  For that matter, TF-IDF doesn't seem like the appropriate metric to apply to the user's skill list at all.  For instance, if a user adds additional skills to their list, the TF for each skill will drop.  In reality, I don't care what the frequency of the skills are in the user's skills list -- I just care that they have those skills (and maybe how well they know those skills).</p>

<p>It seems like a better metric would be to do the following:</p>

<p>1) For each skill that the user has, calculate the TF-IDF of that skill in the career documents</p>

<p>2) For each career, sum the TF-IDF results for all of the user's skill </p>

<p>3) Rank career based on the above sum</p>

<p>Am I thinking along the right lines here?  If so, are there any algorithms that work along these lines, but are more sophisticated than a simple sum? Thanks for the help!</p>
",Vectorization & Embeddings,alternative tf idf cosine similarity comparing document different format working small personal project take user job skill suggests ideal career based skill use database job listing achieve moment code work follows process text job listing extract skill mentioned listing career e g data analyst combine processed text job listing career one document calculate tf idf skill within career document sure method use rank career based list user skill popular method seen would treat user skill document well calculate tf idf skill document use something like cosine similarity calculate similarity skill document career document seem like ideal solution since cosine similarity best used comparing two document format matter tf idf seem like appropriate metric apply user skill list instance user add additional skill list tf skill drop reality care frequency skill user skill list care skill maybe well know skill seems like better metric would following skill user ha calculate tf idf skill career document career sum tf idf result user skill rank career based sum thinking along right line algorithm work along line sophisticated simple sum thanks help
How to calculate cosine similarity with already-calculated TFIDF scores,"<p>I need to calculate cosine similarity between documents with already calculated TFIDF scores. </p>

<p>Usually I would use (e.g.) <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">TFIDFVectorizer</a> which would create a matrix of documents / terms, calculating TFIDF scores as it goes. I can't apply this because it will re-calculate TFIDF scores. This would be incorrect because the documents have already had a large amount of pre-processing including Bag of Words and IDF filtering (I  will not explain why - too long).</p>

<p>Illustrative input CSV file:</p>

<pre><code>Doc, Term,    TFIDF score
1,   apples,  0.3
1,   bananas, 0.7
2,   apples,  0.1
2,   pears,   0.9
3,   apples,  0.6
3,   bananas, 0.2
3,   pears,   0.2
</code></pre>

<p>I need to generate the matrix that would normally be generated by TFIDFVectorizer, e.g.:</p>

<pre><code>  | apples | bananas | pears
1 | 0.3    | 0.7     | 0
2 | 0.1    | 0       | 0.9
3 | 0.6    | 0.2     | 0.2 
</code></pre>

<p>... so that I can calculate cosine similarity between documents. </p>

<p>I'm using Python 2.7 but suggestions for other solutions or tools are welcome. I can't easily switch to Python 3.</p>

<p>Edit:</p>

<p>This isn't really about transposing numpy arrays. It involves mapping TFIDF scores to a document / term matrix, with tokenized terms, and missing values filled in as 0. </p>
",Vectorization & Embeddings,calculate cosine similarity already calculated tfidf score need calculate cosine similarity document already calculated tfidf score usually would use e g tfidfvectorizer would create matrix document term calculating tfidf score go apply calculate tfidf score would incorrect document already large amount pre processing including bag word idf filtering explain long illustrative input csv file need generate matrix would normally generated tfidfvectorizer e g calculate cosine similarity document using python suggestion solution tool welcome easily switch python edit really transposing numpy array involves mapping tfidf score document term matrix tokenized term missing value filled
Cosine similarity with word2vec,"<p>I load a word2vec-format file and I want to calculate the similarities between vectors, but I don't know what this issue means.</p>

<pre><code>from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import KeyedVectors
import numpy as np

model = KeyedVectors.load_word2vec_format('it-vectors.100.5.50.w2v')

similarities = cosine_similarity(model.vectors)


---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
&lt;ipython-input-54-1d4e62f55ebf&gt; in &lt;module&gt;()
----&gt; 1 similarities = cosine_similarity(model.vectors)

/usr/local/lib/python3.5/dist-packages/sklearn/metrics/pairwise.py in cosine_similarity(X, Y, dense_output)
    923         Y_normalized = normalize(Y, copy=True)
    924 
--&gt; 925     K = safe_sparse_dot(X_normalized, Y_normalized.T, dense_output=dense_output)
    926 
    927     return K

/usr/local/lib/python3.5/dist-packages/sklearn/utils/extmath.py in safe_sparse_dot(a, b, dense_output)
    138         return ret
    139     else:
--&gt; 140         return np.dot(a, b)
    141 
    142 

MemoryError: 
</code></pre>

<p>What it means?
Thank you!</p>
",Vectorization & Embeddings,cosine similarity word vec load word vec format file want calculate similarity vector know issue mean mean thank
How to convert gensim Word2Vec model to FastText model?,"<p>I have a Word2Vec model which was trained on a huge corpus. While using this model for Neural network application I came across quite a few ""Out of Vocabulary"" words. Now I need to find word embeddings for these ""Out of Vocabulary"" words. So I did some googling and found that Facebook has recently released a FastText library for this. Now my question is how can I convert my existing word2vec model or Keyedvectors to FastText model?</p>
",Vectorization & Embeddings,convert gensim word vec model fasttext model word vec model wa trained huge corpus using model neural network application came across quite vocabulary word need find word embeddings vocabulary word googling found facebook ha recently released fasttext library question convert existing word vec model keyedvectors fasttext model
Why Doc2vec gives 2 different vectors for the same texts,"<p>I am using <code>Doc2vec</code> to get vectors from words.
Please see my below code:</p>

<pre><code>from gensim.models.doc2vec import TaggedDocument
f = open('test.txt','r')

trainings = [TaggedDocument(words = data.strip().split("",""),tags = [i]) for i,data in enumerate(f)


model = Doc2Vec(vector_size=5,  epochs=55, seed = 1, dm_concat=1)

model.build_vocab(trainings)
model.train(trainings, total_examples=model.corpus_count, epochs=model.epochs)

model.save(""doc2vec.model"")

model = Doc2Vec.load('doc2vec.model')
for i in range(len(model.docvecs)):
    print(i,model.docvecs[i])
</code></pre>

<p>I have a <code>test.txt</code> file that its content has 2 lines and contents of these 2 lines is the same (they are ""a"")
I trained with doc2vec and got the model, but the problem is although the contents of 2 lines is the same, doc2vec gave me 2 different vectors.</p>

<pre><code>0 [ 0.02730868  0.00393569 -0.08150548 -0.04009786 -0.01400406]
1 [ 0.03916578 -0.06423566 -0.05350181 -0.00726833 -0.08292392]
</code></pre>

<p>I dont know why this happened. I thought that these vectors would be the same.
Can you explain that? And if I want to make the same vectors for the sames words, what should I do in this case?</p>
",Vectorization & Embeddings,doc vec give different vector text using get vector word please see code file content ha line content line trained doc vec got model problem although content line doc vec gave different vector dont know happened thought vector would explain want make vector word case
Training Word Vectors on whole corpus?,"<p>I am training word2vec model on my corpus and a friend of mine asked me if it is right to train the word2vec model on the whole corpus? Because when creating word embeddings I am using the whole corpus, so basically I am leaking the testing information to my network in form of these vectors, which is not ideal when training a neural network.</p>

<p>On the contrary, suppose that I am using a pre-trained word embeddings from google or any other source for that matter, if they have used the same document while creating these embedding, which I will be using to test my network, I will leaking the information anyway.</p>

<p>So my question is what is the right way to train the word2vec?</p>

<ul>
<li><p>Separating test and train data before creating word vectors?</p></li>
<li><p>Creating word vectors on the whole corpus?</p></li>
</ul>
",Vectorization & Embeddings,training word vector whole corpus training word vec model corpus friend mine asked right train word vec model whole corpus creating word embeddings using whole corpus basically leaking testing information network form vector ideal training neural network contrary suppose using pre trained word embeddings google source matter used document creating embedding using test network leaking information anyway question right way train word vec separating test train data creating word vector creating word vector whole corpus
Custom Transformer and FeatureUnion for word2vec,"<p>I am trying to classify a set of text documents using multiple sets of features. I am using <a href=""http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#sphx-glr-auto-examples-hetero-feature-union-py"" rel=""nofollow noreferrer"">sklearn's Feature Union</a> to combine different features for fitting into a single model. One of the features includes word embeddings using <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim's word2vec</a>.</p>

<pre><code>import numpy as np
from gensim.models.word2vec import Word2Vec
from sklearn.pipeline import FeatureUnion
from sklearn.pipeline import Pipeline
from sklearn.linear_model import SGDClassifier
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest

categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']
data = fetch_20newsgroups(subset='train', categories=categories)#dummy dataset

w2v_model= Word2Vec(data .data, size=100, window=5, min_count=5, workers=2)
word2vec={w: vec for w, vec in zip(w2v_model.wv.index2word, w2v_model.wv.syn0)} #dictionary of word embeddings
feat_select = SelectKBest(score_func=chi2, k=10) #other features
TSVD = TruncatedSVD(n_components=50, algorithm = ""randomized"", n_iter = 5)
#other features
</code></pre>

<p>In order to include transformers/estimators not already available in sklearn, I am attempting to wrap my word2vec results into a custom transformer class that returns the vector averages. </p>

<pre><code>class w2vTransformer(TransformerMixin):
    """"""
    Wrapper class for running word2vec into pipelines and FeatureUnions
    """"""
    def __init__(self,word2vec,**kwargs):
        self.word2vec=word2vec
        self.kwargs=kwargs
        self.dim = len(word2vec.values())
    def fit(self,x, y=None):
        return self

    def transform(self, X):
        return np.array([
        np.mean([self.word2vec[w] for w in words if w in self.word2vec] 
            or [np.zeros(self.dim)], axis=0)
       for words in X
])
</code></pre>

<p>However when it comes time to fit the model I receive an error.</p>

<pre><code>combined_features = FeatureUnion([(""w2v_class"",w2vTransformer(word2vec)),
     (""feat"",feat_select),(""TSVD"",TSVD)])#join features into combined_features
#combined_features = FeatureUnion([(""feat"",feat_select),(""TSVD"",TSVD)])#runs when word embeddings are not included    
text_clf_svm = Pipeline([('vect', CountVectorizer()),
         ('tfidf', TfidfTransformer()),
         ('feature_selection', combined_features),
          ('clf-svm',  SGDClassifier( loss=""modified_huber"")),
 ]) 

text_clf_svm_1 = text_clf_svm.fit(data.data,data.target) # fits data

text_clf_svm_1 = text_clf_svm.fit(data.data,data.target) # fits data
Traceback (most recent call last):

  File ""&lt;ipython-input-8-a085b7d40f8f&gt;"", line 1, in &lt;module&gt;
    text_clf_svm_1 = text_clf_svm.fit(data.data,data.target) # fits data

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 248, in fit
    Xt, fit_params = self._fit(X, y, **fit_params)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 213, in _fit
    **fit_params_steps[name])

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\memory.py"", line 362, in __call__
    return self.func(*args, **kwargs)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 581, in _fit_transform_one
    res = transformer.fit_transform(X, y, **fit_params)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 739, in fit_transform
    for name, trans, weight in self._iter())

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 779, in __call__
    while self.dispatch_one_batch(iterator):

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 625, in dispatch_one_batch
    self._dispatch(tasks)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 588, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\_parallel_backends.py"", line 111, in apply_async
    result = ImmediateResult(func)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\_parallel_backends.py"", line 332, in __init__
    self.results = batch()

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 131, in &lt;listcomp&gt;
    return [func(*args, **kwargs) for func, args, kwargs in self.items]

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 581, in _fit_transform_one
    res = transformer.fit_transform(X, y, **fit_params)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\base.py"", line 520, in fit_transform
    return self.fit(X, y, **fit_params).transform(X)

  File ""&lt;ipython-input-6-cbc52cd420cd&gt;"", line 16, in transform
    for words in X

  File ""&lt;ipython-input-6-cbc52cd420cd&gt;"", line 16, in &lt;listcomp&gt;
    for words in X

  File ""&lt;ipython-input-6-cbc52cd420cd&gt;"", line 14, in &lt;listcomp&gt;
    np.mean([self.word2vec[w] for w in words if w in self.word2vec]

TypeError: unhashable type: 'csr_matrix'

Traceback (most recent call last):

  File ""&lt;ipython-input-8-a085b7d40f8f&gt;"", line 1, in &lt;module&gt;
    text_clf_svm_1 = text_clf_svm.fit(data.data,data.target) # fits data

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 248, in fit
    Xt, fit_params = self._fit(X, y, **fit_params)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 213, in _fit
    **fit_params_steps[name])

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\memory.py"", line 362, in __call__
    return self.func(*args, **kwargs)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 581, in _fit_transform_one
    res = transformer.fit_transform(X, y, **fit_params)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 739, in fit_transform
    for name, trans, weight in self._iter())

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 779, in __call__
    while self.dispatch_one_batch(iterator):

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 625, in dispatch_one_batch
    self._dispatch(tasks)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 588, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\_parallel_backends.py"", line 111, in apply_async
    result = ImmediateResult(func)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\_parallel_backends.py"", line 332, in __init__
    self.results = batch()

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 131, in &lt;listcomp&gt;
    return [func(*args, **kwargs) for func, args, kwargs in self.items]

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 581, in _fit_transform_one
    res = transformer.fit_transform(X, y, **fit_params)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\base.py"", line 520, in fit_transform
    return self.fit(X, y, **fit_params).transform(X)

  File ""&lt;ipython-input-6-cbc52cd420cd&gt;"", line 16, in transform
    for words in X

  File ""&lt;ipython-input-6-cbc52cd420cd&gt;"", line 16, in &lt;listcomp&gt;
    for words in X

  File ""&lt;ipython-input-6-cbc52cd420cd&gt;"", line 14, in &lt;listcomp&gt;
    np.mean([self.word2vec[w] for w in words if w in self.word2vec]

TypeError: unhashable type: 'csr_matrix'
</code></pre>

<p>I understand that the error is because the variable ""words"" is a csr_matrix, but it needs to be an iterable such as a list. My question is how do I modify the transformer class or data so I can use the word embeddings as features to feed into FeatureUnion? This is my first SO post, please be gentle. </p>
",Vectorization & Embeddings,custom transformer featureunion word vec trying classify set text document using multiple set feature using sklearn feature union combine different feature fitting single model one feature includes word embeddings using gensim word vec order include transformer estimator already available sklearn attempting wrap word vec result custom transformer class return vector average however come time fit model receive error understand error variable word csr matrix need iterable list question modify transformer class data use word embeddings feature feed featureunion first post please gentle
reshape a batch of tensors with dynamic max length in tensorflow,"<p>In natural language processing, it is common to pad a batch of sequences. 
Here is the padding function.</p>

<pre><code>def pad_sequences(sequences, pad_tok=0):
""""""
Args:
    sequences: a generator of list or tuple
    pad_tok: the char to pad with

Returns:
    a list of list where each sublist has same length
    a list record original length of sequences

""""""
sequence_padded, sequence_length = [], []
sequence_padded = tf.keras.preprocessing.sequence.pad_sequences(sequences,
                                    padding='post', value=pad_tok)

max_sen_len = 0
for seq in sequences:
    seq = list(seq)
    if len(seq) &gt; max_sen_len:
        max_sen_len = len(seq)

return sequence_padded, max_sen_len
</code></pre>

<p>Input is a batch of sentences of variable length. 
Each sentence is a list of ids, each id represents one word in vocabulary.</p>

<p>Here is the feeddict</p>

<pre><code>    word_ids_left, maxlen_left = pad_sequences(word_ids_left)
    pos1_ids_left, _ = pad_sequences(pos1_ids_left, pad_tok=499)
    pos2_ids_left, _ = pad_sequences(pos2_ids_left, pad_tok=499)

    word_ids_mid, maxlen_mid = pad_sequences(word_ids_mid)
    pos1_ids_mid, _ = pad_sequences(pos1_ids_mid, pad_tok=499)
    pos2_ids_mid, _ = pad_sequences(pos2_ids_mid, pad_tok=499)

    word_ids_right, maxlen_right = pad_sequences(word_ids_right)
    pos1_ids_right, _ = pad_sequences(pos1_ids_right, pad_tok=499)
    pos2_ids_right, _ = pad_sequences(pos2_ids_right, pad_tok=499)


    # build feed dictionary
    feed = {
        self.word_ids_left:  word_ids_left,
        self.pos1_ids_left:  pos1_ids_left,
        self.pos2_ids_left:  pos2_ids_left,
        self.word_ids_mid:   word_ids_mid,
        self.pos1_ids_mid:   pos1_ids_mid,
        self.pos2_ids_mid:   pos2_ids_mid,
        self.word_ids_right: word_ids_right,
        self.pos1_ids_right: pos1_ids_right,
        self.pos2_ids_right: pos2_ids_right,
        self.maxlen_left:    maxlen_left,
        self.maxlen_mid:     maxlen_mid,
        self.maxlen_right:   maxlen_right
    }
</code></pre>

<p>I prefer to build embeddings with dynamic max sentence length in each batch.</p>

<p>Here is my embedding function</p>

<pre><code>def add_sentence_embeddings_op(self, word_ids, pos1_ids, pos2_ids, maxlen):
    """"""Defines sentence_embeddings

    If self.config.embeddings is not None and is a np array initialized
    with pre-trained word vectors, the word embeddings is just a look-up
    and we don't train the vectors. Otherwise, a random matrix with
    the correct shape is initialized.
    """"""
    with tf.variable_scope(""words"", reuse=tf.AUTO_REUSE):
        if self.config.embeddings is None:
            self.logger.info(""WARNING: randomly initializing word vectors"")
            _word_embeddings = tf.get_variable(
                    name=""_word_embeddings"",
                    dtype=tf.float32,
                    shape=[self.config.nwords, self.config.dim_word])
        else:
            _word_embeddings = tf.Variable(
                    self.config.embeddings,
                    name=""_word_embeddings"",
                    dtype=tf.float32,
                    trainable=self.config.train_word_embeddings)

        word_embeddings = tf.nn.embedding_lookup(_word_embeddings, \
                word_ids, name=""word_embeddings"")


    with tf.variable_scope(""pos1"", reuse=tf.AUTO_REUSE):
        self.logger.info(""randomly initializing pos1 vectors"")
        _pos1_embeddings = tf.get_variable(
                name=""_pos1_embeddings"",
                dtype=tf.float32,
                shape=[self.config.nposition, self.config.dim_pos])

        pos1_embeddings = tf.nn.embedding_lookup(_pos1_embeddings, \
                pos1_ids, name=""pos1_embeddings"")

    with tf.variable_scope(""pos2"", reuse=tf.AUTO_REUSE):
        self.logger.info(""randomly initializing pos2 vectors"")
        _pos2_embeddings = tf.get_variable(
                name=""_pos2_embeddings"",
                dtype=tf.float32,
                shape=[self.config.nposition, self.config.dim_pos])

        pos2_embeddings = tf.nn.embedding_lookup(_pos2_embeddings, \
                pos2_ids, name=""pos2_embeddings"")

    word_emb_shape = word_embeddings.get_shape().as_list()
    pos1_emb_shape = pos1_embeddings.get_shape().as_list()
    pos2_emb_shape = pos2_embeddings.get_shape().as_list()
    assert word_emb_shape[0] == pos1_emb_shape[0] == pos2_emb_shape[0]
    assert word_emb_shape[1] == pos1_emb_shape[1] == pos2_emb_shape[1]
    assert word_emb_shape[2] == self.config.dim_word
    assert pos1_emb_shape[2] == self.config.dim_pos
    assert pos2_emb_shape[2] == self.config.dim_pos

    sentence_embeddings = tf.concat([word_embeddings, \
        pos1_embeddings, pos2_embeddings], 2)

    sen_emb_shape = sentence_embeddings.get_shape().as_list()
    assert sen_emb_shape[2] == self.config.dim
    # (batch_size, max length of sentences in batch, vector representation dimension, 1)
    sentence_embeddings = tf.reshape(sentence_embeddings, [-1, maxlen, self.config.dim, 1])
    return sentence_embeddings
</code></pre>

<p>Unfortunately, the tf.reshape step doesn't work.</p>

<pre><code>sentence_embeddings = tf.reshape(sentence_embeddings, [-1, maxlen, self.config.dim, 1])
</code></pre>

<p>The Errors while compiling.</p>

<pre><code>Traceback (most recent call last):
  File ""train.py"", line 26, in &lt;module&gt;
    main()
  File ""train.py"", line 12, in main
    model.build()
  File ""/Users/randypen/Code/test_pcnn/model/pcnn_model.py"", line 295, in build
    self.add_concat_op()
  File ""/Users/randypen/Code/test_pcnn/model/pcnn_model.py"", line 243, in add_concat_op
    self.pos1_ids_left, self.pos2_ids_left, self.maxlen_left)
  File ""/Users/randypen/Code/test_pcnn/model/pcnn_model.py"", line 202, in add_sentence_embeddings_op
    sentence_embeddings = tf.reshape(sentence_embeddings, [-1, maxlen, self.config.dim, 1])
  File ""/Users/randypen/.virtualenvs/DataEnv/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 6113, in reshape
    ""Reshape"", tensor=tensor, shape=shape, name=name)
  File ""/Users/randypen/.virtualenvs/DataEnv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 528, in _apply_op_helper
(input_name, err))
ValueError: Tried to convert 'shape' to a tensor and failed. Error: Shapes must be equal rank, but are 1 and 0
    From merging shape 1 with other shapes. for 'Reshape/packed' (op: 'Pack') with input shapes: [], [1], [], [].
</code></pre>

<p>I have see several other related repos, some of them define max length as a constant value.
Is it possible to reshape a batch of tensors with dynamic max length in tensorflow?</p>
",Vectorization & Embeddings,reshape batch tensor dynamic max length tensorflow natural language processing common pad batch sequence padding function input batch sentence variable length sentence list id id represents one word vocabulary feeddict prefer build embeddings dynamic max sentence length batch embedding function unfortunately tf reshape step work error compiling see several related repos define max length constant value possible reshape batch tensor dynamic max length tensorflow
Character embeddings with Keras,"<p>I am trying to implement the type of character level embeddings described in <a href=""https://arxiv.org/pdf/1603.01360.pdf"" rel=""noreferrer"">this paper</a> in Keras. The character embeddings are calculated using a bidirectional LSTM.</p>

<p><a href=""https://i.sstatic.net/PSvdx.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/PSvdx.png"" alt=""enter image description here""></a></p>

<p>To recreate this, I've first created a matrix of containing, for each word, the indexes of the characters making up the word:</p>

<pre><code>char2ind = {char: index for index, char in enumerate(chars)}
max_word_len = max([len(word) for sentence in sentences for word in sentence])
X_char = []
for sentence in X:
    for word in sentence:
        word_chars = []
        for character in word:
            word_chars.append(char2ind[character])

        X_char.append(word_chars)
X_char = sequence.pad_sequences(X_char, maxlen = max_word_len)
</code></pre>

<p>I then define a BiLSTM model with an embedding layer for the word-character matrix. I assume the input_dimension will have to be equal to the number of characters. I want a size of 64 for my character embeddings, so I set the hidden size of the BiLSTM to 32:</p>

<pre><code>char_lstm = Sequential()
char_lstm.add(Embedding(len(char2ind) + 1, 64))    
char_lstm.add(Bidirectional(LSTM(hidden_size, return_sequences=True)))
</code></pre>

<p>And this is where I get confused. How can I retrieve the embeddings from the model? I'm guessing I would have to compile the model and fit it then retrieve the weights to get the embeddings, but what parameters should I use to fit it ?</p>

<hr>

<p>Additional details:</p>

<p>This is for an NER task, so the dataset technically could be be anything in the word - label format, although I am specifically working with the WikiGold ConLL corpus available here: <a href=""https://github.com/pritishuplavikar/Resume-NER/blob/master/wikigold.conll.txt"" rel=""noreferrer"">https://github.com/pritishuplavikar/Resume-NER/blob/master/wikigold.conll.txt</a>
The expected output from the network are the labels (I-MISC, O, I-PER...)</p>

<p>I expect the dataset to be large enough to be training character embeddings directly from it. All words are coded with the index of their constituting characters, alphabet size is roughly 200 characters. The words are padded / cut to 20 characters. There are around 30 000 different words in the dataset.</p>

<p>I hope to be able learn embeddings for each characters based on the info from the different words. Then, as in the paper, I would concatenate the character embeddings with the word's glove embedding before feeding into a Bi-LSTM network with a final CRF layer.</p>

<p>I would also like to be able to save the embeddings so I can reuse them for other similar NLP tasks.</p>
",Vectorization & Embeddings,character embeddings kera trying implement type character level embeddings described paper kera character embeddings calculated using bidirectional lstm recreate first created matrix containing word index character making word define bilstm model embedding layer word character matrix assume input dimension equal number character want size character embeddings set hidden size bilstm get confused retrieve embeddings model guessing would compile model fit retrieve weight get embeddings parameter use fit additional detail ner task dataset technically could anything word label format although specifically working wikigold conll corpus available expected output network label misc per expect dataset large enough training character embeddings directly word coded index constituting character alphabet size roughly character word padded cut character around different word dataset hope able learn embeddings character based info different word paper would concatenate character embeddings word glove embedding feeding bi lstm network final crf layer would also like able save embeddings reuse similar nlp task
How to select sentences similar to my sentence by crawling entire website?,"<p>If I give a sentence how to return all the similar sentences to it?<br>
For example:<br>
How much time will interview take?<br>
The similar sentences should be<br>
1.How long the interview lasts.<br>
2.Duration of the interview.  </p>

<p>How to do this?<br>
One method I am thinking of is crawling the faq pages of 30 to 40 company career websites and embed the questions using doc2vec and I'll keep all the similar vectors in a cluster.  </p>

<p>Is there a better method than this?</p>
",Vectorization & Embeddings,select sentence similar sentence crawling entire website give sentence return similar sentence example much time interview take similar sentence long interview last duration interview one method thinking crawling faq page company career website embed question using doc vec keep similar vector cluster better method
How do you compute the distance between text documents for k-means with word2vec?,"<p>I have recently been introduced to word2vec and I'm having some trouble trying to figure out how exactly it is used for k-means clustering. </p>

<p>I do understand how k-means works with tf-idf vectors. For each text document you have a vector of tf-idf values and after choosing some documents as initial cluster centers, you can use the euclidian distance to minimise the the distances between the vectors of the documents. Here's an <a href=""https://www.experfy.com/blog/k-means-clustering-in-text-data"" rel=""nofollow noreferrer"">example</a>.</p>

<p>However, when using word2vec, each word is represented as a vector. Does this mean that each document corresponds to a matrix? And if so, how do you compute the minimum distance w.r.t. other text documents? </p>

<p><strong>Question:</strong> How do you compute the distance between text documents for k-means with word2vec? </p>

<p><strong>Edit:</strong> To explain my confusion in a bit more detail, please consider the following code:</p>

<pre><code>vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(sentences_tfidf)
print(tfidf_matrix.toarray())

model = Word2Vec(sentences_word2vec, min_count=1)

word2vec_matrix = model[model.wv.vocab]
print(len(word2vec_matrix))
for i in range(0,len(word2vec_matrix)):
    print(X[i])
</code></pre>

<p>It returns the following code:</p>

<pre><code>[[ 0.          0.55459491  0.          0.          0.35399075  0.          0.
   0.          0.          0.          0.          0.          0.          0.437249
   0.35399075  0.35399075  0.35399075  0.        ]
 [ 0.          0.          0.          0.44302215  0.2827753   0.          0.
   0.          0.34928375  0.          0.          0.          0.34928375
   0.          0.2827753   0.5655506   0.2827753   0.        ]
 [ 0.          0.          0.35101741  0.          0.          0.27674616
   0.35101741  0.          0.          0.35101741  0.          0.35101741
   0.27674616  0.27674616  0.44809973  0.          0.          0.27674616]
 [ 0.40531999  0.          0.          0.          0.2587105   0.31955894
   0.          0.40531999  0.31955894  0.          0.40531999  0.          0.
   0.          0.          0.2587105   0.2587105   0.31955894]]
20
[  4.08335682e-03  -4.44161100e-03   3.92342824e-03   3.96498619e-03
   6.99949533e-06  -2.14108804e-04   1.20419310e-03  -1.29191438e-03
   1.64671184e-03   3.41688609e-03  -4.94929403e-03   2.90348311e-03
   4.23802016e-03  -3.01274913e-03  -7.36164337e-04   3.47558968e-03
  -7.02908786e-04   4.73567843e-03  -1.42914290e-03   3.17237526e-03
   9.36070050e-04  -2.23833631e-04  -4.03443904e-04   4.97530040e-04
  -4.82502300e-03   2.42140982e-03  -3.61089432e-03   3.37070058e-04
  -2.09900597e-03  -1.82093668e-03  -4.74618562e-03   2.41499138e-03
  -2.15628324e-03   3.43719614e-03   7.50159554e-04  -2.05973233e-03
   1.92534993e-03   1.96503079e-03  -2.02400610e-03   3.99564439e-03
   4.95056808e-03   1.47033704e-03  -2.80071306e-03   3.59585625e-04
  -2.77896033e-04  -3.21732066e-03   4.36303904e-03  -2.16396619e-03
   2.24438333e-03  -4.50925855e-03  -4.70488053e-03   6.30825118e-04
   3.81869613e-03   3.75767215e-03   5.01064525e-04   1.70175335e-03
  -1.26033701e-04  -7.43318116e-04  -6.74833194e-04  -4.76678275e-03
   1.53754558e-03   2.32421421e-03  -3.23472451e-03  -8.32759659e-04
   4.67014220e-03   5.15853462e-04  -1.15449808e-03  -1.63017167e-03
  -2.73897988e-03  -3.95627553e-03   4.04657237e-03  -1.79282576e-03
  -3.26930732e-03   2.85121426e-03  -2.33304151e-03  -2.01760884e-03
  -3.33597139e-03  -1.19233003e-03  -2.12347694e-03   4.36858647e-03
   2.00414215e-03  -4.23572073e-03   4.98410035e-03   1.79121632e-03
   4.81655030e-03   3.33247939e-03  -3.95260006e-03   1.19335402e-03
   4.61675343e-04   6.09758368e-04  -4.74696746e-03   4.91552567e-03
   1.74517138e-03   2.36604619e-03  -3.06009664e-04   3.62954312e-03
   3.56943789e-03   2.92139384e-03  -4.27138479e-03  -3.51175456e-03]
[ -4.14272398e-03   3.45513038e-03  -1.47538856e-04  -2.02292087e-03
  -2.96578306e-04   1.88684417e-03  -2.63865804e-03   2.69249966e-03
   4.57606697e-03   2.19206396e-03   2.01336667e-03   1.47434452e-03
   1.88332598e-03  -1.14452699e-03  -1.35678309e-03  -2.02636060e-04
  -3.26160830e-03  -3.95368552e-03   1.40415027e-03   2.30542314e-03
  -3.18884710e-03  -4.46776347e-03   3.96415358e-03  -2.07852037e-03
   4.98413946e-03  -6.43568579e-04  -2.53325375e-03   1.30117545e-03
   1.26555841e-03  -8.84680718e-04  -8.34991166e-04  -4.15050285e-03
   4.66807076e-04   1.71844949e-04   1.08140183e-03   4.37910948e-03
  -3.28412466e-03   2.09890743e-04   2.29888223e-03   4.70223464e-03
  -2.31004297e-03  -5.10134443e-04   2.57104915e-03  -2.55978899e-03
  -7.55646848e-04  -1.98197929e-04   1.20443532e-04   4.63618943e-03
   1.13036349e-05   8.16594984e-04  -1.65917678e-03   3.29331891e-03
  -4.97825304e-03  -2.03667139e-03   3.60272871e-03   7.44500838e-04
  -4.40325850e-04   6.38399797e-04  -4.23364760e-03  -4.56386572e-03
   4.77551389e-03   4.74880403e-03   7.06148741e-04  -1.24937459e-03
  -9.50689311e-04  -3.88551364e-03  -4.45985980e-03  -1.15060725e-03
   3.27067473e-03   4.54987818e-03   2.62327422e-03  -2.40981602e-03
   4.55576897e-04   3.19155119e-03  -3.84227419e-03  -1.17610034e-03
  -1.45622855e-03  -4.32460709e-03  -4.12792247e-03  -1.74557802e-03
   4.66075348e-04   3.39668151e-03  -4.00651991e-03   1.41077011e-03
  -7.89384532e-04  -6.56061340e-04   1.14822399e-03   4.12205653e-03
   3.60721885e-03  -3.11746349e-04   1.44255662e-03   3.11965472e-03
  -4.93455213e-03   4.80490318e-03   2.79991422e-03   4.93505970e-03
   3.69034940e-03   4.76422161e-03  -1.25827035e-03  -1.94680784e-03]
                                  ...

[ -3.92252317e-04  -3.66805331e-03   1.52376946e-03  -3.81564132e-05
  -2.57118000e-03  -4.46725264e-03   2.36480637e-03  -4.70252614e-03
  -4.18651942e-03   4.54758806e-03   4.38804098e-04   1.28351408e-03
   3.40470579e-03   1.00038981e-03  -1.06557179e-03   4.67202952e-03
   4.50591929e-03  -2.67829909e-03   2.57702312e-03  -3.65824508e-03
  -4.54068230e-03   2.20785337e-03  -1.00554363e-03   5.14690124e-04
   4.64830594e-03   1.91410910e-03  -4.83837258e-03   6.73376708e-05
  -2.37796479e-03  -4.45193471e-03  -2.60163331e-03   1.51159777e-03
   4.06868104e-03   2.55690538e-04  -2.54662265e-03   2.64597777e-03
  -2.62586889e-03  -2.71554058e-03   5.49281889e-04  -1.38776843e-03
  -2.94354092e-03  -1.13887887e-03   4.59292997e-03  -1.02300232e-03
   2.27600057e-03  -4.88117011e-03   1.95790920e-03   4.64376673e-04
   2.56658648e-03   8.90390365e-04  -1.40368659e-03  -6.40658545e-04
  -3.53228673e-03  -1.30717538e-03  -1.80223631e-03   2.94505036e-03
  -4.82233381e-03  -2.16079340e-03   2.58940039e-03   1.60595961e-03
  -1.22245611e-03  -6.72614493e-04   4.47060820e-03  -4.95934719e-03
   2.70283176e-03   2.93257344e-03   2.13279200e-04   2.59435410e-03
   2.98801321e-03  -2.79974379e-03  -1.49789048e-04  -2.53924704e-03
  -7.83207070e-04   1.18357304e-03  -1.27669750e-03  -4.16665291e-03
   1.40916929e-03   1.63017987e-07   1.36708119e-03  -1.26687710e-05
   1.24729215e-03  -2.50442210e-03  -3.20308795e-03  -1.41550787e-03
  -1.05747324e-03  -3.97984264e-03   2.25877413e-03  -1.28316227e-03
   3.60359484e-03  -1.97929185e-04   3.21712159e-03  -4.96298913e-03
  -1.83640339e-03  -9.90608009e-04  -2.03964626e-03  -4.87274351e-03
   7.24950165e-04   3.85614252e-03  -4.18979349e-03   2.73840013e-03]
</code></pre>

<p>Using tfidf, k-means would be implemented by the lines</p>

<pre><code>kmeans = KMeans(n_clusters = 5)
kmeans.fit(tfidf_matrix)
</code></pre>

<p>Using word2vec, k-means would be implemented by the lines</p>

<pre><code>kmeans = KMeans(n_clusters = 5)
kmeans.fit(word2vec_matrix)
</code></pre>

<p>(Here's an <a href=""http://ai.intelligentonlinetools.com/ml/k-means-clustering-example-word2vec/"" rel=""nofollow noreferrer"">example</a> of k-means with word2vec). So in the first case, k-means gets a matrix with the tf-idf values of each word per document, while in the second case k-means gets a vector for each word. How can k-means cluster the documents in the second case if it just has the word2vec representations?</p>
",Vectorization & Embeddings,compute distance text document k mean word vec recently introduced word vec trouble trying figure exactly used k mean clustering understand k mean work tf idf vector text document vector tf idf value choosing document initial cluster center use euclidian distance minimise distance vector document example however using word vec word represented vector doe mean document corresponds matrix compute minimum distance w r text document question compute distance text document k mean word vec edit explain confusion bit detail please consider following code return following code using tfidf k mean would implemented line using word vec k mean would implemented line example k mean word vec first case k mean get matrix tf idf value word per document second case k mean get vector word k mean cluster document second case ha word vec representation
LSTM Embedding output size and No. of LSTM,"<p>I am not sure why we have only output vector of size 32, while have LSTM 100?</p>

<p>What I am confuse is that if we have only 32 words vector, if fetch into LSTM, 32 LSTM should big enough to hold it?</p>

<pre><code>Model.add(Embedding(5000,32)
Model.add(LSTM(100)) 
</code></pre>
",Vectorization & Embeddings,lstm embedding output size lstm sure output vector size lstm confuse word vector fetch lstm lstm big enough hold
Difference between most_similar and similar_by_vector in gensim word2vec?,"<p>I was confused with the results of most_similar and similar_by_vector from gensim's Word2vecKeyedVectors. They are supposed to calculate cosine similarities in the same way - however:</p>

<p>Running them with one word gives identical results, for example:
model.most_similar(['obama']) and similar_by_vector(model['obama'])</p>

<p>but if I give it an equation:</p>

<pre><code>model.most_similar(positive=['king', 'woman'], negative=['man'])
</code></pre>

<p>gives:</p>

<pre><code>[('queen', 0.7515910863876343), ('monarch', 0.6741327047348022), ('princess', 0.6713887453079224), ('kings', 0.6698989868164062), ('kingdom', 0.5971318483352661), ('royal', 0.5921063423156738), ('uncrowned', 0.5911505818367004), ('prince', 0.5909028053283691), ('lady', 0.5904011130332947), ('monarchs', 0.5884358286857605)]
</code></pre>

<p>while with:</p>

<pre><code>q = model['king'] - model['man'] + model['woman']
model.similar_by_vector(q)
</code></pre>

<p>gives:</p>

<pre><code>[('king', 0.8655095100402832), ('queen', 0.7673765420913696), ('monarch', 0.695580005645752), ('kings', 0.6929547786712646), ('princess', 0.6909604668617249), ('woman', 0.6528975963592529), ('lady', 0.6286187767982483), ('prince', 0.6222133636474609), ('kingdom', 0.6208546161651611), ('royal', 0.6090123653411865)]
</code></pre>

<p>There is a noticable difference in cosine distance of the words queen, monarch... etc. I'm wondering why?</p>

<p>Thanks!</p>
",Vectorization & Embeddings,difference similar similar vector gensim word vec wa confused result similar similar vector gensim word veckeyedvectors supposed calculate cosine similarity way however running one word give identical result example model similar obama similar vector model obama give equation give give noticable difference cosine distance word queen monarch etc wondering thanks
Document similarity in production environment,"<p>We are having n number of documents. Upon submission of new document by user, our goal is to inform him about possible duplication of existing document (just like stackoverflow suggests questions may already have answer).</p>

<p>In our system, new document is uploaded every minute and mostly about the same topic (where there are more chance of duplication).</p>

<p>Our current implementation includes gensim doc2vec model trained on documents (tagged with unique document ids). We infer vector for new document and find most_similar docs (ids) with it. Reason behind choosing doc2vec model is that we wanted to take advantage of semantics to improve results. As far as we know, it does not support online training, so we might have to schedule a cron or something that periodically updates the model. But scheduling cron will be disadvantageous as documents come in a burst. User may upload duplicates while model is not yet trained for new data. Also given huge amount of data, training time will be higher.</p>

<p>So i would like to know how such cases are handled in big companies. Are there any better alternative? or better algorithm for such problem?</p>
",Vectorization & Embeddings,document similarity production environment n number document upon submission new document user goal inform possible duplication existing document like stackoverflow suggests question may already answer system new document uploaded every minute mostly topic chance duplication current implementation includes gensim doc vec model trained document tagged unique document id infer vector new document find similar doc id reason behind choosing doc vec model wanted take advantage semantics improve result far know doe support online training might schedule cron something periodically update model scheduling cron disadvantageous document come burst user may upload duplicate model yet trained new data also given huge amount data training time higher would like know case handled big company better alternative better algorithm problem
Semantic similarity of strings - poor results,"<p>My goal is to create a basic program which semantically compares strings and decides which is more similar (in terms of semantics) to which. For now I did not want to built from scratch a new (doc2vec?) model in <code>NTLK</code> or in <code>SKlearn</code> or in <code>Gensim</code> but I wanted to test the already existing APIs which can do semantic analysis. </p>

<p>Specifically, I chose to test <code>ParallelDots AI API</code> and for this reason I wrote the following program in python:</p>

<pre><code>import  paralleldots

api_key = ""*******************************************""

paralleldots.set_api_key(api_key)

phrase1 = ""I have a swelling on my eyelid""
phrase2 = ""I have a lump on my hand""
phrase3 = ""I have a lump on my lid""

print(phrase1, "" VS "", phrase3, ""\n"")
print(paralleldots.similarity(phrase1, phrase3), ""\n\n"")

print(phrase2, "" VS "", phrase3, ""\n"")
print(paralleldots.similarity(phrase2, phrase3))
</code></pre>

<p>This is the response I get from the API:</p>

<pre><code>I have a swelling on my eyelid  VS  I have a lump on my lid 

{'normalized_score': 1.38954, 'usage': 'By accessing ParallelDots API or using information generated by ParallelDots API, you are agreeing to be bound by the ParallelDots API Terms of Use: http://www.paralleldots.com/terms-and-conditions', 'actual_score': 0.114657, 'code': 200} 


I have a lump on my hand  VS  I have a lump on my lid 

{'normalized_score': 3.183968, 'usage': 'By accessing ParallelDots API or using information generated by ParallelDots API, you are agreeing to be bound by the ParallelDots API Terms of Use: http://www.paralleldots.com/terms-and-conditions', 'actual_score': 0.323857, 'code': 200}
</code></pre>

<p>This response is rather disappointing for me. It is obvious that the phrase   </p>

<blockquote>
  <p>I have a lump on my lid</p>
</blockquote>

<p>is almost semantically identical to the phrase</p>

<blockquote>
  <p>I have a swelling on my eyelid</p>
</blockquote>

<p>and it is also related to the phrase </p>

<blockquote>
  <p>I have a lump on my hand</p>
</blockquote>

<p>as they are referring to lumps but obviously it is not at all as close as to the former one. However, <code>ParallelDots AI API</code> outputs almost the exact opposite results.</p>

<p>If I am right, <code>ParallelDots AI API</code> is one of most popular APIs for semantic analysis along with others such as <code>Dandelion API</code> etc but it fetches so disappointing results. I expected that these APIs were using some rich databases of synonyms. I have also tested <code>Dandelion API</code> with these three phrases but the results are poor too (and actually they are even worse).</p>

<p>What can I fix at my program above to retrieve more reasonable results? </p>

<p>Is there any other faster way to semantically compare strings?</p>
",Vectorization & Embeddings,semantic similarity string poor result goal create basic program semantically compare string decides similar term semantics want built scratch new doc vec model wanted test already existing apis semantic analysis specifically chose test reason wrote following program python response get api response rather disappointing obvious phrase lump lid almost semantically identical phrase swelling eyelid also related phrase lump hand referring lump obviously close former one however output almost exact opposite result right one popular apis semantic analysis along others etc fetch disappointing result expected apis using rich database synonym also tested three phrase result poor actually even worse fix program retrieve reasonable result faster way semantically compare string
Document classification using word vectors,"<p>While I was classifying and clustering the documents written in natural language, I came up with a question ...</p>

<p>As word2vec and glove, and or etc, vectorize the word in distributed spaces, I wonder if there are any method recommended or commonly used for document vectorization <strong>USING word vectors.</strong></p>

<p>For example,</p>

<blockquote>
  <p>Document1: ""If you chase two rabbits, you will lose them both.""</p>
</blockquote>

<p>can be vectorized as,</p>

<blockquote>
  <p>[0.1425, 0.2718, 0.8187, .... , 0.1011]</p>
</blockquote>

<p>I know about the one also known as doc2vec, that this document has n dimensions just like word2vec. But this is 1 x n dimensions and I have been testing around to find out the limits of using doc2vec. </p>

<p>So, I want to know how other people apply the word vectors for applications with steady size. </p>

<p>Just stacking vectors with m words will be formed m x n dimensional vectors. In this case, the vector dimension <strong>will not be uniformed since dimension m will depends on the number of words in document</strong>. </p>

<blockquote>
  <p>If: [0.1018, ... , 0.8717]</p>
  
  <p>you: [0.5182, ... , 0.8981]</p>
  
  <p>..: [...]</p>
  
  <p>m th word: [...]</p>
</blockquote>

<p>And this form is not favorable form to run some machine learning algorithms such as CNN. What are the suggested methods to produce the document vectors in steady form using word vectors?</p>

<p>It would be great if it is provided with papers as well.</p>

<p>Thanks!</p>
",Vectorization & Embeddings,document classification using word vector wa classifying clustering document written natural language came question word vec glove etc vectorize word distributed space wonder method recommended commonly used document vectorization using word vector example document chase two rabbit lose vectorized know one also known doc vec document ha n dimension like word vec x n dimension testing around find limit using doc vec want know people apply word vector application steady size stacking vector word formed x n dimensional vector case vector dimension uniformed since dimension depends number word document th word form favorable form run machine learning algorithm cnn suggested method produce document vector steady form using word vector would great provided paper well thanks
Tensorflow DNNclassifier getting bad results,"<p>I am trying to make a classifier to learn if a movie review was positive or negative from its contents. I have using a couple of files that are relevant, a file of the total vocabulary(one word per line) across every document, two CSVs(one for the training set, one for the testing) containing the score each document got in a specific order, and two CSVs(same as above) where on one line it is the the index of each word that appears in that review looking at the vocab as a list. So for every a review like ""I liked this movie"" have something like a score line of 1(0: dislike, 1 like) and a word line of [2,13,64,33]. I use the DNNClassifier and currently am using 1 feature which is an embedding column wrapped around a categorical_column_with_identity. My code runs but it takes absolutely terrible results and I'm not sure why. Perhaps someone with more knowledge about tensor flow could help me out. Also I don't go on here much but I honestly tried and couldn't find a post that directly helps me.</p>

<pre><code>import tensorflow as tf
import pandas as pd
import numpy as np
import os




embedding_d = 18
label_name = ['Label']
col_name = [""Words""]
hidden_unit = [10]*5
BATCH = 50
STEP = 5000

#Ignore some warning messages but an optional compiler
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

##Function to feed into training
def train_input_fn(features, labels, batch_size):
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))
    # Shuffle, repeat, and batch the examples.
    dataset = dataset.shuffle(1000).repeat().batch(batch_size)
    # Return the dataset.
    return dataset

##Orignal Eval. Untouched so far. Mostly likely will need to be changed.
def eval_input_fn(features, labels, batch_size):
    """"""An input function for evaluation or prediction""""""
    features=dict(features)
    if labels is None:
        # No labels, use only features.
        inputs = features
    else:
        inputs = (features, labels)
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices(inputs)
    # Batch the examples
    assert batch_size is not None, ""batch_size must not be None""
    dataset = dataset.batch(batch_size)
    # Return the dataset.
    return dataset

## Produces dataframe for labels and features(words) using pandas
def loadData():
    train_label =pd.read_csv(""aclImdb/train/yaynay.csv"",names=label_name)
    test_label =pd.read_csv(""aclImdb/test/yaynay.csv"",names=label_name)
    train_feat = pd.read_csv(""aclImdb/train/set.csv"", names = col_name)
    test_feat = pd.read_csv(""aclImdb/test/set.csv"", names =  col_name)
    train_feat[col_name] =train_feat[col_name].astype(np.int64)
    test_feat[col_name] =test_feat[col_name].astype(np.int64)
    return (train_feat,train_label),(test_feat,test_label)

## Stuff that I believe is somewhat working
# Get labels for test and training data
(train_x,train_y), (test_x,test_y) = loadData()

## Get the features for each document
train_feature = []
#Currently only one key but this could change in the future
for key in train_x.keys():
    #Create a categorical_column column
    idCol = tf.feature_column.categorical_column_with_identity(
        key= key,
        num_buckets=89528)
embedding_column = tf.feature_column.embedding_column(
    categorical_column= idCol,
    dimension=embedding_d)
train_feature.append(embedding_column)

##Create the neural network
classifier = tf.estimator.DNNClassifier(
    feature_columns=train_feature,
    # Species no. of layers and no. of neurons in each layer
    hidden_units=hidden_unit,
    # Number of output options(here there are 11 for scores 0-10 inclusive)
    n_classes= 2)

    # Train the Model
    #First numerical value is batch size, second is total steps to take.
classifier.train(input_fn= lambda: train_input_fn(train_x, train_y, BATCH),steps=STEP)


#Evaluate the model
eval_result = classifier.evaluate(
    input_fn=lambda:eval_input_fn(test_x, test_y,
                                                BATCH), steps = STEP)
print('\nTest set accuracy: {accuracy:0.3f}\n'.format(**eval_result))
</code></pre>
",Vectorization & Embeddings,tensorflow dnnclassifier getting bad result trying make classifier learn movie review wa positive negative content using couple file relevant file total vocabulary one word per line across every document two csvs one training set one testing containing score document got specific order two csvs one line index word appears review looking vocab list every review like liked movie something like score line dislike like word line use dnnclassifier currently using feature embedding column wrapped around categorical column identity code run take absolutely terrible result sure perhaps someone knowledge tensor flow could help also go much honestly tried find post directly help
Why do we do one-hot encoding in language modelling?,"<p>Could anyone explain why one-hot encoding is good in language modelling? (exspecially in deep learning)</p>

<p>(I know that other encoding methods such as word2vec is better, I just want to know at the root why one-hot encoding works).</p>

<p>Thank you in advance!</p>
",Vectorization & Embeddings,one hot encoding language modelling could anyone explain one hot encoding good language modelling exspecially deep learning know encoding method word vec better want know root one hot encoding work thank advance
embedding word positions in keras,"<p>I am trying to build a relation extraction system for drug-drug interactions using a CNN and need to make embeddings for the words in my sentences. The plan is to represent each word in the sentences as a combination of 3 embeddings: (w2v,dist1,dist2) where w2v is a pretrained word2vec embedding and dist1 and dist2 are the relative distances between each word in the sentence and the two drugs that are possibly related.</p>

<p>I am confused about how I should approach the issue of padding so that every sentence is of equal length. Should I pad the tokenised sentences with some series of strings(what string?) to equalise their lengths before any embedding?</p>
",Vectorization & Embeddings,embedding word position kera trying build relation extraction system drug drug interaction using cnn need make embeddings word sentence plan represent word sentence combination embeddings w v dist dist w v pretrained word vec embedding dist dist relative distance word sentence two drug possibly related confused approach issue padding every sentence equal length pad tokenised sentence series string string equalise length embedding
Pytorch seq2seq learning - using word2vec,"<p>I am following a seq2seq tutorial <a href=""http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"" rel=""nofollow noreferrer"">here</a>. </p>

<p>I want to use pretrained vectors. I have edited the code to get the vector of the word rather than index. Following is the code:</p>

<pre class=""lang-py prettyprint-override""><code>#This piece of code loads the vectors from a json file {'word':[vector]..}
class Lang:
    def __init__(self, name, savedVectorsFile):
        def getSavedVectors(filename):
            import json
            word2vec = {}
            with open(filename) as json_data:
                word2vec = json.load(json_data)
            return word2vec

        self.name = name
        self.word2vector = {}
        self.word2count = {}
        self.index2word = {0: ""SOS"", 1: ""EOS""}
        self.n_words = 2  # Count SOS and EOS

        self.get_saved_vector = getSavedVectors(savedVectorsFile)
        self.word2vector['unknown'] = self.get_saved_vector['unknown']

    def addSentence(self, sentence):
        for word in sentence.split(' '):
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2vector:
            self.word2vector[word] = self.get_saved_vector[word]
            self.word2count[word] = 1
            self.index2word[self.n_words] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1




# this piece deals with returning a vector of given word, all vectors are just concatenated into one giant vector
def vectorFromSentence(lang, sentence):
    vectors = []
    for word in sentence.split(' '):
        if word not in lang.word2vector:
            vectors += (lang.word2vector[""unknown""])
        else:
            vectors += (lang.word2vector[word])
    return vectors



# in the train method, I am passing  a vector instead of index to the encoder
    for ei in range(0, input_length, VEC_SIZE):
        encoder_output, encoder_hidden = encoder(input_variable[ei*VEC_SIZE:(ei+1)*VEC_SIZE], encoder_hidden)
        encoder_outputs[ei] = encoder_output[0][0]
</code></pre>

<p>Now, I cannot figure out how should I change my encoder to incorporate the vectors instead of an index. This is my encoder as of now:</p>

<pre class=""lang-py prettyprint-override""><code>def __init__(self, input_size, hidden_size, n_layers=1):
    super(EncoderRNN, self).__init__()
    self.n_layers = n_layers
    self.hidden_size = hidden_size

    self.embedding = nn.Embedding(input_size, hidden_size)
    self.gru = nn.GRU(hidden_size, hidden_size)
</code></pre>

<p>I get this error that </p>

<pre class=""lang-py prettyprint-override""><code>TypeError: torch.index_select received an invalid combination of arguments - got (torch.cuda.FloatTensor, int, torch.cuda.FloatTensor), but expected (torch.cuda.FloatTensor source, int dim, torch.cuda.LongTensor index)
</code></pre>

<p>I also tried with using VEC_SIZE instead of input_size, but to no avail. </p>

<p>Following is the trace:</p>

<pre class=""lang-py prettyprint-override""><code>Traceback (most recent call last):
  File ""scapula_generation_pretrained_vectors.py"", line 710, in &lt;module&gt;
    trainIters(encoder1, attn_decoder1, 50000, print_every=1000)
  File ""scapula_generation_pretrained_vectors.py"", line 566, in trainIters
    decoder, encoder_optimizer, decoder_optimizer, criterion)
  File ""scapula_generation_pretrained_vectors.py"", line 472, in train
    encoder_output, encoder_hidden = encoder(input_variable[ei*VEC_SIZE:(ei+1)*VEC_SIZE], encoder_hidden)
  File ""/home/sagar/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File ""scapula_generation_pretrained_vectors.py"", line 214, in forward
    embedded = self.embedding(input).view(1, 1, -1)
  File ""/home/sagar/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/sagar/anaconda3/lib/python3.6/site-packages/torch/nn/modules/sparse.py"", line 94, in forward
    self.scale_grad_by_freq, self.sparse
  File ""/home/sagar/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py"", line 53, in forward
    output = torch.index_select(weight, 0, indices.view(-1))
</code></pre>

<p>How to write encoder and decoder to occupy the word2vec embeddings? Several things are involved, like does my output of encoder <code>gru</code> has to be a vector of size <code>VEC_SIZE</code> or not, in the decoder - my loss has to be calculated using some similarity metric. I think I will go for cosine similarity, but before that i will have to make sure that decoder takes the output of encoder and generates a vector of size <code>VEC_SIZE</code>.</p>

<p>I would appreciate if someone has already done this homework in the tutorial and has a code readily available?</p>
",Vectorization & Embeddings,pytorch seq seq learning using word vec following seq seq tutorial want use pretrained vector edited code get vector word rather index following code figure change encoder incorporate vector instead index encoder get error also tried using vec size instead input size avail following trace write encoder decoder occupy word vec embeddings several thing involved like doe output encoder ha vector size decoder loss ha calculated using similarity metric think go cosine similarity make sure decoder take output encoder generates vector size would appreciate someone ha already done homework tutorial ha code readily available
Method of vectors in various vector length to fixed length (NLP),"<p>Recently I have been looking around about Natural Language Processing and its vectorization method and advantages of each vectorizer.</p>

<p>I am into character to vectorize, but it seems like the most concerns about the character vectorizer for each word is the embedding to have fixed length.</p>

<p>I do not want to just embed them with 0, which is well known as 0 padding, for instance, the target fixed length is 100 and 72 characters only exists then all 28 of 0 will be padded at the end.</p>

<blockquote>
  <p>""The example of paragraphs and phrases.... ... in vectorizer form"" &lt; with length 72</p>
</blockquote>

<p>becomes</p>

<blockquote>
  <p>[0, 25, 60, 12, 24, 0, 19, 99, 7, 32, 47, 11, 19, 43, 18, 19, 6, 25,
  43, 99, 0, 32, 40, 14, 20, 5, 37, 47, 99, 11, 29, 7, 19, 47, 18, 20,
  60, 18, 19, 2, 19, 11, 31, 130, 130, 76, 0, 32, 40, 14, 20, 7, 19, 47,
  18, 20, 60, 11, 37, 43, 99, 11, 29, 99, 17, 39, 47, 11, 31, 18, 19,
  43, 0, 19, 77, 0, 0, 0, 0, 0, 0, 0, 0, ...., 0, 0, 0, 0, 0, 0]</p>
</blockquote>

<p>.</p>

<p>.</p>

<p>I want to make the vectors be in a <strong>fair distribution form in N fixed dimensions</strong>, <strong>not like the one above</strong></p>

<p>If you know any papers or algorithms preferring consider this matter, or common way to produce a fixed length vectors from various length of vectors please share .</p>

<p>.</p>

<p>.</p>

<p>Further information added as gojomo requested;</p>

<p>I am trying to get the character level vectors for words in corpus.</p>

<p>Let say, in above example, ""The example of paragraphs...."" starts with</p>

<blockquote>
  <p>T [40] </p>
  
  <p>h [17]</p>
  
  <p>e [3]</p>
  
  <p>e [3] </p>
  
  <p>x [53]</p>
  
  <p>a [1]</p>
  
  <p>m [21]</p>
  
  <p>p [25]</p>
  
  <p>l [14]</p>
  
  <p>e [3]</p>
</blockquote>

<p>Notice that each character has its own number (etc, could be ascii) and word represents the vectors of character vectors combination, for example,</p>

<blockquote>
  <p>The [40, 17, 3]</p>
  
  <p>example [3, 53, 1, 21, 25, 14, 3]</p>
</blockquote>

<p>which the vectors are not in same dimension. With the case mention above, many people are padding 0 at the end to make it in uniform size</p>

<p>For example, if someone wants to make the dimension of each word to be 300, then 297 of 0s will be padded to letter ""The"" and 293 of 0s will be padded to ""example""., like </p>

<blockquote>
  <p>The [40, 17, 3, 0, 0, 0, 0, 0, ...., 0]</p>
  
  <p>example [3, 53, 1, 21, 25, 14, 3, 0, 0, 0, 0, 0, ...., 0]</p>
</blockquote>

<p>Now I do not think this padding method is appropriate to my experiments so I want to know if there are any methods to convert its vectors to in uniform form with not sparsed form(if this term is allowed).</p>

<p>Even with the phrase with two words, ""The example"" only takes 11 characters long , still not long enough either.</p>

<p>Whatever the case is that, I would like to know if there are some well known techniques to convert the informal length of vectors to some fixed length.</p>

<p>Thank you !</p>
",Vectorization & Embeddings,method vector various vector length fixed length nlp recently looking around natural language processing vectorization method advantage vectorizer character vectorize seems like concern character vectorizer word embedding fixed length want embed well known padding instance target fixed length character exists padded end example paragraph phrase vectorizer form length becomes want make vector fair distribution form n fixed dimension like one know paper algorithm preferring consider matter common way produce fixed length vector various length vector please share information added gojomo requested trying get character level vector word corpus let say example example paragraph start h e e x p l e notice character ha number etc could ascii word represents vector character vector combination example example vector dimension case mention many people padding end make uniform size example someone want make dimension word padded letter padded example like example think padding method appropriate experiment want know method convert vector uniform form sparsed form term allowed even phrase two word example take character long still long enough either whatever case would like know well known technique convert informal length vector fixed length thank
How are vectors calculated in doc2vec and what does the size parameter depict?,"<p>If I pass a Sentence containing 5 words to the Doc2Vec model and if the size is 100, there are 100 vectors. I'm not getting what are those vectors. If I increase the size to 200, there are 200 vectors for just a simple sentence. Please tell me how are those vectors calculated.</p>
",Vectorization & Embeddings,vector calculated doc vec doe size parameter depict pas sentence containing word doc vec model size vector getting vector increase size vector simple sentence please tell vector calculated
CNN architecture for word/character n-grams,"<p>I've got a task of sequence labeling, and I's like to build a CNN which would take an input of a fixed number of embeddings (character or word-based) and extract n-gram-like features via convolution/pooling.</p>

<p>I haven't previously used convolution (for text or otherwise), so I'm not sure which architecture makes more sense in this setup:</p>

<ul>
<li>Conv1D/MaxPool1D - extracting n-grams at the Conv stage makes sense, but what does such pooling produce? Is it just 1 dimension with the max value of the embedding?</li>
<li>Conv2D/MaxPool2D - athough I saw it more frequently in the existing approaches, the fact of convolving along the token embedding's dimensions doesn't make sense to me.</li>
</ul>

<p>Could you please share your intuition on that?</p>
",Vectorization & Embeddings,cnn architecture word character n gram got task sequence labeling like build cnn would take input fixed number embeddings character word based extract n gram like feature via convolution pooling previously used convolution text otherwise sure architecture make sense setup conv maxpool extracting n gram conv stage make sense doe pooling produce dimension max value embedding conv maxpool athough saw frequently existing approach fact convolving along token embedding dimension make sense could please share intuition
Fasttext algorithm use only word and subword? or sentences too?,"<p>I read the paper and googled as well if there is any good example of the learning method(or more likely learning procedure)</p>

<p>For word2vec, suppose there is corpus sentence</p>

<blockquote>
  <p>I go to school with lunch box that my mother wrapped every morning</p>
</blockquote>

<p>Then with window size 2, it will try to obtain the vector for 'school' by using surrounding words </p>

<blockquote>
  <p>['go', 'to', 'with', 'lunch']</p>
</blockquote>

<p>Now, FastText says that it uses the subword to obtain the vector, so it is definitely use n gram subword, for example with n=3,</p>

<blockquote>
  <p>['sc', 'sch', 'cho', 'hoo', 'ool', 'school']</p>
</blockquote>

<p>Up to here, I understood.
But it is not clear that if the other words are being used for learning for 'school'. I can only guess that other surrounding words are used as well like the word2vec, since the paper mentions</p>

<p>=> the terms <em>Wc</em> and <em>Wt</em> are both used in functions</p>

<p>where Wc is context word and Wt is word at sequence t.</p>

<p>However, it is not clear that how FastText learns the vectors for word.</p>

<p>.</p>

<p>.</p>

<p>Please clearly explain how FastText learning process goes in procedure?</p>

<p>.</p>

<p>.</p>

<p>More precisely I want to know that if FastText also follows the same procedure as Word2Vec while it learns the n-gram characterized subword <strong><em>in addition</em></strong>. Or only n-gram characterized subword with word being used?</p>

<p>How does it vectorize the subword at initial? etc</p>
",Vectorization & Embeddings,fasttext algorithm use word subword sentence read paper googled well good example learning method likely learning procedure word vec suppose corpus sentence go school lunch box mother wrapped every morning window size try obtain vector school using surrounding word go lunch fasttext say us subword obtain vector definitely use n gram subword example n sc sch cho hoo ool school understood clear word used learning school guess surrounding word used well like word vec since paper mention term wc wt used function wc context word wt word sequence however clear fasttext learns vector word please clearly explain fasttext learning process go procedure precisely want know fasttext also follows procedure word vec learns n gram characterized subword addition n gram characterized subword word used doe vectorize subword initial etc
Document similarity in Spacy vs Word2Vec,"<p>I have a niche corpus of ~12k docs, and I want to test near-duplicate documents with similar meanings across it - think article about the same event covered by different news organisations. </p>

<p>I have tried gensim's Word2Vec, which gives me terrible similarity score(&lt;0.3) even when the test document is <em>within</em> the corpus, and I have tried SpaCy, which gives me >5k documents with similarity > 0.9. I tested SpaCy's most similar documents, and it was mostly useless.</p>

<p>This is the relevant code.                                                              </p>

<pre><code>tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=40)
doc = preprocess(query)
vec_bow = dictionary.doc2bow(doc)
vec_lsi_tfidf = lsi[tfidf[vec_bow]] # convert the query to LSI space
index = similarities.Similarity(corpus = corpus, num_features = len(dictionary), output_prefix = ""pqr"")
sims = index[vec_lsi_tfidf] # perform a similarity query against the corpus
most_similar = sorted(list(enumerate(sims)), key = lambda x:x[1])

for mid in most_similar[-100:]:
    print(mid, file_list[mid[0]])
</code></pre>

<p>Using gensim I have found a decent approach, with some preprocessing, but the similarity score is still quite low. Has anyone faced such a problem, and are there are some resources or suggestions that could be useful?</p>
",Vectorization & Embeddings,document similarity spacy v word vec niche corpus k doc want test near duplicate document similar meaning across think article event covered different news organisation tried gensim word vec give terrible similarity score even test document within corpus tried spacy give k document similarity tested spacy similar document wa mostly useless relevant code using gensim found decent approach preprocessing similarity score still quite low ha anyone faced problem resource suggestion could useful
Regarding Text Autoencoders in KERAS for topic modeling,"<p><strong>Introduction:</strong> I have trained Autoencoders (vanilla &amp; variational) in KERAS for MNIST images, and have observed how good the latent representation in the bottleneck layer looks for clustering them together.</p>

<p><strong>Objective:</strong> I want to do the same for short texts. Tweets specifically! I want to cluster them together based on their semantics using <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">pre-trained GloVe embeddings</a>.</p>

<p>What I am planning to do is create a CNN encoder and a CNN decoder as a start, before moving on to LSTMs/GRUs.</p>

<p><strong>Problem:</strong> 
~~~<em>What should be the correct loss? How do I implement it in Keras?</em>~~~ </p>

<p>This is how my KERAS model looks like</p>

<p>INPUT_TWEET (Word indexes) >> EMBEDDING LAYER >> CNN_ENCODER >> BOTTLENECK >> CNN_DECODER >> OUTPUT_TWEET (Word indexes)</p>

<pre><code>Layer (type)                 Output Shape              Param #   
-----------------------------------------------------------------
Input_Layer (InputLayer)     (None, 64)                0         
embedding_1 (Embedding)      (None, 64, 200)           3299400   
enc_DO_0_layer (Dropout)     (None, 64, 200)           0         
enc_C_1 (Conv1D)             (None, 64, 16)            9616      
enc_MP_1 (MaxPooling1D)      (None, 32, 16)            0         
enc_C_2 (Conv1D)             (None, 32, 8)             392       
enc_MP_2 (MaxPooling1D)      (None, 16, 8)             0         
enc_C_3 (Conv1D)             (None, 16, 8)             200       
enc_MP_3 (MaxPooling1D)      (None, 8, 8)              0         
***bottleneck (Flatten)***   (None, 64)                0         
reshape_2 (Reshape)          (None, 8, 8)              0         
dec_C_1 (Conv1D)             (None, 8, 8)              200       
dec_UpS_1 (UpSampling1D)     (None, 16, 8)             0         
dec_C_2 (Conv1D)             (None, 16, 8)             200       
dec_UpS_2 (UpSampling1D)     (None, 32, 8)             0         
dec_C_3 (Conv1D)             (None, 32, 16)            400       
dec_UpS_3 (UpSampling1D)     (None, 64, 16)            0         
conv1d_2 (Conv1D)            (None, 64, 200)           9800      
dense_2 (Dense)              (None, 64, 1)             201       
flatten_2 (Flatten)          (None, 64)                0         
-----------------------------------------------------------------
</code></pre>

<p>This is clearly wrong because it tries to minimize the MSE loss between the Input and the Output (word indexes), where I think it should do it in the embedding layers (embedding_1 and conv1d_2).</p>

<p>Now how do I do it? Does it make sense? Is there a way to do this in Keras? Please check my code below:</p>

<p><strong>The code:</strong></p>

<pre><code>sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',name=""Input_Layer"")
embedded_sequences = embedding_layer(sequence_input)
embedded_sequences1 = Dropout(0.5, name=""enc_DO_0_layer"")(embedded_sequences)

x = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same',name=""enc_C_1"")(embedded_sequences1)
x = MaxPooling1D(pool_size=2, padding='same',name='enc_MP_1')(x)
x = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same',name=""enc_C_2"")(x)
x = MaxPooling1D(pool_size=2, padding='same',name=""enc_MP_2"")(x)
x = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same',name=""enc_C_3"")(x)
x = MaxPooling1D(pool_size=2, padding='same',name=""enc_MP_3"")(x)

encoded = Flatten(name=""bottleneck"")(x)
x = Reshape((8, 8))(encoded)

x = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same',name=""dec_C_1"")(x)
x = UpSampling1D(2,name=""dec_UpS_1"")(x)
x = Conv1D(8, 3, activation='relu', padding='same',name=""dec_C_2"")(x)
x = UpSampling1D(2,name=""dec_UpS_2"")(x)
x = Conv1D(16, 3, activation='relu',padding='same',name=""dec_C_3"")(x)
x = UpSampling1D(2,name=""dec_UpS_3"")(x)
decoded = Conv1D(200, 3, activation='relu', padding='same')(x)
y = Dense(1)(decoded)
y = Flatten()(y)

autoencoder = Model(sequence_input, y)
autoencoder.compile(optimizer='adam', loss='mean_squared_error')

autoencoder.fit(x = tweet_word_indexes ,y = tweet_word_indexes,
            epochs=10,
            batch_size=128,
            validation_split=0.2)
</code></pre>

<p><strong>Dont want it to do this:</strong></p>

<p>It is obviously just trying to reconstruct the array of word indexes (zero padded) because of the bad loss.</p>

<pre><code>Input  = [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1641 13 2 309 932 1 10 5 6]  
Output = [ -0.31552997 -0.53272009 -0.60824025 -1.14802313 -1.14597917 -1.08642125 -1.10040164 -1.19442761 -1.19560885 -1.19008029 -1.19456315 -1.2288748 -1.22721946 -1.20107424 -1.20624077 -1.24017036 -1.24014354 -1.2400831 -1.24004364 -1.23963416 -1.23968709 -1.24039733 -1.24027216 -1.23946059 -1.23946059 -1.23946059 -1.23946059 -1.23946059 -1.23946059 -1.23946059 -1.23946059 -1.23946059 -1.23946059 -1.14516866 -1.20557368 -1.5288837 -1.48179781 -1.05906188 -1.17691648 -1.94568193 -1.85741842 -1.30418646 -0.83358657 -1.61638248 -1.17812908 0.53077424 0.79578459 -0.40937367 0.35088596 1.29912627 -5.49394751 -27.1003418 -1.06875408 33.78763962 109.41391754 242.43798828 251.05577087 300.13430786 267.90420532 178.17596436 132.06596375 60.63394928 82.10819244 91.18526459]
</code></pre>

<p><strong>Question:</strong>
<em>Does it make sense to you? What should be the correct loss? How do I implement it in Keras?</em></p>
",Vectorization & Embeddings,regarding text autoencoders kera topic modeling introduction trained autoencoders vanilla variational kera mnist image observed good latent representation bottleneck layer look clustering together objective want short text tweet specifically want cluster together based semantics using pre trained glove embeddings planning create cnn encoder cnn decoder start moving lstms grus problem correct loss implement kera kera model look like input tweet word index embedding layer cnn encoder bottleneck cnn decoder output tweet word index clearly wrong try minimize mse loss input output word index think embedding layer embedding conv doe make sense way kera please check code code dont want obviously trying reconstruct array word index zero padded bad loss question doe make sense correct loss implement kera
Compute gradients w.r.t. the values of embedding vectors in PyTorch,"<p>I am trying to train a dual encoder LSTM model for a chatbot using PyTorch.</p>

<p>I defined two classes: the Encoder class defines the LSTM itself and the Dual_Encoder class applies the Encoder to both context and response utterances that I am trying to train on: </p>

<pre><code>class Encoder(nn.Module):

    def __init__(self, 
                 input_size, 
                 hidden_size, 
                 vocab_size, 
                 num_layers = 1, 
                 num_directions = 1, 
                 dropout = 0, 
                 bidirectional = False,
                 rnn_type = 'lstm'): 

                 super(Encoder, self).__init__()

                 self.input_size = input_size
                 self.hidden_size = hidden_size
                 self.vocab_size = vocab_size
                 self.num_layers = 1
                 self.num_directions = 1
                 self.dropout = 0,
                 self.bidirectional = False

                 self.embedding = nn.Embedding(vocab_size, input_size, sparse = False, padding_idx = 0)
                 self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=False, dropout = dropout, bidirectional=False).cuda()

                 self.init_weights()

    def init_weights(self):
        init.orthogonal(self.lstm.weight_ih_l0)

        init.uniform(self.lstm.weight_hh_l0, a=-0.01, b=0.01)

        embedding_weights = torch.FloatTensor(self.vocab_size, self.input_size).cuda()
        init.uniform(embedding_weights, a = -0.25, b= 0.25)

        id_to_vec, emb_dim = create_id_to_vec('/data/train_shuffled_onethousand.csv','/data/glove.6B.100d.txt')

        for id, vec in id_to_vec.items():
            embedding_weights[id] = vec

        del self.embedding.weight
        self.embedding.weight = nn.Parameter(embedding_weights)
        self.embedding.weight.requires_grad = True

        #self.embedding.weight.data.copy_(torch.from_numpy(self.embedding_weights))

    def forward(self, inputs):
        embeddings = self.embedding(inputs)
        outputs, hiddens = self.lstm(embeddings)
        return outputs, hiddens

#%%

class DualEncoder(nn.Module):

    def __init__(self, encoder):
         super(DualEncoder, self).__init__()
         self.encoder = encoder
         self.number_of_layers = 1
         #h_0 (num_layers * num_directions, batch, hidden_size): 
         #tensor containing the initial hidden state for each element in the batch.
         #dual_hidden_size = self.encoder.hidden_size * self.encoder.num_directions

         M = torch.FloatTensor(self.encoder.hidden_size, self.encoder.hidden_size).cuda()

         init.normal(M)

         self.M = nn.Parameter(M, requires_grad = True)

    def forward(self, contexts, responses):
        #output (seq_len, batch, hidden_size * num_directions): 
        #tensor containing the output features (h_t) from the last layer 
        #of the RNN, for each t. 

        #h_n (num_layers * num_directions, batch, hidden_size): 
        #tensor containing the hidden state for t=seq_len
        context_out, context_hn = self.encoder(contexts)

        response_out, response_hn = self.encoder(responses)

        scores_list = []

        y_preds = None

        for e in range(999): 
            context_h = context_out[e][-1].view(1, self.encoder.hidden_size)
            response_h = response_out[e][-1].view(self.encoder.hidden_size,1)


            dot_var = torch.mm(torch.mm(context_h, self.M), response_h)[0][0]

            dot_tensor = dot_var.data
            dot_tensor.cuda()

            score = torch.sigmoid(dot_tensor)
            scores_list.append(score)

        y_preds_tensor = torch.stack(scores_list).cuda()  
        y_preds = autograd.Variable(y_preds_tensor).cuda()

        return y_preds 

#%% TRAINING

torch.backends.cudnn.enabled = False
#%%
vocab = create_vocab('/data/train_shuffled_onethousand.csv')
vocab_len = len(vocab)
emb_dim = get_emb_dim('/data/glove.6B.100d.txt')
#%%

encoder_model = Encoder(
        input_size = emb_dim,
        hidden_size = 300,
        vocab_size = vocab_len)

encoder_model.cuda()
#%%
dual_encoder = DualEncoder(encoder_model)

dual_encoder.cuda()
#%%
loss_func = torch.nn.BCELoss()

loss_func.cuda()

learning_rate = 0.001
epochs = 100
#batch_size = 50

optimizer = optim.Adam(dual_encoder.parameters(),
                       lr = learning_rate)
#%%
for i in range(epochs):

    context_matrix, response_matrix, y = make_matrices('/data/train_shuffled_onethousand.csv')

    context_matrix = autograd.Variable(context_matrix, requires_grad=True).cuda()

    response_matrix = autograd.Variable(response_matrix, requires_grad=True).cuda()

    y_label = y.cuda()

    y_preds = dual_encoder(context_matrix, response_matrix)

    loss = loss_func(y_preds, y_label)

    if i % 10 == 0:
        print(""Epoch: "", i, "", Loss: "", loss.data[0])

    #evaluation metrics...

    dual_encoder.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm(dual_encoder.parameters(), 10)

    optimizer.step()
</code></pre>

<p>The following error occurs:</p>

<pre><code>2018-01-06 06:07:02,148 INFO - result = self.forward(*input, **kwargs)
2018-01-06 06:07:02,148 INFO - File ""all_scripts.py"", line 258, in forward
2018-01-06 06:07:02,148 INFO - context_out, context_hn = self.encoder(contexts)
2018-01-06 06:07:02,149 INFO - File ""/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 325, in __call__
2018-01-06 06:07:02,149 INFO - result = self.forward(*input, **kwargs)
2018-01-06 06:07:02,149 INFO - File ""all_scripts.py"", line 229, in forward
2018-01-06 06:07:02,149 INFO - embeddings = self.embedding(inputs)
2018-01-06 06:07:02,150 INFO - File ""/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 325, in __call__
2018-01-06 06:07:02,150 INFO - result = self.forward(*input, **kwargs)
2018-01-06 06:07:02,150 INFO - File ""/usr/local/lib/python3.6/site-packages/torch/nn/modules/sparse.py"", line 103, in forward
2018-01-06 06:07:02,150 INFO - self.scale_grad_by_freq, self.sparse
2018-01-06 06:07:02,150 INFO - File ""/usr/local/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py"", line 40, in forward
2018-01-06 06:07:02,151 INFO - assert not ctx.needs_input_grad[0], ""Embedding doesn't "" \
2018-01-06 06:07:02,151 INFO - AssertionError: Embedding doesn't compute the gradient w.r.t. the indices
</code></pre>

<p>I do understand why the problem occurs (surely it makes no sense to compute the gradient w.r.t. the indices).
But I do not understand how to adjust the code so that it computes the gradients w.r.t. the content values of the embedding vectors.</p>

<p>All help highly appreciated!</p>

<p>(Also see the <a href=""https://discuss.pytorch.org/t/embedding-returns-an-error-that-i-dont-understand/1072/5"" rel=""nofollow noreferrer"">thread in the PyTorch</a> forum)</p>
",Vectorization & Embeddings,compute gradient w r value embedding vector pytorch trying train dual encoder lstm model chatbot using pytorch defined two class encoder class defines lstm dual encoder class applies encoder context response utterance trying train following error occurs understand problem occurs surely make sense compute gradient w r index understand adjust code computes gradient w r content value embedding vector help highly appreciated also see thread pytorch forum
How to build word embedding model using Tflearn?,"<blockquote>
<p><em><strong>Updated</strong></em></p>
</blockquote>
<p>I am working on the word embedding model for answer Matching score prediction using Tflearn. I have to build a model using sentence vector using tflearn dnn classifier,  Now I have to add a word embedding layer to the dnn model. How to do that? Thanks in advance.</p>
<blockquote>
<p>&quot;JVMdefines&quot;: enables a computer to run a Java program</p>
</blockquote>
<p>is coverted as :</p>
<blockquote>
<p>&quot;JVMdefines&quot;: [[list([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])</p>
<p>enables a computer to run a Java program :
list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]</p>
</blockquote>
<p>My question:  Is there any method that the machine can able to analyze.</p>
<blockquote>
<p>enables a &quot;machine&quot; to run a Java program</p>
</blockquote>
<p>That is It can detect computer and machine as in same meaning.</p>
",Vectorization & Embeddings,build word embedding model using tflearn updated working word embedding model answer matching score prediction using tflearn build model using sentence vector using tflearn dnn classifier add word embedding layer dnn model thanks advance jvmdefines enables computer run java program coverted jvmdefines list enables computer run java program list question method machine able analyze enables machine run java program detect computer machine meaning
Compare bigrams and trigrams from same text,"<p>I have 2 list of normalised bigrams and trigrams from same text.
What I need is a list of trigrams, first two words of which contains a bigram from the same text. For example <code>['spam eggs blabla']</code> should give a match with <code>[spam eggs]</code>. Any ideas?</p>
",Vectorization & Embeddings,compare bigram trigram text list normalised bigram trigram text need list trigram first two word contains bigram text example give match idea
DL4J: How to calculate Cosine Similarity between INDArray obtained from getWordVectorsMean,"<p>I have calculated VectorMean of two sentences like this:</p>

<pre><code>String demoString1 = ""Enter first label"";
String demoString2 = ""Enter first name"";
        Collection&lt;String&gt; label1 = Splitter.on(' ').splitToList(demoString1);
        Collection&lt;String&gt; label2 = Splitter.on(' ').splitToList(demoString2);

        System.out.println(""label1:==&gt;""+label1);
        System.out.println(""getWordVectorMatrix-&gt;INDArray------------------""+vectors.getWordVectorsMean(label1));

        System.out.println(""label2:==&gt;""+label2);
        System.out.println(""getWordVectorMatrix-&gt;INDArray------------------""+vectors.getWordVectorsMean(label2));
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>label1:==&gt;[Enter, first, label]
getWordVectorMatrix-&gt;INDArray------------------[0.02,  -0.14,  0.07,  -0.10,.............100 dimension vector]
label2:==&gt;[Enter, first, name]
getWordVectorMatrix-&gt;INDArray------------------[-0.00,  -0.15,  0.07,  -0.13,............100 dimension vector]
</code></pre>

<p>Now how I can calculate similarity(Cosine Similarity) between both sentences using their mean ?
I searched, but I couldn't find any API available in DL4J.</p>
",Vectorization & Embeddings,dl j calculate cosine similarity indarray obtained getwordvectorsmean calculated vectormean two sentence like output calculate similarity cosine similarity sentence using mean searched find api available dl j
Gensim Doc2Vec most_similar() method not working as expected,"<p>I am struggling with Doc2Vec and I cannot see what I am doing wrong.
I have a text file with sentences. I want to know, for a given sentence, what is the closest sentence we can find in that file.</p>

<p>Here is the code for model creation:</p>

<pre><code>sentences = LabeledLineSentence(filename)

model = models.Doc2Vec(size=300, min_count=1, workers=4, window=5, alpha=0.025, min_alpha=0.025)
model.build_vocab(sentences)
model.train(sentences, epochs=50, total_examples=model.corpus_count)
model.save(modelName)
</code></pre>

<p>For test purposes, here is my file:</p>

<pre><code>uduidhud duidihdd
dsfsdf sdf sddfv
dcv dfv dfvdf g fgbfgbfdgnb
i like dogs
sgfggggggggggggggggg ggfggg
</code></pre>

<p>And here is my test:</p>

<pre><code>test = ""i love dogs"".split()
print(model.docvecs.most_similar([model.infer_vector(test)]))
</code></pre>

<p>No matter what parameter for training, this should obviously tell me that the most similar sentence is the 4th one (SENT_3 or SENT_4, I don't know how their indexes work, but the sentence labels are this form). But here is the result:</p>

<pre><code>[('SENT_0', 0.15669342875480652),
 ('SENT_2', 0.0008485736325383186),
 ('SENT_4', -0.009077289141714573)]
</code></pre>

<p>What am I missing ? And if I try with the same sentence (I LIKE dogs), I have SENT_2, then 1 then 4... I really don't get it. And why such low numbers ? And when I run few times in a row with a load, I don't get the same results either.</p>

<p>Thanks for your help</p>
",Vectorization & Embeddings,gensim doc vec similar method working expected struggling doc vec see wrong text file sentence want know given sentence closest sentence find file code model creation test purpose file test matter parameter training obviously tell similar sentence th one sent sent know index work sentence label form result missing try sentence like dog sent really get low number run time row load get result either thanks help
Cosine similarity is constantly 1.0,"<p>I am trying to build paragraph vectors and perform some inferences on them with the DeepLearning4J framework in Java. When I build my paragraph vectors into a ZIP folder, I am able to get similarities by using line numbers like so:</p>

<pre><code>SentenceIterator sentenceIterator = new BasicLineIterator(new File(inputFilePath));
AbstractCache&lt;VocabWord&gt; abstractCache = new AbstractCache&lt;VocabWord&gt;();
TokenizerFactory tokenizerFactory = new DefaultTokenizerFactory();
tokenizerFactory.setTokenPreProcessor(new CommonPreprocessor());

LabelsSource labelsSource = new LabelsSource(""LINE_"");

ParagraphVectors paragraphVectors = new ParagraphVectors.Builder()
        .minWordFrequency(1)
        .iterations(5)
        .epochs(1)
        .layerSize(100)
        .learningRate(0.025)
        .labelsSource(labelsSource)
        .windowSize(5)
        .iterate(sentenceIterator)
        .trainWordVectors(false)
        .vocabCache(abstractCache)
        .tokenizerFactory(tokenizerFactory)
        .sampling(0)
         .build();
paragraphVectors.fit();

double similarity1 = paragraphVectors.similarity(""LINE_9835"", ""LINE_100"");
System.out.println(""Similarity: "" + similarity1);

WordVectorSerializer.writeParagraphVectors(paragraphVectors, outputParagraphVectorsFilePath);
</code></pre>

<p>The variable <code>inputFilePath</code> refers to the text document that contains some information. The variable <code>outputParagraphVectorsFilePath</code> refers to the location on the disk where the vectors are to be stored. This function works and the similarities are accurate. The problem occurs below:</p>

<pre><code>TokenizerFactory tokenizerFactory = new DefaultTokenizerFactory();
tokenizerFactory.setTokenPreProcessor(new CommonPreprocessor());

ParagraphVectors paragraphVectors = WordVectorSerializer.readParagraphVectors(new File(inputFilePath));
paragraphVectors.setTokenizerFactory(tokenizerFactory);
paragraphVectors.getConfiguration().setIterations(1);

INDArray inferredVectorA = paragraphVectors.inferVector(""This is my world ."");
INDArray inferredVectorA2 = paragraphVectors.inferVector(""This is my world ."");
INDArray inferredVectorB = paragraphVectors.inferVector(""This is my way ."");


System.out.println(""Cosine similarity A/B:"" + Transforms.cosineSim(inferredVectorA, inferredVectorB));
System.out.println(""Cosine similarity A/B2:"" + Transforms.cosineSim(inferredVectorA, inferredVectorA2));
</code></pre>

<p>The <code>inputFilePath</code> variable refers to the location on disk where the ZIP folder is that contains the vectors. When I run this function, I get the following:</p>

<blockquote>
  <p><code>Cosine similarity A/B:1.0</code><br><code>Cosine similarity A/B2:1.0</code></p>
</blockquote>

<p>Even if I change the vectors around and compare them to other vectors, I get the same 1.0. Am I doing something wrong? Any help would be greatly appreciated.</p>
",Vectorization & Embeddings,cosine similarity constantly trying build paragraph vector perform inference deeplearning j framework java build paragraph vector zip folder able get similarity using line number like variable refers text document contains information variable refers location disk vector stored function work similarity accurate problem occurs variable refers location disk zip folder contains vector run function get following even change vector around compare vector get something wrong help would greatly appreciated
word co-occurrence matrix from gensim,"<p>When building a python gensim word2vec <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">model</a>, is there a way to see a doc-to-word matrix?</p>

<p>With input of <code>sentences = [['first', 'sentence'], ['second', 'sentence']]</code> I'd see something like*:</p>

<pre><code>      first  second  sentence
doc0    1       0        1
doc1    0       1        1
</code></pre>

<p>*I've illustrated 'human readable', but I'm looking for a scipy (or other) matrix, indexed to <code>model.wv.index2word</code>.</p>

<p>And, can that be transformed into a word-to-word matrix (to see co-occurences)? Something like:</p>

<pre><code>          first  second  sentence
first       1       0        1
second      0       1        1  
sentence    1       1        2   
</code></pre>

<p>I've already implemented something like <a href=""https://stackoverflow.com/questions/35562789/word-word-co-occurrence-matrix"">word-word co-occurrence matrix</a> using CountVectorizer. It works well. However, I'm already using gensim in my pipeline and speed/code simplicity matter for my use-case. </p>
",Vectorization & Embeddings,word co occurrence matrix gensim building python gensim word vec model way see doc word matrix input see something like illustrated human readable looking scipy matrix indexed transformed word word matrix see co occurences something like already implemented something like href co occurrence matrix using countvectorizer work well however already using gensim pipeline speed code simplicity matter use case
"What is surface pattern, template pattern and ensemble pattern in NLP text matching?","<p>I am working on text matchign using NLP. I have used fuzzywuzzy and cosine similarity. </p>

<p>Someone asked me about surface pattern, template patterns, ensemble pattern approach. </p>

<p>What is that?</p>
",Vectorization & Embeddings,surface pattern template pattern ensemble pattern nlp text matching working text matchign using nlp used fuzzywuzzy cosine similarity someone asked surface pattern template pattern ensemble pattern approach
Error with dimensions in Keras,"<p>I am tying to implement a simple word2vec model but I get the following error</p>

<p><code>ValueError: Error when checking target: expected dense-softmax to have 3 dimensions, but got array with shape (32, 14).</code></p>

<p>the variable <code>train_x</code> and <code>train_y</code> are 32 lines of the form</p>

<pre><code>[[0 0 0 0 0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 1 0 0 0 0 0 0 0 0 0]
                          ...]]
</code></pre>

<p>and the python code is the following</p>

<pre><code>vocal_size = 14
input = Input(shape=(vocal_size, ), dtype='int32', name='input')
embeddings = Embedding(output_dim=5, input_dim= vocal_size)(input)
output = Dense(vocal_size, use_bias=False, activation='softmax')(embeddings)
model = Model(input=input, output=output)
model.compile(optimizer='adam', loss='categorical_crossentropy')
model.summary()
model.fit(train_x, train_y)



_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           (None, 14)                0         
_________________________________________________________________
embeddings (Embedding)       (None, 14, 5)             70        
_________________________________________________________________
dense_1 (Dense)              (None, 14, 14)            70        
=================================================================
Total params: 140
Trainable params: 140
Non-trainable params: 0
</code></pre>

<p><strong>Edit:</strong><br/><br/>
(""I like stackoverflow"") with context size 1, I create the following tuples,<br/>
(""I"", ""like""), (""like"", ""I"") , (""like"", ""stackoverflow""), (""stackoverflow"", ""like"")<br/><br/>
Then I do an one-hot encoding of all of them and feed them to the model.<br/><br/>
train_x[0] -> is one hot encoding of the word ""I""<br/>
train_y[0] -> is one hot encoding of the context word ""like""</p>

<p><strong>Edit 2</strong><br/><br/></p>

<p>Using the first encoding for skip-gram:
Treating 0 as a special word (i.e. not top 10.000 most frequent) and start the counting from 1.
I assume I should give as an input a single number and output a one-hot encoding i.e. (""stack"", ""overflow""), input <code>[3]</code> (""stack"") and the output <code>[0,0,0,0,1,0,0,0,0,0,0]</code>(""overflow""). </p>

<pre><code>Input(shape=(1,)..) -&gt; 
Embedding(output_dim=embedding_size, input_dim=vocab_size, mask_zero=True, ...) -&gt; 
Dense(vocab_size+1, activation=""Softmax"")
model.compile(optimizer='SGD', loss='categorical_crossentropy')
</code></pre>

<p>I.e.
embedding_size = 5, input the sentences in your example, </p>

<p><a href=""https://i.sstatic.net/d09LY.jpg"" rel=""nofollow noreferrer"">https://i.sstatic.net/d09LY.jpg</a></p>
",Vectorization & Embeddings,error dimension kera tying implement simple word vec model get following error variable line form python code following edit like stackoverflow context size create following tuples like like like stackoverflow stackoverflow like one hot encoding feed model train x one hot encoding word train one hot encoding context word like edit using first encoding skip gram treating special word e top frequent start counting assume give input single number output one hot encoding e stack overflow input stack output overflow e embedding size input sentence example
Sequence to sequence modeling in python,"<p>I am trying to make a chatbot that uses a sequence to sequence model to respond to the user's input. The problem is that the input sequence given to the model will almost never be the same. The input sequence is a list of words. I have created a vocabulary that maps each word in this sequence to its own unique id, however, the input is still variable and is not fixed so I can't just use a sequence to sequence model. I understand that it is possible to use an encoder to map the sequence of words to a fixed vector representation and then have a decoder map that vector back to a sequence.</p>

<p>The question I have is how would I go about encoding the sequence of words to a fixed vector? Is there any sort of technique that could be used for this purpose?</p>
",Vectorization & Embeddings,sequence sequence modeling python trying make chatbot us sequence sequence model respond user input problem input sequence given model almost never input sequence list word created vocabulary map word sequence unique id however input still variable fixed use sequence sequence model understand possible use encoder map sequence word fixed vector representation decoder map vector back sequence question would go encoding sequence word fixed vector sort technique could used purpose
What does representation matrix of context word mean in SkipGram?,"<p>I am learning Stanford NLP Course and I have issue understanding a concept in Skipgram from picture below.</p>

<p><a href=""https://i.sstatic.net/0XwOX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0XwOX.jpg"" alt=""enter image description here""></a>
From left to right, the first column vector is one-hot encoder, the second is the word embedding matrix from 1-layer neural network, the third is word representation vector. However, when it comes to the fourth one, which is a matrix with 'v by d' dimension. Not sure if I listen it correctly, but the speaker said this is a representation of context word and these three matrix are identical? </p>

<p>My questions are:
1. Why these three matrix are identical but the three multiplication results are different?
2. How do we get this matrix (v by d dimension)?</p>

<p>The link to the lecture is: </p>

<p><a href=""https://www.youtube.com/watch?v=ERibwqs9p38&amp;t=1481s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=ERibwqs9p38&amp;t=1481s</a></p>
",Vectorization & Embeddings,doe representation matrix context word mean skipgram learning stanford nlp course issue understanding concept skipgram picture left right first column vector one hot encoder second word embedding matrix layer neural network third word representation vector however come fourth one matrix v dimension sure listen correctly speaker said representation context word three matrix identical question three matrix identical three multiplication result different get matrix v dimension link lecture
"Does pre-trained Embedding matrix has &lt;EOS&gt;, &lt;UNK&gt; word vector?","<p>I want to build a seq2seq chatbot with a pre-trained Embedding matrix. Does the pre-trained Embedding matrix, for example GoogleNews-vectors-negative300, FastText and GloVe, has the specific word vector for <code>&lt;EOS&gt;</code> and <code>&lt;UNK&gt;</code>?</p>
",Vectorization & Embeddings,doe pre trained embedding matrix ha eos unk word vector want build seq seq chatbot pre trained embedding matrix doe pre trained embedding matrix example googlenews vector negative fasttext glove ha specific word vector
Word2vec: distangling semantic from syntactic,"<p>I want to use pre-train word vectors (e.g., fasttest on Wikipedia) to find clusters of a set of words. However, in the list of words I have words like 'kindness', 'kind', 'kindly' and they fall in different clusters. That is sometimes words with similar part of speech are clusters together. I want to know how can I have word vectors that only captures meaning?</p>
",Vectorization & Embeddings,word vec distangling semantic syntactic want use pre train word vector e g fasttest wikipedia find cluster set word however list word word like kindness kind kindly fall different cluster sometimes word similar part speech cluster together want know word vector capture meaning
"In word embedding, how to map the vector to word?","<p>I checked all API and couldn't find a way to map vector to word no matter in word2Vec or glove. Google doesn't help that much. </p>

<p>Does anybody know to do this? </p>

<p>Background: I'm training a chatbot by using seq2seq model. But the implementations I found so far are using one-hot encoding. So I want to try use glove embedding and use the output mapping back to the word. </p>
",Vectorization & Embeddings,word embedding map vector word checked api find way map vector word matter word vec glove google help much doe anybody know background training chatbot using seq seq model implementation found far using one hot encoding want try use glove embedding use output mapping back word
How to deal with out-of-vocaboular words in NLP applications when word embedding is used?,"<p>The following NLP application uses word embedding. But I am not sure what if a word in an input text is not available in the embedding. Does anybody know what is the standardard practice to deal with words that are not in the embedding for NLP (or NER in particular)? Thanks.</p>

<p><a href=""https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html"" rel=""nofollow noreferrer"">https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html</a></p>
",Vectorization & Embeddings,deal vocaboular word nlp application word embedding used following nlp application us word embedding sure word input text available embedding doe anybody know standardard practice deal word embedding nlp ner particular thanks
How to vectorize text in order to use it as feature for time series prediction? (Keras),"<p>What is the best way to vectorize text in order to use it as one of many features for time series prediction? </p>

<p>The time series is daily and I have 8 to 10 different news headlines per date (~16,000 headlines in total). Each headline consists of max 25 words. The headlines are cleaned (lower case, punctuation and number removal, stop word removal and lemmatized) and tokenized on a word level. </p>

<p>How can I vectorize the headlines and aggregate them on a daily level so that I can use them as an input feature? Because all other features (e.g. Federal Funds Rate, Gold Price, etc.) are just a single integer per date. </p>

<p>I thought of using word embeddings. But training a word embedding model (word2vec or GloVe) on only 16,000 headlines probably won’t achieve good results. However, even if I use pre-trained word vectors I am worried about the column dimensionality of my time series data frame in R. Since the word vector is 100 dimensional and I have 25 words per headline and 8-10 headlines per date, the dimension of my time series would be 100x25x10 = 25,000 columns and 1,700 rows (1,700 days). </p>

<p>So do you have any idees of how I can include the news headlines as a feature for time series prediction? If it helps, I plan to implement a LSTM Neural Network in Keras using R for predicting the trend (up or down) of a traded asset.</p>

<p>Ideas and advice are much appreciated.</p>

<p>Thanks a lot.</p>
",Vectorization & Embeddings,vectorize text order use feature time series prediction kera best way vectorize text order use one many feature time series prediction time series daily different news headline per date headline total headline consists max word headline cleaned lower case punctuation number removal stop word removal lemmatized tokenized word level vectorize headline aggregate daily level use input feature feature e g fund rate gold price etc single integer per date thought using word embeddings training word embedding model word vec glove headline probably achieve good result however even use pre trained word vector worried column dimensionality time series data frame r since word vector dimensional word per headline headline per date dimension time series would x x column row day idees include news headline feature time series prediction help plan implement lstm neural network kera using r predicting trend traded asset idea advice much appreciated thanks lot
How can I handle a date such as &#39;1990-03-27&#39; in Glove model for creating embeddings?,"<p>I want to create embeddings for words in my document. What do I transform the above mentioned date into so that I can create embeddings in glove?
Thanks in advance. </p>
",Vectorization & Embeddings,handle date glove model creating embeddings want create embeddings word document transform mentioned date create embeddings glove thanks advance
how to expand the words of tfidf vectorizer in sklearn without retraining the whole model from scratch?,"<p>I have a bunch of text documents that I throw at a tfidf vectorizer which I further use for multi-label text classification. I will keep getting a stream of more documents in the future. Now how do I add new words to the vectorizer it has never seen before without retraining the model from scratch? Is partial_fit the only option, cause OvR and pipeline don't work with it? 
Here is the link I am talking about <a href=""http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py"" rel=""nofollow noreferrer"">online learning of text documents</a>.</p>
",Vectorization & Embeddings,expand word tfidf vectorizer sklearn without retraining whole model scratch bunch text document throw tfidf vectorizer use multi label text classification keep getting stream document future add new word vectorizer ha never seen without retraining model scratch partial fit option cause ovr pipeline work link talking online learning text document
Writing a custom NER and POS tagger in pyspark to use in the pipeline method for feature extraction of textual inputs,"<p>I am doing a text classification project and I am using the pipeline method from mllib to chain the feature extraction stages. My dataset consists of English sentences. I have the Tokenizer and the TFIDF Vectorizer libraries from mllib that I can directly use in the pipeline.</p>

<p>But as a part of the feature extraction process, I also need to extract POS tags and NER tags. Pyspark doesn't have a library for it and I don't know how can I write a custom transformer for NER and POS tagging. I am very new to Spark and Python. I am using Spark 1.6 and Python 2.7.</p>
",Vectorization & Embeddings,writing custom ner po tagger pyspark use pipeline method feature extraction textual input text classification project using pipeline method mllib chain feature extraction stage dataset consists english sentence tokenizer tfidf vectorizer library mllib directly use pipeline part feature extraction process also need extract po tag ner tag pyspark library know write custom transformer ner po tagging new spark python using spark python
"How to use vector representation of words (as obtained from Word2Vec,etc) as features for a classifier?","<p>I am familiar with using BOW features for text classification, wherein we first find the size of the vocabulary for the corpus which becomes the size of our feature vector. For each sentence/document, and for all its constituent words, we then put 0/1 depending on the absence/presence of that word in that sentence/document. </p>

<p>However, now that I am trying to use vector representation of each word, is creating a global vocabulary essential? </p>
",Vectorization & Embeddings,use vector representation word obtained word vec etc feature classifier familiar using bow feature text classification wherein first find size vocabulary corpus becomes size feature vector sentence document constituent word put depending absence presence word sentence document however trying use vector representation word creating global vocabulary essential
Very low validation accuracy while doing multi-class classification using word2vec and cnn,"<p>I am doing text classification in Keras. First, I am creating an embedding matrix with Word2Vec and passing it to Keras <code>Embedding</code> layer. Then I am running <code>Conv1D</code> on top of it. This is the <a href=""https://www.dropbox.com/s/bxe63rkqkaqji1x/emotion_merged_dataset.csv?dl=0"" rel=""nofollow noreferrer"">dataset</a> I am using. Here is my code below:</p>

<pre><code>from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np
from keras.models import Sequential
from keras.layers import Embedding,Flatten,Dense,Conv1D,MaxPooling1D,GlobalMaxPooling1D
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils
import pandas as pd
from nltk.tokenize import word_tokenize
from keras.optimizers import RMSprop


# def dataframe_to_list_of_words(df_name, col):
#   df = pd.read_csv(df_name)
#   lst = df[col].drop_duplicates().values.tolist()
#   tokenized_sents = [word_tokenize(i) for i in lst]
#   tokenized_sents_mod = [word for sublist in tokenized_sents for word in sublist]
#   return tokenized_sents_mod

# def convert_data_to_index(string_data, wv):
#     index_data = []
#     for word in string_data:
#         if word in wv:
#             index_data.append(wv.vocab[word].index)
#     return index_data

df=pd.read_csv('emotion_merged_dataset.csv')
texts=df['text']
labels=df['sentiment']

df_tokenized=df.apply(lambda row: word_tokenize(row['text']), axis=1)


model = Word2Vec(df_tokenized, min_count=1,size=300)
##############
embedding_matrix = np.zeros((len(model.wv.vocab), 300))
for i in range(len(model.wv.vocab)):
#     print(model.wv.index2word[i])
    embedding_vector = model.wv[model.wv.index2word[i]]
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
################
labels=df['sentiment']
encoder = LabelEncoder()
encoder.fit(labels)
encoded_Y = encoder.transform(labels)
labels_encoded= np_utils.to_categorical(encoded_Y)
#########################

maxlen=30
tokenizer = Tokenizer(3000)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))
data = pad_sequences(sequences, maxlen=37)
############################
embeddings = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],
                      weights=[embedding_matrix],trainable=False)
model=Sequential()
model.add(embeddings)
model.add(Conv1D(32,7,activation='relu'))
model.add(MaxPooling1D(5))
model.add(Conv1D(32,7,activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(labels_encoded.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(data, labels_encoded, validation_split=0.2, epochs=10, batch_size=100)
</code></pre>

<p>My validation accuracy is hovering around 0.09. I believe there is some conceptual problem in my code. Can someone point it out?</p>
",Vectorization & Embeddings,low validation accuracy multi class classification using word vec cnn text classification kera first creating embedding matrix word vec passing kera layer running top dataset using code validation accuracy hovering around believe conceptual problem code someone point
Error while doing text-classification in Keras,"<p>I am doing text classification in Keras. First, I am creating an embedding matrix with Word2Vec and passing it to Keras <code>Embedding</code> layer. Then I am running <code>Conv1D</code> on top of it. This is the <a href=""https://www.dropbox.com/s/bxe63rkqkaqji1x/emotion_merged_dataset.csv?dl=0"" rel=""nofollow noreferrer"">dataset</a> I am using. Here is my code below:</p>

<pre><code>from gensim.models import Word2Vec
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np
from keras.models import Sequential
from keras.layers import Embedding,Flatten,Dense,Conv1D,MaxPooling1D,GlobalMaxPooling1D
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils
import pandas as pd
from nltk.tokenize import word_tokenize


# def dataframe_to_list_of_words(df_name, col):
#   df = pd.read_csv(df_name)
#   lst = df[col].drop_duplicates().values.tolist()
#   tokenized_sents = [word_tokenize(i) for i in lst]
#   tokenized_sents_mod = [word for sublist in tokenized_sents for word in sublist]
#   return tokenized_sents_mod

# def convert_data_to_index(string_data, wv):
#     index_data = []
#     for word in string_data:
#         if word in wv:
#             index_data.append(wv.vocab[word].index)
#     return index_data

df=pd.read_csv('emotion_merged_dataset.csv')
texts=df['text']
labels=df['sentiment']

df_tokenized=df.apply(lambda row: word_tokenize(row['text']), axis=1)


model = Word2Vec(df_tokenized, min_count=1,size=300)
##############
embedding_matrix = np.zeros((len(model.wv.vocab), 300))
for i in range(len(model.wv.vocab)):
#     print(model.wv.index2word[i])
    embedding_vector = model.wv[model.wv.index2word[i]]
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
################
labels=df['sentiment']
encoder = LabelEncoder()
encoder.fit(labels)
encoded_Y = encoder.transform(labels)
labels_encoded= np_utils.to_categorical(encoded_Y)
#########################

maxlen=30
tokenizer = Tokenizer(3000)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))
data = pad_sequences(sequences, maxlen=37)
############################
embeddings = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],
                      weights=[embedding_matrix],trainable=False)
model=Sequential()
model.add(embeddings)
model.add(Conv1D(32,7,activation='relu'))
model.add(MaxPooling1D(5))
model.add(Conv1D(32,7,activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(labels_encoded.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(data, labels, validation_split=0.2, epochs=10, batch_size=100)
</code></pre>

<p>I am getting the following error when I am running my code:</p>

<pre><code>Error when checking target: expected dense_1 to have shape (None, 8) but got array with shape (19283, 1)
</code></pre>

<p>Can someone please help me?</p>
",Vectorization & Embeddings,error text classification kera text classification kera first creating embedding matrix word vec passing kera layer running top dataset using code getting following error running code someone please help
How to let TF-IDF learn a part of a document with higher priority?,"<p>I use TfIdf from <code>sklearn</code>.</p>

<p>I want to learn similarity between documents. However, these documents contain a title that brings more information than other parts of the documents.</p>

<p>Is it possible to tell TF-IDF that, for instance, if there is a word appears in the title it should be more important than the same word elsewhere?</p>

<p>Thanks</p>
",Vectorization & Embeddings,let tf idf learn part document higher priority use tfidf want learn similarity document however document contain title brings information part document possible tell tf idf instance word appears title important word elsewhere thanks
Recommended algorithms for word similarity,"<p>I'm researching viable algorithms/solutions to implement and solve following problem: 
<strong>match users based on their common interests</strong></p>

<p>Example:<br>
U1: skiing, asian culture, meditation, java, crypto<br>
U2: yoga, meditation, management, travel tips USA<br>
U3: programming, travelling, oriental cuisine</p>

<p>I'm considering three dimensions based on word similarity:  </p>

<ul>
<li>Dictionary synonyms 

<ul>
<li><a href=""https://wordnet.princeton.edu/"" rel=""nofollow noreferrer"">wordnet</a> synsets</li>
</ul></li>
<li>Close semantic similarity (programming > java, travelling > travel tips USA)

<ul>
<li>So far I have considered <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein_distance</a></li>
</ul></li>
<li>Loose semantic similarity (asian culture >> oriental cuisine, programming >> crypto, asian culture >> yoga, yoga >> meditation)

<ul>
<li>Not sure at all, played with <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">word2vec</a></li>
</ul></li>
</ul>

<p>Based on these approaches I would like to calculate a relevancy score and match users accordingly.</p>

<p>Thanks for the input!</p>
",Vectorization & Embeddings,recommended algorithm word similarity researching viable algorithm solution implement solve following problem match user based common interest example u skiing asian culture meditation java crypto u yoga meditation management travel tip usa u programming travelling oriental cuisine considering three dimension based word similarity dictionary synonym wordnet synset close semantic similarity programming java travelling travel tip usa far considered levenshtein distance loose semantic similarity asian culture oriental cuisine programming crypto asian culture yoga yoga meditation sure played word vec based approach would like calculate relevancy score match user accordingly thanks input
Keras LSTM Error: ValueError: setting an array element with a sequence,"<p>I'm loading sentence vectors embeddings (Facebook's InferSent) in my neural network along with other metadata. Here is a glimpse of my dataset:</p>

<p><a href=""https://i.sstatic.net/oGrKM.png"" rel=""nofollow noreferrer"">My dataset</a></p>

<p>Endpoint Vector: Sentence embedding ndarray. Shape: (4096,1)
Description Vector: Sentence embedding ndarray. Shape: (4096,1)
HTTP Path: Categorical variable (0 or 1)
Is Description: Binary dependent variable (to be predicted)
EndpointDesc Count: Numerical variable (int)
Cosine similarity: Similarity between Endpoint &amp; Description vector.</p>

<p>My dataset consists of 5621 rows and these 6 features. I am new to Neural Networks and wanted to use Endpoint vectors (and later use all the other features) to predict ""Is Description"" variable. The input shape to the LSTM in Keras is really tricky. I had to do a lot of research on the same, and I'm still not sure whether the data which I feed into the LSTM is in proper shape.</p>

<pre><code>X = trainDf.iloc[:, [0]].values
y = trainDf.iloc[:, [3]].values
X = X.reshape(len(X), 1, 1)
y = y.reshape(len(y), 1, 1)
model = Sequential()
model.add(LSTM(1, input_shape=(1,1), return_sequences=True, activation=""sigmoid""))
model.add(LSTM(1, input_shape=(1,1), return_sequences=True, activation=""sigmoid""))
model.add(LSTM(1, input_shape=(1,1), return_sequences=True, activation=""sigmoid""))
model.add(LSTM(1, input_shape=(1,1), return_sequences=True, activation=""sigmoid""))
model.add(LSTM(1, input_shape=(1,1), return_sequences=True, activation=""sigmoid""))
model.add(LSTM(1, input_shape=(1,1), return_sequences=True, activation=""sigmoid""))
model.add(LSTM(1, input_shape=(1,1), return_sequences=True, activation=""sigmoid""))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, y, epochs=1, batch_size=1, validation_data=(X, y))

predict = model.predict(X)
</code></pre>

<p>The code throws the following error:</p>

<pre><code>    Train on 5621 samples, validate on 5621 samples
Epoch 1/1

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-217-c46383616bf3&gt; in &lt;module&gt;()
      8 model.add(LSTM(1, input_shape=(1,1), return_sequences=True, activation=""sigmoid""))
      9 model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
---&gt; 10 model.fit(X, y, epochs=1, batch_size=1, validation_data=(X, y))
     11 
     12 predict = model.predict(X)

/var/home/pranavmakhijani/anaconda/lib/python3.5/site-packages/keras/models.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
    963                               initial_epoch=initial_epoch,
    964                               steps_per_epoch=steps_per_epoch,
--&gt; 965                               validation_steps=validation_steps)
    966 
    967     def evaluate(self, x=None, y=None,

/var/home/pranavmakhijani/anaconda/lib/python3.5/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1667                               initial_epoch=initial_epoch,
   1668                               steps_per_epoch=steps_per_epoch,
-&gt; 1669                               validation_steps=validation_steps)
   1670 
   1671     def evaluate(self, x=None, y=None,

/var/home/pranavmakhijani/anaconda/lib/python3.5/site-packages/keras/engine/training.py in _fit_loop(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
   1204                         ins_batch[i] = ins_batch[i].toarray()
   1205 
-&gt; 1206                     outs = f(ins_batch)
   1207                     if not isinstance(outs, list):
   1208                         outs = [outs]

/var/home/pranavmakhijani/anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py in __call__(self, inputs)
   2473         session = get_session()
   2474         updated = session.run(fetches=fetches, feed_dict=feed_dict,
-&gt; 2475                               **self.session_kwargs)
   2476         return updated[:len(self.outputs)]
   2477 

/var/home/pranavmakhijani/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    776     try:
    777       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 778                          run_metadata_ptr)
    779       if run_metadata:
    780         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/var/home/pranavmakhijani/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
    952             np_val = subfeed_val.to_numpy_array()
    953           else:
--&gt; 954             np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)
    955 
    956           if (not is_tensor_handle_feed and

/var/home/pranavmakhijani/anaconda/lib/python3.5/site-packages/numpy/core/numeric.py in asarray(a, dtype, order)
    480 
    481     """"""
--&gt; 482     return array(a, dtype, copy=False, order=order)
    483 
    484 def asanyarray(a, dtype=None, order=None):

ValueError: setting an array element with a sequence.
</code></pre>

<p>Next steps:
I want to include all the other variables as training features in my dataset.</p>
",Vectorization & Embeddings,kera lstm error valueerror setting array element sequence loading sentence vector embeddings facebook infersent neural network along metadata glimpse dataset dataset endpoint vector sentence embedding ndarray shape description vector sentence embedding ndarray shape http path categorical variable description binary dependent variable predicted endpointdesc count numerical variable int cosine similarity similarity endpoint description vector dataset consists row feature new neural network wanted use endpoint vector later use feature predict description variable input shape lstm kera really tricky lot research still sure whether data feed lstm proper shape code throw following error next step want include variable training feature dataset
Document representation with pre-trained Word Vectors for Author Classification/Regression (GP),"<p>I am trying to replicate (<a href=""https://arxiv.org/abs/1704.05513"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1704.05513</a>) to do a Big 5 author classification on Facebook data (posts and Big 5 profiles are given).</p>

<p>After removing the stop words, I embed each word in the file with their pre-trained GloVe word vectors. However, computing the average or coordinate-wise min/max word vector for each user and using those as input for a Gaussian Process/SVM gives me terrible results. Now I wonder what the paper means by:</p>

<blockquote>
  <p>Our method combines Word Embedding with Gaussian
  Processes. We extract the words from the users’ tweets and
  average their Word Embedding representation into a single
  vector. The Gaussian Processes model then takes these
  vectors as an input for training and testing.</p>
</blockquote>

<p>What else can I ""average"" the vectors to get decent results and do they use some specific Gaussian Process?</p>
",Vectorization & Embeddings,document representation pre trained word vector author classification regression gp trying replicate big author classification facebook data post big profile given removing stop word embed word file pre trained glove word vector however computing average coordinate wise min max word vector user using input gaussian process svm give terrible result wonder paper mean method combine word embedding gaussian process extract word user tweet average word embedding representation single vector gaussian process model take vector input training testing else average vector get decent result use specific gaussian process
Sentence2vec and Word2vec involving stop words and Named Entities,"<p>I'm working on a NLP project, involving sentence2vec. I'm presuming I would be using pre-trained word embeddings for converting tokens into vectors and then proceeding to sentence embedding. </p>

<p>Since my sentence involves : 
stop words like <strong>can't, won't, aren't</strong> etc. which <em>NLTK</em> would reduce to <strong>{ca, wo, are} + not</strong>. <br> So I can't reduce them, and I don't want to remove them as stop words since sentences like mentioned below, should have different embedding. </p>

<p><strong>My name is Priyank <br>
My name is not Priyank</strong></p>

<p>Another Important doubt is that how to incorporate Named entities such as the name of a person like <strong>Mark K. Hogg</strong> in my sentence vector.  </p>
",Vectorization & Embeddings,sentence vec word vec involving stop word named entity working nlp project involving sentence vec presuming would using pre trained word embeddings converting token vector proceeding sentence embedding since sentence involves stop word like etc nltk would reduce ca wo reduce want remove stop word since sentence like mentioned different embedding name priyank name priyank another important doubt incorporate named entity name person like mark k hogg sentence vector
How to search Word2Vec or GloVe Embedding to find words by semantic relationship,"<p>Common examples of showing Word Embedding's strength is to show semantic relationship between some words such <code>king:queen = male:female</code>. How can this type of relationship be discovered? Is that through some kind of visualization based on geometric clustering? Any pointer will be appreciated.</p>
",Vectorization & Embeddings,search word vec glove embedding find word semantic relationship common example showing word embedding strength show semantic relationship word type relationship discovered kind visualization based geometric clustering pointer appreciated
How to find semantic similarity using gensim and word2vec in python,"<p>I have a list of words in my python programme. Now I need to iterate through this list and find out the semantically similar words and put them into another list. I have been trying to do this using gensim with word2vec but could find a proper solution.This is what I have implemeted up to now. I need a help on how to iterate through the list of words in the variable sentences and find the semantically similar words and save it in another list.</p>

<pre><code>import gensim, logging

import textPreprocessing, frequentWords , summarizer
from gensim.models import Word2Vec, word2vec

import numpy as np
from scipy import spatial

sentences = summarizer.sorteddict

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
model = word2vec.Word2Vec(sentences, iter=10, min_count=5, size=300, workers=4)
</code></pre>
",Vectorization & Embeddings,find semantic similarity using gensim word vec python list word python programme need iterate list find semantically similar word put another list trying using gensim word vec could find proper solution implemeted need help iterate list word variable sentence find semantically similar word save another list
Doc2vec: clustering resulting vectors,"<p>In the doc2vec model, Can we cluster on the vectors themselves? Should we cluster each resulting <code>model.docvecs[1]</code>vector? How to implement the clustering model?</p>

<pre><code> model = gensim.models.doc2vec.Doc2Vec(size= 100, min_count = 5,window=4, iter = 50, workers=cores)
    model.build_vocab(res) 
    model.train(res, total_examples=model.corpus_count, epochs=model.iter)


    # each of length 100
    len(model.docvecs[1])
</code></pre>
",Vectorization & Embeddings,doc vec clustering resulting vector doc vec model cluster vector cluster resulting vector implement clustering model
Gensim Doc2Vec Most_Similar,"<p>I'm having trouble with the most_similar method in Gensim's Doc2Vec model. When I run most_similar I only get the similarity of the first 10 tagged documents (based on their tags-always from 0-9). For this code I have topn=5, but I've used topn=len(documents) and I still only get the similarity for the first 10 documents</p>

<p>Tagged documents:</p>

<pre><code>tokenizer = RegexpTokenizer(r'\w+')
taggeddoc=[]

for index,wod in enumerate(model_data):
    wordslist=[]
    tagslist=[]
    tokens = tokenizer.tokenize(wod)

    td = TaggedDocument(gensim.utils.to_unicode(str.encode(' '.join(tokens))).split(), str(index)) 
    taggeddoc.append(td)

documents=taggeddoc
</code></pre>

<p>Instantiate the model:</p>

<pre><code>model=gensim.models.Doc2Vec(documents, dm=0, dbow_words=1, iter=1, alpha=0.025, min_alpha=0.025, min_count=10)
</code></pre>

<p>Train the model:</p>

<pre><code>for epoch in range(100):
    if epoch % 10 == 0:
        print(""Training epoch {}"".format(epoch))
    model.train(documents, total_examples=model.corpus_count, epochs=model.iter)
    model.alpha -= 0.002
    model.min_alpha = model.alpha
</code></pre>

<p>Problem is here (I think):</p>

<pre><code>new = model_data[100].split()
new_vector = model.infer_vector(new)
sims = model.docvecs.most_similar([new_vector], topn=5)
print(sims)
</code></pre>

<p>Output:</p>

<pre><code>[('3', 0.3732905089855194), ('1', 0.36121609807014465), ('7', 0.35790640115737915), ('9', 0.3569292724132538), ('2', 0.3521473705768585)]
</code></pre>

<p>Length of documents is the same before and after training the model. Not sure why it's only returning similarity for the first 10 documents.</p>

<p>Side question: In anyone's experience, is it better to use Word2Vec or Doc2Vec if the input documents are very short (~50 words) and there are >2,000 documents? Thanks for the help!</p>
",Vectorization & Embeddings,gensim doc vec similar trouble similar method gensim doc vec model run similar get similarity first tagged document based tag always code topn used topn len document still get similarity first document tagged document instantiate model train model problem think output length document training model sure returning similarity first document side question anyone experience better use word vec doc vec input document short word document thanks help
How to group multiple words that together convey one particular meaning using word embedding NLP,"<p>I am working on a NLP project and I am new to this field. I am doing WORD EMBEDDINGS in KERAS. I wanted to embed multiple words, (which together convey a particular meaning) together as one word.</p>

<p>For eg:  Copper pipe, both of them together convey one meaning but as separate words looses the context completely.</p>

<p>Similarly, Mechanical Engineer, hot water, N-Dimension Vector space  etc</p>

<p>How do I do it so that together they get ONE embedding vector ??</p>
",Vectorization & Embeddings,group multiple word together convey one particular meaning using word embedding nlp working nlp project new field word embeddings kera wanted embed multiple word together convey particular meaning together one word eg copper pipe together convey one meaning separate word loos context completely similarly mechanical engineer hot water n dimension vector space etc together get one embedding vector
Load vectors into gensim Word2Vec model - not KeyedVectors,"<p>I'm attempting to load some pre-trained vectors into a gensim <code>Word2Vec</code> model, so they can be retrained with new data. My understanding is I can do the retraining with <code>gensim.Word2Vec.train()</code>. However, the only way I can find to load the vectors is with <code>gensim.models.KeyedVectors.load_word2vec_format('path/to/file.bin', binary=True)</code> which creates an object of what is usually the <code>wv</code> attribute of a <code>gensim.Word2Vec</code> model. But this object, on it's own, does not have a <code>train()</code> method, which is what I need to retrain the vectors. </p>

<p>So how do I get these vectors into an actual <code>gensim.Word2Vec</code> model?</p>
",Vectorization & Embeddings,load vector gensim word vec model keyedvectors attempting load pre trained vector gensim model retrained new data understanding retraining however way find load vector creates object usually attribute model object doe method need retrain vector get vector actual model
Extract embedded vecor per word from h2o.word2vec object,"<p>I'm trying to create a pre-trained embedding layer, using <code>h2o.word2vec</code>, i'm looking to extract each word in the model and its equivalent embedded vector.</p>

<p>Code:</p>

<pre><code>library(data.table)
library(h2o)
h2o.init(nthreads = -1)

comment &lt;- data.table(comments='ExplanationWhy the edits made under my username Hardcore Metallica 
                      Fan were reverted They werent vandalisms just closure on some GAs after I voted 
                      at New York Dolls FAC And please dont remove the template from the talk page since Im retired now')

comments.hex &lt;- as.h2o(comment, destination_frame = ""comments.hex"", col.types=c(""String""))

words &lt;- h2o.tokenize(comments.hex$comments, ""\\\\W+"")

vectors &lt;- 3 # Only 10 vectors to save time &amp; memory
w2v.model &lt;- h2o.word2vec(words
                          , model_id = ""w2v_model""
                          , vec_size = vectors
                          , min_word_freq = 1
                          , window_size = 2
                          , init_learning_rate = 0.025
                          , sent_sample_rate = 0
                          , epochs = 1) # only a one epoch to save time
print(h2o.findSynonyms(w2v.model, ""the"",2))
</code></pre>

<p>The <code>h2o</code> API enables me to get the cosine of two word, but i'm just looking to get the vector of each work in my vocabulary, how can i get it? couldn't find any simple method in the API that gives it</p>

<p>Thanks in advance</p>
",Vectorization & Embeddings,extract embedded vecor per word h word vec object trying create pre trained embedding layer using looking extract word model equivalent embedded vector code api enables get cosine two word looking get vector work vocabulary get find simple method api give thanks advance
"NLU FastText, Glove or Word2Vec Load Pre-trained model and Add new word to vocabulary","<p>As i say in the title i would like to load pre-tranined model </p>

<p>using gensim is possibile for example but with fasttext say:</p>

<p><a href=""https://radimrehurek.com/gensim/models/wrappers/fasttext.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/wrappers/fasttext.html</a></p>

<pre><code> ""Note that due to limitations in the FastText API, you cannot continue training with a model loaded this way, though you can query for word similarity etc.""
</code></pre>

<p>With Word2vec say it is possibile continue the traning of your own model not a pretranind end i do not know with Glove.</p>

<p>Can you point me to any library or something for load a pre-tranined model and continue the traning on my own sentences ?</p>

<p>Or in case i can load the pretranined model into a neural network and after continue the traning with my own vectors ?  (maybe using get_keras_embedding ? )</p>
",Vectorization & Embeddings,nlu fasttext glove word vec load pre trained model add new word vocabulary say title would like load pre tranined model using gensim possibile example fasttext say word vec say possibile continue traning model pretranind end know glove point library something load pre tranined model continue traning sentence case load pretranined model neural network continue traning vector maybe using get kera embedding
Keras: Embedding and LSTM layers for POS-Tagging task,"<p>I have a list of tagged sentences. I transformed each of them in the following way:</p>

<ul>
<li>For each <strong>word</strong>, get the relative one-hot encoding form (a vector of dimension <code>input_dim</code>);</li>
<li>Insert a pre-padding as explained in the example below;</li>
<li>Split each sentence in <code>len(sentence)</code> sub-sentences, using a window of size <code>time_steps</code> (to get the context for the prediction of the next word).</li>
</ul>

<p>For example, using <code>time_steps=2</code>, a single sentence <code>[""this"", ""is"", 
""an"", ""example""]</code> is transformed in:</p>

<pre><code>[
    [one_hot_enc(""empty_word""), one_hot_enc(""empty_word"")],
    [one_hot_enc(""empty_word""), one_hot_enc(""this"")],
    [one_hot_enc(""this""), one_hot_enc(""is"")],
    [one_hot_enc(""is""), one_hot_enc(""an"")],
]
</code></pre>

<p>At the end, considering the sub-sentences as an unique list, the shape of the train data <code>X_train</code> is <code>(num_samples, time_steps, input_dim)</code>, where:</p>

<ul>
<li><code>input_dim</code>: the size of my vocabulary;</li>
<li><code>time_steps</code>: the length of sequence to use into LSTM;</li>
<li><code>num_samples</code>: the number of samples (sub-sentences);</li>
</ul>

<p>Now, I want to use an <code>Embedding</code> layer, in order to map each word to a smaller an continuous dimensional space, and an <code>LSTM</code>, in which I use the contexts build as above.</p>

<p>I tried something like this:</p>

<pre><code>model = Sequential()
model.add(InputLayer(input_shape=(time_steps, input_dim)))
model.add(Embedding(input_dim, embedding_size, input_length=time_steps))
model.add(LSTM(32))
model.add(Dense(output_dim))
model.add(Activation('softmax'))
</code></pre>

<p>But gives me the following error:</p>

<pre><code>ValueError: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=4
</code></pre>

<p>What I missing? There is some logical error in what I'm trying to do?</p>
",Vectorization & Embeddings,kera embedding lstm layer po tagging task list tagged sentence transformed following way word get relative one hot encoding form vector dimension insert pre padding explained example split sentence sub sentence using window size get context prediction next word example using single sentence transformed end considering sub sentence unique list shape train data size vocabulary length sequence use lstm number sample sub sentence want use layer order map word smaller continuous dimensional space use context build tried something like give following error missing logical error trying
How to get an exact word match to convert words re-using the solution below?,"<p>The solution below was provided on Stack Overflow here: <a href=""https://stackoverflow.com/a/19790352/4513967"">expanding-english-language-contractions-in-python</a></p>

<p>It works <em>great</em> for contractions. I tried to extend it to handle slang words but ran into an issue per below. Also, I'd prefer to use 1 solution to handle all word conversions (e.g.: expansions, slang, etc.)</p>

<p>I extended the contractions_dict to also correct slang, see 3rd entry below:</p>

<pre><code>contractions_dict = {
     ""didn't"": ""did not"",
     ""don't"": ""do not"",
     ""ur"": ""you are""
 }
</code></pre>

<p>However, when I do so on words that include a slang term (ur) like ""surprise"" I get</p>

<p>""syou areprise""</p>

<p>The ""you"" and ""are"" embedded above are where the ""ru"" use to be. </p>

<p>How do you get an <em>exact match</em> on a key in the contractions_dict? </p>

<p>In my code below I tried embedding a more exact word match regex around the ""replace"" function but received an error ""TypeError: must be str, not function"".</p>

<p>The code:</p>

<pre><code>import re

contractions_dict = {
     ""didn't"": ""did not"",
     ""don't"": ""do not"",
     ""ur"": ""you are""
 }

contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))
def expand_contractions(s, contractions_dict=contractions_dict):
    def replace(match):
        return contractions_dict[match.group(0)]
    return contractions_re.sub(replace, s)

result = expand_contractions(""surprise"")
print(result)

# The result is ""syou areprise"".

# ---
# Try to fix it below with a word match regex around the replace function call. 

contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))
def expand_contractions(s, contractions_dict=contractions_dict):
    def replace(match):
        return contractions_dict[match.group(0)]
    return contractions_re.sub(r'(?:\W|^)'+replace+'(?!\w)', s)

# On the line above I get ""TypeError: must be str, not function""

result = expand_contractions(""surprise"")
print(result)
</code></pre>
",Vectorization & Embeddings,get exact word match convert word using solution solution wa provided stack overflow syou areprise embedded ru use get exact match key contraction dict code tried embedding exact word match regex around replace function received error typeerror must str function code
Why word2vec doesn&#39;t use regularization?,"<p>ML models with huge number of parameters will tend to overfit (since they have a large variance). In my opinion, <code>word2vec</code> is one such models. One of the ways to reduce the model variance is to apply a regularization technique, which is very common thing for the other embedding models, such as matrix factorization. However, the basic version of <code>word2vec</code> doesn't have any regularization part. Is there a reason for this?</p>
",Vectorization & Embeddings,word vec use regularization ml model huge number parameter tend overfit since large variance opinion one model one way reduce model variance apply regularization technique common thing embedding model matrix factorization however basic version regularization part reason
How can we use the dependency parser output to text embeddings? or Feature extractions from text?,"<p>Knowing the dependencies between various parts of the sentence
can add some information to existing knowledge from raw texts, Now the question is how can we use this to get a good feature representation, which can be fed into classifier such as logistic regression, sim etc. just as TfIdfvectorizer gives us a vector representation, for text documents. I'd like to know what different methods are there to get these kind of representation using output of  dependency parser?</p>
",Vectorization & Embeddings,use dependency parser output text embeddings feature extraction text knowing dependency various part sentence add information existing knowledge raw text question use get good feature representation fed classifier logistic regression sim etc tfidfvectorizer give u vector representation text document like know different method get kind representation using output dependency parser
Measuring the semantic similarity of two strings in c#,"<p>I've been using the Levenstein Distance to measure the similarity of two strings.</p>

<pre><code>int ComputeLevenshteinDistance(string source, string target)
{
    if ((source == null) || (target == null)) return 0;
    if ((source.Length == 0) || (target.Length == 0)) return 0;
    if (source == target) return source.Length;

    int sourceWordCount = source.Length;
    int targetWordCount = target.Length;

    // Step 1
    if (sourceWordCount == 0)
        return targetWordCount;

    if (targetWordCount == 0)
        return sourceWordCount;

    int[,] distance = new int[sourceWordCount + 1, targetWordCount + 1];

    // Step 2
    for (int i = 0; i &lt;= sourceWordCount; distance[i, 0] = i++);
    for (int j = 0; j &lt;= targetWordCount; distance[0, j] = j++);

    for (int i = 1; i &lt;= sourceWordCount; i++)
    {
        for (int j = 1; j &lt;= targetWordCount; j++)
        {
            // Step 3
            int cost = (target[j - 1] == source[i - 1]) ? 0 : 1;

            // Step 4
            distance[i, j] = Math.Min(Math.Min(distance[i - 1, j] + 1, distance[i, j - 1] + 1), distance[i - 1, j - 1] + cost);
        }
    }

    return distance[sourceWordCount, targetWordCount];
}
</code></pre>

<p>But I would like to either modify or write a new code that measures the semantic similarity of two strings with percentage given.</p>

<p>I tried to search some code samples on web, but it was hard to find a simple one that has some semantic similarity measurement functionality.</p>

<p>What is a clear and simple way of doing this?</p>
",Vectorization & Embeddings,measuring semantic similarity two string c using levenstein distance measure similarity two string would like either modify write new code measure semantic similarity two string percentage given tried search code sample web wa hard find simple one ha semantic similarity measurement functionality clear simple way
How can I get a vector after each training iter in word2vec?,"<p>I want to get a vector of words every few iter in <code>word2vec</code>, e.g., I would like to use the model below.</p>

<pre><code>embedding_model = Word2Vec(test_set, size=300, 
                           window=4, workers=6, 
                           iter=300, sg=1, min_count=10)
</code></pre>

<p>In this model, I want to get the 300-dimensional vectors learned for every 50 iterations, because I want to show continuous learning contents in html d3.</p>

<p>How can I do this?</p>
",Vectorization & Embeddings,get vector training iter word vec want get vector word every iter e g would like use model model want get dimensional vector learned every iteration want show continuous learning content html
Creating questions from sentences,"<p>I'm trying to create an algorithm that could transform sentences to questions. This is the code:</p>
<pre><code>def sentence_to_question(arg):
    hverbs = [&quot;is&quot;, &quot;have&quot;, &quot;had&quot;, &quot;was&quot;, &quot;could&quot;, &quot;would&quot;, &quot;will&quot;, &quot;do&quot;, &quot;did&quot;, &quot;should&quot;, &quot;shall&quot;, &quot;can&quot;, &quot;are&quot;]
    words = arg.split(&quot; &quot;)
    zen_sim = (0, &quot;&quot;, &quot;&quot;)
    for hverb in hverbs:
        for word in words:
            similarity = SequenceMatcher(None, word, hverb).ratio()*100
            if similarity &gt; zen_sim[0]:
                zen_sim = (similarity, hverb, word)
    if zen_sim[0] &lt; 30:
        raise ValueError(&quot;unable to create question.&quot;)
    else:
        words.remove(zen_sim[2])
        words = &quot; &quot;.join(words)[0].lower() + &quot; &quot;.join(words)[1:]
        question = &quot;{0} {1}?&quot;.format(zen_sim[1].capitalize(), words)
        return question
</code></pre>
<p><strong>Explanation</strong>:
there is a prepared list of helping verbs, each word of the sentence is compared to the helping verb. The word that has highest similarity helping verb, will be chosen. <a href=""https://docs.python.org/2/library/difflib.html#difflib.SequenceMatcher"" rel=""nofollow noreferrer"">difflib.SequenceMatcher</a>'s <a href=""https://xlinux.nist.gov/dads/HTML/ratcliffObershelp.html"" rel=""nofollow noreferrer"">Ratcliff/Obershep pattern recognition algorithm</a> is used to compare similarity of two strings. Although if the similarity percentage is less than 30%, I believe that there exists very low probability that user can misspell the helping verb by this much, and high probability that helping verb is not existent in the question, thus the question is not recognized. Finally, chosen helping verb is placed in the beginning of string. (I know the &quot;algorithm&quot; requires much more optimization).</p>
<p><strong>Testing on examples</strong>:</p>
<pre><code>&gt;&gt;&gt; sentence_to_question(&quot;Euclid was a prominent mathematician&quot;)
'Was euclid a prominent mathematician?'
&gt;&gt;&gt; sentence_to_question(&quot;Euclid waz a prominent mathematician&quot;) # does work with small typos
'Was euclid a prominent mathematician?'
&gt;&gt;&gt; sentence_to_question(&quot;The company name, logo and slogan are vital elements of the house style of a company and important elements of corporate design&quot;)
'Are the company name, logo and slogan vital elements of the house style of a company and important elements of corporate design?'
</code></pre>
<p>Now as we know, multiple questions can be generated from a single sentence, but currently my &quot;algorithm&quot; can only generate single question. But what would be the most accurate method to generate questions like these? -</p>
<blockquote>
<p><strong>What</strong> are vital elements of the house style of a company and important elements of corporate design?</p>
<p><strong>Who</strong> was Euclid?</p>
</blockquote>
<p>Note the difference between these two questions, first one has <strong>What</strong> as its initial verb, but the second one has <strong>Who</strong>.</p>
<p>I'm not asking for the code (although i would appreciate example), What is the most accurate method for generating multiple questions from sentence?</p>
<p>Thank You!</p>
",Vectorization & Embeddings,creating question sentence trying create algorithm could transform sentence question code explanation prepared list helping verb word sentence compared helping verb word ha highest similarity helping verb chosen difflib sequencematcher ratcliff obershep pattern recognition algorithm used compare similarity two string although similarity percentage le believe exists low probability user misspell helping verb much high probability helping verb existent question thus question recognized finally chosen helping verb placed beginning string know algorithm requires much optimization testing example know multiple question generated single sentence currently algorithm generate single question would accurate method generate question like vital element house style company important element corporate design wa euclid note difference two question first one ha initial verb second one ha asking code although would appreciate example accurate method generating multiple question sentence thank
Gensim Doc2Vec.infer_vector() equivalent in KeyedVector,"<p>I have a working app using <code>doc2vec</code> from <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">gensim</a>. I know the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer""><code>KeyedVector</code></a> is now the recommended approach, and trying to port over however I am not sure what is the equivalent method for the <code>infer_vector</code> method in <code>Doc2Vec</code>?</p>

<p>Or better put, how do I obtain a document vector for an entire document using the <code>KeyedVector</code> model to write to my Annoy model?</p>
",Vectorization & Embeddings,gensim doc vec infer vector equivalent keyedvector working app using gensim know recommended approach trying port however sure equivalent method method better put obtain document vector entire document using model write annoy model
Gensim word embedding training with initial values,"<p>I have a dataset with documents separated into different years, and my objective is to train an embedding model for each year's data, while at the same time, the same word appearing in different years will have similar vector representations. Like this: for word 'compute', its vector in year 1 is</p>

<pre><code>[0.22, 0.33, 0.20]
</code></pre>

<p>and in year 2 it's something around:</p>

<pre><code>[0.20, 0.35, 0.18]
</code></pre>

<p>Is there a way to accomplish this? For example, train the model of year 2 with both initial values (if the word is trained already in year 1, modify its vector) and randomness (if this is a new word for the corpus).</p>
",Vectorization & Embeddings,gensim word embedding training initial value dataset document separated different year objective train embedding model year data time word appearing different year similar vector representation like word compute vector year year something around way accomplish example train model year initial value word trained already year modify vector randomness new word corpus
Is tensorflow embedding_lookup differentiable?,"<p>Some of the tutorials I came across, described using a randomly initialized embedding matrix and then using the <code>tf.nn.embedding_lookup</code> function to obtain the embeddings for the integer sequences. I am under the impression that since the <code>embedding_matrix</code> is obtained through <code>tf.get_variable</code>, the optimizer would add appropriate <strong>ops</strong> for updating it. </p>

<p>What I don't understand is how backpropagation happens through the lookup function which seems to be hard rather than being soft. What is the gradient of the this operation wrt. one of it's input ids?</p>
",Vectorization & Embeddings,tensorflow embedding lookup differentiable tutorial came across described using randomly initialized embedding matrix using function obtain embeddings integer sequence impression since obtained optimizer would add appropriate ops updating understand backpropagation happens lookup function seems hard rather soft gradient operation wrt one input id
Gensim word2vec most_similar filtering by # prefix,"<p>I have a word2vec model trained on twitter. I imported it into gensim using</p>



<pre class=""lang-py prettyprint-override""><code>from gensim.models.keyedvectors import KeyedVectors
word_vectors = KeyedVectors.load_word2vec_format('./twitter.txt', binary=False)  
</code></pre>

<p>I would like to use a function similar to this one:</p>

<pre class=""lang-py prettyprint-override""><code>word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])
</code></pre>

<p>to show the most similar words, but I want to restrict the results to words that start with a hashtag.
Can somebody please give explain how I can accomplish this?</p>
",Vectorization & Embeddings,gensim word vec similar filtering prefix word vec model trained twitter imported gensim using would like use function similar one show similar word want restrict result word start hashtag somebody please give explain accomplish
What is the most effective and efficient method of encoding text at the char level for input into a tensorflow model?,"<p>What would be the most effective and efficient method of character level input into a Tensorflow model (yes, char level input is necessary).</p>

<p>For a given string ""hello"", 
and a char embedding ""abcdefghijklmnop..."" (~150 chars omitted for brevity), I have tried the following methods:</p>

<p>1) direct translation example: </p>

<pre><code>[7,4,11,14] 
observation_space_shape = (max_length, char_embedding_length)
</code></pre>

<p>2) one hot encoding example:</p>

<pre><code>[`[0.0,0.0,0.0,0.0,0.0,1,0.0 ...], [...], ...] 

observation_space_shape = (max_length, char_embedding_length, 1)`
</code></pre>

<p>Which method would be best for achieving efficient and effective char level encoding on large textual inputs (with numerous characters), or is there a better alternative to the aforementioned solutions I have presented?</p>
",Vectorization & Embeddings,effective efficient method encoding text char level input tensorflow model would effective efficient method character level input tensorflow model yes char level input necessary given string hello char embedding abcdefghijklmnop char omitted brevity tried following method direct translation example one hot encoding example method would best achieving efficient effective char level encoding large textual input numerous character better alternative aforementioned solution presented
Doc2vec: model.docvecs is only of length 10,"<p>I am trying doc2vec for 600000 rows of sentences and my code is below:</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(size= 100, min_count = 5,window=4, iter = 50, workers=cores)
model.build_vocab(res) 
model.train(res, total_examples=model.corpus_count, epochs=model.iter)

#len(res) = 663406

#length of unique words 15581
print(len(model.wv.vocab))

#length of doc vectors is 10
len(model.docvecs)

# each of length 100
len(model.docvecs[1])
</code></pre>

<p>How do I interpret this result? why is the length of vector only 10 with each of size 100? when the length of 'res' is 663406, it does not make sense. I know something is wrong here.</p>

<p>In <a href=""https://stackoverflow.com/questions/37196520/understanding-the-output-of-doc2vec-from-gensim-package?rq=1"">Understanding the output of Doc2Vec from Gensim package</a>, they mention that the length of docvec is determined by 'size' which is not clear. </p>
",Vectorization & Embeddings,doc vec model docvecs length trying doc vec row sentence code interpret result length vector size length doe make sense know something wrong href output doc vec gensim package mention length docvec determined size clear
How to get similar words related to one word?,"<p>I am trying to solve a nlp problem where i have a dict of words like :</p>

<pre><code>list_1={'phone':'android','chair':'netflit','charger':'macbook','laptop','sony'}
</code></pre>

<p>Now if input is 'phone' i can easily use 'in' operator to get the description of phone and its data by key but problem is if input is something like  'phones' or 'Phones' .</p>

<p>I want if i input  'phone' then i get words like </p>

<pre><code>'phone' ==&gt; 'Phones','phones','Phone','Phone's','phone's' 
</code></pre>

<p>I don't know which word2vec i can use and which nlp module can provide solution like this.</p>

<p>second issue is if i give a word 'Dog' can i get words like 'Puppy','Kitty','Dog','dog' etc ?</p>

<p>I tried something like this but its giving synonyms :</p>

<pre><code>from nltk.corpus import wordnet as wn
for ss in wn.synsets('phone'): # Each synset represents a diff concept.
    print(ss)
</code></pre>

<p>but its returning :</p>

<pre><code>Synset('telephone.n.01')
Synset('phone.n.02')
Synset('earphone.n.01')
Synset('call.v.03')
</code></pre>

<p>Instead i wanted :</p>

<pre><code>'phone' ==&gt; 'Phones','phones','Phone','Phone's','phone's' 
</code></pre>
",Vectorization & Embeddings,get similar word related one word trying solve nlp problem dict word like input phone easily use operator get description phone data key problem input something like phone phone want input phone get word like know word vec use nlp module provide solution like second issue give word dog get word like puppy kitty dog dog etc tried something like giving synonym returning instead wanted
Can I export the embedding matrix of words in tensorflow?,"<pre><code>def word_embedding(shape, dtype=tf.float32, name='word_embedding'):
  with tf.device('/cpu:0'), tf.variable_scope(name):
    return tf.get_variable('embedding', shape, dtype=dtype, initializer=tf.random_normal_initializer(stddev=0.1), trainable=True,partitioner=tf.fixed_size_partitioner(20))
embedding = word_embedding([vocab_size, embed_size])
inputs_embedding = tf.contrib.layers.embedding_lookup_unique(embedding, inputs)
</code></pre>

<p>Here is my code and the <code>embedding</code> is the variable for word to look up their own embedding vector. </p>

<p>I have trained the embedding matrix and I want to extract it from the model saved. The model also contain other parameters for example the neural networks above embeddings. Can I implement it?</p>
",Vectorization & Embeddings,export embedding matrix word tensorflow code variable word look embedding vector trained embedding matrix want extract model saved model also contain parameter example neural network embeddings implement
NLP - Embeddings selection of `start` and `end` of sentence tokens,"<p>Suppose we're training a neural network model to learn the mapping from the following input to output, where the output is <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Name Entity</a> (NE).</p>

<p><strong>Input</strong>: EU rejects German call to boycott British lamb .</p>

<p><strong>Output</strong>: ORG O MISC O O O MISC O O</p>

<p>A sliding window is created to capture the context information and its outcomes are fed into the training model as model_input. The sliding window generates results as following:</p>

<pre><code> [['&lt;s&gt;', '&lt;s&gt;', 'EU', 'rejects', 'German'],\
 ['&lt;s&gt;', 'EU', 'rejects', 'German', 'call'],\
 ['EU', 'rejects', 'German', 'call', 'to'],\
 ['rejects', 'German', 'call', 'to', 'boycott'],\
 ['German', 'call', 'to', 'boycott', 'British'],\
 ['call', 'to', 'boycott', 'British', 'lamb'],\
 ['to', 'boycott', 'British', 'lamb', '.'],\
 ['boycott', 'British', 'lamb', '.', '&lt;/s&gt;'],\
 ['British', 'lamb', '.', '&lt;/s&gt;', '&lt;/s&gt;']]
</code></pre>

<p><code>&lt;s&gt;</code> represents start of sentence token and <code>&lt;/s&gt;</code> represents end of sentence token, and every sliding window corresponds to one NE in output. </p>

<p>To process these tokens, a pre-trained embedding model is used converting words to vectors (e.g., Glove), but those pre-trained models do not include tokens such as <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code>. I think random initialization for <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> won't be a good idea here, because the scale of such random results might not be consistent with other Glove embeddings.</p>

<p><strong>Question</strong>:
What suggestions of setting up embeddings for <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> and why?</p>
",Vectorization & Embeddings,nlp embeddings selection sentence token suppose training neural network model learn mapping following input output output name entity ne input eu reject german call boycott british lamb output org misc misc sliding window created capture context information outcome fed training model model input sliding window generates result following represents start sentence token represents end sentence token every sliding window corresponds one ne output process token pre trained embedding model used converting word vector e g glove pre trained model include token think random initialization good idea scale random result might consistent glove embeddings question suggestion setting embeddings
What does a weighted word embedding mean?,"<p>In the <a href=""http://www.aclweb.org/anthology/S17-2100"" rel=""noreferrer"">paper</a> that I am trying to implement, it says,</p>

<blockquote>
  <p>In this work, tweets were modeled using three types of text
  representation. The first one is a bag-of-words model weighted by
  tf-idf (term frequency
  - inverse document frequency) (Section
  2.1.1). The second represents a sentence by averaging the word embeddings of all words (in the sentence) and the third represents a
  sentence by averaging the weighted word embeddings of all words, the
  weight of a word is given by tf-idf (Section
  2.1.2).</p>
</blockquote>

<p>I am not sure about the <em>third representation</em> which is mentioned as the weighted word embeddings which is using the weight of a word is given by tf-idf. I am not even sure if they can used together. </p>
",Vectorization & Embeddings,doe weighted word embedding mean paper trying implement say work tweet modeled using three type text representation first one bag word model weighted tf idf term frequency inverse document frequency section second represents sentence averaging word embeddings word sentence third represents sentence averaging weighted word embeddings word weight word given tf idf section sure third representation mentioned weighted word embeddings using weight word given tf idf even sure used together
TfIDf Vectorizer weights,"<p>Hi I have a lemmatized text in the format as shown by <code>lemma</code>. I want to get TfIdf score for each word this is the function that I wrote:</p>

<pre><code>import numpy as np
import pandas as pd

from sklearn.feature_extraction.text import TfidfVectorizer

lemma=[""'Ah"", 'yes', u'say', 'softly', 'Harry', 
       'Potter', 'Our', 'new', 'celebrity', 'You', 
       'learn', 'subtle', 'science', 'exact', 'art', 
       'potion-making', u'begin', 'He', u'speak', 'barely', 
       'whisper', 'caught', 'every', 'word', 'like', 
       'Professor', 'McGonagall', 'Snape', 'gift', 
       u'keep', 'class', 'silent', 'without', 'effort', 
       'As', 'little', 'foolish', 'wand-waving', 'many', 
       'hardly', 'believe', 'magic', 'I', 'dont', 'expect', 'really', 
       'understand', 'beauty']

def Tfidf_Vectorize(lemmas_name):

    vect = TfidfVectorizer(stop_words='english',ngram_range=(1,2))
    vect_transform = vect.fit_transform(lemmas_name)    

    # First approach of creating a dataframe of weight &amp; feature names

    vect_score = np.asarray(vect_transform.mean(axis=0)).ravel().tolist()
    vect_array = pd.DataFrame({'term': vect.get_feature_names(), 'weight': vect_score})
    vect_array.sort_values(by='weight',ascending=False,inplace=True)

    # Second approach of getting the feature names

    vect_fn = np.array(vect.get_feature_names())    
    sorted_tfidf_index = vect_transform.max(0).toarray()[0].argsort()

    print('Largest Tfidf:\n{}\n'.format(vect_fn[sorted_tfidf_index[:-11:-1]]))

    return vect_array

tf_dataframe=Tfidf_Vectorize(lemma)
print(tf_dataframe.iloc[:5,:])
</code></pre>

<p>The output I am getting by:</p>

<pre><code>print('Largest Tfidf:\n{}\n'.format(vect_fn[sorted_tfidf_index[:-11:-1]]))
</code></pre>

<p>is</p>

<pre><code>Largest Tfidf: 
[u'yes' u'fools' u'fury' u'gale' u'ghosts' u'gift' u'glory' u'glow' u'good'
 u'granger']
</code></pre>

<p>The result of <code>tf_dataframe</code></p>

<pre><code>       term  weight
261  snape       0.027875
238  say         0.022648
211  potter      0.013937
181  mind        0.010453
123  harry       0.010453
60   dark        0.006969
75   dumbledore  0.006969
311  voice       0.005226
125  head        0.005226
231  ron         0.005226
</code></pre>

<p>Shouldn't both approaches lead to the same result of top features? I just want to calculate the tfidf scores and get the top 5 features/weight. What am i doing wrong?</p>
",Vectorization & Embeddings,tfidf vectorizer weight hi lemmatized text format shown want get tfidf score word function wrote output getting result approach lead result top feature want calculate tfidf score get top feature weight wrong
Improving Gensim Doc2vec results,"<p>I tried to apply doc2vec on 600000 rows of sentences: Code as below:</p>

<pre><code>from gensim import models
model = models.Doc2Vec(alpha=.025, min_alpha=.025, min_count=1, workers = 5)
model.build_vocab(res)
token_count = sum([len(sentence) for sentence in res])
token_count

%%time
for epoch in range(100):
    #print ('iteration:'+str(epoch+1))
    #model.train(sentences)
    model.train(res, total_examples = token_count,epochs = model.iter)
    model.alpha -= 0.0001  # decrease the learning rate`
    model.min_alpha = model.alpha  # fix the learning rate, no decay
</code></pre>

<p>I am getting very poor results with the above implementation. 
the change I made apart from what was suggested in the tutorial was change the below line:</p>

<pre><code>  model.train(sentences)
</code></pre>

<p>As:</p>

<pre><code> token_count = sum([len(sentence) for sentence in res])
model.train(res, total_examples = token_count,epochs = model.iter)
</code></pre>
",Vectorization & Embeddings,improving gensim doc vec result tried apply doc vec row sentence code getting poor result implementation change made apart wa suggested tutorial wa change line
Cosine similarity - ValueError: too many values to unpack (expected 2),"<p>I've read through a handful of similar posts, but haven't found an exact duplicate for my issue.</p>

<p>I'm working on calculating cosine similarity for tfidf values, and am getting <code>ValueError: too many values to unpack (expected 2)</code></p>

<p>I understand that this is basically saying that 2 values were expected and there are more than 2 values being fed through, resulting in an error.  But I'm not sure how to move forward from here.</p>

<p>Here's the full code</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

def find_similar(tfidf_matrix, index, top_n = 5):
    cosine_similarities = linear_kernel(tfidf_matrix[index:index+1], tfidf_matrix).flatten()
    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]
    return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]

corpus = []
for doc in nlp.pipe(df['TIP_all_txt'].astype('unicode').values, batch_size=9845,
                        n_threads=3):
    if doc.is_parsed:
        corpus.append([n.text for n in doc if not n.is_punct and not n.is_stop and not n.is_space])
    else:
        corpus.append(None)

tf = TfidfVectorizer(analyzer='word', ngram_range=(1,3), min_df = 0, stop_words = 'english')
tfidf_matrix =  tf.fit_transform([content for doc, doc in corpus])

for me_index, item in enumerate(corpus):
        similar_documents =  [(corpus[index], score) for index, score in find_similar(tfidf_matrix, me_index)]
        me = corpus[me_index]

        document_id = me[0].split(""/"")[1].split(""."")[0]

        for ((raw_similar_document_id, title), score) in similar_documents:
            similar_document_id = raw_similar_document_id.split(""/"")[1].split(""."")[0]
            writer.writerow([document_id, me[1], similar_document_id, title, score])
</code></pre>

<p>and error message:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-75-8522d91e96f8&gt; in &lt;module&gt;()
      8 
      9 tf = TfidfVectorizer(analyzer='word', ngram_range=(1,3), min_df = 0, stop_words = 'english')
---&gt; 10 tfidf_matrix =  tf.fit_transform([content for doc, doc in corpus])
     11 
     12 for me_index, item in enumerate(corpus):

&lt;ipython-input-75-8522d91e96f8&gt; in &lt;listcomp&gt;(.0)
      8 
      9 tf = TfidfVectorizer(analyzer='word', ngram_range=(1,3), min_df = 0, stop_words = 'english')
---&gt; 10 tfidf_matrix =  tf.fit_transform([content for doc, doc in corpus])
     11 
     12 for me_index, item in enumerate(corpus):

ValueError: too many values to unpack (expected 2)
</code></pre>
",Vectorization & Embeddings,cosine similarity valueerror many value unpack expected read handful similar post found exact duplicate issue working calculating cosine similarity tfidf value getting understand basically saying value expected value fed resulting error sure move forward full code error message
How to use Keras LSTM with word embeddings to predict word id&#39;s,"<p>I have problems understanding how to get the correct output when using word embeddings in Keras. My settings are as follows:</p>

<ul>
<li><p>My input are batches of shape <code>(batch_size, sequence_length)</code>. Each row
in a batch represents one sentence, the word are represented by word id's. The
sentences are padded with zeros such that all are of the same length.
For example a <code>(3,6)</code> input batch might look like: <code>np.array([[135600],[174580],[138272]])</code></p></li>
<li><p>My targets are given by the input batch shifted one step to the right.
So for each input word I want to predict the next word: <code>np.array([[356000],[745800],[382720]])</code></p></li>
<li><p>I feed such an input batch into the Keras embedding layer. My embedding
size is 100, so the output will be a 3D tensor of shape <code>(batch_size,
sequence_length, embedding_size)</code>. So in the little example its <code>(3,6,100)</code></p></li>
<li><p>This 3D batch is fed into an LSTM layer</p></li>
<li><p>The output of the LSTM layer is fed into a Dense layer with
<code>(sequence_length)</code> output neurons having a softmax activation
function. So the shape of the output will be like the shape of the input namely <code>(batch_size, sequence_length)</code></p></li>
<li><p>As a loss I am using the categorical crossentropy between the input and target batch</p></li>
</ul>

<p>My question:</p>

<p>The output batch will contain probabilities because of the
softmax activation function. But what I want is the network to predict
integers such that the output fits the target batch of integers.
How can I ""decode"" the output such that I know which word the network is predicting? Or do I have to construct the network differently?</p>

<p><strong>Edit 1:</strong></p>

<p>I have changed the output and target batches from 2D arrays to 3D tensors. So instead of using a target batch of size <code>(batch_size, sequence_length)</code> with integer id's I am now using a one-hot encoded 3D target tensor <code>(batch_size, sequence_length, vocab_size)</code>. To get the same format as an output of the network, I have changed the network to output sequences (by setting <code>return_sequences=True</code> in the LSTM layer). Further, the number of output neurons was changed to <code>vocab_size</code> such that the output layer now produces a batch of size <code>(batch_size, sequence_length, vocab_size)</code>.
With this 3D encoding I can get the predicted word id using <code>tf.argmax(outputs, 2)</code>. This approach seems to work for the moment but I would still be interested whether it's possible to keep the 2D targets/outputs</p>
",Vectorization & Embeddings,use kera lstm word embeddings predict word id problem understanding get correct output using word embeddings kera setting follows input batch shape row batch represents one sentence word represented word id sentence padded zero length example input batch might look like target given input batch shifted one step right input word want predict next word feed input batch kera embedding layer embedding size output tensor shape little example batch fed lstm layer output lstm layer fed dense layer output neuron softmax activation function shape output like shape input namely loss using categorical crossentropy input target batch question output batch contain probability softmax activation function want network predict integer output fit target batch integer decode output know word network predicting construct network differently edit changed output target batch array tensor instead using target batch size integer id using one hot encoded target tensor get format output network changed network output sequence setting lstm layer number output neuron wa changed output layer produce batch size encoding get predicted word id using approach seems work moment would still interested whether possible keep target output
Word embedding Visualization using TSNE not clear,"<p>I have downloaded pre-trained model of word embeddings from <a href=""http://clic.cimec.unitn.it/composes/semantic-vectors.html"" rel=""nofollow noreferrer"">Word Embeddings by M. Baroni et al.</a> 
I want to visualize embeddings of words present in sentences. I have two sentences:</p>

<pre><code>sentence1 = ""Four people died in an accident.""

sentence2 = ""4 men are dead from a collision""
</code></pre>

<p>I have function to load the embeddings file from above link:</p>

<pre><code>def load_data(FileName = './EN-wform.w.5.cbow.neg10.400.subsmpl.txt'):

    embeddings = {}
    file = open(FileName,'r')
    i = 0
    print ""Loading word embeddings first time""
    for line in file:
        # print line

        tokens = line.split('\t')

        #since each line's last token content '\n'
        # we need to remove that
        tokens[-1] = tokens[-1].strip()

        #each line has 400 tokens
        for i in xrange(1, len(tokens)):
            tokens[i] = float(tokens[i])

        embeddings[tokens[0]] = tokens[1:-1]
    print ""finished""
    return embeddings

e = load_data()
</code></pre>

<p>From both the sentences, I compute <strong>lemmas</strong> of words and <strong>ignore stopwords and punctuations</strong>, so now my sentences becomes:</p>

<pre><code>sentence1 = ['Four', 'people', 'died', 'accident']
sentence2 = ['4', 'men', 'dead', 'collision']
</code></pre>

<p>Now, when I try to visualize the embeddings using TSNE(t-distributed stochastic neighbor embedding), I first store labels and tokens for each sentence:</p>

<pre><code>#for sentence store labels and embeddings in list
# tokens contains vector of 400 dimensions for each label
labels1 = []
tokens1 = []
for i in sentence1:
    if i in e:
        labels1.append(i)
        tokens1.append(e[i])
    else:
        print i

labels2 = []
tokens2 = []
for i in sentence2:
    if i in e:
        labels2.append(i)
        tokens2.append(e[i])
    else:
        print i
</code></pre>

<h1>For TSNE</h1>

<pre><code>tsne_model = TSNE(perplexity=40, n_components=2, init='random', n_iter=2000, random_state=23)
# fit transform for tokens of both sentences
new_values = tsne_model.fit_transform(tokens1)
new_values1 = tsne_model.fit_transform(tokens2)

#Plot values
x = []
y = []
x1 = []
y1 = []

for value in new_values:
    x.append(value[0])
    y.append(value[1])

for value in new_values1:
    x1.append(value[0])
    y1.append(value[1])


plt.figure(figsize=(10, 10)) 

for i in range(len(x)):
    plt.scatter(x[i],y[i])
    plt.annotate(labels[i],
                 xy=(x[i], y[i]),
                 xytext=(5, 2),
                 textcoords='offset points',
                 ha='right',
                 va='bottom')

for i in range(len(x1)):
    plt.scatter(x1[i],y1[i])
    plt.annotate(labels[i],
                 xy=(x1[i], y1[i]),
                 xytext=(5, 2),
                 textcoords='offset points',
                 ha='right',
                 va='bottom')

plt.show()
</code></pre>

<p><a href=""https://i.sstatic.net/bATVp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bATVp.png"" alt=""Plot of labels in 2-dimension""></a></p>

<p>My <strong>question</strong> is that, why synonyms words such as ""collision"" and ""accident"", ""people"" and ""people"" have different co-ordinates? if words are same/synonyms, shouldn't they be closer? </p>

<p>distances = euclidean_distances(tokens1)
  # returns shape (8,8)</p>
",Vectorization & Embeddings,word embedding visualization using tsne clear downloaded pre trained model word embeddings word embeddings baroni et al want visualize embeddings word present sentence two sentence function load embeddings file link sentence compute lemma word ignore stopwords punctuation sentence becomes try visualize embeddings using tsne distributed stochastic neighbor embedding first store label token sentence tsne question synonym word collision accident people people different co ordinate word synonym closer distance euclidean distance token return shape
Tensorflow raw_rnn retrieve tensor of shape BATCH x DIM from embedding matrix,"<p>I am implementing encoder-decoder lstm, where I have to do custom computation at each step of the encoder. So, I am using <code>raw_rnn</code>. However, I am facing a problem accessing an element from the embeddings which is shaped  as <code>Batch x Time steps x Embedding dimensionality</code> at time step <code>time</code>.</p>

<p>Here is my setup:</p>

<pre><code>import tensorflow as tf
import numpy as np

batch_size, max_time, input_embedding_size = 5, 10, 16
vocab_size, num_units = 50, 64

encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')
encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')

embeddings = tf.Variable(tf.random_uniform([vocab_size + 2, input_embedding_size], -1.0, 1.0),
                         dtype=tf.float32, name='embeddings')
encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)

cell = tf.contrib.rnn.LSTMCell(num_units)
</code></pre>

<p>The main part:</p>

<pre><code>with tf.variable_scope('ReaderNetwork'):
def loop_fn_initial():
    init_elements_finished = (0 &gt;= encoder_inputs_length)
    init_input = cell.zero_state(batch_size, tf.float32)
    init_cell_state = None
    init_cell_output = None
    init_loop_state = None
    return (init_elements_finished, init_input,
            init_cell_state, init_cell_output, init_loop_state)


def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):
    def get_next_input():
        # **TODO** read tensor of shape BATCH X EMBEDDING_DIM from encoder_inputs_embedded
        #  which has shape BATCH x TIME_STEPS x EMBEDDING_DIM

    elements_finished = (time &gt;= encoder_inputs_length)
    finished = tf.reduce_all(elements_finished)  # boolean scalar
    input_val = tf.cond(finished,
                        true_fn=lambda: tf.zeros([batch_size, input_embedding_size]), false_fn=get_next_input)
    state = previous_state
    output = previous_output
    loop_state = None
    return elements_finished, input_val, state, output, loop_state


def loop_fn(time, previous_output, previous_state, previous_loop_state):
    if previous_state is None:  # time = 0
        assert previous_output is None and previous_state is None
        return loop_fn_initial()
    return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)
</code></pre>

<p>The running part:</p>

<pre><code>reader_loop = loop_fn
encoder_outputs_ta, encoder_final_state, _ = tf.nn.raw_rnn(cell, loop_fn=reader_loop)
outputs = encoder_outputs_ta.stack()

def next_batch():
    return {
        encoder_inputs: np.random.random((batch_size, max_time)),
        encoder_inputs_length: [max_time] * batch_size
    }

init = tf.global_variables_initializer()
with tf.Session() as s:
    s.run(init)
    outs = s.run([outputs], feed_dict=next_batch())
    print len(outs), outs[0].shape
</code></pre>

<p><strong>Question:</strong> How to access part of the embeddings at a time step and return a tensor of shape <code>batch x embedding dim</code>? See function <code>get_next_input</code> within <code>loop_fn_transition</code>.</p>

<p>Thank you.</p>
",Vectorization & Embeddings,tensorflow raw rnn retrieve tensor shape batch x dim embedding matrix implementing encoder decoder lstm custom computation step encoder using however facing problem accessing element embeddings shaped time step setup main part running part question access part embeddings time step return tensor shape see function within thank
How&#39;s it even possible to use softmax for word2vec?,"<p>How is it possible to use softmax for word2vec? I mean softmax outputs probabilities of all classes which sum up to <code>1</code>, e.g. <code>[0, 0.1, 0.8, 0.1]</code>. But if my label is, for example <code>[0, 1, 0, 1, 0]</code> (multiple correct classes), then it is impossible for softmax to output the correct value?</p>

<p>Should I use softmax instead? Or am I missing something?</p>
",Vectorization & Embeddings,even possible use softmax word vec possible use softmax word vec mean softmax output probability class sum e g label example multiple correct class impossible softmax output correct value use softmax instead missing something
Can we feed seq2seq with word2vec?,"<p>I have prepared dataset for training seq2seq.(one file contains questions and another has answers).
Let's say I have 1000 questions(each questions are composed of 15 words). After applying word2vec(word length 30), the shape of questions would be  (1000,15,30). 
I tried to train the network after applying  </p>

<pre><code> question_list.reshape((1000,15,30,1))
</code></pre>

<p>And applied </p>

<pre><code>model.fit(questions, answers, nb_epoch=5, batch_size=32)
</code></pre>

<p>Then I got an error </p>

<pre><code>ValueError: Error when checking input: expected input_4 to have 3 dimensions, but got array with shape (1000, 15, 30, 1)
</code></pre>

<p>This means, each word should be represented as a scholar(not as 30 dimensional vector)? If so, there's no point for applying word embedding using word2vec.....</p>
",Vectorization & Embeddings,feed seq seq word vec prepared dataset training seq seq one file contains question another ha answer let say question question composed word applying word vec word length shape question would tried train network applying applied got error mean word represented scholar dimensional vector point applying word embedding using word vec
How word embeddings work for word similarity?,"<p>I am trying to apply word2vec/doc2vec to find similar sentences. First consider word2vec for word similarity. What I understand is, CBOW can be used to find most suitable word given a context, whereas Skip-gram is used to find the context given some word, so in both cases, I am getting words that co-occur frequently. But how does it work to find similar words? My intuition is, since similar words tend to occur in similar contexts, the words similarity is actually measured from the similarity among contextual/co-occuring words. In the neural net, when the vector representation for some word at the hidden layer is passed through to the output layer, it outputs probabilities of co-occuring words. So, the co-occuring words influence the vectors of some words, and since similar words have similar set of co-occuring words, their vector representations are also similar. To find the similarity, we need to extract the hidden layer weights (or vectors) for each word and measure their similarities. Do I understand it correctly?</p>

<p>Finally, what is a good way to find tweet text (full sentence) similarity using word2vec/doc2vec?</p>
",Vectorization & Embeddings,word embeddings work word similarity trying apply word vec doc vec find similar sentence first consider word vec word similarity understand cbow used find suitable word given context whereas skip gram used find context given word case getting word co occur frequently doe work find similar word intuition since similar word tend occur similar context word similarity actually measured similarity among contextual co occuring word neural net vector representation word hidden layer passed output layer output probability co occuring word co occuring word influence vector word since similar word similar set co occuring word vector representation also similar find similarity need extract hidden layer weight vector word measure similarity understand correctly finally good way find tweet text full sentence similarity using word vec doc vec
How to use final_embeddings?,"<p>From udacity notebook exercise;  After embeddings were trained,  I'm trying to get all related words from an input word and I'm getting wierd results.
Is the code below correct?</p>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb"" rel=""nofollow noreferrer"">udacity exercise</a></p>

<pre><code>final_embeddings = normalized_embeddings.eval()
word='history'
nearest = (-final_embeddings[dictionary[word], :]).argsort()[1:9]
for idx in range(len(nearest)):
    print reverse_dictionary[nearest[idx]]
</code></pre>
",Vectorization & Embeddings,use final embeddings udacity notebook exercise embeddings trained trying get related word input word getting wierd result code correct udacity exercise
Implement TFlearn imdb lstm example by tensorflow,"<p>I'm implementing tflearn's lstm imdb <a href=""https://github.com/tflearn/tflearn/blob/master/examples/nlp/lstm.py"" rel=""nofollow noreferrer"">example</a> by tensorflow. </p>

<p>I used the same dataset, architecture and hyper-parameters (embedding size, max length of sentence and so on) as tflearn model do, but my model's performance is poor than the tflearn example(after 10 epochs, my model got about 52% accuracy while the example got near 80% ).</p>

<p>I'd appreciated it a lot if you can give me some advice to achieve the appropriate performance of the example.</p>

<p>Below is my code:</p>

<pre><code>import tensorflow as tf
from tflearn.data_utils import to_categorical, pad_sequences
from tflearn.datasets import imdb
from tensorflow.contrib.rnn import BasicLSTMCell
import time



n_class = 2
n_words = 10000
EMBEDDING_SIZE = 128
HIDDEN_SIZE = 128
MAX_LENGTH = 100
lr = 1e-3

epoch = 10
TRAIN_SIZE = 22500
validation_size = 2500
batch_size = 128
KP = 0.8

# IMDB Dataset loading
train, test, _ = imdb.load_data(path='imdb.pkl', n_words=n_words,
                                valid_portion=0.1, sort_by_len=False)
trainX, trainY = train
validationX, validationY = test
testX, testY = _


# Data preprocessing
# Sequence padding
trainX = pad_sequences(trainX, maxlen=MAX_LENGTH, value=0.)
validationX = pad_sequences(validationX, maxlen=MAX_LENGTH, value=0.)
testX = pad_sequences(testX, maxlen=MAX_LENGTH, value=0.)

# Converting labels to binary vectors
trainY = to_categorical(trainY, n_class)
validationY = to_categorical(validationY, n_class)
testY = to_categorical(testY, n_class)

graph = tf.Graph()
with graph.as_default():
    # input
    text = tf.placeholder(tf.int32, [None, MAX_LENGTH])
    labels = tf.placeholder(tf.float32, [None, n_class])
    keep_prob = tf.placeholder(tf.float32)

    embeddings_var = tf.Variable(tf.truncated_normal([n_words, EMBEDDING_SIZE]), trainable=True)
    text_embedded = tf.nn.embedding_lookup(embeddings_var, text)

    print(text_embedded.shape)  # [batch_size, length, embedding_size]
    word_list = tf.unstack(text_embedded, axis=1)

    cell = BasicLSTMCell(HIDDEN_SIZE)
    dropout_cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)
    outputs, encoding = tf.nn.static_rnn(dropout_cell, word_list, dtype=tf.float32)

    logits = tf.layers.dense(outputs[-1], n_class, activation=None)

    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))
    optimizer = tf.train.AdamOptimizer(lr).minimize(loss)

    prediction = tf.argmax(logits, 1)
    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(labels, 1)), tf.float32))


train_steps = epoch * TRAIN_SIZE // batch_size + 1
print(""Train steps: "", train_steps)


with tf.Session(graph=graph) as sess:
    tf.global_variables_initializer().run()
    print(""Initialized!"")
    s = time.time()
    offset = 0

    for step in range(train_steps):
        offset = (offset * step) % (TRAIN_SIZE - batch_size)
        batch_text = trainX[offset: offset + batch_size, :]
        batch_label = trainY[offset: offset + batch_size, :]
        fd = {text: batch_text, labels: batch_label, keep_prob: KP}
        _, l, acc = sess.run([optimizer, loss, accuracy], feed_dict=fd)

        if step % 100 == 0:
            print(""Step: %d  loss: %f  accuracy: %f"" % (step, l, acc))

        if step % 500 == 0:
            v_l, v_acc = sess.run([loss, accuracy], feed_dict={
                text: validationX,
                labels: validationY,
                keep_prob: 1.0
            })
            print(""------------------------------------------------"")
            print(""Validation:  step: %d  loss: %f  accuracy: %f"" % (step, v_l, v_acc))
            print(""------------------------------------------------"")
    print(""Training finished, time consumed:"", time.time() - s, "" s"")
    print(""Test accuracy: %f"" % accuracy.eval(feed_dict={
        text: testX,
        labels: testY,
        keep_prob: 1.0
    }))
</code></pre>
",Vectorization & Embeddings,implement tflearn imdb lstm example tensorflow implementing tflearn lstm imdb example tensorflow used dataset architecture hyper parameter embedding size max length sentence tflearn model model performance poor tflearn example epoch model got accuracy example got near appreciated lot give advice achieve appropriate performance example code
language modeling in tensorflow - how to tie embedding and softmax weights,"<p>As suggested by recent language modeling papers, I want to use weight tying in my RNN language model. That is, I want to share the weights between the embedding and softmax layer. However, I am not sure how this can be done in TensorFlow.</p>

<p>My network receives inputs of shape <code>(batch_size, sequence_length)</code>. The embedding matrix has shape <code>(vocab_size, embedding_size)</code> and is created as follows (I am using pre-trained word2vec embeddings):</p>

<pre><code>        with tf.variable_scope('embedding'):
            self.embedding_matrix = tf.Variable(tf.constant(0.0, shape=[self.vocab_size, self.embd_size]), trainable=False, name='embedding')
            self.embed_placeholder = tf.placeholder(tf.float32, [self.vocab_size, self.embd_size])
            self.embed_init = self.embedding_matrix.assign(self.embed_placeholder)
</code></pre>

<p>The logits are computed as follows:</p>

<pre><code>            output, self.final_state = tf.nn.dynamic_rnn(
                cell,
                inputs=self.inputs,
                initial_state=self.init_state)

            self.output_flat = tf.reshape(output, [-1, cell.output_size])
            softmax_w = tf.get_variable(""softmax_w"", [self.n_hidden, self.vocab_size], dtype=tf.float32)

            softmax_b = tf.get_variable(""softmax_b"", [self.vocab_size], dtype=tf.float32)
            logits = tf.nn.xw_plus_b(self.output_flat, softmax_w, softmax_b)
            # Reshape logits to be a 3-D tensor
            self.logits = tf.reshape(logits, [self.batch_size, self.seq_length, self.vocab_size])
</code></pre>

<p>My questions are:</p>

<ol>
<li>The matrix that has to be changed to using the embeddings weights is <code>softmax_w</code>, correct?</li>
<li><code>softmax_w</code> has shape <code>(n_hidden, vocab_size)</code>. How does that fit the size of the embedding matrix? Or do I have to ensure that n_hidden = embedding_size?</li>
<li>How can I reuse the embedding weights in TensorFlow? I know that I have to use <code>reuse=True</code> in the variable_scope.</li>
</ol>
",Vectorization & Embeddings,language modeling tensorflow tie embedding softmax weight suggested recent language modeling paper want use weight tying rnn language model want share weight embedding softmax layer however sure done tensorflow network receives input shape embedding matrix ha shape created follows using pre trained word vec embeddings logits computed follows question matrix ha changed using embeddings weight correct ha shape doe fit size embedding matrix ensure n hidden embedding size reuse embedding weight tensorflow know use variable scope
How to transform multiple features in a PipeLine using FeatureUnion?,"<p>I have a pandas data frame that contains information about messages sent by user.
For my model, I'm interested in predicting missing recipients of a message i,e given recipients A,B,C of a message I want to predict who else should have been part of the recipients.</p>

<p>I'm doing multi-label classification using OneVsRestClassifier and LinearSVC.
For features, I want to use the recipients of the message. subject and body.</p>

<p>Since recipients is a list of users, I want to transform that column using MultiLabelBinarizer. For Subject and Body, I want to use TFIDF</p>

<p>My input pickle file has data as follows: All values are strings except recipients which is a set()</p>

<pre><code>[[message_id,sent_time,subject,body,set(recipients),message_type, is_sender]]
</code></pre>

<p>I'm using feature union with custom transformers in the pipeline to achieve this as follows. </p>

<pre><code>from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.base import TransformerMixin, BaseEstimator
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.svm import SVC, LinearSVC
import pickle
import pandas as pd
import numpy as np

if __name__ == ""__main__"":
class ColumnSelector(BaseEstimator, TransformerMixin):
    def __init__(self, column):
        self.column = column

    def fit(self, X, y=None, **fit_params):
        return self

    def transform(self, X, y=None, **fit_params):
        return X[self.column]

class MultiLabelTransformer(BaseEstimator, TransformerMixin):

    def __init__(self, column):
        self.column = column

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        mlb = MultiLabelBinarizer()
        return mlb.fit_transform(X[self.column])

pipeline = Pipeline([
    ('features', FeatureUnion([
        ('subject_tfidf', Pipeline([
                    ('selector', ColumnSelector(column='Subject')),
                    ('tfidf', TfidfVectorizer(min_df=0.0025, ngram_range=(1, 4)))
                    ])),
        ('body_tfidf', Pipeline([
            ('selector', ColumnSelector(column='Body')),
            ('tfidf', TfidfVectorizer(min_df=0.0025, ngram_range=(1, 4)))
        ])),
        ('recipients_binarizer', Pipeline([
            ('multi_label', MultiLabelTransformer(column='CoRecipients'))
        ])),
    ])),
    ('classifier', OneVsRestClassifier(LinearSVC(), n_jobs=-1))
])

top_recips = ['A', 'B', 'C, 'D]
corpus_data = pickle.load(
    open(""E:\\Data\\messages_items.pkl"", ""rb""))
df = pd.DataFrame(corpus_data, columns=[
    'MessageId', 'SentTime', 'Subject', 'Body', 'Recipients', 'MessageType', 'IsSender'])

df = df.dropna()

# add co recipients and top recipients columns
df['CoRecipients'] = df['Recipients'].apply(
    lambda r: [x for x in r if x not in top_recips])
df['TopRecipients'] = df['Recipients'].apply(
    lambda r: [x for x in top_recips if x in r])

# drop rows where top recipients = 0
df = df.loc[df['TopRecipients'].str.len() &gt; 0]
df_train = df.loc[df['SentTime'] &lt;= '2017-10-15']
df_test = df.loc[(df['SentTime'] &gt; '2017-10-15') &amp; (df['MessageType'] == 'Meeting')]

mlb = MultiLabelBinarizer(classes=top_recips)

train_x = df_train[['Subject', 'Body', 'CoRecipients']]
train_y = mlb.fit_transform(df_train['TopRecipients'])

test_x = df_train[['Subject', 'Body', 'CoRecipients']]
test_y = mlb.fit_transform(df_train['TopRecipients'])

print ""train""
pipeline.fit(train_x, train_y)

print ""predict""
predictions = pipeline.predict(test_x)

print ""done""
</code></pre>

<p>I'm not sure if I'm doing the featurization of the CoRecipients column correctly. As the results dont look right. Any clue?</p>

<blockquote>
  <p><strong>UPDATE 1</strong></p>
</blockquote>

<p>Changed the code of MLB transformer as follows:</p>

<pre><code>class MultiLabelTransformer(BaseEstimator, TransformerMixin):
        def __init__(self, column):
            self.column = column

        def fit(self, X, y=None):
            self.mlb = MultiLabelBinarizer()
            self.mlb.fit(X[self.column])
            return self

        def transform(self, X):
            return self.mlb.transform(X[self.column])
</code></pre>

<p>And fixed the test set to use df_test </p>

<pre><code>mlb = MultiLabelBinarizer(classes=top_recips)

train_x = df_train[['Subject', 'Body', 'CoRecipients']]
train_y = mlb.fit_transform(df_train['TopRecipients'])

test_x = df_test[['Subject', 'Body', 'CoRecipients']]
test_y = mlb.transform(df_test['TopRecipients'])
</code></pre>

<p>Seeing the below KeyError</p>

<pre><code>Traceback (most recent call last):
  File ""E:\Projects\NLP\FeatureUnion.py"", line 99, in &lt;module&gt;
    predictions = pipeline.predict(test_x)
  File ""C:\Python27\lib\site-packages\sklearn\utils\metaestimators.py"", line 115, in &lt;lambda&gt;
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
  File ""C:\Python27\lib\site-packages\sklearn\pipeline.py"", line 306, in predict
    Xt = transform.transform(Xt)
  File ""C:\Python27\lib\site-packages\sklearn\pipeline.py"", line 768, in transform
    for name, trans, weight in self._iter())
  File ""C:\Python27\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 779, in __call__
    while self.dispatch_one_batch(iterator):
  File ""C:\Python27\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 625, in dispatch_one_batch
    self._dispatch(tasks)
  File ""C:\Python27\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 588, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File ""C:\Python27\lib\site-packages\sklearn\externals\joblib\_parallel_backends.py"", line 111, in apply_async
    result = ImmediateResult(func)
  File ""C:\Python27\lib\site-packages\sklearn\externals\joblib\_parallel_backends.py"", line 332, in __init__
    self.results = batch()
  File ""C:\Python27\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""C:\Python27\lib\site-packages\sklearn\pipeline.py"", line 571, in _transform_one
    res = transformer.transform(X)
  File ""C:\Python27\lib\site-packages\sklearn\pipeline.py"", line 426, in _transform
    Xt = transform.transform(Xt)
  File ""E:\Projects\NLP\FeatureUnion.py"", line 37, in transform
    return self.mlb.transform(X[self.column])
  File ""C:\Python27\lib\site-packages\sklearn\preprocessing\label.py"", line 765, in transform
    yt = self._transform(y, class_to_index)
  File ""C:\Python27\lib\site-packages\sklearn\preprocessing\label.py"", line 789, in _transform
    indices.extend(set(class_mapping[label] for label in labels))
  File ""C:\Python27\lib\site-packages\sklearn\preprocessing\label.py"", line 789, in &lt;genexpr&gt;
    indices.extend(set(class_mapping[label] for label in labels))
KeyError: u'cf3024@gmail.com'
</code></pre>

<p><strong>> UPDATE 2</strong></p>

<p>Working code</p>

<pre><code> class MultiLabelTransformer(BaseEstimator, TransformerMixin):
        def __init__(self, column, classes):
            self.column = column
            self.classes = classes
    def fit(self, X, y=None):
        self.mlb = MultiLabelBinarizer(classes=self.classes)
        self.mlb.fit(X[self.column])
        return self

    def transform(self, X):
        return self.mlb.transform(X[self.column])

# drop rows where top recipients = 0
df = df.loc[df['TopRecipients'].str.len() &gt; 0]
df_train = df.loc[df['SentTime'] &lt;= '2017-10-15']
df_test = df.loc[(df['SentTime'] &gt; '2017-10-15') &amp;
                 (df['MessageType'] == 'Meeting')]

mlb = MultiLabelBinarizer(classes=top_recips)

train_x = df_train[['Subject', 'Body', 'CoRecipients']]
train_y = mlb.fit_transform(df_train['TopRecipients'])

test_x = df_test[['Subject', 'Body', 'CoRecipients']]
test_y = mlb.transform(df_test['TopRecipients'])

# get all unique co-recipients
co_recips = list(set([a for b in df.CoRecipients.tolist() for a in b]))

# create pipeline
pipeline = Pipeline([
    ('features', FeatureUnion(
        # list of features
        transformer_list=[
            ('subject_tfidf', Pipeline([
                    ('selector', ColumnSelector(column='Subject')),
                    ('tfidf', TfidfVectorizer(min_df=0.0025, ngram_range=(1, 4)))
                    ])),
            ('body_tfidf', Pipeline([
                ('selector', ColumnSelector(column='Body')),
                ('tfidf', TfidfVectorizer(min_df=0.0025, ngram_range=(1, 4)))
            ])),
            ('recipients_binarizer', Pipeline([
                ('multi_label', MultiLabelTransformer(column='CoRecipients', classes=co_recips))
            ]))
        ],
        # weight components in FeatureUnion
        transformer_weights={
            'subject_tfidf': 3.0,
            'body_tfidf': 1.0,
            'recipients_binarizer': 1.0,
        }
    )),
    ('classifier', OneVsRestClassifier(LinearSVC(), n_jobs=-1))
])

print ""train""
pipeline.fit(train_x, train_y)

print ""predict""
predictions = pipeline.predict(test_x)
</code></pre>
",Vectorization & Embeddings,transform multiple feature pipeline using featureunion panda data frame contains information message sent user model interested predicting missing recipient message e given recipient b c message want predict else part recipient multi label classification using onevsrestclassifier linearsvc feature want use recipient message subject body since recipient list user want transform column using multilabelbinarizer subject body want use tfidf input pickle file ha data follows value string except recipient set using feature union custom transformer pipeline achieve follows sure featurization corecipients column correctly result dont look right clue update changed code mlb transformer follows fixed test set use df test seeing keyerror update working code
Data size after gated convolution for language modeling,"<p>I recently read following paper about language modeling with CNN.</p>

<p><em>Language Modeling with Gated Convolutional Networks
<a href=""https://arxiv.org/abs/1612.08083"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1612.08083</a></em></p>

<p>In this paper, they use below picture as their model. But I am not sure about <code>Convolution</code> figure (third layer). Usually to apply CNNs for text embeddings, filter size is <code>(filter_h, embedding_size)</code>. So the data size after the convolution is <code>(seq_len-filter_h+1, 1)</code> with <code>stride==1</code>. See a bottom figure.</p>

<p>But in this figure, the height of the data size after the Convolution is still large (I mean larger than 1). So if I try to implement this, it needs many padding to expand columns.
Actually I found <a href=""https://github.com/anantzoid/Language-Modeling-GatedCNN/blob/master/model.py#L62"" rel=""nofollow noreferrer"">an reproduced code</a> at github. Yes, it looks they also use so many padding for the columns with <code>padding=SAME</code>.</p>

<p>Why they are using so many paddings? Many paddings doesn't make sense for me..</p>

<p><a href=""https://i.sstatic.net/bHgUC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bHgUC.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/vxNFgm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vxNFgm.png"" alt=""enter image description here""></a></p>
",Vectorization & Embeddings,data size gated convolution language modeling recently read following paper language modeling cnn language modeling gated convolutional network paper use picture model sure figure third layer usually apply cnns text embeddings filter size data size convolution see bottom figure figure height data size convolution still large mean larger try implement need many padding expand column actually found reproduced code github yes look also use many padding column using many padding many padding make sense
Word2Vec embedding and CNN on H2O R example,"<p>I am wondering if it's possible to provide any <code>r</code> sample code for using word2vec and cnn on text classification in H2O DeepWater R version ? There's very very few documentation on either <code>mexnetR</code> or <code>h2o deep water r</code></p>

<p>I have already used the <code>h2o</code> <code>r</code> version package to train my <code>word2vec</code> <code>word embedding</code> vocabulary lookup table and the document word vector matrix. I am wondering if there's any sample code to combine the lookup table and the original raw text into the  using <code>mxnetR</code> (custom iterator) CNN classification model, or using <code>h2o r</code> to build CNN directly.</p>

<p>I am asking because if I convert all data into the array format at once, then my machine will not have enough memory to support it.</p>
",Vectorization & Embeddings,word vec embedding cnn h r example wondering possible provide sample code using word vec cnn text classification h deepwater r version documentation either already used version package train vocabulary lookup table document word vector matrix wondering sample code combine lookup table original raw text using custom iterator cnn classification model using build cnn directly asking convert data array format machine enough memory support
Creating a wordvector model combining words from other models,"<p>I have two different word vector models created using word2vec algorithm . Now issue i am facing is few words from first model is not there in second model . I want to create a third model from two different word vectors models where i can use word vectors from both models without loosing meaning and the context of word vectors. </p>

<p>Can I do this, and if so, how?</p>
",Vectorization & Embeddings,creating wordvector model combining word model two different word vector model created using word vec algorithm issue facing word first model second model want create third model two different word vector model use word vector model without loosing meaning context word vector
how to add fallback intent in chatbot just like api.ai,"<p>I am developing a chat bot in Python. I have a data set for 4 intents. I have managed to build a classifier using tfidf and sklearn library to classify the input text in one of 4 trained intents.</p>

<p>But, I want to add another intent i.e if user asks anything out of those 4 intents it should return fallback intent like it does in api.ai</p>

<p>Now when I predict the intent of any rubbish text it always returns ""intent no.2"".</p>
",Vectorization & Embeddings,add fallback intent chatbot like api ai developing chat bot python data set intent managed build classifier using tfidf sklearn library classify input text one trained intent want add another intent e user asks anything intent return fallback intent like doe api ai predict intent rubbish text always return intent
Get most similar words using GloVe,"<p>I am new to GloVe. I successfully ran their <a href=""https://github.com/stanfordnlp/GloVe"" rel=""nofollow noreferrer"">demo.sh</a> as given in their website. After running demo I got several files created such as <code>vocab</code>, <code>vectors</code> etc. But they haven't any documentation or anything that describes what files we need to use and how to use to find most similar words.</p>

<p>Hence, please help me to find the most similar words given a word in GloVe (using cosine similarity)? (e.g., like <code>most.similar</code> in Gensim word2vec)</p>

<p>Please help me!</p>
",Vectorization & Embeddings,get similar word using glove new glove successfully ran demo sh given website running demo got several file created etc documentation anything describes file need use use find similar word hence please help find similar word given word glove using cosine similarity e g like gensim word vec please help
Keras : addition layer for embeddings / vectors?,"<p>I have 3 word embeddings :</p>

<ul>
<li>embedding#1 : [w11, w12, w13, w14]</li>
<li>embedding#2 : [w21, w22, w23, w24]</li>
<li>embedding#3 : [w31, w32, w33, w34]</li>
</ul>

<p>Is there a way to get a fourth embedding by adding all three vectors, with the trainable weights from all of them, like: </p>

<ul>
<li>embedding#4  : [w11 + w21 + w31, w12 + w22 + w32, w13 + w23 + w33, w14 + w24 + w34]  </li>
</ul>

<p>?  Is there a way to do this in a keras layer? </p>

<p><strong>Problem</strong><br>
I want to learn the word embeddings for Indonesian language.  I plan to do this by training a sequence prediction machine using LSTMs.   </p>

<p>However, the grammar of Indonesian language is different from english.  Especially, in Indonesian, you can modify a word using prefixes and suffixes.  A noun word when given a prefix can become a verb, and when given a suffix can become an adjective.   You can put so many into one word, so that a single base word can have 5 or more variations.   </p>

<p>For example  :   </p>

<ol>
<li>tani means farm (verb)  </li>
<li>pe-tani means farmer  </li>
<li>per-tani-an means farm (noun)  </li>
<li>ber-tani means farm (verb, with slightly different meaning)  </li>
</ol>

<p>The transformation of semantic done by appending a prefix to a word is consistent between words.  For example  :</p>

<ol>
<li>pe-tani is to tani is what pe-layan is to layan, what pe-layar is to layar, what pe-tembak is to tembak, and so on.  </li>
<li>per-main-an is to main is what per-guru-an is to guru, what per-kira-an is to kira, what per-surat-an is to surat, and so on.  </li>
</ol>

<p>Therefore, i plan to represent the prefixes and suffixes as embeddings, which would be used to do an addition to the base word's embedding, producing a new embedding.  So the meaning of the composite word is derived from the embeddings of the base word and the affixes, not stored as a separate embeddings.   However i don't know how to do this in a Keras layer. If it had been asked before, i cannot find it.  </p>
",Vectorization & Embeddings,kera addition layer embeddings vector word embeddings embedding w w w w embedding w w w w embedding w w w w way get fourth embedding adding three vector trainable weight like embedding w w w w w w w w w w w w way kera layer problem want learn word embeddings indonesian language plan training sequence prediction machine using lstms however grammar indonesian language different english especially indonesian modify word using prefix suffix noun word given prefix become verb given suffix become adjective put many one word single base word variation example tani mean farm verb pe tani mean farmer per tani mean farm noun ber tani mean farm verb slightly different meaning transformation semantic done appending prefix word consistent word example pe tani tani pe layan layan pe layar layar pe tembak tembak per main main per guru guru per kira kira per surat surat therefore plan represent prefix suffix embeddings would used addition base word embedding producing new embedding meaning composite word derived embeddings base word affix stored separate embeddings however know kera layer asked find
Extend word embedding layer for incremental word2vec training with Tensorflow,"<p>I'd like to train word vectors/embeddings incrementally. With each incremental run I want to extend the vocabulary of the model and add new rows to the embeddings matrix.</p>

<p>The embeddings matrix is a partitioned variable, so ideally I want to avoid using <code>assign</code> since it's not implemented for partitioned variables.</p>

<p>One way I tried, looks like this:</p>

<pre><code>        # Set prev_vocab_size and new_vocab_size 
        #accordingly to the corpus/text of the current run

        prev_embeddings = tf.get_variable(
            'prev_embeddings',
            shape=[prev_vocab_size, FLAGS.embedding_size],
            dtype=tf.float32,
            initializer=tf.random_uniform_initializer(-1.0, 1.0)
        )

        new_embeddings = tf.get_variable(
            'new_embeddings',
            shape=[new_vocab_to_add,
                   FLAGS.embedding_size],
            dtype=tf.float32,
            initializer=tf.random_uniform_initializer(
                -1.0, 1.0)
        )

        combined_embeddings = tf.concat(
            [prev_embeddings, new_embeddings], 0)

        embeddings = tf.Variable(
            combined_embeddings,
            expected_shape=[prev_vocab_size + new_vocab_to_add, FLAGS.embedding_size],
            dtype=tf.float32,
            name='embeddings')
</code></pre>

<p>Now, this works well for the first run. But if I do the second run, I get a <code>Assign requires shapes of both tensors to match</code> error because the restored original <code>prev_embeddings</code> variable (from the first run) doesn't match the new shape (based on the extended vocab) I declare in the second run. </p>

<p>So I modified the tf.train.Saver to save the <code>new_embeddings</code> as the <code>prev_embeddings</code> like this:</p>

<p><code>saver = tf.train.Saver({""prev_embeddings"": new_embeddings})</code></p>

<p>Now, in the second run, the <code>prev_embeddings</code> has the shape of <code>new_embeddings</code> in the previous run and I don't get an error for this.</p>

<p>However, now the <code>new_embeddings</code> in the second run has a different shape than in the first run and therefore when restoring the variables from the first run, I get another <code>Assign requires shapes of both tensors to match</code> error.</p>

<p>What's the best way to extend/expand the embeddings variable incrementally with new vectors for new words in the vocabulary while keeping the old and trained vectors?</p>

<p>Any help would be much appreciated.</p>
",Vectorization & Embeddings,extend word embedding layer incremental word vec training tensorflow like train word vector embeddings incrementally incremental run want extend vocabulary model add new row embeddings matrix embeddings matrix partitioned variable ideally want avoid using since implemented partitioned variable one way tried look like work well first run second run get error restored original variable first run match new shape based extended vocab declare second run modified tf train saver save like second run ha shape previous run get error however second run ha different shape first run therefore restoring variable first run get another error best way extend expand embeddings variable incrementally new vector new word vocabulary keeping old trained vector help would much appreciated
Understanding input and labels in word2vec (TensorFlow),"<p>I am trying to properly understand the <code>batch_input</code> and <code>batch_labels</code> from the tensorflow <a href=""https://www.tensorflow.org/tutorials/word2vec"" rel=""nofollow noreferrer"">""Vector Representations of Words""</a> tutorial. </p>

<p>For instance, my data </p>

<pre><code> 1 1 1 1 1 1 1 1 5 251 371 371 1685 ...
</code></pre>

<p>... starts with</p>

<pre><code>skip_window = 2 # How many words to consider left and right.
num_skips = 1 # How many times to reuse an input to generate a label.
</code></pre>

<p>Then the generated input array is:</p>

<pre><code>bach_input = 1 1 1 1 1 1 5 251 371 ....  
</code></pre>

<p>This makes sense, starts from after 2 (= window size) and then continuous. The labels:</p>

<pre><code>batch_labels = 1 1 1 1 1 1 251 1 1685 371 589 ...
</code></pre>

<p>I don't understand these labels very well. There are supposed to be 4 labels for each input right (window size 2, on each side). But the <code>batch_label</code> variable is the same length. </p>

<p>From the tensorflow tutorial: </p>

<blockquote>
  <p>The skip-gram model takes two inputs. One is a batch full of integers
  representing the source context words, the other is for the target
  words.</p>
</blockquote>

<p>As per the tutorial, I have declared the two variables as: </p>

<pre><code>  batch = np.ndarray(shape=(batch_size), dtype=np.int32)
  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
</code></pre>

<p>How should I interpret the <code>batch_labels</code>?</p>
",Vectorization & Embeddings,understanding input label word vec tensorflow trying properly understand tensorflow vector representation word tutorial instance data start generated input array make sense start window size continuous label understand label well supposed label input right window size side variable length tensorflow tutorial skip gram model take two input one batch full integer representing source context word target word per tutorial declared two variable interpret
TensorFlow Word Embedding Not Making Sense,"<p>I am not understanding how the <code>VocabularyProcessor</code> works in tensorflow. I loaded in glove encodings and am mapping my data to it with the following code.</p>

<pre><code>vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)
pretrain = vocab_processor.fit(vocab)
X_np_data = np.array(list(vocab_processor.transform(X_data)))
</code></pre>

<p>But comparing the outputs to the vocab index I don't think the results make sense.</p>

<p>For example, in the input data there's this</p>

<pre><code>X_data[1]
Out[84]: ""is upset that he ca n't update his facebook by texting it and might cry as a result school today also blah ! ""
</code></pre>

<p>Word indexes</p>

<pre><code>X_np_data[1]
Out[85]: 
array([  22, 3091,   32,   83,  132,   31, 1741,  203,  380,  123, 1971,
         23], dtype=int64)
</code></pre>

<p>But, if we look at the vocab index that those numbers are referencing it's not the same as what's in the sentence</p>

<pre><code>vocab[22]
Out[86]: 'de'
vocab[3091]
Out[87]: 'entiendo'
</code></pre>

<p>Am I doing something wrong? Or am I not understanding how this works?</p>

<p><strong>Edit</strong>
I think I am not understanding how this works and have not been able to find a good example online. I assumed it would be mapped back to the index of the vocab list of the word and then that would be used to reference an embeddings placeholder where row i is the embedding for word i in the vocab list. But looking at just <code>len(vocab)</code> and <code>len(pretrain.vocabulary_)</code> they don't even have the same number of entries. vocab has 1193514 and pretrian has 1138921. </p>
",Vectorization & Embeddings,tensorflow word embedding making sense understanding work tensorflow loaded glove encoding mapping data following code comparing output vocab index think result make sense example input data word index look vocab index number referencing sentence something wrong understanding work edit think understanding work able find good example online assumed would mapped back index vocab list word would used reference embeddings placeholder row embedding word vocab list looking even number entry vocab ha pretrian ha
Keras word embeddings Glove: can&#39;t prepare the embedding matrix,"<p>I'm trying to implement the Keras code (<a href=""https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"" rel=""nofollow noreferrer"">https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html</a>) on word embeddings and I can't get it straight. I'm using Glove, embedding_index and I have a dataset with 5374 unique words stored in a dict word_index (len(word_index) = 5374. The embedding_matrix is set to zeros with a shape of (5374, 100) -- EMBEDDING DIM = 100</p>

<p>When I execute the code for the embedding_matrix it gives me </p>

<p>IndexError: index 5374 is out of bounds for axis 0 with size 5374</p>

<p>What am I doing wrong? <a href=""https://i.sstatic.net/zT6Vq.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",Vectorization & Embeddings,kera word embeddings glove prepare embedding matrix trying implement kera code word embeddings get straight using glove embedding index dataset unique word stored dict word index len word index embedding matrix set zero shape embedding dim execute code embedding matrix give indexerror index bound axis size wrong enter image description
Cosine similarity between query and document in a search engine,"<p>I am going through the Manning book for Information retrieval. Currently I am at the part about cosine similarity. One thing is not clear for me.<br>
 Lets say I have the tf-idf vectors for the query and a document. I want  to compute the cosine similarity between both vectors. When I compute the magnitude for the document vector do I sum the squares of all the terms in the vector or just the terms in the query?   </p>

<p>Here is an example : we have user query ""cat food beef"" .
Lets say its vector is (0,1,0,1,1).( assume there are only 5 directions in the vector one for each unique word in the query and the document)
We have a document ""Beef is delicious""
Its vector is (1,1,1,0,0). We want to find the cosine similarity between the query and the document vectors.</p>
",Vectorization & Embeddings,cosine similarity query document search engine going manning book information retrieval currently part cosine similarity one thing clear let say tf idf vector query document want compute cosine similarity vector compute magnitude document vector sum square term vector term query example user query cat food beef let say vector assume direction vector one unique word query document document beef delicious vector want find cosine similarity query document vector
Looking to cluster short descriptions of reports. Should I use Word2Vec or Doc2Vec,"<p>So, I have close to 2000 reports and each report has an associated short description of the problem. My goal is to cluster all of these so that we can find distinct trends within these reports. </p>

<p>One of the features I'd like to use some sort of contextual text vector. Now, I've used <code>Word2Vec</code> and think this would be a good option but I also so <code>Doc2Vec</code> and I'm not quite sure what would be a better option for this use case. </p>

<p>Any feedback would be greatly appreciated. </p>
",Vectorization & Embeddings,looking cluster short description report use word vec doc vec close report report ha associated short description problem goal cluster find distinct trend within report one feature like use sort contextual text vector used think would good option also quite sure would better option use case feedback would greatly appreciated
Tensorflow: Reinforcement learning with variable character level text input?,"<p>Taking into consideration a reinforcement learning context whereby the Tensorflow agent will take a ""step"" in the environment for each observation that is received.
<a href=""https://i.sstatic.net/eoeSq.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/eoeSq.png"" alt=""""></a></p>

<p>How would one most effectively accommodate an observation of variable lengths, for instance when an observation exceeds a 1024 placeholder limit <strong>(i.e. 10000 char long Wikipedia article)</strong> noting that placeholders are immutable and can't be dynamically changed during computation:</p>

<pre><code>self.obs = tf.placeholder(tf.float32, shape=(None,1024), name='obs')
</code></pre>

<p><strong>I am familiar with the padding method</strong> whereby a maximum line length is specified and the chars not occupied are filled in with a placeholder. However, this method seems ineffective when processing inputs with varying lengths between 100 chars and 10000 chars, whereby the shape of the placeholder would have to be <code>(None, 10000)</code> even when processing inputs that are only 100 chars in length effectively using 9900 placeholder chars and a large <strong>EXPENSIVE</strong> amount of memory and computation that is effectively useless, compounded even more so in a reinforcement learning context whereby millions of steps are being taken in order to learn an effective policy.</p>

<p><strong>I'm aware of the word embedding method.</strong></p>

<p><a href=""https://i.sstatic.net/4yweN.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/4yweN.png"" alt=""""></a></p>

<p>However, there are a multitude of issues with this method which include but are not limited to: Lack of detail and by extension performance, lack of punctuation understanding and other character and logic representations as in a mathematical formula.etc</p>

<p>How could one effectively structure an input flow whereby a varying input size can effectively be consumed by a Tensorflow agent in a manner that avoids the aforementioned problems?</p>
",Vectorization & Embeddings,tensorflow reinforcement learning variable character level text input taking consideration reinforcement learning context whereby tensorflow agent take step environment observation received would one effectively accommodate observation variable length instance observation exceeds placeholder limit e char long wikipedia article noting placeholder immutable dynamically changed computation familiar padding method whereby maximum line length specified char occupied filled placeholder however method seems ineffective processing input varying length char char whereby shape placeholder would even processing input char length effectively using placeholder char large expensive amount memory computation effectively useless compounded even reinforcement learning context whereby million step taken order learn effective policy aware word embedding method however multitude issue method include limited lack detail extension performance lack punctuation understanding character logic representation mathematical formula etc could one effectively structure input flow whereby varying input size effectively consumed tensorflow agent manner avoids aforementioned problem
Using Custom Word2Vec to find semantic similarity between technical questions?,"<p>We can get the similarity of two sentences like ""The boy is playing football"" and ""A kid is playing football"" using Google news vectors by applying ""SIF Embeddings"".</p>

<p>I would like to get the similarity for two sentences which are technical like ""what is an abstract class?"" and ""what is a class?"".</p>

<p>I have used Google-news Vectors in getting the similarity but it didn't work well.</p>

<p>I would like to know how training data should be?</p>
",Vectorization & Embeddings,using custom word vec find semantic similarity technical question get similarity two sentence like boy playing football kid playing football using google news vector applying sif embeddings would like get similarity two sentence technical like abstract class class used google news vector getting similarity work well would like know training data
Using Custom Word2Vec to find semantic similarity between technical questions?,"<p>We can get the similarity of two sentences like ""The boy is playing football"" and ""A kid is playing football"" using Google news vectors by applying ""SIF Embeddings"".</p>

<p>I would like to get the similarity for two sentences which are technical like ""what is an abstract class?"" and ""what is a class?"".</p>

<p>I have used Google-news Vectors in getting the similarity but it didn't work well.</p>

<p>I would like to know how training data should be?</p>
",Vectorization & Embeddings,using custom word vec find semantic similarity technical question get similarity two sentence like boy playing football kid playing football using google news vector applying sif embeddings would like get similarity two sentence technical like abstract class class used google news vector getting similarity work well would like know training data
TFlearn - VocabularyProcessor ignores parts of given vocabulary,"<p>I am using the <a href=""http://tflearn.org/data_utils/"" rel=""nofollow noreferrer"">VocabularyProcessor</a> of TFlearn to map documents to integer arrays. However, I don't seem to be able to initialize the VocabularyProcessor with my own vocabulary. In the docs it says that I can provide a vocabulary when creating the VocabularyProcessor as in:</p>

<pre><code>vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, vocabulary=vocab)
</code></pre>

<p>However, when creating the VocabularyProcessor like this I cannot transform my documents correctly.
I am providing the vocabulary as a dictionary, using the word indices as values:</p>

<pre><code>vocab={'hello':3, '.':5, 'world':20}
</code></pre>

<p>Sentences are provided as follows:</p>

<pre><code>sentences = ['hello summer .', 'summer is here .', ...]
</code></pre>

<p>It's very important that the VocabularyProcessor uses the given indices to transform the documents, because each index references a certain word embedding. When calling </p>

<pre><code>list(vocab_processor.transform(['hello world .', 'hello'])) 
</code></pre>

<p>the output is</p>

<pre><code>[array([ 3, 20, 0]), array([3, 0, 0])]
</code></pre>

<p>So the sentences weren't transformed according to the provided vocabulary which maps '.' to 5.
How do I provide the vocabulary to the VocabularyProcessor correctly?</p>
",Vectorization & Embeddings,tflearn vocabularyprocessor ignores part given vocabulary using vocabularyprocessor tflearn map document integer array however seem able initialize vocabularyprocessor vocabulary doc say provide vocabulary creating vocabularyprocessor however creating vocabularyprocessor like transform document correctly providing vocabulary dictionary using word index value sentence provided follows important vocabularyprocessor us given index transform document index reference certain word embedding calling output sentence transformed according provided vocabulary map provide vocabulary vocabularyprocessor correctly
Finding which words are likely to occur with word X,"<p>What is the best way to find out which words are frequent near some word X? (note: NOT which words are most similar to word X)</p>

<p>I have GloVe word vectors, so each vector represents a distribution of some word across different environments (each dimension is an environment). So how do I retrieve words from each of those environments? In other words, how do I retrieve words that are similar in only one of the dimensions?</p>

<p>I tried looking for words that are closer to X along only one dimension, ignoring the rest, but that gave me garbage words.</p>

<p>P.S.
What I so far is find the N nearest words (by cosine similarity) to word X, and then apply K-means clustering to those words. It works pretty good, but I am concerned that the N nearest words are not necessarily the words that appear NEAR word X, but rather, words that appear IN SIMILAR ENVIRONMENTS to word X.</p>

<p>EDIT:
Clarification: simply collecting n-gram counts will not suffice, since I do am looking for a way to do this with only the vectors, that is, without access to the corpus itself. The reason is that some freely available pretrained vectors were trained on terrabytes of data. Storing the entire n-gram counts for common crawl, for example, would be very wasteful if this information could somehow be obtained from the pretrained vectors.</p>
",Vectorization & Embeddings,finding word likely occur word x best way find word frequent near word x note word similar word x glove word vector vector represents distribution word across different environment dimension environment retrieve word environment word retrieve word similar one dimension tried looking word closer x along one dimension ignoring rest gave garbage word p far find n nearest word cosine similarity word x apply k mean clustering word work pretty good concerned n nearest word necessarily word appear near word x rather word appear similar environment word x edit clarification simply collecting n gram count suffice since looking way vector without access corpus reason freely available pretrained vector trained terrabytes data storing entire n gram count common crawl example would wasteful information could somehow obtained pretrained vector
Rapidminer- TF-IDF from csv dataset,"<p>I have to calculate tf-idf of two columns of csv file..
Should i have to convert the rows into text files? or is there any method to calculate tf-idf from csv.</p>

<p>how can i calculate tfidf of columns of csv file.</p>
",Vectorization & Embeddings,rapidminer tf idf csv dataset calculate tf idf two column csv file convert row text file method calculate tf idf csv calculate tfidf column csv file
Which embedding layer does skip-gram use?,"<p><a href=""https://i.sstatic.net/ns9mZm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ns9mZm.png"" alt=""enter image description here""></a></p>

<p>In Skip-gram, it has pairs of a focused word and context words. Then, it outputs each pair is correct or wrong pairs.
In contrast to CBOW, Skip-gram uses two embedding layer. After training, which embedding layer should be used?</p>

<p>In below examples in Keras, it looks using first (focused word embedding, not context one) embedding layer.</p>

<p><a href=""https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb"" rel=""nofollow noreferrer"">https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb</a></p>
",Vectorization & Embeddings,embedding layer doe skip gram use skip gram ha pair focused word context word output pair correct wrong pair contrast cbow skip gram us two embedding layer training embedding layer used example kera look using first focused word embedding context one embedding layer
missing word in word embedding,"<p>If I have a word2vec model and I use it for embedding all words in train and test set. But with proper words, in word2vec model does not contain. And can I random a vector as a embedding for all proper words.
If can, please give me some tips and some paper references.
Thank you</p>
",Vectorization & Embeddings,missing word word embedding word vec model use embedding word train test set proper word word vec model doe contain random vector embedding proper word please give tip paper reference thank
N-grams in GloVe,"<p>I want to construct word embeddings for documents using GloVe. I know how to obtain vector embeddings for single words (unigrams) as follows (for their example text document).</p>

<pre><code>$ git clone http://github.com/stanfordnlp/glove
$ cd glove &amp;&amp; make
$ ./demo.sh
</code></pre>

<p>Now, I want to obtain vector embeddings for bigrams. For example;</p>

<ol>
<li>""New york"" -> instead of ""New"", and ""york""</li>
<li>""machine learning"" -> instead of ""machine"", and ""learning""</li>
</ol>

<p>Is it possible to do in GloVe? If yes, how?</p>
",Vectorization & Embeddings,n gram glove want construct word embeddings document using glove know obtain vector embeddings single word unigrams follows example text document want obtain vector embeddings bigram example new york instead new york machine learning instead machine learning possible glove yes
Implementing Luong and Manning&#39;s hybrid model,"<p><a href=""https://i.sstatic.net/vBeCj.png"" rel=""nofollow noreferrer"">hybrid word character model</a></p>

<p>As shown in the above image I need to create a <strong>hybrid</strong> encoder-decoder network(seq2seq) which takes in both word and character embeddings as input.
As shown in image consider the sentence:</p>

<blockquote>
  <p><em>A cute cat</em></p>
</blockquote>

<p>Hypothetically the words in vocabulary are:</p>

<blockquote>
  <p>a , cat</p>
</blockquote>

<p>and Out of vocabulary words are:</p>

<blockquote>
  <p>cute</p>
</blockquote>

<p>we feed the words <strong>a, cat</strong> as their respective embeddings
but since <strong>cute</strong> is out of vocabulary we generally feed it with embedding of a universal token.</p>

<p>But instead in this case I need to pass that unique word (<em>cute</em> which is out of vocabulary) through another seq2seq layer <strong>character by character</strong> to generate its embedding on the fly.</p>

<p>The both seq2seq layers must be trained jointly end to end.
The following is a snippet of my code where I tried the main encoder decoder network which takes word based inputs in <strong>Keras</strong></p>

<pre><code>model=Sequential()
model.add(Embedding(X_vocab_len+y_vocab_len, 300,weights=[embedding_matrix], input_length=X_max_len, mask_zero=True))
for i in range(num_layers):
    return_sequences = i != num_layers-1
    model.add(LSTM(hidden_size,return_sequences=return_sequences))



model.add(RepeatVector(y_max_len))

# Creating decoder network
for _ in range(num_layers):
    model.add(LSTM(hidden_size, return_sequences=True))
model.add(TimeDistributed(Dense(y_vocab_len)))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy',
        optimizer='rmsprop',
        metrics=['accuracy'])
</code></pre>

<p><strong>here X is my input sentence and y is the sentence to be generated ,vocabulary size is what I fixed consisting of frequent words and rare words are considered out of vocabulary based on vocabulary size</strong></p>

<p>here I created a sequential model in Keras where I added embeddings from pre-trained vectors generated by GloVe(embedding_matrix)</p>

<p>How to model input to achieve such senario ?</p>

<p>The reference paper is :
<a href=""http://aclweb.org/anthology/P/P16/P16-1100.pdf"" rel=""nofollow noreferrer"">http://aclweb.org/anthology/P/P16/P16-1100.pdf</a></p>
",Vectorization & Embeddings,implementing luong manning hybrid model hybrid word character model shown image need create hybrid encoder decoder network seq seq take word character embeddings input shown image consider sentence cute cat hypothetically word vocabulary cat vocabulary word cute feed word cat respective embeddings since cute vocabulary generally feed embedding universal token instead case need pas unique word cute vocabulary another seq seq layer character character generate embedding fly seq seq layer must trained jointly end end following snippet code tried main encoder decoder network take word based input kera x input sentence sentence generated vocabulary size fixed consisting frequent word rare word considered vocabulary based vocabulary size created sequential model kera added embeddings pre trained vector generated glove embedding matrix model input achieve senario reference paper
Doc2vec and word2vec with negative sampling,"<p>My current doc2vec code is as follows.</p>

<pre><code># Train doc2vec model
model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4, iter = 20)
</code></pre>

<p>I also have a word2vec code as below.</p>

<pre><code> # Train word2vec model
model = word2vec.Word2Vec(sentences, size=300, sample = 1e-3, sg=1, iter = 20)
</code></pre>

<p>I am interested in using both DM and DBOW in <strong>doc2vec</strong> AND both Skip-gram and CBOW in <strong>word2vec</strong>.</p>

<p>In Gensim I found the below mentioned sentence:
<strong>""Produce word vectors with deep learning via word2vec’s “skip-gram and CBOW models”, using either hierarchical softmax or negative sampling""</strong></p>

<p>Thus, I am confused either to use hierarchical softmax or negative sampling. Please let me know what are the <strong>differences</strong> in these two methods.</p>

<p>Also, I am interested in knowing <strong>what are the parameters that need to be changed</strong> to use <strong>hierarchical softmax</strong> AND/OR <strong>negative sampling</strong> with respect to <strong>dm, DBOW, Skip-gram and CBOW</strong>?</p>

<p>P.s. my application is a recommendation system :)</p>
",Vectorization & Embeddings,doc vec word vec negative sampling current doc vec code follows also word vec code interested using dm dbow doc vec skip gram cbow word vec gensim found mentioned sentence produce word vector deep learning via word vec skip gram cbow model using either hierarchical softmax negative sampling thus confused either use hierarchical softmax negative sampling please let know difference two method also interested knowing parameter need changed use hierarchical softmax negative sampling respect dm dbow skip gram cbow p application recommendation system
Linear activation function in Word to Vector,"<p>In <a href=""https://arxiv.org/pdf/1301.3781.pdf"" rel=""nofollow noreferrer"">word2vec paper</a>, they are using linear activation function. I reason may be that they are giving enough training data for learning word embeddings so that non linear activation function is not necessary, am I correct?</p>

<p>Also if we use non linear activation function in hidden layer then I think results should be better. So why google use linear activation function in case of word to vector? </p>
",Vectorization & Embeddings,linear activation function word vector word vec paper using linear activation function reason may giving enough training data learning word embeddings non linear activation function necessary correct also use non linear activation function hidden layer think result better google use linear activation function case word vector
How to represent a set of words as a vector in my application?,"<p>First of all, let me briefly describe my task:<br>
There are about 10^9 sets of words, each sets contains about 10^4 words, which indicates it will be very space consuming to store all those sets in disk. I want to know the number of common words between two sets. However, I don't care what those common words are. I even don't need to know the accurate number which means that an estimated value is enough to me. The key is to find an appropriate representation for these sets so that they could be stored. <br>
A natural idea is to represent a set of words as a vector, and then train a regression NN model whose input are two vectors, which represent two sets, and output is a value which estimates the common words between two input sets. The problem is how to represent these sets as vectors. It's definitely impossible to directly apply the bag of words model, since the dictionary is too big to store. Maybe I can utilize some dimensionality reduction methods on bag of words first to achieve a smaller representation of vector. Or maybe I can apply word embedding to each word, and then sum them up to represent a set. Is there any advice? Or is there any relative work already?</p>
",Vectorization & Embeddings,represent set word vector application first let briefly describe task set word set contains word indicates space consuming store set disk want know number common word two set however care common word even need know accurate number mean estimated value enough key find appropriate representation set could stored natural idea represent set word vector train regression nn model whose input two vector represent two set output value estimate common word two input set problem represent set vector definitely impossible directly apply bag word model since dictionary big store maybe utilize dimensionality reduction method bag word first achieve smaller representation vector maybe apply word embedding word sum represent set advice relative work already
How to create a model using trained models?,"<p>I have created two models using gensim word2vec. Now I want to merge these two models in a way that I get the union of these two models.</p>

<p>Eg: </p>

<ul>
1. Model one has following vocabulary
</ul>

<pre><code>{""Hi"", ""Hello"", ""World""}
</code></pre>

<ul>
2. Model two has the following vocabulary
</ul>

<pre><code>{""Hi"", ""King"", ""Hello"", ""Human""}
</code></pre>

<p>Now I want to use these two models and create a new model which will have the following vocabulary</p>

<pre><code>{""Hi"", ""Hello"", ""World"", ""King"", ""Human""}
</code></pre>
",Vectorization & Embeddings,create model using trained model created two model using gensim word vec want merge two model way get union two model eg model one ha following vocabulary model two ha following vocabulary want use two model create new model following vocabulary
How to create gensim word2vec model using pre trained word vectors?,"<p>I have created word vectors using a distributed word2vec algorithm. Now I have words and their corresponding vectors. How to build a gensim word2vec model using these words and vectors? </p>
",Vectorization & Embeddings,create gensim word vec model using pre trained word vector created word vector using distributed word vec algorithm word corresponding vector build gensim word vec model using word vector
How to restore words from embedded words in keras?,"<p>I am now using <code>Embedding</code> layer for my NLP problem. My model predicts one word as an answer. So the final output's shape is <code>(None, vocab_size)</code>.</p>

<p>My question is how to restore original word from embedded vectors without new <code>Dense</code> layer? For example, below <code>x</code> is <code>(None, 4, 8) = (None, sequence_length, embedding_dim)</code>. Then, I want to extract most possible <code>i-th word</code> from actual <code>x</code>. </p>

<pre><code>...
inp = Input(shape=(max_length,), dtype='int32') # ex) ['who','are','you','?']
x = Embedding(output_dim=embd_dim,
              input_dim=vocab_size, 
              input_length=max_length, 
              name='my_emb')(inp)
answer = restore_word_from_embedded_data(x) # what i want to do. ex) 'tom'
# No, i want to extract a word without new Dense layer
# answer = Dense(vocab_size)(answer) from current Embedding weights, 'my_emb'.
...
</code></pre>

<p>Here is brief summary of Input and Embedding.</p>

<pre><code>input_6 (InputLayer)         (None, 4(seq_length))                          
_________________________________________________________________
embedding_9 (Embedding)      (None, 4(seq_length), 8(embed_dim))
</code></pre>
",Vectorization & Embeddings,restore word embedded word kera using layer nlp problem model predicts one word answer final output shape question restore original word embedded vector without new layer example want extract possible actual brief summary input embedding
Algorithms to find the distance/similarity among the user tags?,"<p>I want to find the semantic/similarity of user tags.</p>

<p>Each user can use maximum four tags. For example, User-1[""Machine learning"", ""Photography"", ""data science"", ""neural network""], User-2 [""Machine learning"", ""Data Science"", ""Statistics"", ""Mathematics""],  User-2 [""Geophysics"", ""Machine learning"", ""Art and printing"", ""Mathematics""].</p>

<p>Tags are from wide range of skills. I want to find the distance/similarity among the tags.  For example: ""Machine learning,"" ""Data science,"" and ""neural network"" will be close to each other, similarly ""Photography"" and ""Art and printing"" would be close to each other.</p>

<p>I was thinking to use word2vec. But I am hesitating to use it in production since I have to train it with thousand categories of skill set documents. Not only that in real life users always change/update their tags. So I want to build/develop a dynamic algorithm that will be adjusted according to user tag.</p>

<p>I am a newbie in machine learning world.</p>
",Vectorization & Embeddings,algorithm find distance similarity among user tag want find semantic similarity user tag user use maximum four tag example user machine learning photography data science neural network user machine learning data science statistic mathematics user geophysics machine learning art printing mathematics tag wide range skill want find distance similarity among tag example machine learning data science neural network close similarly photography art printing would close wa thinking use word vec hesitating use production since train thousand category skill set document real life user always change update tag want build develop dynamic algorithm adjusted according user tag newbie machine learning world
Gensim &amp; Keras preprocessing text:how to eliminate punctuation when using text_to_word_sequence function,"<p>I use filter like this:</p>

<pre><code>text_to_word_sequence(line,filters='!”#$%&amp;()*+,-./:;&lt;=&gt;?
@[\\]^_`{|}~\t\n\r？！。，“')
</code></pre>

<p>to eliminate punctuation 
but when I check the performance of gensim word2Vec</p>

<pre><code>model.most_similar('country')
</code></pre>

<p>I got </p>

<pre><code>[('nation', 0.8125789761543274),
 ('country,', 0.7982310056686401),
 ('country.', 0.6794269680976868),
 ('region', 0.667543351650238),
 ('continent', 0.6630430221557617),
 ('countries', 0.6313810348510742),
 ('territory', 0.6307457685470581),
 ('nation,', 0.6306755542755127),
 ('britain', 0.6112251281738281),
 ('india', 0.6077225208282471)]
</code></pre>

<p>It seems that not all the punctuation is eliminated,why? </p>
",Vectorization & Embeddings,gensim kera preprocessing text eliminate punctuation using text word sequence function use filter like eliminate punctuation check performance gensim word vec got seems punctuation eliminated
Working with text in Quora Pairs Kaggle Challenge,"<p>I recently started to play with the dataset from the Quora Question Pairs Challenge.</p>

<p><a href=""https://www.kaggle.com/c/quora-question-pairs/data"" rel=""nofollow noreferrer"">Quora Question Pairs Challenge Dataset</a></p>

<p>So i did some basic stuff like visualizing the data a bit,cleaning it(Lematization,stop word reomval,punctuation removal etc.).And i have also generated embeddings from the question pairs using word2vec model from the gensim package.I am a bit confused as to what do i have to now?</p>

<p>How do i fit the model and enable it to make a prediction.Any help in this regard is welcome.</p>

<p>Here is the code which i have written for the same.</p>

<pre><code>   '''Opening directory path'''

path=os.path.normpath('M:\PycharmProjects\AI+DL+CP\QQP')

train_df=0
test_dataset=0

for subdir,dir,files in os.walk(path):
    for file in files:
        #print(file)
        if file =='QQPT.csv':
            train_df=pd.read_csv(os.path.join(subdir,file),encoding='utf-8')

        elif file=='QQPTest.csv':
            test_dataset=pd.read_csv(os.path.join(subdir,file),encoding='utf-8')

eng_stopwords = set(stopwords.words('english'))

def remove_special_characters_after_tokenization(tokens):
    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))
    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])
    #print('Without special')
    return list(filtered_tokens)


def remove_stop_words(tokens):
    filtered_tokens=[word for word in tokens if word not in eng_stopwords]
    return list(filtered_tokens)

def remove_repeated_characters(tokens):
    repeat_pattern = re.compile(r'(\w*)(\w)\2(\w*)')
    match_substitution = r'\1\2\3'

    def replace(old_word):
        if wordnet.synsets(old_word):
            return old_word
        new_word = repeat_pattern.sub(match_substitution, old_word)
        return replace(new_word) if new_word != old_word else new_word
    correct_tokens = [replace(word) for word in tokens]
    return list(correct_tokens)

'''Stopword removal and extra character removal'''
tokenizer=nltk.word_tokenize

print(train_df['question2'])
#print(train_df.dtypes.index)
lemmatizer=nltk.stem.WordNetLemmatizer()

print('Processing question2')
train_df['question2']=train_df['question2'].apply(lambda x:str(x))
train_df['question2']=[word.lower() for word in train_df['question2']]
train_df['question2']=train_df['question2'].apply(tokenizer)

print('Processing question1')
train_df['question1']=[word.lower() for word in train_df['question1']]
train_df['question1']=train_df['question1'].apply(tokenizer)

for something in train_df['question1']:
    for item in something:
        item=lemmatizer.lemmatize(item)

for something in train_df['question2']:
    for item in something:
        item=lemmatizer.lemmatize(item)

train_df['question2']=train_df['question2'].apply(lambda x:remove_repeated_characters(x))
train_df['question2']=train_df['question2'].apply(lambda z:remove_special_characters_after_tokenization(z))
train_df['question2']=train_df['question2'].apply(lambda x:remove_stop_words(x))

train_df['question1']=train_df['question1'].apply(lambda x:remove_repeated_characters(x))
train_df['question1']=train_df['question1'].apply(lambda z:remove_special_characters_after_tokenization(z))
train_df['question1']=train_df['question1'].apply(lambda x:remove_stop_words(x))



'''Creating Embeddings[For both question together]'''

print('Creating Embeddings')
embeddings=gensim.models.Word2Vec(train_df['question1']+train_df['question2'])

print(embeddings)
</code></pre>
",Vectorization & Embeddings,working text quora pair kaggle challenge recently started play dataset quora question pair challenge quora question pair challenge dataset basic stuff like visualizing data bit cleaning lematization stop word reomval punctuation removal etc also generated embeddings question pair using word vec model gensim package bit confused fit model enable make prediction help regard welcome code written
Mixing Word Vectors from Different models,"<p>While working with Word2Vec to find ways to <strong>disambiguate word senses using word vectors representation</strong>, one strategy that came to my mind was the following:</p>

<p><strong>Train a model using a corpus where you know the senses of the words of interest</strong>, in my case english words which are also gene names.
Then, whenever a paragraph of interest appeared in a unknown corpus, <strong>train a small model using the paragraph with the word of interest</strong>.
With the word vectors built from this snippet, <strong>compare the representations</strong> of the specific word in the known context and in the unknown context to see how close they are in the vector space.</p>

<p>While trying this approach, I noticed that even 2 models trained on the same corpus have quite different word representations for the same word. In other words, the cosine similarity between these 2 word vectors is quite low.</p>

<p>So my question is, is this difference due to the model somehow building different base vectors to represent the space? And if so, is there a way to lock those to the euclidian one during the training? Or is the difference due to something else?</p>
",Vectorization & Embeddings,mixing word vector different model working word vec find way disambiguate word sens using word vector representation one strategy came mind wa following train model using corpus know sens word interest case english word also gene name whenever paragraph interest appeared unknown corpus train small model using paragraph word interest word vector built snippet compare representation specific word known context unknown context see close vector space trying approach noticed even model trained corpus quite different word representation word word cosine similarity word vector quite low question difference due model somehow building different base vector represent space way lock euclidian one training difference due something else
How do I find cosine similarity between two text documents using Java?,"<p>I need to compare a large number of tweets containing a particular hashtag to display the tweet which has the highest content in it. For the same, I need to find pair-wise cosine similarity between each one of them and display the tweet with highest pair-wise cosine similarity as output. I've been reading a lot about vector space Models, tf-idf vectors, word2vec/doc2vec etc. but couldn't grasp anything completely. I need to implement the same using Java. Is there any alternative to scikit-learn's TfidfVectorizer or NLTK's synsets?</p>
",Vectorization & Embeddings,find cosine similarity two text document using java need compare large number tweet containing particular hashtag display tweet ha highest content need find pair wise cosine similarity one display tweet highest pair wise cosine similarity output reading lot vector space model tf idf vector word vec doc vec etc grasp anything completely need implement using java alternative scikit learn tfidfvectorizer nltk synset
How to tag sentences for spacy&#39;s Sence2vec implementation,"<p>SpaCy has implemented a sense2vec word embeddings package which they document <a href=""https://github.com/explosion/sense2vec"" rel=""nofollow noreferrer"">here </a></p>

<p>The vectors are all of the form <code>WORD|POS</code>. For example, the sentence </p>

<pre><code>Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of trouble
</code></pre>

<p>Needs to be converted into</p>

<pre><code>Dear|ADJ local|ADJ newspaper|NOUN ,|PUNCT I|PRON think|VERB effects|NOUN computers|NOUN have|VERB on|ADP people|NOUN are|VERB great|ADJ learning|NOUN skills/affects|NOUN because|ADP they|PRON give|VERB us|PRON time|NOUN to|PART chat|VERB with|ADP friends/new|ADJ people|NOUN ,|PUNCT helps|VERB us|PRON learn|VERB about|ADP the|DET globe(astronomy|NOUN )|PUNCT and|CONJ keeps|VERB us|PRON out|ADP of|ADP trouble|NOUN !|PUNCT
</code></pre>

<p>In order to be interpretable by the sense2vec pretrained embeddings and in order to be in the sense2vec format.</p>

<p>How can this be done?</p>
",Vectorization & Embeddings,tag sentence spacy sence vec implementation spacy ha implemented sense vec word embeddings package document vector form example sentence need converted order interpretable sense vec pretrained embeddings order sense vec format done
What is the initial value of Embedding layer?,"<p>I am studying embedding for word representations. In many dnn libraries, they support embedding layer. And this is really nice tutorial.</p>

<p><a href=""http://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"" rel=""nofollow noreferrer"">Word Embeddings: Encoding Lexical Semantics</a></p>

<p>But I am not still sure how to calculate embed value. In below example, it outputs some value even before any trainings. Does it use some random weights? I realize a purpose of <code>Embedding(2, 5)</code>, but not sure its initial calculation. And I am no sure about how to learn weights of its Embedding too.</p>

<pre><code>word_to_ix = {""hello"": 0, ""world"": 1}
embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings
lookup_tensor = torch.LongTensor([word_to_ix[""hello""]])
hello_embed = embeds(autograd.Variable(lookup_tensor))
print(hello_embed)
--------
Variable containing:
-2.9718  1.7070 -0.4305 -2.2820  0.5237
[torch.FloatTensor of size 1x5]
</code></pre>

<p>I break down my thought to be sure. First of all, upper <code>Embedding(2, 5)</code> is a matrix of shape <code>(2, 5)</code>.</p>

<pre><code>Embedding(2, 5) = 
 [[0.1,-0.2,0.3,0.4,0.1],
 [-0.2,0.1,0.8,0.2,0.3]] # initiated by some function, like random normal distribution
</code></pre>

<p>Then, <code>hello</code> is <code>[1, 0]</code>. Then <code>hello</code> representation is calculated by <code>[1, 0].dot(Embedding(2, 5)) = [0.1,-0.2,0.3,0.4,0.1]</code>. This is actually first row of the Embedding. Am I understanding right?</p>

<hr>

<h1>Updates</h1>

<p>I found a code of embedding which is exactly use normal distribution for its value. Yes, but it is just a default value, and we can set arbitrary weights for embedding layers.
<a href=""https://github.com/chainer/chainer/blob/adba7b846d018b9dc7d19d52147ef53f5e555dc8/chainer/links/connection/embed_id.py#L58"" rel=""nofollow noreferrer"">https://github.com/chainer/chainer/blob/adba7b846d018b9dc7d19d52147ef53f5e555dc8/chainer/links/connection/embed_id.py#L58</a></p>
",Vectorization & Embeddings,initial value embedding layer studying embedding word representation many dnn library support embedding layer really nice tutorial word embeddings encoding lexical semantics still sure calculate embed value example output value even training doe use random weight realize purpose sure initial calculation sure learn weight embedding break thought sure first upper matrix shape representation calculated actually first row embedding understanding right update found code embedding exactly use normal distribution value yes default value set arbitrary weight embedding layer
How is SpaCy&#39;s similarity computed?,"<p>Beginner NLP Question here:</p>
<h1>How does the .similiarity method work?</h1>
<p>Wow spaCy is great! Its tfidf model could be easier to preprocess, but w2v with only one line of code (token.vector)?! - Awesome!</p>
<p>In his  <a href=""https://github.com/cytora/pycon-nlp-in-10-lines/blob/master/00_spacy_intro.ipynb"" rel=""nofollow noreferrer"">10 line tutorial on spaCy</a> andrazhribernik show's us the .similarity method that can be run on tokens, sents, word chunks, and docs.</p>
<p>After <code>nlp = spacy.load('en')</code> and <code>doc = nlp(raw_text)</code>
we can do .similarity queries between tokens and chunks.
However, what is being calculated behind the scenes in this <code>.similarity</code> method?</p>
<p>SpaCy already has the incredibly simple <code>.vector</code>, which computes the w2v vector as trained from the GloVe model (how cool would a <code>.tfidf</code> or <code>.fasttext</code> method be?).</p>
<p>Is the model similarity model simply computing the cosine similarity between these two w2v-GloVe-vectors or doing something else? The specifics aren't clear in the <a href=""https://spacy.io/docs/usage/word-vectors-similarities"" rel=""nofollow noreferrer"">documentation</a>; any help appreciated!</p>
",Vectorization & Embeddings,spacy similarity computed beginner nlp question doe similiarity method work wow spacy great tfidf model could easier preprocess w v one line code token vector awesome line tutorial spacy andrazhribernik show u similarity method run token sent word chunk doc similarity query token chunk however calculated behind scene method spacy already ha incredibly simple computes w v vector trained glove model cool would method model similarity model simply computing cosine similarity two w v glove vector something else specific clear documentation help appreciated
How to see relevence of a feature in any model in ScikitLearn?,"<p>I am using Logistic Regression and Linear SVC from scikit learn for categorisation of a document into 2 categories denoted by label 0 and label 1. I am using TFIDF Vectorizer for feature selection and I have extracted all the non-zero features from test document using transform function of TFIDF Vectorizer on it and now I want to know whether a particular feature is being used for label 1 or label 0. </p>

<p>Basically I want to know if my classifier is giving me answer as label 0, then based on what features it gave that answer.</p>
",Vectorization & Embeddings,see relevence feature model scikitlearn using logistic regression linear svc scikit learn categorisation document category denoted label label using tfidf vectorizer feature selection extracted non zero feature test document using transform function tfidf vectorizer want know whether particular feature used label label basically want know classifier giving answer label based feature gave answer
How word2Vec or wod2Doc understand user sentiments,"<p>I have gone through numerous documents to read about doc2Vec and word2Vec. I do understand how powerful it is to represent the words as a vector and to perform simple operations like vector addition , subtraction to yield meaningful analogy between the words. 
Although one thing I am still not able to understand is how this technique can be used to understand user sentiments . </p>

<p>Can someone please elaborate as to how user sentiments are analysed using these techniques ?
Thanks
Samir</p>
",Vectorization & Embeddings,word vec wod doc understand user sentiment gone numerous document read doc vec word vec understand powerful represent word vector perform simple operation like vector addition subtraction yield meaningful analogy word although one thing still able understand technique used understand user sentiment someone please elaborate user sentiment analysed using technique thanks samir
gensim Doc2Vec vs tensorflow Doc2Vec,"<p>I'm trying to compare my implementation of Doc2Vec (via tf) and gensims implementation. It seems atleast visually that the gensim ones are performing better.</p>

<p>I ran the following code to train the gensim model and the one below that for tensorflow model. My questions are as follows:</p>

<ol>
<li>Is my tf implementation of Doc2Vec correct. Basically is it supposed to be concatenating the word vectors and the document vector to predict the middle word in a certain context?</li>
<li>Does the <code>window=5</code> parameter in gensim mean that I am using two words on either side to predict the middle one? Or is it 5 on either side. Thing is there are quite a few documents that are smaller than length 10.</li>
<li>Any insights as to why Gensim is performing better? Is my model any different to how they implement it?</li>
<li>Considering that this is effectively a matrix factorisation problem, why is the TF model even getting an answer? There are infinite solutions to this since its a rank deficient problem. &lt;- This last question is simply a bonus.</li>
</ol>

<h3>Gensim</h3>

<pre><code>model = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=10, hs=0, min_count=2, workers=cores)
model.build_vocab(corpus)
epochs = 100
for i in range(epochs):
    model.train(corpus)
</code></pre>

<h3>TF</h3>

<pre><code>batch_size = 512
embedding_size = 100 # Dimension of the embedding vector.
num_sampled = 10 # Number of negative examples to sample.


graph = tf.Graph()

with graph.as_default(), tf.device('/cpu:0'):
    # Input data.
    train_word_dataset = tf.placeholder(tf.int32, shape=[batch_size])
    train_doc_dataset = tf.placeholder(tf.int32, shape=[batch_size/context_window])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size/context_window, 1])

    # The variables   
    word_embeddings =  tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))
    doc_embeddings = tf.Variable(tf.random_uniform([len_docs,embedding_size],-1.0,1.0))
    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, (context_window+1)*embedding_size],
                             stddev=1.0 / np.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))

    ###########################
    # Model.
    ###########################
    # Look up embeddings for inputs and stack words side by side
    embed_words = tf.reshape(tf.nn.embedding_lookup(word_embeddings, train_word_dataset),
                            shape=[int(batch_size/context_window),-1])
    embed_docs = tf.nn.embedding_lookup(doc_embeddings, train_doc_dataset)
    embed = tf.concat(1,[embed_words, embed_docs])
    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,
                                   train_labels, num_sampled, vocabulary_size))

    # Optimizer.
    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)
</code></pre>

<h2>Update:</h2>

<p>Check out the jupyter notebook <a href=""https://github.com/sachinruk/doc2vec_tf"" rel=""noreferrer"">here</a> (I have both models working and tested in here). It still feels like the gensim model is performing better in this initial analysis.</p>
",Vectorization & Embeddings,gensim doc vec v tensorflow doc vec trying compare implementation doc vec via tf gensims implementation seems atleast visually gensim one performing better ran following code train gensim model one tensorflow model question follows tf implementation doc vec correct basically supposed concatenating word vector document vector predict middle word certain context doe parameter gensim mean using two word either side predict middle one either side thing quite document smaller length insight gensim performing better model different implement considering effectively matrix factorisation problem tf model even getting answer infinite solution since rank deficient problem last question simply bonus gensim tf update check jupyter notebook model working tested still feel like gensim model performing better initial analysis
Using Text Sentiment as feature in machine learning model?,"<p>I am researching what features I'll have for my machine learning model, with the data I have. My data contains a lot of textdata, so I was wondering how to extract valuable features from it. Contrary to my previous belief, this often consists of representation with Bag-of-words, or something like word2vec: (<a href=""http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction</a>) </p>

<p>Because my understanding of the subject is limited, I dont understand why I can't analyze the text first to get numeric values. (for example: textBlob.sentiment =<a href=""https://textblob.readthedocs.io/en/dev/"" rel=""nofollow noreferrer"">https://textblob.readthedocs.io/en/dev/</a>, google Clouds Natural Language =<a href=""https://cloud.google.com/natural-language/"" rel=""nofollow noreferrer"">https://cloud.google.com/natural-language/</a>)</p>

<p>Are there problems with this, or could I use these values as features for my machine learning model?</p>

<p>Thanks in advance for all the help!</p>
",Vectorization & Embeddings,using text sentiment feature machine learning model researching feature machine learning model data data contains lot textdata wa wondering extract valuable feature contrary previous belief often consists representation bag word something like word vec understanding subject limited dont understand analyze text first get numeric value example textblob sentiment google cloud natural language problem could use value feature machine learning model thanks advance help
How Word Mover&#39;s Distance (WMD) uses word2vec embedding space?,"<p>According to WMD <a href=""http://proceedings.mlr.press/v37/kusnerb15.pdf"" rel=""nofollow noreferrer"">paper</a>, it's inspired by word2vec model and use word2vec vector space for moving document 1 towards document 2 (in the context of Earth Mover Distance metric). From the paper:</p>

<pre><code>Assume we are provided with a word2vec embedding matrix
X ∈ Rd×n for a finite size vocabulary of n words. The 
ith column, xi ∈ Rd, represents the embedding of the ith
word in d-dimensional space. We assume text documents
are represented as normalized bag-of-words (nBOW) vectors,
d ∈ Rn. To be precise, if word i appears ci times in
the document, we denote di = ci/cj (for j=1 to n). An nBOW vector
d is naturally very sparse as most words will not appear in
any given document. (We remove stop words, which are
generally category independent.)
</code></pre>

<p>I understand the concept from the paper, however, I couldn't understand how wmd uses word2vec embedding space from the code in Gensim. </p>

<p><strong><em>Can someone explain it in a simple way? Does it calculate the word vectors in a different way because I couldn't understand where in this code word2vec embedding matrix is used?</em></strong> </p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""nofollow noreferrer"">WMD Fucntion from Gensim:</a></p>

<pre><code>   def wmdistance(self, document1, document2):
    # Remove out-of-vocabulary words.
    len_pre_oov1 = len(document1)
    len_pre_oov2 = len(document2)
    document1 = [token for token in document1 if token in self]
    document2 = [token for token in document2 if token in self]

    dictionary = Dictionary(documents=[document1, document2])
    vocab_len = len(dictionary)

    # Sets for faster look-up.
    docset1 = set(document1)
    docset2 = set(document2)

    # Compute distance matrix.
    distance_matrix = zeros((vocab_len, vocab_len), dtype=double)
    for i, t1 in dictionary.items():
        for j, t2 in dictionary.items():
            if t1 not in docset1 or t2 not in docset2:
                continue
            # Compute Euclidean distance between word vectors.
            distance_matrix[i, j] = sqrt(np_sum((self[t1] - self[t2])**2))

    def nbow(document):
        d = zeros(vocab_len, dtype=double)
        nbow = dictionary.doc2bow(document)  # Word frequencies.
        doc_len = len(document)
        for idx, freq in nbow:
            d[idx] = freq / float(doc_len)  # Normalized word frequencies.
        return d

    # Compute nBOW representation of documents.
    d1 = nbow(document1)
    d2 = nbow(document2)

    # Compute WMD.
    return emd(d1, d2, distance_matrix)
</code></pre>
",Vectorization & Embeddings,word mover distance wmd us word vec embedding space according wmd paper inspired word vec model use word vec vector space moving document towards document context earth mover distance metric paper understand concept paper however understand wmd us word vec embedding space code gensim someone explain simple way doe calculate word vector different way understand code word vec embedding matrix used wmd fucntion gensim
Getting probability of the text given word embedding model in gensim word2vec model,"<p>I am trying to get most probable sequence of word using gensim word2vec model. I have found a pretrained model which provides these files:</p>

<pre><code>word2vec.bin
word2vec.bin.syn0.npy
word2vec.bin.syn1neg.npy
</code></pre>

<p>This is my code trying to get the probability of the sentence with this model:</p>

<pre><code>model = model.wv.load(word_embedding_model_path)
model.hs = 1
model.negative = 0
print model.score(sentence.split("" ""))
</code></pre>

<p>While running this code I am getting this error:</p>

<pre><code>AttributeError: 'Word2Vec' object has no attribute 'syn1'
</code></pre>

<p>Can anyone help me figure out how to solve the problem. In general, I want to use some pretrained model to get the probability of sequence of word appearing together.</p>
",Vectorization & Embeddings,getting probability text given word embedding model gensim word vec model trying get probable sequence word using gensim word vec model found pretrained model provides file code trying get probability sentence model running code getting error anyone help figure solve problem general want use pretrained model get probability sequence word appearing together
How does word embedding/ word vectors work/created?,"<p>How does word2vec create vectors for words? I trained two word2vec models using two different files (from commoncrawl website) but I am getting same word vectors for a given word from both models. </p>

<p>Actually, I have created multiple word2vec models using different text files from the commoncrawl website. Now I want to check which model is better among all. How can select the best model out of all these models and why I am getting same word vectors for different models?</p>

<p>Sorry, If the question is not clear. </p>
",Vectorization & Embeddings,doe word embedding word vector work created doe word vec create vector word trained two word vec model using two different file commoncrawl website getting word vector given word model actually created multiple word vec model using different text file commoncrawl website want check model better among select best model model getting word vector different model sorry question clear
Word level sentence generation using keras,"<p>I am new to keras. I am trying to build word level sentence generation module using keras.
I use a vocabulary size of 8000 and one-hot-vector representation of words in my corpus.</p>

<p>This is my model</p>

<pre><code>model = Sequential()
model.add(LSTM(200, return_sequences=False, unroll=True, stateful=False,input_shape=(1,len(itw))  ))
model.add(Activation('tanh'))
model.add(Dense(len(itw)))
model.add(Activation('softmax'))
optimizer = RMSprop(lr=0.01)
model.compile(loss=""categorical_crossentropy"",optimizer=optimizer,metrics=[""accuracy""])
a,b =batch1()
model.fit(a,b,batch_size=45,nb_epoch=5,verbose=1)
</code></pre>

<p>here input dimension is 3 (as keras lstm expects 3d input)  input shape such that (batch_size,1,8000). I made the 2nd dimension as 1 because I want to feed word by word.
Batch_size is 45 because that is the avarage length of a sentence in the corpus.
All sentences are pretended with a ""START_TOKEN"" and appended with a ""END_TOKEN"".</p>

<p>len(itw) returns length of vocabulary which in my case is 8000</p>

<p>Now after training I wished to loop the generated words back as input until a stop token is encountered to generated a sentence.
But it seems that the model only considers the current input.</p>

<p>I hoped the model to consider the inputs before also and not just the current one.</p>

<p>So how should I change my model</p>

<p>Also is Keras unfit for nlp applications involving varying time serise</p>

<p>How to change the model such that it will generate the next word given n words.
Where n is any natural number</p>
",Vectorization & Embeddings,word level sentence generation using kera new kera trying build word level sentence generation module using kera use vocabulary size one hot vector representation word corpus model input dimension kera lstm expects input input shape batch size made nd dimension want feed word word batch size avarage length sentence corpus sentence pretended start token appended end token len itw return length vocabulary case training wished loop generated word back input stop token encountered generated sentence seems model considers current input hoped model consider input also current one change model also kera unfit nlp application involving varying time serise change model generate next word given n word n natural number
gensim doc2vec &quot;intersect_word2vec_format&quot; command,"<p>Just reading through the doc2vec commands on the gensim page. </p>

<p>I am curious about  the command""intersect_word2vec_format"" . </p>

<p>My understanding of this command is it lets me inject vector values from a pretrained word2vec model into my doc2vec model and then train my doc2vec model using the pretrained word2vec values rather than generating the word vector values from my document corpus. The result is that I get a more accurate doc2vec model because I am using pretrained w2v values which was generated from a much larger corpus of data compared to my relatively small document corpus. </p>

<p>Is my understanding of this command correct or not even close?  ;-) </p>
",Vectorization & Embeddings,gensim doc vec intersect word vec format command reading doc vec command gensim page curious command intersect word vec format understanding command let inject vector value pretrained word vec model doc vec model train doc vec model using pretrained word vec value rather generating word vector value document corpus result get accurate doc vec model using pretrained w v value wa generated much larger corpus data compared relatively small document corpus understanding command correct even close
what is the minimum dataset size needed for good performance with doc2vec?,"<p>How does doc2vec perform when trained on different sized datasets? There is no mention of dataset size in the original corpus, so I am wondering what is the minimum size required to get good performance out of doc2vec. </p>
",Vectorization & Embeddings,minimum dataset size needed good performance doc vec doe doc vec perform trained different sized datasets mention dataset size original corpus wondering minimum size required get good performance doc vec
Character- and word-level convolutional neural network implementation,"<p>To process text, I'm trying to use character- and word-level CNN's to classify messy sentences. Conceptually, it seems this would help with mis-/nontraditional spellings, typos, and truncated words.</p>

<p>I'm confused how to manipulate the data to be fed in and processed correctly.</p>

<p>I'd like to iterate over the sentences in the text, using the character-level CNN to map the words of that sentence to corresponding vectors, and concatenating those vectors to the word-embedding output vectors for those words. The resultant tensor can be used again in another CNN for classification.</p>

<p>But the text is being fed in as a tensor of strings, with each element being a sentence, so iterating as if the text is a list of sentences won't work. </p>

<p>Example pseudocode of what I'm trying to do:</p>

<pre><code>self.X_word_input = tf.placeholder(
    tf.int32, 
    [None, self.max_sentence_length], 
    name='x_word_input'
)

self.X_char_input = tf.placeholder(
    tf.string,
    [None, self.max_char_length],
    name='x_char_input'
)

logits = inference(self.X_word_input,
    self.X_char_input,
    self.max_sentence_length,
    self.max_word_length,
    self.num_classes,
    self.word_vocab_size,
    self.char_vocab_size,
    self.word_embedding_dim,
    self.char_embedding_dim,
    self.word_filter_sizes,
    self.num_word_filters,
    self.char_filter_sizes,
    self.num_char_filters
)

def inference(...):
    for sentence in x_char_input:
        words = sentence.split(' ')
        words_tensor = tf.constant(words)
        char_embedding = get_embedding(words_tensor)
        char_conv = get_convolutions(char_embedding)
        char_logits = get_logits(char_conv)
        words_map.append(char_logits)
    word_embedding = get_embedding(x_word_input)
    combined_embedding = tf.append([word_embedding, words_map], 1)
    combined_conv = get_convolutions(combined_embedding)
    predictions = get_logits(combined_conv)
</code></pre>

<p>Even if I can iterate to extract the sentences as strings, I need to turn each word, in each sentence, into its own tensor of characters so that the character-level CNN can produce the output vector for each word from its character-tensors. </p>

<p>How do I feed in the sentences, break them up, and iteratively generate the word mappings from the character CNN? Thanks!</p>
",Vectorization & Embeddings,character word level convolutional neural network implementation process text trying use character word level cnn classify messy sentence conceptually seems would help mi nontraditional spelling typo truncated word confused manipulate data fed processed correctly like iterate sentence text using character level cnn map word sentence corresponding vector concatenating vector word embedding output vector word resultant tensor used another cnn classification text fed tensor string element sentence iterating text list sentence work example pseudocode trying even iterate extract sentence string need turn word sentence tensor character character level cnn produce output vector word character tensor feed sentence break iteratively generate word mapping character cnn thanks
Gensim word2vec WMD similarity dictionary,"<p>I'm using word2vec on a 1 million abstracts dataset (2 billion words). To find most similar documents, I use the <code>gensim.similarities.WmdSimilarity</code> class. When trying to retrieve the best match using <code>wmd_similarity_index[query]</code>, the calculation spends most of its time building a dictionary. Here is a piece of log:</p>

<pre><code>2017-08-25 09:45:39,441 : INFO : built Dictionary(127 unique tokens: ['empirical', 'model', 'estimating', 'vertical', 'concentration']...) from 2 documents (total 175 corpus positions)                                                        
2017-08-25 09:45:39,445 : INFO : adding document #0 to Dictionary(0 unique tokens: [])          
</code></pre>

<p>What does this part ? Is it dependent on the query ? Is there a way to do these calculations once for all ?</p>

<p><strong>EDIT:</strong> training and scoring phases in my code:</p>

<p>Training and saving to disk:</p>

<pre><code>w2v_size = 300
word2vec = gensim.models.Word2Vec(texts, size=w2v_size, window=9, min_count=5, workers=1, sg=1, hs=1, iter=20) # sg=1 means skip gram is used 
word2vec.save(utils.paths.PATH_DATA_GENSIM_WORD2VEC)
corpus_w2v_wmd_index = gensim.similarities.WmdSimilarity(texts, word2vec.wv)
corpus_w2v_wmd_index.save(utils.paths.PATH_DATA_GENSIM_CORPUS_WORD2VEC_WMD_INDEX)
</code></pre>

<p>Loading and scoring:</p>

<pre><code>w2v = gensim.models.Word2Vec.load(utils.paths.PATH_DATA_GENSIM_WORD2VEC)
words = [t for t in proc_text if t in w2v.wv]
corpus_w2v_wmd_index = gensim.similarities.docsim.Similarity.load(utils.paths.PATH_DATA_GENSIM_CORPUS_WORD2VEC_WMD_INDEX)
scores_w2v = np.array(corpus_w2v_wmd_index[words])  
</code></pre>
",Vectorization & Embeddings,gensim word vec wmd similarity dictionary using word vec million abstract dataset billion word find similar document use class trying retrieve best match using calculation spends time building dictionary piece log doe part dependent query way calculation edit training scoring phase code training saving disk loading scoring
Can we use tf-idf and cosine similarity for document recommendation system?,"<p>The document dataset is having 9000 documents and its growing day by day, most of the documents having average 1-3 paragraphs. </p>

<p>The idea is to make a recommendation with the help of user's past usage of documents. </p>

<p>Lets say the dataset is having 10 documents (main corpus) in total and out of them 4 related to sports, 2 related to Fashion, 4 related to technology. </p>

<p>Now if say user x has read 2 sports and 1 technology related documents then to get the tf-idf of the user x we will combine these 3 documents and assume it as user x corpus. </p>

<p>Now get the tf-idf of the 10 documents (main corpus) which will get us the individual tf idf vector of all those 10 documents. </p>

<p>Now compare the cosine similarity of user x corpus with the main corpus (10 document) </p>

<p>Result should be 2 sports document and 1 technology document which user x has not yet read. </p>

<p>Let me know if this idea makes sense. </p>
",Vectorization & Embeddings,use tf idf cosine similarity document recommendation system document dataset document growing day day document average paragraph idea make recommendation help user past usage document let say dataset document main corpus total related sport related fashion related technology say user x ha read sport technology related document get tf idf user x combine document assume user x corpus get tf idf document main corpus get u individual tf idf vector document compare cosine similarity user x corpus main corpus document result sport document technology document user x ha yet read let know idea make sense
Generate matrix of context or simple word vector embedding in Python,"<p>I have two sentences:
<code>Skies are blue. Grass is green</code></p>

<p>I would like to compute simple matrix of word vector space embedding or matrix of co-occurrences, I am not sure what proper terminology is. But here is that I want.
So I have 6 distinct words from two sentences above, so my matrix will be 6 by 6. Assume that my words have the following ordering corresponding to rows or column ordering: 0 - Skies, 1 - are, 2 - blue, 3 - Grass, 4 - is, 5 - green. Then I would like to count co-occurrence using size of window = 2 (meaning 2 words prior to current word and 2 words after current word). </p>

<ul>
<li>Element with index [0,0] will have value 0, since <code>Skies</code> do not
co-occur with <code>Skies</code>.</li>
<li>Element with index [0,1] will have value,
since <code>are</code> occurs next to <code>Skies</code> only once </li>
<li>Element with index [0,2]
will have value, since <code>blue</code> occurs next to <code>Skies</code> only once.</li>
</ul>

<p>So on and so forth. Is there scikit module for it? I looked at the following <a href=""https://stackoverflow.com/questions/35562789/word-word-co-occurrence-matrix"">question </a>, but it does not seem to answer my question.</p>

<p><strong>Update</strong> This matrix is key object of distributional hypothesis.</p>
",Vectorization & Embeddings,generate matrix context simple word vector embedding python two sentence would like compute simple matrix word vector space embedding matrix co occurrence sure proper terminology want distinct word two sentence matrix assume word following ordering corresponding row column ordering sky blue grass green would like count co occurrence using size window meaning word prior current word word current word element index value since co occur element index value since occurs next element index value since occurs next forth scikit module looked following href doe seem answer question update matrix key object distributional hypothesis
Evaluating Glove model by finding linear algebraic structure of words,"<p>I have built Glove model on my text corpus using in c application following this implementation <a href=""https://github.com/stanfordnlp/GloVe/tree/master/src"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/GloVe/tree/master/src</a>.
I want to find the word embeddings in such a way that</p>

<blockquote>
  <p>If A is related to B and C is related to D, then A-C+B should be equal
  to D. For example, embedding vector arithmetic of
  ""Australia""-""Canberra""+""India"" should be equal to the embedding of
  ""New Delhi"".</p>
</blockquote>

<p>I want to evaluate these embeddings in python.</p>
",Vectorization & Embeddings,evaluating glove model finding linear algebraic structure word built glove model text corpus using c application following implementation want find word embeddings way related b c related c b equal example embedding vector arithmetic australia canberra india equal embedding new delhi want evaluate embeddings python
What are the specifc steps for computing sentence vectors from word2vec word vectors using the averaging method?,"<p>Beginner question, but I am a bit puzzled by this. Hope the answer to this question can benefit other beginners in NLP as well. </p>

<p>Here are some more details: </p>

<p>I know that you can compute sentence vectors from word vectors generated by word2vec. But what are the actual steps involved to make these sentence vectors. Can anyone provide a intuitive example and then some calculations to explain this process?</p>

<p>eg: Suppose I have a sentence with three words: Today is hot. And suppose these words have hypothetical vector values of: (1,2,3)(4,5,6)(7,8,9). Do I get the sentence vector by performing component-wise averaging of these word vectors? And what if the vectors are of different length eg: (1,2)(4,5,6)(7,8,9,23,76) what does the averaging process look like for these cases? </p>
",Vectorization & Embeddings,specifc step computing sentence vector word vec word vector using averaging method beginner question bit puzzled hope answer question benefit beginner nlp well detail know compute sentence vector word vector generated word vec actual step involved make sentence vector anyone provide intuitive example calculation explain process eg suppose sentence three word today hot suppose word hypothetical vector value get sentence vector performing component wise averaging word vector vector different length eg doe averaging process look like case
Evaluating Word2Vec model by finding linear algebraic structure of words,"<p>I have built Word2Vecmodel using gensim library in python.I want to evaluate my word embedding as follows</p>

<blockquote>
  <p>If A is related to B and C is related to D, then A-C+B should be equal to D. For example, embedding vector arithmetic of ""India""-""Rupee""+""Japan"" should be equal to the embedding of ""Yen"".</p>
</blockquote>

<p>I have used in built functions of gensim like predict_output_word,most_similar but couldn't get desired results.</p>

<pre><code>new_model.predict_output_word(['india','rupee','japan'],topn=10)
new_model.most_similar(positive=['india', 'rupee'], negative=['japan'])
</code></pre>

<p>Kindly help me in evaluating my model as per the criteria above.</p>
",Vectorization & Embeddings,evaluating word vec model finding linear algebraic structure word built word vecmodel using gensim library python want evaluate word embedding follows related b c related c b equal example embedding vector arithmetic india rupee japan equal embedding yen used built function gensim like predict output word similar get desired result kindly help evaluating model per criterion
Is there a semantic similarity method that outperforms word2vec approach for semantic accuracy?,"<p>I am looking at various semantic similarity methods such as word2vec, word mover distance (WMD), and fastText. fastText is not better than Word2Vec as for as semantic similarity is concerned. WMD and Word2Vec have almost similar results. </p>

<p>I was wondering if there is an alternative which has outperformed the Word2Vec model for semantic accuracy? </p>

<p><strong>My use case:</strong>
<em>Finding word embeddings for two sentences, and then use cosine similarity to find their similarity.</em> </p>
",Vectorization & Embeddings,semantic similarity method outperforms word vec approach semantic accuracy looking various semantic similarity method word vec word mover distance wmd fasttext fasttext better word vec semantic similarity concerned wmd word vec almost similar result wa wondering alternative ha outperformed word vec model semantic accuracy use case finding word embeddings two sentence use cosine similarity find similarity
Python: clustering similar words based on word2vec,"<p>This might be the naive question which I am about to ask. I have a tokenized corpus on which I have trained Gensim's Word2vec model. The code is as below</p>

<pre><code>site = Article(""http://www.datasciencecentral.com/profiles/blogs/blockchain-and-artificial-intelligence-1"")
site.download()
site.parse()

def clean(doc):
    stop_free = "" "".join([i for i in word_tokenize(doc.lower()) if i not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = "" "".join(lemma.lemmatize(word) for word in punc_free.split())
    snowed = "" "".join(snowball.stem(word) for word in normalized.split())
    return snowed   

b = clean(site.text)
model = gensim.models.Word2Vec([b],min_count=1,size=32)
print(model) ### Prints: Word2Vec(vocab=643, size=32, alpha=0.025) ####
</code></pre>

<p>To cluster similar words, I am using PCA to visualize the clusters of similar words. But the problem is that it is forming only big cluster as seen in the image.</p>

<p><strong>PCA &amp; scatter plot Code:</strong></p>

<pre><code>vocab = list(model.wv.vocab)
X = model[vocab]
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

df = pd.concat([pd.DataFrame(X_pca),
                pd.Series(vocab)],
               axis=1)
df.columns = ['x','y','word']

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
ax.scatter(df['x'],df['y'])
plt.show()
</code></pre>

<p>So, I have three questions here:</p>

<p>1) Is just one article enough to have the clear segregation of the clusters?</p>

<p>2) If I have a model trained with huge corpus and I want to predict the similar words in the new article and visualize them (i.e. words in the article I'm predicting) in the form of clusters, is there a way to do that?</p>

<p>I highly appreciate your suggestions. Thank you.</p>
",Vectorization & Embeddings,python clustering similar word based word vec might naive question ask tokenized corpus trained gensim word vec model code cluster similar word using pca visualize cluster similar word problem forming big cluster seen image pca scatter plot code three question one article enough clear segregation cluster model trained huge corpus want predict similar word new article visualize e word article predicting form cluster way highly appreciate suggestion thank
Context-oriented encoding of WinAPI functions,"<p>Long story short, I've got a list of WinAPI calls from a random program and I want to perform some data analysis on it (f.e. classification or sequence labeling).</p>

<p>Obviously, I cannot work with symbolic names, so I wondering is there any way to represent functions as a vectors based on their context (by analogy with word embedding) or there is no or quite weak dependency beetween context and purpose of function. </p>

<p>So, briefly speaking, <em>I'm looking for the algorithm (or at least for some ideas to start with) which maps WinAPI function to its vector representation, depending on function context (argument list).</em></p>
",Vectorization & Embeddings,context oriented encoding winapi function long story short got list winapi call random program want perform data analysis f e classification sequence labeling obviously work symbolic name wondering way represent function vector based context analogy word embedding quite weak dependency beetween context purpose function briefly speaking looking algorithm least idea start map winapi function vector representation depending function context argument list
Why are all numbers not represented in Glove?,"<p>Not all numbers are represented in Stanford's word embedding framework 'glove'. Why is that?</p>

<p>For example, the vector representation for '8900' exists, but for '8594' it throws an error.</p>
",Vectorization & Embeddings,number represented glove number represented stanford word embedding framework glove example vector representation exists throw error
Recent methods for finding semantic similarity between two short sentences or articles (on a concept level),"<p>I'm working on finding similarities between short sentences and articles. I used many existing methods such as tf-idf, word2vec etc but the results are just okay. The most relevant measure which I found was word moving distance, however, its results are not that better than the other measures. I know it's a challenging problem, however, I am wondering if there are any new methods to find an approximate similarity more on a higher or concept level than just matching words.  Especially, any alternative new methods like word moving distance which looks at slightly higher semantic of a sentence or article?</p>
",Vectorization & Embeddings,recent method finding semantic similarity two short sentence article concept level working finding similarity short sentence article used many existing method tf idf word vec etc result okay relevant measure found wa word moving distance however result better measure know challenging problem however wondering new method find approximate similarity higher concept level matching word especially alternative new method like word moving distance look slightly higher semantic sentence article
similarity measurement among names?,"<p>I have a list of names with me and iam trying to find the most similar 5 names from the list of any given name as a query.
I thought of applying word2vec or else using Text.similar() from nltk.
but iam not sure whether these will work for names as well.</p>

<p>any similarity measure would work for me.
any suggestions?
this not for any project but just i wanted to learn new things.</p>
",Vectorization & Embeddings,similarity measurement among name list name iam trying find similar name list given name query thought applying word vec else using text similar nltk iam sure whether work name well similarity measure would work suggestion project wanted learn new thing
"Optimizing search of similar sentences, Word2Vec","<p>I am trying to find all the similar sentences amongst a set of sentences, and I am wondering how I could optimize it.</p>

<p>I am using a Word2Vec model, so in order to find similar sentences I sum all the vectors in the 1st sentence and 2nd sentence, then do the cosine of both, and if the result is higher than 0.9 I add it to the list of similar sentences.</p>

<p>The problem is right now I am comparing all the sentences with the others, meaning a O(n^2) complexity, which is not so good if I have a large set of sentences.</p>

<p>So my question : <strong>is there any way to pre-process the set of sentences in order to reduce the number of comparisons (and get a O(nlogn) complexity)?</strong></p>

<p>I could not get my head around this as I am pretty new with this Word2Vec representation and I do not really see a way to sort the sentences in a way that would help.</p>
",Vectorization & Embeddings,optimizing search similar sentence word vec trying find similar sentence amongst set sentence wondering could optimize using word vec model order find similar sentence sum vector st sentence nd sentence cosine result higher add list similar sentence problem right comparing sentence others meaning n complexity good large set sentence question way pre process set sentence order reduce number comparison get nlogn complexity could get head around pretty new word vec representation really see way sort sentence way would help
how to prevent overlapping in word2vec?,"<p>As you know, the skip-gram model learns vector representations of elements based on long sequences of elements and the contexts of each. This model has most commonly been applied to natural language by concatenating giant collections of text. These documents of are often concatenated into a single very long line of text, with no distinction of when a new document begins and ends. This ends up not being much of an issue in NLP because the percentage of the model training instances involving overlapping documents is a small percentage of the total number of instances. In Education data, this overlap can be much higher because of shorter sequences and high numbers of users (formerly ""documents"" in NLP). This is also a problem in other behavioral datasets, not just education. The problem manifests itself when inspecting the learned vectors and finding that the model has determined that many of the students' first encountered elements are very similar to the students' very last encountered elements. This is a bi-product of the ""wrapping"" of lines in the input to gensim (instances spanning the end of one student's sequence and the beginning of another). How can I identify where in the code this overlapping occurs and prohibit this overlap from happening during training in gensim. </p>
",Vectorization & Embeddings,prevent overlapping word vec know skip gram model learns vector representation element based long sequence element context model ha commonly applied natural language concatenating giant collection text document often concatenated single long line text distinction new document begin end end much issue nlp percentage model training instance involving overlapping document small percentage total number instance education data overlap much higher shorter sequence high number user formerly document nlp also problem behavioral datasets education problem manifest inspecting learned vector finding model ha determined many student first encountered element similar student last encountered element bi product wrapping line input gensim instance spanning end one student sequence beginning another identify code overlapping occurs prohibit overlap happening training gensim
Pipeline using CountVectorizer (max_df) before tfidf,"<p>Currently i am not sure if this quation is for stackoverflow or another more theoretical statistical QA. But im confused about the following. </p>

<p>I am doing a binairy tekst classification task. For this task i use a pipeline, one of the example codes is below: </p>

<pre><code>pipeline = Pipeline([
('vect', CountVectorizer()),
('tfidf', TfidfTransformer()),
('clf', LogisticRegression())
])

parameters = {
    'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],  
    'vect__stop_words': [None, stopwords.words('dutch'), stopwordList],
    'clf__C': [0.1, 1, 10, 100, 1000]
}
</code></pre>

<p>So nothing really strange about this,  but then i started playing with the parameter options/settings and noted that the code below (so the steps and parameters in the code) had the highest accuracy score (f1 score):</p>

<pre><code>pipeline = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', LogisticRegression())
    ])

parameters = {
    'vect__ngram_range': [(1,1)],  
    'vect__stop_words': [None],
    'vect__max_df': [0.2], 
    'vect__max_features': [10000],
    'clf__C': [100]
}
</code></pre>

<p>So im pleased to sort of find out with which parameter settings and methods i get the highest score, but i cant figure out the exact meaning. As with the 'vectorizor'-step the settings for max_df (ignoring terms that appear in more 20% of the documents) seems to be strange to apply before tfidf (or somehow double) </p>

<p>Furthermore it also uses max_features of 10.000. What step is used before the max_df or the max_features? and how do i interpret max_features setting this parameter and doing tfidf afterwards. Does it then perform a tfidf over the 10.000 features? </p>

<p>For me it seems rather strange to do a tfidf after using parameters such as max_df and max_features? Am i correct? and why? or should i just do what gives the highest outcome.. </p>

<p>I hope someone can help me in the right direction, thanks a lot in advance. </p>
",Vectorization & Embeddings,pipeline using countvectorizer max df tfidf currently sure quation stackoverflow another theoretical statistical qa im confused following binairy tekst classification task task use pipeline one example code nothing really strange started playing parameter option setting noted code step parameter code highest accuracy score f score im sort find parameter setting method get highest score cant figure exact meaning vectorizor step setting max df ignoring term appear document seems strange apply tfidf somehow double furthermore also us max feature step used max df max feature interpret max feature setting parameter tfidf afterwards doe perform tfidf feature seems rather strange tfidf using parameter max df max feature correct give highest outcome hope someone help right direction thanks lot advance
Training a network to find similar bodies of text,"<p>I have multiple text files and I am trying to find a way to identify similar bodies of text. The files themselves consist of an ""average"" sized paragraph. On top of this I also have some data that could be used as lables for the data if I were to go down the root of a neural networks such as a saimese network.</p>

<p>While that was one option another possibility I was wondering about was using something such as doc2vec in order to process all of the paragraphs (with the removal of stopwords and such) and then attempting to find similar files of text based upon the cosine from doc2vec. </p>

<p>I was wondering how do the methods outlined above generally compare to each other in terms of results they produce and is doc2vec robust and accurate enough to consider it a viable option? Also I may be overlooking a good method for this.</p>
",Vectorization & Embeddings,training network find similar body text multiple text file trying find way identify similar body text file consist average sized paragraph top also data could used lables data go root neural network saimese network wa one option another possibility wa wondering wa using something doc vec order process paragraph removal stopwords attempting find similar file text based upon cosine doc vec wa wondering method outlined generally compare term result produce doc vec robust accurate enough consider viable option also may overlooking good method
Tfidf Vectorizer not working,"<p>I have a corpus(<strong>Hotel Reviews</strong>) and I want to do some NLP process including Tfidf. My problem is when I Applied Tfidf and print 100 features it doesn't appear as a single word but the entire sentence.
Here is my code:</p>

<p><strong>Note: clean_doc is a function return my corpus cleaning from stopwords, stemming, and etc</strong></p>

<pre><code>vectorizer = TfidfVectorizer(analyzer='word',tokenizer=clean_doc, 
max_features=100, lowercase = False, ngram_range=(1,3), min_df = 1)
vz  = vectorizer.fit_transform(list(data['Review']))
feature_names = vectorizer.get_feature_names()
for feature in feature_names:
  print(feature)
</code></pre>

<p>it returns something like this:</p>

<pre><code>love view  good room
food amazing recommended 
bad services location far
-----
</code></pre>

<p>any idea why? Thanks in Advance</p>
",Vectorization & Embeddings,tfidf vectorizer working corpus hotel review want nlp process including tfidf problem applied tfidf print feature appear single word entire sentence code note clean doc function return corpus cleaning stopwords stemming etc return something like idea thanks advance
how does gensim&#39;s word2vec differ from tensorflow vector representation?,"<p>I am fairly new to the NLP embedding world. I used gensim's word2vec model and tensorflow vector representation.</p>

<p>I have a question that while training gensim's word2vec model it takes tokenize sentences, while tensorflow takes a long list of words. How does it differ in training. Is there any quality impact?
Also how does then tensorflow cater to the needs of skip-gram as now the data is a list of words and no more sentences. 
I am referring to the tensorflow's tutorial found at link <a href=""https://www.tensorflow.org/tutorials/word2vec"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/word2vec</a></p>

<p>Pardon me if my understanding in this domain is wrong would appreciate if my understanding is cleared.<br>
Thank you for your guidance and help.</p>
",Vectorization & Embeddings,doe gensim word vec differ tensorflow vector representation fairly new nlp embedding world used gensim word vec model tensorflow vector representation question training gensim word vec model take tokenize sentence tensorflow take long list word doe differ training quality impact also doe tensorflow cater need skip gram data list word sentence referring tensorflow tutorial found link pardon understanding domain wrong would appreciate understanding cleared thank guidance help
Word2vec: add external word to every context,"<p>I'm looking for a simple ""hack"" to implement the following idea: I want to have a specific word appear artificially in the context of every word (the underlying goal is to try and use word2vec for supervised sentence classification).</p>

<p>An example is best:</p>

<p>Say I have the sentence: <code>""The dog is in the garden""</code>, and a window of 1.</p>

<p>So we would get the following pais of (target, context):</p>

<pre><code>(dog, The), (dog, is), (is, dog), (is, in), etc.
</code></pre>

<p>But what I would like to feed to the word2vec algo is this:</p>

<pre><code>(dog, The), (dog, is), **(dog, W)**, (is, dog), (is, in), **(is, W)**, etc.,
</code></pre>

<p>as if my word W was in the context of every word.</p>

<p>where W is a word of my choosing, not in the existing vocabulary.</p>

<p>Is there an easy way to do this in R or python ?</p>
",Vectorization & Embeddings,word vec add external word every context looking simple hack implement following idea want specific word appear artificially context every word underlying goal try use word vec supervised sentence classification example best say sentence window would get following pais target context would like feed word vec algo word w wa context every word w word choosing existing vocabulary easy way r python
"How to use word embeddings/word2vec .. differently? With an actual, physical dictionary","<p>If my title is incorrect/could be better, please let me know.</p>

<p>I've been trying to find an existing paper/article describing the problem that I'm having: I'm trying to create vectors for words so that they are equal to the sum of their parts.
For example: Cardinal(the bird) would be equal to the vectors of: red, bird, and ONLY that.
In order to train such a model, the input might be something like a dictionary, where each word is defined by it's attributes.
Something like: </p>

<p>Cardinal: bird, red, ....</p>

<p>Bluebird: blue, bird,....</p>

<p>Bird: warm-blooded, wings, beak, two eyes, claws....</p>

<p>Wings: Bone, feather....</p>

<p>So in this instance, each word-vector is equal to the sum of the word-vector of its parts, and so on.</p>

<p>I understand that in the original word2vec, semantic distance was preserved, such that Vec(Madrid)-Vec(Spain)+Vec(Paris) = approx Vec(Paris).</p>

<p>Thanks!</p>

<p>PS: Also, if it's possible, new words should be able to be added later on.</p>
",Vectorization & Embeddings,use word embeddings word vec differently actual physical dictionary title incorrect could better please let know trying find existing paper article describing problem trying create vector word equal sum part example cardinal bird would equal vector red bird order train model input might something like dictionary word defined attribute something like cardinal bird red bluebird blue bird bird warm blooded wing beak two eye claw wing bone feather instance word vector equal sum word vector part understand original word vec semantic distance wa preserved vec madrid vec spain vec paris approx vec paris thanks p also possible new word able added later
Compare texts within a table in python,"<p>I want to compare text in a python list with each other .
For example </p>

<pre><code>Url         | text
            |
www.xyz.com | "" hello bha njik **bhavd bhavd** bjavd manhbd kdkndsik wkjdk""
            | 
www.abc.com | ""bhavye jsbsdv sjbs jcsbjd adjbsd jdfhjdb jdshbjf jdsbjf""
            |
www.lokj.com| ""bsjgad adhuad jadshjasd kdashda kdajikd kdfsj **bhavd bhavd** ""
</code></pre>

<p>Now I want to compare 1st text with  other rows   so as to know how many words are similar in the texts.
and progressively second row with the following rows and so on ....</p>

<p>What should be the approach i use and what data structure shall I use ?  </p>
",Vectorization & Embeddings,compare text within table python want compare text python list example want compare st text row know many word similar text progressively second row following row approach use data structure shall use
String similarity TF-IDF Bag of words or Word2vec,"<p>I am trying to create an application that computes the similarity between 2 strings.
The strings are not long. 3 Sentences long at maximum.
I did some research and I came across some possible solution paths.</p>

<p>First one use bag of words: count words and compare the 2 produced vectors ( cosine similarity)</p>

<p>The second use TF-IDF and compare produced vectors.</p>

<p>The third is use word2vec and compare vectors.</p>

<p>Now for the questions.</p>

<p>Performance wise is word2vec performance better that TF-IDF for short sentences?</p>

<p>What is the best way to train word2vec model? Should I use a large amount of text ( wikipedia dump for example) or train it using just the sentences that are being compared.</p>

<p>How to get sentence similarity from word2vec. should I average the words in each sentence or is there a better solution?</p>
",Vectorization & Embeddings,string similarity tf idf bag word word vec trying create application computes similarity string string long sentence long maximum research came across possible solution path first one use bag word count word compare produced vector cosine similarity second use tf idf compare produced vector third use word vec compare vector question performance wise word vec performance better tf idf short sentence best way train word vec model use large amount text wikipedia dump example train using sentence compared get sentence similarity word vec average word sentence better solution
How can I improve the cosine similarity of two documents(sentences) in doc2vec model?,"<p>I am building a NLP chat application in Python using <code>gensim</code> library through <code>doc2vec</code> model. I have hard coded documents and given a set of training examples, I am testing the model by throwing a user question and then finding most similar documents as a first step. In this case my test question is an exact copy of a document from training example. </p>

<pre><code>import gensim
from gensim import models
sentence = models.doc2vec.LabeledSentence(words=[u'sampling',u'what',u'is',u'tell',u'me',u'about'],tags=[""SENT_0""])
sentence1 = models.doc2vec.LabeledSentence(words=[u'eligibility',u'what',u'is',u'my',u'limit',u'how',u'much',u'can',u'I',u'claim'],tags=[""SENT_1""])
sentence2 = models.doc2vec.LabeledSentence(words=[u'eligibility',u'I',u'am',u'retiring',u'how',u'much',u'can',u'claim',u'have', u'resigned'],tags=[""SENT_2""])
sentence3 = models.doc2vec.LabeledSentence(words=[u'what',u'is',u'my',u'eligibility',u'post',u'my',u'promotion'],tags=[""SENT_3""])
sentence4 = models.doc2vec.LabeledSentence(words=[u'what',u'is', u'my',u'eligibility' u'post',u'my',u'promotion'], tags=[""SENT_4""])
sentences = [sentence, sentence1, sentence2, sentence3, sentence4]
class LabeledLineSentence(object):
    def __init__(self, filename):
        self.filename = filename
    def __iter__(self):
        for uid, line in enumerate(open(filename)):
            yield LabeledSentence(words=line.split(), labels=['SENT_%s' % uid])
model = models.Doc2Vec(alpha=0.03, min_alpha=.025, min_count=2)
model.build_vocab(sentences)
for epoch in range(30):
    model.train(sentences, total_examples=model.corpus_count, epochs = model.iter)
    model.alpha -= 0.002  # decrease the learning rate`
    model.min_alpha = model.alpha  # fix the learning rate, no decay
model.save(""my_model.doc2vec"")
model_loaded = models.Doc2Vec.load('my_model.doc2vec')
print (model_loaded.docvecs.most_similar([""SENT_4""]))
</code></pre>

<p>Result:</p>

<pre><code>[('SENT_1', 0.043695494532585144), ('SENT_2', 0.0017897281795740128), ('SENT_0', -0.018954679369926453), ('SENT_3', -0.08253869414329529)]
</code></pre>

<p>Similarity of <code>SENT_4</code> and <code>SENT_3</code> is only <code>-0.08253869414329529</code> when it should be 1 since they are exactly same. How should I improve this accuracy? Is there a specific way of training documents and I am missing something out?  </p>
",Vectorization & Embeddings,improve cosine similarity two document sentence doc vec model building nlp chat application python using library model hard coded document given set training example testing model throwing user question finding similar document first step case test question exact copy document training example result similarity since exactly improve accuracy specific way training document missing something
Group texts based on their similarity to LDA topics/feature clusters,"<p>I am working on a database containing forum threads content (first post + replies). I would like to group/clusterize these documents based on the topics of the first posts. </p>

<p>I computed LDA/LSI topics and tried K-means clusterizing, but I can't find documentation on how to get the related text documents from the cluster. I only get the main features of each cluster, but I want the whole documents or like some ID associated to it from each cluster.</p>

<p>I also tried a classifier and a recommender with known machine learning algorithms, but their results are not really useful. I also tried computing similarity using word2vec and doc2vec, the results are not very precise either.</p>

<p>So to sum up, I would like to group text documents based on their similar features/topics, without loosing a link to their content (like an ID). I have also more advanced methods in mind but I would like this to work first. The goal is to group posts from users that have the same issues using software and maybe the same reasons for these issues. </p>
",Vectorization & Embeddings,group text based similarity lda topic feature cluster working database containing forum thread content first post reply would like group clusterize document based topic first post computed lda lsi topic tried k mean clusterizing find documentation get related text document cluster get main feature cluster want whole document like id associated cluster also tried classifier recommender known machine learning algorithm result really useful also tried computing similarity using word vec doc vec result precise either sum would like group text document based similar feature topic without loosing link content like id also advanced method mind would like work first goal group post user issue using software maybe reason issue
Cosine similarity alternative for tf-idf (triangle inequality),"<p>I am trying to use tf-idf to cluster similar documents. One of the major drawback of my system is that it uses cosine similarity to decide which vectors should be group together. </p>

<p>The problem is that cosine similarity does not satisfy triangle inequality. Because in my case I cannot have the same vector in multiple clusters, I have to merge every cluster with an element in common, which can cause two documents to be grouped together even if they're not similar to each other.</p>

<p>Is there another way of measure the similarity of two documents so that:</p>

<ul>
<li>Vectors score as very similar based on their direction regardless of their magnitude</li>
<li>Satisfy triangle inequality: if A is similar to B and B is similar to C then A is also similar to C</li>
</ul>
",Vectorization & Embeddings,cosine similarity alternative tf idf triangle inequality trying use tf idf cluster similar document one major drawback system us cosine similarity decide vector group together problem cosine similarity doe satisfy triangle inequality case vector multiple cluster merge every cluster element common cause two document grouped together even similar another way measure similarity two document vector score similar based direction regardless magnitude satisfy triangle inequality similar b b similar c also similar c
What method should I use to convert words into features for Machine Learning applications?,"<p>I am planning on building a gender classifier. I know the two popular models are tf-idf and word2vec. 
While tf-idf focuses on the importance of a word in a document and similarity of documents, word2vec focuses more on the relationship between words and similarity between them. </p>

<p>However none of theme seem to be perfect for building vector features to be used for gender classification. Is there any other alternative vectorization model that might suit this task? </p>
",Vectorization & Embeddings,method use convert word feature machine learning application planning building gender classifier know two popular model tf idf word vec tf idf focus importance word document similarity document word vec focus relationship word similarity however none theme seem perfect building vector feature used gender classification alternative vectorization model might suit task
&quot;attention heads&quot; in seq2seq.embedding_attention_seq2seq of tensorflow,"<p>I'm new to the tensorflow and trying to implement the ""seq2seq"" model according to the tutorial. I'm not sure about one argument ""num_heads"" (default=1) of the func ""embedding_attention_seq2seq"". What does it represent? I didn't find it in the related papers.</p>
",Vectorization & Embeddings,attention head seq seq embedding attention seq seq tensorflow new tensorflow trying implement seq seq model according tutorial sure one argument num head default func embedding attention seq seq doe represent find related paper
Combing the NLTK text features with sklearn Vectorized features,"<p>I am trying to combine the dict type features used in NLTK along with the SKLEARN tfidf feature for each instance. </p>

<p>Sample Input:
instances=[[""I am working with text data""],[""This is my second sentence""]]
instance = ""I am working with text data ""</p>

<pre><code>    def generate_features(instance):
        featureset[""suffix""]=tokenize(instance)[-1]
        featureset[""tfidf""]=self.tfidf.transform(instance)
        return features

    from sklearn.linear_model import LogisticRegressionCV
    from nltk.classify.scikitlearn import SklearnClasskifier
    self.classifier = SklearnClassifier(LogisticRegressionCV())
    self.classifier.train(feature_sets)
</code></pre>

<p>This tfidf is trained on all the instances. But when I train the nltk classifier using this featureset it throws the following error.</p>

<pre><code>self.classifier.train(feature_sets)
File ""/Library/Python/2.7/site-packages/nltk/classify/scikitlearn.py"", line 115, in train
X = self._vectorizer.fit_transform(X)
File ""/Library/Python/2.7/site
packages/sklearn/feature_extraction/dict_vectorizer.py"", line 226, in fit_transform
return self._transform(X, fitting=True)
File ""/Library/Python/2.7/site-packages/sklearn/feature_extraction/dict_vectorizer.py"", line 174, in _transform
values.append(dtype(v))
TypeError: float() argument must be a string or a number
</code></pre>

<p>I understand the issue here, that it cannot vectorize the already vectorized features. But is there a way to fix this ?</p>
",Vectorization & Embeddings,combing nltk text feature sklearn vectorized feature trying combine dict type feature used nltk along sklearn tfidf feature instance sample input instance working text data second sentence instance working text data tfidf trained instance train nltk classifier using featureset throw following error understand issue vectorize already vectorized feature way fix
Top m topics in a collection of comments,"<p>I have a collection of comments and each comment discusses a topic. I want to figure out the top m topics discussed in these comments. Also, I am receiving these comments in an online fashion(i.e. I don't get the entire comments in one go, instead I have to process these comments one-by-one). I thought of using Word2Vec for feature extraction and then applying some clustering algorithm like k-means(cluster would correspond to a topic) and then I can get the answer from the top m clusters(which have most number of points in them). But the problem is that I don't know the number of clusters and also at any point of time, the number of different topics(clusters) is not fixed because a new comment might discuss a new topic(so, this problem can't be solved by applying k-means with different values of k). So, should I use some other clustering algorithm(like DBSCAN) and what should be the approach in that case or should I use a totally different approach?</p>
",Vectorization & Embeddings,top topic collection comment collection comment comment discus topic want figure top topic discussed comment also receiving comment online fashion e get entire comment one go instead process comment one one thought using word vec feature extraction applying clustering algorithm like k mean cluster would correspond topic get answer top cluster number point problem know number cluster also point time number different topic cluster fixed new comment might discus new topic problem solved applying k mean different value k use clustering algorithm like dbscan approach case use totally different approach
How to combine tfidf features with selfmade features,"<p>For a simple web page classification system I am trying to combine some selfmade features (frequency of HTML tags, frequency of certain word collocations) with the features obtained after applying tfidf. I am facing the following problem, however, and I don't really know how to proceed from here.</p>

<p>Right now I am trying to put all of these together in one dataframe, mainly by following the code from the following <a href=""https://github.com/JonathanReeve/milton-analysis/blob/v0.1/tfidf-scikit.ipynb"" rel=""nofollow noreferrer"">link</a> :</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

vectorizer = TfidfVectorizer(stop_words=""english"")
X_train_counts = vectorizer.fit_transform(train_data['text_no_punkt'])
feature_names = vectorizer.get_feature_names()
dense = X_train_counts.todense()
denselist = dense.tolist()

tfidf_df = pd.DataFrame(denselist, columns=feature_names, index=train_data['text_no_punkt'])
</code></pre>

<p>But this doesn't return the index (from 0 to 2464) I had in my original dataframe with the other features, neither does it seem to produce readable column names and instead of using the different words as titles, it uses numbers.</p>

<p>Furthermore I am not sure if this is the right way to combine features as this will result in an extremely high-dimensional dataframe which will probably not benefit the classifiers.</p>
",Vectorization & Embeddings,combine tfidf feature selfmade feature simple web page classification system trying combine selfmade feature frequency html tag frequency certain word collocation feature obtained applying tfidf facing following problem however really know proceed right trying put together one dataframe mainly following code following link return index original dataframe feature neither doe seem produce readable column name instead using different word title us number furthermore sure right way combine feature result extremely high dimensional dataframe probably benefit classifier
word2vec check next/prev word probability,"<p>While using word2vec, if the current word is <code>US</code> or <code>monday</code>, how can I check that word <code>the</code> has a high probability to come before <code>US</code>; and <code>next</code> has a common occurrence before the word <code>monday</code>, and so on and so forth?</p>
",Vectorization & Embeddings,word vec check next prev word probability using word vec current word check word ha high probability come ha common occurrence word forth
How to structure an LSTM neural network for classification,"<p>I have data that has various conversations between two people. Each sentence has some type of classification. I am attempting to use an NLP net to classify each sentence of the conversation. I tried a convolution net and get decent results (not ground breaking tho). I figured that since this a back and forth conversation, and LSTM net may produce better results, because what was previously said may have a large impact on what follows.</p>

<p><a href=""https://i.sstatic.net/lpl08.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/lpl08.png"" alt=""Type of RNN nets""></a></p>

<p>If I follow the structure above, I would assume that I am doing a many-to-many. My data looks like.</p>

<pre><code>X_train = [[sentence 1],  
           [sentence 2],
           [sentence 3]]
Y_train = [[0],
           [1],
           [0]]
</code></pre>

<p>Data has been processed using word2vec. I then design my network as follows..</p>

<pre><code>model = Sequential()      
model.add(Embedding(len(vocabulary),embedding_dim,
          input_length=X_train.shape[1]))
model.add(LSTM(88))
model.add(Dense(1,activation='sigmoid'))
model.compile(optimizer='rmsprop',loss='binary_crossentropy',
              metrics['accuracy'])
model.fit(X_train,Y_train,verbose=2,nb_epoch=3,batch_size=15)
</code></pre>

<p>I assume that this setup will feed one batch of sentences in at a time. However, if in model.fit, shuffle is not equal to false its receiving shuffled batches, so why is an LSTM net even useful in this case? From research on the subject, to achieve a many-to-many structure one would need to change the LSTM layer too</p>

<pre><code>model.add(LSTM(88,return_sequence=True))
</code></pre>

<p>and the output layer would need to be...</p>

<pre><code>model.add(TimeDistributed(Dense(1,activation='sigmoid')))
</code></pre>

<p>When switching to this structure I get an error on the input size. I'm unsure of how to reformat the data to meet this requirement, and also how to edit the embedding layer to receive the new data format.</p>

<p>Any input would be greatly appreciated. Or if you have any suggestions on a better method, I am more than happy to hear them!</p>
",Vectorization & Embeddings,structure lstm neural network classification data ha various conversation two people sentence ha type classification attempting use nlp net classify sentence conversation tried convolution net get decent result ground breaking tho figured since back forth conversation lstm net may produce better result wa previously said may large impact follows follow structure would assume many many data look like data ha processed using word vec design network follows assume setup feed one batch sentence time however model fit shuffle equal false receiving shuffled batch lstm net even useful case research subject achieve many many structure one would need change lstm layer output layer would need switching structure get error input size unsure reformat data meet requirement also edit embedding layer receive new data format input would greatly appreciated suggestion better method happy hear
Keras SimpleRNN input shape and masking,"<p>Newbie to Keras alert!!!</p>

<p>I've got some questions related to Recurrent Layers in Keras (over theano)</p>

<ol>
<li>How is the input supposed to be formatted regarding timesteps (say for instance I want a layer that will have 3 timesteps 1 in the future 1 in the past and 1 current) I see some <a href=""https://stackoverflow.com/questions/36136562/python-keras-simplernn-wrong-number-of-dimensions-on-model-fit"" title=""input for simpleRNN question"">answers</a> and the API proposing padding and using the embedding layer or to shape the input using a time window (3 in this case) and in any case I can't make heads or tails of the API and SimpleRNN examples are scarce and don't seem to agree.</li>
<li>How would the input time window formatting work with a masking layer?</li>
<li>Some related answers propose performing masking with an embedding layer. What does masking have to do with embedding layers anyway, aren't embedding layers basically 1-hot word embeddings? (my application would use phonemes or characters as input)</li>
</ol>
",Vectorization & Embeddings,kera simplernn input shape masking newbie kera alert got question related recurrent layer kera theano input supposed formatted regarding timesteps say instance want layer timesteps future past current see answer api proposing padding using embedding layer shape input using time window case case make head tail api simplernn example seem agree would input time window formatting work masking layer related answer propose performing masking embedding layer doe masking embedding layer anyway embedding layer basically hot word embeddings application would use phoneme character input
Keras SimpleRNN input shape and masking,"<p>Newbie to Keras alert!!!</p>

<p>I've got some questions related to Recurrent Layers in Keras (over theano)</p>

<ol>
<li>How is the input supposed to be formatted regarding timesteps (say for instance I want a layer that will have 3 timesteps 1 in the future 1 in the past and 1 current) I see some <a href=""https://stackoverflow.com/questions/36136562/python-keras-simplernn-wrong-number-of-dimensions-on-model-fit"" title=""input for simpleRNN question"">answers</a> and the API proposing padding and using the embedding layer or to shape the input using a time window (3 in this case) and in any case I can't make heads or tails of the API and SimpleRNN examples are scarce and don't seem to agree.</li>
<li>How would the input time window formatting work with a masking layer?</li>
<li>Some related answers propose performing masking with an embedding layer. What does masking have to do with embedding layers anyway, aren't embedding layers basically 1-hot word embeddings? (my application would use phonemes or characters as input)</li>
</ol>
",Vectorization & Embeddings,kera simplernn input shape masking newbie kera alert got question related recurrent layer kera theano input supposed formatted regarding timesteps say instance want layer timesteps future past current see answer api proposing padding using embedding layer shape input using time window case case make head tail api simplernn example seem agree would input time window formatting work masking layer related answer propose performing masking embedding layer doe masking embedding layer anyway embedding layer basically hot word embeddings application would use phoneme character input
Gensim save_word2vec_format() vs. model.save(),"<p>I am using gensim version <code>0.12.4</code> and have trained two separate word embeddings using the same text and same parameters. After training I am calculating the Pearsons correlation between the word occurrence-frequency and vector-length.  One model I trained using <code>save_word2vec_format(fname, binary=True)</code> and then loaded using <code>load_word2vec_format</code> the other I trained using <code>model.save(fname)</code> and then loaded using <code>Word2Vec.load()</code>. I understand that the word2vec algorithm is non deterministic so the results will vary however the difference in the correlation between the two models is quite drastic. Which method should I be using in this instance?</p>
",Vectorization & Embeddings,gensim save word vec format v model save using gensim version trained two separate word embeddings using text parameter training calculating pearsons correlation word occurrence frequency vector length one model trained using loaded using trained using loaded using understand word vec algorithm non deterministic result vary however difference correlation two model quite drastic method using instance
Is there a way to load the wiki-fasttext model faster with load_word2vec_format,"<p>Loading the wiki-fasttext model with the gensim library takes <strong>six</strong> minutes. </p>

<p>I'm aware of ways to cache the model but I'm looking for ways to speedup the initial model loading. The specific api is below:</p>

<pre><code>en_model = KeyedVectors.load_word2vec_format(os.path.join(root_dir, model_file))
</code></pre>

<p>Granted, wiki-fasttext a very large model, however I have load the same model in many languages.  </p>
",Vectorization & Embeddings,way load wiki fasttext model faster load word vec format loading wiki fasttext model gensim library take six minute aware way cache model looking way speedup initial model loading specific api granted wiki fasttext large model however load model many language
Implementing Word2Vec for stemming/lemmatizing Turkish,"<p>I'm working on building a stemming programm for Turkish which is a morphologically rich language. None of the ressources that I've found are accurate enough for my purpose and I don't think they are built on deep learning technology. </p>

<p>How could I use word embeddings to train a stemmer/lemmatizer? What are the main steps I should be considering?</p>
",Vectorization & Embeddings,implementing word vec stemming lemmatizing turkish working building stemming programm turkish morphologically rich language none ressources found accurate enough purpose think built deep learning technology could use word embeddings train stemmer lemmatizer main step considering
Document similarity: Vector embedding versus Tf-Idf performance?,"<p>I have a collection of documents, where each document is rapidly growing with time. The task is to find similar documents at any fixed time. I have two potential approaches:</p>

<ol>
<li><p>A vector embedding (word2vec, GloVe or fasttext), averaging over word vectors in a document, and using cosine similarity.</p></li>
<li><p>Bag-of-Words: tf-idf or its variations such as BM25. </p></li>
</ol>

<p>Will one of these  yield a significantly better result? Has someone done a quantitative comparison of tf-idf versus averaging word2vec for document similarity? </p>

<p>Is there another approach, that allows to dynamically refine the document's vectors as more text is added? </p>
",Vectorization & Embeddings,document similarity vector embedding versus tf idf performance collection document document rapidly growing time task find similar document fixed time two potential approach vector embedding word vec glove fasttext averaging word vector document using cosine similarity bag word tf idf variation bm one yield significantly better result ha someone done quantitative comparison tf idf versus averaging word vec document similarity another approach allows dynamically refine document vector text added
Word2vec classification and clustering tensorflow,"<p>I am trying to cluster some sentences using similarity (maybe cosine) and then maybe use a classifier to put text in predefined classes. </p>

<p>My idea is to use tensorflow to generate the word embedding then average them for each sentence. Next use a clustering/classification algorithm.</p>

<p>Does tensorflow provide ready to use word2vec generation algorithm?</p>

<p>Would a bag of words model generate a good output?</p>
",Vectorization & Embeddings,word vec classification clustering tensorflow trying cluster sentence using similarity maybe cosine maybe use classifier put text predefined class idea use tensorflow generate word embedding average sentence next use clustering classification algorithm doe tensorflow provide ready use word vec generation algorithm would bag word model generate good output
Trying to understand CNNs for NLP tutorial using Tensorflow,"<p>I am following <a href=""http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"" rel=""nofollow noreferrer"">this tutorial</a> in order to understand CNNs in NLP. There are a few things which I don't understand despite having the code in front of me. I hope somebody can clear a few things up here.</p>

<hr>

<p>The first rather minor thing is the <code>sequence_length</code>parameter of the <code>TextCNN</code> object. In the example on github this is just <code>56</code> which I think is the max-length of all sentences in the training data. This means that  <code>self.input_x</code> is  a 56-dimensional vector which will contain just the indices from the dictionary of a sentence for each word. </p>

<p>This list goes into <code>tf.nn.embedding_lookup(W, self.intput_x)</code> which will return a matrix consisting of the word embeddings of those words given by <code>self.input_x</code>. According to <a href=""https://stackoverflow.com/a/34877590/826983"">this answer</a> this operation is similar to using indexing with numpy:</p>

<pre class=""lang-py prettyprint-override""><code>matrix = np.random.random([1024, 64]) 
ids = np.array([0, 5, 17, 33])
print matrix[ids]
</code></pre>

<p>But the problem here is that <code>self.input_x</code> most of the time looks like <code>[1 3 44 25 64 0 0 0 0 0 0 0 .. 0 0]</code>. So am I correct if I assume that <code>tf.nn.embedding_lookup</code> ignores the value 0?</p>

<hr>

<p>Another thing I don't get is how <code>tf.nn.embedding_lookup</code> is working here:</p>

<pre class=""lang-py prettyprint-override""><code># Embedding layer
with tf.device('/cpu:0'), tf.name_scope(""embedding""):
    W = tf.Variable(
        tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),
            name=""W"")
    self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)
    self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)
</code></pre>

<p>I assume, taht <code>self.embedded_chars</code> is the matrix which is the <em>actual</em> input to the CNN where each row represents the word embedding of <em>one</em> word. But how can <code>tf.nn.embedding_lookup</code> know about those indices given by <code>self.input_x</code>? </p>

<hr>

<p>The last thing which I don't understand here is </p>

<blockquote>
  <p><code>W</code> is our embedding matrix that we learn during training. We initialize it using a random uniform distribution. <code>tf.nn.embedding_lookup</code> creates the actual embedding operation. The result of the embedding operation is a 3-dimensional tensor of shape <code>[None, sequence_length, embedding_size]</code>.</p>
</blockquote>

<p>Does this mean that we are <em>actually</em> learning the word embeddings <em>here</em>? The tutorial states at the beginning:</p>

<blockquote>
  <p>We will not used pre-trained word2vec vectors for our word embeddings. Instead, we learn embeddings from scratch.</p>
</blockquote>

<p>But I don't see a line of code where this is actually happening. The <a href=""https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py#L22"" rel=""nofollow noreferrer"">code of the embedding layer</a> does not look like as if there is anything being trained or learned - so where is it happening?</p>
",Vectorization & Embeddings,trying understand cnns nlp tutorial using tensorflow following tutorial order understand cnns nlp thing understand despite code front hope somebody clear thing first rather minor thing parameter object example github think max length sentence training data mean dimensional vector contain index dictionary sentence word list go return matrix consisting word embeddings word given according problem time look like correct assume ignores value another thing get working assume taht matrix actual input cnn row represents word embedding one word know index given last thing understand embedding matrix learn training initialize using random uniform distribution creates actual embedding operation result embedding operation dimensional tensor shape doe mean actually learning word embeddings tutorial state beginning used pre trained word vec vector word embeddings instead learn embeddings scratch see line code actually happening code embedding layer doe look like anything trained learned happening
Implement Character Convolution in Keras,"<p>I am tring to implement this <a href=""https://arxiv.org/abs/1603.01354"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1603.01354</a> in Keras, but I have a problem with Convolutional Layer. I don't understand how to set the hyperparameters like the nb_filter ecc., the real problem infact is to match the max pool output with the embedding word layer output.</p>

<p>MAX_CHARACTER_LENGTH 61
MAX_SEQUENCE_LENGTH 124</p>

<p>INPUT DATA</p>

<pre><code> x_train = pad_sequences(dataset.token_indices['train'],  maxlen=MAX_SEQUENCE_LENGTH)
 x_char_train= np.array(dataset.character_indices['train'])
 x_char_train=x_char_train.reshape((x_char_train.shape[0],  x_char_train.shape[1]*x_char_train.shape[2]))
 y_train = pad_sequences(dataset.label_indices['train'], maxlen=MAX_SEQUENCE_LENGTH)
 x_val = pad_sequences(dataset.token_indices['valid'], maxlen=MAX_SEQUENCE_LENGTH)
 x_char_val= np.array(dataset.character_indices['valid'])
 x_char_val=x_char_val.reshape((x_char_val.shape[0],   x_char_val.shape[1]*x_char_val.shape[2]))
 y_val = pad_sequences(dataset.label_indices['valid'], maxlen=MAX_SEQUENCE_LENGTH)
 y_train=np.expand_dims(y_train,-1)
 y_val=np.expand_dims(y_val,-1)
</code></pre>

<p>THE MODEL</p>

<pre><code>EMBEDDING_WORD_DIM= 100
EMBEDDING_CHAR_DIM= 30
N_FILTERS=20
embedding_char= True

class Model(object):
    def __init__(self, embedding_weights, dictonary_size, MAX_SEQUENCE_LENGTH,MAX_CHARACTER_LENGTH, alfabeth_size, tags):
       word_input= Sequential()
       word_input.add(Embedding(dictonary_size + 1,
                                EMBEDDING_WORD_DIM, 
                                weights=[embedding_weights],
                                        input_length=MAX_SEQUENCE_LENGTH,trainable=False))
       if embedding_char:
           character_input=Sequential()
           character_input.add(Embedding(alfabeth_size+1,
                                      EMBEDDING_CHAR_DIM,
                                      input_length=MAX_SEQUENCE_LENGTH*MAX_CHARACTER_LENGTH))
           print(character_input.output_shape)
           character_input.add(Reshape((character_input.output_shape[2],character_input.output_shape[1])))
           print(character_input.output_shape)
           character_input.add(Convolution1D(nb_filter=N_FILTERS, filter_length=3, activation='tanh'))
           print(character_input.output_shape)
           character_input.add(MaxPooling1D(pool_length=N_FILTERS))
           print(character_input.output_shape)
           character_input.add(Reshape((MAX_SEQUENCE_LENGTH,N_FILTERS)))

           self.model=Sequential()
           self.model.add(Merge([word_input,character_input], mode='concat'))
       else:
           self.model=word_input

   self.model.add(Dropout(0.5))
   self.model.add(Bidirectional(LSTM(MAX_SEQUENCE_LENGTH, return_sequences=True)))
   self.model.add(Dropout(0.5))
   self.model.add(TimeDistributed(Dense(tags)))
   crf=ChainCRF()
   self.model.add(crf)
   self.model.compile(loss=crf.loss, optimizer='rmsprop', metrics=['accuracy'])
</code></pre>

<p>Shapes: initial (14041,7564), after embedding (None,7564,30), after reshape1 (None,30,7564), after conv1d (None,28,20), after maxpool1d (None,1,20)</p>
",Vectorization & Embeddings,implement character convolution kera tring implement kera problem convolutional layer understand set hyperparameters like nb filter ecc real problem infact match max pool output embedding word layer output max character length max sequence length input data model shape initial embedding none reshape none conv none maxpool none
tensorflow basic word2vec example: Shouldn&#39;t we be using weights [nce_weight Transpose] for the representation and not embedding matrix?,"<p>I am referreing to <a href=""https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">this sample code</a><br/>
in the code snippet below:</p>

<pre><code>embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
embed = tf.nn.embedding_lookup(embeddings, train_inputs)

# Construct the variables for the NCE loss
nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],stddev=1.0 / math.sqrt(embedding_size)))
nce_biases = tf.Variable(tf.zeros([vocabulary_size]))

loss = tf.reduce_mean(
    tf.nn.nce_loss(weights=nce_weights,
    biases=nce_biases,
    labels=train_labels,
    inputs=embed,
    num_sampled=num_sampled,
    num_classes=vocabulary_size))

optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)
</code></pre>

<p>Now NCE_Loss function is nothing but a single hidden layer neural network with softmax at the optput layer [knowing is takes only a few negative sample]</p>

<p>This part of the graph will only update the weights of the network, it is not doing anything to the ""embeddings"" matrix/ tensor.</p>

<p>so ideally once the network is trained we must again pass it once through the embeddings_matrix first and then multiply by the transpose of the ""nce_weights"" [considering it as the same weight auto-encoder, at input &amp; output layers] to reach to the hidden layer representation of each word, which we are are calling word2vec (?)</p>

<p>But if look at the later part of the code, the value of the <code>embeddings</code> matrix is being used a word representation. <a href=""https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L250"" rel=""nofollow noreferrer"">This</a></p>

<p>Even the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss"" rel=""nofollow noreferrer"">tensorflow doc for NCE loss</a>, mentions input (to which we are passing <code>embed</code>, which uses <code>embeddings</code>) as just the 1st layer input activation values.</p>

<pre><code>inputs: A Tensor of shape [batch_size, dim]. The forward activations of the input network.
</code></pre>

<p>A normal back propagation stops at the first layer of the network, 
does this implementation of NCE loss, goes beyond and propagates the loss to the input values (and hence to the embedding) ?</p>

<p>This seems an extra step?
<a href=""http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"" rel=""nofollow noreferrer"">Refer this</a> for why I am calling it an extra step, he has a same explanation.</p>
",Vectorization & Embeddings,tensorflow basic word vec example using weight nce weight transpose representation embedding matrix referreing sample code code snippet nce loss function nothing single hidden layer neural network softmax optput layer knowing take negative sample part graph update weight network anything embeddings matrix tensor ideally network trained must pas embeddings matrix first multiply transpose nce weight considering weight auto encoder input output layer reach hidden layer representation word calling word vec look later part code value matrix used word representation even tensorflow doc nce loss mention input passing us st layer input activation value normal back propagation stop first layer network doe implementation nce loss go beyond propagates loss input value hence embedding seems extra step refer calling extra step ha explanation
How to print gensim dictionary and corpus,"<p>I am unable to understand how to print the output for the below code</p>

<pre><code># make gensim dictionary and corpus
dictionary = gensim.corpora.Dictionary(boc_texts)
corpus = [dictionary.doc2bow(boc_text) for boc_text in boc_texts]
tfidf = gensim.models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
</code></pre>

<p>I want to print the keyphrases and their tfidf scores</p>

<p>Thank you</p>
",Vectorization & Embeddings,print gensim dictionary corpus unable understand print output code want print keyphrases tfidf score thank
Cosine similarity for already known pairs of duplicates,"<p>I have a list of duplicate document pairs saved in a csv file. Each ID from column 1 is a duplicate to the corresponding ID in column 2.
The file goes something like this: </p>

<pre><code>Document_ID1    Document_ID2
12345           87565
34546           45633
56453           78645
35667           67856
13636           67845
</code></pre>

<p>Each Document ID is associated with text that is saved somewhere else. I pulled this text and saved each column of IDs and associated texts into two lsm databases.<br>
So I have <code>db1</code> which has all the IDs from <code>Document_ID1</code> as <em>keys</em> and their corresponding texts as the <em>values</em> for the respective keys. Therefore, like a dictionary. Similarly, <code>db2</code> for all the IDs from <code>Document_ID2</code>.<br>
So, when I say <code>db1[12345]</code>, I get the text associated with the ID 12345.<br></p>

<p>Now, I want to get the cosine similarity scores between each of these pairs to determine their duplicate-ness. Until now I ran a tfidf model to do the same. I created a tfidf matrix with all the documents in db1 as the corpus, and I measured the cosine similarity of each of the tfidf vectors from db2 against the tfidf matrix. For security reasons, I cannot provide the complete code. Code goes like this: </p>

<pre><code># Generator function to pick one key (document) at a time for comparison against other documents
def generator(db):
    for key in db.keys():
        text = db[key]
        yield text

# Use spaCy to create a function to preprocess text from the generator function
nlp = spacy.load('en')
def spacy(generator_object):
    for doc in generator_object:
        words = &lt;code to make words lower case, remove stop words, spaces and punctuations&gt;
        yield u' '.join(words)

# TF-IDF Vectorizer
tfidf = TfidfVectorizer(min_df = 2)

# Applying tf-idf transformer to each key from db1 individually in the generator function.
tfidf_matrix = tfidf.fit_transform(spacy(generator(db1)))

# Function to calculate cosine similarity values between the tfidf matrix and the tfidf vector of a new key
def similarity(tfidf_vector, tfidf_matrix, keys):    
    sim_vec = &lt;code to get cosine similarity&gt;
    return sim_vec.sort_values(ascending=False)

# Applying tf-idf transformer on db2 keys on a loop and getting cosine similarity scores for each key from db2.
for key in db2.keys():
    # Create a new temporary db for each key from db2 to enter into generator function
    new = &lt;code to create a temporary new lsm database&gt;
    text = db2[key]
    new[key] = text
    new_key = &lt;code to get next key from the temporary new lsm database&gt;
    tfidf_vector = tfidf.transform(spacy_proc(corpus_gen(new)))
    similarity_values = similarity(tfidf_vector, tfidf_matrix, list(db1.keys()))
    for idx, i in similarity_values.iteritems(): 
            print new_key, idx, i
    del new[key]
</code></pre>

<p>But this gives me cosine similarity scores against all keys in db1 for  each key in db2. Example: If there are 5 keys in db1 and 5 keys in db2, I get 25 rows as result with this code. <br>
What I want is to get the cosine similarity scores for just corresponding key from db1 for the key in db2. Which means if there are 5 keys each in db1 and db2, I should have only 5 rows as a result - the cosine similarity score for each pair of duplicates only.<br><br>How should I tweak my code to get that?</p>
",Vectorization & Embeddings,cosine similarity already known pair duplicate list duplicate document pair saved csv file id column duplicate corresponding id column file go something like document id associated text saved somewhere else pulled text saved column id associated text two lsm database ha id key corresponding text value respective key therefore like dictionary similarly id say get text associated id want get cosine similarity score pair determine duplicate ness ran tfidf model created tfidf matrix document db corpus measured cosine similarity tfidf vector db tfidf matrix security reason provide complete code code go like give cosine similarity score key db key db example key db key db get row result code want get cosine similarity score corresponding key db key db mean key db db row result cosine similarity score pair duplicate tweak code get
Average of word embeddings with TF-IDF score,"<p>I have been developing a python script to classify if an article is related to a body text or not. For that I have been using ML (SVM classifier) with some features, including average of word embeddings. </p>

<p>The code for calculating the average of word embeddings between the list of articles and bodies is the following:</p>

<pre><code>word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
setm = set(word2vec_model.index2word)

def avg_feature_vector(words, model, num_features, index2word_set):
        #function to average all words vectors in a given paragraph 
        featureVec = np.zeros((num_features,), dtype=""float32"")
        nwords = 0
        for word in words:
            if word in index2word_set and word not in stop:
                try:
                    featureVec = np.add(featureVec, model[word])
                    nwords = nwords+1
                except:
                    pass
        if(nwords&gt;0):
            featureVec = np.divide(featureVec, nwords)
        return featureVec

def doc_similatiry(headlines, bodies):
    X = []
    docs = []
    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):
        headline_avg_vector = avg_feature_vector(lemmatize_str(clean(headline)).split(), word2vec_model, 300, setm)
        body_avg_vector = avg_feature_vector(lemmatize_str(clean(body)).split(), word2vec_model, 300, setm)
        similarity =  1 - distance.cosine(headline_avg_vector, body_avg_vector)
        X.append(similarity)
    return X, docs
</code></pre>

<p>It seems like the average word2vec is being calculated correctly. However, it has worse scores than the TF-IDF cosine alone. Therefore, my idea was to group these 2 features, by that means, multiplying the TF-IDF score of each word to the word2vec.</p>

<p>Here is my code to do that:</p>

<pre><code>def avg_feature_vector(words, model, num_features, index2word_set, tfidf_vec, vec_repr, pos):
        #function to average all words vectors in a given paragraph (with tfidf feature)
        featureVec = np.zeros((num_features,), dtype=""float32"")
        nwords = 0

        for word in words:
            if word in index2word_set and word not in stop:
                try:
                    a = tfidf_vec.vocabulary_[word]
                    featureVec = np.add(featureVec, model[word]) * vec_repr[pos, a]
                    nwords = nwords+1
                except:
                    pass    
        if(nwords&gt;0):
            featureVec = np.divide(featureVec, nwords)
        return featureVec

def doc_similatiry_with_tfidf(headlines, bodies):

    X = []
    docs = []
    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):
        docs.append(lemmatize_str(clean(headline)))
        docs.append(lemmatize_str(clean(body)))
    vectorizer = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=True, stop_words=stop, sublinear_tf=True)
    sklearn_representation = vectorizer.fit_transform(docs)

    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):
        a = (clean(headline))
        headline_avg_vector = avg_feature_vector(nltk.word_tokenize(a), word2vec_model, 300, setm, vectorizer, sklearn_representation, 2*i)
        a = (clean(body))
        body_avg_vector = avg_feature_vector(nltk.word_tokenize(a), word2vec_model, 300, setm, vectorizer, sklearn_representation, 2*i+1)

        similarity =  1 - distance.cosine(headline_avg_vector, body_avg_vector)
        X.append(similarity)

    return X, docs
</code></pre>

<p>My problem is that this method is getting awful results and I don't know know if there's some logic that explains that (because in theory it should have better results) or if I'm doing something wrong in my code.</p>

<p>Can anyone help me figure this out? Also, I am open to new solutions to solve this problem.</p>

<p>Note: There are some functions used there which I didn't post the code since I thought they were not necessary. If there is something you don't understand I am here to explain it better.</p>
",Vectorization & Embeddings,average word embeddings tf idf score developing python script classify article related body text using ml svm classifier feature including average word embeddings code calculating average word embeddings list article body following seems like average word vec calculated correctly however ha worse score tf idf cosine alone therefore idea wa group feature mean multiplying tf idf score word word vec code problem method getting awful result know know logic explains theory better result something wrong code anyone help figure also open new solution solve problem note function used post code since thought necessary something understand explain better
Word Embedding: Dimensions of word-id list,"<p>I would like to perform word embedding with pretrained glove embeddings which I have downloaded <a href=""https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models"" rel=""nofollow noreferrer"">here.</a></p>

<p>I am using a 6 word sentence as a test, and set a max doc length of 30.
I am using the <strong>learn.preprocessing.VocabularyProcessor()</strong> object to learn the token-id dictionary. I am using the <strong>transform()</strong> method of this object to transform the input sentence to a list of word ids, so that I can look them up in the embedding matrix.</p>

<p>Why does the <strong>VocabularyProcessor.transform()</strong> method return a 6 x 30 array?
I would expect it to simply return a list of the ids, for each of the words in the test sentence.</p>

<pre><code>#show vocab and embedding
print('vocab size:%d\n' % vocab_size)
print('embedding dim:%d\n' %embedding_dim)
#test input
test_input_sentence=""the cat sat on the mat""
test_words_list=test_input_sentence.split()
print (test_words_list)

#create embedding matrix W, and define a placeholder to be fed
W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),
            trainable=False, name=""W"")
embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])
embedding_init = W.assign(embedding_placeholder)
print('initalised embedding')
print(embedding_init.get_shape())

with tf.Session() as sess:
    sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})
    #init a vocab processor object
    vocab_processor =   learn.preprocessing.VocabularyProcessor(max_document_length)
#fit = Learn a vocabulary dictionary of all tokens in the raw documents.
pretrain = vocab_processor.fit(vocab)
print('vocab preprocessor done')
#transform input to word-id matrix.
x = np.array(list(vocab_processor.transform(test_words_list)))

print('word id list shape:') 
print (x.shape)
print('embedding tensor shape:')
print(W.get_shape())
vec=tf.nn.embedding_lookup(W,x)
print ('vectors shape:')
print (vec.get_shape())
print ('embeddings:')
print (sess.run(vec))
</code></pre>
",Vectorization & Embeddings,word embedding dimension word id list would like perform word embedding pretrained glove embeddings downloaded using word sentence test set max doc length using learn preprocessing vocabularyprocessor object learn token id dictionary using transform method object transform input sentence list word id look embedding matrix doe vocabularyprocessor transform method return x array would expect simply return list id word test sentence
How can I use my .conll file from nlp parser for feature selection,"<p>I have an outputted .conll format file from Malt Parser, which is using the engmalt.linear-1.7.mco training model. My original input was a large text file of sentences. How can I use this file for feature selection?</p>

<p>I am using python with Scikit-learn (currently using tfidf bag of words to select features). However, I want to utilize nlp, by for example, only searching for adjectives. How can I do this with a conll file?</p>
",Vectorization & Embeddings,use conll file nlp parser feature selection outputted conll format file malt parser using engmalt linear mco training model original input wa large text file sentence use file feature selection using python scikit learn currently using tfidf bag word select feature however want utilize nlp example searching adjective conll file
Keras embedding layer masking. Why does input_dim need to be |vocabulary| + 2?,"<p>In the Keras docs for <code>Embedding</code> <a href=""https://keras.io/layers/embeddings/"" rel=""noreferrer"">https://keras.io/layers/embeddings/</a>, the explanation given for <code>mask_zero</code> is </p>

<blockquote>
  <p>mask_zero: Whether or not the input value 0 is a special ""padding"" value that should be masked out. This is useful when using recurrent layers which may take variable length input. If this is True then all subsequent layers in the model need to support masking or an exception will be raised. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal |vocabulary| + 2).</p>
</blockquote>

<p>Why does input_dim need to be 2 + number of words in vocabulary? Assuming 0 is masked and can't be used, shouldn't it just be 1 + number of words? What is the other extra entry for?</p>
",Vectorization & Embeddings,kera embedding layer masking doe input dim need vocabulary kera doc explanation given mask zero whether input value special padding value masked useful using recurrent layer may take variable length input true subsequent layer model need support masking exception raised mask zero set true consequence index used vocabulary input dim equal vocabulary doe input dim need number word vocabulary assuming masked used number word extra entry
What are some good resources for multi-class text classification using word2vec followed by SVM/ANN / Deep Networks?,"<p>I need to implement a multi-class text classifier. I thought of using word2vec, can someone lead me to good papers/resources which talk about this.
i would have 4-5 classes and I have loads of data. I have to manually label some of them. It would also be great if someone could throw light on the training size. I plan to use pre-trained word2vec for word embedding.</p>
",Vectorization & Embeddings,good resource multi class text classification using word vec followed svm ann deep network need implement multi class text classifier thought using word vec someone lead good paper resource talk would class load data manually label would also great someone could throw light training size plan use pre trained word vec word embedding
What is relation between tsne and word2vec?,"<p>As I know of, <code>tsne</code> is reducing dimension of word vector. </p>

<p><code>Word2vec</code> is generate word embedding model with huge amount of data.</p>

<p>What is the relation between two?</p>

<p>Does <code>Word2vec</code> use <code>tsne</code> inside? </p>

<p>(I use <code>Word2vec</code> from <code>Gensim</code>)</p>
",Vectorization & Embeddings,relation tsne word vec know reducing dimension word vector generate word embedding model huge amount data relation two doe use inside use
can anyone tell me about the model (skipgram/ CBOW ) used by Gensim?,"<p>word2vec uses either of the model for distributed representation of words. I was checking out the codes of gensim but it is not defined about the model used by gensim .</p>
",Vectorization & Embeddings,anyone tell model skipgram cbow used gensim word vec us either model distributed representation word wa checking code gensim defined model used gensim
LSTM labeling all samples as the same class,"<p>I'm trying to design an LSTM network using Keras to combine word embeddings and other features in a binary classification setting. My test set contains 250 samples per class. </p>

<p>When I run my model using only the word embedding layers (the ""model"" layer in the code), I get an average F1 of around 0.67. When I create a new branch with the other features of fixed size that I compute separately (""branch2"") and merge these with the word embeddings using ""concat"", the predictions all revert to a single class (giving perfect recall for that class), and average F1 drops to 0.33.</p>

<p>Am I adding in the features and training/testing incorrectly?</p>

<pre><code>def create_model(embedding_index, sequence_features, optimizer='rmsprop'):
    # Branch 1: word embeddings
    model = Sequential()
    embedding_layer = create_embedding_matrix(embedding_index, word_index)
    model.add(embedding_layer)
    model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='tanh'))
    model.add(MaxPooling1D(pool_length=2))
    model.add(Bidirectional(LSTM(100)))
    model.add(Dropout(0.2))
    model.add(Dense(2, activation='sigmoid'))

    # Branch 2: other features
    branch2 = Sequential()
    dim = sequence_features.shape[1]
    branch2.add(Dense(15, input_dim=dim, init='normal', activation='tanh'))
    branch2.add(BatchNormalization())

    # Merging branches to create final model
    final_model = Sequential()
    final_model.add(Merge([model,branch2], mode='concat'))
    final_model.add(Dense(2, init='normal', activation='sigmoid'))
    final_model.compile(loss='categorical_crossentropy', optimizer=optimizer,
           metrics=['accuracy','precision','recall','fbeta_score','fmeasure'])
    return final_model

def run(input_train, input_dev, input_test, text_col, label_col, resfile, embedding_index):
    # Processing text and features
    data_train, labels_train, data_test, labels_test = vectorize_text(input_train, input_test, text_col,label_col)
    x_train, y_train = data_train, labels_train
    x_test, y_test = data_test, labels_test
    seq_train = get_sequence_features(input_train).as_matrix()
    seq_test = get_sequence_features(input_test).as_matrix()

    # Generating model
    filepath = lstm_config.WEIGHTS_PATH
    checkpoint = ModelCheckpoint(filepath, monitor='val_fmeasure', verbose=1, save_best_only=True, mode='max')
    callbacks_list = [checkpoint]
    model = create_model(embedding_index, seq_train)
    model.fit([x_train, seq_train], y_train, validation_split=0.33, nb_epoch=3, batch_size=100, callbacks=callbacks_list, verbose=1)

    # Evaluating
    scores = model.evaluate([x_test, seq_test], y_test, verbose=1)
    time.sleep(0.2)
    preds = model.predict_classes([x_test, seq_test])
    preds = to_categorical(preds)
    print(metrics.f1_score(y_true=y_test, y_pred=preds, average=""micro""))
    print(metrics.f1_score(y_true=y_test, y_pred=preds, average=""macro""))
    print(metrics.classification_report(y_test, preds))
</code></pre>

<p>Output:</p>

<blockquote>
  <p>Using Theano backend. Found 2999999 word vectors. 
  Processing text dataset Found 7165 unique tokens. 
  Shape of data tensor: (1996, 50)
  Shape of label tensor: (1996, 2) 
  1996 train 500 test 
  Train on 1337 samples, validate on 659 samples </p>
  
  <p>Epoch 1/3 1300/1337
  [============================>.] - ETA: 0s - loss: 0.6767 - acc:
  0.6669 - precision: 0.5557 - recall: 0.6815 - fbeta_score: 0.6120 - fmeasure: 0.6120Epoch 00000: val_fmeasure im1337/1337
  [==============================] - 10s - loss: 0.6772 - acc: 0.6672 -
  precision: 0.5551 - recall: 0.6806 - fbeta_score: 0.6113 - fmeasure:
  0.6113 - val_loss: 0.7442 - val_acc: 0 .0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00 - val_fmeasure: 0.0000e+00 </p>
  
  <p>Epoch 2/3 1300/1337
  [============================>.] - ETA: 0s - loss: 0.6634 - acc:
  0.7269 - precision: 0.5819 - recall: 0.7292 - fbeta_score: 0.6462 - fmeasure: 0.6462Epoch 00001: val_fmeasure di1337/1337
  [==============================] - 9s - loss: 0.6634 - acc: 0.7263 -
  precision: 0.5830 - recall: 0.7300 - fbeta_score: 0.6472 - fmeasure:
  0.6472 - val_loss: 0.7616 - val_acc: 0. 0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00 - val_fmeasure: 0.0000e+00 </p>
  
  <p>Epoch 3/3 1300/1337
  [============================>.] - ETA: 0s - loss: 0.6542 - acc:
  0.7354 - precision: 0.5879 - recall: 0.7308 - fbeta_score: 0.6508 - fmeasure: 0.6508Epoch 00002: val_fmeasure di1337/1337
  [==============================] - 8s - loss: 0.6545 - acc: 0.7337 -
  precision: 0.5866 - recall: 0.7307 - fbeta_score: 0.6500 - fmeasure:
  0.6500 - val_loss: 0.7801 - val_acc: 0. 0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00 - val_fmeasure: 0.0000e+00 500/500 [==============================] - 0s
  500/500 [==============================] - 1s<br>
  0.5 /usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1074:</p>
  
  <p>UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in
  labels with no predicted samples.   'precision', 'predicted', average,
  warn_for)
  0.333333333333 /usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1074:</p>
  
  <p>UndefinedMetricWarning: Precision and F-score are ill-defined and
  being set to 0.0 in labels with no predicted samples.</p>

<pre><code>         precision    recall  f1-score   support

      0       0.00      0.00      0.00       250
      1       0.50      1.00      0.67       250
avg / total       0.25      0.50      0.33       500
</code></pre>
</blockquote>
",Vectorization & Embeddings,lstm labeling sample class trying design lstm network using kera combine word embeddings feature binary classification setting test set contains sample per class run model using word embedding layer model layer code get average f around create new branch feature fixed size compute separately branch merge word embeddings using concat prediction revert single class giving perfect recall class average f drop adding feature training testing incorrectly output using theano backend found word vector processing text dataset found unique token shape data tensor shape label tensor train test train sample validate sample epoch eta loss acc precision recall fbeta score fmeasure epoch val fmeasure im loss acc precision recall fbeta score fmeasure val loss val acc e val precision e val recall e val fbeta score e val fmeasure e epoch eta loss acc precision recall fbeta score fmeasure epoch val fmeasure di loss acc precision recall fbeta score fmeasure val loss val acc e val precision e val recall e val fbeta score e val fmeasure e epoch eta loss acc precision recall fbeta score fmeasure epoch val fmeasure di loss acc precision recall fbeta score fmeasure val loss val acc e val precision e val recall e val fbeta score e val fmeasure e usr local lib python dist package sklearn metric classification py undefinedmetricwarning f score ill defined set label predicted sample precision predicted average warn usr local lib python dist package sklearn metric classification py undefinedmetricwarning precision f score ill defined set label predicted sample
Feature vector as input to an RNN,"<p>For fun and in order to get a deeper understanding of recurrent neural networks, I am writing my own RNN-based syntactic dependency parser from scratch. In this case, each sentence is a sequence of words, from which a sequence of parser actions is to be predicted.</p>

<p>Now if each word was simply represented by a number, this would not be a problem, as there would have been a one-to-one mapping between the dimensionality of the input to the network, and the network's output. However, in my case each word is represented by an N-dimensional word embedding, whereas each predicted parser action is an integer value within a limited space.</p>

<p>This means my network needs to accept a sequence of feature vectors as input, and then output a sequence of integers.</p>

<p>Currently (parts of) my initialization and forward code looks like this:</p>

<pre><code>def __init__(self, trans_dim, out_dim, feature_vec_size=1, hidden_dim=100, trunc=4):
    self.trans_dim = trans_dim
    self.hidden_dim = hidden_dim
    self.out_dim = out_dim
    self.trunc = trunc

    # U = input -&gt; hidden weights
    self.U = np.random.uniform(-np.sqrt(1./trans_dim), np.sqrt(1./trans_dim), (hidden_dim, trans_dim))
    # V = hidden -&gt; output weights
    self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (out_dim, hidden_dim))
    # W = s1 -&gt; s2 weights
    self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))

def forward(self, steps):
    # Number of time steps
    t_steps = len(steps)
    print t_steps
    # The initial hidden
    states = np.zeros((t_steps + 1, self.hidden_dim))
    # Output at each time step
    output = np.zeros((t_steps, self.out_dim))

    # Looping through time steps
    for t in np.arange(t_steps):
        print steps[t]
        # This is where the problem is. Indexing U by steps[t] will not work when steps
        # contains floats.
        states[t] = np.tanh(self.U[:, steps[t]] + self.W.dot(states[t-1]))
        output[t] = self.softmax(self.V.dot(states[t]))
</code></pre>

<p>Which leads to the question -- how can I make my network accept a sequence of feature vectors as inputs, and then output a sequence of integers representing parser actions in each time step?</p>

<p>I am sorry if this is a dumb question; I am merely a novice when it comes to neural networks, but I wish to learn. Any help would be greatly appreciated.</p>
",Vectorization & Embeddings,feature vector input rnn fun order get deeper understanding recurrent neural network writing rnn based syntactic dependency parser scratch case sentence sequence word sequence parser action predicted word wa simply represented number would problem would one one mapping dimensionality input network network output however case word represented n dimensional word embedding whereas predicted parser action integer value within limited space mean network need accept sequence feature vector input output sequence integer currently part initialization forward code look like lead question make network accept sequence feature vector input output sequence integer representing parser action time step sorry dumb question merely novice come neural network wish learn help would greatly appreciated
word2vec_basic output: trying to test word similarity versus human similarity scores,"<p>As a way to get familiar with Tensorflow, I am trying to verify that the word embeddings generated by <code>word2vec_basic.py</code> (see <a href=""https://www.tensorflow.org/tutorials/word2vec"" rel=""nofollow noreferrer"">tutorial</a>) make sense when checked against human similarity scores. However, the results are surprisingly disappointing. Here's what I do. </p>

<p>In <code>word2vec_basic.py</code>, I add another step at the very end to save the embeddings and the reverse dictionary to disk (so I don't have to regenerate them every time):</p>

<pre><code>with open(""embeddings"", 'wb') as f:
    np.save(f, final_embeddings)
with open(""reverse_dictionary"", 'wb') as f:
    pickle.dump(reverse_dictionary, f, pickle.HIGHEST_PROTOCOL)
</code></pre>

<p>In my own word2vec_test.py, I load them and create a direct dictionary for lookups:</p>

<pre><code>with open(""embeddings"", 'rb') as f:
    embeddings = np.load(f)
with open(""reverse_dictionary"", 'rb') as f:
    reverse_dictionary = pickle.load(f)
dictionary = dict(zip(reverse_dictionary.values(), reverse_dictionary.keys()))
</code></pre>

<p>I then define similarity as euclidean distance between the embedding vectors:</p>

<pre><code>def distance(w1, w2):
    try:
        return np.linalg.norm(embeddings[dictionary[w1]] - embeddings[dictionary[w2]])
    except:
        return None # no such word in our dictionary
</code></pre>

<p>So far the results make sense, for example <code>distance('before', 'after')</code> is less then <code>distance('before', 'into')</code>.</p>

<p>Then, I downloaded human scores from <a href=""http://alfonseca.org/pubs/ws353simrel.tar.gz"" rel=""nofollow noreferrer"">http://alfonseca.org/pubs/ws353simrel.tar.gz</a> (I borrowed the link and code below from Swivel project from ""Model Zoo""). I compare the human scores for similarity and embedding distances as follows:</p>

<pre><code>with open(""wordsim353_sim_rel/wordsim_relatedness_goldstandard.txt"", 'r') as lines:
  for line in lines:
    w1, w2, act = line.strip().split('\t')
    pred = distance(w1, w2)
    if pred is None:
      continue

    acts.append(float(act))
    preds.append(-pred)
</code></pre>

<p>I use <code>-pred</code> because the human scores increase with increased similarity, so the distance ordering needs to be inverted to match (smaller distances mean larger similarity).</p>

<p>Then I calculate correlation coefficient:</p>

<pre><code>rho, _ = scipy.stats.spearmanr(acts, preds)
print(str(rho))
</code></pre>

<p>but it turns out very small, like 0.006. I retrained word2vec_basic with 4 words of context and vector length of 256, but it didn't improve at all. I then used cosine similarity instead of euclidean distance:</p>

<pre><code>def distance(w1, w2):
    return scipy.spatial.distance.cosine(embeddings[dictionary[w1]], embeddings[dictionary[w2]])
</code></pre>

<p>Still no correlation. </p>

<p>So, what is it that I'm misunderstanding or doing wrong?</p>
",Vectorization & Embeddings,word vec basic output trying test word similarity versus human similarity score way get familiar tensorflow trying verify word embeddings generated see tutorial make sense checked human similarity score however result surprisingly disappointing add another step end save embeddings reverse dictionary disk regenerate every time word vec test py load create direct dictionary lookup define similarity euclidean distance embedding vector far result make sense example le downloaded human score borrowed link code swivel project model zoo compare human score similarity embedding distance follows use human score increase increased similarity distance ordering need inverted match smaller distance mean larger similarity calculate correlation coefficient turn small like retrained word vec basic word context vector length improve used cosine similarity instead euclidean distance still correlation misunderstanding wrong
Gensim: What is difference between word2vec and doc2vec?,"<p>I'm kinda newbie and not native english so have some trouble understanding <code>Gensim</code>'s <code>word2vec</code> and <code>doc2vec</code>.</p>

<p>I think both give me some words most similar with query word I request, by <code>most_similar()</code>(after training).</p>

<p>How can tell which case I have to use <code>word2vec</code> or <code>doc2vec</code>?</p>

<p>Someone could explain difference in short word, please?</p>

<p>Thanks.</p>
",Vectorization & Embeddings,gensim difference word vec doc vec kinda newbie native english trouble understanding think give word similar query word request training tell case use someone could explain difference short word please thanks
Tensorflow examples for RNN,"<p>I am trying to implement basic NLP tasks in Tensorflow without using the build in modules as much as possible (just for learning sake)</p>

<p>I have been trying to implement a Part of Speech tagger using data from <a href=""http://www.cnts.ua.ac.be/conll2000/chunking/"" rel=""nofollow noreferrer"">http://www.cnts.ua.ac.be/conll2000/chunking/</a></p>

<p>I am having a little difficulty with implementing a RNN code from scratch using an embedding layer in front and was wondering if there are examples and implementations on the same.</p>

<p>I have seen tons of examples using Theano and on MNIST data but havent been able to find a concrete implementation of RNNs from scratch in Tensorflow on textual data.</p>

<p>Any suggestions?</p>
",Vectorization & Embeddings,tensorflow example rnn trying implement basic nlp task tensorflow without using build module much possible learning sake trying implement part speech tagger using data little difficulty implementing rnn code scratch using embedding layer front wa wondering example implementation seen ton example using theano mnist data havent able find concrete implementation rnns scratch tensorflow textual data suggestion
Product merge layers with Keras functionnal API for Word2Vec model,"<p>I am trying to implement a Word2Vec CBOW with negative sampling with Keras, following the code found <a href=""https://github.com/abaheti95/Deep-Learning/blob/master/word2vec/keras/cbow_model.py"" rel=""nofollow noreferrer"">here</a>:</p>

<pre><code>EMBEDDING_DIM = 100

sentences = SentencesIterator('test_file.txt')
v_gen = VocabGenerator(sentences=sentences, min_count=5, window_size=3,
                       sample_threshold=-1, negative=5)

v_gen.scan_vocab()
v_gen.filter_vocabulary()
reverse_vocab = v_gen.generate_inverse_vocabulary_lookup('test_lookup')

# Generate embedding matrix with all values between -1/2d, 1/2d
embedding = np.random.uniform(-1.0 / (2 * EMBEDDING_DIM),
                              1.0 / (2 * EMBEDDING_DIM),
                              (v_gen.vocab_size + 3, EMBEDDING_DIM))

# Creating CBOW model
# Model has 3 inputs
# Current word index, context words indexes and negative sampled word indexes
word_index = Input(shape=(1,))
context = Input(shape=(2*v_gen.window_size,))
negative_samples = Input(shape=(v_gen.negative,))

# All inputs are processed through a common embedding layer
shared_embedding_layer = (Embedding(input_dim=(v_gen.vocab_size + 3),
                                    output_dim=EMBEDDING_DIM,
                                    weights=[embedding]))

word_embedding = shared_embedding_layer(word_index)
context_embeddings = shared_embedding_layer(context)
negative_words_embedding = shared_embedding_layer(negative_samples)

# Now the context words are averaged to get the CBOW vector
cbow = Lambda(lambda x: K.mean(x, axis=1),
              output_shape=(EMBEDDING_DIM,))(context_embeddings)

# Context is multiplied (dot product) with current word and negative
# sampled words
word_context_product = merge([word_embedding, cbow], mode='dot')
negative_context_product = merge([negative_words_embedding, cbow],
                                 mode='dot',
                                 concat_axis=-1)

# The dot products are outputted
model = Model(input=[word_index, context, negative_samples],
              output=[word_context_product, negative_context_product])

# Binary crossentropy is applied on the output
model.compile(optimizer='rmsprop', loss='binary_crossentropy')
print(model.summary())

model.fit_generator(v_gen.pretraining_batch_generator(reverse_vocab),
                    samples_per_epoch=10,
                    nb_epoch=1)
</code></pre>

<p>However, I get an  error during the merge part because Embedding layer is a 3D tensor while cbow is only 2 dimensions. I assume I need to reshape the embedding (which is [?, 1, 100]) to [1, 100] but I can't find how to reshape with the functional API.
I am using the Tensorflow backend.</p>

<p>Also, if someone can point to an other implementation of CBOW with Keras (Gensim free), I would love to have a look to it!</p>

<p>Thank you!</p>

<p>EDIT: Here is the error</p>

<pre><code>Traceback (most recent call last):
  File ""cbow.py"", line 48, in &lt;module&gt;
    word_context_product = merge([word_embedding, cbow], mode='dot')
    .
    .
    .
ValueError: Shape must be rank 2 but is rank 3 for 'MatMul' (op: 'MatMul') with input shapes: [?,1,100], [?,100].
</code></pre>
",Vectorization & Embeddings,product merge layer kera functionnal api word vec model trying implement word vec cbow negative sampling kera following code found however get error merge part embedding layer tensor cbow dimension assume need reshape embedding find reshape functional api using tensorflow backend also someone point implementation cbow kera gensim free would love look thank edit error
Get selected feature names TFIDF Vectorizer,"<p>I'm using python and I want to get the TFIDF representation for a large corpus of data, I'm using the following code to convert the docs into their TFIDF form.</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(
    min_df=1,  # min count for relevant vocabulary
    max_features=4000,  # maximum number of features
    strip_accents='unicode',  # replace all accented unicode char 
    # by their corresponding  ASCII char
    analyzer='word',  # features made of words
    token_pattern=r'\w{1,}',  # tokenize only words of 4+ chars
    ngram_range=(1, 1),  # features made of a single tokens
    use_idf=True,  # enable inverse-document-frequency reweighting
    smooth_idf=True,  # prevents zero division for unseen words
    sublinear_tf=False)

tfidf_df = tfidf_vectorizer.fit_transform(df['text'])
</code></pre>

<p>Here I pass a parameter <code>max_features</code>. The vectorizer will select the best features and return a scipy sparse matrix. Problem is I dont know which features are getting selected and how do I map those feature names back to the scipy matrix I get? Basically for the <code>n</code> selected features from the <code>m</code> number of documents, I want a <code>m x n</code> matrix with the selected features as the column names instead of their integer ids. How do I accomplish this?</p>
",Vectorization & Embeddings,get selected feature name tfidf vectorizer using python want get tfidf representation large corpus data using following code convert doc tfidf form pas parameter vectorizer select best feature return scipy sparse matrix problem dont know feature getting selected map feature name back scipy matrix get basically selected feature number document want matrix selected feature column name instead integer id accomplish
Difference between pre-trained word embedding and training word embedding in keras,"<p>I am new to Deep Learning and I want to explore Deep Learning for NLP. I went through word embeddings and tested them in gensim word2vec. I also heard about pre-trained models. I am confused about the difference between pre-trained models and training the model yourself, and how to use the results.</p>

<p>I want to apply it in keras because I do not want to write formulas and all in Theano or Tensorflow.</p>
",Vectorization & Embeddings,difference pre trained word embedding training word embedding kera new deep learning want explore deep learning nlp went word embeddings tested gensim word vec also heard pre trained model confused difference pre trained model training model use result want apply kera want write formula theano tensorflow
NLP: Way to efficiently compare and identify trends between text,"<p>Are there algorithms or methods in which common trends/themes between text items could be evaluated?</p>

<p>For example let's say that there are four data points (text entries):</p>

<ul>
<li>""I found school very stressful today""</li>
<li>""The test in physics was pretty easy.""</li>
<li>""My physics test wasn't challenging at all""</li>
<li>""Everyone left early because the physics test was straight-forward and we finished it early.""</li>
</ul>

<p>Based on those four entries the first one is an outlier and has no relation to the rest but the other three mention how the ""physics test"" was easy (more generally, the other three express a positive sentiment around the ""physics test"").</p>

<p>Are there methods to extract the common thread between related sentences? These sentences are totally open ended and aren't restricted to simply expressing sentiment about an object - they could be talking about anything.</p>

<p>I understand this is a fairly broad question but I thought I'd ask it so see if people know of existing solutions or ways people have tackled this problem in the past.</p>
",Vectorization & Embeddings,nlp way efficiently compare identify trend text algorithm method common trend theme text item could evaluated example let say four data point text entry found school stressful today test physic wa pretty easy physic test challenging everyone left early physic test wa straight forward finished early based four entry first one outlier ha relation rest three mention physic test wa easy generally three express positive sentiment around physic test method extract common thread related sentence sentence totally open ended restricted simply expressing sentiment object could talking anything understand fairly broad question thought ask see people know existing solution way people tackled problem past
How to create a corpus with a set of text files - python?,"<p>I have a set of document <code>ID</code>s (keys.csv) that I am using to get a set of text documents from a document source. I would like to collect all these text documents into a corpus for further analysis (like cosine similarity).</p>

<p>I am using the below code to append each text document into the corpus, but I'm not sure if this is going to work. Is there a better way to create a corpus with these text documents? <br></p>

<pre><code>keys = pandas.read_csv(keys.csv)
for i in keys:
    ID = i
    doc = function_to_get_document(ID)
    corpus = corpus.append(doc)
</code></pre>
",Vectorization & Embeddings,create corpus set text file python set document key csv using get set text document document source would like collect text document corpus analysis like cosine similarity using code append text document corpus sure going work better way create corpus text document
How to get word vector representation when using Deep Learning in NLP,"<p>How to get word vector representation when using Deep Learning in NLP ? The words are represented by a fixed length vector, see <a href=""http://machinelearning.wustl.edu/mlpapers/paper_files/BengioDVJ03.pdf"" rel=""nofollow"">http://machinelearning.wustl.edu/mlpapers/paper_files/BengioDVJ03.pdf</a> for more details.</p>
",Vectorization & Embeddings,get word vector representation using deep learning nlp get word vector representation using deep learning nlp word represented fixed length vector see detail
Why we need fine tune word embedding in recurrent neural networks?,"<p>In the theano's tutorial about <a href=""http://deeplearning.net/tutorial/rnnslu.html"" rel=""nofollow"">RNN</a>, at the final part it mentioned that </p>

<blockquote>
  <p>We keep the word embeddings on the unit sphere by normalizing them after each update:</p>
</blockquote>

<pre><code>self.normalize = theano.function(inputs=[],
                                  updates={self.emb:
                                              self.emb /
                                              T.sqrt((self.emb**2)
                                              .sum(axis=1))
                                              .dimshuffle(0, 'x')})
                                              .sum(axis=1))
                                              .dimshuffle(0, 'x')})
</code></pre>

<p>I searched online and only find the paper <a href=""http://www.iro.umontreal.ca/~lisa/pointeurs/RNNSpokenLanguage2013.pdf"" rel=""nofollow"">Investigation of Recurrent-Neural-Network Architectures and Learning Methods for Spoken Language Understanding</a> mentioned it is fine-tuning word embedding in Chapter 3.5.1.</p>

<p>I don't understand why we can fine tune word embedding.     <code>self.emb</code> is the imput of this RNN model, right? How could we change the input value?</p>
",Vectorization & Embeddings,need fine tune word embedding recurrent neural network theano tutorial rnn final part mentioned keep word embeddings unit sphere normalizing update searched online find paper investigation recurrent neural network architecture learning method spoken language understanding mentioned fine tuning word embedding chapter understand fine tune word embedding imput rnn model right could change input value
Document similarity- Odd one out,"<p>Lets say I have ""n"" number of documents over a specific topic giving certain details. I want to get those documents who are not similar to the majority of the documents. As vague as this might seem, I know how to find cosine similarity between 2 documents. But lets say, I ""know"" I have 10 documents that are similar to each other, I introduce an 11th document and I need a way to judge how similar is this document with those 10 collectively and not just with every individual document. </p>

<p>I am working with scikit learn, so an answer or technique with its reference will help! </p>
",Vectorization & Embeddings,document similarity odd one let say n number document specific topic giving certain detail want get document similar majority document vague might seem know find cosine similarity document let say know document similar introduce th document need way judge similar document collectively every individual document working scikit learn answer technique reference help
WordNet Python words similarity,"<p>I'm trying to find a reliable way to measure the semantic similarity of 2 terms.
The first metric could be the path distance on a hyponym/hypernym graph (eventually a linear combination of 2-3 metrics could be better..).</p>

<pre><code>from nltk.corpus import wordnet as wn
dog = wn.synset('dog.n.01')
cat = wn.synset('cat.n.01')
print(dog.path_similarity(cat))
</code></pre>

<ul>
<li>I still don't get what <code>n.01</code> means and why it's necessary.</li>
<li>there is a way to visually show the computed path between 2 terms?</li>
<li>Which other nltk semantic metric could I use?</li>
</ul>
",Vectorization & Embeddings,wordnet python word similarity trying find reliable way measure semantic similarity term first metric could path distance hyponym hypernym graph eventually linear combination metric could better still get mean necessary way visually show computed path term nltk semantic metric could use
How does gensim calculate doc2vec paragraph vectors,"<p>i am going thorugh this paper <a href=""http://cs.stanford.edu/~quocle/paragraph_vector.pdf"">http://cs.stanford.edu/~quocle/paragraph_vector.pdf</a></p>

<p>and it states that</p>

<blockquote>
  <p>"" Theparagraph vector and word vectors are averaged or concatenated
  to predict the next word in a context. In the experiments, we use
  concatenation as the method to combine the vectors.""</p>
</blockquote>

<p>How does concatenation or averaging work?</p>

<p>example (if paragraph 1 contain word1 and word2):</p>

<pre><code>word1 vector =[0.1,0.2,0.3]
word2 vector =[0.4,0.5,0.6]

concat method 
does paragraph vector = [0.1+0.4,0.2+0.5,0.3+0.6] ?

Average method 
does paragraph vector = [(0.1+0.4)/2,(0.2+0.5)/2,(0.3+0.6)/2] ?
</code></pre>

<p>Also from this image:</p>

<p>It is stated that :</p>

<blockquote>
  <p>The paragraph token can be thought of as another word. It acts as a
  memory that remembers what is missing from the current context – or
  the topic of the paragraph. For this reason, we often call this model
  the Distributed Memory Model of Paragraph Vectors (PV-DM).</p>
</blockquote>

<p>Is the paragraph token equal to the paragraph vector which is equal to <code>on</code>?</p>

<p><a href=""https://i.sstatic.net/EQO9m.png""><img src=""https://i.sstatic.net/EQO9m.png"" alt=""enter image description here""></a></p>
",Vectorization & Embeddings,doe gensim calculate doc vec paragraph vector going thorugh paper doe concatenation averaging work example paragraph contain word word also image stated paragraph token thought another word act memory remembers missing current context topic paragraph reason often call model distributed memory model paragraph vector pv dm paragraph token equal paragraph vector equal
What is gensim&#39;s &#39;docvecs&#39;?,"<p><a href=""https://i.sstatic.net/ofJqR.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/ofJqR.png"" alt=""Doc2Vec Figure 2""></a></p>

<p>The above picture is from <a href=""https://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""noreferrer"">Distributed Representations of Sentences and Documents</a>, the paper introducing Doc2Vec. I am using Gensim's implementation of Word2Vec and Doc2Vec, which are great, but I am looking for clarity on a few issues.</p>

<ol>
<li>For a given doc2vec model <code>dvm</code>, what is <code>dvm.docvecs</code>? My impression is that it is the averaged or concatenated vector that includes all of the word embedding <em>and</em> the paragraph vector, <code>d</code>. Is this correct, or is it d?</li>
<li>Supposing <code>dvm.docvecs</code> is not <code>d</code>, can one access d by itself? How?</li>
<li>As a bonus, how is <code>d</code> calculated? The paper only says:</li>
</ol>

<blockquote>
  <p>In our Paragraph Vector framework (see Figure 2), every
  paragraph is mapped to a unique vector, represented by a
  column in matrix D and every word is also mapped to a
  unique vector, represented by a column in matrix W.</p>
</blockquote>

<p>Thanks for any leads!</p>
",Vectorization & Embeddings,gensim docvecs picture distributed representation sentence document paper introducing doc vec using gensim implementation word vec doc vec great looking clarity issue given doc vec model impression averaged concatenated vector includes word embedding paragraph vector correct supposing one access bonus calculated paper say paragraph vector framework see figure every paragraph mapped unique vector represented column matrix every word also mapped unique vector represented column matrix w thanks lead
Large word embedding matrix update in tensorflow,"<p>Recently I am using a CNN to do text classification which is described in <a href=""http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"" rel=""nofollow noreferrer"">http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/</a>. </p>

<p>My dataset is quite large and the vocabulary size is more than 1M words. My training algorithm becomes much slower when the vocabulary size gets bigger. There is a warning message saying that  ""....tensorflow/python/ops/gradients.py:87: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 145017088 elements. This may consume a large amount of memory."" </p>

<p>I am thinking this may be caused by a dense gradient update on the embedding matrix. Does anyone have any ideas on that? There is a post <a href=""https://stackoverflow.com/questions/35803425/update-only-part-of-the-word-embedding-matrix-in-tensorflow"">Update only part of the word embedding matrix in Tensorflow</a> discussing similar issue. The top answer states that tensorflow only updates specific rows. Why did it change in my case?</p>

<p>Thanks!</p>
",Vectorization & Embeddings,large word embedding matrix update tensorflow recently using cnn text classification described dataset quite large vocabulary size word training algorithm becomes much slower vocabulary size get bigger warning message saying tensorflow python ops gradient py userwarning converting sparse indexedslices dense tensor element may consume large amount memory thinking may caused dense gradient update embedding matrix doe anyone idea post href part word embedding matrix tensorflow discussing similar issue top answer state tensorflow update specific row change case thanks
How to use Word2Vec with two inputs in a loop?,"<p>I'm trying to create a similarity between two words using word2vec, I was successful, while doing it manually. but I have two big txt files. I want to create a loop. I tried a couple methods for looping but I was unsuccessful. so I decided to ask expert. </p>

<p>my code :</p>

<pre><code>import gensim

model = gensim.models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
with open('myfile1.txt', 'r') as f:
    data1 = f.readlines()

with open('myfile2.txt', 'r') as f:
    data2 = f.readlines()

data = zip(data1, data2)

with open('myoutput.txt', 'a') as f:
    for x in data: 
        output = model.similarity(x[1], x[0])  # reading each word form each files
        out = '{} : {} : {}\n'.format(x[0].strip(), x[1].strip(),output)  
        f.write(out)
</code></pre>

<p>my input1, (text1)</p>

<pre><code>street 
spain 
ice
man
</code></pre>

<p>my input2 (text2)</p>

<pre><code>florist
paris 
cold 
kid
</code></pre>

<p>I want this output (output.txt)</p>

<pre><code>street florist 0.19991447551502498
spain paris 0.5380033328157873
ice cold 0.40968857572410483
man kid  0.42953233870042506
</code></pre>
",Vectorization & Embeddings,use word vec two input loop trying create similarity two word using word vec wa successful manually two big txt file want create loop tried couple method looping wa unsuccessful decided ask expert code input text input text want output output txt
What are the algorithms which could be sued to match sentences?,"<p>Let's say we have a list of 50 sentences and we have an input sentence. How can i choose the closest sentence to the input sentence from the list?</p>

<p>I have tried many methods/algorithms such as averaging word2vec vector representations of each token of the sentence and then cosine similarity of result vectors.</p>

<p>For example I want the algorithm to give a high similarity score between ""what is the definition of book?"" and ""please define book"".</p>

<p>I am looking for a method (probably a combinations of methods) which
1. looks for semantics
2. looks for syntax
3. gives different weights for different tokens with different role (e.g. in the first example 'what' and 'is' should get lower weights) </p>

<p>I know this might be a bit general but any suggestion is appreciated.</p>

<p>Thanks,</p>

<p>Amir</p>
",Vectorization & Embeddings,algorithm could sued match sentence let say list sentence input sentence choose closest sentence input sentence list tried many method algorithm averaging word vec vector representation token sentence cosine similarity result vector example want algorithm give high similarity score definition book please define book looking method probably combination method look semantics look syntax give different weight different token different role e g first example get lower weight know might bit general suggestion appreciated thanks amir
In what format should the input into a sequence to sequence learning model using RNNs be in?,"<p>I'm attempting to implement the neural conversational model described in <a href=""http://cs224d.stanford.edu/papers/ancm.pdf"" rel=""nofollow noreferrer"">Vinyals et al 2015</a>. The question I have is in what format should the inputted sequence be in? I'm thinking of using word2vec or GloVe vector representations of the words in the sentence that I'm inputting into the encoder, but it seems computationally expensive to derive the vectors for all the words in my training data using word2vec or GloVe. </p>

<p>It is also to my understanding that the model can potentially output vectors that don't quite exist in the vector space of the embeddings created from the training vocabulary, and that I would have to use a KNN algorithm to find the nearest existing vector to the one outputted by the model, and this also seems very computationally expensive to do repeatedly while training the model. </p>
",Vectorization & Embeddings,format input sequence sequence learning model using rnns attempting implement neural conversational model described vinyals et al question format inputted sequence thinking using word vec glove vector representation word sentence inputting encoder seems computationally expensive derive vector word training data using word vec glove also understanding model potentially output vector quite exist vector space embeddings created training vocabulary would use knn algorithm find nearest existing vector one outputted model also seems computationally expensive repeatedly training model
what are methods for comparing documents,"<p>recently I started to a do some research in standardising products data.</p>

<p>Supermarkets often sell the same products at different prices, and it is useful to compare these prices.  To do this, we need to know we are matching the same products from each supermarket.  The problem is, supermarkets will often have small differences in how they name their products and list them on their websites.  We need a tool that can standardise product names, recognising two differently-named products as the same product, while successfully recognising different but similarly-named products as well as differences in quantity. For example, I want to buy rasher, and when you go to search the rasher , we are going to code all rashers even though differently-named and map to HS-codes , I want to know what technologies behind this process?</p>

<p>Additionally, we need these products prices converted to standard units and the products to be aligned with groups defined by the World trade HS-codes. lets say  price of rasher is 2.99 euro per 180g, but now I want to change it to 16.62 euro per kg with some technologies,  An examination of the appropriate Natural language techniques to determine which method best fulfils these goals.</p>
",Vectorization & Embeddings,method comparing document recently started research standardising product data supermarket often sell product different price useful compare price need know matching product supermarket problem supermarket often small difference name product list website need tool standardise product name recognising two differently named product product successfully recognising different similarly named product well difference quantity example want buy rasher go search rasher going code rasher even though differently named map h code want know technology behind process additionally need product price converted standard unit product aligned group defined world trade h code let say price rasher euro per g want change euro per kg technology examination appropriate natural language technique determine method best fulfils goal
Is there any sentence embedding Tensorflow language model?,"<p>I found tensorflow 1b_lm project: <a href=""https://github.com/tensorflow/models/tree/master/lm_1b"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/tree/master/lm_1b</a></p>

<p>I'm just quite confused about the forth example</p>

<blockquote>
  <p>Give a sentence, dump the embedding from the LSTM state.</p>
</blockquote>

<p>However, the results of this example includes 7 '.npy' files. It seems like it just generates every word embedding for every word in the sentence?</p>
",Vectorization & Embeddings,sentence embedding tensorflow language model found tensorflow b lm project quite confused forth example give sentence dump embedding lstm state however result example includes npy file seems like generates every word embedding every word sentence
Machine Learning - Information extraction from a document,"<p>I'm trying to train a couple of neural networks (using tensorflow) to be able to extract semantic information from invoices. After a long list of reading I came up with this:</p>

<ul>
<li>Use <a href=""https://www.tensorflow.org/versions/master/tutorials/word2vec/index.html"" rel=""nofollow noreferrer"">word2vec</a> to generate word embeddings (more on the corpus below).</li>
<li>Feed the output of <code>word2vec</code> to a CNN since vectors that are close together share similar semantic meanings.</li>
</ul>

<p>So the very high level approach I described above seems quite alright to me. I would love for it to be corrected if anything looks wrong.</p>

<p>A couple of concerns that I have:</p>

<ol>
<li>Corpus selection. Is it sufficient to use a generic corpus of, for instance, wikipedia? Or should I use a specialized corpus for invoices? If it's the latter, how can I generate this corpus? I do have a big dataset of invoices that I can utilize.</li>
<li>Information extraction. Let's say all of the above work fine and I'm able to understand semantic information from a new unseen invoice. How do I go about extracting certain pieces of information? For instance, let's say we introduce a new invoice that has <code>order number: 12345</code>, assuming <code>order number</code> is understood to be the <strong>invoice number</strong> (or whatever vectors that lie in the same vicinity of <code>order number</code>), how do I extract the value <code>12345</code>? One area I was looking at is <a href=""https://github.com/tensorflow/models/tree/master/syntaxnet"" rel=""nofollow noreferrer"">SyntaxNet</a> that could help here.</li>
</ol>

<p>Any help/insight is appreciated.</p>

<p><strong>Follow up to @wasi-ahmad's question</strong>:
The reason I'm trying to understand semantic information about an invoice is to ultimately be able to extract values out of it. So, for instance, if I present an unseen invoice to my neural network it would find the invoice's number (whatever its label is called) and extract its value.</p>
",Vectorization & Embeddings,machine learning information extraction document trying train couple neural network using tensorflow able extract semantic information invoice long list reading came use word vec generate word embeddings corpus feed output cnn since vector close together share similar semantic meaning high level approach described seems quite alright would love corrected anything look wrong couple concern corpus selection sufficient use generic corpus instance wikipedia use specialized corpus invoice latter generate corpus big dataset invoice utilize information extraction let say work fine able understand semantic information new unseen invoice go extracting certain piece information instance let say introduce new invoice ha assuming understood invoice number whatever vector lie vicinity extract value one area wa looking syntaxnet could help help insight appreciated follow wasi ahmad question reason trying understand semantic information invoice ultimately able extract value instance present unseen invoice neural network would find invoice number whatever label called extract value
NLP Classification / Inference on Small Dataset -&gt; Word Embedding Approach,"<p>I would like to create a model that is given a series of keywords extracted from the description about a company and classifies the 'type' of the company. Let me illustrate with an example.</p>
<blockquote>
<p>&quot;Snapchat is an image messaging and multimedia mobile application created by Evan Spiegel, Bobby Murphy, and Reggie Brown,[3] former students at Stanford University, and developed by Snap Inc., originally Snapchat Inc. &quot;</p>
<blockquote>
<p>Sample Extracted Keywords: &quot;image messaging&quot; ; &quot;multimedia mobile application&quot;</p>
</blockquote>
<p>(from Wikipedia page on Snapchat)</p>
</blockquote>
<p>Given this info, my model will need to infer 'IT' and 'SNS' from &quot;image messaging&quot; and &quot;multimedia mobile application&quot;.</p>
<p>(In case you are asking why not go with the extracted keywords, I would like to categorize them into as few labels as possible for all companies, so 'IT' and 'SNS' are more general terms compared to 'image messaging' and such.)</p>
<p>Currently, my dataset is not too big. For about hundreds of data entries, about ~80 % contain info in the manner that I want. Given this info, I would like to process the keywords extracted from descriptions about the company and give them correct labels.</p>
<p>Any suggestions to aid me in this project would be great.</p>
",Vectorization & Embeddings,nlp classification inference small dataset word embedding approach would like create model given series keywords extracted description company classifies type company let illustrate example snapchat image messaging multimedia mobile application created evan spiegel bobby murphy reggie brown former student stanford university developed snap inc originally snapchat inc sample extracted keywords image messaging multimedia mobile application wikipedia page snapchat given info model need infer sn image messaging multimedia mobile application case asking go extracted keywords would like categorize label possible company sn general term compared image messaging currently dataset big hundred data entry contain info manner want given info would like process keywords extracted description company give correct label suggestion aid project would great
Feature extraction for Sentiment Analysis using scikit-learn,"<p>Which feature extractor (Countvectorizer, TfIdf)  will be best for sentiment analysis of tweets?
Can someone please explain the difference between each and which is most relevant for different classifiers.</p>

<p>I have planned to use 3 different classifiers- Naive Bayes,SVM and MaxEnt</p>
",Vectorization & Embeddings,feature extraction sentiment analysis using scikit learn feature extractor countvectorizer tfidf best sentiment analysis tweet someone please explain difference relevant different classifier planned use different classifier naive bayes svm maxent
Products Price Comparison Tool: Difficulty in matching identical items,"<p>I'm working on creating an e-comm products price comparison tool(in python) which is somewhat similar to <a href=""http://camelcamelcamel.com"" rel=""nofollow noreferrer"">camelcamelcamel.com</a>, both for fun and profit. I'm facing the difficult when I want to match the identical items from the list that I gathered from various websites using a search term. I'm using <em>Cosine similarity</em> and thinking of using <em>Levenshtein</em>'s Algorithm for product matching, to match the titles of the various items against each other to find the identical items.</p>

<p>For example, I have the following items and their price values as,</p>

<pre><code>{
    product_0: {
        title: ""Apple MacBook Air MMGF2HN/A 13.3-inch Laptop (Core i5/8GB/128GB/Mac OS X/Integrated Graphics)"",
        price: ""xxxx"",
    },
    product_1: {
        title: ""Apple MacBook Air MMGF2HN/A 13.3-inch Laptop (Core i5/8GB/128GB/Mac OS X/Integrated Graphics) cover"",
        price: ""xyzy""
    },
    product_2: {
        title: ""Apple Macbook Air MMGF2HNA Notebook (Intel Core i5- 8GB RAM- 128GB SSD- 33.78 cm(13.3)- OS X El Capitan) (Silver)""
        price: ""xxyy""
    },
    product_3: {
        title: ""...."",
        price: ""....""
    },

    ...

    product_99: {
        // product title and price
    }

}
</code></pre>

<p>When I used cosine similarity on the above list(data) of items, the values are as follows</p>

<pre><code>cosine(product_0 * product_1) = 0.973328526785
cosine(product_0 * product_2) = 0.50251890763
</code></pre>

<p>But in reality <code>product_0</code> and <code>product_1</code> are two different items but their consine similarity value shows that the items are identical; and <code>product_0</code> and <code>product_2</code> are from same entity but their cosine value shows that they are two different items.</p>

<p>I've been struggling to solve this problem on my own, thought I could ask for some suggestion/advice here in stackoverflow. Am I in the right direction using cosine similarity to match the similarities of items?. If not could you please channel me in the right direction.</p>

<p>My basic idea is to do a price comparison on identical items, i.e., Semantic Analysis of various similar product items.</p>

<p>Thanks for your time.</p>
",Vectorization & Embeddings,product price comparison tool difficulty matching identical item working creating e comm product price comparison tool python somewhat similar camelcamelcamel com fun profit facing difficult want match identical item list gathered various website using search term using cosine similarity thinking using levenshtein algorithm product matching match title various item find identical item example following item price value used cosine similarity list data item value follows reality two different item consine similarity value show item identical entity cosine value show two different item struggling solve problem thought could ask suggestion advice stackoverflow right direction using cosine similarity match similarity item could please channel right direction basic idea price comparison identical item e semantic analysis various similar product item thanks time
k-means using word2vec : Find nearest words to centroids,"<p>I am using word2vec for vectorization of texts and then k-means for clustering of texts using scikit-learn. After clustering, how do I get the top 5 or 10 words nearest to the centroid of each cluster? I am able to get all the words in the cluster but not able to get the nearest words. It was straight-forward when I was using tf-idf vectorizer as each feature in tf-idf maps to a word, but it is not the case with word2vec</p>

<p>Here's how I am using word2vec for k-means</p>

<pre><code>model = gensim.models.Word2Vec.load('w2v.mdel')
word_vectors =  vecTransform(input) #Convert input text to word vectors
km = KMeans(n_clusters=5)
idx = km.fit_predict(word_vectors)
</code></pre>
",Vectorization & Embeddings,k mean using word vec find nearest word centroid using word vec vectorization text k mean clustering text using scikit learn clustering get top word nearest centroid cluster able get word cluster able get nearest word wa straight forward wa using tf idf vectorizer feature tf idf map word case word vec using word vec k mean
How Can I use gensim package in Azure ML?,"<p>I am using text analysis with Azure ML. So in my python script I want to create a bag of word model and then calculate TFIDF of each words. For that I am using gensim model, It's not working on Azure ML. So is there any options for me? </p>
",Vectorization & Embeddings,use gensim package azure ml using text analysis azure ml python script want create bag word model calculate tfidf word using gensim model working azure ml option
How to find the most meaningful words in the text with using word2vec?,"<p>So, for instance, I'm typing, as an input, some sentence with some semantic meaning and, as an output, I get some list of closest (in cosine distance) words (mostly single words).</p>

<p>But I want to understand which cluster my sentence belongs to and compute how far is located each word from it. And eliminate non-meaningful words from sentence.</p>

<p>For example:</p>

<p>""I want to buy a pizza"";</p>

<p>""pizza"": 0.99123</p>

<p>""buy"": 0.7834</p>

<p>""want"": 0.1443</p>

<p>How such requirement can be achieved out of the box, without any C coding? </p>

<p>Maybe I need to compute cosine distance equation for this? </p>

<p>Thank you!</p>
",Vectorization & Embeddings,find meaningful word text using word vec instance typing input sentence semantic meaning output get list closest cosine distance word mostly single word want understand cluster sentence belongs compute far located word eliminate non meaningful word sentence example want buy pizza pizza buy want requirement achieved box without c coding maybe need compute cosine distance equation thank
"ngram vectorization - if new token found which not exists in corpus, what should I do with it","<p>I'm building custom ngram vectorizer for bag of word model. I'm qurious - what should I do if during vectorizing of a short text I found new token, which not exists in corpus vocabulary. Should it be just skipped or what?</p>
",Vectorization & Embeddings,ngram vectorization new token found exists corpus building custom ngram vectorizer bag word model qurious vectorizing short text found new token exists corpus vocabulary skipped
How do I get the TF-IDF score for a particular word in Gensim,"<p>wiki= gensim.corpora.MmCorpus(r'C:\Users\Public\Documents\Python Scripts\wiki_en_vocab200k.mm')</p>

<p>tfidf= gensim.models.TfidfModel.load(r'C:\Users\tfidf_model')</p>

<p>corpus_tfidf = tfidf[wiki]</p>

<p>I have the above steps but now I want to be able to find individual word scores</p>

<p>Many thanks in advance</p>
",Vectorization & Embeddings,get tf idf score particular word gensim wiki gensim corpus mmcorpus r c user public document python script wiki en vocab k mm tfidf gensim model tfidfmodel load r c user tfidf model corpus tfidf tfidf wiki step want able find individual word score many thanks advance
How to estimate memory needed for a cosine_similarity matrix?,"<p>I'm trying to compute a cosine_similarity matrix by <code>sklearn.metrics.pairwise.cosine_similarity</code>. The output matrix might be 60k*50k. However, it returns a MemoryError with 32G memory. I'm considering to add more memory to my server, however I'd like to estimate how much more do I really need. </p>

<p>Thanks.</p>
",Vectorization & Embeddings,estimate memory needed cosine similarity matrix trying compute cosine similarity matrix output matrix might k k however return memoryerror g memory considering add memory server however like estimate much really need thanks
Doc2vec Gensim: the word embeddings not updating during each epoch,"<p>I use Gensim Doc2vec model to train document vectors.
I printed out representations for the word 'good', but I found every epoch, I found not updating! While I printed out representations for the document with id '3', every epoch different! </p>

<p>My codes are below, do not know what is happening. </p>

<pre><code>model = gensim.models.Doc2Vec(dm = 0, alpha=0.1, size= 20, min_alpha=0.025)

model.build_vocab(documents)

print ('Building model....',(time4-time3))
for epoch in range(10):
    model.train(documents)

    print('Now training epoch %s' % epoch)
    print(model['good'])
    print(model.docvecs[str(3)])
</code></pre>
",Vectorization & Embeddings,doc vec gensim word embeddings updating epoch use gensim doc vec model train document vector printed representation word good found every epoch found updating printed representation document id every epoch different code know happening
Python - &quot;Undo&quot; text-wrap,"<p>I need to take a text and remove the \n character, which I believe I've done. The next task is to remove the hyphen from words where it should not appear but to leave the hyphen in compound words where it should appear. For example, 'encyclo-\npedia to 'encyclopedia' and 'long-\nterm' to 'long-term'. The suggestion is to compare it with an original text.</p>

<pre><code>with open('C:\Users\Paul\Desktop\Comp_Ling_Research_1\BROWN_A1_hypenated.txt', 'rU') as myfile:
data=myfile.read().replace('\n', '')
</code></pre>

<p>I have a general idea of what to do but NLP is quite new to me.</p>
",Vectorization & Embeddings,python undo text wrap need take text remove n character believe done next task remove hyphen word appear leave hyphen compound word appear example encyclo npedia encyclopedia long nterm long term suggestion compare original text general idea nlp quite new
How to detect duplicates among text documents and return the duplicates&#39; similarity?,"<p>I'm writing a crawler to get content from some website, but the content can duplicated, I want 
to avoid that. So I need a function can return the same percent between two text to detect two content maybe duplicated Example: </p>

<ul>
<li>Text 1:""I'm writing a crawler to""</li>
<li>Text 2:""I'm writing a some text crawler to get""</li>
</ul>

<p>The compare function will return text 2 as the same text 1 by 5/8%(with 5 is words number of text 2 same text 1(compare by word order), and 8 is total words of text 2). If remove the ""some text"" then text 2 as the same text 1(I need detect the situation).How can I do that?</p>
",Vectorization & Embeddings,detect duplicate among text document return duplicate similarity writing crawler get content website content duplicated want avoid need function return percent two text detect two content maybe duplicated example text writing crawler text writing text crawler get compare function return text text word number text text compare word order total word text remove text text text need detect situation
Embedding lookup from multiple embeddings in tensorflow,"<p>Building a doc2Vec algorithm, there is a need for having multiple embeddings around. There are embeddings for the word vectors, while at the same time there are embeddings for the documents themselves. The way the algorithm works is similar to that of a CBOW model, but the document embedding is also used per each document being trained with a given window. So if we have a window of 5 words, we go ahead and go through those 5 words, but per each window we will always include the document embedding vector itself so that we can update it. </p>
",Vectorization & Embeddings,embedding lookup multiple embeddings tensorflow building doc vec algorithm need multiple embeddings around embeddings word vector time embeddings document way algorithm work similar cbow model document embedding also used per document trained given window window word go ahead go word per window always include document embedding vector update
Modify tf-idf vectorizer for some keywords,"<p>I am creating a tf-idf matrix for finding cosine similarity. But I want some frequent words from a set to have more weightage(i.e, tf-idf value).     </p>

<pre><code>tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(documents)
</code></pre>

<p>How can I modify the above tfidf_matrix for words in a particular set.</p>
",Vectorization & Embeddings,modify tf idf vectorizer keywords creating tf idf matrix finding cosine similarity want frequent word set weightage e tf idf value modify tfidf matrix word particular set
Understanding another&#39;s text-mining function that removes similar strings,"<p>I’m trying to replicate the methodology from this article, <a href=""http://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/"" rel=""nofollow"">538 Post about Most Repetitive Phrases</a>, in which the author mined US presidential debate transcripts to determine the most repetitive phrases for each candidate.</p>

<p>I'm trying to implement this methodology with another dataset in R with the <code>tm</code> package.</p>

<p>Most of the code (<a href=""https://github.com/fivethirtyeight/data/blob/master/repeated-phrases-gop/robopol2.py"" rel=""nofollow"">GitHub repository</a>) concerns mining the transcripts and assembling counts of each ngram, but I get lost at the <code>prune_substrings()</code> function code below:</p>

<pre><code>def prune_substrings(tfidf_dicts, prune_thru=1000):

    pruned = tfidf_dicts

    for candidate in range(len(candidates)):
        # growing list of n-grams in list form
        so_far = []

        ngrams_sorted = sorted(tfidf_dicts[candidate].items(), key=operator.itemgetter(1), reverse=True)[:prune_thru]
        for ngram in ngrams_sorted:
            # contained in a previous aka 'better' phrase
            for better_ngram in so_far:
                if overlap(list(better_ngram), list(ngram[0])):
                    #print ""PRUNING!! ""
                    #print list(better_ngram)
                    #print list(ngram[0])

                    pruned[candidate][ngram[0]] = 0
            # not contained, so add to so_far to prevent future subphrases
            else:
                so_far += [list(ngram[0])]

    return pruned 
</code></pre>

<p>The input of the function, <code>tfidf_dicts</code>, is an array of dictionaries (one for each candidate) with ngrams as keys and tf-idf scores as values. For example, Trump's tf-idf dict begins like this:</p>

<pre><code>trump.tfidf.dict = {'we don't win': 83.2, 'you have to': 72.8, ... }
</code></pre>

<p>so the structure of the input is like this:</p>

<pre><code>tfidf_dicts = {trump.tfidf.dict, rubio.tfidf.dict, etc }
</code></pre>

<p>MY understanding is that <code>prune_substrings</code> does the following things, but I'm stuck on the <code>else if</code> clause, which is a pythonic thing I don't understand yet.</p>

<blockquote>
  <p>A.    create list : pruned as tfidf_dicts; a list of tfidf dicts for each candidate</p>
  
  <p>B loop through each candidate:</p>
  
  <ol>
  <li>so_far = start an empty list of ngrams gone through so so_far</li>
  <li>ngrams_sorted = sorted member's tf-idf dict from smallest to biggest</li>
  <li>loop through each ngram in sorted
  
  <ul>
  <li>loop through each better_ngram in so_far
  
  <ol>
  <li>IF overlap b/w (below) == TRUE:
  
  <ul>
  <li>better_ngram (from so_far) and</li>
  <li>ngram (from ngrams_sorted)</li>
  <li>THEN zero out tf-idf for ngram</li>
  </ul></li>
  <li>ELSE if (WHAT?!?)
  
  <ul>
  <li>add ngram to list, so_far</li>
  </ul></li>
  </ol></li>
  </ul></li>
  </ol>
  
  <p>C. return pruned, i.e. list of unique ngrams sorted in order</p>
</blockquote>

<p>Any help at all is much appreciated!</p>
",Vectorization & Embeddings,understanding another text mining function remove similar string trying replicate methodology article post repetitive phrase author mined u debate transcript determine repetitive phrase candidate trying implement methodology another dataset r package code github repository concern mining transcript assembling count ngram get lost function code input function array dictionary one candidate ngrams key tf idf score value example trump tf idf dict begin like structure input like understanding doe following thing stuck clause pythonic thing understand yet create list pruned tfidf dicts list tfidf dicts candidate b loop candidate far start empty list ngrams gone far ngrams sorted sorted member tf idf dict smallest biggest loop ngram sorted loop better ngram far overlap b w true better ngram far ngram ngrams sorted zero tf idf ngram else add ngram list far c return pruned e list unique ngrams sorted order help much appreciated
What impact does vocabulary_size have on word2vec tensorflow implementation?,"<p>I've performed the steps <a href=""https://www.tensorflow.org/versions/r0.10/tutorials/word2vec/index.html"" rel=""nofollow"" title=""This"">this</a> guide to generate a vector representation of words. </p>

<p>Now I'm using a custom dataset of 45'000 words I'm running word2vec on. </p>

<p>To run I modified word2vec_basic.py to use my own dataset by modifying <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L57"" rel=""nofollow"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L57</a> to <code>words = read_data('mytextfile.zip')</code></p>

<p>I encountered an issue similar to <a href=""https://github.com/tensorflow/tensorflow/issues/2777"" rel=""nofollow"">https://github.com/tensorflow/tensorflow/issues/2777</a> and so reduced the <code>vocabulary_size</code> to 200 . It now runs but the results do not appear to be capturing the context. For example here is a sample output : </p>

<pre><code>Nearest to Leave: Employee, it, •, due, You, appeal, Employees, which,
</code></pre>

<p>What can I infer from this output ? Will increasing/decreasing <code>vocabulary_size</code> improve results ?</p>

<p>I'm using python3 so to run I use <code>python3 word2vec_basic2.py</code> </p>
",Vectorization & Embeddings,impact doe vocabulary size word vec tensorflow implementation performed step guide generate vector representation word using custom dataset word running word vec run modified word vec basic py use dataset modifying encountered issue similar reduced run result appear capturing context example sample output infer output increasing decreasing improve result using python run use
measure of semantic similarity of 2 sentence,"<p>I need to measure the similarity between two sentences. For example:</p>

<pre><code>s1 = ""she is good a dog ""
s2 = ""she is nice a heel""
</code></pre>

<p>I need to prove that <code>""good""</code> is similar to <code>""nice""</code>. For nouns and verbs the measures of similarity by path is working like this pseudo code:</p>

<pre><code>def get max :
for loop
(wn.synset ('dog ')).path_similarity(wn.synset ('animal'))
</code></pre>

<p>Result: <code>.33</code>, which is a high value, then these words are related and I can say it's similar. But for adverbs (<code>""nice""</code> and <code>""good""</code>) the value <code>.09</code> is low!</p>

<p>Any ideas?</p>
",Vectorization & Embeddings,measure semantic similarity sentence need measure similarity two sentence example need prove similar noun verb measure similarity path working like pseudo code result high value word related say similar adverb value low idea
measure of semantic similarity of 2 sentence,"<p>I need to measure the similarity between two sentences. For example:</p>

<pre><code>s1 = ""she is good a dog ""
s2 = ""she is nice a heel""
</code></pre>

<p>I need to prove that <code>""good""</code> is similar to <code>""nice""</code>. For nouns and verbs the measures of similarity by path is working like this pseudo code:</p>

<pre><code>def get max :
for loop
(wn.synset ('dog ')).path_similarity(wn.synset ('animal'))
</code></pre>

<p>Result: <code>.33</code>, which is a high value, then these words are related and I can say it's similar. But for adverbs (<code>""nice""</code> and <code>""good""</code>) the value <code>.09</code> is low!</p>

<p>Any ideas?</p>
",Vectorization & Embeddings,measure semantic similarity sentence need measure similarity two sentence example need prove similar noun verb measure similarity path working like pseudo code result high value word related say similar adverb value low idea
"Train some embeddings, keep others fixed","<p>I do sequence classification with Keras, using an RNN and embeddings. My sequences are a bit weird. I have words mixed with special symbols. Words are associated with fixed, pre-trained embeddings, but the special symbol embeddings have to be modified during training.</p>

<p>In an Embedding layer during learning, how can I keep some embeddings fixed while updating others? Is there a way to mask those indices which shouldn't be modified? Or is this a case for a custom Embedding layer?</p>
",Vectorization & Embeddings,train embeddings keep others fixed sequence classification kera using rnn embeddings sequence bit weird word mixed special symbol word associated fixed pre trained embeddings special symbol embeddings modified training embedding layer learning keep embeddings fixed updating others way mask index modified case custom embedding layer
Embedding jape rules in java (Gate),"<p>I am trying to write my own rule that annotates Author (From author,jape) in my java code i have initialized my new processing resource.The code runs fine but does not annotates ma text:
input: who is the author of xyz
output: it should get annotated as author and shd save the name of book in some temporary variable.
my java code:</p>

<pre><code>    Gate.init();
Gate.getCreoleRegister().registerDirectories(
           new File(Gate.getPluginsHome(), ""ANNIE"").toURI().toURL());
SerialAnalyserController pipeline =
          (SerialAnalyserController)gate.Factory.createResource(
             ""gate.creole.SerialAnalyserController"");
LanguageAnalyser tokeniser = (LanguageAnalyser)gate.Factory.createResource(
             ""gate.creole.tokeniser.DefaultTokeniser"");
LanguageAnalyser jape = (LanguageAnalyser)gate.Factory.createResource(
          ""gate.creole.Transducer"", gate.Utils.featureMap(
              ""grammarURL"", new File(""E:\\GATE_Developer_7.1\\plugins\\ANNIE\\resources\\NE\\Author.jape"").toURI().toURL(),
              ""encoding"", ""UTF-8""));
pipeline.add(tokeniser);
pipeline.add(jape);
Corpus corpus = gate.Factory.newCorpus(null);
Document doc = gate.Factory.newDocument(""Who is author of Inception"");
DocumentContent dc=doc.getContent();        
corpus.add(doc);
pipeline.setCorpus(corpus);
pipeline.execute();
System.out.println(""Found annotations of the following types: "" +
          doc.getAnnotations().getAllTypes());
</code></pre>

<p>in output it only gives token,space token
Can anyone help me to workout the problem.?</p>
",Vectorization & Embeddings,embedding jape rule java gate trying write rule annotates author author jape java code initialized new processing resource code run fine doe annotates text input author xyz output get annotated author shd save name book temporary variable java code output give token space token anyone help workout problem
How to apply RNN to sequence-to-sequence NLP task?,"<p>I'm quite confused about sequence-to-sequence RNN on NLP tasks. Previously, I have implemented some neural models of classification tasks. In those tasks, the models take word embeddings as input and use a softmax layer at the end of the networks to do classification. But how do neural models do seq2seq tasks? If the input is word embedding, then what is the output of the neural model? Examles of these tasks include question answering, dialogue systems and machine translation.</p>
",Vectorization & Embeddings,apply rnn sequence sequence nlp task quite confused sequence sequence rnn nlp task previously implemented neural model classification task task model take word embeddings input use softmax layer end network classification neural model seq seq task input word embedding output neural model examles task include question answering dialogue system machine translation
Tf-Idf vectorizer analyze vectors from lines instead of words,"<p>I'm trying to analyze a text which is given by lines, and I wish to vectorize the lines using sckit-learn package's TF-IDF-vectorization in python.
The problem is that the vectorization can be done either by words or n-grams but I want them to be done for lines, and I already ruled out a work around that just vectorize each line as a single word (since in that way the words and their meaning wont be considered).</p>

<p>Looking through the documentation I didnt find how to do that, so is there any such option?</p>
",Vectorization & Embeddings,tf idf vectorizer analyze vector line instead word trying analyze text given line wish vectorize line using sckit learn package tf idf vectorization python problem vectorization done either word n gram want done line already ruled work around vectorize line single word since way word meaning wont considered looking documentation didnt find option
How to incorporate Deeplearning4j word2vec with Spark to convert words into its vector representation?,"<p>I want the DeepLearning4j Word2Vec with incorporate with Spark.I have around 80000 words data for which I want to get the vector representation. Later on, I want to find the synonyms using that vector representation. I am struggling where to write the Word2Vec code portion so that it can be provided to all the vCPU's?</p>
",Vectorization & Embeddings,incorporate deeplearning j word vec spark convert word vector representation want deeplearning j word vec incorporate spark around word data want get vector representation later want find synonym using vector representation struggling write word vec code portion provided vcpu
Different models with gensim Word2Vec on python,"<p>I am trying to apply the word2vec model implemented in the library gensim in python. I have a list of sentences (each sentences is a list of words).</p>

<p>For instance let us have:</p>

<pre><code>sentences=[['first','second','third','fourth']]*n
</code></pre>

<p>and I implement two identical models:</p>

<pre><code>model = gensim.models.Word2Vec(sententes, min_count=1,size=2)
model2=gensim.models.Word2Vec(sentences, min_count=1,size=2)
</code></pre>

<p>I realize that the models sometimes are the same, and sometimes are different, depending on the value of n. </p>

<p>For instance, if n=100 I obtain</p>

<pre><code>print(model['first']==model2['first'])
True
</code></pre>

<p>while, for n=1000:</p>

<pre><code>print(model['first']==model2['first'])
False
</code></pre>

<p>How is it possible?</p>

<p>Thank you very much!</p>
",Vectorization & Embeddings,different model gensim word vec python trying apply word vec model implemented library gensim python list sentence sentence list word instance let u implement two identical model realize model sometimes sometimes different depending value n instance n obtain n possible thank much
"In Elasticsearch, is possible to cluster documents that share the most similar texts, without giving an initial query to compare to?","<p>In Elasticsearch, is possible to group documents that share the most similar texts, without giving an initial query to compare to?</p>

<p>I know is possible to query and get ""more like this document"" but, is possible to cluster documents within an index according to a field values?</p>

<p>For instance:</p>

<p>document 1: <strong>The quick brown fox jumps over the lazy dog</strong></p>

<p>document 2: <strong>Barcelona is a great city</strong></p>

<p>document 3: <strong>The fast orange fox jumps over the lazy dog</strong></p>

<p>document 4: <strong>Madrid is a great city</strong></p>

<p>document 5: <strong>I do not like to eat fish</strong></p>

<p>Now, perform some kind of aggregation that, without giving a search query, it can group:</p>

<p><strong>Group 1</strong>: document 1 and document 3</p>

<p><strong>Group 2</strong>: document 2 and document 4</p>

<p><strong>Group 3</strong>: document 5</p>

<p>I will really appreciate any clue!</p>
",Vectorization & Embeddings,elasticsearch possible cluster document share similar text without giving initial query compare elasticsearch possible group document share similar text without giving initial query compare know possible query get like document possible cluster document within index according field value instance document quick brown fox jump lazy dog document barcelona great city document fast orange fox jump lazy dog document madrid great city document like eat fish perform kind aggregation without giving search query group group document document group document document group document really appreciate clue
Is there a Doc2vec model in tensorflow?,"<p>I know I am not suppose to ask for a tool, resource, etc on stackoverflow: But I think this is an important question and people will benefit from it. Here comes the question: I have found word2vec but failed to find doc2vec implementation in the tensorflow package, and will be surprised if it is not supported in tensorflow.</p>
",Vectorization & Embeddings,doc vec model tensorflow know suppose ask tool resource etc stackoverflow think important question people benefit come question found word vec failed find doc vec implementation tensorflow package surprised supported tensorflow
Datasets in Biodomain like Word similarity datasets used in word2vec and Glove,"<p>I am training word2vec on biomedical texts. In order to perform word similarity and word analogy tests I want to have pairs of biomedical terms having same relationships(could be any), just like we have a comprehensive list of City-State data in word2vec. I tried searching the web but since I am new to the domain I am finding it confusing.  </p>

<p>So, where can I find the list relevant to Drug-gene or Protein-action, etc? Or how can I mine this data. Please suggest publicly available such datasets. Also, please suggest any additional interesting relationships which I can also query. </p>

<p>Another way would be to use available ontologies as they include relations between concepts such as has-part, is-a-way-of-doing, is-a-cause-of, is-a-symptom-of etc. Can I use ontologies to extract such pairs? If yes, then what ontologies and how? </p>

<p>Are there any gold standard datasets already available that can serve my purpose? </p>
",Vectorization & Embeddings,datasets biodomain like word similarity datasets used word vec glove training word vec biomedical text order perform word similarity word analogy test want pair biomedical term relationship could like comprehensive list city state data word vec tried searching web since new domain finding confusing find list relevant drug gene protein action etc mine data please suggest publicly available datasets also please suggest additional interesting relationship also query another way would use available ontology include relation concept ha part way cause symptom etc use ontology extract pair yes ontology gold standard datasets already available serve purpose
Cosine similarity of representation of sentences formed with word vectors now measure word order?,"<p>I know, the original cosine similarity, when applied to representation of two documents by frequency of specific words, do not measure word order. I now see a whole bunch of papers applying cosine similarity to representation of pairs of sentences formed by words vectors. I assume they flatten the token# x embedding length matrix of each sentence to a long vector whose length is token# x embedding length of the original sentence. So ""I love you"" and ""you love me(normalized to ""I"") would not yield 1 in this new way of applying cosine similarity whereas the old way would yield 1. Am I correct? Thanks for any enlightening answer.</p>
",Vectorization & Embeddings,cosine similarity representation sentence formed word vector measure word order know original cosine similarity applied representation two document frequency specific word measure word order see whole bunch paper applying cosine similarity representation pair sentence formed word vector assume flatten token x embedding length matrix sentence long vector whose length token x embedding length original sentence love love normalized would yield new way applying cosine similarity whereas old way would yield correct thanks enlightening answer
How to create document features from word-vectors?,"<p>I have around 100000 documents of varying word length. I also have trained a word2vec model on the entire corpus. Now how do I go from having this word-vectors to create features of same dimension for each individual documents?      </p>

<p>I am aware of a couple of techniques of how this can be done, one is to take simple average of vectors of all the words in the document and another is to do k-means clustering.</p>

<p>Can you suggest some other way of carrying out this task?</p>
",Vectorization & Embeddings,create document feature word vector around document varying word length also trained word vec model entire corpus go word vector create feature dimension individual document aware couple technique done one take simple average vector word document another k mean clustering suggest way carrying task
How to deal with search queries which have spaces in the wrong place while making a search engine of an e-commerce website?,"<p>Suppose I have an entry pepe jeans in the website. The user searches pepejeans. I am currently using tf-idf and cosine similarity for returning the most significant results, but I have kept the memory usage as small as possible because the content is large. So what can I use to deal with this problem? One solution is to keep the record with space and without space of all ngrams of the content.</p>

<p>Another example-
If the user searches for nikeshoes. Now nike is a brand but nikeshoes is not a brand. It the search query was nike shoes then it would have returned nike as brand.
The content here refers to brand names. I am trying to identify the brand name from the query</p>
",Vectorization & Embeddings,deal search query space wrong place making search engine e commerce website suppose entry pepe jean website user search pepejeans currently using tf idf cosine similarity returning significant result kept memory usage small possible content large use deal problem one solution keep record space without space ngrams content another example user search nikeshoes nike brand nikeshoes brand search query wa nike shoe would returned nike brand content refers brand name trying identify brand name query
Python Pandas - compare column text and provide matched word count,"<p>I am trying to develop a string compare tool. I have two sets of json data as below.</p>

<p>DF 1:</p>

<pre><code>ID  Subject
1   Angular JS : getting unexpected cross symbol with Image
2   Cordova debug: the specified file was not found
3   get custom mask for phone numbers
4   Remove files for the Xcode Bots Unit Test Coverage
5   ""Upload to Mongodb collection in aldeed:autoform
6   Mask for phone numbers
</code></pre>

<p>DF 2:</p>

<pre><code>ID  Subject
1   Please provide custom mask for phone numbers
2   Files for the Xcode Bots Unit Test Coverage need to be removed
3   Upload to Mongodb collection
</code></pre>

<p>Now, with python + pandas , for each of the Table 2 ID, I want to find an entry in Table 1 row which matches closely, words sequence doesnt matter, need to eliminate special characters from comparison.</p>

<p>For example:</p>

<pre><code>For ID 1 - ID 2 has 5 matching words
For ID 1 - ID 6 has 4 matching words
For ID 2 - ID 4 has 8 matching words
For ID 3 - ID 4 has 4 matching words
</code></pre>

<p>Any pointers?</p>
",Vectorization & Embeddings,python panda compare column text provide matched word count trying develop string compare tool two set json data df df python panda table id want find entry table row match closely word sequence doesnt matter need eliminate special character comparison example pointer
Should I use word2vec to do word embedding including testing data?,"<p>I am a new people in NLP and I am try do the text classification job. Before doing the job, I know that we should do word embedding.
My question is should I do word embedding job only on training data <strong>(so that testing data get vector just from pre-trained vec-model of training data)</strong>, or both on training data &amp; testing data?</p>
",Vectorization & Embeddings,use word vec word embedding including testing data new people nlp try text classification job job know word embedding question word embedding job training data testing data get vector pre trained vec model training data training data testing data
Where can I get CoNLL-X training data?,"<p>I'm trying to train the Stanford Neural Network Dependency Parser to check phrase similarity.</p>

<p>The way I tried is:</p>

<pre><code>java edu.stanford.nlp.parser.nndep.DependencyParser -trainFile trainPath -devFile devPath -embedFile wordEmbeddingFile -embeddingSize wordEmbeddingDimensionality -model modelOutputFile.txt.gz
</code></pre>

<p>The error that I got is:</p>

<pre><code>Train File: C:\Users\rohit\Downloads\CoreNLP-master\CoreNLP-master\data\edu\stanford\nlp\parser\trees\en-onetree.txt
Dev File: null
Model File: modelOutputFile.txt.gz
Embedding File: null
Pre-trained Model File: null
################### Train
#Trees: 1
0 tree(s) are illegal (0.00%).
1 tree(s) are legal but have multiple roots (100.00%).
0 tree(s) are legal but not projective (0.00%).
###################
#Word: 3
#POS:3
#Label: 2
###################
#Transitions: 3
#Labels: 1
ROOTLABEL: null
Random generator initialized with seed 1459831358061
Exception in thread ""main"" java.lang.NullPointerException
    at edu.stanford.nlp.parser.nndep.Util.scaling(Util.java:49)
    at edu.stanford.nlp.parser.nndep.DependencyParser.readEmbedFile.  (DependencyParser.java:636)
    at edu.stanford.nlp.parser.nndep.DependencyParser.setupClassifierForTraining(DependencyParser.java:787)
    at edu.stanford.nlp.parser.nndep.DependencyParser.train(DependencyParser.java:676)
    at edu.stanford.nlp.parser.nndep.DependencyParser.main(DependencyParser.java:1247)
</code></pre>

<p>The help embedded within the code says that the training file should be a - ""Path to a training treebank in CoNLL-X format"". </p>

<p>Does anyone know where I can find some CoNLL-X training data to train?
I gave training file but not embedding file and got this error.
My guess is if I give the embedding file it might work.</p>

<p>Please shed some light on which training file &amp; embedding file I should use and where I can find them.</p>
",Vectorization & Embeddings,get conll x training data trying train stanford neural network dependency parser check phrase similarity way tried error got help embedded within code say training file path training treebank conll x format doe anyone know find conll x training data train gave training file embedding file got error guess give embedding file might work please shed light training file embedding file use find
constructing skip-gram input vector,"<p>I am following the tutorial <a href=""https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/"" rel=""nofollow noreferrer"">here</a> for implementing word2vec, and I am not sure if I understand how the skip-gram input vector is constructed. </p>

<p><a href=""https://i.sstatic.net/Gmh8w.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Gmh8w.png"" alt=""enter image description here""></a></p>

<p>This is the part I am confused about. I thought we were not doing one-hot encoding in word2vec. </p>

<p>For example, if we were to have two sentences ""dogs like cats"", ""cats like dogs"", or some more informative sentences, what would the input vector look like? Thank you.</p>
",Vectorization & Embeddings,constructing skip gram input vector following tutorial implementing word vec sure understand skip gram input vector constructed part confused thought one hot encoding word vec example two sentence dog like cat cat like dog informative sentence would input vector look like thank
information retrieval probabilistic model,"<p>Do you know where I can find source code(any language) to program an information retrieval system based on the probabilistic model?</p>

<p>I tried to search it on the web and found an algorithm named bm25 or bmf25, but I don't know if it is useful.</p>

<p>Basically I´m trying to compare the performance of 3 IR algorithms: Vector space model, boolean model and the probabilistic model. Right now I have found the vector space and the boolean models. Depending on the results we need to use the best of them to develop a question-answering system</p>

<p>Thanks in advance</p>
",Vectorization & Embeddings,information retrieval probabilistic model know find source code language program information retrieval system based probabilistic model tried search web found algorithm named bm bmf know useful basically trying compare performance ir algorithm vector space model boolean model probabilistic model right found vector space boolean model depending result need use best develop question answering system thanks advance
output of MLP is always 0,"<p>I'm doing matching sentence using CNN, the main algorithm is described as bellow(detailed algorithm is <a href=""http://www.hangli-hl.com/uploads/3/1/6/8/3168008/hu-etal-nips2014.pdf"" rel=""nofollow"">here</a>):</p>

<ol>
<li>get m words from sentence A, also get m words from sentence B. Then combine them, thus get a matrix M1 whose shape is 2m*word_embedding_dim. Next, do a convolution to the matrix we get, thus get a single value.</li>
<li>repeat step 1 until we get all the compositions of words from sentence A and B. At the same time,composite all the single value we get by convolution to form a new matrix M2.</li>
<li>Treat M2 as an image, do next: M2->conv_1->pool_1->conv_2->pool_2->fully_connect->MLP_1->single value(similarity, value in [0,1])</li>
</ol>

<p>The code is <a href=""https://github.com/GaigeLynch/text_similarity_cnn"" rel=""nofollow"">here</a></p>

<p>In my training set, the labels is 0(not similar) or 1(similar), However, the output is always '0'. I have no idea where is wrong. This problem is making me mad. Is the network is too deep? Or, is the network is somewhere wrong. Could please someone help me? Any advice is appreciated. </p>
",Vectorization & Embeddings,output mlp always matching sentence using cnn main algorithm described bellow detailed algorithm get word sentence also get word sentence b combine thus get matrix whose shape word embedding dim next convolution matrix get thus get single value repeat step get composition word sentence b time composite single value get convolution form new matrix treat image next conv pool conv pool fully connect mlp single value similarity value code training set label similar similar however output always idea wrong problem making mad network deep network somewhere wrong could please someone help advice appreciated
Word2Vec for PoS Tagging,"<p>May we use Word2Vec for implementing Parts of Speech(PoS) Tagging, or similar other problems of label sequencing. 
I feel we may address this problem from clustering, so I was curious. 
Generally any good lead or discussion would be nice, but a Python example preferably with Gensim learn may be great, as it may have a Word2Vec implementation.  </p>
",Vectorization & Embeddings,word vec po tagging may use word vec implementing part speech po tagging similar problem label sequencing feel may address problem clustering wa curious generally good lead discussion would nice python example preferably gensim learn may great may word vec implementation
cosine() similarity with sparse matrices in R -- how to speed-up ? (NLP,"<p>I'm using R cosine() function {""lsa"" package} to determine the similarity between 2 vectors that are stored in sparse matrices. There are 240,000 lines in each matrix, and 3100 columns.  The calculation time for 1000 lines is about 2 minutes, which translates to 8 hours processing of the full data-set.   I'm looking for ways to expedite this processing time, since I'll have to run it several times. </p>

<p>This is my code:</p>

<pre><code>t1 &lt;- Sys.time()
for (i in 1:NROWS) {
  title_cosine_sim[i] &lt;- cosine(dtm_search[i,],dtm_title[i,])
  desc_cosine_sim[i] &lt;- cosine(dtm_search[i,], dtm_desc[i, ])
  if (i%%1000 == 0 ) {
    cat(""processed "", i, ""rows out of 240,760..."", system.time(), ""\n"")
  }
}
print( difftime( Sys.time(), t1, units = 'sec'))
cat(""finish"")
</code></pre>

<p>I tried to use parallel processing but it didn't speed up the process: </p>

<pre><code>c1 &lt;- makeCluster(4)
registerDoParallel(c1)

    #calculate title similarity vector
    t1 &lt;- Sys.time()
    title_cosine_sim &lt;- foreach(z = 1:NUMROWS, .combine = 'rbind', .packages =  c('lsa', 'Matrix')) %dopar% {
        cosine( dtm_search[z,],  dtm_title[z,] )
    }

    # calculate description similarity vector
    t1 &lt;- Sys.time() 
    desc_cosine_sim &lt;- foreach(z = 1:NUMROWS, .combine = 'rbind', .packages =  c('lsa', 'Matrix')) %dopar% {
        cosine( dtm_search[z,],dtm_title[z,] )
    }


stopCluster(c1)
print( difftime( Sys.time(), t1, units = 'sec'))
cat(""finish PARALLEL processing"")
</code></pre>

<p>Each of the above loops took about 65 seconds for 1000 lines. So there is not much savings in here... </p>

<p>Any idea about improving the code using parallel computing or otherwise are welcome. 
Thank you!</p>

<p>p.s.   Dell 4 cores, 16G RAM, Windows 10. </p>
",Vectorization & Embeddings,cosine similarity sparse matrix r speed nlp using r cosine function lsa package determine similarity vector stored sparse matrix line matrix column calculation time line minute translates hour processing full data set looking way expedite processing time since run several time code tried use parallel processing speed process loop took second line much saving idea improving code using parallel computing otherwise welcome thank p dell core g ram window
How to compute word similarity using TF-IDF or LSA with gensim?,"<p>I know that word2vec in gensim can compute similarity between words. But now I want to compute word similarity using TF-IDF or LSA with <strong>gensim</strong>. How to do it? </p>

<p>note: Computing document similarity using LSA with gensim is easy: <a href=""http://radimrehurek.com/gensim/wiki.html"" rel=""nofollow"">http://radimrehurek.com/gensim/wiki.html</a></p>
",Vectorization & Embeddings,compute word similarity using tf idf lsa gensim know word vec gensim compute similarity word want compute word similarity using tf idf lsa gensim note computing document similarity using lsa gensim easy
Average sentence length for every text in corpus (python3 &amp; nltk),"<p>I'm analyzing the inaugural address corpus in the NLTK package as part of an introduction to python programming course. I'd like to find out the average sentence length of each text within the corpus (so that I can later compare them), but I seem to be stuck here. </p>

<p>I've created this function:</p>

<pre><code>def averageSentence(text):
    sents = inaugural.sents(fileids=['fileid_here.txt']  
    avg = sum(len(word) for word in sents) / len(sents)  
    print(avg)
</code></pre>

<p>which (if I'm correct) should give me the average sentence length for an individual text. Now, I know I need a <em>for loop</em>. Shouldn't I be able to make a relatively easy and straightforward for loop with this function I just defined? This is very frustrating. </p>

<p>EDIT: this is how far I have gotten:</p>

<pre><code>for fileid in inaugural.fileids():
    avg_sents = averageSentence(fileid)
    print = sum(avg_sents) / avg_sents
</code></pre>
",Vectorization & Embeddings,average sentence length every text corpus python nltk analyzing inaugural address corpus nltk package part introduction python programming course like find average sentence length text within corpus later compare seem stuck created function correct give average sentence length individual text know need loop able make relatively easy straightforward loop function defined frustrating edit far gotten
Training a CNN with pre-trained word embeddings is very slow (TensorFlow),"<p>I'm using TensorFlow (0.6) to train a CNN on text data. I'm using a method similar to the second option specified in <a href=""https://stackoverflow.com/questions/35687678/using-a-pre-trained-word-embedding-word2vec-or-glove-in-tensorflow"">this SO thread</a> (with the exception that the embeddings are trainable). My dataset is pretty small and the vocabulary is around 12,000 words. When I train using random word embeddings everything works nicely. However, when I switch to <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">the pre-trained embeddings from the word2vec site</a>, the vocabulary grows to over 3,000,000 words and training iterations become over 100 times slower. I'm also seeing this warning:</p>

<blockquote>
  <p>UserWarning: Converting sparse IndexedSlices to a dense Tensor with
  900482700 elements</p>
</blockquote>

<p>I saw the discussion on <a href=""https://github.com/tensorflow/tensorflow/issues/464"" rel=""nofollow noreferrer"">this TensorFlow issue</a>, but I'm still not sure if the slowdown I'm experiencing is expected or if it's a bug. I'm using the Adam optimizer but it's pretty much the same thing with Adagrad.</p>

<p>One workaround I guess I could try is to train using a minimal embedding matrix with only the ~12,000 words in my dataset, serialize the resulting embeddings and at runtime merge them with the remaining words from the pre-trained embeddings. I think this should work but it sounds hacky. </p>

<p>Is that currently the best solution or am I missing something?</p>
",Vectorization & Embeddings,training cnn pre trained word embeddings slow tensorflow using tensorflow train cnn text data using method similar second option specified pre trained embeddings word vec site vocabulary grows word training iteration become time slower also seeing warning userwarning converting sparse indexedslices dense tensor element saw discussion tensorflow issue still sure slowdown experiencing expected bug using adam optimizer pretty much thing adagrad one workaround guess could try train using minimal embedding matrix word dataset serialize resulting embeddings runtime merge remaining word pre trained embeddings think work sound hacky currently best solution missing something
finding novelty of document,"<p>I have a collection of documents created at different times. I need to know for each new incoming document, how similar it is to the set of documents already added. New documents can add new terms, and hence in such documents I would expect ""novelty"" to be high. I need to get a sense of this novelty (or alternatively, distance) </p>

<p>For example, say there are d0, d1, d2 , d3 already and I have a new document d4 </p>

<p>I want to get a sense of how different d4 is from d0, d1, d2 and d3. </p>

<p>I have thought of a few ways but there are  a few limitations: </p>

<p>a) comput cosine similarity between each of (d0, d4) , (d1, d4), (d2, d4) , (d3, d4) </p>

<ul>
<li>find the average cosine similarity.</li>
</ul>

<p>OR </p>

<p>find the min of negative of cosine angle between the new document, d4,  and each previously seen document i.e. d0, d1, d2, d4</p>

<p>the idea being that the minimum will give a sense of the novelty  of d4. </p>

<p>b) combine d0, d1, d2, d3  and compare it to d4 
and then find cosine similarity </p>

<p>Do these approaches seem ok? Moreover, are there more suitable ways to get a sense of novelty, perhaps with K-means clustering? </p>
",Vectorization & Embeddings,finding novelty document collection document created different time need know new incoming document similar set document already added new document add new term hence document would expect novelty high need get sense novelty alternatively distance example say already new document want get sense different thought way limitation comput cosine similarity find average cosine similarity find min negative cosine angle new document previously seen document e idea minimum give sense novelty b combine compare find cosine similarity approach seem ok moreover suitable way get sense novelty perhaps k mean clustering
How to get highly contextualized vectors for a few sentences/paragraphs with doc2vec,"<p>How to get highly contextualized vectors for a few sentences? I suppose if I use a little corpus (where uniquely a few sentences are present) for training doc2vec, the trained vectors could not be well trained. So how to add context in an efficient fashion? </p>

<p>My initial idea is to add sentences which I'm interested in to a mass corpus (e.g. Wikipedia). Then retrieve the corresponding vectors. Nevertheless, I think this approach could be expensive because the algorithm has to consider all sentences in the corpus (e.g. millions of them, but it is probably the logic assumtion).</p>

<p>Thank you.</p>
",Vectorization & Embeddings,get highly contextualized vector sentence paragraph doc vec get highly contextualized vector sentence suppose use little corpus sentence present training doc vec trained vector could well trained add context efficient fashion initial idea add sentence interested mass corpus e g wikipedia retrieve corresponding vector nevertheless think approach could expensive algorithm ha consider sentence corpus e g million probably logic assumtion thank
word2vec : Parameterization of the skip-gram model,"<p>I am reading this paper <a href=""http://arxiv.org/pdf/1402.3722v1.pdf"" rel=""nofollow"">word2vec Explained</a> and came across equation (3). I dont exactly understand what is Vc and Vw. It says they are vector representation of words. But still what is it? How to get its value? I am about to implement it and want to know what exactly it means. </p>

<p><strong>Edit</strong>: Are Vc and Vw one-hot vector of words as mentioned in <a href=""http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf"" rel=""nofollow"">word2vec Parameter Learning Explained
</a></p>
",Vectorization & Embeddings,word vec parameterization skip gram model reading paper word vec explained came across equation dont exactly understand vc vw say vector representation word still get value implement want know exactly mean edit vc vw one hot vector word mentioned word vec parameter learning explained
extract common elements in several lists,"<hr>

<p>In general, what I want to do is to extract common elements in the sharing column of ""word"" in several csv files. (2008.csv, 2009.csv, 2010.csv .... 2015.csv)</p>

<hr>

<p>All files are in the same format:'word','count'</p>

<p>'word' contain all frequent words in one document in a particular year.</p>

<hr>

<p>here is a snapshot of one of files:</p>

<p><a href=""https://i.sstatic.net/Dwiwn.png"" rel=""nofollow"">file 2008.csv</a></p>

<hr>

<p>As long as there are two out of 8 files having common elements, I want to know those sharing elements and whichever files are they in. (this is quite like tfidf calculation...btw)</p>

<p>Anyway, my goal is to know some trends of frequent words appearance in those files.
(To my knowledge, one element can be in at most five files.)</p>

<p>And I want to know the words when they first appear, which means, a word in file C but not in both file B and A. </p>

<p>I know for + if might solve the problem here, but it is quite tedious, I need to compare 2 out of 8, 3 out of 8, 4 out of 8... columns, in that case, to find sharing elements. </p>

<p>this is the code I worked out so far... far away from what I need... I just compare elements in two out of 8 files:
<a href=""https://i.sstatic.net/e56TV.png"" rel=""nofollow"">code</a></p>

<p>Can anyone help?</p>
",Vectorization & Embeddings,extract common element several list general want extract common element sharing column word several csv file csv csv csv csv file format word count word contain frequent word one document particular year snapshot one file file csv long two file common element want know sharing element whichever file quite like tfidf calculation btw anyway goal know trend frequent word appearance file knowledge one element five file want know word first appear mean word file c file b know might solve problem quite tedious need compare column case find sharing element code worked far far away need compare element two file code anyone help
Stanford GloVe&#39;s lack of punctuation?,"<p>I understand that <a href=""http://nlp.stanford.edu/projects/glove/"" rel=""nofollow"">GloVe</a> trains vectors by noticing what frequently co-occurs, etc, but how come commas and periods are not included? For anything NLP, it seems like it would be an important feature to have a vector representation. I realize that something like (king - man = queen) would make no sense with (word - , = ?), but is there a way to represent punctuation marks and Numbers? </p>

<p>Is there a pre-made data set that includes such things? Would this even work? </p>

<p>I tried training GloVe with my own data set, but I ran into a problem with separating the punctuation (with a blank space) between words, etc.</p>
",Vectorization & Embeddings,stanford glove lack punctuation understand glove train vector noticing frequently co occurs etc come comma period included anything nlp seems like would important feature vector representation realize something like king man queen would make sense word way represent punctuation mark number pre made data set includes thing would even work tried training glove data set ran problem separating punctuation blank space word etc
Variable importance in classification,"<p>For example: I have 100 books with 1000 words each. They belong to different classes (comedy,drama,...). Each class consist of 15 different books. 
When I do tfidf on my data, I get the importance for every word in a book in context of all books.
I see that the books belonging to the same class have similar tfidf values for each variable.</p>

<p>Let's say drama and comedy are pretty similar. 
How can I tell what words make a difference in between those two classes?
What words do I have to change in book that belongs to comedy so the book now belongs to drama now? </p>

<p>I can check one by one; but I have 2000 books, 17500 words each; 950 classes. It would take a decade :)</p>
",Vectorization & Embeddings,variable importance classification example book word belong different class comedy drama class consist different book tfidf data get importance every word book context book see book belonging class similar tfidf value variable let say drama comedy pretty similar tell word make difference two class word change book belongs comedy book belongs drama check one one book word class would take decade
How is TF calculated in Sklearn,"<p>I have been experimenting with sklearn's <code>Tfidfvectorizer</code>.
I am only concerned with TF, and not idf, so my settings have <code>use_idf = FALSE</code>
Complete settings are:</p>

<pre><code>vectorizer = TfidfVectorizer(max_df=0.5, max_features= n_features,
                         ngram_range=(1,3), use_idf=False)
</code></pre>

<p>I have been trying to replicate the output of <code>.fit_transform</code> but haven't managed to do it so far and was hoping someone could explain the calculations for me. </p>

<p>My toy example is:</p>

<pre><code>document = [""one two three one four five"",
            ""two six eight ten two""]

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
n_features = 5
vectorizer = TfidfVectorizer(max_df=0.5, max_features= n_features,
                             ngram_range=(1,3), use_idf=False)

X = vectorizer.fit_transform(document)

count = CountVectorizer(max_df=0.5, max_features= n_features,
                             ngram_range=(1,3))
countMat = count.fit_transform(document)
</code></pre>

<p>I have assumed the counts from the Count Vectorizer will be the same as the counts used int he Tfidf Vectorizer. So am trying to change the countMat object to match X. </p>
",Vectorization & Embeddings,tf calculated sklearn experimenting sklearn concerned tf idf setting complete setting trying replicate output managed far wa hoping someone could explain calculation toy example assumed count count vectorizer count used int tfidf vectorizer trying change countmat object match x
How can I evaluate my technique?,"<p>I am dealing with a problem of text summarization i.e. given a large chunk(s) of text, I want to find the most representative ""topics"" or the subject of the text. For this, I used various information theoretic measures such as TF-IDF, Residual IDF and Pointwise Mutual Information to create a ""dictionary"" for my corpus. This dictionary contains important words mentioned in the text. </p>

<p>I manually sifted through the entire 50,000 list of phrases sorted on their TFIDF measure and hand-picked 2,000 phrases (I know! It took me 15 hours to do this...) that are the ground truth i.e. these are important for sure. Now when I use this as a dictionary and run a simple frequency analysis on my text and extract the top-k phrases, I am basically seeing what the subject is and I agree with what I am seeing.</p>

<p>Now how can I evaluate this approach? There is no machine learning or classification involved here. Basically, I used some NLP techniques to create a dictionary and using the dictionary alone to do simple frequency analysis is giving me the topics I am looking for. However, is there a formal analysis I can do for my system to measure its accuracy or something else?</p>
",Vectorization & Embeddings,evaluate technique dealing problem text summarization e given large chunk text want find representative topic subject text used various information theoretic measure tf idf residual idf pointwise mutual information create dictionary corpus dictionary contains important word mentioned text manually sifted entire list phrase sorted tfidf measure hand picked phrase know took hour ground truth e important sure use dictionary run simple frequency analysis text extract top k phrase basically seeing subject agree seeing evaluate approach machine learning classification involved basically used nlp technique create dictionary using dictionary alone simple frequency analysis giving topic looking however formal analysis system measure accuracy something else
Can we compute word and sentence vectors at the same time using Word2Vec or Doc2Vec?,"<p>I know we can sum up all the word vectors and then we can take the average to represent a sentence, but is there any other better way to represent a sentence?</p>
",Vectorization & Embeddings,compute word sentence vector time using word vec doc vec know sum word vector take average represent sentence better way represent sentence
How to measure term constraint in sentence?,"<p>I am trying to figure a way to algorithmically compute the informativeness of a word in a sentence based on how constrained the sentence is with that word missing. For example, how can I compute how much information the word ""fox"" provides in the sentence:</p>

<p>""the quick brown ___ jumped over the fence""</p>

<p>That is, in the absence of the target word, how can I compute how much more difficult it is for a user to fill in the blanks?</p>

<p>I have so far tried an approach where I look at the total word2vec semantic similarity of the target word with all of the surrounding words but that doesn't seem to work all the times. Is there any existing work that has been done on this?</p>
",Vectorization & Embeddings,measure term constraint sentence trying figure way algorithmically compute informativeness word sentence based constrained sentence word missing example compute much information word fox provides sentence quick brown jumped fence absence target word compute much difficult user fill blank far tried approach look total word vec semantic similarity target word surrounding word seem work time existing work ha done
Spark: Word classification,"<p>I got a question about word classification in Spark. I am working on a simple classification model that takes a word (a single word), as an input and its predict the race of the named person (it is from a fictitious universe). For example, Gimli -> dwarf, Legolas -> elf.</p>

<p>My issue is on how to process the words. I know that Spark includes a two feature vectorization methods, tf–idf and word2vec. However, I am having difficulties on understanding them and do not know which one to use.</p>

<p>Could anyone explained them to me and guide through the process?. And more importantly, I would like to know which of these methods is the most appropriate for this case.</p>

<p>Thanks</p>
",Vectorization & Embeddings,spark word classification got question word classification spark working simple classification model take word single word input predict race named person fictitious universe example gimli dwarf legolas elf issue process word know spark includes two feature vectorization method tf idf word vec however difficulty understanding know one use could anyone explained guide process importantly would like know method appropriate case thanks
Building Vector for a sentence in doc2vec from an untrained data set,"<p>I have a <code>doc2vec</code> model build from my data, now I have a new sentence in run time which does not belong to the trained data set.</p>

<p>How can I build or predict a vector for this sentence from my model?</p>

<p>How should I handle unknown words in this sentence?</p>
",Vectorization & Embeddings,building vector sentence doc vec untrained data set model build data new sentence run time doe belong trained data set build predict vector sentence model handle unknown word sentence
Gensim&#39;s Document similarity can be used as supervised classification?,"<p>Gensim has this document similarity feature which when inputted a query document, it outputs the similarity of that particular document with all the documents it has in its index</p>

<ol>
<li><p>Can this be used like an ""approximate"" version of supervised classification?</p></li>
<li><p>I know gensim's word2vec uses Deep Learning, is this involved during the above step?</p></li>
</ol>
",Vectorization & Embeddings,gensim document similarity used supervised classification gensim ha document similarity feature inputted query document output similarity particular document document ha index used like approximate version supervised classification know gensim word vec us deep learning involved step
NLP - Open Vocabulary Word Embedding,"<p>How can I convert words to vectors (Word embedding) if I don't have a predefined dictionary of words? Most word embedding implementations like <a href=""https://code.google.com/p/word2vec/"" rel=""nofollow"">Word2vec</a> and <a href=""http://nlp.stanford.edu/projects/glove/"" rel=""nofollow"">GloVe</a> have a fixed dictionary of words. The input to the neural networks are one-hot encoded and the hidden layer sizes also depend on the vocab size, which makes adding a new word later on without re-training all the vectors again impossible. I need a network that outputs a fixed dimensional vector for any arbitrary word input. But how to input the 'word' into the network? one hot encoding is not possible, as i don't have a fixed dictionary of words.</p>

<p>Will converting the word to a trigram vector or a bigram vector work? Trigram vectors have been used for Sentence embedding (<a href=""http://arxiv.org/pdf/1502.06922.pdf"" rel=""nofollow"">Deep Sentence Embedding Using Long Short-Term Memory Networks</a>), but I doubt if it will work equally as well for word embedding, as there are changes in both the network architecture (Word embedding uses shallow network whereas sentence embedding uses RNNs) and the auxiliary task. Please help.</p>

<p>Note:</p>

<p>By ""converting to trigram vector"" I mean the following :</p>

<ul>
<li>Let the input word be ""CAT"" Add #s at the beginning and at the end :
""#CAT#""</li>
<li>List all the possible tri-grams:  #CA, CAT, AT#</li>
<li>Each trigram is converted to a one hot encoded vector of dimension NxNxN 
where N is my character set size. eg., E(""#CA"") = {0,0,0,0,0,...,0,<strong>1</strong>,0,0,0} </li>
<li>The one hot encoded vector of every trigram of the word is added to
get the ""tri-gram vector"" of the word.
e.g., trigram_vec(""CAT"") = {0,0,0,0,...0,<strong>1</strong>,0,0,...0,0,<strong>1</strong>,0,...0,0,<strong>1</strong>,0,0,0,0}</li>
</ul>

<p>Thanks for any help in advance!</p>
",Vectorization & Embeddings,nlp open vocabulary word embedding convert word vector word embedding predefined dictionary word word embedding implementation like word vec glove fixed dictionary word input neural network one hot encoded hidden layer size also depend vocab size make adding new word later without training vector impossible need network output fixed dimensional vector arbitrary word input input word network one hot encoding possible fixed dictionary word converting word trigram vector bigram vector work trigram vector used sentence embedding deep sentence embedding using long short term memory network doubt work equally well word embedding change network architecture word embedding us shallow network whereas sentence embedding us rnns auxiliary task please help note converting trigram vector mean following let input word cat add beginning end cat list possible tri gram ca cat trigram converted one hot encoded vector dimension nxnxn n character set size eg e ca one hot encoded vector every trigram word added get tri gram vector word e g trigram vec cat thanks help advance
How to get word count from TF*IDF value in sklearn,"<p>I want to get the count of a word in a given sentence using only tf*idf matrix of a set of sentences. I use TfidfVectorizer from sklearn.feature_extraction.text.</p>

<p>Example : </p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

sentences = (""The sun is shiny i like the sun"",""I have been exposed to sun"")
vect = TfidfVectorizer(stop_words=""english"",lowercase=False)
tfidf_matrix = vect.fit_transform(sentences).toarray()
</code></pre>

<p>I want to be able to calculate the number of times the term ""sun"" occurs in the first sentence (which is 2) using only tfidf_matrix[0] and probably vect.idf_ .
I know there are infinite ways to get term frequency and words count but I have a special case where I only have a tf<em>idf matrix.
I already tried to divide the tf</em>idf value of the word ""sun"" in the first sentence by its idf value to get tf. Then I multiplied tf by the total number of words in the sentence to get the words count. Unfortunately, I get wrong values.</p>
",Vectorization & Embeddings,get word count tf idf value sklearn want get count word given sentence using tf idf matrix set sentence use tfidfvectorizer sklearn feature extraction text example want able calculate number time term sun occurs first sentence using tfidf matrix probably vect idf know infinite way get term frequency word count special case tfidf matrix already tried divide tfidf value word sun first sentence idf value get tf multiplied tf total number word sentence get word count unfortunately get wrong value
convolutional neural networks for sentiment analysis,"<p>I was trying to modify YoonKim's code for <a href=""https://github.com/yoonkim/CNN_sentence"" rel=""nofollow"">sentiment analysis using CNN's.</a> He applies three filters of <code>heights=[3,4,5]</code> and <code>width=300</code> on </p>

<pre><code>input=(batch_size, 1, len(sentence_vector), len(wordVector))
</code></pre>

<p>I'm stuck after the first <code>Conv,Pool</code> computation. Consider</p>

<pre><code>input=(batch_size, 1, 64, 300)
</code></pre>

<p>64 is the length of every sentence vector and 300 is the word embedding size.</p>

<pre><code>map=(20, 1, 3, 300)
</code></pre>

<p>In his implementation, he first applies a kernel of height=3 and width=300. Hence the output would be</p>

<pre><code>convolution_output=(batch_size, 20, 62, 1)
</code></pre>

<p>After which he downsamples using <code>poolsize=(62, 1)</code>. The output after MaxPooling becomes</p>

<pre><code>maxpool_output=(batch_size, 20, 1, 1)
</code></pre>

<p>This is where i'm stuck.
In the paper he applies 3 filters of <code>heights[3,4,5]</code> and <code>width=300</code>. But after applying the first filter, there is no input left for convolution. How(<strong>and on what</strong>) do I apply the second kernel?.</p>

<p>Any help or suggestions would be great. The git page contains a link to the paper.         </p>
",Vectorization & Embeddings,convolutional neural network sentiment analysis wa trying modify yoonkim code sentiment analysis using cnn applies three filter stuck first computation consider length every sentence vector word embedding size implementation first applies kernel height width hence output would downsamples using output maxpooling becomes stuck paper applies filter applying first filter input left convolution apply second kernel help suggestion would great git page contains link paper
Normalize ranking score with weights,"<p>I am working on a document search problem where given a set of documents and a search query I want to find the document closest to the query. The model that I am using is based on TfidfVectorizer in scikit. I created 4 different tf_idf vectors for all the documents by using 4 different types of tokenizers. Each tokenizer splits the string into n-grams where n is in the range 1 ... 4 . </p>

<p>For example:</p>

<pre><code>doc_1 = ""Singularity is still a confusing phenomenon in physics""
doc_2 = ""Quantum theory still wins over String theory""
</code></pre>

<p>So model_1 will use a 1-gram tokenizer, model_2 will use a 2-gram tokenizer.</p>

<p>Next for a given search query, I calculate the cosine similarity between the search term and all the other documents using these 4 models.</p>

<p>For example, search query: Singularity in quantum physics.
The search query is broken down into n-grams and tf_idf values are computed from the corresponding n-gram model.</p>

<p>There fore for each query-document pair I have 4 values of similarity based on the n-gram model used.
For example:</p>

<pre><code>1-gram similarity = 0.4370303325246957
2-gram similarity = 0.36617374546988996
3-gram similarity = 0.29519246156322099
4-gram similarity = 0.2902998188509896
</code></pre>

<p>All of these similarity scores are normalized on a scale of 0 to 1. Now I want to calculate an aggregated normalized score such that for any query-document pair, the higher n-gram similarity gets a really high weight. Basically, higher the ngram similarity, higher it has the impact on the overall score.</p>

<p>Can someone please suggest a solution?</p>
",Vectorization & Embeddings,normalize ranking score weight working document search problem given set document search query want find document closest query model using based tfidfvectorizer scikit created different tf idf vector document using different type tokenizers tokenizer split string n gram n range example model use gram tokenizer model use gram tokenizer next given search query calculate cosine similarity search term document using model example search query singularity physic search query broken n gram tf idf value computed corresponding n gram model fore query document pair value similarity based n gram model used example similarity score normalized scale want calculate aggregated normalized score query document pair higher n gram similarity get really high weight basically higher ngram similarity higher ha impact overall score someone please suggest solution
Word2Vec Data Setup,"<p>In the Word2Vec Skip-gram setup that follows, what is the data setup for the output layer? Is it a matrix that is zero everywhere but with a single ""1"" in each of the C rows - that represents the words in the C context? </p>

<p><img src=""https://i.sstatic.net/igSuE.png"" alt=""enter image description here""></p>

<p><strong>Add to describe Data Setup Question:</strong></p>

<p>Meaning what the dataset would look like that was presented to the NN? Lets consider this to be ""what does a single training example look like""?. I assume the <em>total</em> input is a matrix, where each row is a word in the vocabulary (and there is a column for each word as well and each cell is zero except where for the specific word - one hot encoded)? Thus, a single training example is 1xV as shown below (all zeros except for the specific word, whose value is a 1). This aligns with the picture above in that the input is V-dim. I expected that the total input matrix would have duplicated rows however - where the same one-hot encoded vector would be repeated for each time the word was found in the corpus (as the output or target variable would be different).</p>

<p>The Output (target) is more confusing to me. I expected it would exactly mirror the input -- a single training example has a ""multi""-hot encoded vector that is zero except is a ""1"" in C of the cells, denoting that a particular word was in the context of the input word (C = 5 if we are looking, for example, 2 words behind and 3 words ahead of the given input word instance). The picture doesn't seem to agree with this though. I dont understand what appears like C different output layers that share the same W' weight matrix?   </p>
",Vectorization & Embeddings,word vec data setup word vec skip gram setup follows data setup output layer matrix zero everywhere single c row represents word c context add describe data setup question meaning dataset would look like wa presented nn let consider doe single training example look like assume total input matrix row word vocabulary column word well cell zero except specific word one hot encoded thus single training example xv shown zero except specific word whose value aligns picture input v dim expected total input matrix would duplicated row however one hot encoded vector would repeated time word wa found corpus output target variable would different output target confusing expected would exactly mirror input single training example ha multi hot encoded vector zero except c cell denoting particular word wa context input word c looking example word behind word ahead given input word instance picture seem agree though dont understand appears like c different output layer share w weight matrix
How do I calculate the shortest path (geodesic) distance between two adjectives in WordNet using Python NLTK?,"<p>Computing the semantic similarity between two synsets in WordNet can be easily done with several built-in similarity measures, such as:</p>

<pre><code>synset1.path_similarity(synset2)
</code></pre>

<p><code>synset1.lch_similarity(synset2)</code>, Leacock-Chodorow Similarity</p>

<p><code>synset1.wup_similarity(synset2)</code>, Wu-Palmer Similarity</p>

<p><a href=""https://stackoverflow.com/questions/22031968/how-to-find-distance-between-two-synset-using-python-nltk-in-wordnet-hierarchy"">(as seen here)</a> </p>

<p>However, all of these exploit WordNet's taxonomic relations, which are relations for nouns and verbs. Adjectives and adverbs are related via synonymy, antonymy and pertainyms. How can one measure the distance (number of hops) between two adjectives? </p>

<p>I tried <code>path_similarity()</code>, but as expected, it returns <code>'None'</code>:</p>

<pre><code>from nltk.corpus import wordnet as wn
x = wn.synset('good.a.01')
y = wn.synset('bad.a.01')


print(wn.path_similarity(x,y))
</code></pre>

<p>If there is any way to compute the distance between one adjective and another, pointing it out would be greatly appreciated.</p>
",Vectorization & Embeddings,calculate shortest path geodesic distance two adjective wordnet using python nltk computing semantic similarity two synset wordnet easily done several built similarity measure leacock chodorow similarity wu palmer similarity href seen however exploit wordnet taxonomic relation relation noun verb adjective adverb related via synonymy antonymy pertainym one measure distance number hop two adjective tried expected return way compute distance one adjective another pointing would greatly appreciated
LDA Results Errors,"<p>So, I am relatively new using Gensim and LDA in general. The problem right now is that when I run LDA on my corpus, the topics' tokens' weights are all 0:</p>

<p>2015-06-15 12:21:12,439 : INFO : topic diff=0.082235, rho=0.250000</p>

<p>2015-06-15 12:21:12,454 : <strong>INFO : topic #0</strong> (0.100): 0.000*sundayes + 0.000*nowe + 0.000*easter + 0.000*iniunctions + 0.000*eyther + 0.000*christ, + 0.000*authoritie + 0.000*sir + 0.000*saint + 0.000*thinge</p>

<p>2015-06-15 12:21:12,468 : <strong>INFO : topic #1</strong> (0.100): 0.000*eu'n + 0.000*ioseph + 0.000*pharohs + 0.000*pharoh + 0.000*iosephs + 0.000*lo! + 0.000*egypts + 0.000*iacob + 0.000*ioseph, + 0.000*beniamin</p>

<p>2015-06-15 12:21:12,482 : <strong>INFO : topic #2</strong> (0.100): 0.000*agreeable + 0.000*creede, + 0.000*fourme + 0.000*conteined + 0.000*apostolike, + 0.000*vicars, + 0.000*sacrament + 0.000*contrarywise + 0.000*parsons, + 0.000*propitiatorie</p>

<p>2015-06-15 12:21:12,495 : <strong>INFO : topic #3</strong> (0.100): 0.000*yf + 0.000*suche + 0.000*lyke + 0.000*shoulde + 0.000*moste + 0.000*youre + 0.000*oure + 0.000*lyfe, + 0.000*anye + 0.000*thinges</p>

<p>2015-06-15 12:21:12,507 : <strong>INFO : topic #4</strong> (0.100): 0.000*heau'nly + 0.000*eu'n + 0.000*heau'n + 0.000*sweet + 0.000*peace + 0.000*eu'ry + 0.000*constance + 0.000*constant + 0.000*doth + 0.000*oh</p>

<p>2015-06-15 12:21:12,521 : <strong>INFO : topic #5</strong> (0.100): 0.000*eu'n + 0.000*ioseph + 0.000*pharohs + 0.000*pharoh + 0.000*vel + 0.000*iosephs + 0.000*heau'n + 0.000*lo! + 0.000*ac + 0.000*seu'n</p>

<p>2015-06-15 12:21:12,534 : <strong>INFO : topic #6</strong> (0.100): 0.000*thou + 0.000*would + 0.000*love + 0.000*king + 0.000*sir, + 0.000*doe + 0.000*thee + 0.000*1. + 0.000*never + 0.000*2.</p>

<p>2015-06-15 12:21:12,546 : <strong>INFO : topic #7</strong> (0.100): 0.000*quae + 0.000*vt + 0.000*qui + 0.000*ij + 0.000*non + 0.000*ad + 0.000*si + 0.000*vel + 0.000*atque + 0.000*cum</p>

<p>2015-06-15 12:21:12,558 : <strong>INFO : topic #8</strong> (0.100): 0.000*suspected + 0.000*supersticious + 0.000*squire + 0.000*parsons + 0.000*ordinarie + 0.000*vsed, + 0.000*english, + 0.000*fortnight + 0.000*squire, + 0.000*offenders</p>

<p>2015-06-15 12:21:12,572 : <strong>INFO : topic #9</strong> (0.100): 0.001*/ + 0.001*ile + 0.000*y^e + 0.000*che + 0.000*much + 0.000*tis + 0.000*could + 0.000*oh + 0.000*neuer + 0.000*heart</p>

<p>I have 307 documents and I'm running my LDA with the following code after removing the stopwords:</p>

<p>texts = [[token for token in text if frequency[token] > 3 ] for text in texts]</p>

<p>dictionary = corpora.Dictionary(texts)</p>

<p>corpus = [dictionary.doc2bow(text) for text in texts]</p>

<p>tfidf = models.TfidfModel(corpus)
tfidf_corpus = tfidf[corpus]</p>

<p>lda = models.LdaModel(tfidf_corpus, id2word = dictionary, update_every=1, chunksize= 20, num_topics = 10, passes = 1)</p>

<p>lda[tfidf_corpus]</p>

<p>lda.print_topics(10)</p>

<p>I am not sure what is wrong but everytime I run this, the token weights are 0. What might be causing this and how could I correct this? </p>
",Vectorization & Embeddings,lda result error relatively new using gensim lda general problem right run lda corpus topic token weight info topic diff rho info topic sundayes nowe easter iniunctions eyther christ authoritie sir saint thinge info topic eu n ioseph pharohs pharoh iosephs lo egypt iacob ioseph beniamin info topic agreeable creede fourme conteined apostolike vicar sacrament contrarywise parson propitiatorie info topic yf suche lyke shoulde moste youre oure lyfe anye thinges info topic heau nly eu n heau n sweet peace eu ry constance constant doth oh info topic eu n ioseph pharohs pharoh vel iosephs heau n lo ac seu n info topic thou would love king sir doe thee never info topic quae vt qui ij non ad si vel atque cum info topic supersticious squire parson ordinarie vsed english fortnight squire offender info topic ile e che much ti could oh neuer heart document running lda following code removing stopwords text token token text frequency token text text dictionary corpus dictionary text corpus dictionary doc bow text text text tfidf model tfidfmodel corpus tfidf corpus tfidf corpus lda model ldamodel tfidf corpus id word dictionary update every chunksize num topic pass lda tfidf corpus lda print topic sure wrong everytime run token weight might causing could correct
Are word-vector orientations universal?,"<p>I have recently been experimenting with Word2Vec and I noticed whilst trawling through forums that a lot of other people are also creating their own vectors from their own databases.</p>

<p>This has made me curious as to how vectors look across databases and whether vectors take a universal orientation?</p>

<p>I understand that the vectors are created as a result of the context they are found in the corpus. So in that sense perhaps you wouldn't expect words to have the same orientation across databases. However, if the language of the documents are constant, then the contexts should be at least somewhat similar across different databases (excluding ambiguous words like bank (for money) and (river) bank). And if they are somewhat similar, it seems plausible that as we look at more commonly occurring words their direction may converge?</p>
",Vectorization & Embeddings,word vector orientation universal recently experimenting word vec noticed whilst trawling forum lot people also creating vector database ha made curious vector look across database whether vector take universal orientation understand vector created result context found corpus sense perhaps expect word orientation across database however language document constant context least somewhat similar across different database excluding ambiguous word like bank money river bank somewhat similar seems plausible look commonly occurring word direction may converge
Biasing word2vec towards special corpus,"<p>I am new to stackoverflow. Please forgive my bad English.</p>

<p>I am using <strong>word2vec</strong> for a school project. I want to work with a domain specific corpus (like Physics Textbook) for creating the word vectors using <strong>Word2Vec</strong>. This standalone does not provide good results due to lesser size of the corpus. This especially hurts as we want to evaluate on words that may very well be outside the vocabulary of the text book.</p>

<p>We want the textbook to encode the domain specific relationships and semantic ""nearness"". ""Quantum"" and ""Heisenberg"" are especially close in this textbook for eg. which may not hold true for background corpus. To handle the generic words (like ""any"") we need the basic background model(like the one provided by Google on word2vec site).  </p>

<p>Is there any way that we can supplant to the background model using our newer corpus. Just training on the corpus etc. doesnot work well.</p>

<p>Are there any attempts to combine vector representations from two corpus- general and specific. I could not find any in my searches. </p>
",Vectorization & Embeddings,biasing word vec towards special corpus new stackoverflow please forgive bad english using word vec school project want work domain specific corpus like physic textbook creating word vector using word vec standalone doe provide good result due lesser size corpus especially hurt want evaluate word may well outside vocabulary text book want textbook encode domain specific relationship semantic nearness heisenberg especially close textbook eg may hold true background corpus handle generic word like need basic background model like one provided google word vec site way supplant background model using newer corpus training corpus etc doesnot work well attempt combine vector representation two corpus general specific could find search
Natural Language Processing to Machine Learning,"<p>I'm new in Natural Language Processing but I've studied lot of techniques in Machine Learning, specially in Matlab. My big question is that when I work with texts (in Matlab), we have three problems for using ML algorithms:</p>

<ol>
<li>The data space of the words is not numerical, for that, I can't use the algorithms directly. Do I need some transformation? I read about Kernel String, word embedding but I don't know how they resolve this problem.</li>
<li>The number of features is variable for each word, I mean, when we obtain the vectors, we could have the next and previous words + POS tagging of the actual word and not all of them have. Can we set this features to 0 for computing this in a matrix?. In general, I have problems to resolve the dependencies like grammar parsing in a vector.</li>
<li>Labels in a text are separated by sequences (sentence level). How we know in a vector the separation between sentences?</li>
</ol>
",Vectorization & Embeddings,natural language processing machine learning new natural language processing studied lot technique machine learning specially matlab big question work text matlab three problem using ml algorithm data space word numerical use algorithm directly need transformation read kernel string word embedding know resolve problem number feature variable word mean obtain vector could next previous word po tagging actual word set feature computing matrix general problem resolve dependency like grammar parsing vector label text separated sequence sentence level know vector separation sentence
NLP - Word Representations,"<p>I am working on a Word representation algorithm, similar to <a href=""https://code.google.com/p/word2vec/"" rel=""nofollow"">Word2Vec</a> and <a href=""http://nlp.stanford.edu/projects/glove/"" rel=""nofollow"">GloVe</a>.I have been asked to make it more dynamic, such that new words could be added to the vocabulary,and new documents could be submitted to the program even after the representations (vectors) have been created.</p>

<p>The problem is, how do I know if my representation work? How do I know if it actually captures the meaning of each word? How do I compare my representation with other existing vector space models?</p>

<p>As of now, I am doing the following tests to check the quality of my word vectors:</p>

<p>Distance test:</p>

<p>Does the cosine distance between vectors reflect the semantic distance between words?</p>

<p>Analogy test:</p>

<p>Can the representation be used to solve problems like ""King is to queen what man is to ________ "", (the answer should be woman)</p>

<p>Picking the odd one out:</p>

<p>Can the vectors be used to pick the odd word in a given list of words. If the input is {""cat"",""dog"",""phone""}, the output should be ""phone""?</p>

<p>What are the other tests that I should do to check the quality of the vectors? What other tasks are word vectors expected to be capable of doing? Is there a benchmark for vector space models? </p>
",Vectorization & Embeddings,nlp word representation working word representation algorithm similar word vec glove asked make dynamic new word could added vocabulary new document could submitted program even representation vector created problem know representation work know actually capture meaning word compare representation existing vector space model following test check quality word vector distance test doe cosine distance vector reflect semantic distance word analogy test representation used solve problem like king queen man answer woman picking odd one vector used pick odd word given list word input cat dog phone output phone test check quality vector task word vector expected capable benchmark vector space model
proper algo to convert strings to integers while keeping semantic value,"<p>I've been trying to convert natural language strings into integers for use in a long short-term neural-network. I tried converting to binary, using a bag-of-words, and an associative-array with each letter corresponding to a prime-number.</p>

<p>I looked into Google's word2vec just to convert the words into word-vectors, but I'm looking for something I can implement in the browser. This is why I am looking for an algorithm that I can write in js.</p>

<p>I know there's node.js implementations of word2vec, but they just run word2vec in the command-line.</p>

<p>This is different than this question, <a href=""https://stackoverflow.com/q/29880071/3743069"">here</a>, that I asked earlier because I am looking for something that retains semantic meaning. I thought about using word similarity techniques, but didn't know how to implement resnik similarity in js.</p>

<p>I greatly appreciate any help or direction in converting nl sentences, or just the topic of them, to word-vectors or an array of ints.</p>
",Vectorization & Embeddings,proper algo convert string integer keeping semantic value trying convert natural language string integer use long short term neural network tried converting binary using bag word associative array letter corresponding prime number looked google word vec convert word word vector looking something implement browser looking algorithm write j know node j implementation word vec run word vec command line different question href asked earlier looking something retains semantic meaning thought using word similarity technique know implement resnik similarity j p greatly appreciate help direction converting nl sentence topic word vector array ints
Using word2vec to calculate similarity between users,"<p>I recently came to know about this tool called word2vec. For my current work, I need to find out users that are similar to a given user. A single user has entities associated with it like age, qualifications, insitute/organisaions, languages known and list of certains tags. If we consider a each of these entities/columns together as random chunk of words for a user, can we correspondingly calculate the vector value for that user and use these values to deduce similarities between users? Would a wiki training vector help us get meaningful results?Any other way to do it?</p>
",Vectorization & Embeddings,using word vec calculate similarity user recently came know tool called word vec current work need find user similar given user single user ha entity associated like age qualification insitute organisaions language known list certains tag consider entity column together random chunk word user correspondingly calculate vector value user use value deduce similarity user would wiki training vector help u get meaningful result way
TfidfVectorizer does not use the whole set of words in all documents?,"<p>I am trying to build a TFIDF model with TfidfVectorizer. The feature name list namely the number of column of sparse matrix is shorter than the length of word set of documents even though I set min_df as 1. What happened? </p>
",Vectorization & Embeddings,tfidfvectorizer doe use whole set word document trying build tfidf model tfidfvectorizer feature name list namely number column sparse matrix shorter length word set document even though set min df happened
Vector Space Model Introduction,"<p>What are different types of <a href=""http://en.wikipedia.org/wiki/Vector_space_model"" rel=""nofollow"">VSM</a> (vector space model)? </p>

<p>One which I know (as per wiki) is <code>tf-idf</code> (cosine similarity is used in this method, but its not a separate method). Which are other ways?</p>

<p>Also what are different dimensions of a word in a document (except frequency) being talked about in wiki?</p>

<p>Is there any hierarchy for VSMs?</p>

<p>P.S. Please correct me if I am wrong anywhere...</p>
",Vectorization & Embeddings,vector space model introduction different type vsm vector space model one know per wiki cosine similarity used method separate method way also different dimension word document except frequency talked wiki hierarchy vsms p please correct wrong anywhere
Comparing documents - document similarity,"<p>I am currently conducting a java project in NLP/IR, and are fairly new to this. 
The project consists of a collection with around 1000 documents, where each document has about 100 words, structured as bag of words with term-frequency. I want to find similar documents based on a document(from the collection).</p>

<p>Using TF-IDF, calculating tf-idf for the query(a given document) and every other document in the collection, then comparing these values as a vector with cosine similarity. Could this give some insight in their similarity? Or would it not be reasonable, because of the big query(document)?
Is there any other similarity measures that could work better?</p>

<p>Thanks for the help</p>
",Vectorization & Embeddings,comparing document document similarity currently conducting java project nlp ir fairly new project consists collection around document document ha word structured bag word term frequency want find similar document based document collection using tf idf calculating tf idf query given document every document collection comparing value vector cosine similarity could give insight similarity would reasonable big query document similarity measure could work better thanks help
Ruby Meaningful Words -- Remove Stopwords,"<p>I have an array of speech files, and I need to compare to an array of stopwords to remove the stopwords and leave the remaining meaningful words.</p>

<p>So far I have something like this:</p>

<pre><code>stopwords = File.readlines('PATH TO TXT FILE')
speeches = []

Dir.glob('PATH TO ALL SPEECHES').each do |speech|
    #code to read each speech and store into an array
    f = File.readlines(speech)
    speeches &lt;&lt; f
end

lincolnSpeech = speeches[0]

def process_file(file_name)
    all_words = file_name.scan(/\w+/)
    meaningful_words = all_words.select { |word| !stopwords.include?(word) }
    return meaningful_words
end
</code></pre>

<p>I'm embedding the result of this function into my HTML, like so:</p>

<pre><code>&lt;ul&gt;
      &lt;li&gt;&lt;pre style=""white-space: pre-wrap;word-wrap: break-word""&gt;#{process_file(lincolnSpeech)}&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
</code></pre>

<p>But this breaks the page and causes my HTML to disappear entirely. I've narrowed the problem down to the line in the function:</p>

<pre><code>meaningful_words = all_words.select { |word| !stopwords.include?(word) }
</code></pre>

<p>And this line is the culprit. I'm not sure why it breaks my code. Maybe parts of this are deprecated? Can anyone offer some ideas about why this doesn't work, and perhaps alternate ways to achieve the effect I'm going for?</p>
",Vectorization & Embeddings,ruby meaningful word remove stopwords array speech file need compare array stopwords remove stopwords leave remaining meaningful word far something like embedding result function html like break page cause html disappear entirely narrowed problem line function line culprit sure break code maybe part deprecated anyone offer idea work perhaps alternate way achieve effect going
Anyone knows about text-based emotion detection systems that offer a demo?,"<p>I recently finished work on my text-based emotion detection engine and I am looking for other existing working systems to compare with mine in order to know what should be improved and also report comparisons in an upcoming paper. </p>

<p>I have come across many companies claiming to do emotion detection from text but only this one offers a demo that I can use to compare with my system: <a href=""http://www.o2mc.io/portfolio-posts/text-analysis-restful-api-language-polarity-and-emotion/"" rel=""nofollow"">http://www.o2mc.io/portfolio-posts/text-analysis-restful-api-language-polarity-and-emotion/</a> (scroll all the way down to see the ""try it yourself"" section). </p>

<p>Please notice that I am not looking for polarity classification, which is the simpler task of saying if a text is positive or negative. What I am looking for is for emotions (sadness, anger, joy, etc...). Does anyone here know about any company/university/person offering a demo to such system?
As a reference, here is the link to my own system's demo:
<a href=""http://demo.soulhackerslabs.com/emotion/"" rel=""nofollow"">http://demo.soulhackerslabs.com/emotion/</a></p>

<p>Your help is very much appreciated.</p>
",Vectorization & Embeddings,anyone know text based emotion detection system offer demo recently finished work text based emotion detection engine looking existing working system compare mine order know improved also report comparison upcoming paper come across many company claiming emotion detection text one offer demo use compare system scroll way see try section please notice looking polarity classification simpler task saying text positive negative looking emotion sadness anger joy etc doe anyone know company university person offering demo system reference link system demo help much appreciated
Correct order output in K Means and document clustering,"<p>I am doing a single document clustering with K Means, I am now working on preparing the data to be clustered and represent N sentences in their vector representations.</p>

<p>However, if I understand correctly, KMeans algorithm is set to create k clusters based on the euclidean distance to k center points. Regardless of the sentences order.</p>

<p>My problem is that I want to keep the order of the sentences and consider them in the clustering task.</p>

<p>Let say <code>S = {1...n}</code> a set of n vectors representing sentences, <code>S_1 = sentence 1 , S_2 = sentence 2 .. etc</code>.</p>

<p>I want that the clusters will be <code>K_1 = S[1..i], K_2 = S[i..j] etc..</code></p>

<p>I thought maybe transform this into 1D and sum the index of each sentence to the transformed value. But not sure if it will help. And maybe there's a smarter way.</p>
",Vectorization & Embeddings,correct order output k mean document clustering single document clustering k mean working preparing data clustered represent n sentence vector representation however understand correctly kmeans algorithm set create k cluster based euclidean distance k center point regardless sentence order problem want keep order sentence consider clustering task let say set n vector representing sentence want cluster thought maybe transform sum index sentence transformed value sure help maybe smarter way
How to select hyper parameters for SVC estimator in scikit learn?,"<p>I'm classifiying some texts with SVC and I would like to run a grid search so I followed the example provided at the <a href=""http://scikit-learn.org/stable/auto_examples/grid_search_text_feature_extraction.html#example-grid-search-text-feature-extraction-py"" rel=""nofollow"">documentation</a>. In the example they are using SGDClassifier with the following parameters:</p>

<pre><code>parameters = {
    'vect__max_df': (0.5, 0.75, 1.0),
    #'vect__max_features': (None, 5000, 10000, 50000),
    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams
    #'tfidf__use_idf': (True, False),
    #'tfidf__norm': ('l1', 'l2'),
    'clf__alpha': (0.00001, 0.000001),
    'clf__penalty': ('l2', 'elasticnet'),
    #'clf__n_iter': (10, 50, 80),
}
</code></pre>

<p>My issue is, what kind of parameters should I try for SVC classifier, tfidf, hashing vectorizer and CountVectorizer?. How should I select this parameters if this is a multi class classification problem?.</p>
",Vectorization & Embeddings,select hyper parameter svc estimator scikit learn classifiying text svc would like run grid search followed example provided documentation example using sgdclassifier following parameter issue kind parameter try svc classifier tfidf hashing vectorizer countvectorizer select parameter multi class classification problem
Compare many text files that contain duplicate &quot;stubs&quot; from the previous and next file and remove duplicate text automatically,"<p>I have a large number of text files (1000+) each containing an article from an academic journal. Unfortunately each article's file also contains a ""stub"" from the end of the previous article (at the beginning) and from the beginning of the next article (at the end). </p>

<p>I need to remove these stubs in preparation for running a frequency analysis on the articles because the stubs constitute duplicate data.</p>

<p>There is no simple field that marks the beginning and end of each article in all cases. However, the duplicate text does seem to formatted the same and on the same line in both cases.</p>

<p>A script that compared each file to the next file and then removed 1 copy of the duplicate text would be perfect. This seems like it would be a pretty common issue when programming so I am surprised that I haven't been able to find anything that does this.</p>

<p>The file names sort in order, so a script that compares each file to the next sequentially should work. E.G.</p>

<pre>
bul_9_5_181.txt
bul_9_5_186.txt
</pre>

<p>are two articles, one starting on page 181 and the other on page 186. Both of these articles are included bellow. </p>

<p>There is two volumes of test data located at [<a href=""http://drop.io/fdsayre][1]"" rel=""nofollow noreferrer"">http://drop.io/fdsayre][1]</a></p>

<p>Note: I am an academic doing content analysis of old journal articles for a project in the history of psychology. I am no programmer, but I do have 10+ years experience with linux and can usually figure things out as I go. </p>

<p>Thanks for your help</p>

<p><b>FILENAME: bul_9_5_181.txt</b></p>

<p>
SYN&amp;STHESIA</p>

<p>ISI</p>

<p>the majority of Portugese words signifying black objects or ideas relating to black. This association is, admittedly, no true synsesthesia, but the author believes that it is only a matter of degree between these logical and spontaneous associations and genuine cases of colored audition.
REFERENCES</p>

<p>DOWNEY, JUNE E. A Case of Colored Gustation. Amer. J. of Psycho!., 1911, 22, S28-539MEDEIROS-E-ALBUQUERQUE. Sur un phenomene de synopsie presente par des millions de sujets. / . de psychol. norm, et path., 1911, 8, 147-151. MYERS, C. S. A Case of Synassthesia. Brit. J. of Psychol., 1911, 4, 228-238.</p>

<p>AFFECTIVE PHENOMENA — EXPERIMENTAL
BY PROFESSOR JOHN F. .SHEPARD
University of Michigan</p>

<p>Three articles have appeared from the Leipzig laboratory during the year. Drozynski (2) objects to the use of gustatory and olfactory stimuli in the study of organic reactions with feelings, because of the disturbance of breathing that may be involved. He uses rhythmical auditory stimuli, and finds that when given at different rates and in various groupings, they are accompanied by characteristic feelings in each subject. He records the chest breathing, and curves from a sphygmograph and a water plethysmograph. Each experiment began with a normal record, then the stimulus was given, and this was followed by a contrast stimulus; lastly, another normal was taken. The length and depth of breathing were measured (no time line was recorded), and the relation of length of inspiration to length of expiration was determined. The length and height of the pulsebeats were also measured. Tabular summaries are given of the number of times the author finds each quantity to have been increased or decreased during a reaction period with each type of feeling. The feeling state accompanying a given rhythm is always complex, but the result is referred to that dimension which seemed to be dominant. Only a few disconnected extracts from normal and reaction periods are reproduced from the records. The author states that excitement gives increase in the rate and depth of breathing, in the inspiration-expiration ratio, and in the rate and size of pulse. There are undulations in the arm volume. In so far as the effect is quieting, it causes decrease in rate and depth of</p>

<p>182</p>

<p>JOHN F. SHEPARD</p>

<p>breathing, in the inspiration-expiration ratio, and in the pulse rate and size. The arm volume shows a tendency to rise with respiratory waves. Agreeableness shows</p>
",Vectorization & Embeddings,compare many text file contain duplicate stub previous next file remove duplicate text automatically large number text file containing article academic journal unfortunately article file also contains stub end previous article beginning beginning next article end need remove stub preparation running frequency analysis article stub constitute duplicate data simple field mark beginning end article case however duplicate text doe seem formatted line case script compared file next file removed copy duplicate text would perfect seems like would pretty common issue programming surprised able find anything doe file name sort order script compare file next sequentially work e g bul txt bul txt two article one starting page page article included bellow two volume test data located note academic content analysis old journal article project history psychology programmer year experience linux usually figure thing go thanks help filename bul txt syn sthesia isi majority portugese word signifying black object idea relating black association admittedly true synsesthesia author belief matter degree logical spontaneous association genuine case colored audition reference downey june e case colored gustation amer j psycho medeiros e albuquerque sur un phenomene de synopsie presente par de million de sujets de psychol norm et path myers c case synassthesia brit j psychol affective experimental professor john f shepard university michigan three article appeared leipzig laboratory year drozynski object use gustatory olfactory stimulus study organic reaction feeling disturbance breathing may involved us rhythmical auditory stimulus find given different rate various grouping accompanied characteristic feeling subject record chest breathing curve sphygmograph water plethysmograph experiment began normal record stimulus wa given wa followed contrast stimulus lastly another normal wa taken length depth breathing measured time line wa recorded relation length inspiration length expiration wa determined length height pulsebeats also measured tabular summary given number time author find quantity increased decreased reaction period type feeling feeling state given rhythm always complex result referred dimension seemed dominant disconnected extract normal reaction period reproduced record author state excitement give increase rate depth breathing inspiration expiration ratio rate size pulse undulation arm volume far effect quieting cause decrease rate depth john f shepard breathing inspiration expiration ratio pulse rate size arm volume show tendency rise respiratory wave agreeableness show
Text clustering within a log file,"<p>I am working on a problem of finding similar content in a log file. Let's say I have a log file which looks like this:</p>

<pre><code> show version
 Operating System (OS) Software

 Software
 BIOS:      version 1.0.10
 loader:    version N/A
 kickstart: version 4.2(7b)
 system:    version 4.2(7b)
 BIOS compile time:       01/08/09
 kickstart image file is: bootflash:/m9500-sf2ek9-kickstart-mz.4.2.7b.bin
 kickstart compile time:  8/16/2010 13:00:00 [09/29/2010 23:10:48]
 system image file is:    bootflash:/m9500-sf2ek9-mz.4.2.7b.bin
 system compile time:     8/16/2010 13:00:00 [09/30/2010 00:46:36]`

 Hardware
 xxxx MDS 9509 (9 Slot) Chassis (""xxxxxxx/xxxxx-2"")
 xxxxxxx, xxxx with 1033100 kB of memory.
 Processor Board ID xxxx

 Device name: xxx-xxx-1 
 bootflash:    1000440 kB 
 slot0:              0 kB (expansion flash)
</code></pre>

<p>For a human eye, it can easily be understood that ""Software"" and the data below is a section and ""Hardware"" and the data below is another section. Is there a way I can model using machine learning or some other technique to cluster similar sections based on a pattern? Also, I have shown 2 similar kinds of pattern but the patterns between sections might vary and hence should identify as different section. I have tried to find similarity using cosine similarity but it doesn't help much because the words aren't similar but the pattern is.</p>
",Vectorization & Embeddings,text clustering within log file working problem finding similar content log file let say log file look like human eye easily understood software data section hardware data another section way model using machine learning technique cluster similar section based pattern also shown similar kind pattern pattern section might vary hence identify different section tried find similarity using cosine similarity help much word similar pattern
What means this sparse matrix in scipy?,"<p>I have a NLP task and I'm using scikit-learn. Reading the <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow"">tutorials</a> i found have to vectorize text and how to use this vectorization models to feed a classification algorithm. Assume that i have some text and i would like to vectorize it as follows:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer

corpus =['''Computer science is the scientific and
practical approach to computation and its applications.'''
#this is another opinion
'''It is the systematic study of the feasibility, structure,
expression, and mechanization of the methodical
procedures that underlie the acquisition,
representation, processing, storage, communication of,
and access to information, whether such information is encoded
as bits in a computer memory or transcribed in genes and
protein structures in a biological cell.'''
         #anotherone
'''A computer scientist specializes in the theory of
computation and the design of computational systems''']

vectorizer = CountVectorizer(analyzer='word')

X = vectorizer.fit_transform(corpus)

print X
</code></pre>

<p>The problem is that i dont understand the meaning of the output, i dont see any relation with the text and the matrix that is returned by the vectorizer:</p>

<pre><code>  (0, 12)   3
  (0, 33)   1
  (0, 20)   3
  (0, 45)   7
  (0, 34)   1
  (0, 2)    6
  (0, 28)   1
  (0, 4)    1
  (0, 47)   2
  (0, 10)   2
  (0, 22)   1
  (0, 3)    1
  (0, 21)   1
  (0, 42)   1
  (0, 40)   1
  (0, 26)   5
  (0, 16)   1
  (0, 38)   1
  (0, 15)   1
  (0, 23)   1
  (0, 25)   1
  (0, 29)   1
  (0, 44)   1
  (0, 49)   1
  (0, 1)    1
  : :
  (0, 30)   1
  (0, 37)   1
  (0, 9)    1
  (0, 0)    1
  (0, 19)   2
  (0, 50)   1
  (0, 41)   1
  (0, 14)   1
  (0, 5)    1
  (0, 7)    1
  (0, 18)   4
  (0, 24)   1
  (0, 27)   1
  (0, 48)   1
  (0, 17)   1
  (0, 31)   1
  (0, 39)   1
  (0, 6)    1
  (0, 8)    1
  (0, 35)   1
  (0, 36)   1
  (0, 46)   1
  (0, 13)   1
  (0, 11)   1
  (0, 43)   1
</code></pre>

<p>Also i dont understand what's happening with the output when i use the <code>toarray()</code> method:</p>

<pre><code>print X.toarray()
</code></pre>

<p>What exactly means the output and what relation has with the corpus?:</p>

<pre><code>[[1 1 6 1 1 1 1 1 1 1 2 1 3 1 1 1 1 1 4 2 3 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1
  1 1 1 1 1 1 1 1 7 1 2 1 1 1]]
</code></pre>
",Vectorization & Embeddings,mean sparse matrix scipy nlp task using scikit learn reading tutorial found vectorize text use vectorization model feed classification algorithm assume text would like vectorize follows problem dont understand meaning output dont see relation text matrix returned vectorizer also dont understand happening output use method exactly mean output relation ha corpus
How to find similarity of sentence?,"<p>How to find the semantic similarity between any two given sentences?</p>

<p>Eg:
what movies did ron howard direct?</p>

<p>movies directed by ron howard.</p>

<p>I know its a hard problem. But, would like to ask the views of experts.
I don't know how to use the Parts of Speech to achieve this.
<a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow"">http://nlp.stanford.edu:8080/parser/index.jsp</a></p>
",Vectorization & Embeddings,find similarity sentence find semantic similarity two given sentence eg movie ron howard direct movie directed ron howard know hard problem would like ask view expert know use part speech achieve
Fastest Count Vectorizer Implementation,"<p>I'm looking for an implementation of n-grams count vectorization that is more efficient than scikit-learn's <code>CountVectorizer</code>.  I've identified the <code>CountVectorizer.transform()</code> call as a huge bottleneck in a bit of software, and can dramatically increase model throughput if we're able to make this part of the pipeline more efficient.  Fit time is not important, we're only concerned with transform time.  The end output must be a <code>scipy.sparse</code> vector.  If anyone has any leads for potential alternatives it would be much appreciated.</p>
",Vectorization & Embeddings,fastest count vectorizer implementation looking implementation n gram count vectorization efficient scikit learn identified call huge bottleneck bit software dramatically increase model throughput able make part pipeline efficient fit time important concerned transform time end output must vector anyone ha lead potential alternative would much appreciated
How to Normalize similarity measures from Wordnet,"<p>I am trying to calculate semantic similarity between two words. I am using Wordnet-based similarity measures i.e Resnik measure(RES), Lin measure(LIN), Jiang and Conrath measure(JNC) and Banerjee and Pederson measure(BNP).</p>

<p>To do that, I am using nltk and Wordnet 3.0. Next, I want to combine the similarity values obtained from different measure. To do that i need to normalize the similarity values as some measure give values between 0 and 1, while others give values greater than 1.</p>

<p>So, my question is how do I normalize the similarity values obtained from different measures.</p>

<p><strong>Extra detail</strong> on what I am actually trying to do: I have a set of words. I calculate pairwise similarity between the words. and remove the words that are not strongly correlated with other words in the set.</p>
",Vectorization & Embeddings,normalize similarity measure wordnet trying calculate semantic similarity two word using wordnet based similarity measure e resnik measure lin measure lin jiang conrath measure jnc banerjee pederson measure bnp using nltk wordnet next want combine similarity value obtained different measure need normalize similarity value measure give value others give value greater question normalize similarity value obtained different measure extra detail actually trying set word calculate pairwise similarity word remove word strongly correlated word set
How to create a dictionary of dictionary with these functions?,"<p>I have a dictionary like this:</p>

<pre><code>dict = {in : [0.01, -0.07, 0.09, -0.02], and : [0.2, 0.3, 0.5, 0.6], to : [0.87, 0.98, 0.54, 0.4]}
</code></pre>

<p>I want to calculate the cosine similarity between each word for which I have a cosine similarity function that takes two vectors. First, it will take value for 'in' and 'and', then it should take value for 'in' and 'to' and so on.</p>

<p>I want it to store the result of this in another dictionary, where 'in' should be the key, and the values should be a dictionary of each computed cosine similarity value with that key. Like I want the output to be like this:</p>

<pre><code>{in : {and : 0.4321, to : 0.218}, and : {in : 0.1245, to : 0.9876}, to : { in : 0.8764, and : 0.123}}
</code></pre>

<p>Below is the code which is doing all of this:</p>

<pre><code>def cosine_similarity(vec1,vec2):
    sum11, sum12, sum22 = 0, 0, 0
    for i in range(len(vec1)):
        x = vec1[i]; y = vec2[i]
        sum11 += x*x
        sum22 += y*y
        sum12 += x*y
    return sum12/math.sqrt(sum11*sum22)

def resultInDict(result,name,value,keyC):
    new_dict={}
    new_dict[keyC]=value       
    if name in result:
        result[name] = new_dict
    else:
         result[name] = new_dict

def extract():
    result={}
    res={}
    with open('file.txt') as text:
        for line in text:
            record = line.split()
            key = record[0]
            values = [float(value) for value in record[1:]]
            res[key] = values
    for key,value in res.iteritems():
            temp = 0
            for keyC,valueC in res.iteritems():

                if keyC == key:
                    continue
                temp = cosine_similarity(value,valueC)
                resultInDict(result,key,temp,keyC)
    print result
</code></pre>

<p>But, it's giving the result like this:</p>

<pre><code>{'and': {'in': 0.12241083209661485}, 'to': {'in': -0.0654517869126785}, 'from': {'in': -0.5324142931780856}, 'in': {'from': -0.5324142931780856}}
</code></pre>

<p>I want it to be like this:</p>

<pre><code>{in : {and : 0.4321, to : 0.218}, and : {in : 0.1245, to : 0.9876}, to : { in : 0.8764, and : 0.123}}
</code></pre>

<p>I feel it is because in the resultInDict function I am defining a new dictionary new_dict to add key values for the inner dictionary, but each time the function resultInDict is called, it empties the new_dict on this line <code>new_dict={}</code>, and only adds the one key value pair.</p>

<p>How can I fix this??</p>
",Vectorization & Embeddings,create dictionary dictionary function dictionary like want calculate cosine similarity word cosine similarity function take two vector first take value take value want store result another dictionary key value dictionary computed cosine similarity value key like want output like code giving result like want like feel resultindict function defining new dictionary new dict add key value inner dictionary time function resultindict called empty new dict line add one key value pair fix
Finding semantic similarity and relations between different words,"<p>I am working on a project which involves computation of relatedness between different concepts. </p>

<p>Examples:
""landing"" - related to: ""to arrive; to come"" and ""land"",
""telephone"" - related to: ""electronic; electricity"" and ""to talk; to communicate"",
""movie"" - related to: ""to move; to change"" and ""picture; image""
, and so on.</p>

<p>I have checked the WordNet framework and WordNet::Similarity Perl module, but in some cases they are not usable, especially in cases where I need to connect nouns with verbs (for example a noun ""dog"" and a verb ""to bark""). Also, WordNet organizes all synsets in hierarchical structure (ex: computer -> machine -> device -> instrumentation), and this is not that I really need in my project (of course, I may be wrong).</p>

<p>The question is - which framework or database, what approach should I use to solve the problem? Where do I start?</p>

<p>It would be really helpful if you could give me some advice.</p>
",Vectorization & Embeddings,finding semantic similarity relation different word working project involves computation relatedness different concept example landing related arrive come land telephone related electronic electricity talk communicate movie related move change picture image checked wordnet framework wordnet similarity perl module case usable especially case need connect noun verb example noun dog verb bark also wordnet organizes synset hierarchical structure ex computer machine device instrumentation really need project course may wrong question framework database approach use solve problem start would really helpful could give advice
Why the similarity beteween two bag-of-words in gensim.word2vec calculated this way?,"<pre><code>def n_similarity(self, ws1, ws2):
    v1 = [self[word] for word in ws1]
    v2 = [self[word] for word in ws2]
    return dot(matutils.unitvec(array(v1).mean(axis=0)), matutils.unitvec(array(v2).mean(axis=0)))
</code></pre>

<p>This is the code I excerpt from gensim.word2Vec, I know that two single words' similarity can be calculated by cosine distances, but what about two word sets? The code seems to use the mean of each wordvec and then calculated on the two mean vectors' cosine distance. I know few in word2vec, is there some foundations of such process?</p>
",Vectorization & Embeddings,similarity beteween two bag word gensim word vec calculated way code excerpt gensim word vec know two single word similarity calculated cosine distance two word set code seems use mean wordvec calculated two mean vector cosine distance know word vec foundation process
Scikit-learn TfidfTranformer yielding wrong results?,"<p>I'm getting ""weird"" results using scikit-learn's Tfidf transformer. Normally, I would expect a word, that occurs in all documents in a corpus to have an idf equal to 0 (using no sort of smoothing or normalization), as the formular I would use would be the logarithm of the number of document in the corpus divided by the number of documents containing the term. Apparently (as illustrated below) scikit-learn's implementation adds one to each idf value compared to my manual implementation. Does anybody know why? Again, notice that I have set smoothing and normalization equal to None/False.</p>

<pre><code>In [101]: from sklearn.feature_extraction.text import TfidfTransformer

In [102]: counts
Out[102]: 
array([[3, 0, 1],
       [2, 0, 0],
       [3, 0, 0],
       [4, 0, 0],
       [3, 2, 0],
       [3, 0, 2]])

In [103]: transformer = TfidfTransformer(norm=None, smooth_idf=False)

In [104]: transformer
Out[104]: 
TfidfTransformer(norm=None, smooth_idf=False, sublinear_tf=False,
         use_idf=True)

In [105]: tfidf = transformer.fit_transform(counts)

In [106]: tfidf.toarray()
Out[106]: 
array([[ 3.        ,  0.        ,  2.09861229],
       [ 2.        ,  0.        ,  0.        ],
       [ 3.        ,  0.        ,  0.        ],
       [ 4.        ,  0.        ,  0.        ],
       [ 3.        ,  5.58351894,  0.        ],
       [ 3.        ,  0.        ,  4.19722458]])

In [107]: transformer.idf_
Out[107]: array([ 1.        ,  2.79175947,  2.09861229])

In [108]: idf1 = np.log(6/6)

In [109]: idf1
Out[109]: 0.0

In [110]: idf2 = np.log(6/1)

In [111]: idf2
Out[111]: 1.791759469228055

In [112]: idf3 = np.log(6/2)

In [113]: idf3
Out[113]: 1.0986122886681098
</code></pre>

<p>I have been unable to find any source that justifies adding one to the idf values. I'm using scikit-learn version '0.14.1'.</p>

<p>Btw another solution than scikit-learn is not really useful to me, as I need to build a scikit-learn pipeline for gridsearch.</p>
",Vectorization & Embeddings,scikit learn tfidftranformer yielding wrong result getting weird result using scikit learn tfidf transformer normally would expect word occurs document corpus idf equal using sort smoothing normalization formular would use would logarithm number document corpus divided number document containing term apparently illustrated scikit learn implementation add one idf value compared manual implementation doe anybody know notice set smoothing normalization equal none false unable find source justifies adding one idf value using scikit learn version btw another solution scikit learn really useful need build scikit learn pipeline gridsearch
Cosine Similarity of Vectors of different lengths?,"<p><a href=""https://stackoverflow.com/questions/3113428/classifying-documents-into-categories/3114191#3114191"">I'm trying to use TF-IDF</a> to sort documents into categories.  I've calculated the tf_idf for some documents, but now when I try to calculate the Cosine Similarity between two of these documents I get a traceback saying:</p>

<pre><code>#len(u)==201, len(v)==246

cosine_distance(u, v)
ValueError: objects are not aligned

#this works though:
cosine_distance(u[:200], v[:200])
&gt;&gt; 0.52230249969265641
</code></pre>

<p>Is slicing the vector so that len(u)==len(v) the right approach?  I would think that cosine similarity would work with vectors of different lengths.</p>

<p>I'm using <a href=""https://stackoverflow.com/questions/2380394/simple-implementation-of-n-gram-tf-idf-and-cosine-similarity-in-python/2754261#2754261"">this function</a>:</p>

<pre><code>def cosine_distance(u, v):
    """"""
    Returns the cosine of the angle between vectors v and u. This is equal to
    u.v / |u||v|.
    """"""
    return numpy.dot(u, v) / (math.sqrt(numpy.dot(u, u)) * math.sqrt(numpy.dot(v, v))) 
</code></pre>

<p>Also -- is the order of the tf_idf values in the vectors important?  Should they be sorted -- or is it of no importance for this calculation?</p>
",Vectorization & Embeddings,cosine similarity vector different length also order tf idf value vector important sorted importance calculation
Finding topics of an unseen document via Gensim,"<p>I am using Gensim to do some large-scale topic modeling. I am having difficulty understanding how to determine predicted topics for an unseen (non-indexed) document. For example: I have 25 million documents which I have converted to vectors in LSA (and LDA) space. I now want to figure out the topics of a new document, lets call it x.</p>

<p>According to the Gensim documentation, I can use:</p>

<pre><code>topics = lsi[doc(x)]
</code></pre>

<p>where doc(x) is a function that converts x into a vector.</p>

<p>The problem is, however, that the above variable, topics, returns a vector. The vector is useful if I am comparing x to additional documents because it allows me to find the cosine similarity between them, but I am unable to actually return specific words that are associated with x itself.</p>

<p>Am I missing something, or does Gensim not have this capability?</p>

<p>Thank you,</p>

<p><strong>EDIT</strong></p>

<p>Larsmans has the answer.</p>

<p>I was able to show the topics by using:</p>

<pre><code>for t in topics:
    print lsi.show_topics(t[0])
</code></pre>
",Vectorization & Embeddings,finding topic unseen document via gensim using gensim large scale topic modeling difficulty understanding determine predicted topic unseen non indexed document example million document converted vector lsa lda space want figure topic new document let call x according gensim documentation use doc x function convert x vector problem however variable topic return vector vector useful comparing x additional document allows find cosine similarity unable actually return specific word associated x missing something doe gensim capability thank edit larsmans ha answer wa able show topic using
Clustering from the cosine similarity values,"<p>I have extracted words from a set of URLs and calculated cosine similarity between each URL's contents.And also I have normalized the values between 0-1(using Min-Max).Now i need to cluster the URLs based on cosine similarity values to find out similar URLs.which clustering algorithm will be most suitable?.Please suggest me a Dynamic clustering method because it will be useful since i could increase number of URL's on demand and also it will be more natural.Please correct me if you feel i'm making the progress in a wrong way.Thanks in anticipation.    </p>
",Vectorization & Embeddings,clustering cosine similarity value extracted word set url calculated cosine similarity url content also normalized value using min max need cluster url based cosine similarity value find similar url clustering algorithm suitable please suggest dynamic clustering method useful since could increase number url demand also natural please correct feel making progress wrong way thanks anticipation
Algorithm for text filtering in NLP,"<p>I'm making a question/answering system where my program reads data from internet and replies. For that I've made a java program to fetch data from YQL(yahoo query language) </p>

<pre><code>    String baseUrl = ""http://query.yahooapis.com/v1/public/yql?q="";

    String query = ""select ChosenAnswer from answers.search where query=\""what is benzene ring\"";

    String fullUrlStr = baseUrl + URLEncoder.encode(query, ""UTF-8"") + ""&amp;format=json"";
    JSONObject json = readJsonFromUrl(fullUrlStr);
</code></pre>

<p>These are few of the result I get:</p>

<pre><code>     ""ChosenAnswer"": ""A benzene ring is a hexagonal (6 sided) ring of 6 carbon atoms.  Each carbon has one single bond and one double bond and one hydrogen so that each carbon has 4 bonds total.  The double bonds can alternate with the single bonds so that the result is a pi electron cloud in a ring shape above and below the plane of the carbon ring.  Benzene rings are very common and stable.  Because they have double bonds, they are not saturated.  If you saturated benzene, you would get cyclohexane.""
    }
     ""ChosenAnswer"": ""The IUPAC name for Benzene Ring is Benzene. It forms the basis for other IUPAC-named benzene derivatives like 1,2-dimethylbenzene etc. \n\nBenzene as a substituent group is called the phenyl group. (e.g. phenylethylamine\n\nBenzene is the IUPAC name for an aromatic hydrocarbon with the formula C6H6. It is also called benzol, or cyclohexa-1,3,5-triene. \n  \n\nUses of Benzene - As an industrial solvent for fats and oils, rubber, resins etc. As a starting material for dyes, drugs, perfumes and explosives and polymers For dry-cleaning of woollen cl..""
    },
    {
     ""ChosenAnswer"": ""Benzene rings aren't metallic bonds.  Metallic bonds have the special property of having a \""sea of electrons\"" which basically means that the electrons don't really belong to any single atom and just kind of flow around the element.  That is pretty much what makes metals conduct electricity well.  Other materials that don't form metallic bonds can conduct electricity as well, but usually these consists of certain ions so they have positive or negative charges.  A benzene rings doesn't have any charge with it so it's definitely not going to conduct electricity in this way.""
    },
    {
     ""ChosenAnswer"": ""You can draw the benzene ring in any orientation you would like, point up or side up, although a point up is more common.  You can also draw the alternating single and double bonds any way you want.  Although you should keep in mind that there really aren't alternating single and double bonds.  Every C-C bond in benzene is exactly alike and the bonds have characteristics that are half-way between a single bond and a double bond.  That is why you will also see benzene with a circle in the middle.  Benzene exhibits delocalized pi bonding that accounts for the many interesting properties of C6H6.\n\n.""
    },
    {
     ""ChosenAnswer"": ""Benzene and phenyl seems look the same because they are both aromatic and all aromatic compounds are based on benzene C6H6.  Phenyl or phenyl functional group is a hydrocarbon derived from benzene by removing 1 H, making it a C6H5 then attaching it to something else.""
    },
    {
     ""ChosenAnswer"": ""since benzene is an aromatic compound it is highly stable which means it has to be activated.you can activate benzene by adding electron donor groups to his ring(electron donor groups: -OH,-CH3).the electron donor groups stabilize the ring(they help to maintain resonance of the ring by delocalizing electrons into it) while the reaction occurs.benzene undergoes the reactions called electrophilic  aromatic substitution so check that out it will be more clear to you then.""
    },
    {
     ""ChosenAnswer"": ""C6H6 is benzene.  It is a 6 membered carbon ring each carbon has one H bonded to it and ONE resonance structure is with double bonds alternating between single bonds.  Ary is a radical meaning it is used to describe the benzene ring portion of a molecule that has some other group attached where one of the H's was.  C6H12 if you are discusing a single ring structure is cyclohexane.""
    },
    {
     ""ChosenAnswer"": ""benzene ring is mostly a compound of carbon and hydrogen in a hexagon shape structure""
    },
</code></pre>

<p>Now most of the answers here are opinion based answers(as we all know yahoo answers, where anyone replies). But i have to figure a way to filter these answers. Either i can make my query more efficient by using some joins etc or I can use some algorithm (maybe cosine similarity) to filter the answers and get max top 3 efficient answers. Please suggest me some algorithm which i can implement in java to get a relevant answer.
E.g in the case above the <strong>first</strong> 'ChosenAnswer' is the most suitable one.
(i know this is an enormous topic, i just want to know some good algorithm i can use here)</p>
",Vectorization & Embeddings,algorithm text filtering nlp making question answering system program read data internet reply made java program fetch data yql yahoo query language result get answer opinion based answer know yahoo answer anyone reply figure way filter answer either make query efficient using join etc use algorithm maybe cosine similarity filter answer get max top efficient answer please suggest algorithm implement java get relevant answer e g case first chosenanswer suitable one know enormous topic want know good algorithm use
Implementing alternative forms of LDA,"<p>I am using Latent Dirichlet Allocation with a corpus of news data from six different sources. I am interested in topic evolution, emergence, and want to compare how the sources are alike and different from each other over time. I know that there are a number of modified LDA algorithms such as the Author-Topic model, Topics Over Time, and so on.</p>

<p>My issue is that very few of these alternate model specifications are implemented in any standard format. A few are available in Java, but most exist as conference papers only. What is the best way to go about implementing some of these algorithms on my own? I am fairly proficient in R and jags, and can stumble around in Python when given long enough. I am willing to write the code, but I don't really know where to start and I don't know C or Java. Can I build a model in JAGS or Python just having the formulas from the manuscript? If so, can someone point me at an example of doing this? Thanks.</p>
",Vectorization & Embeddings,implementing alternative form lda using latent dirichlet allocation corpus news data six different source interested topic evolution emergence want compare source alike different time know number modified lda algorithm author topic model topic time issue alternate model specification implemented standard format available java exist conference paper best way go implementing algorithm fairly proficient r jag stumble around python given long enough willing write code really know start know c java build model jag python formula manuscript someone point example thanks
Detecting Similarity in Strings,"<p>If I search for something on Google News, I can click on the ""Explore in depth"" button and get the same news article from multiple sources. What kind of algorithm is used to compare articles of text and then determine that it is regarding the same thing? I have seen the Question here: </p>

<p><a href=""https://stackoverflow.com/questions/62328/is-there-an-algorithm-that-tells-the-semantic-similarity-of-two-phrases"">Is there an algorithm that tells the semantic similarity of two phrases</a> </p>

<p>However, using methods mentioned there, I feel that if there were articles that were similar in nature but regarding different stories, they would be grouped together using the methods mentioned there. Is there a standard way of detecting Strings that are about the same thing and grouping them, while keeping Strings that are just similar separate? Eg. If I search ""United States Border"" I might get stories about problems at the USA's border, but what would prevent these from all getting grouped together? All I can think of is the date of publication, but what if many stories were published very close to each other?</p>
",Vectorization & Embeddings,detecting similarity string search something google news click explore depth button get news article multiple source kind algorithm used compare article text determine regarding thing seen question
Calculating Cosine Similarity between two graph nodes with different number of vectors,"<p>I'm implementing the paper titled ""<a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.175.4079&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">Efficient Graph-Based Semi-Supervised Learning of Structured Tagging Models</a>"" as part of my research. </p>

<p>As part of graph construction procedure which is the Section 3 of the paper, I need to define some kind of similarity measure to calculate edge weight for each edge connecting a pair of nodes. According to the paper, I have to create a PMI (Pointwise Mutual Information) vector for this purpose. What I have to do is to calculate PMIs for features occurring on each token.</p>

<p>Each n-gram is named ""type"" and each of its occurrences is named ""token"" in this paper.</p>

<p>As an example if we take x2-x3-x4 to be our current Type which occurs in two contexts x1-x2-x3-x4-x5 and x6-x2-x3-x4-x7 I have to compute a set of features relating to the type x2-x3-x4. But somehow this procedure seems complex and unclear. This is what I got:</p>

<ul>
<li>I should calculate PMI's for each feature on every Token. Which results in a vector of PMIs for each token and the final result would be an array of PMI vectors for the current Type. The array size will be equal to the count of tokens of a given type. Now as a final step I should measure similarity of different nodes. But the problem is that the resulting vector array of each type has a different size, so I can not compare these arrays with each other.</li>
</ul>

<p>So, what is the solution? Did I made a mistake here?</p>
",Vectorization & Embeddings,calculating cosine similarity two graph node different number vector implementing paper titled efficient graph based semi supervised learning structured tagging model part research part graph construction procedure section paper need define kind similarity measure calculate edge weight edge connecting pair node according paper create pmi pointwise mutual information vector purpose calculate pmis feature occurring token n gram named type occurrence named token paper example take x x x current type occurs two context x x x x x x x x x x compute set feature relating type x x x somehow procedure seems complex unclear got calculate pmi feature every token result vector pmis token final result would array pmi vector current type array size equal count token given type final step measure similarity different node problem resulting vector array type ha different size compare array solution made mistake
Memory overhead of Case classes in scala,"<p>What is the memory overhead of a case class in scala ?</p>

<p>I've implemented some code to hold a lexicon with multiple types of interned tokens for NLP processing. I've got a case class for each token type. </p>

<p>For example, the canonical lemma/stem token is as follows:</p>

<pre><code>sealed trait InternedLexAtom extends LexAtom{
    def id      : Int
}
case class Lemma(id: Int) extends InternedLexAtom
</code></pre>

<p>I'm going to be returning document vectors of these interned tokens, the reason I wrap them in case classes is to be able to add methods to the tokens via <a href=""http://docs.scala-lang.org/overviews/core/implicit-classes.html"" rel=""nofollow"">implicit classes</a>. The reason I use this way of adding behaviour to the lexeme's is because I want the lexemes to have different methods based on different contexts.</p>

<p>So I'm hoping the answer will be zero memory overhead due to type erasure. Is this the case ?  </p>

<p>I have a suspicion that a single pointer might be packed with the parameters for some of the magic Scala can do :(</p>

<p><strong>justification</strong></p>

<p>To put things in perspective. The JVM uses 1.5-2gigs of memory with my lexicon loaded (the lexicon does not use cases classes in it's in-memory representation), and C++ does the same in 500-700 mb of memory. If my codebase keeps scaling it's memory requirements the way it is now I'm not going to be able to do this stuff on my laptop (in-memory)</p>

<p>I'll sidestep the problem by structuring my code differently. For example I can just strip away the case classes in vector representations if I need to. Would be nice if I didn't have to.</p>

<p><strong>Question Extension.</strong></p>

<p>Robin and Pedro have addressed the use-case, thank you. In this case I was missing value classes. With those there are no more downsides. <strong>additionally</strong>: I tried my best not to mention C++'s POD concept. But now I must ask :D A c++ POD is just a struct with primitive values. If I wanted to pack more than just one value into value class, how would I achieve this ? I am assuming this would be what I want to do ? </p>

<pre><code>class SuperTriple(val underlying: Tuple2[Int,Int]) extends AnyVal {
    def super: underlying._1
    def triple: underlying._2
}
</code></pre>

<p>I do actually need the above construct, since a <code>SuperTriple</code> is what I am using as my vector model symbol :D </p>

<p>The <strong>original question</strong> still remains ""what is the overhead of a case class"". </p>
",Vectorization & Embeddings,memory overhead case class scala memory overhead case class scala implemented code hold lexicon multiple type interned token nlp processing got case class token type example canonical lemma stem token follows going returning document vector interned token reason wrap case class able add method token via implicit class reason use way adding behaviour lexeme want lexeme different method based different context hoping answer zero memory overhead due type erasure case suspicion single pointer might packed parameter magic scala justification put thing perspective jvm us gig memory lexicon loaded lexicon doe use case class memory representation c doe mb memory codebase keep scaling memory requirement way going able stuff laptop memory sidestep problem structuring code differently example strip away case class vector representation need would nice question extension robin pedro addressed use case thank case wa missing value class downside additionally tried best mention c pod concept must ask c pod struct primitive value wanted pack one value value class would achieve assuming would want actually need construct since using vector model symbol original question still remains overhead case class
Word vectorization for neural network input?,"<p>I wanted to try some NLP things on a Neural-Network, but for the input I need vectors of word, ""one-of-k"" can't be used because of the big vocabulary. So I tried to do ""multidimensional scaling"",which for reasons unknown to me doesn't work. ""Programming Collective Intelligence"" was the book that I followed for this.</p>

<p>This isn't actually my Problem on which I wanted to work on. So if there would be a library available which will do this work, I could overcome this obstacle and experiment on my actual problem.</p>
",Vectorization & Embeddings,word vectorization neural network input wanted try nlp thing neural network input need vector word one k used big vocabulary tried multidimensional scaling reason unknown work programming collective intelligence wa book followed actually problem wanted work would library available work could overcome obstacle experiment actual problem
tf-idf on a somewhat large (65k) amount of text files,"<p>I want to try tfidf with scikit-learn (or nltk or am open to other suggestions).  The data I have is a relatively large amount of discussion forum posts (~65k) we have scraped and stored in a mongoDB.  Each post has a Post title, Date and Time of post, Text of the post message (or a re: if a reply to an existing post), User name, message ID and whether it is a child or parent post (in a thread, where you have the original post, and then replies to this op, or nested replies, the tree).</p>

<p>I figure each post, would be a separate document, and similar to the 20newsgroups, each document would have the fields I mentioned at the top, and the text of the message post at the bottom which I would extract out of mongo and write into the required format for each text file.</p>

<p>For loading the data into scikit, I know of:<br>
<a href=""http://scikit-learn.org/dev/modules/generated/sklearn.datasets.load_files.html"" rel=""nofollow"">http://scikit-learn.org/dev/modules/generated/sklearn.datasets.load_files.html</a>  (but my data is not categorized)
<a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow"">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a> - For the input, I know I would be using filenames, but because I would have a large amount of files (each post), is there a way to either have filenames read from a text file? Or is there some example implementation someone could point me towards?</p>

<p>Also, any advice on structuring the filenames for each these discussion forum posts, for later identifying when I get the tfidf vectors and cosine similarity array</p>

<p>Thanks</p>
",Vectorization & Embeddings,tf idf somewhat large k amount text file want try tfidf scikit learn nltk open suggestion data relatively large amount discussion forum post k scraped stored mongodb post ha post title date time post text post message reply existing post user name message id whether child parent post thread original post reply op nested reply tree figure post would separate document similar newsgroups document would field mentioned top text message post bottom would extract mongo write required format text file loading data scikit know data categorized input know would using filename would large amount file post way either filename read text file example implementation someone could point towards also advice structuring filename discussion forum post later identifying get tfidf vector cosine similarity array thanks
Do tf-idf weights affect the cosine similarity?,"<p>I'm clustering text documents. I'm using tf-idf and cosine similarity. However there's something I don't really understand even tho I'm using these measures. Do the tf-idf weights affect the similarity calculations between two documents?</p>

<p>Suppose I have these two documents:</p>

<p>1- High trees.</p>

<p>2- High trees High trees High trees High trees.</p>

<p>Then the similarity between the two documents will be 1, although the tf-idf vectors of the two documents are different. Where the second should normally have higher weights for the terms compared to the first document. </p>

<p>Suppose the weights for the two vectors are (just suppose):</p>

<p>v1(1.0, 1.0)</p>

<p>v2(5.0, 8.0)</p>

<p>calculating the cosine similarity gives 1.0.</p>

<p>Here is a sketch of two random vectors that share the same terms but with different weights. </p>

<p>There's an obvious angel between the vectors, so the weights should play a role!</p>

<p><img src=""https://i.sstatic.net/SVPzt.png"" alt=""enter image description here""></p>

<p>This triggers the question, where do the tf/idf weights play a role in the similarity calculations? Because what I understood so far is that the similarity here only cares about the presence and absence of the terms.</p>
",Vectorization & Embeddings,tf idf weight affect cosine similarity clustering text document using tf idf cosine similarity however something really understand even tho using measure tf idf weight affect similarity calculation two document suppose two document high tree high tree high tree high tree high tree similarity two document although tf idf vector two document different second normally higher weight term compared first document suppose weight two vector suppose v v calculating cosine similarity give sketch two random vector share term different weight obvious angel vector weight play role trigger question tf idf weight play role similarity calculation understood far similarity care presence absence term
Methods for extracting locations from text?,"<p>What are the recommended methods for extracting locations from free text? </p>

<p>What I can think of is to use regex rules like ""words ... in location"". But are there better approaches than this?</p>

<p>Also I can think of having a lookup hash table table with names for countries and cities and then compare every extracted token from the text to that of the hash table.</p>

<p>Does anybody know of better approaches?</p>

<p>Edit: I'm trying to extract locations from tweets text. So the issue of high number of tweets might also affect my choice for a method.</p>
",Vectorization & Embeddings,method extracting location text recommended method extracting location free text think use regex rule like word location better approach also think lookup hash table table name country city compare every extracted token text hash table doe anybody know better approach edit trying extract location tweet text issue high number tweet might also affect choice method
Calculating Cosine Similarity of two Vectors of Different Size,"<p>I have 2 questions,</p>

<ol>
<li><p>I've made a vector from a document by finding out how many times each word appeared in a document. Is this the right way of making the vector? Or do I have to do something else also?</p></li>
<li><p>Using the above method I've created vectors of 16 documents, which are of different sizes. Now i want to apply cosine similarity to find out how similar each document is. The problem I'm having is getting the dot product of two vectors because they are of different sizes. How would i do this?</p></li>
</ol>
",Vectorization & Embeddings,calculating cosine similarity two vector different size question made vector document finding many time word appeared document right way making vector something else also using method created vector document different size want apply cosine similarity find similar document problem getting dot product two vector different size would
Java Google Engine Library,"<p>i want a java library, to search a text on Google and returns some pages based on text and semantic similarity. is there any API doing this job?
i wrote a crawler myself and search to specific depth from a root page, but i dont know how to input a text as a search query for Google.  </p>
",Vectorization & Embeddings,java google engine library want java library search text google return page based text semantic similarity api job wrote crawler search specific depth root page dont know input text search query google
Calculating TF-IDF Similarity Between 2 Documents Using Gensim,"<p>I'm using Gensim to calculate the similarity between 2 documents. For some reason the line tfidf[corpus] returns an empty list. I'm not sure why though</p>

<pre><code>    articles = []
#make a corpus by adding each of the top 25 documents to a list
for x in range(0,25):
    articles.append(str(WikiDoc(sorted_links[0]).jsonify()['text']))
#puts all of the top 25 documents into a list
texts = [[word for word in document.lower().split()] for document in articles]
print texts
#load precomputed dictionary
articles_dict = corpora.Dictionary(texts)
articles_dict.save('./articles.dict')
articles_dict = Dictionary.load('./articles.dict')
#articles_corpus = [articles_dict.doc2bow(text) for text in texts]
#corpora.MmCorpus.serialize('./articles.mm', articles_corpus)
corpus = [articles_dict.doc2bow(text) for text in texts]
corpora.MmCorpus.serialize('./articles.mm', corpus)
corpus = corpora.MmCorpus('./articles.mm')
#build the tfidf model based on the 25 documents so that we can find similarities 
#with respect to each of these documents
tfidf = models.TfidfModel(corpus)
#get the other document and process to produce dictionary representation
one_doc_bow = WikiDoc('SpongeBob')
one_doc_bow = articles_dict.doc2bow(one_doc_bow.jsonify()['text'].lower().split())
print tfidf[one_doc_bow]
top = tfidf[one_doc_bow]
corpus_tfidf = tfidf[corpus]
</code></pre>

<p>When I print the dictionary I get: Dictionary(2204 unique tokens)
When I print the MmCorpus I get: MmCorpus(25 documents, 2204 features, 55100 non-zero entries)
tfidf[corpus] yield [].
Can anyone diagnose my problem? Thanks a lot!</p>
",Vectorization & Embeddings,calculating tf idf similarity document using gensim using gensim calculate similarity document reason line tfidf corpus return empty list sure though print dictionary get dictionary unique token print mmcorpus get mmcorpus document feature non zero entry tfidf corpus yield anyone diagnose problem thanks lot
Classification/Prediction in R,"<p>I have a corpus of <strong>N</strong> documents classified as <em>spam</em> / <em>no-spam</em>. I am following the standard procedure to pre-process the data in R(<a href=""https://gist.github.com/elyase/5365552"" rel=""nofollow"">code here</a>). The pre-processing ends with a <code>DocumenTermMatrix</code> using weights as <strong>tfidf</strong>.</p>

<p>Now I want to classify new documents with my model. </p>

<p>How can I calculate the corresponding <code>DocumentVector</code> (using the <strong>tf</strong> of the document and the <strong>idfs</strong> of the corpus) for a single new document? I would like to avoid recalculating the <code>DocumentTermMatrix</code> for the whole corpus.</p>
",Vectorization & Embeddings,classification prediction r corpus n document classified spam spam following standard procedure pre process data r code pre processing end using weight tfidf want classify new document model calculate corresponding using tf document idf corpus single new document would like avoid recalculating whole corpus
lexical-level similarity word clustering tool,"<p>Is there any open software toolkit that compares the lexcial-level similarities among words and group similar words together? For example, Blue jean, Blue jeans, and blue jea (miss-spelled) should be grouped together? I don't need to look for semantic similarity here. </p>
",Vectorization & Embeddings,lexical level similarity word clustering tool open software toolkit compare lexcial level similarity among word group similar word together example blue jean blue jean blue jea miss spelled grouped together need look semantic similarity
Efficiently Compare Successive Characters in String,"<p>I'm doing some text analysis, and need to record the frequencies of character transitions in a <code>String</code>. I have <em>n</em> categories of characters: for the sake of example, <code>isUpperCase()</code>, <code>isNumber()</code>, and <code>isSpace()</code>.</p>

<p>Given that there are <em>n</em> categories, there will be <em>n^2</em> categories of transitions, e.g. ""<code>isUpperCase()</code> --> <code>isUpperCase()</code>"", ""<code>isUpperCase</code> --> <code>isLetter()</code>"", ""<code>isLetter()</code> --> <code>isUpperCase()</code>"", etc.</p>

<p>Given a block of text, I would like to record the number of transitions that took place. I would imagine constructing a <code>Map</code> with the transition types as the <code>Keys</code>, and an <code>Integer</code> as each <code>Value</code>. </p>

<p>For the block of text ""<code>TO</code>"", the <code>Map</code> would look like <code>[isUpper -&gt; isUpper : 1, isUpper -&gt; isSpace : 1]</code></p>

<p>The part I cannot figure out, though, is how to construct a <code>Map</code> where, from what I can see, the <code>Key</code> would consist of 2 <code>boolean</code> methods.</p>
",Vectorization & Embeddings,efficiently compare successive character string text analysis need record frequency character transition n category character sake example given n category n category transition e g etc given block text would like record number transition took place would imagine constructing transition type block text would look like part figure though construct see would consist method
Comparing context vectors,"<p>I'm using vectors to represent context around words and I need to compare contexts with each other. The following is a simplified version of my problem:</p>

<p>Let's say I have a vector <code>a=[1,1,15,2,0]</code>. Then I have a vector <code>b=[0,0,15,0,0]</code> and <code>c=[1,1,11,0,1]</code>. When comparing the two vectors by cosine similarity <code>b</code> is closest to <code>a</code>. However, since the vectors are representing context <code>c</code> makes more sense in my case since <code>b</code> is just a context which happens to have one word common with the original and has the same score. </p>

<p>How could I return <code>c</code> as the most similar? Another similarity measure? Or maybe my reasoning is flawed somewhere?</p>

<p>As I've said, this is a simplification of my problem. I am already normalizing the vectors and for scoring context words I'm using log-likelihood.</p>

<p>Thanks!</p>
",Vectorization & Embeddings,comparing context vector using vector represent context around word need compare context following simplified version problem let say vector vector comparing two vector cosine similarity closest however since vector representing context make sense case since context happens one word common original ha score could return similar another similarity measure maybe reasoning flawed somewhere said simplification problem already normalizing vector scoring context word using log likelihood thanks
"scikit-learn, add features to a vectorized set of documents","<p>I am starting with scikit-learn and I am trying to transform a set of documents into a format on which I could apply clustering and classification. I have seen the details about the vectorization methods, and the tfidf transformations to load the files and index their vocabularies.</p>

<p>However, I have extra metadata for each documents, such as the authors, the division that was responsible, list of topics, etc.</p>

<p>How can I add features to each document vector generated by the vectorizing function?</p>
",Vectorization & Embeddings,scikit learn add feature vectorized set document starting scikit learn trying transform set document format could apply clustering classification seen detail vectorization method tfidf transformation load file index vocabulary however extra metadata document author division wa responsible list topic etc add feature document vector generated vectorizing function
Representing documents in vector space model,"<p>I have a very fundamental question. I have two sets of documents, one for training and one for testing. I would like to train a Logistic regression classifier with the training documents. I want to know if I'm doing the right thing.</p>

<ol>
<li>First find the list of all unique words in the training document and call it vocabulary.</li>
<li>For each  word in the vocabulary, find its TFIDF in every training document. A document is then represented as vector of these TFIDF scores.</li>
</ol>

<p>My question is: 
1.  How do I represent the test documents?  Say, one of the test documents does not have any word that is in the vocabulary. In that case , the TFIDF scores will be zero for all words in the vocabulary for that document.</p>

<ol>
<li>I'm trying to use LIBSVM which uses the sparse vector format. For the case of the above document, which has all entries set to 0 in its vector representation, how do I represent it?</li>
</ol>
",Vectorization & Embeddings,representing document vector space model fundamental question two set document one training one testing would like train logistic regression classifier training document want know right thing first find list unique word training document call vocabulary word vocabulary find tfidf every training document document represented vector tfidf score question represent test document say one test document doe word vocabulary case tfidf score zero word vocabulary document trying use libsvm us sparse vector format case document ha entry set vector representation represent
semantic similarity between words using dynamic techniques(using wikipedia),"<p>I wrote a program in python to find semantic similarity between words using word net. But I feel it is static. I want to give a dynamic approach to it. I want to access the definition of each word from wikipedia. How can I access the definition of a word like that? When I googled I found that by parsing wikipedia dump file we can get the defintion. But i don't know how to parse that. Does any one can implement a parser to get the definiton of word from dumped file. Is this the only way? Method is true.</p>
",Vectorization & Embeddings,semantic similarity word using dynamic technique using wikipedia wrote program python find semantic similarity word using word net feel static want give dynamic approach want access definition word wikipedia access definition word like googled found parsing wikipedia dump file get defintion know parse doe one implement parser get definiton word dumped file way method true
How to compute TF-IDF,"<p>I want to get the semantic similarity of two words using cosine similarity method using TF-IDF.
For that first I want to take the meaning of those words from wikipedia or word-net.After that I want to pre-process the text and find the TF-IDF. When I googled the problem I found that for finding the TF-IDF we should have a train set and test set. In my case which one is train set and which one is test set? How can I calculate cosine similarity using computed result?</p>
",Vectorization & Embeddings,compute tf idf want get semantic similarity two word using cosine similarity method using tf idf first want take meaning word wikipedia word net want pre process text find tf idf googled problem found finding tf idf train set test set case one train set one test set calculate cosine similarity using computed result
How to ignore certain characters while doing diff in google-diff-match-patch?,"<p>I'm using <a href=""https://code.google.com/p/google-diff-match-patch/"" rel=""noreferrer"">google-diff-match-patch</a> to compare plain text in natural languages.</p>

<p>How can I make google-diff-match-patch to ignore certain characters?
(Some tiny differences which I don't care.)</p>

<p>For example, given text1:</p>

<pre><code>give me a cup of bean-milk. Thanks.
</code></pre>

<p>and text2:</p>

<pre><code>please give mom a cup of bean milk!  Thank you.
</code></pre>

<p>(Note that there are two space characters before 'Thank you'.)</p>

<p>google-diff-match-patch outputs something like this:</p>

<pre><code>[please] give m(e)[om] a cup of bean(-)[ ]milk(.)[!] Thank(s)[ you].
</code></pre>

<p>It seems that google-diff-match-patch only ignores different numbers of white spaces.</p>

<p>How can I tell google-diff-match-patch to also ignore characters like <code>[-.!]</code>?</p>

<p>The expect result would be</p>

<pre><code>[please] give m(e)[om] a cup of bean-milk. Thank(s)[ you].
</code></pre>

<p>Thanks.</p>
",Vectorization & Embeddings,ignore certain character diff google diff match patch using google diff match patch compare plain text natural language make google diff match patch ignore certain character tiny difference care example given text text note two space character thank google diff match patch output something like seems google diff match patch ignores different number white space tell google diff match patch also ignore character like expect result would thanks
Algorithm to compare similarity of ideas (as strings),"<p>Consider an arbitrary text box that records the answer to the question, what do you want to do before you die?</p>

<p>Using a collection of response strings (max length 240), I'd like to somehow sort and group them and count them by idea (which may be just string similarity as described in <a href=""https://stackoverflow.com/questions/653157/a-better-similarity-ranking-algorithm-for-variable-length-strings"">this question</a>).</p>

<ol>
<li>Is there another or better way to do something like this?</li>
<li><em>Is this any different</em> than string similarity?</li>
<li>Is this the right question to be asking?</li>
</ol>

<p>The idea here is to have people write in a text box over and over again, and me to provide a number that describes, generally speaking, that 802 people <em>wrote approximately the same thing</em></p>
",Vectorization & Embeddings,algorithm compare similarity idea string consider arbitrary text box record answer question want die using collection response string max length like somehow sort group count idea may string similarity described href question another better way something like different string similarity right question asking idea people write text box provide number describes generally speaking people wrote approximately thing
How to implement LSA (Latent semantic analysis) in Python?,"<p>How to implement Latent semantic analysis in Python and compare corps of text against query using Cosine similarity ?</p>
",Vectorization & Embeddings,implement lsa latent semantic analysis python implement latent semantic analysis python compare corp text query using cosine similarity
How to implement LSA (Latent semantic analysis) in Python?,"<p>How to implement Latent semantic analysis in Python and compare corps of text against query using Cosine similarity ?</p>
",Vectorization & Embeddings,implement lsa latent semantic analysis python implement latent semantic analysis python compare corp text query using cosine similarity
algorithm to calculate similarity between texts,"<p>I am trying to score similarity between posts from social networks, but didn't find any good algorithms for that, thoughts?</p>

<p>I just tried Levenshtein, JaroWinkler, and others, but those one are more used to compare texts without sentiments. In posts we can get one text saying ""I really love dogs"" and an other saying ""I really hate dogs"", we need to classify this case as totally different.</p>

<p>Thanks</p>
",Vectorization & Embeddings,algorithm calculate similarity text trying score similarity post social network find good algorithm thought tried levenshtein jarowinkler others one used compare text without sentiment post get one text saying really love dog saying really hate dog need classify case totally different thanks
Generating easy-to-remember random identifiers,"<p>As all developers do, we constantly deal with some kind of identifiers as part of our daily work. Most of the time, it's about bugs or support tickets. Our software, upon detecting a bug, creates a package that has a name formatted from a timestamp and a version number, which is a cheap way of creating reasonably unique identifiers to avoid mixing packages up. Example: ""<em>Bug Report 20101214 174856 6.4b2</em>"".</p>

<p>My brain just isn't that good at remembering numbers. What I would love to have is a simple way of <strong>generating alpha-numeric identifiers that are easy to remember</strong>.</p>

<p>It takes about 5 minutes to whip up an algorithm like the following in python, which produces halfway usable results:</p>

<pre><code>import random

vowels = 'aeiuy' # 0 is confusing
consonants = 'bcdfghjklmnpqrstvwxz'
numbers = '0123456789'

random.seed()

for i in range(30):
    chars = list()
    chars.append(random.choice(consonants))
    chars.append(random.choice(vowels))
    chars.append(random.choice(consonants + numbers))
    chars.append(random.choice(vowels))
    chars.append(random.choice(vowels))
    chars.append(random.choice(consonants))
    print ''.join(chars)
</code></pre>

<p>The results look like this:</p>

<pre><code>re1ean
meseux
le1ayl
kuteef
neluaq
tyliyd
ki5ias
</code></pre>

<p>This is already quite good, but I feel it is still easy to forget how they are spelled exactly, so that if you walk over to a colleagues desk and want to look one of those up, there's still potential for difficulty.</p>

<p>I know of algorithms that perform trigram analysis on text (say you feed them a whole book in German) and that can generate strings that look and feel like German words and are thus easier to handle generally. This requires lots of data, though, and makes it slightly less suitable for embedding in an application just for this purpose.</p>

<p>Do you know of any published algorithms that solve this problem?</p>

<p>Thanks!</p>

<p>Carl</p>
",Vectorization & Embeddings,generating easy remember random identifier developer constantly deal kind identifier part daily work time bug support ticket software upon detecting bug creates package ha name formatted timestamp version number cheap way creating reasonably unique identifier avoid mixing package example bug report b brain good remembering number would love simple way generating alpha numeric identifier easy remember take minute whip algorithm like following python produce halfway usable result result look like already quite good feel still easy forget spelled exactly walk colleague desk want look one still potential difficulty know algorithm perform trigram analysis text say feed whole book german generate string look feel like german word thus easier handle generally requires lot data though make slightly le suitable embedding application purpose know published algorithm solve problem thanks carl
