Title,Description,category,combined_text
Difference between constituency parser and dependency parser,"<p>What is the difference between a <em>constituency parser</em> and a <em>dependency parser</em>? What are the different usages of the two?</p>
",Parsing & POS Tagging,difference constituency parser dependency parser difference constituency parser dependency parser different usage two
How to use Chunker Class in OpenNLP?,"<p>The <code>ChunkerME</code> class in OpenNLP has a <code>chunk()</code> method which takes two <code>String[]</code>. The first one should be the tags (tags from part of speech tagging process) and the second one is the actual terms.</p>

<p>I'm having a tagged string in the format of <code>Sir_NNP Arthur_NNP Conan_NNP...</code> and I'd like to chunk it using the ChunkerME class. However the chunker does not accept this string as is. however the OpenNLP command line has a command (opennlp ChunkerME en-chunker.bin) which directly accepts a tagged sentence and return a chunked sentence. </p>

<p>How can I use something like the one in the command line.</p>
",Parsing & POS Tagging,use chunker class opennlp class opennlp ha method take two first one tag tag part speech tagging process second one actual term tagged string format like chunk using chunkerme class however chunker doe accept string however opennlp command line ha command opennlp chunkerme en chunker bin directly accepts tagged sentence return chunked sentence use something like one command line
How to use &quot;Previous Tag&quot; as a feature?,"<p>I was implementing a CRF based POS tagging model using CRFsuite. I have included the features like checking if the word is capitalized, prefix of the word, suffix of the word, etc. However, these features are word based features.
I want to include POS tags based features in my prediction model.</p>
<p>If I simply define the POS tag feature as follows</p>
<pre><code>features = {
    ...
    'prevpos' : '' if i==0 else sentence[i-1][1],
    ...
}
</code></pre>
<p>I have no doubt that this will work for training my model on the corpus. But, when I will use it to predict POS tags for my test sentence, I am unsure about the behavior of this model. Because I think that the feature to extract the previous tag should not work as we do not know them previously.</p>
<p>Also, I do not know the behavior shown by the line <code>X_test_sentence = sent2features(test_sentence_tagged)</code> which will extract the features of the test sentence. And when I use my training model for predicting, <code>tags = crf.predict_single(X_test_sentence)</code> I have no idea about how the previous tags (say we assign dummy tags to test sentence words) will affect (or will not affect) the prediction.</p>
",Parsing & POS Tagging,use previous tag feature wa implementing crf based po tagging model using crfsuite included feature like checking word capitalized prefix word suffix word etc however feature word based feature want include po tag based feature prediction model simply define po tag feature follows doubt work training model corpus use predict po tag test sentence unsure behavior model think feature extract previous tag work know previously also know behavior shown line extract feature test sentence use training model predicting idea previous tag say assign dummy tag test sentence word affect affect prediction
NLTK RegEx Chunker not capturing defined grammar patterns with wildcards,"<p>I am trying to chunk a sentence using NLTK's POS tags as regular expressions. 2 rules are defined to identify phrases, based on the tags of words in the sentence.</p>
<p>Mainly, I wanted to capture the chunk of <strong>one or more verbs followed by an optional determiner and then one or more nouns at the end</strong>. This is the first rule in definition. But it is not getting captured as Phrase Chunk.</p>
<pre><code>import nltk

## Defining the POS tagger 
tagger = nltk.data.load(nltk.tag._POS_TAGGER)


## A Single sentence - input text value
textv=&quot;This has allowed the device to start, and I then see glitches which is not nice.&quot;
tagged_text = tagger.tag(textv.split())

## Defining Grammar rules for  Phrases
actphgrammar = r&quot;&quot;&quot;
     Ph: {&lt;VB*&gt;+&lt;DT&gt;?&lt;NN*&gt;+}  # verbal phrase - one or more verbs followed by optional determiner, and one or more nouns at the end
     {&lt;RB*&gt;&lt;VB*|JJ*|NN*\$&gt;} # Adverbial phrase - Adverb followed by adjective / Noun or Verb
     &quot;&quot;&quot;

### Parsing the defined grammar for  phrases
actp = nltk.RegexpParser(actphgrammar)

actphrases = actp.parse(tagged_text)
</code></pre>
<p>The input to the chunker, tagged_text is as below.</p>
<blockquote>
<p>tagged_text
Out[7]:
[('This', 'DT'),
('has', 'VBZ'),
('allowed', 'VBN'),
('the', 'DT'),
('device', 'NN'),
('to', 'TO'),
('start,', 'NNP'),
('and', 'CC'),
('I', 'PRP'),
('then', 'RB'),
('see', 'VB'),
('glitches', 'NNS'),
('which', 'WDT'),
('is', 'VBZ'),
('not', 'RB'),
('nice.', 'NNP')]</p>
</blockquote>
<p>In the final output, only the adverbial phrase ('<strong>then see</strong>'), that is matching the second rule is being captured.
I expected the verbal phrase ('<strong>allowed the device</strong>') to match with the first rule and get captured as well, but its not.</p>
<blockquote>
<p>actphrases Out[8]: Tree('S', [('This', 'DT'), ('has', 'VBZ'),
('allowed', 'VBN'), ('the', 'DT'), ('device', 'NN'), ('to', 'TO'),
('start,', 'NNP'), ('and', 'CC'), ('I', 'PRP'), <em><strong>Tree('Ph', [('then',
'RB'), ('see', 'VB')])</strong></em>, ('glitches', 'NNS'), ('which', 'WDT'), ('is',
'VBZ'), ('not', 'RB'), ('nice.', 'NNP')])</p>
</blockquote>
<p>NLTK version used is 2.0.5 (Python 2.7)
Any help or suggestion would be greatly appreciated.</p>
",Parsing & POS Tagging,nltk regex chunker capturing defined grammar pattern wildcards trying chunk sentence using nltk po tag regular expression rule defined identify phrase based tag word sentence mainly wanted capture chunk one verb followed optional determiner one noun end first rule definition getting captured phrase chunk input chunker tagged text tagged text dt ha vbz allowed vbn dt device nn start nnp cc prp rb see vb glitch nns wdt vbz rb nice nnp final output adverbial phrase see matching second rule captured expected verbal phrase allowed device match first rule get captured well actphrases tree dt ha vbz allowed vbn dt device nn start nnp cc prp tree ph rb see vb glitch nns wdt vbz rb nice nnp nltk version used python help suggestion would greatly appreciated
How can I prevent the benepar parser from splitting a specific substring when parsing a string?,"<p>I use the <a href=""https://github.com/nikitakit/self-attentive-parser"" rel=""nofollow noreferrer"">benepar parser</a> to parse sentences into trees. How can I prevent the benepar parser from splitting a specific substring when parsing a string?</p>
<p>E.g., the token <code>gonna</code> is split by benepar into two tokens <code>gon</code> and <code>na</code>, which I don't want.</p>
<hr />
<p>Code example, with pre-requisites:</p>
<pre><code>pip install spacy benepar
python -m nltk.downloader punkt benepar_en3
python -m spacy download en_core_web_md
</code></pre>
<p>If I run:</p>
<pre class=""lang-py prettyprint-override""><code>import benepar, spacy
import nltk
benepar.download('benepar_en3')
nlp = spacy.load('en_core_web_md')
if spacy.__version__.startswith('2'):
    nlp.add_pipe(benepar.BeneparComponent(&quot;benepar_en3&quot;))
else:
    nlp.add_pipe(&quot;benepar&quot;, config={&quot;model&quot;: &quot;benepar_en3&quot;})
doc = nlp(&quot;This is gonna be fun.&quot;)
sent = list(doc.sents)[0]
print(sent._.parse_string)
</code></pre>
<p>It'll output:</p>
<pre><code>(S (NP (DT This)) (VP (VBZ is) (VP (TO gon) (VP (TO na) (VP (VB be) (NP (NN fun)))))) (. .))
</code></pre>
<p>The issue is that the token <code>gonna</code> is split into two tokens <code>gon</code> and <code>na</code>. How can I prevent that?</p>
",Parsing & POS Tagging,prevent benepar parser splitting specific substring parsing string use benepar parser parse sentence tree prevent benepar parser splitting specific substring parsing string e g token split benepar two token want code example pre requisite run output issue token split two token prevent
How to save the output of constituency parsing diagram as an image?,"<p>Using the svgling module I have generated the constituency parse tree. Here is the github link of svgling: <a href=""https://github.com/rawlins/svgling"" rel=""nofollow noreferrer"">svgling module github</a></p>
<pre><code>pip install svgling
import svgling
import nltk
var ='(S (NP this tree) (VP (V is) (AdjP pretty)))'
svgling.disable_nltk_png()
random=svgling.draw_tree(nltk.Tree.fromstring(var))
display(random)

</code></pre>
<p>Using this code I have got the diagram as an output (given below) of the constituency parse tree. I want to generate .png file of this output.
<a href=""https://i.sstatic.net/7BJrG.png"" rel=""nofollow noreferrer"">Constituency parse tree (Output of the above code)</a></p>
<p>To save the output as .png file I have run this following code.</p>
<pre><code>import os
from nltk.tree import Tree
from nltk.draw.tree import TreeView
from nltk.draw.util import CanvasFrame
from nltk.draw import TreeWidget
os.system('Xvfb :1 -screen 0 1600x1200x16  &amp;')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8
os.environ['DISPLAY']=':1.0' 
t = Tree.fromstring('(S (NP this tree) (VP (V is) (AdjP pretty)))')
cf = CanvasFrame()
TreeView(t)._cframe.print_to_file('output.ps')
os.system('convert output.ps output.png')
</code></pre>
<p>However, this code is showing error in google colab <code>TclError: couldn't connect to display &quot;:1.0&quot; </code> and returning a numerical value (ex: 4) in jupyter notebook.</p>
<p>I tried to look into it on several websites but couldn't find any solution.</p>
",Parsing & POS Tagging,save output constituency parsing diagram image using svgling module generated constituency parse tree github link svgling svgling module github using code got diagram output given constituency parse tree want generate png file output constituency parse tree output code save output png file run following code however code showing error google colab returning numerical value ex jupyter notebook tried look several website find solution
How to read constituency based parse tree,"<p>I have a corpus of sentences that were preprocessed by Stanford's <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""noreferrer"">CoreNLP</a> systems. One of the things it provides is the sentence's Parse Tree (Constituency-based). While I can understand a parse tree when it's drawn (like a tree), I'm not sure how to read it in this format:</p>

<p>E.g.:</p>

<pre><code>          (ROOT
          (FRAG
          (NP (NN sent28))
          (: :)
          (S
          (NP (NNP Rome))
          (VP (VBZ is)
          (PP (IN in)
          (NP
          (NP (NNP Lazio) (NN province))
          (CC and)
          (NP
          (NP (NNP Naples))
          (PP (IN in)
          (NP (NNP Campania))))))))
          (. .)))
</code></pre>

<p>The original sentence is:</p>

<pre><code>sent28: Rome is in Lazio province and Naples in Campania .
</code></pre>

<p>How am I supposed to read this tree, or alternatively, is there a code (in python) that does it properly?
Thanks.</p>
",Parsing & POS Tagging,read constituency based parse tree corpus sentence preprocessed stanford corenlp system one thing provides sentence parse tree constituency based understand parse tree drawn like tree sure read format e g original sentence supposed read tree alternatively code python doe properly thanks
Is there a method to load caseless models to Stanford&#39;s NLP sentiment analysis?,"<p>In the Stanford documentation, <a href=""https://stanfordnlp.github.io/CoreNLP/caseless.html"" rel=""nofollow noreferrer"">the authors mention using caseless models to process case-insensitive text.</a> Namely the ability to load the GATE Twitter POS annotator. It is a POS annotator, but it doesn't mention that sentiment analysis processes this model to predict sentiment. How do I load it to CoreNLP and let the sentiment annotator use the more accurate models, not just for part-of-speech tagging?</p>
",Parsing & POS Tagging,method load caseless model stanford nlp sentiment analysis stanford documentation author mention using caseless model process case insensitive text namely ability load gate twitter po annotator po annotator mention sentiment analysis process model predict sentiment load corenlp let sentiment annotator use accurate model part speech tagging
Determining whether a word is a noun or not,"<p>Given an input word, I want to determine whether it is a noun or not (in case of ambiguity, for instance <code>cook</code> can be a noun or a verb, the word must be identified as a noun).</p>

<p>Actually I use the POS tagger from the Stanford Parser (i give it a single word as input, and i extract only the POS tag from the result). The results are quite good but it takes a very long time.</p>

<p>Is there a way (in python, please :) to perform this task quicker than what I do actually?</p>
",Parsing & POS Tagging,determining whether word noun given input word want determine whether noun case ambiguity instance noun verb word must identified noun actually use po tagger stanford parser give single word input extract po tag result result quite good take long time way python please perform task quicker actually
How to get all noun phrases in Spacy,"<p>I am new to <code>Spacy</code> and I would like to extract ""all"" the noun phrases from a sentence. I'm wondering how I can do it. I have the following code:</p>

<pre><code>import spacy

nlp = spacy.load(""en"")

file = open(""E:/test.txt"", ""r"")
doc = nlp(file.read())
for np in doc.noun_chunks:
    print(np.text)
</code></pre>

<p>But it returns only the base noun phrases, that is, phrases which don't have any other <code>NP</code> in them. That is, for the following phrase, I get the result below:</p>

<p>Phrase: <code>We try to explicitly describe the geometry of the edges of the images.</code></p>

<p>Result: <code>We, the geometry, the edges, the images</code>.</p>

<p>Expected result: <code>We, the geometry, the edges, the images, the geometry of the edges of the images, the edges of the images.</code></p>

<p>How can I get all the noun phrases, including nested phrases?</p>
",Parsing & POS Tagging,get noun phrase spacy new would like extract noun phrase sentence wondering following code return base noun phrase phrase following phrase get result phrase result expected result get noun phrase including nested phrase
Extracting sentences from a text document,"<p>I have a text document from which I'd like to extract the Noun phrases. In the first step I extract sentences and then I do a part of speech (pos) tagging for each sentence and then using the pos I do a chunking. I used StanfordNLP for these task, and this is the code for extracting the sentences.</p>

<pre><code>Reader reader = new StringReader(text);
DocumentPreprocessor dp = new DocumentPreprocessor(reader);
</code></pre>

<p>I think <code>DocumentPreprocessor</code> does a pos under the hood in order to extract the sentences. However, I'm doing another pos for extracting the noun phrases in the second phase as well. That is, pos is done twice and because pos is a computationally expensive task, I'm looking for a way to do it only once. Is there any way to do pos only once to extract sentences and noun phrases?</p>
",Parsing & POS Tagging,extracting sentence text document text document like extract noun phrase first step extract sentence part speech po tagging sentence using po chunking used stanfordnlp task code extracting sentence think doe po hood order extract sentence however another po extracting noun phrase second phase well po done twice po computationally expensive task looking way way po extract sentence noun phrase
How to get original token position in string from Stanza constituency parse tree?,"<p>I am using Stanza to extract noun phrases from texts. I am using this code to extract the NPs and store them according to their depth.</p>
<pre><code>nlp = stanza.Pipeline('en', tokenize_pretokenized=True)
sentence_tokens = ['This', 'is', 'a', 'sentence', '.']
doc = nlp(sentence_tokens)
for sent in doc.sentences:
    tree = sent.constituency

    def extract_NPs(tree, np_dict):
        for child in tree.children:
            if child.label=='NP':
                np_dict[child.depth()].append(child)
            np_dict = extract_NPs(child, np_dict)
        return np_dict
    nps = extract_NPs(tree, np_dict=defaultdict(list))
</code></pre>
<p>The output dictionary has the depth as the key, and a list of NP trees with that depth. Each NP is a Tree, described in the Stanza github <a href=""https://github.com/stanfordnlp/stanza/blob/b18e6e80fae7cefbfed7e5255c7ba4ef6f1adae5/stanza/models/constituency/parse_tree.py#L12"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I have combed over the code and documentation, and I cannot seem to find a way to map the text of the NPs back to the position in the original input sentence. Simply finding the index of a token in the sentence_tokens doesn't work for me as many of these sentences have repeat tokens.</p>
<p>Any ideas?</p>
",Parsing & POS Tagging,get original token position string stanza constituency parse tree using stanza extract noun phrase text using code extract np store according depth output dictionary ha depth key list np tree depth np tree described stanza github combed code documentation seem find way map text np back position original input sentence simply finding index token sentence token work many sentence repeat token idea
Spacy get pos &amp; tag for specific word,"<p>I came across a situation where i have to get the pos_ &amp; tag_ from spacy doc objects. </p>

<p>For example, </p>

<pre><code>text = ""Australian striker John hits century""
doc = nlp(text)
for nc in doc.noun_chunks:
    print(nc) #Australian striker John
doc[1].tag_ # gives for striker
</code></pre>

<p>if I want to get <code>pos_</code> &amp; <code>tag_</code> for word 'striker' do I need to again give that sentence to <code>nlp()</code> ??</p>

<p>Also doc[1].tag_ is there, but I need something like doc['striker'].tag_ .. </p>

<p>Is there any possibility ? </p>
",Parsing & POS Tagging,spacy get po tag specific word came across situation get po tag spacy doc object example want get word striker need give sentence also doc tag need something like doc striker tag possibility
Custom spaCy tagger to tag all words that are in a dictionary,"<p>I'm trying spaCy to extract specific information from a text.
So I need to configure a custom tokenizer to identify them and a custom tagger to label all the words that are in an external dictionary in JSON format.</p>
<p>The tokenizer worked on several attempts, but the labeler has been having problems when processing simple text.
I hope that the label I will add to the words is a custom POS-Tag &quot;UNM&quot; and that I can attribute it to token.pos_ like all other labels &quot;NOUN&quot;, &quot;VERB&quot;, etc.</p>
<pre><code>import requests

#keywords dictionary
dictionary = requests.get(
    &quot;https://github.com/dglopes/NBR15575/raw/main/unidades_medidas.json&quot;).json()

    
#Creating the Custom Tagger
Doc.set_extension('pos_tag', default=None, force=True)

@Language.factory(&quot;keyword_pos_tagger&quot;)
class KeywordPosTagger:
   def __init__(self, name, nlp, keywords, pos_tag):
       self.keywords = keywords
       self.pos_tag = pos_tag
       #Doc.set_extension('pos_tag', default=None, force=True)

   def __call__(self, doc):
       for token in doc:
           if token.text in self.keywords:
               token._.pos_tag = self.pos_tag
       return doc

nlp = spacy.load('pt_core_news_md')


keywords = ('m²', 'm2', '(W/K)', 'ºC')
pos_tag = 'UNM' # substitua por seu rótulo POS

keyword_pos_tagger = KeywordPosTagger(nlp, 'keyword_pos_tagger', keywords, pos_tag)

config = {&quot;nlp&quot;: nlp, &quot;keywords&quot;: keywords, &quot;pos_tag&quot;: pos_tag}

nlp.add_pipe('keyword_pos_tagger', config = config)
</code></pre>
<p>&lt;<strong>main</strong>.KeywordPosTagger at 0x78d568e4cee0&gt;</p>
<p>And when I use the custom tagger:</p>
<pre><code>doc = nlp('A temperatura tem 159ºC ou 20 ºC. Também precisa ter 20m de largura e 14 m² de área, caso contrário terá 1 Kelvin (W/K)')
for token in doc:
   print(token.text, token._.pos_tag)
</code></pre>
<p>it returns this error</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-5-3c241e1c89fd&gt; in &lt;cell line: 1&gt;()
----&gt; 1 doc = nlp('A temperatura tem 159ºC ou 20 ºC. Também precisa ter 20m de largura e 14 m² de área, caso contrário terá 1 Kelvin (W/K)')
      2 for token in doc:
      3    print(token.text, token._.pos_tag)

4 frames
/usr/local/lib/python3.10/dist-packages/spacy/tokens/underscore.py in __setattr__(self, name, value)
     74     def __setattr__(self, name: str, value: Any):
     75         if name not in self._extensions:
---&gt; 76             raise AttributeError(Errors.E047.format(name=name))
     77         default, method, getter, setter = self._extensions[name]
     78         if setter is not None:

AttributeError: [E047] Can't assign a value to unregistered extension attribute 'pos_tag'. Did you forget to call the `set_extension` method?
</code></pre>
",Parsing & POS Tagging,custom spacy tagger tag word dictionary trying spacy extract specific information text need configure custom tokenizer identify custom tagger label word external dictionary json format tokenizer worked several attempt labeler ha problem processing simple text hope label add word custom po tag unm attribute token po like label noun verb etc main keywordpostagger x e cee use custom tagger return error
suPar constituency parser with POS tags,"<p>I have trained a <a href=""https://pypi.org/project/supar/"" rel=""nofollow noreferrer"">SuPar Constituency Parser</a> with my own dataset. I have a POS tagger model, too. How can I get an output that shows POS tags in it? The supar constituency parser outputs a sentence tree without POS tags.
example:</p>
<pre><code>&gt;&gt;&gt; con = Parser.load('crf-con-en')
&gt;&gt;&gt; con.predict(['I', 'saw', 'Sarah', 'with', 'a', 'telescope', '.'], verbose=False)[0].pretty_print()
              TOP                       
               |                         
           S                        
  _____________|______________________   
 |             VP                     | 
 |    _________|____                  |  
 |   |    |         PP                | 
 |   |    |     ____|___              |  
 NP  |    NP   |        NP            | 
 |   |    |    |     ___|______       |  
 _   _    _    _    _          _      _ 
 |   |    |    |    |          |      |  
 I  saw Sarah with  a      telescope  . 
</code></pre>
<p>I know that there is a tag argument in the constituency parse model (which can be seen <a href=""https://github.com/yzhangcs/parser/blob/main/supar/models/const/crf/model.py"" rel=""nofollow noreferrer"">here</a>), but I don't know how to use it.</p>
",Parsing & POS Tagging,supar constituency parser po tag trained supar constituency parser dataset po tagger model get output show po tag supar constituency parser output sentence tree without po tag example know tag argument constituency parse model seen know use
Using Python to find noun phrase and dependency,"<p>I'm trying to find a noun phrase from a set of sentences and output whether the sentence is true or false based on its presence.</p>
<p>For an example, following are four samples</p>
<pre><code>s1 = &quot;he reported red light&quot;
s2 = &quot;he found a red light at the end of the road&quot;
s3 = &quot;he denied to have seen red light&quot;
s4 = &quot;he said that the red light was absent&quot;
</code></pre>
<p>The output which I'm looking for is &quot;True&quot; for s1 and s2 (because from the sentence it is clear that the red light is present&quot;, whereas &quot;False&quot; for s3 &amp; s4, when the input string is &quot;red light&quot;</p>
<p>I tried using TextBlob which returns &quot;red light&quot; as noun phrase.</p>
<pre><code>print(TextBlob(s1).sentences[0].pos_tags)
print(TextBlob(s1).sentences[0].noun_phrases)
</code></pre>
<p>The example above are samples which can be part of a long sentence also.
Also, there can be combinations of positive words like &quot;present&quot;,&quot;found&quot;, &quot;spotted&quot;,&quot;has&quot; etc, same is the case with negative words</p>
<p>Also, if there are multiple noun phrases, but I want to find the presence of a particular noun phrase, how can I do it?</p>
",Parsing & POS Tagging,using python find noun phrase dependency trying find noun phrase set sentence output whether sentence true false based presence example following four sample output looking true sentence clear red light present whereas false input string red light tried using textblob return red light noun phrase example sample part long sentence also also combination positive word like present found spotted ha etc case negative word also multiple noun phrase want find presence particular noun phrase
Dependency parsing as graph,"<p>I am using <code>spacy</code> for dependency parsing which I can visualize also .
Code snippet</p>
<pre><code>import spacy
from spacy import displacy

nlp = spacy.load(&quot;en_core_web_sm&quot;)
doc = nlp(&quot;This is a sentence.&quot;)
displacy.serve(doc, style=&quot;dep&quot;) 
</code></pre>
<p>Is it possible somehow I store this in any graph DB - say <code>neo4j</code>. If yes, then how? Also can this be done on paragraph level (multiple sentences).</p>
<p>My idea is to store multiple texts/paragraph into graph DB and apply graph algorithm for example <code>graph similarity</code>.</p>
<p>Does it make also sense to apply standard graph algorithm on dependency parsing ?</p>
<p>Thanks in advance</p>
",Parsing & POS Tagging,dependency parsing graph using dependency parsing visualize also code snippet possible somehow store graph db say yes also done paragraph level multiple sentence idea store multiple text paragraph graph db apply graph algorithm example doe make also sense apply standard graph algorithm dependency parsing thanks advance
Is there a parsing algorithm for languages generated by context-sensitive grammars?,"<h2>Details</h2>
<p>I am already familiar with Chart Parsing using <a href=""https://en.wikipedia.org/wiki/Earley_parser"" rel=""nofollow noreferrer"">Earley Parsers</a>. I am however, unsure about whether there even is a meta-syntax like <a href=""https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form"" rel=""nofollow noreferrer"">Backus-Naur Form</a> (BNF) that can represent a <a href=""https://en.wikipedia.org/wiki/Context-sensitive_grammar"" rel=""nofollow noreferrer"">Context-Sensitive Grammar (CSG)</a>. Since parsing using a <a href=""https://en.wikipedia.org/wiki/Linear_bounded_automaton"" rel=""nofollow noreferrer"">Linear Bounded Automaton</a> is non-deterministic, I understand that it may be very inefficient.</p>
<h2>Goal</h2>
<p>My goal is to create a Probabilistic Context-Sensitive Grammar generator from a corpus (in the same fashion as a <a href=""https://en.wikipedia.org/wiki/Probabilistic_context-free_grammar"" rel=""nofollow noreferrer"">PCFG</a>). Ambiguous grammars will handled by generating parse forests and possibly utilizing something like <a href=""https://www.sciencedirect.com/science/article/pii/S1571066108001497"" rel=""nofollow noreferrer"">SPPF</a> if there is an equivalent of Earley Parsers for CSGs.</p>
<h2>What I've Tried</h2>
<p>I was recommended <a href=""https://datatracker.ietf.org/doc/rfc5234/"" rel=""nofollow noreferrer"">ABNF</a>, but it's not obvious if it supports CSGs. Another recommendation was to use Dependency Parsing instead of Constituency Parsing. But that is an entirely different approach from what I want to solve.</p>
<p>This <a href=""https://stackoverflow.com/questions/29482667/how-to-parse-context-sensitive-grammar"">answer</a> seems to indicate that simply simulating an LBA with backtracking is enough. However, again that seems incredibly inefficient.</p>
",Parsing & POS Tagging,parsing algorithm language generated context sensitive grammar detail already familiar chart parsing using earley parser however unsure whether even meta syntax like backus naur form bnf represent context sensitive grammar csg since parsing using linear bounded automaton non deterministic understand may inefficient goal goal create probabilistic context sensitive grammar generator corpus fashion pcfg ambiguous grammar handled generating parse forest possibly utilizing something like sppf equivalent earley parser csgs tried wa recommended abnf obvious support csgs another recommendation wa use dependency parsing instead constituency parsing entirely different approach want solve href seems indicate simply simulating lba backtracking enough however seems incredibly inefficient p
What is CoNLL data format?,"<p>I am using a open source jar (Mate Parser) which outputs in the CoNLL 2009 format after dependency parsing. I want to use the dependency parsing results for Information Extraction, however, I only understand part of the output in the CoNLL data format.</p>
<p>Can someone explain the CoNLL data format?</p>
",Parsing & POS Tagging,conll data format using open source jar mate parser output conll format dependency parsing want use dependency parsing result information extraction however understand part output conll data format someone explain conll data format
Return list of sentences with a particular subject,"<p>I am exploring a small corpus of texts, and one of the things I am doing is examining the actions associated with various subjects. I have already inventoried how many times, for example, &quot;man&quot; is the subject of a sentence in which the verb is &quot;love&quot;: that work was done with subject-verb-object triplets using Textacy.</p>
<p>As I work through the various statistics, I would like to be able to go back into the data and see sentences that have the subjects in their original context. NLTK has a concordance feature built right in, but it does not pay attention to part-of-speech tagging. I have gotten this far with the code.</p>
<p>What I am trying to do is <code>find_the_subject(&quot;noun&quot;, corpus)</code>, such that if I input &quot;man&quot; I would get back a list of sentences with man as the subject of the sentence:</p>
<blockquote>
<p>A man walked down the street and said why am I short in the middle?</p>
<p>The man comes around.</p>
</blockquote>
<p>So far, I have the following code which will grab all the sentences with &quot;man&quot; but not just the ones with man as subject.</p>
<pre class=""lang-py prettyprint-override""><code>def find_sentences_with_noun(subject_noun, sentences):
    # Start with two empty lists
    noun_subjects = []
    noun_sentences = []
    # Work through the sentences
    for sentence in sentences:
        words = word_tokenize(sentence)
        tagged_words = nltk.tag.pos_tag(words)
        # This works but doesn't get me the subject
        for word, tag in tagged_words:
            if &quot;NN&quot; in tag and word == subject_noun:
                noun_subjects.append(word)
                noun_sentences.append(sentence)
    return noun_sentences
</code></pre>
<p>I cannot for the life of me figure out how to grab the noun in the subject position.</p>
",Parsing & POS Tagging,return list sentence particular subject exploring small corpus text one thing examining action associated various subject already inventoried many time example man subject sentence verb love work wa done subject verb object triplet using textacy work various statistic would like able go back data see sentence subject original context nltk ha concordance feature built right doe pay attention part speech tagging gotten far code trying input man would get back list sentence man subject sentence man walked street said short middle man come around far following code grab sentence man one man subject life figure grab noun subject position
Parsing NLTK Chart diagram,"<p>I am trying to do parsing using context free grammar which is returning multiple parse trees.
I can visualize these parse trees one by one using the code below:</p>
<pre><code>grammar = nltk.CFG.fromstring(&quot;&quot;&quot;
  S   -&gt; VP NP PUN | NP VP PUN | NP PUN | VP PUN
  NP  -&gt; ADP NP | NOUN | ADJ NP | ADV ADJ NP |ADJ | CONJ NP | NOUN NP | DET NP |ADV
  VP  -&gt; VERB | ADV VP | ADV | VERB VP | VERB NP
  
  ADP -&gt; 'as' | 'in' | 'along' | 'with' | 'of' | 'a'
  NOUN -&gt;  'three' | 'parts' | 'milk' | 'oak' | 'body' | 'center' | 'stage' | 'notes' | 'date' | 'rice' | 'structure' | 'a' | 'hint' | 'blossom' | 'candy' | 'chocolate' | 'lemon' | 'thyme' | 'espresso' | 'cacao' | 'tart' | 'honeysuckle' | 'citrus' | 'apricot' | 'finish' | 'paste' | 'grapefruit' | 'cherry' | 'mouthfeel' | 'spice' | 'cup' | 'vanilla' | 'narcissus' | 'savory-tart' | 'aroma' | 'leads' | 'zest' | 'herb' | 'florals' | 'tones' | 'peppercorn' | 'intimations' | 'nib'
  PUN -&gt; '.' | ',' | ';' | '(' | ')'
  DET -&gt; 'the' | 'all' | 'a'
  ADV -&gt; 'as' | 'deeply' | 'long' | 'all' | 'gently' | 'crisply' | 'richly' | 'delicately' | 'sweetly' 
  VERB -&gt; 'evaluated' | 'take' | 'notes' | 'follow' | 'roasted' | 'dried' | 'fruit' | 'finish' | 'drying' | 'leads' | 'intensify'
  X -&gt; 'a'
  CONJ -&gt; 'and'
  PRT -&gt; 'in'
  ADJ -&gt; 'rich' | 'dark' | 'black' | 'short' | 'small' | 'sweet' | 'white' | 'floral' | 'silky' | 'pink' | 'thyme-like' | 'syrupy' | 'roasted' | 'dried' | 'floral-toned' | 'chocolaty' | 'sweet-toned' | 'cocoa-toned' | 'plush' | 'floral-driven' | 'herb-toned' | 'delicate' | 'resonant' | 'flavor-saturated'
  &quot;&quot;&quot;)

statement = nltk.word_tokenize(&quot;Crisply sweet cocoa-toned Lemon blossom roasted cacao nib date rice candy white peppercorn in aroma and cup.&quot;)
statement = [i.lower().strip() for i in statement]

rd_parser = nltk.RecursiveDescentParser(grammar)
for pos, tree in enumerate(rd_parser.parse(statement)):
    tree.draw() 
</code></pre>
<p>But the problem is that it generates the tree diagram one by one and the program stops untill I stop all of the charts manually. Is there a way by which I can generate all the charts into single image and display it in my jupyter notebook all at once?</p>
",Parsing & POS Tagging,parsing nltk chart diagram trying parsing using context free grammar returning multiple parse tree visualize parse tree one one using code problem generates tree diagram one one program stop untill stop chart manually way generate chart single image display jupyter notebook
How to get pos-tag lemmatiser to iterate through df,"<p>I want to use POS-labelling and lemmatisation on my text data. I've found this example code from kaggle. This applies it to a sentence, but I want to modify this code in order to apply it to a column of a dataframe.</p>
<pre><code>#Kaggle example code: 
from nltk import pos_tag
from nltk.stem import WordNetLemmatizer

wnl = WordNetLemmatizer()

def penn2morphy(penntag):
    &quot;&quot;&quot; Converts Penn Treebank tags to WordNet. &quot;&quot;&quot;
    morphy_tag = {'NN':'n', 'JJ':'a',
                  'VB':'v', 'RB':'r'}
    try:
        return morphy_tag[penntag[:2]]
    except:
        return 'n' # if mapping isn't found, fall back to Noun.
    
# `pos_tag` takes the tokenized sentence as input, i.e. list of string,
# and returns a tuple of (word, tg), i.e. list of tuples of strings
# so we need to get the tag from the 2nd element.

walking_tagged = pos_tag(word_tokenize('He is walking to school'))
#print(walking_tagged)

testing[&quot;text&quot;].apply(penn2morphy)
#[wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in walking_tagged]
</code></pre>
<p>I presumed you would just use the apply function but that doesnt work. The first line where the pos_tag is being applied is just labelling each row as n, so i presume it isnt iterating through each row.</p>
<pre><code>#Example data
    r1 = [&quot;he, has, a, glass, of, water, together, with, a, mirror&quot;],&quot;Pass&quot;
    r2 = [&quot;lamp, lens, right, left&quot;], &quot;Fail&quot;
    r3 = [&quot;candle, clock, vase, spoon&quot;], &quot;Fail&quot;
    d=(r1,r2,r3)
    ex_df = pd.DataFrame(d, columns=[&quot;col1&quot;, &quot;col2&quot;])

walking_tagged2 = ex_df[&quot;col1&quot;].apply(pos_tag)
[wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in walking_tagged2]
</code></pre>
<p>Any ideas? Thank you</p>
",Parsing & POS Tagging,get po tag lemmatiser iterate df want use po labelling lemmatisation text data found example code kaggle applies sentence want modify code order apply column dataframe presumed would use apply function doesnt work first line po tag applied labelling row n presume isnt iterating row idea thank
Spacy - Force POS tag for quoted text,"<p>I want Spacy to treat quoted text as e.g. a NOUN or PROPN so that it forms a compound with the adjacent word. For example in the following text I want the text &quot;drink me&quot; to be attached to the word &quot;label&quot;.</p>
<pre><code>Find the bottle which has the label &quot;drink me&quot;.
</code></pre>
<p>I can successfully merge all the text inside the quotes into a single token but the token is simply tagged as PUNCT (since this is the POS of the first token of the merged group) but then it gets inserted in the wrong place in the dependency tree. I need to be able to modify the POS so that it goes to the right place in the tree.</p>
<p><a href=""https://i.sstatic.net/EJJCS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EJJCS.png"" alt=""dependency tree"" /></a></p>
",Parsing & POS Tagging,spacy force po tag quoted text want spacy treat quoted text e g noun propn form compound adjacent word example following text want text drink attached word label successfully merge text inside quote single token token simply tagged punct since po first token merged group get inserted wrong place dependency tree need able modify po go right place tree
How to apply PoS tags to a nested list in R across multiple rows?,"<p>I am trying to apply PoS tagging in R to a column that contains a variable that was a sentence. This variable has been tokenised with all irrelavanices removed (punctuation, spaces, spelling etc)</p>
<p>Here are the first few rows of the data</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Row_id</th>
<th>no_punct</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>[dec,brighton,hove,women,have,applied]</td>
</tr>
<tr>
<td>2</td>
<td>[jan,england,has,joined,tottenha,from,chelsea]</td>
</tr>
</tbody>
</table>
</div>
<p>My code so far looks like:</p>
<pre><code>udmodel &lt;- udpipe_download_model(language = &quot;english&quot;)
udmodel &lt;- udpipe_load_model(file = udmodel$file_model)

df$pos_tags &lt;- udpipe_annotate(udmodel, 
                    df$no_punct_no_spaces)
</code></pre>
<p>However, the output is reading as 'NULL'. My desired output is to create a new tuples variable named 'pos_tags' that contains the word and the tag (i.e. &quot;dec, N&quot;) across each row.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Row_id</th>
<th>pos_tags</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>[dec NN,brighton NN,hove NN,women NN,have AD,applied PN]</td>
</tr>
</tbody>
</table>
</div>
<p>Which package in R would you suggest to do this with?
And if you could provide a code example that'd be great.</p>
<p>Thank you</p>
<p>My code so far looks like:</p>
<pre><code>udmodel &lt;- udpipe_download_model(language = &quot;english&quot;)
udmodel &lt;- udpipe_load_model(file = udmodel$file_model)

df$pos_tags &lt;- udpipe_annotate(udmodel, 
                    df$no_punct_no_spaces)
</code></pre>
<p>However, the output is reading as 'NULL'.</p>
",Parsing & POS Tagging,apply po tag nested list r across multiple row trying apply po tagging r column contains variable wa sentence variable ha tokenised irrelavanices removed punctuation space spelling etc first row data row id punct dec brighton hove woman applied jan england ha joined tottenha chelsea code far look like however output reading null desired output create new tuples variable named po tag contains word tag e dec n across row row id po tag dec nn brighton nn hove nn woman nn ad applied pn package r would suggest could provide code example great thank code far look like however output reading null
Dependency parsing tree in Spacy,"<p>I have a sentence <strong>John saw a flashy hat at the store</strong><br>
How to represent this as a dependency tree as shown below?</p>

<pre><code>(S
      (NP (NNP John))
      (VP
        (VBD saw)
        (NP (DT a) (JJ flashy) (NN hat))
        (PP (IN at) (NP (DT the) (NN store)))))
</code></pre>

<p>I got this script from <a href=""https://stackoverflow.com/questions/36610179/how-to-get-the-dependency-tree-with-spacy"">here</a></p>

<pre><code>import spacy
from nltk import Tree
en_nlp = spacy.load('en')

doc = en_nlp(""John saw a flashy hat at the store"")

def to_nltk_tree(node):
    if node.n_lefts + node.n_rights &gt; 0:
        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])
    else:
        return node.orth_


[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]
</code></pre>

<p>I am getting the following but I am looking for a tree(NLTK) format. </p>

<pre><code>     saw                 
  ____|_______________    
 |        |           at 
 |        |           |   
 |       hat        store
 |     ___|____       |   
John  a      flashy  the
</code></pre>
",Parsing & POS Tagging,dependency parsing tree spacy sentence john saw flashy hat store represent dependency tree shown got script href pre getting following looking tree nltk format
Saving nltk drawn parse tree to image file,"<p><img src=""https://i.sstatic.net/0wi0Q.png"" alt=""enter image description here""></p>

<p>Is there any way to save the draw image from tree.draw() to an image file programmatically? I tried looking through the documentation, but I couldn't find anything.</p>
",Parsing & POS Tagging,saving nltk drawn parse tree image file way save draw image tree draw image file programmatically tried looking documentation find anything
Is there a way to get entire constituents using SpaCy?,"<p>I guess I'm trying to navigate SpaCy's parse tree in a more blunt way than is provided.</p>

<p>For instance, if I have sentences like: ""He was a genius"" or ""The dog was green,"" I want to be able to save the objects to variables (""a genius"" and ""green""). </p>

<p>token.children provides the IMMEDIATE syntactic dependents, so, for the first example, the children of ""was"" are ""he"" and ""genius,"" and then ""a"" is a child of ""genius."" This isn't so helpful if I just want the entire constituent ""a genius."" I'm not sure how to reconstruct it from the token.children or if there's a better way.</p>

<p>I can figure out how to match ""is"" and ""was"" using token.text (part of what I'm trying to do), but I can't figure out how to return the whole constituent ""a genius"" using the info provided about children.</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_sm')

sent = nlp(""He was a genius."")

for token in sent:
     print(token.text, token.tag_, token.dep_, [child for child in token.children])
</code></pre>

<h1>This is the output:</h1>

<p>He PRP nsubj []</p>

<p>was VBD ROOT [He, genius, .]</p>

<p>a DT det []</p>

<p>genius NN attr [a]</p>

<p>. . punct []</p>
",Parsing & POS Tagging,way get entire constituent using spacy guess trying navigate spacy parse tree blunt way provided instance sentence like wa genius dog wa green want able save object variable genius green token child provides immediate syntactic dependent first example child wa genius child genius helpful want entire constituent genius sure reconstruct token child better way figure match wa using token text part trying figure return whole constituent genius using info provided child output prp nsubj wa vbd root genius dt det genius nn attr punct
How to count the number of nouns from Spacy from a dataframe column?,"<p>I have a dataframe like that (as an example).</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>text</th>
</tr>
</thead>
<tbody>
<tr>
<td>I left the country.</td>
</tr>
<tr>
<td>Andrew is from America and he loves apples.</td>
</tr>
</tbody>
</table>
</div>
<p>I want to add a new column, number of nouns, where Spacy should count the NOUNS pos tags. How do I convert that in Python?</p>
<pre><code>import pandas as pd
import spacy

# the dataframe

# NLP Spacy with POS tags
nlp = spacy.load(&quot;en_core_web_sm&quot;)
</code></pre>
<p>My question is, how to apply nlp on the &quot;text&quot; column, check if the pos is NOUN and count it and give it as a feature?</p>
<p>Thanks!</p>
",Parsing & POS Tagging,count number noun spacy dataframe column dataframe like example text left country andrew america love apple want add new column number noun spacy count noun po tag convert python question apply nlp text column check po noun count give feature thanks
Extract Noun Phrases with Stanza and CoreNLPClient,"<p>I am trying to extract noun phrases from sentences using Stanza(with Stanford CoreNLP). This can only be done with the CoreNLPClient module in Stanza. </p>

<pre><code># Import client module
from stanza.server import CoreNLPClient
# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001
client = CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse'], memory='4G', endpoint='http://localhost:9001')
</code></pre>

<p>Here is an example of a sentence, and I am using the <code>tregrex</code> function in client to get all the noun phrases. <code>Tregex</code> function returns a <code>dict of dicts</code> in python. Thus I needed to process the output of the <code>tregrex</code> before passing it to the <code>Tree.fromstring</code> function in NLTK to correctly extract the Noun phrases as strings. </p>

<pre><code>pattern = 'NP'
text = ""Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity.""
matches = client.tregrex(text, pattern) ``
</code></pre>

<p>Hence, I came up with the method <code>stanza_phrases</code> which has to loop through the <code>dict of dicts</code> which is the output of <code>tregrex</code> and correctly format for <code>Tree.fromstring</code> in NLTK.</p>

<pre><code>def stanza_phrases(matches):
  Nps = []
  for match in matches:
    for items in matches['sentences']:
      for keys,values in items.items():
        s = '(ROOT\n'+ values['match']+')'
        Nps.extend(extract_phrase(s, pattern))
  return set(Nps)
</code></pre>

<p>generates a tree to be used by NLTK </p>

<pre><code>from nltk.tree import Tree
def extract_phrase(tree_str, label):
    phrases = []
    trees = Tree.fromstring(tree_str)
    for tree in trees:
        for subtree in tree.subtrees():
            if subtree.label() == label:
                t = subtree
                t = ' '.join(t.leaves())
                phrases.append(t)

    return phrases
</code></pre>

<p>Here is my output:</p>

<pre><code>{'Albert Einstein', 'He', 'a German-born theoretical physicist', 'relativity',  'the theory', 'the theory of relativity'}
</code></pre>

<p>Is there a way I can make this more code efficient with less number of lines (especially, <code>stanza_phrases</code> and <code>extract_phrase</code> methods)</p>
",Parsing & POS Tagging,extract noun phrase stanza corenlpclient trying extract noun phrase sentence using stanza stanford corenlp done corenlpclient module stanza example sentence using function client get noun phrase function return python thus needed process output passing function nltk correctly extract noun phrase string hence came method ha loop output correctly format nltk generates tree used nltk output way make code efficient le number line especially method
"Generate modifiers for a keyword using dependency parsing, NLP","<p>I have a dataframe containing reviews of a particular product with the columns month and review. I want perform a type of text analysis on the review column, whereby you can query the for a particular keyword and it will return a list of modifiers for that keyword.</p>
<pre><code>df = pd.DataFrame({'month': ['Jan', 'Feb', 'Mar', 'Apr', 'Apr'],
                   'review': ['there should be 'share' button on each item. right now when my wife wants me to buy her something, she has to dictate the item id which is horrendous.', 'always nice but high prices', 'this app currently needs more than 3 gigs of space on my phone. that is ridiculous. guess it has to go. /edit cool, trying again, thanks for the answer.', 'impossible to login in the app, is there any way to get the barcode of the card? if i click the link in the email for the card print thingy it just shows a broken image.', 'i cannot change my location and language preference'],
                   'sentiment': [&quot;positive&quot;, &quot;negative&quot;, &quot;positive&quot;, &quot;negative&quot;, &quot;neutral&quot;]})
</code></pre>
<p>For example, say the reviews dataset is from the hospitality industry, and performed sentiment analysis. Upon checking the most frequent words in the positive and negative reviews, you got this.</p>
<p><strong>Positive</strong>: hotel, location, staff, view, room, breakfast</p>
<p><strong>Negative</strong>: hotel, staff, room, breakfast, window, bed, Wi-Fi</p>
<p>You wanted to go deeper into the analysis and uncover exactly what it was about these objects that were – or were not – working as expected by customers. For example, why were windows such a prominent aspect of negative reviews?</p>
<p>So, you  set out to create a syntactic dependency tree, which connects all terms in the input text according to their syntactic relation. Then, you queried this tree to pinpoint precisely what it was about a given keyword (for example, &quot;room&quot; or &quot;location&quot;) that customers did or did not especially like (<strong>this is where I need help, I don't know how to implement this in code</strong>)</p>
<p>I want a resulting list of modifiers so I can create word clouds to visualize the frequency of each modifier for the given keyword, such as the word cloud below, for the keyword &quot;room&quot;:</p>
<p><a href=""https://i.sstatic.net/izB1F.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Honestly I don't even know where to start, I'm currently working with spaCy's dependency parsing to see how it works and what it returns. So while I do that, I also seek help from here.</p>
",Parsing & POS Tagging,generate modifier keyword using dependency parsing nlp dataframe containing review particular product column month review want perform type text analysis review column whereby query particular keyword return list modifier keyword example say review dataset hospitality industry performed sentiment analysis upon checking frequent word positive negative review got positive hotel location staff view room breakfast negative hotel staff room breakfast window bed wi fi wanted go deeper analysis uncover exactly wa object working expected customer example window prominent aspect negative review set create syntactic dependency tree connects term input text according syntactic relation queried tree pinpoint precisely wa given keyword example room location customer especially like need help know implement code want resulting list modifier create word cloud visualize frequency modifier given keyword word cloud keyword room enter image description honestly even know start currently working spacy dependency parsing see work return also seek help
How to extract noun-based compound words from a sentence using Python?,"<p>I'm using <code>nltk</code> via the following code to extract nouns from a sentence:</p>
<pre class=""lang-py prettyprint-override""><code>words = nltk.word_tokenize(sentence)
tags = nltk.pos_tag(words)
</code></pre>
<p>And then I choose the words tagged with the <code>NN</code> and <code>NNP</code> Part of Speech (PoS) tags. However, it only extracts single nouns like &quot;book&quot; and &quot;table&quot;, yet ignores the pair of nouns like &quot;basketball shoe&quot;. What should I do to expand the results to contain such compond noun pairs?</p>
",Parsing & POS Tagging,extract noun based compound word sentence using python using via following code extract noun sentence choose word tagged part speech po tag however extract single noun like book table yet ignores pair noun like basketball shoe expand result contain compond noun pair
Python - Loop to replace elements in string with second element of elements in second string?,"<p>I am trying to replace elements that are in one string with the second element (also string type) of elements in a second string.</p>
<p><code>a_list</code> and <code>replace</code> are short versions of the lists that I am working with. I want to replace the elements in the first list <code>a_list</code> with the first elements of each list in the second list, if the second element (POS tags) of each list in the second list matches and then check the rest of the list.</p>
<p>Here is an example part of the lists that I am working with and my not working function:</p>
<pre><code>a_list = [&quot;JJ&quot;,&quot;MM&quot;,&quot;JJ&quot;]

replace = [['Innovative', 'JJ'],['methods', 'NNS'],['test', 'MM'],['extra', 'JJ']]

new_list = []

for item in a_list:
    if a_list[i] == replace[i][1]:
        new_list.append(item.replace(replace[i][1], replace[i][0]))
        
    else:
        new_list.append(item)

print(new_list)  

['JJ', 'MM', 'JJ']
</code></pre>
<p>what I want: <code>['Innovative', &quot;test&quot;, &quot;extra&quot;]</code></p>
<p>a_list, a_list[1], replace, replace[1], and replace[1][1] are all strings. They were tuples but those were harder for me to work with.</p>
<p>Not sure how to remove that specific string after it was used to replace and not sure how to iterate over both strings. <code>replace</code> is a lot longer than <code>a_list</code>. I have been working on this for a couple of hours now I'm sure there is something that can do this - just need help, it seems so close! There are many similar questions but I have tried and they have not worked for me</p>
<p>Thank you!</p>
",Parsing & POS Tagging,python loop replace element string second element element second string trying replace element one string second element also string type element second string short version list working want replace element first list first element list second list second element po tag list second list match check rest list example part list working working function want list list replace replace replace string tuples harder work sure remove specific string wa used replace sure iterate string lot longer working couple hour sure something need help seems close many similar question tried worked thank
Extracting noun+noun or (adj|noun)+noun from Text,"<p>Is it possible to extract <code>noun+noun</code> or <code>(adj|noun)+noun</code> using the R package <code>openNLP</code>? That is, I would like to use linguistic filtering to extract candidate noun phrases. Could you direct me how to do?
Many thanks.</p>
<hr />
<p>Thanks for the responses.
here is the code:</p>
<pre><code>library(&quot;openNLP&quot;)

acq &lt;- &quot;Gulf Applied Technologies Inc said it sold its subsidiaries engaged in
        pipeline and terminal operations for 12.2 mln dlrs. The company said 
        the sale is subject to certain post closing adjustments, 
        which it did not explain. Reuter.&quot; 

acqTag &lt;- tagPOS(acq)    
acqTagSplit = strsplit(acqTag,&quot; &quot;)
acqTagSplit

qq = 0
tag = 0

for (i in 1:length(acqTagSplit[[1]])){
    qq[i] &lt;-strsplit(acqTagSplit[[1]][i],'/')
    tag[i] = qq[i][[1]][2]
}

index = 0

k = 0

for (i in 1:(length(acqTagSplit[[1]])-1)) {
    
    if ((tag[i] == &quot;NN&quot; &amp;&amp; tag[i+1] == &quot;NN&quot;) | 
        (tag[i] == &quot;NNS&quot; &amp;&amp; tag[i+1] == &quot;NNS&quot;) | 
        (tag[i] == &quot;NNS&quot; &amp;&amp; tag[i+1] == &quot;NN&quot;) | 
        (tag[i] == &quot;NN&quot; &amp;&amp; tag[i+1] == &quot;NNS&quot;) | 
        (tag[i] == &quot;JJ&quot; &amp;&amp; tag[i+1] == &quot;NN&quot;) | 
        (tag[i] == &quot;JJ&quot; &amp;&amp; tag[i+1] == &quot;NNS&quot;))
    {      
            k = k +1
            index[k] = i
    }

}

index
</code></pre>
<hr />
<p>Reader can refer <strong>index</strong> on <strong>acqTagSplit</strong> to do <code>noun+noun</code> or <code>(adj|noun)+noun</code> extraction. (The code is not optimal, but it works. If you have any idea, please let me know.)</p>
<p>I have an additional problem:</p>
<p>Justeson and Katz (1995) proposed another linguistic filtering to extract candidate noun phrases:</p>
<pre class=""lang-none prettyprint-override""><code>((Adj|Noun)+|((Adj|Noun)*(Noun-Prep)?)(Adj|Noun)*)Noun
</code></pre>
<p>I cannot understand its meaning well. Could you do me a favor and explain it? Or show how to code the filtering rule in the R language?
Many thanks.</p>
",Parsing & POS Tagging,extracting noun noun adj noun noun text possible extract using r package would like use linguistic filtering extract candidate noun phrase could direct many thanks thanks response code reader refer index acqtagsplit extraction code optimal work idea please let know additional problem justeson katz proposed another linguistic filtering extract candidate noun phrase understand meaning well could favor explain show code filtering rule r language many thanks
"Extracting &quot;((Adj|Noun)+|((Adj|Noun)(Noun-Prep)?)(Adj|Noun))Noun&quot; from Text (Justeson &amp; Katz, 1995)","<p>Is it possible to extract <code>((Adj|Noun)+|((Adj|Noun)(Noun-Prep)?)(Adj|Noun))Noun</code> proposed by Justeson and Katz (1995) using the R package <code>openNLP</code>?</p>
<p>That is, I would like to use this linguistic filtering to extract candidate noun phrases.</p>
<p>I cannot understand its meaning well.</p>
<p>Could you do me a favor to explain it? Or show how to code the filtering rule in the R language?</p>
<p>Many thanks.</p>
<h2>Maybe we can start the sample code from:</h2>
<pre><code>library(&quot;openNLP&quot;)  

acq &lt;- &quot;This paper describes a novel optical thread plug
gauge (OTPG) for internal thread inspection using machine
vision. The OTPG is composed of a rigid industrial
endoscope, a charge-coupled device camera, and a two
degree-of-freedom motion control unit. A sequence of
partial wall images of an internal thread are retrieved and
reconstructed into a 2D unwrapped image. Then, a digital
image processing and classification procedure is used to
normalize, segment, and determine the quality of the
internal thread.&quot; 

acqTag &lt;- tagPOS(acq)     

acqTagSplit = strsplit(acqTag,&quot; &quot;)
</code></pre>
<p>I was told to open a new question for this. The original question is <a href=""https://stackoverflow.com/questions/4600612/extracting-nounnoun-or-adjnounnoun-from-text"">here</a>.</p>
",Parsing & POS Tagging,extracting adj noun adj noun noun prep adj noun noun text justeson katz possible extract proposed justeson katz using r package would like use linguistic filtering extract candidate noun phrase understand meaning well could favor explain show code filtering rule r language many thanks maybe start sample code wa told open new question original question href
How to extract only the desired text using for loop,"<p>So I have list of Tibetan words with their POS tag as shown below:</p>
<pre><code>ད་གདོད DET
ད་གཟོད PART
ད་ཏྲིག NO_POS
ད་དུང PART
ད་དྲག NOUN
</code></pre>
<p>How can I strip out these POS tags and output only the below:</p>
<pre><code>ད་གདོད
ད་གཟོད
ད་ཏྲིག 
ད་དུང 
ད་དྲག 
</code></pre>
",Parsing & POS Tagging,extract desired text using loop list tibetan word po tag shown strip po tag output
Change tokenizer when loading Dependency Parsing model from AllenNLP,"<p>I am using a pretrained dependency parsing model from AllenNLP, namely <a href=""https://demo.allennlp.org/dependency-parsing"" rel=""nofollow noreferrer"">this one</a>.</p>
<p>I have the sentence <code>How do I find work-life balance?</code>, and when extracting the dependency graph, the tokenizer used by the AllenNLP model splits the sentence as <code>['How', 'do', 'I', 'find', 'work', '-', 'life', 'balance', '?']</code>. However, I would prefer to split the sentence as <code>['How', 'do', 'I', 'find', 'work-life', 'balance', '?']</code> (notice <code>work-life</code> as a single word) as given by the function <code>word_tokenize</code> from NLTK.</p>
<p>Is there a way to change the tokenizer used by the pretrained model? Was the model trained using a tokenizer that always splits the hyphenated words? I cannot find the answers in the official documentation. Thanks in advance for any help you can provide.</p>
",Parsing & POS Tagging,change tokenizer loading dependency parsing model allennlp using pretrained dependency parsing model allennlp namely one sentence extracting dependency graph tokenizer used allennlp model split sentence however would prefer split sentence notice single word given function nltk way change tokenizer used pretrained model wa model trained using tokenizer always split hyphenated word find answer official documentation thanks advance help provide
Finding Tense of A sentence using stanford nlp,"<p>Q1.I am trying to get tense of a complete sentence,just don't know how to do it using nlp.
Any help appreciated.</p>

<p>Q2 .What all information can be extracted from a sentence using nlp?</p>

<p>Currently I can,
I get : 1.Voice of sentence
        2.subject object verb
        3.POS tags.</p>

<p>Any more info can be extracted please let me know.</p>
",Parsing & POS Tagging,finding tense sentence using stanford nlp q trying get tense complete sentence know using nlp help appreciated q information extracted sentence using nlp currently get voice sentence subject object verb po tag info extracted please let know
spaCy token.tag_ full list,"<p>The official documentation of <a href=""https://spacy.io/docs#token-postags"" rel=""noreferrer""><code>token.tag_</code></a> in <code>spaCy</code> is as follows:</p>

<blockquote>
  <p>A fine-grained, more detailed tag that represents the word-class and some basic morphological information for the token. These tags are primarily designed to be good features for subsequent models, particularly the syntactic parser. They are language and treebank dependent. The tagger is trained to predict these fine-grained tags, and then a mapping table is used to reduce them to the coarse-grained  .pos tags.</p>
</blockquote>

<p>But it doesn't list the full available tags and each tag's explanation. Where can I find it?</p>
",Parsing & POS Tagging,spacy token tag full list official documentation follows fine grained detailed tag represents word class basic morphological information token tag primarily designed good feature subsequent model particularly syntactic parser language treebank dependent tagger trained predict fine grained tag mapping table used reduce coarse grained po tag list full available tag tag explanation find
Retrieve a list of model-specific POS tags using spaCy,"<p>I am looking for a way to get a list of <strong>all possibly usable POS tags for a specific language</strong> model in spaCy.</p>
<p>In <a href=""https://stackoverflow.com/a/60303305/11583484"">an answer to another question, spaCy's <code>TAG_MAP</code> has been referenced to</a>, but I am not sure how to access this. The documentation of spaCy says that <a href=""https://spacy.io/usage/v3#incompat"" rel=""nofollow noreferrer"">this attribute has been replaced</a>. Since spaCy only uses a specific subset of all POS tags for a specific language, I would like to retrieve a list of all POS tags that are currently used with the initialized language model.</p>
<p>I did currently just set up a model this way:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy

tagger = spacy.load(&quot;de_dep_news_trf&quot;)

# TODO print(pos_tags)
</code></pre>
<p>Now, how do I print a list of all possible pos tags for this model?</p>
",Parsing & POS Tagging,retrieve list model specific po tag using spacy looking way get list possibly usable po tag specific language model spacy attribute ha replaced since spacy us specific subset po tag specific language would like retrieve list po tag currently used initialized language model currently set model way print list possible po tag model
Spacy Span as_doc() Loses Components from Original Doc (Benepar),"<p>I have a string with a few sentences in it. I want to get the constituency parse for each of those sentences. I am doing this by doing a nlp parse of the full string to get the spacy <code>Doc</code>, then looping through the <code>doc.sents</code> and converting the Spans to Docs with <code>span.as_doc()</code>. However it looks like when I convert the Spans back to the Docs not all of the original data is preserved. Specifically, the benepar constituency parse is no longer there.</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
import benepar

nlp = spacy.load(&quot;en_core_sci_md&quot;, disable=[&quot;ner&quot;, &quot;lemmatizer&quot;, &quot;textcat&quot;])
nlp.add_pipe('benepar', config={'model': BENEPAR_DIR})
nlp_test1 = nlp('The quick brown fox jumps over the lazy dog')
print(list(nlp_test1.sents)[0]._.parse_string) # Uses benepar (works)

nlp_test2 = list(nlp_test1.sents)[0].as_doc()
print(list(nlp_test2.sents)[0]._.parse_string) # No constituency parse found (no benepar)

nlp_test3 = list(nlp_test.sents)[0].as_doc(array_head=nlp_test._get_array_attrs())
print(list(nlp_test3.sents)[0]._.parse_string) # Doesn't work either
</code></pre>
<p>How do I convert a <code>Span</code> into a <code>Doc</code> while keeping the benepar constituency parse data? Or is this not possible and benepar only parses the first of the <code>doc.sents</code>?</p>
",Parsing & POS Tagging,spacy span doc loses component original doc benepar string sentence want get constituency parse sentence nlp parse full string get spacy looping converting span doc however look like convert span back doc original data preserved specifically benepar constituency parse longer convert keeping benepar constituency parse data possible benepar par first
How to extract dependency relations (dependency grammer) between POS tags?,"<p>Currently, I am working on a project and need some ideas to solve it.
I am thinking of implementing a dependency relation extractor by myself, so it can show if there is a dependency relation between two POS tags. In previous steps, I have implemented a technique using word co-occurrence to group words based on their syntactic roles. For example, Nouns are gathered in the same group and are labeled with 1, or verbs are grouped in group 2 and their label is 2, adverbs are in Group 3, etc.</p>
<p>In reality, there is a dependency relation between noun and verbs, or between verbs and adverbs, or dependency between noun and adjective, but there is not any dependency between noun and adverb or between verb and adjective.</p>
<p>So I want to use POS-tags Co-Occurrences or any other technique to show that there is a dependency relation between Group 1 and Group 2 (between noun group and verb group) and do the same to the other groups. In other words, I am not trying to construct a dependency tree between words, I am trying to just study between which of the detected syntactic groups there are dependency relations.</p>
<p>I would be so thankful if you give my mind to starting this project.</p>
",Parsing & POS Tagging,extract dependency relation dependency grammer po tag currently working project need idea solve thinking implementing dependency relation extractor show dependency relation two po tag previous step implemented technique using word co occurrence group word based syntactic role example noun gathered group labeled verb grouped group label adverb group etc reality dependency relation noun verb verb adverb dependency noun adjective dependency noun adverb verb adjective want use po tag co occurrence technique show dependency relation group group noun group verb group group word trying construct dependency tree word trying study detected syntactic group dependency relation would thankful give mind starting project
How to remove POS-tag &#39;VERBS&#39; from dataframe,"<p>I have imported an Excel file as Pandas Dataframe. This file consists of &gt;4000 rows (documents) and 12 columns. I extracted the column 'Text' for NLP.</p>
<p>The text in the column 'Text' is in Dutch. I'm using a Spacy model for Dutch language 'nl_core_news_lg'</p>
<pre><code>import spacy 
import pandas as pd

spacy.load('nl_core_news_lg')
import nl_core_news_lg
nlp = nl_core_news_lg.load()

df = pd.read_excel (*file path*)
text_article = (df['Text'])
</code></pre>
<p>I have preprocessed df['Text'']. I've removed digits and interpunction, and converted the text to all lower case. Resulting in the following variable: text_article['lower']</p>
<p>Next, I've tokenized the text.</p>
<pre><code>def tokenization(text):
    tokens = re.split('W+',text)
    return tokens

text_article['tokens'] = text_article['lower'].apply(lambda x: nlp.tokenizer(x)) 
</code></pre>
<p>I now want to add Part-Of-Speech (POS) tags to every token. Hereafter, I want to remove all tokens with the POS-tag 'VERB'.</p>
<p>I've tried the following code.</p>
<pre><code>text_article['final'] = text_article['tokens'].apply(lambda text: &quot; &quot;.join(token.lemma_ for token in nlp(text) if not token.is_stop or token.pos_ == 'VERB'))
</code></pre>
<p>This code does not produce an error. But when I print a document as an example (e.g. doc 42) the text still includes verbs.</p>
<pre><code>print(text_article['final'][42])
</code></pre>
<p>I'm running out of ideas here and really hope somebody can help me out! Thanks in advance.</p>
",Parsing & POS Tagging,remove po tag verb dataframe imported excel file panda dataframe file consists row document column extracted column text nlp text column text dutch using spacy model dutch language nl core news lg preprocessed df text removed digit interpunction converted text lower case resulting following variable text article lower next tokenized text want add part speech po tag every token hereafter want remove token po tag verb tried following code code doe produce error print document example e g doc text still includes verb running idea really hope somebody help thanks advance
SpaCy doc.similarity limitations,"<p>I'm building an information retrieval tool that receives an user's request and returns the most similar label in the corpus.</p>
<p>With Spacy's vanilla similarity, I have the following limitation :</p>
<pre><code>request = nlp(&quot;cute cat&quot;)

label1 = nlp(&quot;cute dog&quot;)
label2 = nlp(&quot;lovable cat&quot;)

print(request.similarity(label1))
print(request.similarity(label2))

# Returns 
# 0.9046133562567831
# 0.8776915657921017
</code></pre>
<p>In this case I would like cat label to have a higher similarity because the request is about a (cute/lovable/...) cat.
Also, &quot;ugly cat&quot; should have a lower score then &quot;cute dog&quot;.</p>
<p>I'm thinking of overwriting Spacy's similarity so doc.similarity will be a weighted sum of similarity between nouns and similarity between adjectives. The first one will have a higher weight.</p>
<p><strong>Do you think it would be a good idea ? Do you know better ways or tools for this ?</strong></p>
<p>Also, labels are not that simple. I'm thinking of dependency parsing to handle labels as &quot;cute dog in a garden&quot; (I'm inventing them). Here <em>dog</em> and <em>garden</em> are nouns but <em>dog</em> is the 'main' one.</p>
",Parsing & POS Tagging,spacy doc similarity limitation building information retrieval tool receives user request return similar label corpus spacy vanilla similarity following limitation case would like cat label higher similarity request cute lovable cat also ugly cat lower score cute dog thinking overwriting spacy similarity doc similarity weighted sum similarity noun similarity adjective first one higher weight think would good idea know better way tool also label simple thinking dependency parsing handle label cute dog garden inventing dog garden noun dog main one
"How to show token.text &quot;[wants,pizza]&quot; according arrow in SPACY Dependency Parser with Python?","<p>Other case I have try successfully, based on SPACY dependency parser, word 'pizza' is &quot;dobj&quot; and link to word 'cola' as 'conj'. But, I don't know how to show text '[wants,pizza]' ?</p>
<pre><code>import deplacy
import spacy
nlp = spacy.load('en_core_web_sm')

text='John said &quot;He wants a pizza and cola&quot;'
doc = nlp(text)

deplacy.render(doc)

for token in doc:
    if token.dep_=='dobj':
        dobj=[token.text]
        conj=[t.text for t in token.conjuncts]
    dobj_conj=dobj+conj
print(dobj_conj)
</code></pre>
<p><a href=""https://i.sstatic.net/h75fM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/h75fM.png"" alt=""enter image description here"" /></a></p>
",Parsing & POS Tagging,show token text want pizza according arrow spacy dependency parser python case try successfully based spacy dependency parser word pizza dobj link word cola conj know show text want pizza
Merge tokens based on preceeding POS tags,"<p>I would like to implement some text manipulation as a pre-processing to keyphrases extraction. Look at the below example:</p>
<pre><code>import spacy
text = &quot;conversion of existing underground gas storage facilities into storage facilities dedicated to hydrogen-storage&quot;

nlp = spacy.load(&quot;en_core_web_sm&quot;)
doc = nlp(text)

for token in doc:
    print(f'{token.text:{8}} {token.pos_:{6}} {token.tag_:{6}} {token.dep_:{6}} {spacy.explain(token.pos_):{20}} {spacy.explain(token.tag_)}')
</code></pre>
<p>result:</p>
<pre><code>conversion NOUN   NN     ROOT   noun                 noun, singular or mass
of       ADP    IN     prep   adposition           conjunction, subordinating or preposition
existing VERB   VBG    amod   verb                 verb, gerund or present participle
underground ADJ    JJ     amod   adjective            adjective (English), other noun-modifier (Chinese)
gas      NOUN   NN     compound noun                 noun, singular or mass
storage  NOUN   NN     compound noun                 noun, singular or mass
facilities NOUN   NNS    pobj   noun                 noun, plural
into     ADP    IN     prep   adposition           conjunction, subordinating or preposition
storage  NOUN   NN     compound noun                 noun, singular or mass
facilities NOUN   NNS    pobj   noun                 noun, plural
dedicated VERB   VBN    acl    verb                 verb, past participle
to       ADP    IN     prep   adposition           conjunction, subordinating or preposition
hydrogen NOUN   NN     compound noun                 noun, singular or mass
-        PUNCT  HYPH   punct  punctuation          punctuation mark, hyphen
storage  NOUN   NN     pobj   noun                 noun, singular or mass
</code></pre>
<p>I would like to recognize when a given word (for example storage) is preceeded by a NOUN (like in the case of gas storage) in order to replace the space characted with an hyphen (as already done in hydrogen-storage), but I don't want to change the space character when my word is preceeded by a POS element that is not NOUN (example: into storage).</p>
<p>Expected output: &quot;conversion of existing underground <em>gas-storage</em> facilities into storage facilities dedicated to hydrogen-storage&quot;</p>
<p>Is there an efficient way to do this?</p>
<p>Thank you in advance for any help</p>
",Parsing & POS Tagging,merge token based preceeding po tag would like implement text manipulation pre processing keyphrases extraction look example result would like recognize given word example storage preceeded noun like case gas storage order replace space characted hyphen already done hydrogen storage want change space character word preceeded po element noun example storage expected output conversion existing underground gas storage facility storage facility dedicated hydrogen storage efficient way thank advance help
How to identify abstract and concrete sentences using python NLP tools?,"<p>How to identify if a sentence quantifies something in concrete or abstract terms using tools like spacy, nltk, or any other NLP tools available in python.</p>
<p>How do I approach this problem? Are there any pre-trained classifiers that can serve this requirement? Is there a way to use the POS tags and other methods to achieve this?</p>
<p>This is a pretty simple classification problem, but I could not find any previous work on this by searching on google, kaggle, and StackOverflow. Please share your valuable comments, suggestions, or references on this topic.</p>
<pre><code>Example:

We got a ton of earnings reports last week (concrete)
Life is full of learnings (abstract)
We had a terrific quarter, and our earnings per share were over 7 percent (concrete)
We strive to achieve results, and that is our goal, our motto (abstract)
</code></pre>
",Parsing & POS Tagging,identify abstract concrete sentence using python nlp tool identify sentence quantifies something concrete abstract term using tool like spacy nltk nlp tool available python approach problem pre trained classifier serve requirement way use po tag method achieve pretty simple classification problem could find previous work searching google kaggle stackoverflow please share valuable comment suggestion reference topic
Iterate over spacy tokens and extract the BILOU tags,"<p>How should I annotate the following sentence with BILOU tags?</p>
<p>I have a function called <code>get_dataset2</code> what this function do is it will give the tokens, POS tags and BILOU tags but the things is that am stuck at BILOU tags.</p>
<p>Function:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>def get_dataset2(sent):
  head_entity = """"
  candidate_entity = """"

  prv_tok_dep = """"    
  prv_tok_text = """"  

  prefix = """"
  words_ = []
  label_ = []
  tags_ = []

  doc = nlp(sent) 
  
  for tok in doc:
      words_.append(tok.text)
      label_.append(tok.pos_)

      if(tok.text=='JUDGMENT'):
          tags_.append('O')
          next_token1 = doc[tok.i+1]
          #next_tok_loc1 = tok.i+1
          next_token2 = doc[tok.i+2]
          #next_tok_loc2 = tok.i+2

      if(tok.text==next_token1 and (next_token2.pos_=='PUNCT' or next_token2.pos_=='NUM')):
          tags_.append('U-Parties')


      #if(next_token1.pos_=='PROPN' and next_token2.pos_=='PROPN'):
          #tags_.append('U-Parties')

      else:
          tags_.append('O')    

  return (pd.DataFrame({'Token': words_, 'POS': label_,'Tags': tags_}))</code></pre>
</div>
</div>
</p>
<p>Problem: <code>get_dataset2('JUDGMENT Gajendragadkar, J. 1.')</code> when i pass this sentence to that function then it will successfully extract the tokens and POS but not the BILOU tags.</p>
<p>It should be like :</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>Tokens         POS       BILOU Tags
JUDGMENT       PROPN     O
Gajendragadkar PROPN     U-Parties
,              PUNCT     O</code></pre>
</div>
</div>
</p>
<p>I wan to iterate over tokens like after JUDGMENT I want to identify the second and third token and then I will assign the BILOU tags if it is single then U-parties.</p>
<p>Thanks!</p>
",Parsing & POS Tagging,iterate spacy token extract bilou tag annotate following sentence bilou tag function called function give token po tag bilou tag thing stuck bilou tag function problem pas sentence function successfully extract token po bilou tag like wan iterate token like judgment want identify second third token assign bilou tag single u party thanks
Question about natural language processing,"<p>I am working on a graduation project related to &quot;Aspect extraction (AE)&quot;.</p>
<p>I'm pretty confused about POS taging, syntax tree, grammar rules, and other low-level NLP stuff. I need a reference that teaches me these things in detail, so if any of you know I hope you don't mind me?</p>
<p>I know my question is not about programming directly and this may not agree with the site, but I really need to.</p>
",Parsing & POS Tagging,question natural language processing working graduation project related aspect extraction ae pretty confused po taging syntax tree grammar rule low level nlp stuff need reference teach thing detail know hope mind know question programming directly may agree site really need
What&#39;s the most convenient way to analyze a sentence phrases and structure using NLTK or SpaCy?,"<p><a href=""https://i.sstatic.net/fuZ3G.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fuZ3G.png"" alt=""enter image description here"" /></a></p>
<p>What's the most convenient way to analyze a sentence phrases and structure using NLTK or SpaCy?
The main goal is to get a well organized and clean data in order to apply some inferential statistics on it.</p>
<p>Here is a simple example of what I need, as shown in the tree above:</p>
<li>NP which is a Noun Phrase
<li>VP, a Verb Phrase
<li>ADJP, Adjective Phrase 
<li>-, a coordinating conjunction, implying that it is a compound sentence
<li>PP, a Prepositional Phrase
",Parsing & POS Tagging,convenient way analyze sentence phrase structure using nltk spacy convenient way analyze sentence phrase structure using nltk spacy main goal get well organized clean data order apply inferential statistic simple example need shown tree np noun phrase vp verb phrase adjp adjective phrase coordinating conjunction implying compound sentence pp prepositional phrase
How to exclude sentences from Spacy results if it contains a token with a specific dep_?,"<p>I would like to negative filter Spacy results. Actually, I would like to get sentences includes only 'pobj' but not 'dobj' in dependency parsing. However, since sentences with 'dobj' are likely to included 'pobj' but not vice versa, Spacy lists also sentences with 'dobj' included.</p>
<p>For instance;</p>
<p><em>'He pushed the book off the shelf'</em>:</p>
<pre><code>He nsubj
pushed ROOT
the det
book dobj
off prep
the det
shelf pobj
</code></pre>
<p>'<em>The book fell off the table</em>'</p>
<pre><code>The det
book nsubj
fell ROOT
off prep
the det
table pobj
</code></pre>
<p>In both sentence, <code>prep</code> is the immediate head of <code>pobj</code>, therefore;</p>
<pre><code>doc = nlp('He pushed the book off the shelf.The book fell off the table')
for t in doc:
     if t.dep_ == 'pobj': 
         print(t.sent)
     
</code></pre>
<p>would give me the both sentences in return. How can I negative filter correctly to not to list sentences including both '<code>dobj</code>' and '<code>pobj</code>' but to list sentence only '<code>pobj</code>' included</p>
",Parsing & POS Tagging,exclude sentence spacy result contains token specific dep would like negative filter spacy result actually would like get sentence includes pobj dobj dependency parsing however since sentence dobj likely included pobj vice versa spacy list also sentence dobj included instance pushed book shelf book fell table sentence immediate head therefore would give sentence return negative filter correctly list sentence including list sentence included
Finding most common adjective in text (part of speech tagging),"<p>I have a dataset where i'm trying to find the most common adjective/verb/noun, I already used NLTK to tag the word, so now my dataframe is looking like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Index</th>
<th>POS</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>[('the', 'DT'),('quality', 'NN'),('of', 'IN'),('food', 'NN'),('was', 'VBD'),('poor', 'JJ')]</td>
</tr>
<tr>
<td>1</td>
<td>[('good', 'JJ'), ('food', 'NN'), ('for', 'IN'), ('the', 'DT'), ('price', 'NN')]</td>
</tr>
</tbody>
</table>
</div>
<p>Now how do i find what word is most commonly used as adjective for example</p>
",Parsing & POS Tagging,finding common adjective text part speech tagging dataset trying find common adjective verb noun already used nltk tag word dataframe looking like index po dt quality nn food nn wa vbd poor jj good jj food nn dt price nn find word commonly used adjective example
how to extract head nouns from a phrase in python?,"<p>I am doing a keyphrase classification task and for this i am working with the head noun extraction from keyphrases in python. The little help available on internet is not of good use. i am struggling with this.</p>
",Parsing & POS Tagging,extract head noun phrase python keyphrase classification task working head noun extraction keyphrases python little help available internet good use struggling
What is the default nltk part of speech tagset?,"<p>While experimenting with NLTK part of speech tagging, I noticed a lot of <code>VBP</code> tags in the output of my calls to <code>nltk.pos_tag</code>.  I noticed this tag is not in the Brown Corpus part of speech tagset.  It is however a part of the UPenn tagset.</p>

<p>What tagset does nltk use by default?  I can't find this in the official documentation or the apidocs.</p>
",Parsing & POS Tagging,default nltk part speech tagset experimenting nltk part speech tagging noticed lot tag output call noticed tag brown corpus part speech tagset however part upenn tagset tagset doe nltk use default find official documentation apidocs
ConditionalFreqDist to find most frequent POS tags for words,"<p>I am trying to fidn the most frequent POS tag for words in the dataset but struggling with the ConditionalFrewDist part.</p>
<pre><code>import nltk
tw = nltk.corpus.brown.tagged_words()

train_idx = int(0.8*len(tw))
training_set = tw[:train_idx]
test_set = tw[train_idx:]

words= list(zip(*training_set))[0]

from nltk import ConditionalFreqDist
ofd= ConditionalFreqDist(word for word in list(zip(*training_set))[0])

tags= list(zip(*training_set))[1]
ofd.tabulate(conditions= words, samples= tags)
</code></pre>
<blockquote>
<p>ValueError: too many values to unpack (expected 2)</p>
</blockquote>
",Parsing & POS Tagging,conditionalfreqdist find frequent po tag word trying fidn frequent po tag word dataset struggling conditionalfrewdist part valueerror many value unpack expected
Is it possible to find uncertainties of spaCy token dependencies?,"<p>I am using spaCy in order to match text against certain dependency patterns. I'm facing the problem that my DependencyParser gives different results even in simple sentences when a single word (of same ground-true POS-Tag) is changed. E.g. 'The baker and supervisor support the baking' finds that 'support' is a VERB, 'baker' and 'supervisor' are NOUNS. 'baker' and 'supervisor' have nsubj dependency to support, 'baking' is dobj of support. <a href=""https://i.sstatic.net/Cfjgr.png"" rel=""nofollow noreferrer"">See here</a>. Now changing this to 'The baker and oven support the baking' results in an ADV POS-Tag for 'oven' instead of NOUN and has dependency advmod to 'support'. <a href=""https://i.sstatic.net/EtW98.png"" rel=""nofollow noreferrer"">See here</a>. This makes absolutely no sense as oven is never an adverb. I thought that the DependencyParser probably uses the POS-Tags and that changing them could change the resulting dependencies.
I found this question [3] and managed to extract the probabilities of all POS-Tags for each token with <code>Tagger.model.predict([doc])</code> which delivers a matrix of shape len(doc) x len(tagger.labels). In the first sentence 'supervisor' got 99,8 % for NOUN, while 'oven' only got 62 % for ADV. So I produced multiple docs for the same text, where I changed the Token.pos_ to the second and third most probable candidates if there is uncertainty (most probable tag &lt; 90 %). I then ran the DependencyMatcher with all three docs, thinking that the different POS-Tags would lead to a change in dependencies, but it doesn't. When 'oven' has POS-Tag NOUN (third most probable tag) it still has advmod dependency with 'support', which doesn't make sense.
So similar to [3] I want to inspect the probabilities of all values for Token.dep_ for each token in the doc. Unfortunately, <code>DependencyParser.model.predict([doc])</code> doesn't deliver this (afaik).</p>
<p>[3] <a href=""https://stackoverflow.com/questions/65218606/is-it-possible-to-find-uncertainties-of-spacy-pos-tags"">Is it possible to find uncertainties of spaCy POS tags?</a></p>
",Parsing & POS Tagging,possible find uncertainty spacy token dependency using spacy order match text certain dependency pattern facing problem dependencyparser give different result even simple sentence single word ground true po tag changed e g baker supervisor support baking find support verb baker supervisor noun baker supervisor nsubj dependency support baking dobj support see changing baker oven support baking result adv po tag oven instead noun ha dependency advmod support see make absolutely sense oven never adverb thought dependencyparser probably us po tag changing could change resulting dependency found question managed extract probability po tag token delivers matrix shape len doc x len tagger label first sentence supervisor got noun oven got adv produced multiple doc text changed token po second third probable candidate uncertainty probable tag ran dependencymatcher three doc thinking different po tag would lead change dependency oven ha po tag noun third probable tag still ha advmod dependency support make sense similar want inspect probability value token dep token doc unfortunately deliver afaik href possible find uncertainty spacy po tag
Searching for known phrases in text using Azure Cognitive Services,"<p>I'm trying to ascertain the &quot;right tool for the job&quot; here, and I believe Cognitive Services can do this but without disappearing down an R&amp;D rabbit-hole I thought I'd make sure I was tunnelling in the right direction first.</p>
<p>So, here is the brief:</p>
<p>I have a collection of known existing phrases which I want to look for, but these might be written in slightly different ways, be that grammar or language.</p>
<p>I want to be able to parse a (potentially large) volume of text to scan and look for those phrases so that I can identify them.</p>
<p>For example, my phrase could be &quot;the event will be in person&quot; but that also needs to identify different uses of language; for example &quot;in-person event&quot;, &quot;face to face event&quot;, or &quot;on-site event&quot; - as well as the various synonyms and variations you can get with such things.</p>
<p><strong>LUIS</strong> initially appeared to be the go-to tool for this kind of thing, and includes the ability to write your own Features (aka Phrase Lists) to augment the model, but it isn't clear whether that would hit the brief - LUIS <em>appears</em> to be much more about &quot;intent&quot; and user interaction (for example building a chat Bot, or understanding intent from emails).</p>
<p><strong>Text Analytics</strong> also seems a likely candidate, but again seems more focused about identifying &quot;entities&quot; (such as people / places / organisations) rather than a natural language &quot;phrase&quot; - would this tool work if I was defining my own &quot;Topics&quot; or is that really just barking up the wrong tree?</p>
<p>.. or ... is there actually something else I should be looking at completely different?</p>
<p>At this point - I'm really looking for a &quot;which tool should I spend lots of time learning about&quot;.</p>
<p>Thanks all in advance - I appreciate this is a fairly open-ended requirement.</p>
",Parsing & POS Tagging,searching known phrase text using azure cognitive service trying ascertain right tool job believe cognitive service without disappearing r rabbit hole thought make sure wa tunnelling right direction first brief collection known existing phrase want look might written slightly different way grammar language want able parse potentially large volume text scan look phrase identify example phrase could event person also need identify different us language example person event face face event site event well various synonym variation get thing luis initially appeared go tool kind thing includes ability write feature aka phrase list augment model clear whether would hit brief luis appears much intent user interaction example building chat bot understanding intent email text analytics also seems likely candidate seems focused identifying entity people place organisation rather natural language phrase would tool work wa defining topic really barking wrong tree actually something else looking completely different point really looking tool spend lot time learning thanks advance appreciate fairly open ended requirement
Rename spacy&#39;s pos tagger labels,"<p>i'm looking for something specific and didn't really found an answer: I'm looking to rename the pos tags label of spacy.
E.g. if i have this code:</p>
<pre><code>def eng(textstr):    
    nlp = spacy.load(&quot;en_core_web_sm&quot;)
    doc = nlp(textstr)
    for token in doc:
    print(&quot;Word: &quot;+token.text+ &quot; &quot;+&quot;POS: &quot;+token.pos_)
</code></pre>
<p>I want token.pos_ to give me NIA instead of NOUN, BO instead of VERB, etc... I don't want to retrain anything if i can. The results given by the pos tagger are accurate enough for me, i just want to rename each label (Noun to NIA, Verb to BO, etcc..). So instead of having a NOUN i want token.pos_ to give me back NIA. First is this possible and if it is, how can i do it?
The first thing that came me to mind is to use simply an if statement:</p>
<pre><code>if token.pos_ == &quot;NOUN&quot;
  print(&quot;Word: &quot;+token.text+ &quot; &quot;+&quot;POS: NIA&quot;)
</code></pre>
<p>but that cannot be done because than i have to change about 5000 functions, which is impossible. Is there another way? Thank you very much for your help!</p>
",Parsing & POS Tagging,rename spacy po tagger label looking something specific really found answer looking rename po tag label spacy e g code want token po give nia instead noun bo instead verb etc want retrain anything result given po tagger accurate enough want rename label noun nia verb bo etcc instead noun want token po give back nia first possible first thing came mind use simply statement done change function impossible another way thank much help
Label schemes by language in Spacy,"<p>From the <a href=""https://spacy.io/usage/linguistic-features"" rel=""nofollow noreferrer"">Spacy documentation</a>:</p>
<blockquote>
<p>For a list of the fine-grained and coarse-grained part-of-speech tags assigned by spaCy’s models across different languages, see the label schemes documented in the models directory.</p>
</blockquote>
<p>I assume this is referring to the <a href=""https://spacy.io/usage/linguistic-features#pos-tagging"" rel=""nofollow noreferrer"">parts of speech tags</a>, eg: <code>VERB</code>, <code>NOUN</code>, <code>NUM</code> etc., and that this list will be different for each language.</p>
<p>Is this a correct assumption?</p>
<p>I followed the link in the documentation <a href=""https://spacy.io/models"" rel=""nofollow noreferrer"">to the models directory</a>, but could not find a list of the valid POS tags for each language.</p>
<p><a href=""https://spacy.io/usage/linguistic-features#pos-tagging"" rel=""nofollow noreferrer"">https://spacy.io/usage/linguistic-features#pos-tagging</a></p>
<p><strong>Answer</strong></p>
<p>Thanks to @polm23 for the answer, here's a screen shot with the navigation, in case anyone else can't find it.</p>
<p><a href=""https://i.sstatic.net/w0SW7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/w0SW7.png"" alt=""enter image description here"" /></a></p>
",Parsing & POS Tagging,label scheme language spacy spacy documentation list fine grained coarse grained part speech tag assigned spacy model across different language see label scheme documented model directory assume referring part speech tag eg etc list different language correct assumption followed link documentation model directory could find list valid po tag language answer thanks polm answer screen shot navigation case anyone else find
Split complex/compound sentence into Simple sentences Python NLP,"<p>I am working on a project that works with small sentences, So If user passes long sentences or complex/Compound sentences, I want to parse it to simple sentences and then pass it to the software.</p>
<p>I tried the spacy method, but it only works with conjunctions:
i.e.: I am going to market and I will buy a book.
After parsing: I am going to market. I will buy a book.
(It splits it into two simple sentences)</p>
<p>But when I tried with more complex sentences like:</p>
<ul>
<li>I have to save this coupon in case I come back to the store tomorrow. (should split as I have to save this coupon. I came back to store tommorrow.</li>
<li>In a way it curbs the number of crime cases happening every day. (should split as In a way it curbs the number of crime cases. Happening every day)</li>
</ul>
<p>The code I have:</p>
<pre><code>import spacy

en = spacy.load('en_core_web_sm')

text = &quot;In a way it curbs the number of crime cases happening every day.&quot;

doc = en(text)

seen = set() # keep track of covered words

chunks = []
for sent in doc.sents:
    heads = [cc for cc in sent.root.children if cc.dep_ == 'conj']

    for head in heads:
        print(head.subtree)
        words = [ww for ww in head.subtree]
        for word in words:
            seen.add(word)
        chunk = (' '.join([ww.text for ww in words]))
        chunks.append( (head.i, chunk) )

    unseen = [ww for ww in sent if ww not in seen]
    chunk = ' '.join([ww.text for ww in unseen])
    chunks.append( (sent.root.i, chunk) )

chunks = sorted(chunks, key=lambda x: x[0])

for ii, chunk in chunks:
    print(chunk)
</code></pre>
<p>Is there any library/framework for doing that easily. or anyone suggest how to generate sentence tree on spacy and parse it, so I can break it on desired place.</p>
",Parsing & POS Tagging,split complex compound sentence simple sentence python nlp working project work small sentence user pass long sentence complex compound sentence want parse simple sentence pas software tried spacy method work conjunction e going market buy book parsing going market buy book split two simple sentence tried complex sentence like save coupon case come back store tomorrow split save coupon came back store tommorrow way curb number crime case happening every day split way curb number crime case happening every day code library framework easily anyone suggest generate sentence tree spacy parse break desired place
How to parse the special character in Context Free Grammar?,"<p>I have a context free grammar (CFG) which involves punctuation. e.g.
nltk.parse_cfg(""""""PP-CLR -> IN `` NP-TTL"""""")</p>

<p>The `` is a valid Penn Treebank POS tag. But nltk cannot recognize it. In fact, nltk.parse_cfg cannot recognize any character other than alphanumeric and dash. While Penn Treebank POS tag has several punctuation, such as $ # : . (</p>

<p>Then, should I keep the punctuation in my dataset? Or is there anyway to parse these characters?</p>

<p>Thanks</p>
",Parsing & POS Tagging,parse special character context free grammar context free grammar cfg involves punctuation e g nltk parse cfg pp clr np ttl valid penn treebank po tag nltk recognize fact nltk parse cfg recognize character alphanumeric dash penn treebank po tag ha several punctuation keep punctuation dataset anyway parse character thanks
Regex NLTK chunking - Can&#39;t get my regex rule to identify certain pos tags,"<p>Hi I am attempting to identify very specific sentence structures but the rule i am writing in regex seems to skip occasional parts of my test samples. Here is an example:</p>
<pre><code>chunkRule= r&quot;&quot;&quot;Action: {&lt;PRP|PRP$|NNP|NN&gt;+&lt;VB|VBD|VBG|VBN|VBZ|RB|JJ|NNP|NN&gt;+&lt;VBG|RP|RB|NNP|NN|PRP$&gt;*}&quot;&quot;&quot;
</code></pre>
<p>Input text: My wife goes out</p>
<p>POS Tag: [('My', 'PRP$'), ('wife', 'NN'), ('goes', 'VBZ'), ('out', 'RP')
<p>Return Value: (Action wife/NN goes/VBZ out/RP)
<p>As you can see it's skipping &quot;My&quot;/PRP$ POS tag. Does anyone have any ideas how to adjust this to allow it to detect this?</p>
<p>Thanks for your help in advanced!</p>
",Parsing & POS Tagging,regex nltk chunking get regex rule identify certain po tag hi attempting identify specific sentence structure rule writing regex seems skip occasional part test sample example input text wife go po tag prp wife nn go vbz rp return value action wife nn go vbz rp see skipping prp po tag doe anyone idea adjust allow detect thanks help advanced
Converting POS tags from TextBlob into Wordnet compatible inputs,"<p>I'm using Python and nltk + Textblob for some text analysis. It's interesting that you can add a POS for wordnet to make your search for synonyms more specific, but unfortunately the tagging in both nltk and Textblob aren't ""compatible"" with the kind of input that wordnet expects for it's synset class. </p>

<p><strong>Example</strong>
Wordnet.synsets() requires that the POS you give it is one of n,v,a,r, like so</p>

<pre><code>wn.synsets(""dog"", POS=""n,v,a,r"")
</code></pre>

<p>But a standard POS tagging from upenn_treebank looks like </p>

<pre><code>JJ, VBD, VBZ, etc.
</code></pre>

<p>So I'm looking for a good way to convert between the two.</p>

<p>Does anyone know of a good way to make this conversion happen, besides brute force?</p>
",Parsing & POS Tagging,converting po tag textblob wordnet compatible input using python nltk textblob text analysis interesting add po wordnet make search synonym specific unfortunately tagging nltk textblob compatible kind input wordnet expects synset class example wordnet synset requires po give one n v r like standard po tagging upenn treebank look like looking good way convert two doe anyone know good way make conversion happen besides brute force
How to determine structure of answer for a WH-question,"<p>Consider a wh-question such as &quot;Who closed the door?&quot;. Personally, I can determine that an answer will look like &quot;NP closed the door.&quot;, where NP would be a noun phrase. Another example is &quot;What is John looking at?&quot;. I can determine that answer will look like &quot;John is looking at NP&quot;.</p>
<p>Is there a proper way to determine structure of such answers computationally? Are there any specifications for what a proper english question must look like in terms of POS-tags?</p>
",Parsing & POS Tagging,determine structure answer wh question consider wh question closed door personally determine answer look like np closed door np would noun phrase another example john looking determine answer look like john looking np proper way determine structure answer computationally specification proper english question must look like term po tag
Incorrect (?) lemma returned by NLTK WordNetLemmatizer for particular words,"<p>I was reading this <a href=""https://towardsdatascience.com/building-a-text-normalizer-using-nltk-ft-pos-tagger-e713e611db8"" rel=""nofollow noreferrer"">article</a> and experimenting on my own data, I found both the examples given in the article and one of my words didn't work as described.
You can refer to the article for more information, though the question here has everything to get going.</p>
<pre><code># stemmed root words: Books, Braveri, Harri, Transpar
from nltk.stem.wordnet import WordNetLemmatizer as Lemmatizer

# the article shared the same lemmatizer initialization.
lem = WordNetLemmatizer()

# returned 'harry' in the example without pos tag
In [269]: lem.lemmatize('harri', pos='n')
Out[269]: 'harri'

In [270]: lem.lemmatize(&quot;Books&quot;, pos='n')
Out[269]: 'Books'

# returned 'book' in the example with pos tag
In [270]: lem.lemmatize(&quot;Books&quot;, pos='v')
Out[269]: 'Books'

# my example root word, didn't change at all
[ins] In [278]: lem.lemmatize(&quot;Transpar&quot;, pos=&quot;a&quot;)
Out[278]: 'Transpar'
[ins] In [281]: lem.lemmatize(&quot;Transpar&quot;, pos=&quot;n&quot;)
Out[281]: 'Transpar'

# returned 'bravery' in the example without pos tag
[ins] In [280]: lem.lemmatize(&quot;Braveri&quot;, pos=&quot;n&quot;)
Out[280]: 'Braveri'
</code></pre>
<p>the default pos tag for this lemmatizer is just <code>wordnet.NOUN</code> therefore providing pos tag or not wouldn't make a difference.
FYI, <code>transpar</code> was originally <code>transparent</code>.</p>
<p>The only difference was that the author stemmed the words using NLTK stemmer whereas I was using <code>texthero.stem</code>.</p>
<p>Is it that I did it wrong or something has changed in NLTK?</p>
",Parsing & POS Tagging,incorrect lemma returned nltk wordnetlemmatizer particular word wa reading article experimenting data found example given article one word work described refer article information though question ha everything get going default po tag lemmatizer therefore providing po tag make difference fyi wa originally difference wa author stemmed word using nltk stemmer whereas wa using wrong something ha changed nltk
How to find a common tree of two syntactic trees,"<p>I want to find common nodes of two parse tree. Currently, I used Stanford parser and got two parse trees of two sentences. How can I find the common tree/nodes of these two sentence parse trees? I think if I can find the common tree, it will be easier to find the common nodes.</p>

<p>I want to post a capture of one paper, but I don't have enough reputation. If you need more clear explanation, I can send the capture later when I get enough reputation. Thanks!</p>

<p>Here is the link to the capture. In this capture, you can see the common nodes are in gray. My goal is to find these nodes and calculate the similarity between two trees. 
<a href=""https://drive.google.com/file/d/1g7zoGyaSAktDvH_q_Bz1mikkeQgS1v9M/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1g7zoGyaSAktDvH_q_Bz1mikkeQgS1v9M/view?usp=sharing</a></p>
",Parsing & POS Tagging,find common tree two syntactic tree want find common node two parse tree currently used stanford parser got two parse tree two sentence find common tree node two sentence parse tree think find common tree easier find common node want post capture one paper enough reputation need clear explanation send capture later get enough reputation thanks link capture capture see common node gray goal find node calculate similarity two tree
Spacy Dependency Matcher problematic and sensitive for long verb-noun phrases,"<p>I am trying to construct a dependency matcher that catches certain phrases in document and prints out paragraphs containing those phrases. These are a pre-existing long list of verb-noun combinations.</p>
<p>The wider purpose of this exercise is to pore through a large set of PDF documents to analyze what types of activities were undertaken, by whom, and with what frequency. The task is split into two parts. The first is to extract paragraphs containing these phrases(verb-noun etc) for humans to look at and verify a random sample of so we know the parsing is working properly. Then using other characteristics associated with each PDF, do further analysis of the types of tasks (drafting/create/perform &gt; document/task type &quot;x&quot;) being performed, by whom, when, etc etc.</p>
<p>One example is &quot;draft/prepare&quot; &gt; &quot;procurement and market risk assessment&quot;.</p>
<p>I looked at the dependency tree of a sample sentence and then set up the Dependency matcher to work with that. Please see example below.</p>
<p>The sample sentence is &quot;He drafted the procurement &amp; market risk assessment&quot;. The dependency seems to be <em><strong>draft &gt; assessment &gt; procurement &gt; risk &gt; market</strong></em></p>
<p><a href=""https://i.sstatic.net/HPZm9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HPZm9.png"" alt=""enter image description here"" /></a></p>
<pre><code>    import spacy
    nlp = spacy.load('en_core_web_sm')
    from spacy.matcher import DependencyMatcher
    
    dmatcher = DependencyMatcher(nlp.vocab)
    
    doc = nlp(&quot;&quot;&quot;He drafted the procurement &amp; market risk assessment.&quot;&quot;&quot;)
    
    lemma_list_drpr = ['draft', 'prepare']
    
    print(&quot;----- Using Dependency Matcher -----&quot;)
    
    deppattern22 = [
        {'SPEC' : {&quot;NODE_NAME&quot;: &quot;drpr&quot;}, &quot;PATTERN&quot;:{&quot;LEMMA&quot;: {&quot;IN&quot;: lemma_list_drpr}}},
        {&quot;SPEC&quot;: {&quot;NBOR_NAME&quot;: &quot;drpr&quot;, &quot;NBOR_RELOP&quot;: &quot;&gt;&quot;, &quot;NODE_NAME&quot;: &quot;ass2&quot;}, &quot;PATTERN&quot;: 
              {&quot;LEMMA&quot;: &quot;assessment&quot;}},
        {&quot;SPEC&quot;: {&quot;NBOR_NAME&quot;: &quot;ass2&quot;, &quot;NBOR_RELOP&quot;: &quot;&gt;&quot;, &quot;NODE_NAME&quot;: &quot;proc2&quot;}, &quot;PATTERN&quot;: 
             {&quot;LEMMA&quot;: &quot;procurement&quot;}}
        ]
    
    dmatcher.add(&quot;Pat22&quot;, patterns = [deppattern22])
    
    for number, mylist in dmatcher(doc):
        for item in mylist:
           print(doc[item[0]].sent)
</code></pre>
<p>When I do this, it works.</p>
<p><a href=""https://i.sstatic.net/5tNR9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5tNR9.png"" alt=""enter image description here"" /></a></p>
<p>However, there are many problems here.</p>
<ol>
<li><p>When I try to add &quot;risk&quot; and &quot;market&quot; terms to the matcher then it no longer works:</p>
<pre><code>deppattern22a = [
         {'SPEC' : {&quot;NODE_NAME&quot;: &quot;drpr&quot;}, &quot;PATTERN&quot;:{&quot;LEMMA&quot;: {&quot;IN&quot;: lemma_list_drpr}}},
         {&quot;SPEC&quot;: {&quot;NBOR_NAME&quot;: &quot;drpr&quot;, &quot;NBOR_RELOP&quot;: &quot;&gt;&quot;, &quot;NODE_NAME&quot;: &quot;ass2&quot;}, &quot;PATTERN&quot;: 
               {&quot;LEMMA&quot;: &quot;assessment&quot;}},
         {&quot;SPEC&quot;: {&quot;NBOR_NAME&quot;: &quot;ass2&quot;, &quot;NBOR_RELOP&quot;: &quot;&gt;&quot;, &quot;NODE_NAME&quot;: &quot;proc2&quot;}, &quot;PATTERN&quot;: 
              {&quot;LEMMA&quot;: &quot;procurement&quot;}},
           {&quot;SPEC&quot;: {&quot;NBOR_NAME&quot;: &quot;proc2&quot;, &quot;NBOR_RELOP&quot;: &quot;&gt;&quot;, &quot;NODE_NAME&quot;: &quot;risk2&quot;}, &quot;PATTERN&quot;: 
             {&quot;LEMMA&quot;: &quot;risk&quot;}},
          {&quot;SPEC&quot;: {&quot;NBOR_NAME&quot;: &quot;risk2&quot;, &quot;NBOR_RELOP&quot;: &quot;&gt;&quot;, &quot;NODE_NAME&quot;: &quot;mkt2&quot;}, &quot;PATTERN&quot;: 
               {&quot;LEMMA&quot;: &quot;market&quot;}}
         ]
</code></pre>
</li>
<li><p>Moreover, when I change the sentence text a little bit, by replacing &quot;&amp;&quot; by &quot;and&quot; then the dependency changes so my dependency matcher doesn't work again. The dependency becomes <em><strong>draft &gt; procurement &gt; assessment &gt; ...</strong></em> whereas in the earlier sample sentence it was <em><strong>draft &gt; assessment &gt; procurement &gt; ...</strong></em></p>
</li>
</ol>
<p><a href=""https://i.sstatic.net/f77iB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/f77iB.png"" alt=""enter image description here"" /></a></p>
<ol start=""3"">
<li>The dependency changes back when I add other text to the sentence.</li>
</ol>
<p><a href=""https://i.sstatic.net/WZD2H.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WZD2H.png"" alt=""enter image description here"" /></a></p>
<p>What would be a good way to find such matches that are not sensitive to minor changes in sentence structure?</p>
",Parsing & POS Tagging,spacy dependency matcher problematic sensitive long verb noun phrase trying construct dependency matcher catch certain phrase document print paragraph containing phrase pre existing long list verb noun combination wider purpose exercise pore large set pdf document analyze type activity undertaken frequency task split two part first extract paragraph containing phrase verb noun etc human look verify random sample know parsing working properly using characteristic associated pdf analysis type task drafting create perform document task type x performed etc etc one example draft prepare procurement market risk assessment looked dependency tree sample sentence set dependency matcher work please see example sample sentence drafted procurement market risk assessment dependency seems draft assessment procurement risk market work however many problem try add risk market term matcher longer work moreover change sentence text little bit replacing dependency change dependency matcher work dependency becomes draft procurement assessment whereas earlier sample sentence wa draft assessment procurement dependency change back add text sentence would good way find match sensitive minor change sentence structure
Tags in Google Ngrams dataset,"<p><strong>tl;dr</strong> : I can't find a comprehensive list of all tags used in <a href=""http://storage.googleapis.com/books/ngrams/books/datasetsv2.html"" rel=""nofollow noreferrer"">Google Grams Dataset</a> besides <a href=""https://books.google.com/ngrams/info"" rel=""nofollow noreferrer"">that one</a> which only includes PoS tags and <code>_START_</code>, <code>_ROOT_</code> and <code>_END_</code>.  </p>

<p>What do tokens like <code>,_.</code>, <code>._.</code>, <code>_._</code> mean ? Given their frequencies -- see below -- I'd strongly assume they're <strong>tags</strong> (they can't be proper tokens).</p>

<hr>

<p><strong>Context :</strong><br>
I am trying to extract information from Google's n-grams dataset and have troubles understanding some of their tags, and how to take them into account.</p>

<p>Ultimately, I would like to approximate how likely a word will follow another one.<br>
For example, calculating how likely the token <code>protection</code> will follow <code>equal</code> would roughly mean calculating <code>count(""equal protection"") / count(""equal *"")</code> where <code>*</code> is the wildcard : any 1gram in the corpus.</p>

<p>The tricky part is calculating that <code>count(""equal *"")</code>.<br>
Indeed, for example, the bi-gram <code>equal to</code> accounts many times in the Google n-grams dataset : </p>

<ul>
<li>as <code>equal to</code>, </li>
<li>as <code>equal to_PRT</code> (disambiguated PoS version)</li>
<li>as <code>equal _PRT_</code> (aggregated for all PRT i.e. particles that might follow <code>equal</code>).</li>
</ul>

<p>As shows when I compute this on pyspark :</p>

<pre><code>&gt;&gt;&gt; total = ggrams.filter(ggrams.ngram.startswith(""equal "")).groupby(""ngram"") \
             .sum(""match_count"")

&gt;&gt;&gt; total.sort(""sum(match_count)"", ascending=False).show(n=15)

+------------+----------------+  
|       ngram|sum(match_count)|  
+------------+----------------+  
|equal _NOUN_|        20130934|  
| equal _PRT_|        16620727|  
|    equal to|        16598291|  
|equal to_PRT|        16598291|  
|   equal _._|         5119672|  
| equal _ADP_|         3037747|  
|     equal ,|         2276119|  
|   equal ,_.|         2276119|  
|    equal in|         1682835|  
|equal in_ADP|         1682176|  
|     equal .|         1628257|  
|   equal ._.|         1628257|  
|equal _CONJ_|         1363739|  
|    ...     |             ...|  
</code></pre>

<p>So to avoid accounting the same bigram multiple times, my idea was to rather just sum all counts for all patterns like <code>""equal &lt;POS&gt;""</code> where <code>&lt;POS&gt;</code> is in the described PoS set <code>[_PRT_, _NOUN_, ...]</code> (findable <a href=""https://books.google.com/ngrams/info"" rel=""nofollow noreferrer"">here</a>)</p>

<p>Doing this I obtain sum figures that are 1/3rd of the one I'd get from the displayed dataframe above. Which strenghthen my hypothesis above that one count will account three times. But I can't help persuading myself what the best way to do it is, especially notifying these weird tokens <code>,_.</code>, <code>._.</code>, <code>_._</code> which meanings I don't have any clue.</p>
",Parsing & POS Tagging,tag google ngrams dataset tl dr find comprehensive list tag used google gram dataset besides one includes po tag token like mean given frequency see strongly assume tag proper token context trying extract information google n gram dataset trouble understanding tag take account ultimately would like approximate likely word follow another one example calculating likely token follow would roughly mean calculating wildcard gram corpus tricky part calculating indeed example bi gram account many time google n gram dataset disambiguated po version aggregated prt e particle might follow show compute pyspark avoid accounting bigram multiple time idea wa rather sum count pattern like described po set findable obtain sum figure rd one get displayed dataframe strenghthen hypothesis one count account three time help persuading best way especially notifying weird token meaning clue
How can use Python to mark words in a sentence string depending on whether they come after one specific word and before a full stop?,"<p>I have a list of strings containing job descriptions like the following:</p>
<p><code>direct or coordinate an organization's financial or budget activities to fund operations, maximize investments, or increase efficiency. may serve as liaisons between organizations, shareholders, and outside organizations. may attend and participate in meetings of municipal councils or council committees. represent organizations or promote their objectives at official functions, or delegate representatives to do so.</code></p>
<p>I already have some python code that splits up each word in the description, and gives it a number of attributes, for example how many times it appears in the description, its position (in terms of numerical rank) or its POS tag (whether it's a noun, verb etc.). So for example, if the job description was just &quot;plan schedules&quot;, my program can already give me the following:</p>
<p><code>[('plan', 'plan', 'NN', 0, 2, 5, 'construction managers', '11-9021.00', 245), ('schedule', 'schedul', 'NN', 1, 1, 1, 'construction managers', '11-9021.00', 245)]</code></p>
<p>I wanted to add to this a flag/boolean which would highlight, for each word in the definition, whether it comes <em>after</em> the word 'may' and <em>before</em> a full stop. Essentially, I would be looking for a list of booleans for each description, which I could zip to the above structure as the 10th attribute and know for each word whether it comes between 'may' and a full stop.</p>
<p>Any suggestions on how I could achieve this?</p>
",Parsing & POS Tagging,use python mark word sentence string depending whether come one specific word full stop list string containing job description like following already python code split word description give number attribute example many time appears description position term numerical rank po tag whether noun verb etc example job description wa plan schedule program already give following wanted add flag boolean would highlight word definition whether come word may full stop essentially would looking list booleans description could zip structure th attribute know word whether come may full stop suggestion could achieve
How to lemmatise nouns?,"<p>I am trying to lemmatise words like &quot;Escalation&quot; to &quot;Escalate&quot; using NLTK.stem Wordlemmatizer.</p>
<pre><code>word_lem = WordNetLemmatizer()

print( word_lem.lemmatize(&quot;escalation&quot;, pos = &quot;n&quot;)
</code></pre>
<p>Which pos tag should be used to get result like &quot;escalate&quot;</p>
",Parsing & POS Tagging,lemmatise noun trying lemmatise word like escalation escalate using nltk stem wordlemmatizer po tag used get result like escalate
Python NLTK pos_tag not returning the correct part-of-speech tag,"<p>Having this:</p>

<pre><code>text = word_tokenize(""The quick brown fox jumps over the lazy dog"")
</code></pre>

<p>And running:</p>

<pre><code>nltk.pos_tag(text)
</code></pre>

<p>I get:</p>

<pre><code>[('The', 'DT'), ('quick', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'NN'), ('dog', 'NN')]
</code></pre>

<p>This is incorrect. The tags for <code>quick brown lazy</code> in the sentence should be:</p>

<pre><code>('quick', 'JJ'), ('brown', 'JJ') , ('lazy', 'JJ')
</code></pre>

<p>Testing this through their <a href=""http://nlp.stanford.edu:8080/corenlp/process"">online tool</a> gives the same result; <code>quick</code>, <code>brown</code> and <code>fox</code> should be adjectives not nouns.</p>
",Parsing & POS Tagging,python nltk po tag returning correct part speech tag running get incorrect tag sentence testing href tool give result adjective noun
How to conjugate a verb in NLTK given POS tag?,"<p>Given a POS tag, such as VBD, how can I conjugate a verb to match with NLTK?</p>

<p>e.g.</p>

<pre><code>VERB: go
POS: VBD
RESULT: went
</code></pre>
",Parsing & POS Tagging,conjugate verb nltk given po tag given po tag vbd conjugate verb match nltk e g
Difference in Tense when using Spacy POS on the same sentence on different PCs,"<p>I was taking a course on Udemy about NLP. So I reached the POS section. So when the instructor was demonstrating the POS tag attribute, I was having a difference in tense on the same sentence which we both were entering.</p>
<p>So I was using the sentence &quot;I read books on SpaCy.&quot;. He was also using the same sentence but when we applied token.tag_ I got VBD where as he got VBP. Can anyone explain why it happened?</p>
",Parsing & POS Tagging,difference tense using spacy po sentence different pc wa taking course udemy nlp reached po section instructor wa demonstrating po tag attribute wa difference tense sentence entering wa using sentence read book spacy wa also using sentence applied token tag got vbd got vbp anyone explain happened
Spacy Dependency Parsing with Pandas dataframe,"<p>I would like to extract noun-adjective pair for Aspect Based Sentiment Analysis using Spacy's Dependency parser on my pandas dataframe. I was trying this code on Amazon fine food reviews dataset from Kaggle: <a href=""https://stackoverflow.com/questions/60967134/named-entity-recognition-in-aspect-opinion-extraction-using-dependency-rule-matc"">Named Entity Recognition in aspect-opinion extraction using dependency rule matching</a></p>
<p>However, something seems to be wrong the way I feed my pandas dataframe to spacy. My results are not the way I would expect them to be. Could someone help me debug this please. Thanks a lot.</p>
<pre><code>!python -m spacy download en_core_web_lg
import nltk
nltk.download('vader_lexicon')

import spacy
nlp = spacy.load(&quot;en_core_web_lg&quot;)

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()


def find_sentiment(doc):
    # find roots of all entities in the text
  for i in df['Text'].tolist():
    doc = nlp(i)
    ner_heads = {ent.root.idx: ent for ent in doc.ents}
    rule3_pairs = []
    for token in doc:
        children = token.children
        A = &quot;999999&quot;
        M = &quot;999999&quot;
        add_neg_pfx = False
        for child in children:
            if(child.dep_ == &quot;nsubj&quot; and not child.is_stop): # nsubj is nominal subject
                if child.idx in ner_heads:
                    A = ner_heads[child.idx].text
                else:
                    A = child.text
            if(child.dep_ == &quot;acomp&quot; and not child.is_stop): # acomp is adjectival complement
                M = child.text
            # example - 'this could have been better' -&gt; (this, not better)
            if(child.dep_ == &quot;aux&quot; and child.tag_ == &quot;MD&quot;): # MD is modal auxiliary
                neg_prefix = &quot;not&quot;
                add_neg_pfx = True
            if(child.dep_ == &quot;neg&quot;): # neg is negation
                neg_prefix = child.text
                add_neg_pfx = True
        if (add_neg_pfx and M != &quot;999999&quot;):
            M = neg_prefix + &quot; &quot; + M
        if(A != &quot;999999&quot; and M != &quot;999999&quot;):
            rule3_pairs.append((A, M, sid.polarity_scores(M)['compound']))
    return rule3_pairs
df['three_tuples'] = df['Text'].apply(find_sentiment) 
df.head()
</code></pre>
<p>My result is coming like this which clearly means something is wrong with my loop:
<a href=""https://i.sstatic.net/TXp2I.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TXp2I.png"" alt=""enter image description here"" /></a></p>
",Parsing & POS Tagging,spacy dependency parsing panda dataframe would like extract noun adjective pair aspect based sentiment analysis using spacy dependency parser panda dataframe wa trying code amazon fine food review dataset kaggle
How do we find out Adjective Verb Adverb Phrases in sentence using NTLK?,"<p>Below code from StackOverflow. (It is working for NOUN PHRASE but I need for VERB, ADVERB, ADJECTIVE PHRASES). I am new to NLP and don't know much about the grammar rules. I tried to get help from blogs and documentation but unable to figure it out.</p>
<p>Changes Required in this block of code.</p>
<h1>Rule for NP chunk and VB Chunk</h1>
<pre><code>    NBAR:
        {&lt;NN.*|JJ&gt;*&lt;NN.*&gt;}  # Nouns and Adjectives, terminated with Nouns
        {&lt;RB.?&gt;*&lt;VB.?&gt;*&lt;JJ&gt;*&lt;VB.?&gt;+&lt;VB&gt;?} # Verbs and Verb Phrases

    NP:
        {&lt;NBAR&gt;}
        {&lt;NBAR&gt;&lt;IN&gt;&lt;NBAR&gt;}  # Above, connected with in/of/etc...
</code></pre>
<h1>Complete Code (it is working for noun phrase)</h1>
<pre><code>from nltk import word_tokenize, pos_tag
from nltk.corpus import wordnet

from IPython.display import display
lemmatizer = nltk.WordNetLemmatizer()

#word tokenizeing and part-of-speech tagger
document = 'The little brown dog barked at the black cat'
tokens = [nltk.word_tokenize(sent) for sent in [document]]
postag = [nltk.pos_tag(sent) for sent in tokens][0]

# Rule for NP chunk and VB Chunk
grammar = r&quot;&quot;&quot;
    NBAR:
        {&lt;NN.*|JJ&gt;*&lt;NN.*&gt;}  # Nouns and Adjectives, terminated with Nouns
        {&lt;RB.?&gt;*&lt;VB.?&gt;*&lt;JJ&gt;*&lt;VB.?&gt;+&lt;VB&gt;?} # Verbs and Verb Phrases

    NP:
        {&lt;NBAR&gt;}
        {&lt;NBAR&gt;&lt;IN&gt;&lt;NBAR&gt;}  # Above, connected with in/of/etc...

&quot;&quot;&quot;
#Chunking
cp = nltk.RegexpParser(grammar)

# the result is a tree
tree = cp.parse(postag)

def leaves(tree):
    &quot;&quot;&quot;Finds NP (nounphrase) leaf nodes of a chunk tree.&quot;&quot;&quot;
    for subtree in tree.subtrees(filter = lambda t: t.label() =='NP'):
        yield subtree.leaves()

def get_word_postag(word):
    if pos_tag([word])[0][1].startswith('J'):
        return wordnet.ADJ
    if pos_tag([word])[0][1].startswith('V'):
        return wordnet.VERB
    if pos_tag([word])[0][1].startswith('N'):
        return wordnet.NOUN
    else:
        return wordnet.NOUN

def normalise(word):
    &quot;&quot;&quot;Normalises words to lowercase and stems and lemmatizes it.&quot;&quot;&quot;
    word = word.lower()
    postag = get_word_postag(word)
    word = lemmatizer.lemmatize(word,postag)
    return word

def get_terms(tree):    
    for leaf in leaves(tree):
        terms = [normalise(w) for w,t in leaf]
        yield terms

terms = get_terms(tree)

features = []
for term in terms:
    _term = ''
    for word in term:
        _term += ' ' + word
    features.append(_term.strip())
features
</code></pre>
",Parsing & POS Tagging,find adjective verb adverb phrase sentence using ntlk code stackoverflow working noun phrase need verb adverb adjective phrase new nlp know much grammar rule tried get help blog documentation unable figure change required block code rule np chunk vb chunk complete code working noun phrase
Convert String Token into Tree in Python (Stanford NLP),"<p>I am working on <a href=""https://github.com/stanfordnlp/stanfordnlp"" rel=""nofollow noreferrer"">Stanford NLP</a> for one my <code>Python</code> project. I want to fetch <code>word</code>, <code>lemma</code>, <code>xpos</code>, <code>governor</code> and <code>dependencies</code> from it. But the output produced by the API is in String format and like this :</p>
<pre><code>&lt;Token index=4;words=[&lt;Word index=4;text=born;lemma=bear;upos=VERB;xpos=VBN;feats=Tense=Past|VerbForm=Part|Voice=Pass;governor=0;dependency_relation=root&gt;]&gt;
&lt;Token index=5;words=[&lt;Word index=5;text=in;lemma=in;upos=ADP;xpos=IN;feats=_;governor=6;dependency_relation=case&gt;]&gt;
&lt;Token index=6;words=[&lt;Word index=6;text=Hawaii;lemma=Hawaii;upos=PROPN;xpos=NNP;feats=Number=Sing;governor=4;dependency_relation=obl&gt;]&gt;
&lt;Token index=7;words=[&lt;Word index=7;text=.;lemma=.;upos=PUNCT;xpos=.;feats=_;governor=4;dependency_relation=punct&gt;]&gt;
</code></pre>
<p>I want to know how to parse the result to get it into an easy and accessible format. Or Can I convert it into tree form? Or Is there any other library available that gives me lemma, pos tag and dependencies like this?</p>
",Parsing & POS Tagging,convert string token tree python stanford nlp working stanford nlp one project want fetch output produced api string format like want know parse result get easy accessible format convert tree form library available give lemma po tag dependency like
How to create parse tree for generalized way to split unique sentences by subordinate or independent clauses?,"<p>I have been looking for a way to take a sentence and split it into clauses. So far I have found this example online: <a href=""https://stackoverflow.com/questions/39320015/how-to-split-an-nlp-parse-tree-to-clauses-independent-and-subordinate"">How to split an NLP parse tree to clauses (independent and subordinate)?</a></p>
<p>However, the parse tree is given in this example. How can I create one for any sentence? I would like to find out how to create the parse tree that is used for the solution's input in Python.</p>
",Parsing & POS Tagging,create parse tree generalized way split unique sentence subordinate independent clause looking way take sentence split clause far found example online href split nlp parse tree clause independent subordinate however parse tree given example create one sentence would like find create parse tree used solution input python
Stanford Core NLP Tree Parser Sentence Limits wrong - suggestions?,"<p>I'm dealing with <em>german</em> law documents and would like to generate parse trees for sentences. I could find and use Standford <a href=""https://nlp.stanford.edu/software/lex-parser.html"" rel=""nofollow noreferrer"">CoreNLP Parser</a>. However, it does not recognize sentence limits as good as other tools (e.g. spaCy) when parsing the sentences of a document. For example, it would break sentences at every single '.'-character, incl. the dot at the end of abbreviations such as &quot;incl.&quot;)
Since it is crucial to cover the whole sentence for creating syntax trees, this does not really work out for me.</p>
<p>I would appreciate any suggestions to tackle this problem, espacially pointers to other software that might be better suited for my problem. If I overlooked the possibility to tweak the Stanford parser, I would be very grateful for any hints on how to make it better detect sentence limits.</p>
",Parsing & POS Tagging,stanford core nlp tree parser sentence limit wrong suggestion dealing german law document would like generate parse tree sentence could find use standford corenlp parser however doe recognize sentence limit good tool e g spacy parsing sentence document example would break sentence every single character incl dot end abbreviation incl since crucial cover whole sentence creating syntax tree doe really work would appreciate suggestion tackle problem espacially pointer software might better suited problem overlooked possibility tweak stanford parser would grateful hint make better detect sentence limit
"What is it called, to extract an address from HTML via NLP","<p>I have 300k+ html documents, which I want to extract postal addresses from. The data is different structures, so regex wont work.</p>

<p>I have done a heap of reading on NLP and NLTK for python, however I am still struggling on where to start with this.</p>

<p>Is this approach called Part-of-Speech Tagging or Chunking / Partial Parsing? I can't find any document on how to actually TAG a page so I can train a model on it, or even what I should be training.</p>

<p>So my questions;</p>

<ul>
<li>What is this approach called?</li>
<li>How can I tag some documents to train from</li>
</ul>
",Parsing & POS Tagging,called extract address html via nlp k html document want extract postal address data different structure regex wont work done heap reading nlp nltk python however still struggling start approach called part speech tagging chunking partial parsing find document actually tag page train model even training question approach called tag document train
Function to Parse AST Tree Into String of Rules,"<p>I'm currently working on a code generation problem, I'm trying to create a dataset of different scripts and their plain-text AST trees, so that I can map text to code (not the code directly but the AST tree instead).</p>
<p>Dataset sample:</p>
<pre><code>       script                              AST tree rules                               
'params_fu_nc_na_me = []'       &quot;root Assign targets Name id params_fu_nc_na_me ^ ^ ^ ^ value List ^ ^ ^ ^&quot;
</code></pre>
<p>Now, In order to create this dataset, I need to, given a script like this:</p>
<pre><code>params_fu_nc_na_me = []
</code></pre>
<p>Get the rules from the ast tree, to get the rules from an ast tree, first I need to get the ast tree of the script:</p>
<pre><code>import ast

st = ast.parse('params_fu_nc_na_me = []')
ast_tree = ast.dump(st)
print(ast_tree)

# &quot;Module(body=[Assign(targets=[Name(id='params_fu_nc_na_me', ctx=Store())], value=List(elts=[], ctx=Load()))])&quot;
</code></pre>
<p>And then, I need to pass this <code>ast_tree</code> variable to a function which output should be:</p>
<pre><code>parsed_ast_tree = &quot;root Assign targets Name id params_fu_nc_na_me ^ ^ ^ ^ value List ^ ^ ^ ^&quot;
</code></pre>
<p>I have seen this representation on <a href=""https://github.com/zysszy/TreeGen/blob/master/ATIS/ground/1.txt"" rel=""nofollow noreferrer"">this repository</a>, in the repository, there is no preprocessing step, it just gets directly the ast tree rules, but in my case I need to process this rules from the ast tree.</p>
<p>So this is my issue: Having <code>ast_tree</code> as input, I need to create a function that outputs <code>parsed_ast_tree</code>. Then I could generalize the function to match more complex inputs but If you could provide me some ideas of how to start with this, that would be awesome.</p>
",Parsing & POS Tagging,function parse ast tree string rule currently working code generation problem trying create dataset different script plain text ast tree map text code code directly ast tree instead dataset sample order create dataset need given script like get rule ast tree get rule ast tree first need get ast tree script need pas variable function output seen representation repository repository preprocessing step get directly ast tree rule case need process rule ast tree issue input need create function output could generalize function match complex input could provide idea start would awesome
Does spaCy retokenizer do the dependency parsing again?,"<p>I am retokenizing some spaCy docs and then I need the dependency trees (<code>&quot;parser&quot;</code> pipeline component) for them.</p>
<p>However, I do not know for certain if spaCy handles this correctly. I could not find any info about how the retokenizer works in the docs and the spacy tutorial. The only thing I found is the original retokenizer cython source code and they do handle the dependencies, however it looks like they only address them, they don't do the analysis again.</p>
<p>So I need to know if I can trust that for any weird retokenizations I could make, or I have to make the dependency tree again.</p>
",Parsing & POS Tagging,doe spacy retokenizer dependency parsing retokenizing spacy doc need dependency tree pipeline component however know certain spacy handle correctly could find info retokenizer work doc spacy tutorial thing found original retokenizer cython source code handle dependency however look like address analysis need know trust weird retokenizations could make make dependency tree
Extracting Prepositional Phrases from Sentence,"<p>I'm trying to extract prepositional phrases from sentences using NLTK. Is there a way for me to do this automatically (e.g. feed a function a sentence and get back its prepositional phrases)?</p>

<p>The examples <a href=""http://nltk.org/book/ch08.html"" rel=""nofollow"">here</a> seem to require that you start with a grammar before you can get a parse tree. Can I automatically get the grammar and use that to get the parse tree?</p>

<p>Obviously I could tag a sentence, pick out prepositions and the subsequent noun, but this is complicated when the prepositional complement is compound. </p>
",Parsing & POS Tagging,extracting prepositional phrase sentence trying extract prepositional phrase sentence using nltk way automatically e g feed function sentence get back prepositional phrase example seem require start grammar get parse tree automatically get grammar use get parse tree obviously could tag sentence pick preposition subsequent noun complicated prepositional complement compound
Is there a way to force the Apache OpenNLP parser to see a verb phrase instead of a noun phrase?,"<p>I'm writing a command parser using Apache's OpenNLP. The problem is that OpenNLP sees some commands as noun phrases. For example, if I parse something like &quot;open door&quot;, OpenNLP gives me <code>(NP (JJ open) (NN door))</code>. In other words, it sees the phrase as &quot;an open door&quot; instead of &quot;open the door&quot;. I want it to parse as <code>(VP (VB open) (NP (NN door)))</code>. If I parse &quot;open the door&quot; it produces a VP, But I can't count on a person using determiners.</p>
<p>I'm currently trying to figure out how to perform surgery on the incorrect parse tree but the API documentation is severely lacking.</p>
",Parsing & POS Tagging,way force apache opennlp parser see verb phrase instead noun phrase writing command parser using apache opennlp problem opennlp see command noun phrase example parse something like open door opennlp give word see phrase open door instead open door want parse parse open door produce vp count person using determiner currently trying figure perform surgery incorrect parse tree api documentation severely lacking
How to turn spacy doc into nested list of tokens,"<p>Im using spacy and stanfordnlp for dependency parsing and i got a spacy doc. How could i turn that doc into nested list, where each sublist consists of children tokens of head</p>
",Parsing & POS Tagging,turn spacy doc nested list token im using spacy stanfordnlp dependency parsing got spacy doc could turn doc nested list sublist consists child token head
count elements of nested lists of a list python,"<p>I have a list of lists of tokens of noun phrase chunks:</p>
<pre class=""lang-py prettyprint-override""><code>NP = [[&quot;The dog&quot;], [&quot;it&quot;], [&quot;black car&quot;], [&quot;one cow&quot;], [&quot;the gift in the box&quot;]]
</code></pre>
<p>I need to count the number of tokens inside each list. So, <code>NP[0]</code> is <code>[The dog]</code>, and <code>&quot;the dog&quot;</code> is two tokens. How can I count this for every element in the nested lists?</p>
",Parsing & POS Tagging,count element nested list list python list list token noun phrase chunk need count number token inside list two token count every element nested list
NLTK Parts Of Speech Tag is considering &quot;apps&quot; and &quot;please&quot; as verbs?,"<p>Problem statement: lemmatize those words which are verbs.</p>
<p>Text:</p>
<blockquote>
<p>['I would love to try or hear the sample audio your app can produce.',
&quot;I do not want to purchase, because I've purchased so many apps that
say they do something and do not deliver.&quot;,  &quot;Can you please add audio
samples with text you've converted?&quot;,  &quot;I'd love to see the end
results.&quot;,  'Thanks!']</p>
</blockquote>
<p>Code for <code>POS</code>:</p>
<pre><code>tokenized_word=nltk.word_tokenize(str(file))
postag=nltk.pos_tag(tokenized_word)


def pos_tagger(nltk_tag):
    if nltk_tag.startswith('V'):
        return wordnet.VERB
    else:
        return None
wt=list(map(lambda x: (x[0],pos_tagger(x[1])), postag))
</code></pre>
<p>But it is also picking up <em>apps</em> and <em>please</em> as verbs, since the POS-tags of these are coming out as <code>(&quot;apps&quot;,&quot;VBP&quot;)</code> , <code>(&quot;please&quot;,&quot;VB&quot;)</code>.</p>
<p>How to solve this issue?</p>
",Parsing & POS Tagging,nltk part speech tag considering apps please verb problem statement lemmatize word verb text would love try hear sample audio app produce want purchase purchased many apps say something deliver please add audio sample text converted love see end result thanks code also picking apps please verb since po tag coming solve issue
How to split an NLP parse tree to clauses (independent and subordinate)?,"<p>Given an NLP parse tree like </p>

<pre><code>(ROOT (S (NP (PRP You)) (VP (MD could) (VP (VB say) (SBAR (IN that) (S (NP (PRP they)) (ADVP (RB regularly)) (VP (VB catch) (NP (NP (DT a) (NN shower)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ adds) (PP (TO to) (NP (NP (PRP$ their) (NN exhilaration)) (CC and) (NP (FW joie) (FW de) (FW vivre))))))))))))) (. .)))
</code></pre>

<p>Original sentence is ""You could say that they regularly catch a shower, which adds to their exhilaration and joie de vivre.""</p>

<p>How could the clauses be extracted and reverse engineered?
We would be splitting at S and SBAR (to preserve the type of clause, eg subordinated)</p>

<pre><code> - (S (NP (PRP You)) (VP (MD could) (VP (VB say) 
 - (SBAR (IN that) (S (NP (PRP they)) (ADVP (RB regularly)) (VP (VB catch) (NP (NP (DT a) (NN shower))
 - (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ adds) (PP (TO to)
   (NP (NP (PRP$ their) (NN exhilaration)) (CC and) (NP (FW joie) (FW
   de) (FW vivre))))))))))))) (. .)))
</code></pre>

<p>to arrive at</p>

<pre><code> - You could say
 - that they regularly catch a shower 
 - , which adds to their exhilaration and joie de vivre.
</code></pre>

<p>Splitting at S and SBAR seems very easy. The problem seems to be stripping away all the POS tags and chunks from the fragments.</p>
",Parsing & POS Tagging,split nlp parse tree clause independent subordinate given nlp parse tree like original sentence could say regularly catch shower add exhilaration joie de vivre could clause extracted reverse engineered would splitting sbar preserve type clause eg subordinated arrive splitting sbar seems easy problem seems stripping away po tag chunk fragment
"Python POS tagging: return the most common POS for a word, after training","<p>I am trying to write a code that will create a dictionary with key(word), values(POS tags that word appears with + respective count). <strong>The final goal is to know what POS tag is most frequent for a given word</strong></p>
<p>An example:
<code>most_common({&quot;NOUN&quot;: 2, &quot;DET&quot;: 5, &quot;ADP&quot;: 1 }) returns &quot;DET&quot;</code> because the given word appears most frequently as a determiner.</p>
<p>First, I want to train my code on a small annotated corpus. This is what I have so far:</p>
<pre><code>import pprint

trainfile = open(&quot;small_train.connlu&quot;)
list_of_lists = []
for line in trainfile:
    stripped_line = line.strip()
    line_list = stripped_line.split()
    list_of_lists.append(line_list)

list_keys = [] #a list that will contain all the keys (including duplicates)
list_values = [] #a list that will contain all the values

for line in list_of_lists:
    if line == []:
        pass
    elif line != []:
        list_keys.append(line[1]) # second column of the file contains all words
        list_values.append(line[3]) # fourth column of the file contains all POS tags (see below)

list_keys = [key.lower() for key in list_keys] #lowercase all keys - 'The' and 'the' should be assigned the same POS
</code></pre>
<p>I am stuck at this point. I now need to create a dictionary with all the words that appeared in the corpus followed by the respective POS tags they appeared with (and the count of how many times each word appeared with a certain POS tag). This is the closest I have got:</p>
<pre><code>dict = {}

for key in range(len(list_keys)):
    dict[list_keys[key]] = list_values[key]

pprint.pprint(dict)
</code></pre>
<p>This returns keys with a correct POS tag, however, I don't know how to implement the count. Anything I have tried has resulted in errors.</p>
<p>This is how the training data is formatted (small_train.connlu)</p>
<pre><code>1   The       _   DET   _ _ _ _ _ _
2   hottest   _   ADJ   _ _ _ _ _ _
3   item      _   NOUN  _ _ _ _ _ _
4   on        _   ADP   _ _ _ _ _ _
5   Christmas _   PROPN _ _ _ _ _ _
6   wish      _   NOUN  _ _ _ _ _ _
7   lists     _   NOUN  _ _ _ _ _ _
8   this      _   DET   _ _ _ _ _ _
9   year      _   NOUN  _ _ _ _ _ _
10  is        _   AUX   _ _ _ _ _ _
11  nuclear   _   ADJ   _ _ _ _ _ _
12  weapons   _   NOUN  _ _ _ _ _ _
13  .         _   PUNCT _ _ _ _ _ _

1   I         _   PRON  _ _ _ _ _ _
2   wish      _   VERB  _ _ _ _ _ _
3   you       _   PRON  _ _ _ _ _ _
4   all       _   DET   _ _ _ _ _ _
5   of        _   ADP   _ _ _ _ _ _
6   the       _   DET   _ _ _ _ _ _
7   best      _   ADJ   _ _ _ _ _ _
</code></pre>
<p>I would be very grateful if someone could help. Thanks a lot :)</p>
",Parsing & POS Tagging,python po tagging return common po word training trying write code create dictionary key word value po tag word appears respective count final goal know po tag frequent given word example given word appears frequently determiner first want train code small annotated corpus far stuck point need create dictionary word appeared corpus followed respective po tag appeared count many time word appeared certain po tag closest got return key correct po tag however know implement count anything tried ha resulted error training data formatted small train connlu would grateful someone could help thanks lot
Clause extraction / long sentence segmentation in python,"<p>I'm currently working on a project involving sentence vectors (from a RoBERTa pretrained model). These vectors are lower quality when sentences are long, and my corpus contains many long sentences with subclauses.</p>
<p>I've been looking for methods for clause extraction / long sentence segmentation, but I was surprised to see that none of the major NLP packages (e.g., spacy or stanza) offer this out of the box.</p>
<p>I suppose this could be done by using either spacy or stanza's dependency parsing, but it would probably be quite complicated to handle all kinds of convoluted sentences and edge cases properly.</p>
<p>I've come across <a href=""https://github.com/mmxgn/spacy-clausie"" rel=""noreferrer"">this implementation</a> of the the ClausIE information extraction system with spacy that does something similar, but it hasn't been updated and doesn't work on my machine.</p>
<p>I've also come across <a href=""https://github.com/freyamehta99/Sentence-Simplification"" rel=""noreferrer"">this repo</a> for sentence simplification, but I get an annotation error from Stanford coreNLP when I run it locally.</p>
<p>Is there any obvious package/method that I've overlooked? If not, is there a simple way to implement this with stanza or spacy?</p>
",Parsing & POS Tagging,clause extraction long sentence segmentation python currently working project involving sentence vector roberta pretrained model vector lower quality sentence long corpus contains many long sentence subclauses looking method clause extraction long sentence segmentation wa surprised see none major nlp package e g spacy stanza offer box suppose could done using either spacy stanza dependency parsing would probably quite complicated handle kind convoluted sentence edge case properly come across implementation clausie information extraction system spacy doe something similar updated work machine also come across repo sentence simplification get annotation error stanford corenlp run locally obvious package method overlooked simple way implement stanza spacy
How to best store tokens in a container?,"<p>I'm rather new to programming and am trying to create a small parser with the spacy package.
What I would like to do would be to parse through any kind of text (as string), and store each word plus its POS-tag. I thought about doing this with a dictionary in which each word is the key and its POS-tag its value so it would look like this:</p>
<pre><code>    import spacy
    tokendictionary = {}
    nlp = spacy.load(&quot;en_core_web_sm&quot;)
    doc = nlp(&quot;I will not regret this, so this is not a regret.&quot;)
    for token in doc:
         tokendictionary[token] = token.pos_
</code></pre>
<p>And I would like to get something like this:</p>
<pre><code>    {I: 'PRON', will: 'VERB', not: 'PART', regret: 'VERB', this: 'DET', ,: 'PUNCT', so: 'CCONJ', this: 'DET', is: 'AUX', not: 'PART', a: 'DET', regret: 'NOUN', .: 'PUNCT', Do: 'AUX', you: 'PRON', regret: 'VERB', this: 'DET', ?: 'PUNCT'}
</code></pre>
<p>However, I know I cannot store identical keys, but I need to keep each token stored (like in this example for 'regret', so even if it appears twice or more, it should still be stored individually and should be easily accessible. Which would be the best way to do that?</p>
",Parsing & POS Tagging,best store token container rather new programming trying create small parser spacy package would like would parse kind text string store word plus po tag thought dictionary word key po tag value would look like would like get something like however know store identical key need keep token stored like example regret even appears twice still stored individually easily accessible would best way
Not condition in NLTK Regex Parser,"<p>I need to create a not condition as part of my grammar in NLTK's regex parser. I would like to chunk those words which are of structure <code>'Coffee &amp; Tea'</code> but it should not chunk if there is a word of type <code>&lt;IN&gt;</code> before the sequence. For example <code>'in London and Paris'</code> should not be chunked by the parser.</p>

<p>My code is as follows:</p>

<pre><code>grammar = r'''NP: {(^&lt;IN&gt;)&lt;NNP&gt;+&lt;CC&gt;&lt;NN.*&gt;+}'''
</code></pre>

<p>I tried the above grammar to solve the problem but it is not working could someone please tell me what I am doing wrong.</p>

<p>Example:</p>

<pre><code>def parse_sentence(sentence):
    pos_sentence = nltk.pos_tag(nltk.word_tokenize(sentence))
    grammar = r'''NP: {&lt;NNP&gt;+&lt;CC&gt;&lt;NN.*&gt;+}'''
    parser = nltk.RegexpParser(grammar)
    result = parser.parse(pos_sentence)
    print result

sentence1 = 'Who is the front man of the band that wrote Coffee &amp; TV?'
parse_sentence(sentence1)

sentence2 = 'Who of those resting in Westminster Abbey wrote a book set in London and Paris?'
parse_sentence(sentence2)

Result for sentence 1 is:
(S
  Who/WP
  is/VBZ
  the/DT
  front/JJ
  man/NN
  of/IN
  the/DT
  band/NN
  that/WDT
  wrote/VBD
  (NP Coffee/NNP &amp;/CC TV/NN)
  ?/.)

Result for sentence2 is:
(S
  Who/WP
  of/IN
  those/DT
  resting/VBG
  in/IN
  Westminster/NNP
  Abbey/NNP
  wrote/VBD
  a/DT
  book/NN
  set/VBN
  in/IN
  (NP London/NNP and/CC Paris/NNP)
  ?/.)
</code></pre>

<p>As can be seen in both sentence1 and sentence2 the phrases <code>Coffee &amp; Tea</code> and <code>London and Paris</code> get chunked as a group although I do not wish to chunk <code>London and Paris</code>. One way of doing that is to ignore those patterns which are preceded by a <code>&lt;IN&gt;</code> POS Tag.</p>

<p><strong>In a nutshell I need to know how to add NOT(negation) conditions for POS tags in a regex parser's grammar. Standard syntax of using '^' followed by the tag definition does not seem to work</strong></p>
",Parsing & POS Tagging,condition nltk regex parser need create condition part grammar nltk regex parser would like chunk word structure chunk word type sequence example chunked parser code follows tried grammar solve problem working could someone please tell wrong example seen sentence sentence phrase get chunked group although wish chunk one way ignore pattern preceded po tag nutshell need know add negation condition po tag regex parser grammar standard syntax using followed tag definition doe seem work
Extract Main- and Subclauses from German Sentence with SpaCy,"<p>In German, how can I extract the main- and subclauses (aka &quot;subordinate clauses&quot;, &quot;dependent clauses&quot;) from a sentence with SpaCy?</p>
<p>I know how to use SpaCy's tokenizer, part-of-speech tagging and dependency parser, but I cannot figure out how to represent the grammatical rules of German using the information SpaCy can extract.</p>
",Parsing & POS Tagging,extract main subclauses german sentence spacy german extract main subclauses aka subordinate clause dependent clause sentence spacy know use spacy tokenizer part speech tagging dependency parser figure represent grammatical rule german using information spacy extract
How to force a pos tag in spacy before/after tagger?,"<p>If I process the sentence </p>

<blockquote>
  <p>'Return target card to your hand'</p>
</blockquote>

<p>with spacy and the en_web_core_lg model, it recognize the tokens as below:</p>

<blockquote>
  <p>Return NOUN target NOUN card NOUN to ADP your ADJ hand NOUN</p>
</blockquote>

<p><strong>How can I force 'Return' to be tagged as a VERB?</strong> And how can I do it before the parser, so that the parser can better interpret relations between tokens?</p>

<p>There are other situations in which this would be useful. I am dealing with text which contains specific symbols such as <code>{G}</code>. These three characters should be considered a NOUN, as a whole, and <code>{T}</code> should be a VERB. But right now I do not know how to achieve that, without developing a new model for tokenizing and for tagging. If I could ""force"" a token, I could replace these symbols for something that would be recognized as one token and force it to be tagged appropriately. For example, I could replace {G} with SYMBOLG and force tagging SYMBOLG as NOUN.</p>
",Parsing & POS Tagging,force po tag spacy tagger process sentence return target card hand spacy en web core lg model recognize token return noun target noun card noun adp adj hand noun force return tagged verb parser parser better interpret relation token situation would useful dealing text contains specific symbol three character considered noun whole verb right know achieve without developing new model tokenizing tagging could force token could replace symbol something would recognized one token force tagged appropriately example could replace g symbolg force tagging symbolg noun
Adding POS part-of-speech column in pyspark dataframe,"<p>I'm working in pyspark dataframe where I want to add a column to see if the word is NOUN, VERB, ADJ, ADV, ADP, PROPN using pyspark or nltk.pos_tag</p>
<p>Here is the pyspark table.</p>
<pre><code>------------------------
| event_dt   | words   |
------------------------
| 2020-09-02 | mifi    |
| 2020-09-02 | hotspot |
| 2020-09-03 | service |
| 2020-09-03 | word    |
| 2020-09-03 | plan    |
</code></pre>
<p>and both columns continues.</p>
<p>Here is the result I'm looking for using pyspark.</p>
<pre><code>-------------------------------
| event_dt   | words   | pos  |
-------------------------------
| 2020-09-02 | mifi    | ADJ  |
| 2020-09-02 | hotspot | ADJ  |
| 2020-09-03 | service | ADJ  |
| 2020-09-03 | word    | NOUN |
| 2020-09-03 | plan    | NOUN |
</code></pre>
<p>I'm not sure where the words in column 'words' are being compared to get the result being ADJ, NOUN, PRONOUN etc.</p>
<p>Thanks for the help in advance!</p>
",Parsing & POS Tagging,adding po part speech column pyspark dataframe working pyspark dataframe want add column see word noun verb adj adv adp propn using pyspark nltk po tag pyspark table column continues result looking using pyspark sure word column word compared get result adj noun pronoun etc thanks help advance
Need advice on Negation Handling while doing Aspect Based Sentiment Analysis in Python,"<p>I'm trying to write a Python code that does <strong>Aspect Based Sentiment Analysis</strong> of product reviews using Dependency Parser. I created an example review:</p>
<p>&quot;The Sound Quality is great but the battery life is bad.&quot;</p>
<p>The output is : <strong>[['soundquality', ['great']], ['batterylife', ['bad']]]</strong></p>
<p>I can properly get the aspect and it's adjective with this sentence but when I change the text to:</p>
<p>&quot;The Sound Quality is not great but the battery life is not bad.&quot;</p>
<p>The output still stays the same. How can I add a negation handling to my code? And are there ways to improve what I currently have?</p>
<pre><code>import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem.wordnet import WordNetLemmatizer
import stanfordnlp

stanfordnlp.download('en')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

txt = &quot;The Sound Quality is not great but the battery life is not bad.&quot;

txt = txt.lower()
sentList = nltk.sent_tokenize(txt)

taggedList = []
for line in sentList:
    txt_list = nltk.word_tokenize(line) # tokenize sentence
    taggedList = taggedList + nltk.pos_tag(txt_list) # perform POS-Tagging
print(taggedList)

newwordList = []
flag = 0
for i in range(0,len(taggedList)-1):
    if(taggedList[i][1]=='NN' and taggedList[i+1][1]=='NN'):
        newwordList.append(taggedList[i][0]+taggedList[i+1][0])
        flag=1
    else:
        if(flag == 1):
            flag=0
            continue
        newwordList.append(taggedList[i][0])
        if(i==len(taggedList)-2):
            newwordList.append(taggedList[i+1][0])
finaltxt = ' '.join(word for word in newwordList)
print(finaltxt)

stop_words = set(stopwords.words('english'))
new_txt_list = nltk.word_tokenize(finaltxt)
wordsList = [w for w in new_txt_list if not w in stop_words]
taggedList = nltk.pos_tag(wordsList)

nlp = stanfordnlp.Pipeline()
doc = nlp(finaltxt)
dep_node = []
for dep_edge in doc.sentences[0].dependencies:
    dep_node.append([dep_edge[2].text, dep_edge[0].index, dep_edge[1]])
for i in range(0, len(dep_node)):
    if(int(dep_node[i][1]) != 0):
        dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]
print(dep_node)

featureList = []
categories = []
totalfeatureList = []
for i in taggedList:
    if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):
        featureList.append(list(i))
        totalfeatureList.append(list(i)) # stores all the features for every sentence
        categories.append(i[0])
print(featureList)
print(categories)

fcluster = []
for i in featureList:
    filist = []
    for j in dep_node:
        if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [&quot;nsubj&quot;, &quot;acl:relcl&quot;, &quot;obj&quot;, &quot;dobj&quot;, &quot;agent&quot;, &quot;advmod&quot;, &quot;amod&quot;, &quot;neg&quot;, &quot;prep_of&quot;, &quot;acomp&quot;, &quot;xcomp&quot;, &quot;compound&quot;])):
            if(j[0]==i[0]):
                filist.append(j[1])
            else:
                filist.append(j[0])
    fcluster.append([i[0], filist])
print(fcluster)

finalcluster = []
dic = {}
for i in featureList:
    dic[i[0]] = i[1]
for i in fcluster:
    if(dic[i[0]]=='NN'):
        finalcluster.append(i)
print(finalcluster)
</code></pre>
",Parsing & POS Tagging,need advice negation handling aspect based sentiment analysis python trying write python code doe aspect based sentiment analysis product review using dependency parser created example review sound quality great battery life bad output soundquality great batterylife bad properly get aspect adjective sentence change text sound quality great battery life bad output still stay add negation handling code way improve currently
"Extracting (subject,predicate,object) from dependency tree","<p>I'm interested in extracting triples (subject,predicate,object) from questions.</p>

<p>For example, I would like to transform the following question : </p>

<blockquote>
  <p>Who is the wife of the president of the USA?</p>
</blockquote>

<p>to : </p>

<blockquote>
  <p>(x,isWifeOf,y) &wedge; (y,isPresidentof,USA)</p>
</blockquote>

<p>x and y are unknows that we have to find in order to answer the question (/\ denotes the conjunction).</p>

<p>I have read a lot of papers about this topic and I would like to perform this task using existing parsers such as Stanford parser. I know that parsers output 2 types of data : </p>

<ul>
<li>parse structure tree (constituency relations)</li>
<li>dependency tree (dependency relations)</li>
</ul>

<p>Some papers try to build triples from the parse structure tree (e.g., <a href=""http://ailab.ijs.si/delia_rusu/Papers/is_2007.pdf"" rel=""nofollow noreferrer"">Triple Extraction from Sentences</a>), however this approach seems to be too weak to deal with complicated questions.</p>

<p>On the other hand, dependency trees contain a lot of relevant information to perform the triple extraction. A lot of papers claim to do that, however I didn't find any of them that gives explicitely a detailed procedure or an algorithm. Most of the time, authors say they analyze the dependencies to produce triples according to some rules they didn't give.</p>

<p>Does anyone know any paper with more information on extracting (subject,predicate,object) from dependency tree of a question?</p>
",Parsing & POS Tagging,extracting subject predicate object dependency tree interested extracting triple subject predicate object question example would like transform following question wife president usa x iswifeof ispresidentof usa x unknows find order answer question denotes conjunction read lot paper topic would like perform task using existing parser stanford parser know parser output type data parse structure tree constituency relation dependency tree dependency relation paper try build triple parse structure tree e g triple extraction sentence however approach seems weak deal complicated question hand dependency tree contain lot relevant information perform triple extraction lot paper claim however find give explicitely detailed procedure algorithm time author say analyze dependency produce triple according rule give doe anyone know paper information extracting subject predicate object dependency tree question
How to extract all possible noun phrases from text,"<p>I want to extract some desirable concepts (noun phrases) in the text automatically. My plan is to extract all noun phrases and then label them as two classifications (i.e., desirable phrases and non-desirable phrases). After that, train a classifier to classify them. What I am trying now is to extract all possible phrases as the training set first. For example, one sentence is <code>Where a shoulder of richer mix is required at these junctions, or at junctions of columns and beams, the items are so described.</code> I want to get all phrases like <code>shoulder</code>, <code>richer mix</code>, <code>shoulder of richer mix</code>,<code>junctions</code>,<code>junctions of columns and beams</code>, <code>columns and beams</code>, <code>columns</code>, <code>beams</code> or whatever possible. The desirable phrases are <code>shoulder</code>, <code>junctions</code>,  <code>junctions of columns and beams</code>. But I don't care the correctness at this step, I just want to get the training set first. Are there available tools for such task?</p>
<p>I tried Rake in rake_nltk, but the results failed to include my desirable phrases (i.e., it did not extract all possible phrases)</p>
<pre><code>from rake_nltk import Rake
data = 'Where a shoulder of richer mix is required at these junctions, or at junctions of columns and beams, the items are so described.'
r = Rake()
r.extract_keywords_from_text(data)
phrase = r.get_ranked_phrases()
print(phrase)enter code herenter code here
</code></pre>
<p>Result: <code>['richer mix', 'shoulder', 'required', 'junctions', 'items', 'described', 'columns', 'beams']</code>
(Missed <code>junctions of columns and beams</code> here)</p>
<p>I also tried phrasemachine, the results also missed some desirable ones.</p>
<pre><code>import spacy
import phrasemachine
matchedList=[]
doc = nlp(data)
tokens = [token.text for token in doc]
pos = [token.pos_ for token in doc]
out = phrasemachine.get_phrases(tokens=tokens, postags=pos, output=&quot;token_spans&quot;)
print(out['token_spans'])
while len(out['token_spans']):
    start,end = out['token_spans'].pop()
    print(tokens[start:end])
</code></pre>
<p>Result:</p>
<pre><code>[(2, 6), (4, 6), (14, 17)]
['junctions', 'of', 'columns']
['richer', 'mix']
['shoulder', 'of', 'richer', 'mix'] 
</code></pre>
<p>(Missed many noun phrases here)</p>
",Parsing & POS Tagging,extract possible noun phrase text want extract desirable concept noun phrase text automatically plan extract noun phrase label two classification e desirable phrase non desirable phrase train classifier classify trying extract possible phrase training set first example one sentence want get phrase like whatever possible desirable phrase care correctness step want get training set first available tool task tried rake rake nltk result failed include desirable phrase e extract possible phrase result missed also tried phrasemachine result also missed desirable one result missed many noun phrase
Explaining my deep learning model with LIME text explainer in python on Twitter sentiment analysis,"<p>I have a dataset of Tweets labelled with sentiments. I have pre-processed the data and done parts of speech tagging (all via NLTK in python). After preprocessing the data looks like this:</p>
<p><a href=""https://i.sstatic.net/vdXQx.png"" rel=""nofollow noreferrer"">Pre-processed tweets</a></p>
<p>After preprocessing training data is prepared with the following code:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>full_text = list(train['content'].values) + list(test['content'].values)
tokenizer = Tokenizer(num_words=20000,lower = True, filters = '')
tokenizer.fit_on_texts(full_text)
train_tokenized = tokenizer.texts_to_sequences(train['content'])
test_tokenized = tokenizer.texts_to_sequences(test['content'])

max_len = 50
X_train = pad_sequences(train_tokenized, maxlen = max_len)
X_test = pad_sequences(test_tokenized, maxlen = max_len)
embed_size = 300
max_features = 20000

def get_coefs(word,*arr):
    return word, np.asarray(arr, dtype='float32')
def get_embed_mat(embedding_path):
    
    embedding_index = dict(get_coefs(*o.strip().split("" "")) for o in open(embedding_path,encoding=""utf8""))

    word_index = tokenizer.word_index
    nb_words = min(max_features, len(word_index))
    print(nb_words)
    embedding_matrix = np.zeros((nb_words + 1, embed_size))
    for word, i in word_index.items():
        if i &gt;= max_features:
            continue
        embedding_vector = embedding_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        
    return embedding_matrix</code></pre>
</div>
</div>
</p>
<p>A deep learning model is built with Word embeddings as a layer. The code for model building is given below:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>def build_model1(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):
    inp = Input(shape = (max_len,))
    x = Embedding(20001, embed_size, weights = [embedding_matrix], trainable = False)(inp)
    x1 = SpatialDropout1D(dr)(x)

    x_lstm = Bidirectional(LSTM(units, return_sequences = True))(x1)
    x1 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_lstm)
    avg_pool1_lstm1 = GlobalAveragePooling1D()(x1)
    max_pool1_lstm1 = GlobalMaxPooling1D()(x1)
    
    
    x_lstm = Bidirectional(LSTM(units, return_sequences = True))(x1)
    x1 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_lstm)
    avg_pool1_lstm = GlobalAveragePooling1D()(x1)
    max_pool1_lstm = GlobalMaxPooling1D()(x1)
    
    
    
    x = concatenate([avg_pool1_lstm1, max_pool1_lstm1,
                    avg_pool1_lstm, max_pool1_lstm])
    #x = BatchNormalization()(x)
    x = Dropout(0.1)(Dense(128,activation='relu') (x))
    x = BatchNormalization()(x)
    x = Dropout(0.1)(Dense(64,activation='relu') (x))
    x = Dense(8, activation = ""sigmoid"")(x)
    model = Model(inputs = inp, outputs = x)
    model.compile(loss = ""binary_crossentropy"", optimizer = Adam(lr = lr, decay = lr_d), metrics = [""accuracy""])
    history = model.fit(X_train, y_one_hot, batch_size = 128, epochs = 20, validation_split=0.1, 
                        verbose = 1, callbacks = [check_point, early_stop])
    model = load_model(file_path)
    return model</code></pre>
</div>
</div>
</p>
<p><em><strong>I want to use LIME to explain the predictions of this model (as given in the below image). But it is not working.</strong></em></p>
<p><a href=""https://i.sstatic.net/xKEk5.png"" rel=""nofollow noreferrer"">Lime Text explanation of Model</a></p>
",Parsing & POS Tagging,explaining deep learning model lime text explainer python twitter sentiment analysis dataset tweet labelled sentiment pre processed data done part speech tagging via nltk python preprocessing data look like pre processed tweet preprocessing training data prepared following code deep learning model built word embeddings layer code model building given want use lime explain prediction model given image working lime text explanation model
Get elements between two or more indexes dynamically in Python without hardcoding number of index variables,"<p>I am trying to extract POS tags from an input text and extract all the words between 2 or more 'IN' tags. So, the idea is if there is 1 'IN' tag, the extraction happens from the index of the tag to the end of the sentence. If there are more than 2 'IN' tags, extraction should happen from the index of 1 tag to the other 'IN' tag, segregating the phrases into groups. I have written the code that does the same.
The code is :</p>
<pre><code>def extractor(text):
    text = nltk.word_tokenize(text)
    pos_tagged = nltk.pos_tag(text)
#    print(pos_tagged)
#    Get tuple index of preposition
    indices = [i for i, tupl in enumerate(pos_tagged) if tupl[1] == 'IN']
#    print(indices)
    if len(indices) == 1:
        idx = indices[0]
        phrase = pos_tagged[idx:]
        words = [i[0] for i in phrase]
        comb_words = ' '.join(i for i in words)
        return comb_words 
        
    else:
        idx1 = indices[0]
        idx2 = indices[1]
        phrase1 = pos_tagged[idx1:idx2]
        words1 = [i[0] for i in phrase1]
        comb_words1 = ' '.join(i for i in words1)

        phrase2 = pos_tagged[idx2:]
        words2 = [i[0] for i in phrase2]
        comb_words2 = ' '.join(i for i in words2)
                        
        return comb_words1, comb_words2
        

extractor(&quot;hunger increases in the morning during workout&quot;)
</code></pre>
<p>and the output is as expected.
The only concern is that I have had to specifically hardcode the scenario if there are 2 'IN' tags in my text.
<code>idx1 = indices[0] idx2 = indices[1]</code></p>
<p>So, this way, if there are 10 'IN' tags, I need to create 10 index variable in this way. Is there a better approach to solve this so that the index variables can be created dynamically based on the number of tags present in the input</p>
",Parsing & POS Tagging,get element two index dynamically python without hardcoding number index variable trying extract po tag input text extract word tag idea tag extraction happens index tag end sentence tag extraction happen index tag tag segregating phrase group written code doe code output expected concern specifically hardcode scenario tag text way tag need create index variable way better approach solve index variable created dynamically based number tag present input
how to apply chunking for the below tagged pos ? (nltk),"<pre><code>from nltk import word_tokenize, pos_tag, pos_tag_sents
tagged_pos = pos_tag_sents(map(word_tokenize, data))
tagged_pos
</code></pre>
<blockquote>
<p>the following are pos tags:
[('dpdb', 'NN'),('engine', 'NN'),('bottom', 'NN'),('power', 'NN'),('gain', 'NN'),('curve', 'NN'),
('every', 'DT')],[('dpdb', 'NN'),('engine', 'NN'),('center', 'NN'),('power', 'NN'),('gain', 'NN'),
('curve', 'NN'),('every', 'DT')]]</p>
</blockquote>
<p>I tried with:</p>
<pre><code>grammar = &quot;NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}&quot;
chunk_parse = nltk.RegexpParser(grammar)

chunked=[]
for s in tagged_pos:
    chunked.append(chunk_parse.parse(s))
</code></pre>
<p>got the following output:
Warning: parsing empty text
Warning: parsing empty text
Warning: parsing empty text
Warning: parsing empty text
Warning: parsing empty text
Warning: parsing empty text
Warning: parsing empty text</p>
",Parsing & POS Tagging,apply chunking tagged po nltk following po tag dpdb nn engine nn bottom nn power nn gain nn curve nn every dt dpdb nn engine nn center nn power nn gain nn curve nn every dt tried got following output warning parsing empty text warning parsing empty text warning parsing empty text warning parsing empty text warning parsing empty text warning parsing empty text warning parsing empty text
Absolute position of leaves in NLTK tree,"<p>I am trying to find the span (start index, end index) of a noun phrase in a given sentence. The following is the code for extracting noun phrases</p>

<pre><code>sent=nltk.word_tokenize(a)
sent_pos=nltk.pos_tag(sent)
grammar = r""""""
    NBAR:
        {&lt;NN.*|JJ&gt;*&lt;NN.*&gt;}  # Nouns and Adjectives, terminated with Nouns

    NP:
        {&lt;NBAR&gt;}
        {&lt;NBAR&gt;&lt;IN&gt;&lt;NBAR&gt;}  # Above, connected with in/of/etc...
    VP:
        {&lt;VBD&gt;&lt;PP&gt;?}
        {&lt;VBZ&gt;&lt;PP&gt;?}
        {&lt;VB&gt;&lt;PP&gt;?}
        {&lt;VBN&gt;&lt;PP&gt;?}
        {&lt;VBG&gt;&lt;PP&gt;?}
        {&lt;VBP&gt;&lt;PP&gt;?}
""""""

cp = nltk.RegexpParser(grammar)
result = cp.parse(sent_pos)
nounPhrases = []
for subtree in result.subtrees(filter=lambda t: t.label() == 'NP'):
  np = ''
  for x in subtree.leaves():
    np = np + ' ' + x[0]
  nounPhrases.append(np.strip())
</code></pre>

<p>For <em>a = ""The American Civil War, also known as the War between the States or simply the Civil War, was a civil war fought from 1861 to 1865 in the United States after several Southern slave states declared their secession and formed the Confederate States of America.</em>"", the noun phrases extracted are</p>

<p><em>['American Civil War', 'War', 'States', 'Civil War', 'civil war fought', 'United States', 'several Southern', 'states', 'secession', 'Confederate States', 'America'].</em></p>

<p>Now I need to find the span (start position and end position of the phrase) of noun phrases. For example, the span of above noun phrases will be </p>

<p><em>[(1,3), (9,9), (12, 12), (16, 17), (21, 23), ....]</em>.</p>

<p>I'm fairly new to NLTK and I've looked into <a href=""http://www.nltk.org/_modules/nltk/tree.html"" rel=""noreferrer"">http://www.nltk.org/_modules/nltk/tree.html</a>. I tried to use <em>Tree.treepositions()</em> but I couldn't manage to extract absolute positions using these indices. Any help would be greatly appreciated. Thank You!</p>
",Parsing & POS Tagging,absolute position leaf nltk tree trying find span start index end index noun phrase given sentence following code extracting noun phrase american civil war also known war state simply civil war wa civil war fought united state several southern slave state declared secession formed confederate state america noun phrase extracted american civil war war state civil war civil war fought united state several southern state secession confederate state america need find span start position end position phrase noun phrase example span noun phrase fairly new nltk looked tried use tree treepositions manage extract absolute position using index help would greatly appreciated thank
Training custom Swedish spacy model,"<p>my intention was to train a custom POS-Tagger and Dependency Parser in spaCy for the swedish language. 
I followed the instructions on <a href=""https://spacy.io/usage/training"" rel=""nofollow noreferrer"">https://spacy.io/usage/training</a> and trained the models on the Swedish-Talbanken treebank conllu files. 
These steps went well and I ended up with a custom model. Then I loaded the model and tried a little example:</p>

<pre><code>nlp = spacy.load(name=os.path.join(spacy_path, 'models/model-best'))
doc = nlp(u'Jag heter Alex Nilsson. Hon heter Lina')
# My name is Alex Nilsson. Her name is Lina

for token in doc:
    print(token.text, token.pos_, token.dep_)

# OUTPUT:

#Jag PRON nsubj
#heter VERB ROOT
#Alex PROPN obj
#Nilsson PROPN flat:name
#. PUNCT punct
#Hon PRON nsubj
#heter VERB parataxis
#Lina PROPN obj
</code></pre>

<p>Both POS-Tagger and Dependency Parser seem to work. What didn’t work was the sentence segmentation and the noun chunks. </p>

<pre><code>for sent in doc.sents:
    print(sent.text)

# OUTPUT:

#Jag heter Alex. Hon heter Lina

for chunk in doc.noun_chunks:
    print(chunk.text, chunk.root.text, chunk.root.dep_,
          chunk.root.head.text)

# OUTPUT:

#
</code></pre>

<p>So, no splitting for the sentences and no output for noun chunks. As far as I understand spaCy uses the Dependency Parser for both functionalities. But as shown above the Dependency Parser should work just fine. Is there something more that it required for these two to work? Maybe I am missing something obvious? </p>

<p>I am thankful for any help! </p>
",Parsing & POS Tagging,training custom swedish spacy model intention wa train custom po tagger dependency parser spacy swedish language followed instruction trained model swedish talbanken treebank conllu file step went well ended custom model loaded model tried little example po tagger dependency parser seem work work wa sentence segmentation noun chunk splitting sentence output noun chunk far understand spacy us dependency parser functionality shown dependency parser work fine something required two work maybe missing something obvious thankful help
Using Python and NLP to get the most frequent POS tag from a list,"<p>I'm trying to get the most frequent POS tags (top five) from a list.</p>

<pre><code>pos_list = nltk.pos_tag(list)
#pos_list = [('caught', 'NN'), ('black', 'NN'), ('a', 'DT'), ('striped', 'JJ'), ('eel', 'NN')]
tag_fd = nltk.FreqDist(tag for (word, tag) in pos_list)
</code></pre>

<p>I've also tried looping through <code>pos_list</code> to count the tags that way but there seems like there has to be a way to do this using <code>NLTK</code>.  I've also tried to create a string out of the list and trying the same approach but that isn't working either.</p>

<pre><code>str_of_list = "" "".join(list)
tag_fd = nltk.FreqDist(tag for (word, tag) in str_of_list)
</code></pre>

<p>Thanks any help is appreciated!</p>
",Parsing & POS Tagging,using python nlp get frequent po tag list trying get frequent po tag top five list also tried looping count tag way seems like ha way using also tried create string list trying approach working either thanks help appreciated
How to extract chunks with multiple patterns from pos tagged sentences?,"<p>Given an input sentence that is pos tagged using pos_tag function in nltk :</p>
<p>[('Veer', 'NNP'),
('Singh', 'NNP'),
('Rathore', 'NNP'),
('auctioned', 'VBD'),
('his', 'PRP$'),
('gigantic', 'JJ'),
('house', 'NN'),
('in', 'IN'),
('New', 'NNP'),
('York', 'NNP'),
('.', '.')]</p>
<p>I need to extract the phrases which follow a certain pattern. For example, 'NNP NNP' or 'JJ NN'.
There can be 'n' no. of patterns that we might want to extract. For example, here we need 2 patterns namely 'NNP NNP' and 'JJ NN'.</p>
<p>The output that I want for the above inputted sentence is a list of the phrases like :</p>
<p>output :</p>
<blockquote>
<p>['Veer Singh Rathore', 'gigantic house', 'New York']</p>
</blockquote>
<p>I have tried something like this :</p>
<pre><code>&gt; grammar = (''' Chunk:{&lt;JJ&gt;&lt;NN&gt;|&lt;NNP&gt;+&lt;NNP&gt;} ''')
&gt; 
&gt; def pos_and_chunking(question):
&gt;     words = word_tokenize(question)
&gt;     pos_words = pos_tag(words)
&gt;     chunkParser = RegexpParser(grammar)
&gt;     chunked_phrases = chunkParser.parse(pos_words)
&gt;     chunked_phrases.draw()
&gt;     for subtree in chunked_phrases.subtrees():
&gt;         print(subtree)
</code></pre>
<p>But the output I am getting is in the form of a tree.</p>
<p>Output :</p>
<blockquote>
<p>(S   (Chunk Veer/NNP Singh/NNP Rathore/NNP)   auctioned/VBD   his/PRP$
(Chunk gigantic/JJ house/NN)   in/IN   (Chunk New/NNP York/NNP)   ./.)
(Chunk Veer/NNP Singh/NNP Rathore/NNP) (Chunk gigantic/JJ house/NN)
(Chunk New/NNP York/NNP)
<a href=""https://i.sstatic.net/TNMaI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TNMaI.png"" alt=""enter image description here"" /></a></p>
</blockquote>
<p>How can this be resolved?</p>
<p>I referred this link for Chunking :
<a href=""https://www.codespeedy.com/chunking-rules-in-nlp/"" rel=""nofollow noreferrer"">https://www.codespeedy.com/chunking-rules-in-nlp/</a></p>
",Parsing & POS Tagging,extract chunk multiple pattern po tagged sentence given input sentence po tagged using po tag function nltk veer nnp singh nnp rathore nnp auctioned vbd prp gigantic jj house nn new nnp york nnp need extract phrase follow certain pattern example nnp nnp jj nn n pattern might want extract example need pattern namely nnp nnp jj nn output want inputted sentence list phrase like output veer singh rathore gigantic house new york tried something like output getting form tree output chunk veer nnp singh nnp rathore nnp auctioned vbd prp chunk gigantic jj house nn chunk new nnp york nnp chunk veer nnp singh nnp rathore nnp chunk gigantic jj house nn chunk new nnp york nnp resolved referred link chunking
How to determine plurality of a word in Python without using Inflect or NLTK?,"<p>I am writing a program using Python which reads a list of words and needs to determine the plurality of each one. I tried using Inflect but, as far as I can tell, it does not provide any great methods for determining the plurality of a generic word. I used the singular_noun and plural_noun methods but they both require that the word to be inflected be both a noun and plural/singular, respectively. According to documentation:</p>
<blockquote>
<p>&quot;All of the plural... plural inflection methods take the word to be inflected as their first argument and return the corresponding inflection. <strong>Note that all such methods expect the singular form of the word. The results of passing a plural form are undefined (and unlikely to be correct).</strong> Similarly, the si... singular inflection method expects the plural form of the word.&quot;</p>
</blockquote>
<p>I have also tried using NLTK pos_tag but it is too inaccurate.</p>
<p>Is there a better Inflect method or a different Python package that can take any word of any part of speech and accurately determine its plurality?</p>
",Parsing & POS Tagging,determine plurality word python without using inflect nltk writing program using python read list word need determine plurality one tried using inflect far tell doe provide great method determining plurality generic word used singular noun plural noun method require word inflected noun plural singular respectively according documentation plural plural inflection method take word inflected first argument return corresponding inflection note method expect singular form word result passing plural form undefined unlikely correct similarly si singular inflection method expects plural form word also tried using nltk po tag inaccurate better inflect method different python package take word part speech accurately determine plurality
Lemmatizing POS tagged words with NLTK?,"<p>I have POS tagged some words with nltk.pos_tag(), so they are given treebank tags. I would like to lemmatize these words using the known POS tags, but I am not sure how. I was looking at Wordnet lemmatizer, but I am not sure how to convert the treebank POS tags to tags accepted by the lemmatizer. How can I perform this conversion simply, or is there a lemmatizer that uses treebank tags?</p>
",Parsing & POS Tagging,lemmatizing po tagged word nltk po tagged word nltk po tag given treebank tag would like lemmatize word using known po tag sure wa looking wordnet lemmatizer sure convert treebank po tag tag accepted lemmatizer perform conversion simply lemmatizer us treebank tag
How can I train NLTK on the entire Penn Treebank corpus?,"<p>I was originally using the following Penn Treebank tagger from NLTK:</p>

<pre><code>POS_Tagger = UnigramTagger(treebank.tagged_sents(), backoff=DefaultTagger('NN'))
</code></pre>

<p>However, this falls short on spoken text. For instance, ""hello"" is not recognized as an interjection when it should be. I read from here (<a href=""https://stackoverflow.com/questions/5932227/in-nltk-pos-tag-why-hello-is-classified-as-noun"">In NLTK pos_tag, why “hello” is classified as Noun?</a>) that if I want to tag spoken text, I'll ""need to train the tagger on the whole Penn Treebank, which includes something like 3 million words of spoken English."" The question I now have is <strong>HOW</strong> can I do this? I've been here (<a href=""http://www.cis.upenn.edu/~treebank/"" rel=""nofollow noreferrer"">Penn Treebank Project</a>) but can't find anything on it.</p>

<p>If training on the whole Penn Treebank is too difficult, what would be an alternative? I'm considering the Brown corpus instead however the POS tags are different, making me have to rewrite other sections of the program.</p>
",Parsing & POS Tagging,train nltk entire penn treebank corpus wa originally using following penn treebank tagger nltk however fall short spoken text instance hello recognized interjection read penn treebank project find anything training whole penn treebank difficult would alternative considering brown corpus instead however po tag different making rewrite section program
Best way to extract keywords from input NLP sentence,"<p>I'm working on a project where I need to extract important keywords from a sentence.  I've been using a rules based system based on the POS tags.  However, I run into some ambiguous terms that I've been unable to parse.  Is there some machine learning classifier that I can use to extract relevant keywords based on a training set of different sentences?</p>
",Parsing & POS Tagging,best way extract keywords input nlp sentence working project need extract important keywords sentence using rule based system based po tag however run ambiguous term unable parse machine learning classifier use extract relevant keywords based training set different sentence
How to get indices of words in a Spacy dependency parse?,"<p>I am trying to use Spacy to extract word relations/dependencies, but am a little unsure about how to use the information it gives me. I understand how to generate the visual dependency tree for debugging.</p>
<p>Specifically, I don’t see a way to map the list of children of a token to a specific token. There is no index—just a list of words.</p>
<p>Looking at the example here: <a href=""https://spacy.io/usage/linguistic-features#dependency-parse"" rel=""nofollow noreferrer"">https://spacy.io/usage/linguistic-features#dependency-parse</a></p>
<p><code>nlp(&quot;Autonomous cars shift insurance liability toward manufacturers&quot;)</code></p>
<p>Also, if the sentence were <code>nlp(&quot;Autonomous cars shift insurance liability toward manufacturers of cars”)</code>, how would I disambiguate between the two instances of cars?</p>
<p>The only thing I can think of is that maybe these tokens are actually reference types that I can map to indices myself. Is that the case?</p>
<p>Basically, I am looking to start with getting the predicates and args to understand “who did what to whom and how/using what”.</p>
",Parsing & POS Tagging,get index word spacy dependency parse trying use spacy extract word relation dependency little unsure use information give understand generate visual dependency tree debugging specifically see way map list child token specific token index list word looking example also sentence would disambiguate two instance car thing think maybe token actually reference type map index case basically looking start getting predicate args understand using
Where I can find the complete list of SpaCy Dependency Parsing labels or annotations?,"<p>I have try to refer to spaCy official website <a href=""https://spacy.io/api/annotation#dependency-parsing"" rel=""noreferrer"">https://spacy.io/api/annotation#dependency-parsing</a>
but I only got list of universal dependency relation which also on <a href=""https://universaldependencies.org/u/dep/"" rel=""noreferrer"">https://universaldependencies.org/u/dep/</a></p>

<p>while, when I try to parse some sentences, I also got labels or annotations that not have been listed. Such as: <em>prep, dative,</em> and <em>dobj</em>, even though that those labels can be associated with preposition for <em>prep</em>, direct object for <em>dobj</em>, and ??? for <em>dative</em>.</p>

<p><a href=""https://i.sstatic.net/yC8TB.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/yC8TB.png"" alt=""enter image description here""></a>  </p>

<p>Is there any reference for me to find the complete list of SpaCy Dependency Parsing labels or annotations?</p>
",Parsing & POS Tagging,find complete list spacy dependency parsing label annotation try refer spacy official website got list universal dependency relation also try parse sentence also got label annotation listed prep dative dobj even though label associated preposition prep direct object dobj dative reference find complete list spacy dependency parsing label annotation
singularize noun phrases with spacy,"<p>I am looking for a way to singularize noun chunks with spacy</p>

<pre><code>S='There are multiple sentences that should include several parts and also make clear that studying Natural language Processing is not difficult '
nlp = spacy.load('en_core_web_sm')
doc = nlp(S)

[chunk.text for chunk in doc.noun_chunks]
# = ['an example sentence', 'several parts', 'Natural language Processing']
</code></pre>

<p>You can also get the ""root"" of the noun chunk:</p>

<pre><code>[chunk.root.text for chunk in doc.noun_chunks]
# = ['sentences', 'parts', 'Processing']
</code></pre>

<p>I am looking for a way to singularize those roots of the chunks.</p>

<p>GOAL: Singulirized: ['sentence', 'part', 'Processing']</p>

<p>Is there any obvious way? Is that always depending on the POS of every root word?</p>

<p>Thanks</p>

<p>note:
I found this: <a href=""https://www.geeksforgeeks.org/nlp-singularizing-plural-nouns-and-swapping-infinite-phrases/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/nlp-singularizing-plural-nouns-and-swapping-infinite-phrases/</a>
but that approach looks to me that leads to many many different methods and of course different for every language. ( I am working in EN, FR, DE)</p>
",Parsing & POS Tagging,singularize noun phrase spacy looking way singularize noun chunk spacy also get root noun chunk looking way singularize root chunk goal singulirized sentence part processing obvious way always depending po every root word thanks note found approach look lead many many different method course different every language working en fr de
Are there recognized strategies to get the adjectives associated with a person or a place in NLP (or the general impression of a person or a place)?,"<p>I am acquainted with part of speach tags (POS-tag), but even with this metodology does not seem trivial to do this.</p>
",Parsing & POS Tagging,recognized strategy get adjective associated person place nlp general impression person place acquainted part speach tag po tag even metodology doe seem trivial
finding the POS of the root of a noun_chunk with spacy,"<p>When using spacy you can easily loop across the noun_phrases of a text as follows:</p>

<pre><code>S='This is an example sentence that should include several parts and also make clear that studying Natural language Processing is not difficult'
nlp = spacy.load('en_core_web_sm')
doc = nlp(S)

[chunk.text for chunk in doc.noun_chunks]
# = ['an example sentence', 'several parts', 'Natural language Processing']
</code></pre>

<p>You can also get the ""root"" of the noun chunk:</p>

<pre><code>[chunk.root.text for chunk in doc.noun_chunks]
# = ['sentence', 'parts', 'Processing']
</code></pre>

<p>How can I get the POS of every of those words (even if looks like the root of a noun_phrase is always a noun), and how can I get the lemma, the shape and the word in singular of that particular word.</p>

<p>Is that even possible?</p>

<p>thx.</p>
",Parsing & POS Tagging,finding po root noun chunk spacy using spacy easily loop across noun phrase text follows also get root noun chunk get po every word even look like root noun phrase always noun get lemma shape word singular particular word even possible thx
POS Tagging in NLP,"<p>I am doing a course in NLTK Python which has a hands-on problem(on Katacoda) on ""Text Corpora"" and it is not accepting my solution mentioned below. Have been stuck on this problem since long. Need to complete this hands-on to proceed foreword in course.</p>

<h1>Problem Defenition</h1>

<ol>
<li>Import the text corpus brown.</li>
<li><p>Extract the list of tagged words from the corpus brown. Store the result in brown_tagged_words</p></li>
<li><p>Generate trigrams of brown_tagged_words and store the result in brown_tagged_trigrams.</p></li>
</ol>

<p>4.For every trigram of brown_tagged_trigrams, determine the tags associated with each word. This results in a list of tuples, where each tuple contain pos tags of 3 consecutive words, occurring in text. Store the result in brown_trigram_pos_tags.</p>

<p>5.Determine the frequency distribution of brown_trigram_pos_tags and store the result in brown_trigram_pos_tags_freq.
6.Print the number of occurrences of trigram ('JJ','NN','IN')</p>

For this I have tried below solution:

<pre><code>import nltk
from nltk.corpus import brown
brown_tagged_words = [w for w in brown.tagged_words()]
brown_tagged_trigrams = nltk.trigrams(brown_tagged_words)
brown_trigram_pos_tags = [(w1[1],w2[1],w2[1]) for w1,w2,w3 in brown_tagged_trigrams]
brown_trigram_pos_tags_freq = nltk.FreqDist(brown_trigram_pos_tags)
print(brown_trigram_pos_tags_freq[('JJ', 'NN', 'IN')])
</code></pre>
",Parsing & POS Tagging,po tagging nlp course nltk python ha hand problem katacoda text corpus accepting solution mentioned stuck problem since long need complete hand proceed foreword course problem defenition import text corpus brown extract list tagged word corpus brown store result brown tagged word generate trigram brown tagged word store result brown tagged trigram every trigram brown tagged trigram determine tag associated word result list tuples tuple contain po tag consecutive word occurring text store result brown trigram po tag determine frequency distribution brown trigram po tag store result brown trigram po tag freq print number occurrence trigram jj nn tried solution
How to filter data in list of list with Tuples,"<p>POS tag filtering</p>

<pre><code># Dummy data

""Sukanya is getting married next year. "" \ 
""Marriage is a big step in one’s life."" \ 
""It is both exciting and frightening. "" \ 
""But friendship is a sacred bond between people."" \ 
""It is a special kind of love between us. "" \ 
""Many of you must have tried searching for a friend ""\ 
""but never found the right one.""
</code></pre>

<pre><code>import nltk 
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize, sent_tokenize 
stop_words = set(stopwords.words('english'))

def get_pos_tags(text):
    tokenized = sent_tokenize(txt) 
    for i in tokenized: 

        # Word tokenizers is used to find the words  
        # and punctuation in a string 
        wordsList = nltk.word_tokenize(i) 

        # removing stop words from wordList 
        wordsList = [w for w in wordsList if not w in stop_words]  

        #  Using a Tagger. Which is part-of-speech  
        # tagger or POS-tagger.  
        tagged = nltk.pos_tag(wordsList) 

    return tagged

df[""tagged""] = df[""text""].apply(lambda x: get_pos_tags(x))

</code></pre>

<p>I have dataframe(df). Each row is a list of lists, with tuples inside.</p>

<p>Example row:</p>

<pre><code>[[('Sukanya', 'NNP'), ('getting', 'VBG'), ('married', 'VBN'), ('next', 'JJ'), ('year', 'NN')],
[('Marriage', 'NN'), ('big', 'JJ'), ('step', 'NN'), ('one', 'CD'), ('’', 'NN'), ('life', 'NN')],
[('It', 'PRP'), ('exciting', 'VBG'), ('frightening', 'VBG')], 
[('But', 'CC'), ('friendship', 'NN'), ('sacred', 'VBD'), ('bond', 'NN'), ('people', 'NNS')], 
[('It', 'PRP'), ('special', 'JJ'), ('kind', 'NN'), ('love', 'VB'), ('us', 'PRP')], 
[('Many', 'JJ'), ('must', 'MD'), ('tried', 'VB'), ('searching', 'VBG'), ('friend', 'NN'), ('never','RB'),
 ('found', 'VBD'), ('right', 'RB'), ('one', 'CD')]]
</code></pre>

<p>Now I'm trying to filter the POS tags of <strong>adjective , noun, verb, adverb</strong> to a separate column <code>filtered_tags</code></p>

<pre><code>def filter_pos_tags(tagged_text):
    filtererd_tags = []
    for i in tagged_text:
        for j in i:
            if j[-1].startswith((""J"", ""V"", ""N"", ""R"")): filtered_tags.append(j[0])
    return filtered_tags

df[""filtered_tags""] = df[""tagged""].apply(lambda x: get_pos_tags(x))
</code></pre>

<p>The Output I got:</p>

<pre><code>['Sukanya', 'getting', 'married', 'next', 'year', 'Marriage', 'big', 'step', 'life', 'exciting', 'frightening', 'friendship', 'sacred', 'bond', 'people', 'special', 'kind', 'love', 'Many', 'tried', searching', 'friend', 'found', 'right']
</code></pre>

<p><strong>Required Output</strong></p>

<pre><code>[['Sukanya', 'getting', 'married', 'next', 'year'], ['Marriage', 'big', 'step', 'life' ], ['exciting', 'frightening'], ['friendship', 'sacred', 'bond', 'people'], ['special', 'kind', 'love'], ['Many', 'tried', searching', 'friend'], ['found', 'right']]
</code></pre>
",Parsing & POS Tagging,filter data list list tuples po tag filtering dataframe df row list list tuples inside example row trying filter po tag adjective noun verb adverb separate column output got required output
Filtering SpaCy noun_chunks by pos_tag,"<p>As the subj line says, I'm trying to extract elements of noun_chunks based on their individual POS tags. It seems that elements of a noun_chunk do not have access to the global sentence POS tags.</p>

<p>To demonstrate the issue:</p>

<pre class=""lang-py prettyprint-override""><code>
[i.pos_ for i in nlp(""Great coffee at a place with a great view!"").noun_chunks]
&gt;&gt;&gt; 
AttributeError: 'spacy.tokens.span.Span' object has no attribute 'pos_'
</code></pre>

<p>Here is my inefficient solution:</p>

<pre class=""lang-py prettyprint-override""><code>def parse(text):
    doc = nlp(text.lower())
    tags = [(idx,i.text,i.pos_) for idx,i in enumerate(doc)]

    chunks = [i for i in doc.noun_chunks]

    indices = []
    for c in chunks:
        indices.extend(j for j in range(c.start_char,c.end_char))
    non_chunks = [w for w in ''.join([i for idx,i in enumerate(text) if idx not in indices]).split(' ') 
                  if w != '']

    chunk_words = [tup[1] for tup in tags if tup[1] not in non_chunks and tup[2] not in ['DET','VERB','SYM','NUM']] #these are the POS tags which I wanted to filter out from the beginning!

    new_chunks = []
    for c in chunks:
        new_words = [w for w in str(c).split(' ') if w in chunk_words]
        if len(new_words) &gt; 1:
            new_chunk = ' '.join(new_words)
            new_chunks.append(new_chunk)
    return new_chunks

parse(
""""""
I may be biased about Counter Coffee since I live in town, but this is a great place that makes a great cup of coffee. I have been coming here for about 2 years and wish I would have found it sooner. It is located right in the heart of Forest Park and there is a ton of street parking. The coffee here is great....many other words could describe it, but that sums it up perfectly. You can by coffee by the pound, order a hot drink, and they also have food. On the weekend, there are donuts brought in from Do-Rite Donuts which have almost a cult like following. The food is a little on the high end price wise, but totally worth it. I am a self admitted latte snob and they make an amazing latte here. You can add skim, whole, almond or oat milk and they will make it happen. I always order easy foam and they always make it perfectly. My girlfriend loves the Chai Latte with Oat Milk and I will admit it is pretty good. Give them a try.
"""""")

&gt;&gt;&gt;
['counter coffee',
 'great place',
 'great cup',
 'forest park',
 'street parking',
 'many other words',
 'hot drink',
 'almost cult',
 'high end price',
 'latte snob',
 'amazing latte',
 'oat milk',
 'easy foam',
 'chai latte',
 'oat milk']

</code></pre>

<p>Any quicker approaches to the same solution would be welcomed!</p>
",Parsing & POS Tagging,filtering spacy noun chunk po tag subj line say trying extract element noun chunk based individual po tag seems element noun chunk access global sentence po tag demonstrate issue inefficient solution quicker approach solution would welcomed
where can I find the catalog for stanford coreNlp depency types,"<p>I am using Stanford CoreNlp lib. I am able to see the parse tree with all the nodes. I was able to look up most of the dependency types, but what is an 'S' node? I can't find the definition any where.</p>
",Parsing & POS Tagging,find catalog stanford corenlp depency type using stanford corenlp lib able see parse tree node wa able look dependency type node find definition
Parse Tree in StanfordCoreNLP and Stanza giving different result (representation structure),"<p>I did dependency parsing using StanfordCoreNLP  using the code below</p>

<pre><code>from stanfordcorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('stanford-corenlp-full-2018-10-05', lang='en')

sentence = 'The clothes in the dressing room are gorgeous. Can I have one?'
tree_str = nlp.parse(sentence)
print(tree_str)
</code></pre>

<p>And I got the output:</p>

<pre><code>  (S
    (NP
      (NP (DT The) (NNS clothes))
      (PP (IN in)
        (NP (DT the) (VBG dressing) (NN room))))
    (VP (VBP are)
      (ADJP (JJ gorgeous)))
    (. .)))
</code></pre>

<p>How can I get this same output in Stanza??</p>

<pre><code>import stanza
from stanza.server import CoreNLPClient
classpath='/stanford-corenlp-full-2020-04-20/*'
client = CoreNLPClient(be_quite=False, classpath=classpath, annotators=['parse'], memory='4G', endpoint='http://localhost:8900')
client.start()
text = 'The clothes in the dressing room are gorgeous. Can I have one?'
ann = client.annotate(text)
sentence = ann.sentence[0]
dependency_parse = sentence.basicDependencies
print(dependency_parse)

</code></pre>

<p>In stanza It appears I have to split the sentences that makes up the sentence. Is there something I am doing wrong?</p>

<p>Please note that my objective is to extract noun phrases. </p>
",Parsing & POS Tagging,parse tree stanfordcorenlp stanza giving different result representation structure dependency parsing using stanfordcorenlp using code got output get output stanza stanza appears split sentence make sentence something wrong please note objective extract noun phrase
Syntactic dependency extraction using SpaCy?,"<p>I have a relatively large corpus of sentences, such as:</p>

<p><em>This phone rocks. It is everything I have ever dreamed of. You can go running with it, take it swimming, do what ever. It even holds battery very well. However, I am very displeased with the customer service by the company, so I won't be coming back there</em> </p>

<p>There is a lot of information in every sentence, why I am trying to use dependency parsing to break the sentences up, potentially being able to weigh each sub-sentence based on the relationship. </p>

<p>By doing</p>

<pre><code>sentence = nlp(text) #refers to the quote above
displacy.serve(sentence, style = 'dep')
</code></pre>

<p>I get a nice visualization, such as parse tree : </p>

<p><a href=""https://i.sstatic.net/9qpU4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9qpU4.png"" alt=""enter image description here""></a></p>

<p>However, I am having great difficulty traversing this parse tree and meaningfully splitting up the reviews. </p>

<p>What I have is as follows: </p>

<pre><code>for chunk in sentence.noun_chunks:
    print(chunk.text)
</code></pre>

<p>Which outputs:</p>

<pre><code>This phone rocks 

It

everything
I
You
it
it
what
It
battery
I
the customer service
the company
I

TL;DR:
</code></pre>

<p>How can I efficiently traverse the tree to subtract meaningful syntactic dependencies, such as:</p>

<pre><code>'This phone rocks', 'it is everything I have ever dreamed of', However, I am very displeased with the customer service by the company, so I won't be coming back there
</code></pre>
",Parsing & POS Tagging,syntactic dependency extraction using spacy relatively large corpus sentence phone rock everything ever dreamed go running take swimming ever even hold battery well however displeased customer service company coming back lot information every sentence trying use dependency parsing break sentence potentially able weigh sub sentence based relationship get nice visualization parse tree however great difficulty traversing parse tree meaningfully splitting review follows output efficiently traverse tree subtract meaningful syntactic dependency
What is the difference between parsing and Part Of Speech Tagging?,"<p>I know that POS tagging labels each and every word in a sentence with its appropriate Part Of Speech , But isn't that what a Parser does too ? i.e, break a sentence into its component parts? 
I've looked this up on the internet but couldn't find any satisfactory explanation . 
Please clear my doubt.
Thanks in advance </p>
",Parsing & POS Tagging,difference parsing part speech tagging know po tagging label every word sentence appropriate part speech parser doe e break sentence component part looked internet find satisfactory explanation please clear doubt thanks advance
Search Corpus by Part-of-Speach,"<p>I am new to NLP. I am trying to search a corpus for Part-of-speech sequence. The goal would be to search for a sequence of POS tags and find all sentences that match sequence from a given corpus.</p>

<p>Input: The quick brown fox jumped over the lazy dogs. 
Tagger will process tag the sentence:
POS tagging results in: [DT][JJ][JJ][NN][VBD][IN][DT][JJ][NNS][.] 
Apply search will result in any sentence that matches this sequence or longer.</p>

<p>How do I search by Part-of-Speech? Is there a direct function in NLTK or spacy?</p>

<p>I would appreciate some guidance on the steps needed to solve the problem and the challenges that I might face.</p>

<p>Note that I found someone who posted a similar question on stackoverflow, but I think the problem he was facing was more specific. <a href=""https://stackoverflow.com/questions/52353452/querying-part-of-speech-tags-with-lucene-7-opennlp"">Search POS</a></p>
",Parsing & POS Tagging,search corpus part speach new nlp trying search corpus part speech sequence goal would search sequence po tag find sentence match sequence given corpus input quick brown fox jumped lazy dog tagger process tag sentence po tagging result dt jj jj nn vbd dt jj nns apply search result sentence match sequence longer search part speech direct function nltk spacy would appreciate guidance step needed solve problem challenge might face note found someone posted similar question stackoverflow think problem wa facing wa specific href po
How to get POS tags for merged phrases from Spacy?,"<p>I am currently working on an NLP problem where POS tagging is one of the essential steps. I was using Stanford CoreNLP for this task until I came across spaCy. I am new to spaCy and have been playing around with its online demo available at <a href=""https://explosion.ai/demos/displacy"" rel=""nofollow noreferrer"">explosion-Demo</a></p>
<p>For the given sentence:</p>
<blockquote>
<p>&quot;My only disappointment with the 13&quot; model is that they're the only
ones in the MacBook Pro line up without an i5 or i7 processor and dual
graphics cards, however, the trade-off is that you achieve a longer
battery life (of about two more hours).&quot;</p>
</blockquote>
<p>The demo provided the following output:</p>
<blockquote>
<p>My only disappointment - NOUN with - ADP <strong>the 13&quot; model - NOUN</strong> is - AUX
that - SCONJ they - PRON 're - AUX the only ones - NOUN in - ADP the
<strong>MacBook Pro line - NOUN</strong> up - ADP without - ADP <strong>an i5 or i7 processor -
NOUN</strong> and - CCONJ <strong>dual graphics cards, - NOUN</strong>, however - ADV the
trade-off - NOUN is - AUX that - SCONJ you - PRON achieve - VERB a
<strong>longer battery life ( - NOUN</strong> of - ADP about two more hours). - NOUN</p>
</blockquote>
<p>This output is perfect for my use case since it is tagging the word phrases (bold results) correctly. The way we can do this in the demo is by checking &quot;merging punctuation&quot; and &quot;merging phrases.&quot;</p>
<p><strong>I was wondering if it is possible to get the same output via code?</strong> I have done some research on this but haven't found anything useful so far. The thing which I came across multiple times is that you can merge chunks based on their tags.</p>
<p>Any help or pointers would be highly appreciated!</p>
",Parsing & POS Tagging,get po tag merged phrase spacy currently working nlp problem po tagging one essential step wa using stanford corenlp task came across spacy new spacy playing around online demo available explosion demo given sentence model one macbook pro line without processor dual graphic card however trade achieve longer battery life two hour demo provided following output noun adp model noun aux sconj pron aux one noun adp macbook pro line noun adp without adp processor noun cconj dual graphic card noun however adv trade noun aux sconj pron achieve verb longer battery life noun adp two hour noun output perfect use case since tagging word phrase bold result correctly way demo checking merging punctuation merging phrase wa wondering possible get output via code done research found anything useful far thing came across multiple time merge chunk based tag help pointer would highly appreciated
How to search for a pattern of strings through a list of strings and return the respective indexes?,"<p>I have already thought of some solutions in order to try to solve the problem but none seems to me to be appropriate. I'm gonna explain:</p>

<p>Let's imagine that we have the following list of strings (sequence of PoS tags from Part of Speech Tagging):</p>

<p><strong>['PROPN', 'AUX', 'ADV', 'VERB', 'SCONJ', 'PROPN', 'AUX', 'NOUN', 'CCONJ', 'PROPN', 'AUX', 'NOUN', 'PUNCT']</strong></p>

<p>My goal is to find the following pattern in the list:</p>

<p><strong>PROPN - AUX - (ANYTHING IN BETWEEN) - PUNCT</strong></p>

<p>by returning these two possible results:</p>

<p><strong>[0,1,2,3,4,5,6,7,8,9,10,11,12]</strong> and <strong>[9,10,11,12]</strong> </p>

<p>I know that one of the possible ways would be to concatenate all the strings in the list and use regex in python but that method will have a problem:</p>

<p>The indexes that would match are related only with the indexes of the characters of the that string and, after that, it will not be adequate (in my opinion) to convert these indexes to the indexes of the positions of the words in the original list. It is important to maintain the integrity of the tokenization that is done in the initial list. </p>

<p>I would be grateful if someone could propose me a solution for this problem.</p>

<p>Thanks in advance.</p>
",Parsing & POS Tagging,search pattern string list string return respective index already thought solution order try solve problem none seems appropriate gon na explain let imagine following list string sequence po tag part speech tagging propn aux adv verb sconj propn aux noun cconj propn aux noun punct goal find following pattern list propn aux anything punct returning two possible result know one possible way would concatenate string list use regex python method problem index would match related index character string adequate opinion convert index index position word original list important maintain integrity tokenization done initial list would grateful someone could propose solution problem thanks advance
"Extract Date from text using nltk and pos tagging , python","<p>I am trying to extract dates from text with formats being like <strong>January 2017 to February 2018</strong> or <strong>Jan 2018 to Feb 2018</strong>. I am using nltk and i am getting pos tags of each sentence.I want to extract tuple of all the dates with date's in date range like <strong>January 2017 to February 2018</strong>. How do i get the relevant information. Currently i am using pattern matching with regex and extracting sentences with set of words in it. Is there any other better approach and how can i capture the required tuples.</p>

<p>My code:</p>

<pre><code>res_lines = []
for res in parsed_resume:
    try:
        text = res['content']
        lines = [line.strip() for line in text.split(""\n"") if len(line) &gt; 0]
        lines = [nltk.word_tokenize(line) for line in lines]
        lines = [nltk.pos_tag(line) for line in lines]
    except(AttributeError) as e:
        pass
    res_lines.append(lines)
</code></pre>

<p>res_lines contains list of all the tokenized and pos tags of sentences. To capture this pattern i can capture postags where NNP is followed by CD for January(NNP) 2018(CD).</p>

<p>Sample text:</p>

<pre><code>JOB DETAILS
FROM     TO COMPANY  JOB TITLE
AUGUST 2019 OCTOBER 2019    ALMANSOORI PRODUCTION SERVICE, OMAN.
JUNE  2019  JULY  2019  ALMANSOORI PRODUCTION SERVICE, KENYA.
JANUARY 2018    MAY  2019   ALMANSOORI PRODUCTION SERVICE, OMAN.
JUNE 2015   DECEMBER 2017   GAS AND OILFIELD SERVICES COMPANY(GOFSCO)
SEPTEMBER 2010  JUNE 2015   OIL TEST WELL SERVICE LIMITED
</code></pre>

<p>How do i execute this logic?</p>
",Parsing & POS Tagging,extract date text using nltk po tagging python trying extract date text format like january february jan feb using nltk getting po tag sentence want extract tuple date date date range like january february get relevant information currently using pattern matching regex extracting sentence set word better approach capture required tuples code line contains list tokenized po tag sentence capture pattern capture postags nnp followed cd january nnp cd sample text execute logic
How to generate bi/tri-grams using spacy/nltk,"<p>The input text are always list of dish names where there are 1~3 adjectives  and a noun</p>

<p>Inputs</p>

<pre><code>thai iced tea
spicy fried chicken
sweet chili pork
thai chicken curry
</code></pre>

<p>outputs:</p>

<pre><code>thai tea, iced tea
spicy chicken, fried chicken
sweet pork, chili pork
thai chicken, chicken curry, thai curry
</code></pre>

<p>Basically, I am looking to parse the sentence tree and try to generate bi-grams by pairing an adjective with the noun.</p>

<p>And I would like to achieve this with spacy or nltk</p>
",Parsing & POS Tagging,generate bi tri gram using spacy nltk input text always list dish name adjective noun input output basically looking parse sentence tree try generate bi gram pairing adjective noun would like achieve spacy nltk
How to go from type theory to first-order logic lambda-expressions,"<p>As can be seen in the <a href=""http://www.nltk.org/book_1ed/"" rel=""nofollow noreferrer"">O'Reilly NLTK book</a>, Chapter 10, when I want to model the syntax tree of sentence “Bob loves Alice,” namely</p>

<p><a href=""https://i.sstatic.net/SXVPV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SXVPV.png"" alt=""enter image description here""></a></p>

<p>into first-order logic lambda-expressions, I get the following:</p>

<p><a href=""https://i.sstatic.net/8RpAc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8RpAc.png"" alt=""enter image description here""></a></p>

<p>where on the left I have the tree of types and on the right the tree of λ-expressions. I have chosen to type-raise both Bob and Alice.</p>

<p>My question is the following: from the tree of types I can easily calculate that the type of ""loves"" must be <code>&lt;&lt;&lt;e,t&gt;,t&gt;,&lt;e,t&gt;&gt;</code> but how can I deduce from this that the corresponding λ-expression must be </p>

<p>λR.λx.R(λy.loves(x,y))</p>

<p>Is there some method to obtain the λ-expression of a leave of the syntax tree from its type and from the surrounding λ-expressions?</p>
",Parsing & POS Tagging,go type theory first order logic lambda expression seen reilly nltk book chapter want model syntax tree sentence bob love alice namely first order logic lambda expression get following left tree type right tree expression chosen type raise bob alice question following tree type easily calculate type love must deduce corresponding expression must r x r love x method obtain expression leave syntax tree type surrounding expression
"Python: Chunking others than noun phrases (e.g. prepositional) using Spacy, etc","<p>Since I was told Spacy was such a powerful Python module for natural speech processing, I am now desperately looking for a way to group words together to more than noun phrases, most importantly, prepositional phrases.
I doubt there is a Spacy function for this but that would be the easiest way I guess (SpacySpaCy import is already implemented in my project).
Nevertheless, I'm open for any possibility of phrase recognition/ chunking. </p>
",Parsing & POS Tagging,python chunking others noun phrase e g prepositional using spacy etc since wa told spacy wa powerful python module natural speech processing desperately looking way group word together noun phrase importantly prepositional phrase doubt spacy function would easiest way guess spacyspacy import already implemented project nevertheless open possibility phrase recognition chunking
How to iterate through the synset list generated from wordnet using python 3.4.2,"<p>I am using wordnet to find the synonyms for a particular word as shown below</p>

<pre><code>synonyms = wn.synsets('good','a')
</code></pre>

<p>where wn is wordnet. This returns a list of synsets like </p>

<pre><code>Synset('good.a.01')    
Synset('full.s.06')    
Synset('good.a.03')    
Synset('estimable.s.02')    
Synset('beneficial.s.01')
</code></pre>

<p>etc...</p>

<p>How to iterate through each synset and get the name and the pos tag of each synset?</p>
",Parsing & POS Tagging,iterate synset list generated wordnet using python using wordnet find synonym particular word shown wn wordnet return list synset like etc iterate synset get name po tag synset
How does a Transition-based Dependency parser decide which operation to do next in its configuration stage?,"<p>I understand that the model uses previously trained Part of Speech tagging during its configuration stage. But what if most of the words are new, how would the parser decide its operation then? </p>
",Parsing & POS Tagging,doe transition based dependency parser decide operation next configuration stage understand model us previously trained part speech tagging configuration stage word new would parser decide operation
What does the tag SBAR mean in Stanford’s parse-tree representation?,"<p>When the <a href=""http://nlp.stanford.edu:8080/parser/"" rel=""nofollow noreferrer"">Online Stanford Parser tool</a> is fed this original sentence: </p>

<blockquote>
  <p>After she ate the cake, Emma visited Tony in his room.</p>
</blockquote>

<p>It produces the following parse-tree representation as its output:</p>

<pre><code>(ROOT
  (S
    (SBAR (IN After)
      (S
        (NP (PRP she))
        (VP (VBD ate)
          (NP (DT the) (NN cake)))))
    (, ,)
    (NP (NNP Emma))
    (VP (VBD visited)
      (NP
        (NP (NNP Tony))
        (PP (IN in)
          (NP (PRP$ his) (NN room)))))
    (. .)))
</code></pre>

<p>My questions are:</p>

<ol>
<li>What does the <code>SBAR</code> tag mean?</li>
<li>Why are there two different <code>S</code> tags?</li>
<li>What is the correct NLP parse-tree representation of this sentence?</li>
</ol>
",Parsing & POS Tagging,doe tag sbar mean stanford parse tree representation online stanford parser tool fed original sentence ate cake emma visited tony room produce following parse tree representation output question doe tag mean two different tag correct nlp parse tree representation sentence
"Spacy, noun phrases: How to locate noun phrase span start and end token of every noun_chunk in doc with spacy","<p>I am using spacy to get the noun phrases of a text.
What I want to do is locate those noun phrases in the text with respect to the token index of the words.</p>

<p>For instance </p>

<pre><code>import spacy

# Load English 
nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""The blue car is nicer than the white car""
noun_chunks = list(doc.noun_chunks)

for i,noun_chunk in enumerate(noun_chunks):
    for j,token in enumerate(noun_chunk):
        print(i,noun_chunk,j,token.text)
</code></pre>

<p>The value j is an index of the token.text within the span of the noun chunk, but I want to get the token.i number of the first and last word of the noun_chunk</p>

<p>In the example the two noun chunks are:
""the red car"" 
and
""the white car""</p>

<p>the desired output would be:</p>

<p>tokens:
The 1
blue 2
car 3
is 4
nicer 5
than 6
the 7
white 8
car 9</p>

<p>noun chunk 1: ""the blue car""; starts 1, ends 3</p>

<p>noun chunk 2: ""the white car""; starts 7, ends 9</p>

<p>with the start and end of a noun chunk I will be able to identify the span of the noun chunk in the doc</p>

<p>Thanks</p>
",Parsing & POS Tagging,spacy noun phrase locate noun phrase span start end token every noun chunk doc spacy using spacy get noun phrase text want locate noun phrase text respect token index word instance value j index token text within span noun chunk want get token number first last word noun chunk example two noun chunk red car white car desired output would token blue car nicer white car noun chunk blue car start end noun chunk white car start end start end noun chunk able identify span noun chunk doc thanks
An efficient way to &#39;sentencize&#39; Spacy docs and then apply POS tags,"<p>I'm looking to use the 'sentencizer' as I want to create some custom POS groupings that need to follow a rule in each individual sentence, thus I can't rely on the standard POS tagging as a first step.</p>

<p>I can think of a way to run the POS tagger first, get the idx of each tag, group out each sentence, and then just get the POS' I want... but I'm lazy and hoping there's something more elegant.</p>

<p>Is there anything I missed from the docs?</p>

<p><strong>Research and Methods</strong>
Here's one alternative... <a href=""https://stackoverflow.com/questions/56437945/how-to-use-tokenized-sentence-as-input-for-spacys-pos-tagger"">stackoverflow.com/questions/56437945/how-to-use-tokenized-sentence-as-input-for-spacys-pos-tagger</a></p>

<p>Example that obviously won't work as the nlp object needs a str:</p>

<pre><code>import spacy; from spacy.lang.en import English; nlp = English()
nlp.add_pipe(nlp.create_pipe('sentencizer'))

def token_pos_pair(token):
    lst = []
    doc = nlp(token)
    lst.append([sent for sent in doc.sents]) 
    print(lst)
    token = [(sent, sent.pos_) for sent in nlp(lst)]

doc = 'Designing of both simple and complicated Campaign assets like Email, Forms, and Landing pages. Programers program programs with programming languages'

token = token_pos_pair(doc)
</code></pre>

<blockquote>
  <p>TypeError: Argument 'string' has incorrect type (expected str, got
  list)</p>
</blockquote>
",Parsing & POS Tagging,efficient way sentencize spacy doc apply po tag looking use sentencizer want create custom po grouping need follow rule individual sentence thus rely standard po tagging first step think way run po tagger first get idx tag group sentence get po want lazy hoping something elegant anything missed doc research method one alternative href p example obviously work nlp object need str typeerror argument string ha incorrect type expected str got list
Size of vocabulary SpaCy model &#39;en_core_web_sm&#39;,"<p>I tried to see the number of words in vocabulary in SpaCy small model:  </p>

<pre><code>model_name=""en_core_web_sm""

nlpp=spacy.load(model_name)

len(list(nlpp.vocab.strings))
</code></pre>

<p>which only gave me 1185 words. I also tried in my colleagues' machines and gave me different results (1198 and 1183).</p>

<p>Is it supposed to be like this to have only such a small vocabulary to train Part-Of-Speech tagging? When I use this in my dataset, I lose a lot of words. Why the number of words vary in different machines?</p>

<p>Thanks!</p>
",Parsing & POS Tagging,size vocabulary spacy model en core web sm tried see number word vocabulary spacy small model gave word also tried colleague machine gave different result supposed like small vocabulary train part speech tagging use dataset lose lot word number word vary different machine thanks
Is there a way to correctly tag (PoS Tagging) the words which are forming a phrase together?,"<p>I tried various means to correctly tag a bunch of words which form a phrase (especially Noun Phrase) but could not succeed.</p>

<p>Example: 'the', 'first', 'early','morning', 'sunbeams'</p>

<p>'early' and 'morning' are wrongly being tagged as 'Noun' where expected outcome should be: ('first', 'adverb'), ('early', 'adverb'), ('morning', 'adjective'), ('sunbeams', 'noun')</p>

<p>Could you please suggest a procedure to tag these words correctly?</p>

<p>Thanks in advance.</p>
",Parsing & POS Tagging,way correctly tag po tagging word forming phrase together tried various mean correctly tag bunch word form phrase especially noun phrase could succeed example first early morning sunbeam early morning wrongly tagged noun expected outcome first adverb early adverb morning adjective sunbeam noun could please suggest procedure tag word correctly thanks advance
Keyword Text Recognition and Extraction,"<p>I'm trying to build an architecture that recognizes words that are related to a subject word from a paragraph of text. These ""related"" words can be words that describes the subject word or provides information about the subject word.</p>

<p>Here's a basic example: </p>

<p>John is 36, male and lives in New York. He's a skinny, about 5'9 with fair skin. </p>

<p>In this example, the subject word would be, ""John"". 
The related words are ""36"", ""male"", ""new york"", ""skinny"", 5'9"", ""fair skin"". </p>

<p>I already have a ruled based approach to identify the subject word which is working perfectly fine. Identifying the ""related"" words is not yielding the accuracy I'm hoping for. To identify the related words, I've taken supervised learning and LSTM structure approach. While I used a combination of PoS tags and dependency tags in the beginning, I have now switched to pure embeddings (transformer-based models). </p>

<p>Any architectural or method recommendations would be greatly appreciated.</p>
",Parsing & POS Tagging,keyword text recognition extraction trying build architecture recognizes word related subject word paragraph text related word word describes subject word provides information subject word basic example john male life new york skinny fair skin example subject word would john related word male new york skinny fair skin already ruled based approach identify subject word working perfectly fine identifying related word yielding accuracy hoping identify related word taken supervised learning lstm structure approach used combination po tag dependency tag beginning switched pure embeddings transformer based model architectural method recommendation would greatly appreciated
proposed nlp algorithm for text tagging,"<p>I was looking for opensource tool which can help to identify the <code>tags</code> for any user post on social media and identifying topic/off-topic or spam comment on that post. Even after looking for entire day, I could not find any suitable tool/library. </p>

<p>Here I have proposed my own algorithm for tagging user post belonging to 7 categories (jobs, discussion, events, articles, services, buy/sell, talents).</p>

<p>Initially when user makes post, he tags his post. Tags can be like <code>marketing, suggestion, entrepreneurship, MNC etc</code>. So consider for some posts I have tags and to which category they belongs.</p>

<p>Steps:</p>

<ol>
<li><p>Perform POS (part of speech) tagging on user post. 
Here two things can be done.</p>

<ul>
<li><p>considering only nouns. Nouns may represent the tag for post more
intuitively I guess</p></li>
<li><p>Considering Nouns and adjectives both. Here we can collect large
numbers of nouns and adjectives. Frequency of such words can be used
to identify tag for that post.</p></li>
</ul></li>
<li><p>For each user defined tag, we will collect POS for that post belonging to particular tag. Example. Consider user assigned tag <code>marketing</code> and post for this tag contains POS words <code>SEO</code> and <code>adwords</code>. Suppose 10 post of <code>marketing</code> tag contains <code>SEO and adwords</code> 5 and 7 times respectively. So next time when user post comes which does not have any tag but contains POS word <code>SEO</code>. <code>SEO</code> is occurring maximum times <code>7</code> in marketing tag, So we will predict <code>marketing</code> tag for this post </p></li>
<li><p>NExt steps is for identify spam or off-topic comment for POST.
Consider one user post for <code>Job</code> category. This post contains tag <code>marketing</code>. Now I will check in database for TOP  most frequent 10-15 Part of speech tags(i.e. nouns and adjective) for marketing. </p>

<p>Parallel I have POS tag for that comment. I will check whether POS(noun &amp; adj) of this post contains top most frequent tags(we can consider 15-20 such POS tags) belonging to <code>marketing</code>. </p></li>
</ol>

<p>If POS in comments does not match with any of the most frequent, top POS for marketing then that comment can be said off-topic/span</p>

<p><strong>DO YOU HAVE ANY SUGGESTION TO MAKE THIS ALGO MORE INTUITIVE??</strong></p>

<p><strong>I guess SVM can help for classification, any suggestion for this?</strong></p>

<p><strong>Apart from this WhIch machine learning technique can help here to learn system to predict tag and spam(off topic) comments</strong> </p>
",Parsing & POS Tagging,proposed nlp algorithm text tagging wa looking opensource tool help identify user post social medium identifying topic topic spam comment post even looking entire day could find suitable tool library proposed algorithm tagging user post belonging category job discussion event article service buy sell talent initially user make post tag post tag like consider post tag category belongs step perform po part speech tagging user post two thing done considering noun noun may represent tag post intuitively guess considering noun adjective collect large number noun adjective frequency word used identify tag post user defined tag collect po post belonging particular tag example consider user assigned tag post tag contains po word suppose post tag contains time respectively next time user post come doe tag contains po word occurring maximum time marketing tag predict tag post next step identify spam topic comment post consider one user post category post contains tag check database top frequent part speech tag e noun adjective marketing parallel po tag comment check whether po noun adj post contains top frequent tag consider po tag belonging po comment doe match frequent top po marketing comment said topic span suggestion make algo intuitive guess svm help classification suggestion apart machine learning technique help learn system predict tag spam topic comment
How to use StanfordNLP Python package to do dependency parsing?,"<p>I am trying to use the new NN-based parser at <a href=""https://github.com/stanfordnlp/stanfordnlp"" rel=""nofollow noreferrer"">here</a> to find all adjective phrases in a sentence (e.g., <code>good</code> and <code>extremely good</code> in <code>The weather is extremely good</code>), however, it's very lack of documentation and I could not get it working. My current code is</p>

<pre><code>import stanfordnlp
nlp = stanfordnlp.Pipeline()
doc = nlp(""The weather is extremely good"")
doc.sentences[0].print_dependencies()
</code></pre>

<p>which gives me</p>

<pre><code>('The', '2', 'det')
('weather', '5', 'nsubj')
('is', '5', 'cop')
('extremely', '5', 'advmod')
('good', '0', 'root')
</code></pre>

<p>But it is not clear how to extract the information I need, as this does not seem to be a tree structure. Does anyone have an idea?</p>
",Parsing & POS Tagging,use stanfordnlp python package dependency parsing trying use new nn based parser find adjective phrase sentence e g however lack documentation could get working current code give clear extract information need doe seem tree structure doe anyone idea
nltk tree.draw cannot plot the Syntax Tree of Chinese,"<pre><code>from stanfordcorenlp import StanfordCoreNLP
import logging

nlp = StanfordCoreNLP('http://localhost', port=9000, 
                      lang='zh',
                     logging_level=logging.DEBUG)
from nltk.tree import Tree

sentence ='今天天气很好，一起去散步吧'
parser = nlp.parse(sentence)
parser
tree = Tree.fromstring(parser)
# tree.pretty_print()
tree
tree.draw()
</code></pre>

<p>This is my code.
I have tried many ways.
I know tree.draw() is based on tkinter.
I can draw English, but cannot draw Chinese.</p>

<p><img src=""https://i.sstatic.net/uktq2.png"" alt=""enter image description here""></p>

<p>help me.
in this picture, Chinese Characters are gone.</p>

<p>The right form is like this
<img src=""https://i.sstatic.net/rnRWJ.png"" alt=""enter image description here""></p>
",Parsing & POS Tagging,nltk tree draw plot syntax tree chinese code tried many way know tree draw based tkinter draw english draw chinese help picture chinese character gone right form like
spaCy : issue in finding ROOT word in sentence using Dependency Parsing,"<p>I am trying to parse the following sentence using spaCy:</p>

<pre><code>text = ""Parsley Energy to acquire Jagged Peak Energy in an 
          all-stock deal.""

doc = nlp(text)
for token in doc:
    print(token.text, ' ====&gt; ', token.dep_)
</code></pre>

<blockquote>
  <p>This gives me <strong>Energy</strong> as Root verb which is not correct. </p>
</blockquote>

<p>But if I slightly change the sentence i.e. from <strong>to acquire</strong> to <strong>will acquire</strong>: </p>

<pre><code>new_text = ""Parsley Energy will acquire Jagged Peak Energy in an 
            all-stock deal.""
</code></pre>

<blockquote>
  <p>I get the Root verb correctly as <strong>acquire</strong>.</p>
</blockquote>

<p>is there a way I can get acquire as Root verb in first case as well?</p>
",Parsing & POS Tagging,spacy issue finding root word sentence using dependency parsing trying parse following sentence using spacy give energy root verb correct slightly change sentence e acquire acquire get root verb correctly acquire way get acquire root verb first case well
Select only &#39;NN&#39; and &#39;VB&#39; words from NTLK pos_tag,"<p>I need to print only 'NN' and 'VB' words from an entered sentence.</p>

<pre><code>import nltk
import re
import time

var = raw_input(""Please enter something: "")


exampleArray = [var]


def processLanguage():
    try:
        for item in exampleArray:
            tokenized = nltk.word_tokenize(item)
            tagged = nltk.pos_tag(tokenized)
            print tagged

            time.sleep(555)


    except Exception, e:
        print str(e)

processLanguage()
</code></pre>
",Parsing & POS Tagging,select nn vb word ntlk po tag need print nn vb word entered sentence
How to use POS tags as the features,"<p>I am looking forward to know how could I use POS tags as the features in Matlab. Adding partOfSpeechDetails after tokenizing the document has tagged every word with its respective POS. But how could I take these tags as the features to fed into a classifier?(I know several threads are there, yet I am quite confused in these)</p>

<p>As an example, for the sentence, ""hello. how are you?"", I got the POS details as the following:</p>

<pre><code>Token     DocumentNumber    SentenceNumber    LineNumber       Type        Language     PartOfSpeech 
    _______    ______________    ______________    __________    ___________    ________    ______________
    ""hello""          1                 1               1         letters           en       interjection  
    "".""              1                 1               1         punctuation       en       punctuation   
    ""how""            1                 2               1         letters           en       adverb        
    ""are""            1                 2               1         letters           en       auxiliary-verb
    ""you""            1                 2               1         letters           en       pronoun       
    ""?""              1                 2               1         punctuation       en       punctuation  
</code></pre>

<p>Now, how could I take the PartOfSpeech columns as a feature for the sentence?Should I numbering every occurrences of part of speech in a sentence like this following way?</p>

<p>If the sentence has the form like this</p>

<pre><code>noun pronoun verb adverb adjective preposition conjunction interjection punctuation
</code></pre>

<p>then my given sentence would be encoded as</p>

<pre><code>0 1 1 1 0 0 0 1 2
</code></pre>

<p>Is it the way of encoding part of speech? And do u shed some light on how many part of speeches are available in Matlab?</p>
",Parsing & POS Tagging,use po tag feature looking forward know could use po tag feature matlab adding partofspeechdetails tokenizing document ha tagged every word respective po could take tag feature fed classifier know several thread yet quite confused example sentence hello got po detail following could take partofspeech column feature sentence numbering every occurrence part speech sentence like following way sentence ha form like given sentence would encoded way encoding part speech u shed light many part speech available matlab
Rule based Part of Speech parsing dilemma,"<p>I'm currently trying to build a sentence parser that extracts unknown parts of speech. Its a bit abstract but my methodology is basically creating a set of grammatical rules that the function can use to parse the text.</p>

<p>I'm using Spacy's PoS tagger right now just to extract the pos tags from an example sentence. I know Spacy also has a dependency parser but from what I've read on the documentation its used for matching a <strong>known</strong> phrase.</p>

<p>So my question is this:</p>

<p>By creating a set of grammatical rules, whats the best way to extract an <strong>unknown</strong> target word from a string based off of those rules?</p>

<p>For example:</p>

<pre><code>import spacy

nlp = spacy.load('en_core_web_sm')

Example = ""I really hate all people who are green, I wish they would go back home""
ex_string = Example.split()
doc = nlp(Example)
pos_tagged_context = [token.tag_ for token in doc]
Word_Dict = {}
</code></pre>

<p>The first rule in this case would be the PoS tag list of <code>pos_tagged_context</code> which matches the sentence structure of <code>ex_string</code></p>

<pre><code>['PRP', 'RB', 'VBP', 'DT', 'NNS', 'WP', 'VBP', 'JJ', ',', 'PRP', 'VBP', 'PRP', 'MD', 'VB', 'RB', 'RB']
</code></pre>

<p>Two problems arise from this though, the easier one being that when printing <code>Word_Dict</code> several PoS tags are lost:</p>

<pre><code>{'I': ',', 'really': 'RB', 'hate': 'VBP', 'all': 'DT', 'people': 'NNS', 'who': 'WP', 'are': 'VBP', 'green,': 'JJ', 'wish': 'PRP', 'they': 'VBP', 'would': 'PRP', 'go': 'MD', 'back': 'VB', 'home': 'RB'}
</code></pre>

<p>The second problem is more abstract, since the structure of a ""negative"" sentence is inherently relative is there a good ""general form"" when creating these rules?</p>

<p>An ideal output would use the structure of the sentence and identify the target word within it, in this case ""green"".</p>

<p>Let me know if the question is too abstract or needs more clarification!</p>
",Parsing & POS Tagging,rule based part speech parsing dilemma currently trying build sentence parser extract unknown part speech bit abstract methodology basically creating set grammatical rule function use parse text using spacy po tagger right extract po tag example sentence know spacy also ha dependency parser read documentation used matching known phrase question creating set grammatical rule whats best way extract unknown target word string based rule example first rule case would po tag list match sentence structure two problem arise though easier one printing several po tag lost second problem abstract since structure negative sentence inherently relative good general form creating rule ideal output would use structure sentence identify target word within case green let know question abstract need clarification
NLTK WordNet Lemmatizer: Shouldn&#39;t it lemmatize all inflections of a word?,"<p>I'm using the NLTK WordNet Lemmatizer for a Part-of-Speech tagging project by first modifying each word in the training corpus to its stem (in place modification), and then training only on the new corpus. However, I found that the lemmatizer is not functioning as I expected it to.</p>

<p>For example, the word <code>loves</code> is lemmatized to <code>love</code> which is correct, but the word <code>loving</code> remains <code>loving</code> even after lemmatization. Here <code>loving</code> is as in the sentence ""I'm loving it"".</p>

<p>Isn't <code>love</code> the stem of the inflected word <code>loving</code>? Similarly, many other 'ing' forms remain as they are after lemmatization. Is this the correct behavior?</p>

<p>What are some other lemmatizers that are accurate? (need not be in NLTK) Are there morphology analyzers or lemmatizers that also take into account a word's Part Of Speech tag, in deciding the word stem? For example, the word <code>killing</code> should have <code>kill</code> as the stem if <code>killing</code> is used as a verb, but it should have <code>killing</code> as the stem if it is used as a noun (as in <code>the killing was done by xyz</code>).</p>
",Parsing & POS Tagging,nltk wordnet lemmatizer lemmatize inflection word using nltk wordnet lemmatizer part speech tagging project first modifying word training corpus stem place modification training new corpus however found lemmatizer functioning expected example word lemmatized correct word remains even lemmatization sentence loving stem inflected word similarly many ing form remain lemmatization correct behavior lemmatizers accurate need nltk morphology analyzer lemmatizers also take account word part speech tag deciding word stem example word stem used verb stem used noun
"NLTK RegexpParser, chunk phrase by matching exactly one item","<p>I'm using NLTK's <code>RegexpParser</code> to chunk a noun phrase, which I define with a grammar as </p>

<pre><code> grammar = ""NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN|NNS&gt;+}""
 cp = RegexpParser(grammar)
</code></pre>

<p>This is grand, it is matching a noun phrase as:</p>

<ul>
<li>DT if it exists</li>
<li>JJ in whatever number</li>
<li>NN or NNS, at least one</li>
</ul>

<p>Now, what if I want to match the same but having the <em>whatever number</em> for JJ transformed into <em>only one</em>? So I want to match DT if it exists, <strong>one</strong> JJ and 1+ NN/NNS. If there are more than one JJ, I want to match only one of them, the one nearest to the noun (and DT if there is, and NN/NNS).</p>

<p>The grammar</p>

<pre><code>grammar = ""NP: {&lt;DT&gt;?&lt;JJ&gt;&lt;NN|NNS&gt;+}""
</code></pre>

<p>would match only when there is just one JJ, the grammar</p>

<pre><code>grammar = ""NP: {&lt;DT&gt;?&lt;JJ&gt;{1}&lt;NN|NNS&gt;+}""
</code></pre>

<p>which I thought would work given the <a href=""http://www.rexegg.com/regex-quickstart.html"" rel=""nofollow"">typical Regexp patterns</a>, raises a ValueError.</p>

<p>For example, in ""This beautiful green skirt"", I'd like to chunk ""This green skirt"".</p>

<p>So, how would I proceed?</p>
",Parsing & POS Tagging,nltk regexpparser chunk phrase matching exactly one item using nltk chunk noun phrase define grammar grand matching noun phrase dt exists jj whatever number nn nns least one want match whatever number jj transformed one want match dt exists one jj nn nns one jj want match one one nearest noun dt nn nns grammar would match one jj grammar thought would work given typical regexp pattern raise valueerror example beautiful green skirt like chunk green skirt would proceed
What is the best way to split a sentence for a keyword extraction task?,"<p>I'm doing a keyword extraction using TD-IDF on a large number of documents. Currenly I'm splitting each sentence based on n-gram. More particularly I'm using tri-gram. However, this is not the best way to split each sentence into ints constituting keywords. For example a noun phrase like 'triple heart bypass' may not always get detected as one term. </p>

<p>The other alternative to chunk each sentence into its constituting elements look to be part of speech tagging and <a href=""https://opennlp.apache.org/documentation/1.6.0/manual/opennlp.html#tools.chunker"" rel=""nofollow"">chunking</a> in <a href=""https://opennlp.apache.org/"" rel=""nofollow"">Open NLP</a>. In this approach phrase like 'triple heart bypass' always gets extracted as a whole but the downside is in TF-IDF the frequency of extracted terms (phrases) dramatically drops. </p>

<p>Does anyone have any suggestion on either or these two approaches or have any other ideas to improve the quality of the keywords?</p>
",Parsing & POS Tagging,best way split sentence keyword extraction task keyword extraction using td idf large number document currenly splitting sentence based n gram particularly using tri gram however best way split sentence ints constituting keywords example noun phrase like triple heart bypass may always get detected one term alternative chunk sentence constituting element look part speech tagging chunking open nlp approach phrase like triple heart bypass always get extracted whole downside tf idf frequency extracted term phrase dramatically drop doe anyone suggestion either two approach idea improve quality keywords
How to tell if two natural language queries have the same meaning,"<p>I am building a system to change natural language questions into SQL queries. Right now what I am implementing is a refactoring of a natural language question to be more structured so that I will have an easier time converting it into a sql statement. </p>

<p>The restructured language will follow these rules: 
  </p>

<p> what they want to do ex. ""Find"" ""List"" ""Give""
 attributes they want us to retrieve ex. Table attributes from sql schema 
 entities that they want us to match on </p>

<p>This refactored language is great and can easily be transformed into SQL, but the problem is that I am creating a large combination of all the noun chunks and entities which means lots of sentences. Future development will help minimize these but that is for later. </p>

<p>So from the large amount of sentences I need to find which one is most similar to the original query. </p>

<p>So my question is, what kind of similarity functions would you recommend? ex. parse tree structure, semantic and syntactic similarity...</p>

<p>Thanks for the help, I am building this for open-source so any help is going to a good cause</p>
",Parsing & POS Tagging,tell two natural language query meaning building system change natural language question sql query right implementing refactoring natural language question structured easier time converting sql statement restructured language follow rule want ex find list give attribute want u retrieve ex table attribute sql schema entity want u match refactored language great easily transformed sql problem creating large combination noun chunk entity mean lot sentence future development help minimize later large amount sentence need find one similar original query question kind similarity function would recommend ex parse tree structure semantic syntactic similarity thanks help building open source help going good cause
Pattern-lib : Which word is the ROOT(dependency parsing) of the sentence?,"<p>In spacy the start of the dependency begins at ROOT.</p>

<p>In <strong>pattern.en</strong> the same verb is just tagged as VP. If there are multiple VP's how do you know which one is the ROOT ?
Is it always the one marked with ""-1"" ?  </p>

<pre><code> : pattern.en.parse('John hit the ball', relations=True)
 : u'John/NNP/B-NP/O/NP-SBJ-1 hit/VBD/B-VP/O/VP-1 the/DT/B-NP/O/NP-OBJ-1 ball/NN/I-NP/O/NP-OBJ-1'


 : doc = nlp(u'John hit the ball')
 : [ (t.text, t.dep_) for t in doc ]
 : [(u'John', u'nsubj'), (u'hit', u'ROOT'), (u'the', u'det'), (u'ball', u'dobj')]
</code></pre>

<p>What about multiple OBJ and SUBJ ?</p>
",Parsing & POS Tagging,pattern lib word root dependency parsing sentence spacy start dependency begin root pattern en verb tagged vp multiple vp know one root always one marked multiple obj subj
Escape parentheses in NLTK parse tree,"<p>In NLTK we can convert a parentheses tree into an actual Tree object. However, when a token contains parentheses, the parsing is not what you would expect since NLTK parses those parentheses as a new node.</p>

<p>As an example, take the sentence</p>

<blockquote>
  <p>They like(d) it a lot</p>
</blockquote>

<p>This could be parsed as </p>

<pre><code>(S (NP (PRP They)) (VP like(d) (NP (PRP it)) (NP (DT a) (NN lot))) (. .))
</code></pre>

<p>But if you parse this with NLTK into a tree, and output it - it is clear that the <code>(d)</code> is parsed as a new node, which is no surprise.</p>

<pre class=""lang-py prettyprint-override""><code>from nltk import Tree

s = '(S (NP (PRP They)) (VP like(d) (NP (PRP it)) (NP (DT a) (NN lot))) (. .))'

tree = Tree.fromstring(s)
print(tree)
</code></pre>

<p>The result is</p>

<pre><code>(S
  (NP (PRP They))
  (VP like (d ) (NP (PRP it)) (NP (DT a) (NN lot)))
  (. .))
</code></pre>

<p>So <code>(d )</code> is a node inside the VP rather than part of the token <code>like</code>. Is there a way in the tree parser to escape parentheses?</p>
",Parsing & POS Tagging,escape parenthesis nltk parse tree nltk convert parenthesis tree actual tree object however token contains parenthesis parsing would expect since nltk par parenthesis new node example take sentence like lot could parsed parse nltk tree output clear parsed new node surprise result node inside vp rather part token way tree parser escape parenthesis
How to represent an unknown/blank word from a transcription in spacy?,"<p>I am dealing with text from audio transcripts, and there are some unknown words. There are markers for each unknown word (e.g. ""He unknown to the store""). I'm looking for the best way to represent the ""unknown"" word so as to mess up spacy's sentence dependency parsing the least. </p>

<p>What is the best replacement for  to increase odds that spacy's sentence dependency parser works the best across the widest range of sentences? Is a space/' ' or a '___' or a '...' or does it not matter? There is no structure to when/where the \ occur.</p>

<p>thanks!</p>
",Parsing & POS Tagging,represent unknown blank word transcription spacy dealing text audio transcript unknown word marker unknown word e g unknown store looking best way represent unknown word mess spacy sentence dependency parsing least best replacement increase odds spacy sentence dependency parser work best across widest range sentence space doe matter structure occur thanks
How to build several syntax trees from a sentence?,"<p>I am working on a parser for Russian based on context-free grammar and I would like to build syntax tree for each sentence. It is evidence that for each sentence there can be several trees.</p>

<p>I am not sure about what an efficient way is to build all these trees.</p>

<p>What I have done:</p>

<p>1) after some proccessing I have a sentence where each word has several tags
2) then I build a tree where the leaves are words and then I reduce words to more general phrases (according to grammar); each reduction - new node.
3) when all reductions are done, try to make root with ""S -> NP[sbj] VP"" rule. It happens only if there is agreement between grammemes. </p>

<p>Result is all trees where root exists.</p>

<pre class=""lang-py prettyprint-override""><code>parser.parse(sent)
[['я', ""NP[case='nomn']"", ['NPRO', 'sing', '1per', None, 'nomn', None, None]]]

[['хочу', 'V[tran]', ['VERB', 'sing', '1per', None, None, 'tran', 'pres']]]

[['писать', 'VP', ['INFN', None, None, None, None, 'tran', None]], ['писать', 'VP', ['INFN', None, None, None, None, 'intr', None]]]

[['письмо', ""NP[case='accs']"", ['NOUN', 'sing', None, 'neut', 'accs', None, None]], ['письмо', ""NP[case='nomn']"", ['NOUN', 'sing', None, 'neut', 'nomn', None, None]]]

[['другу', ""NP[case='datv']"", ['NOUN', 'sing', None, None, 'datv', None, None]]]

# it is simple example, usually, each word has 3-4 tags.

</code></pre>

<p>Naive way to do it is just take all possible combinations of words and try to make trees from them. Complexety of this approach is very big. How can I accomplish this task in a more efficient way?</p>
",Parsing & POS Tagging,build several syntax tree sentence working parser russian based context free grammar would like build syntax tree sentence evidence sentence several tree sure efficient way build tree done proccessing sentence word ha several tag build tree leaf word reduce word general phrase according grammar reduction new node reduction done try make root np sbj vp rule happens agreement grammemes result tree root exists naive way take possible combination word try make tree complexety approach big accomplish task efficient way
python spacy look for chunks backwards (before a reference),"<p>I am using spacy for a NLP project.
when creating a doc with Spacy you can find out the noun chunks in the text (also known as ""noun phrases"") in the following way:</p>

<pre><code>import spacy
nlp = spacy.load(""en_core_web_sm"")
doc = nlp(u""The companies building cars do not want to spend more money in improving diesel engines because the government will not subsidise such engines anymore."")
for chunk in doc.noun_chunks:
    print(chunk.text)
</code></pre>

<p>This will give a list of the noun phrases.</p>

<p>In this case for instance the first noun phrase is ""The companies"".</p>

<p>Suppose you have a text where noun chunks are referenced with a number.</p>

<p>like:</p>

<pre><code>doc=nlp(the Window (23) is closed because the wall (34) of the beautiful building (45) is not covered by the insurance (45))
</code></pre>

<p>assume I have the code to identify the references for instance tagging them:</p>

<pre><code>myprocessedtext=the Window &lt;ref&gt;(23)&lt;/ref&gt; is closed because the wall &lt;ref&gt;(34)&lt;/ref&gt; of the beautiful building &lt;ref&gt;(45)&lt;/ref&gt; is not covered by the insurance &lt;ref&gt;(45)&lt;/ref&gt;
</code></pre>

<p>How could I get the noun chunks (noun phrases) immediately preceding the references?</p>

<p>my idea: passing the 10 words preceding every reference to a spacy doc object, extract the noun chunks and getting the last one. This is highly inefficient since creating the doc objects is very high time consuming.</p>

<p>Any other idea without having to create extra nlp objects?</p>

<p>thanks.</p>
",Parsing & POS Tagging,python spacy look chunk backwards reference using spacy nlp project creating doc spacy find noun chunk text also known noun phrase following way give list noun phrase case instance first noun phrase company suppose text noun chunk referenced number like assume code identify reference instance tagging could get noun chunk noun phrase immediately preceding reference idea passing word preceding every reference spacy doc object extract noun chunk getting last one highly inefficient since creating doc object high time consuming idea without create extra nlp object thanks
Visualize Parse Tree Structure,"<p>I would like to display the parsing (POS tagging) from <strong>openNLP</strong> as a tree structure visualization.  Below I provide the parse tree from <strong>openNLP</strong> but I can not plot as a visual tree common to <a href=""http://www.nltk.org/book/ch08.html#ubiquitous-ambiguity"" rel=""nofollow noreferrer"">Python's parsing</a>. </p>

<pre><code>install.packages(
    ""http://datacube.wu.ac.at/src/contrib/openNLPmodels.en_1.5-1.tar.gz"",  
    repos=NULL, 
    type=""source""
)

library(NLP)
library(openNLP)

x &lt;- 'Scroll bar does not work the best either.'
s &lt;- as.String(x)

## Annotators
sent_token_annotator &lt;- Maxent_Sent_Token_Annotator()
word_token_annotator &lt;- Maxent_Word_Token_Annotator()
parse_annotator &lt;- Parse_Annotator()

a2 &lt;- annotate(s, list(sent_token_annotator, word_token_annotator))
p &lt;- parse_annotator(s, a2)
ptext &lt;- sapply(p$features, `[[`, ""parse"")
ptext
Tree_parse(ptext)

## &gt; ptext
## [1] ""(TOP (S (NP (NNP Scroll) (NN bar)) (VP (VBZ does) (RB not) (VP (VB work) (NP (DT the) (JJS best)) (ADVP (RB either))))(. .)))""
## &gt; Tree_parse(ptext)
## (TOP
##   (S
##     (NP (NNP Scroll) (NN bar))
##     (VP (VBZ does) (RB not) (VP (VB work) (NP (DT the) (JJS best)) (ADVP (RB either))))
##     (. .)))
</code></pre>

<p>The tree structure should look similar to this:</p>

<p><a href=""https://i.sstatic.net/Njv6T.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Njv6T.png"" alt=""enter image description here""></a></p>

<p>Is there a way to display this tree visualization? </p>

<p>I found <a href=""https://stackoverflow.com/a/33499140/1000343"">this related tree viz</a> question for plotting numeric expressions that may be of use but that I could not generalize to sentence parse visualization.</p>
",Parsing & POS Tagging,visualize parse tree structure would like display parsing po tagging opennlp tree structure visualization provide parse tree opennlp plot visual tree common python parsing tree structure look similar way display tree visualization found href related tree viz question plotting numeric expression may use could generalize sentence parse visualization
How to test whether a word is in singular form or not in python?,"<p>I am trying to get whether a word is in singular form or in plural form by using nltk pos_tag. But the results are not accurate.</p>

<p>So, I need a way to find how can get whether a word is in singular form or in plural form? moreover I need it without using any python package.</p>
",Parsing & POS Tagging,test whether word singular form python trying get whether word singular form plural form using nltk po tag result accurate need way find get whether word singular form plural form moreover need without using python package
StanfordCoreNLP: Different parses for same sentece when using local CoreNLP server and Web interface,"<p>I'm using Stanford's CoreNLP excellent architecture and it basically works very well. However, occassionally I stumble upon seemingly simple sentences where the parses are arguably off. For example for the sentence:</p>

<ul>
<li>I've heard that John never trains</li>
</ul>

<p>I get the following parse tree:</p>

<pre><code>(ROOT
  (S
    (NP (PRP I))
      (VP (VBP 've)
      (VP (VBN heard)
        (SBAR (IN that)
          (NP
            (NP (NNP John))
            (NP
              (ADVP (RB never))
              (NP (NNS trains)))))))
    (. .)))
</code></pre>

<p>where the parser interprets ""trains"" as noun. But when I enter this sentence in the official <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow noreferrer"">demo interface</a>, the result is:</p>

<pre><code>(ROOT
  (S
    (NP (PRP I))
    (VP (VBP 've)
      (VP (VBN heard)
        (SBAR (IN that)
          (S
            (NP (NNP John))
            (ADVP (RB never))
            (VP (VBZ trains))))))
    (. .)))
</code></pre>

<p>which looks much more meaningful. At the bottom of the page it says <code>Parser englishPCFG.ser.gz</code>. I'm using the same model but also tried <code>englishSR.ser.gz</code> with no change.</p>

<p>I'm running the <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow noreferrer"">CoreNLP as a server</a>, starting it with:</p>

<pre><code>java -mx8g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000
</code></pre>

<p>In my Java code I create a client as follow:</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,parse,ner,depparse"");
StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""http://localhost"", 9000, 2);
</code></pre>

<p>What I need to change in my setup to get the same result as in the demo interface? </p>
",Parsing & POS Tagging,stanfordcorenlp different par sentece using local corenlp server web interface using stanford corenlp excellent architecture basically work well however occassionally stumble upon seemingly simple sentence par arguably example sentence heard john never train get following parse tree parser interprets train noun enter sentence official demo interface result look much meaningful bottom page say using model also tried change running corenlp server starting java code create client follow need change setup get result demo interface
"Gensim: Manual generation of training tuples of (target, context, label)","<p>I am asking this question as a lazy researcher who just wants to try out random crazy ideas quickly, without spending a ton of time reinventing wheels. I completely understand these aren't the intended use cases.</p>

<p>To test a number of hypothesis, I would love to</p>

<ul>
<li>generate the (target, context, +1) tuples differently, instead of the default sliding window.</li>
<li>generate the negative samples (target, random_context, -1) tuples based on some rules, instead of from random NCE draws.</li>
</ul>

<p>For example, I can get the parse tree of a sentence and use parent-child relationship to generate tuples, which is a non-linear window(somebody already tried it in NLP research community, hand-coded ofc...). I can also get an antonyms dictionary to lookup and to generate more negative samples in addition to the random ones (not sure, may help with faster convergence).</p>

<p>Are there some private member functions (something that starts with <code>_XX</code>)I can override to achieve these?</p>
",Parsing & POS Tagging,gensim manual generation training tuples target context label asking question lazy researcher want try random crazy idea quickly without spending ton time reinventing wheel completely understand intended use case test number hypothesis would love generate target context tuples differently instead default sliding window generate negative sample target random context tuples based rule instead random nce draw example get parse tree sentence use parent child relationship generate tuples non linear window somebody already tried nlp research community hand coded ofc also get antonym dictionary lookup generate negative sample addition random one sure may help faster convergence private member function something start override achieve
Basic and enhanced dependencies give different results in Stanford coreNLP,"<p>I am using dependency parsing of coreNLP for a project of mine. The basic and enhanced dependencies are different result for a particular dependency.
I used the following code to get enhanced dependencies.</p>

<pre><code>val lp = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")
lp.setOptionFlags(""-maxLength"", ""80"")
val rawWords = edu.stanford.nlp.ling.Sentence.toCoreLabelList(tokens_arr:_*)
val parse = lp.apply(rawWords)
val tlp = new PennTreebankLanguagePack()
val gsf:GrammaticalStructureFactory = tlp.grammaticalStructureFactory()
val gs:GrammaticalStructure = gsf.newGrammaticalStructure(parse)
val tdl = gs.typedDependenciesCCprocessed()
</code></pre>

<p>For the following example, </p>

<pre><code>Account name of ramkumar.
</code></pre>

<p>I use simple API to get basic dependencies. The dependency i get between
(account,name) is (compound). But when i use the above code to get enhanced dependency i get the relation between (account,name) as (dobj).</p>

<p>What is the fix to this? Is this a bug or am i doing something wrong? </p>
",Parsing & POS Tagging,basic enhanced dependency give different result stanford corenlp using dependency parsing corenlp project mine basic enhanced dependency different result particular dependency used following code get enhanced dependency following example use simple api get basic dependency dependency get account name compound use code get enhanced dependency get relation account name dobj fix bug something wrong
Create sentence (row) to POS tags counts (column) matrix from a dataframe,"<p>I am trying to build a matrix where the first row will be a part of speech, first column a sentence. values in the matrix should show the number of such POS in a sentence.</p>

<p>So I am creating POS tags in this way:</p>

<pre><code>data = pd.read_csv(open('myfile.csv'),sep=';') 

target = data[""label""]
del data[""label""]

data.sentence = data.sentence.str.lower() # All strings in data frame to lowercase

for line in data.sentence:
    Line_new= nltk.pos_tag(nltk.word_tokenize(line))
    print(Line_new)
</code></pre>

<p>The output is:</p>

<pre><code>[('together', 'RB'), ('with', 'IN'), ('the', 'DT'), ('6th', 'CD'), ('battalion', 'NN'), ('of', 'IN'), ('the', 'DT')]
</code></pre>

<p>How can I create a matrix which I have described above from such output? </p>

<p>UPDATE:
The desired output is</p>

<pre><code>                   NN  VB    IN    VBZ    DT
 I was there       1   1     1      0     0
 He came there     0   0     1      1     1
</code></pre>

<p>myfile.csv:</p>

<pre><code>""A child who is exclusively or predominantly oral (using speech for communication) can experience social isolation from his or her hearing peers, particularly if no one takes the time to explicitly teach them social skills that other children acquire independently by virtue of having normal hearing."";""certain""
""Preliminary Discourse to the Encyclopedia of Diderot"";""certain""
""d'Alembert claims that it would be ignorant to perceive that everything could be known about a particular subject."";""certain""
""However, as the overemphasis on parental influence of psychodynamics theory has been strongly criticized in the previous century, modern psychologists adopted interracial contact as a more important determinant than childhood experience on shaping people’s prejudice traits (Stephan &amp; Rosenfield, 1978)."";""uncertain""
""this can also be summarized as a distinguish behaviour on the peronnel level"";""uncertain""
</code></pre>
",Parsing & POS Tagging,create sentence row po tag count column matrix dataframe trying build matrix first row part speech first column sentence value matrix show number po sentence creating po tag way output create matrix described output update desired output myfile csv
How do I do dependency parsing in NLTK?,"<p>Going through the NLTK book, it's not clear how to generate a dependency tree from a given sentence.</p>

<p>The relevant section of the book: <a href=""https://www.nltk.org/book/ch08.html#dependencies-and-dependency-grammar"" rel=""noreferrer"">sub-chapter on dependency grammar</a> gives an <a href=""https://www.nltk.org/book/ch08.html#fig-depgraph0"" rel=""noreferrer"">example figure</a> but it doesn't show how to parse a sentence to come up with those relationships - or maybe I'm missing something fundamental in NLP?</p>

<p><strong>EDIT:</strong>
I want something similar to what the <a href=""http://nlp.stanford.edu:8080/parser/"" rel=""noreferrer"">stanford parser</a> does:
Given a sentence ""I shot an elephant in my sleep"", it should return something like:</p>

<pre><code>nsubj(shot-2, I-1)
det(elephant-4, an-3)
dobj(shot-2, elephant-4)
prep(shot-2, in-5)
poss(sleep-7, my-6)
pobj(in-5, sleep-7)
</code></pre>
",Parsing & POS Tagging,dependency parsing nltk going nltk book clear generate dependency tree given sentence relevant section book sub chapter dependency grammar give example figure show parse sentence come relationship maybe missing something fundamental nlp edit want something similar stanford parser doe given sentence shot elephant sleep return something like
Dependency parsing of noun chunks in spacy,"<p>There's a <a href=""https://stackoverflow.com/questions/39241709/how-to-generate-bi-tri-grams-using-spacy-nltk?rq=1"">couple</a> <a href=""https://stackoverflow.com/questions/53598243/is-there-a-bi-gram-or-tri-gram-feature-in-spacy/53612262#53612262"">of</a> existing questions about getting noun chunks in spacy, which is <a href=""https://spacy.io/usage/linguistic-features#noun-chunks"" rel=""nofollow noreferrer"">relatively straightforward</a>.</p>

<p>What I'm interested in is replicating the dependency parsing on top of pre-specified ngrams inside a sentence. Like in the example below, from <a href=""https://youtu.be/jpWqz85F_4Y?t=991"" rel=""nofollow noreferrer"">this</a> spacy talk, where <code>Alex Smith</code> and <code>East London</code> are treated as a single token in the dependency parse.</p>

<p><a href=""https://i.sstatic.net/c5ktp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/c5ktp.png"" alt=""enter image description here""></a></p>
",Parsing & POS Tagging,dependency parsing noun chunk spacy relatively straightforward interested replicating dependency parsing top pre specified ngrams inside sentence like example spacy talk treated single token dependency parse
extract noun phrases using opennlp in java,"<p>I am trying to extract the noun phrases from sentences. I am using opennlp librari ""en-parser-chunking.bin"".</p>

<p>code example: </p>

<pre><code> ArrayList&lt;opennlp.tools.parser.Parse&gt; nounPhrases = new ArrayList&lt;&gt;();

 searchmethod(""what is the nickname of the British flag?"");
 for(int t =0; t&lt;50; t++)
 {
     str= text.get(t);
     InputStream is = new FileInputStream(""en-parser-chunking.bin"");
     ParserModel model = new ParserModel(is);
     opennlp.tools.parser.Parser parser = ParserFactory.create(model);
     opennlp.tools.parser.Parse[] topParses = ParserTool.parseLine(str, parser, 1);
     for (opennlp.tools.parser.Parse p : topParses){
          p.show();
          if (p.getType().equals(""NP"")) {
              nounPhrases.add(p);
          }
     }                                        
  }
</code></pre>

<p>With this code i get the following result: </p>

<pre><code>(TOP (S (NP (NP (DT The) (NN nickname)) (PP (IN for) (NP (DT the) (JJ British) (NN flag)))) (VP (VBZ is) (NP (NP (DT the) (NNP Union) (NNP Jack.)) (SBAR (IN Although) (S (NP (PRP it)) (VP (VBZ is) (ADVP (RB only) (RB correctly)) (VP (VBN known) (PP (IN as) (NP (DT this) (NN when) (NN flown))) (PP (IN on) (NP (DT a) (NN ship.)))))))))))  
</code></pre>

<p>How can i extract from that result the noun phrases? </p>

<p>Any help would be greatly appreciated.                           </p>
",Parsing & POS Tagging,extract noun phrase using opennlp java trying extract noun phrase sentence using opennlp librari en parser chunking bin code example code get following result extract result noun phrase help would greatly appreciated
How to get all noun phrases in Spacy(Python),"<p>I would like to extract &quot;all&quot; the noun phrases from a sentence. I'm wondering how I can do it. I have the following code:</p>
<pre><code>doc2 = nlp(&quot;what is the capital of Bangladesh?&quot;)
for chunk in doc2.noun_chunks:
    print(chunk)
</code></pre>
<h3>Output:</h3>
<pre><code>1. what

2. the capital

3. bangladesh
</code></pre>
<h3>Expected:</h3>
<blockquote>
<p>the capital of Bangladesh</p>
</blockquote>
<p>I have tried answers from spacy doc and StackOverflow. Nothing worked. It seems only <code>cTakes</code> and <code>Stanford core NLP</code> can give such complex NP.</p>
<p>Any help is appreciated.</p>
",Parsing & POS Tagging,get noun phrase spacy python would like extract noun phrase sentence wondering following code output expected capital bangladesh tried answer spacy doc stackoverflow nothing worked seems give complex np help appreciated
Recursion in nltk&#39;s RegexpParser,"<p>Based on the <a href=""http://www.nltk.org/book/ch07.html#code-cascaded-chunker"" rel=""nofollow noreferrer"">grammar in the chapter 7 of the NLTK Book</a>:</p>

<pre><code>grammar = r""""""
      NP: {&lt;DT|JJ|NN.*&gt;+} # ...
""""""
</code></pre>

<p>I want to expand <strong>NP</strong> (noun phrase) to include multiple <strong>NP</strong> joined by <strong>CC</strong> (coordinating conjunctions: <em>and</em>) or <strong>,</strong> (commas) to capture noun phrases like:</p>

<ul>
<li><em>The house and tree</em></li>
<li><em>The apple, orange and mango</em></li>
<li><em>Car, house, and plane</em></li>
</ul>

<p>I cannot get my modified grammar to capture those as a single <strong>NP</strong>:</p>

<pre><code>import nltk

grammar = r""""""
  NP: {&lt;DT|JJ|NN.*&gt;+(&lt;CC|,&gt;+&lt;NP&gt;)?}
""""""

sentence = 'The house and tree'
chunkParser = nltk.RegexpParser(grammar)
words = nltk.word_tokenize(sentence)
tagged = nltk.pos_tag(words)
print(chunkParser.parse(tagged))
</code></pre>

<p>Results in:</p>

<pre><code>(S (NP The/DT house/NN) and/CC (NP tree/NN))
</code></pre>

<p>I've tried moving the <strong>NP</strong> to the beginning: <code>NP: {(&lt;NP&gt;&lt;CC|,&gt;+)?&lt;DT|JJ|NN.*&gt;+}</code> but I get the same result</p>

<pre><code>(S (NP The/DT house/NN) and/CC (NP tree/NN))
</code></pre>
",Parsing & POS Tagging,recursion nltk regexpparser based grammar chapter nltk book want expand np noun phrase include multiple np joined cc coordinating conjunction comma capture noun phrase like house tree apple orange mango car house plane get modified grammar capture single np result tried moving np beginning get result
POS tagging not consistent using Spacy en_core_web_lg model,"<ul>
<li><p>POS tagging for PROPN <strong>not working</strong> in an expected manner using the <strong>en_core_web_lg</strong> model.</p></li>
<li><p>POS tagging works more predictably using the <strong>_md</strong> model.</p></li>
</ul>

<p>Given the (poorly-formed) sentence:
<em>""CK7, CK-20, GATA 3, PSA, are all negative.""</em></p>

<p>When using the _lg model, ""CK7"" is tagged as a NOUN(NNS).</p>

<p>When using the _md model, ""CK7"" is tagged as a PROPN(NNP). <strong>This is correct.</strong></p>

<p>When using the <strong>_lg</strong> model, and replacing ""CK7"" in the sentence for:</p>

<ul>
<li><p>""CK1"" tagged as PROPN</p></li>
<li><p>""CK2"" tagged as PROPN</p></li>
<li><p>""CK3"" ,""CK4"" tagged as PROPN</p></li>
<li><p>""CK5"" tagged as <strong>ADJ</strong></p></li>
<li><p>""CK6"" tagged as PROPN</p></li>
<li><p>""CK7"" tagged as <strong>NOUN</strong></p></li>
<li><p>""CK8"" tagged as PROPN</p></li>
<li><p>""CK9"" tagged as <strong>ADP</strong></p></li>
<li><p>""CK22"", ""CK222"",  tagged as  PROPN</p></li>
</ul>

<p>When using the <em>_md</em> model, and replacing ""CK7"" as described above, all were tagged PROPN, <strong>as expected</strong>.</p>

<p>As <strong>most</strong> of the sentences I will be analyzing will be <strong>poorly</strong> formed, I thought that the <strong>_lg</strong> model's 'deeper' <strong>dependency parsing</strong> would serve better, only to find the above issues with <strong>POS tagging</strong>.</p>

<p>Please advise on:</p>

<ol>
<li>How to deal with the counter-intuitive POS tagging when using the en_core_web_lg model? </li>
<li>Which model is best for dependency parsing poorly-formed sentences?</li>
</ol>

<p>Thank you very much.</p>
",Parsing & POS Tagging,po tagging consistent using spacy en core web lg model po tagging propn working expected manner using en core web lg model po tagging work predictably using md model given poorly formed sentence ck ck gata psa negative using lg model ck tagged noun nns using md model ck tagged propn nnp correct using lg model replacing ck sentence ck tagged propn ck tagged propn ck ck tagged propn ck tagged adj ck tagged propn ck tagged noun ck tagged propn ck tagged adp ck ck tagged propn using md model replacing ck described tagged propn expected sentence analyzing poorly formed thought lg model deeper dependency parsing would serve better find issue po tagging please advise deal counter intuitive po tagging using en core web lg model model best dependency parsing poorly formed sentence thank much
How to delineate or extract complex noun phrases that include verb phrases in their makeup using SpaCy?,"<p>I know about the <code>noun_chunks</code> that's inbuilt in SpaCy. However it is not exactly suitable for the position I found myself in.<br>
To give an example -   </p>

<pre><code>Skynet will be decommissioned soon.   
</code></pre>

<p>This is a very simple sentence where <code>noun_chunks</code> will output <em>Skynet</em>.<br>
Now lets replace <em>Skynet</em> with a <em>complex noun phrase</em>(for a lack of moniker to describe what I need) as follows -  </p>

<pre><code>The machine that kills life will be decommissioned soon.  
</code></pre>

<p>So here <code>Skynet</code> -> <code>The machine that kills life</code><br>
However <code>noun_chunks</code> will identify <code>The machine</code> and <code>life</code> as noun phrases which is correct but I need something to extract <code>The machine that kills life</code>  as one complex noun phrase.  </p>

<p>Using Dependency parse might help but I am not so sure how to go about doing this. Any direct help or a recommendation to read up on how to make use of dependency parse (more importantly the construction and grammatical relationships in English) so that I may solve this issue myself would be much appreciated.</p>
",Parsing & POS Tagging,delineate extract complex noun phrase include verb phrase makeup using spacy know inbuilt spacy however exactly suitable position found give example simple sentence output skynet let replace skynet complex noun phrase lack moniker describe need follows however identify noun phrase correct need something extract one complex noun phrase using dependency parse might help sure go direct help recommendation read make use dependency parse importantly construction grammatical relationship english may solve issue would much appreciated
"spacy noun-chunking creates unexpected lemma, pos, tag, and dep","<p>I am using spacy to parse documents and unfortunately I am unable to process noun chunks the way I would have expected them to be processed.  Below is my code:</p>

<pre><code># Import spacy
import spacy
nlp = spacy.load(""en_core_web_lg"")

# Add noun chunking to the pipeline
merge_noun_chunks = nlp.create_pipe(""merge_noun_chunks"")
nlp.add_pipe(merge_noun_chunks)

# Process the document
docs = nlp.pipe([""The big dogs chased the fast cat""])

# Print out the tokens
for doc in docs:
    for token in doc:
        print(""text: {}, lemma: {}, pos: {}, tag: {}, dep: {}"".format(tname, token.text, token.lemma_, token.pos_, token.tag_, token.dep_))
</code></pre>

<p>The output I get is as follows:</p>

<pre><code>text: The big dogs, lemma: the, pos: NOUN, tag: NNS, dep: nsubj
text: chased, lemma: chase, pos: VERB, tag: VBD, dep: ROOT
text: the fast cat, lemma: the, pos: NOUN, tag: NN, dep: dobj
</code></pre>

<p>The issue is in the first line of output, where ""the big dogs"" was parsed in an unexpected fashion:  It create a ""lemma"" of ""the"" and indicated that it is a ""pos"" of ""NOUN"", a ""tag"" of ""NNS"", and a ""dep"" of ""nsubj"".</p>

<p>The output I was hoping to get is as follows:</p>

<pre><code>text: The big dogs, lemma: the big dog, pos: NOUN, tag: NNS, dep: nsubj
text: chased, lemma: chase, pos: VERB, tag: VBD, dep: ROOT
text: the fast cat, lemma: the fast cat, pos: NOUN, tag: NN, dep: dobj
</code></pre>

<p>I expected a ""lemma"" would be the phrase ""the big dog"" with plural form changed to singular and the phrase would be ""pos"" of ""NOUN"", a ""tag"" of ""NNS"", and a ""dep"" of ""nsubj"".</p>

<p>Is this the correct behaviour, or am I using spacy incorrectly?  If I am using spacy incorrectly, please let me know the correct manner in which to perform this task.</p>
",Parsing & POS Tagging,spacy noun chunking creates unexpected lemma po tag dep using spacy parse document unfortunately unable process noun chunk way would expected processed code output get follows issue first line output big dog wa parsed unexpected fashion create lemma indicated po noun tag nns dep nsubj output wa hoping get follows expected lemma would phrase big dog plural form changed singular phrase would po noun tag nns dep nsubj correct behaviour using spacy incorrectly using spacy incorrectly please let know correct manner perform task
Chunking complex noun phrases in lists with spaCy and generating augmented list,"<p>So I'm trying to convert an ""abbreviated"" list of noun phrases describing occupations into a ""complete"" list of those noun phrases. It should look something like this:</p>

<p>Examples (input --> output):</p>

<ul>
<li>General and operations managers --> [general manager, operations manager]</li>
<li>Bus and truck mechanics and diesel engine specialists --> [bus mechanics, truck mechanics, diesel engine specialists]</li>
<li>Claims adjusters, appraisers, and examiners --> [claims adjusters, claims appraisers, claims examiners]</li>
<li>Heavy vehicle and mobile equipment service technicians and mechanics --> [heavy vehicle service technicians, heavy vehicle mechanics, mobile equipment service technicians, mobile equipment mechanics]</li>
</ul>

<p>I tried a heavy rule-based approach, but it wasn't to my liking.
I also tried simple chunking with spaCy, but the initial results are also not perfect.</p>

<p>The spaCy code looks like this:</p>

<pre><code>import spacy
nlp = spacy.load(""en_core_web_sm"")

def get_chunks(sentence):
    res = []

    doc = nlp(u'{}'.format(sentence))

    for np in doc.noun_chunks:
        res.append(np.text)

    return res
</code></pre>

<p>For the exact inputs as above, the code returns:</p>

<ul>
<li>['operations', 'managers']</li>
<li>['Bus and truck mechanics', 'diesel engine specialists']</li>
<li>['Claims adjusters', 'appraisers', 'examiners', 'investigators']</li>
<li>['Heavy vehicle', 'mobile equipment service technicians', 'mechanics']</li>
</ul>

<p>I'm considering going to a rule-based approach from here, but maybe someone has a better suggestion?</p>

<p>Thanks and cheers!</p>
",Parsing & POS Tagging,chunking complex noun phrase list spacy generating augmented list trying convert abbreviated list noun phrase describing occupation complete list noun phrase look something like example input output general operation manager general manager operation manager bus truck mechanic diesel engine specialist bus mechanic truck mechanic diesel engine specialist claim adjuster appraiser examiner claim adjuster claim appraiser claim examiner heavy vehicle mobile equipment service technician mechanic heavy vehicle service technician heavy vehicle mechanic mobile equipment service technician mobile equipment mechanic tried heavy rule based approach liking also tried simple chunking spacy initial result also perfect spacy code look like exact input code return operation manager bus truck mechanic diesel engine specialist claim adjuster appraiser examiner investigator heavy vehicle mobile equipment service technician mechanic considering going rule based approach maybe someone ha better suggestion thanks cheer
Extracting definitions and the text corresponding to the definition,"<p>I am trying to extract definitions and the text that goes along with that definition.</p>

<p>The definition extraction is relatively easy because they are typically in parentheses and in quotes, so I can run a Regular Expression to find them. </p>

<p>The part I am having trouble with is getting the text that goes along with the definition- it will typically show up before the definition but I don't know how many words will be part of it.</p>

<p>For example:</p>

<blockquote>
  <p>""(a) The Company has the requisite corporate power and authority to execute and deliver this Agreement, to perform its obligations hereunder and, subject to receipt of the Requisite Company Vote, to consummate the Transactions. The execution, delivery and performance by the Company of this Agreement and the consummation by the Company of the Transactions have been duly authorized by the Company Board and no other corporate action on the part of the Company is necessary to authorize the execution and delivery by the Company of this Agreement, the Plan of Merger and the consummation by it of the Transactions, in each case, subject only to the authorization and approval of this Agreement, the Plan of Merger and the Transactions by way of (i) a shareholders’ special resolution by the affirmative vote of holders of Shares representing at least two-thirds of the voting power of the Shares present and voting in person or by proxy as a single class at the Shareholders’ Meeting, (ii) a shareholders’ resolution by the affirmative vote of holders of Shares representing a majority of the aggregate voting power of the outstanding Shares of the Company and (iii) a shareholders’ resolution by the affirmative vote of holders of a majority of the total outstanding Class A Shares (collectively clauses (i), (ii) and (iii), the “ Requisite Company Vote ”), in each case, in accordance with Section 233(6) of the CICL and the Ninth Amended and Restated Articles of Association of the Company, adopted by special resolution on December 28, 2015 (the “ Company Articles ”). This Agreement has been duly and validly executed and delivered by the Company and, assuming the due authorization, execution and delivery by Parent and Merger Sub, constitutes a legal, valid and binding obligation of the Company, enforceable against the Company in accordance with its terms, subject to bankruptcy, insolvency, fraudulent transfer, reorganization, moratorium and similar Laws of general applicability relating to or affecting creditors’ rights and to general principles of equity (the “ Bankruptcy and Equity Exception ”).""</p>
</blockquote>

<p>has three definition we extract- ""<strong>Requisite Company Vote</strong>"", ""<strong>Company Articles</strong>"" and ""<strong>Bankruptcy and Equity Exception</strong>"". </p>

<p>""<strong>Requisite Company Vote</strong>"" looks like the definition should be ""collectively clauses (i), (ii) and (iii)"" but really should also probably add ""of subsection (a)"" or I should actually grab the text of clauses (i), (ii), and (iii)</p>

<p>""<strong>Company Articles""</strong> looks like it should be ""Ninth Amended and Restated Articles of Association of the Company, adopted by special resolution on December 28, 2015""</p>

<p>""<strong>Bankruptcy and Equity Exception</strong>"" looks like it should be ""bankruptcy, insolvency, fraudulent transfer, reorganization, moratorium and similar Laws of general applicability relating to or affecting creditors’ rights and to general principles of equity""</p>

<p>These are just three examples and I have to build something dynamic enough to be able to handle different types of definitions.</p>

<p>I started by building Noun Phrases using NLTK and a grammar but the NPs don't seem to be capturing everything I need even with testing various different grammars. I thought about using edit distance measurements- potentially by building the string before word by word and testing distance but all things I can think of seem to have their own flaws so I am looking for ideas.</p>
",Parsing & POS Tagging,extracting definition text corresponding definition trying extract definition text go along definition definition extraction relatively easy typically parenthesis quote run regular expression find part trouble getting text go along definition typically show definition know many word part example company ha requisite corporate power authority execute deliver agreement perform obligation hereunder subject receipt requisite company vote consummate transaction execution delivery performance company agreement consummation company transaction duly authorized company board corporate action part company necessary authorize execution delivery company agreement plan merger consummation transaction case subject authorization approval agreement plan merger transaction way shareholder special resolution affirmative vote holder share representing least two third voting power share present voting person proxy single class shareholder meeting ii shareholder resolution affirmative vote holder share representing majority aggregate voting power outstanding share company iii shareholder resolution affirmative vote holder majority total outstanding class share collectively clause ii iii requisite company vote case accordance section cicl ninth amended restated article association company adopted special resolution december company article agreement ha duly validly executed company assuming due authorization execution delivery parent merger sub constitutes legal valid binding obligation company enforceable company accordance term subject bankruptcy insolvency fraudulent transfer reorganization moratorium similar law general applicability relating affecting creditor right general principle equity bankruptcy equity exception ha three definition extract requisite company vote company article bankruptcy equity exception requisite company vote look like definition collectively clause ii iii really also probably add subsection actually grab text clause ii iii company article look like ninth amended restated article association company adopted special resolution december bankruptcy equity exception look like bankruptcy insolvency fraudulent transfer reorganization moratorium similar law general applicability relating affecting creditor right general principle equity three example build something dynamic enough able handle different type definition started building noun phrase using nltk grammar np seem capturing everything need even testing various different grammar thought using edit distance measurement potentially building string word word testing distance thing think seem flaw looking idea
Labelling words from the sentence they came from,"<p>I have a dataframe with 3 columns namely <strong>'word', 'pos-tag', 'label'</strong>. The words are originally from a text file.Now I would like to have another column 'sentences#' stating the index of sentences the words originally came from.</p>

<pre><code>Current state:-
WORD POS-Tag Label
my   PRP$     IR
name  NN      IR 
is   VBZ      IR
ron  VBN      PERSON
.     .
my   PRP$     IR
name NN       IR
is   VBZ      IR
harry VBN     Person
.      .      IR
Desired state:-
Sentence#  WORD    Pos-Tag  Label
 1          My       PRP      IR
 1          name     NN       IR
 1           is      VBZ      IR
 1           ron     VBN      Person
 1            .       .       IR
 2            My     PRP      IR
 2            name   NN       IR
 2             is    VBZ      IR
 2           harry   VBN      Person
 2              .     .       IR
</code></pre>

<p>code I used till now:-</p>

<pre><code>#necessary libraries
import pandas as pd
import numpy as np
import nltk 
import string
document=open(r'C:\Users\xyz\newfile.txt',encoding='utf8')
content=document.read()

sentences = nltk.sent_tokenize(content)
sentences = [nltk.word_tokenize(sent) for sent in sentences]
sentences = [nltk.pos_tag(sent) for sent in sentences]


flat_list=[]

# flattening a nested list
for x in sentences:
    for y in x:
        flat_list.append(y)

df = pd.DataFrame(flat_list, columns=['word','pos_tag']) 

#importing data to create the 'Label' column
data=pd.read_excel(r'C:\Users\xyz\pname.xlsx')
pname=list(set(data['Product']))

df['Label']=['drug' if x in fl else 'IR' for x in df['word']]
</code></pre>
",Parsing & POS Tagging,labelling word sentence came dataframe column namely word po tag label word originally text file would like another column sentence stating index sentence word originally came code used till
How to find similarity between 2 dependency trees using spaCy?,"<p>I am working on a question-answering task. I am planning to use dependency parsing to find candidate answers from a passage to a query. However, I am not sure how I can find similarity between dependency trees of the query and the sentences from the passage, respectively. Below is the reproducible code.</p>

<pre><code>import spacy
from spacy import displacy
nlp = spacy.load('en_core_web_sm')

doc1 = nlp('Wall Street Journal just published an interesting piece on crypto currencies')
doc2 = nlp('What did Wall Street Journal published')

displacy.render(doc1, style='dep', jupyter=True, options={'distance': 90})
displacy.render(doc2, style='dep', jupyter=True, options={'distance': 90})
</code></pre>
",Parsing & POS Tagging,find similarity dependency tree using spacy working question answering task planning use dependency parsing find candidate answer passage query however sure find similarity dependency tree query sentence passage respectively reproducible code
Using Illinois Chunker,"<p>I am trying to extract noun phrases from text using the <a href=""http://cogcomp.cs.illinois.edu/page/software_view/Chunker"" rel=""nofollow"">illinois chunker</a> however i seem to be getting something wrong. I am trying to run the command in a centos6.4 system with jdk1.8 having downloaded the jar given in the link.</p>

<pre><code>java -Xmx512m -classpath /path/to/LBJChunk.jar edu.illinois.cs.cogcomp.lbj.chunk.ChunksAndPOSTags snippeting.txt
</code></pre>

<p>this returns a class not found exception, what am i doing wrong >:( do i have to download an other jar for the parser?</p>

<pre><code>Exception in thread ""main"" java.lang.NoClassDefFoundError: LBJ2/parse/Parser
        at java.lang.Class.getDeclaredMethods0(Native Method)
        at java.lang.Class.privateGetDeclaredMethods(Class.java:2688)
        at java.lang.Class.getMethod0(Class.java:2937)
        at java.lang.Class.getMethod(Class.java:1771)
        at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)
        at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)
Caused by: java.lang.ClassNotFoundException: LBJ2.parse.Parser
        at java.net.URLClassLoader$1.run(URLClassLoader.java:372)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:360)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 6 more
</code></pre>
",Parsing & POS Tagging,using illinois chunker trying extract noun phrase text using illinois chunker however seem getting something wrong trying run command centos system jdk downloaded jar given link return class found exception wrong download jar parser
Consistuency vs Dependency Parsing with example,"<p>I would like to illustrate the difference between those two parser. Yet I am not sure if my representation is correct and would love to get some input:</p>

<p><a href=""https://i.sstatic.net/s3UPF.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/s3UPF.jpg"" alt=""enter image description here""></a> </p>
",Parsing & POS Tagging,consistuency v dependency parsing example would like illustrate difference two parser yet sure representation correct would love get input
Why are WDT words being marked as a sentence subject by dependency parsing?,"<p>I want to report the subject of each sentence; and also extract all its modifiers. (E.g. ""Donald Trump"" not just ""Trump""; ""(The) average remaining lease term"" not just ""term"".)</p>

<p>Here is my test code:</p>

<pre><code>import spacy

nlp = spacy.load('en_core_web_sm')

def handle(doc):
    for sent in doc.sents:
        shownSentence = False
        for token in sent:
            if(token.dep_==""nsubj""):
                if(not shownSentence):
                    print(""----------"")
                    print(sent)
                    shownSentence = True
                print(""{0}/{1}"".format(token.text, token.tag_))
                print([ [t,t.tag_] for t in token.children])

handle(nlp('Donald Trump, legend in his own lifetime, said: ""This transaction is a continuation of our main strategy to invest in assets which offer growth potential and that are coloured pink."" The average remaining lease term is six years, and Laura Palmer was killed by Bob. Trump added he will sell up soon.'))
</code></pre>

<p>The output is below. I'm wondering why I get ""which/WDT"" as a subject? Is it just model noise, or is it considered correct behaviour?  (Incidentally, in my real sentence, which had the same structure, I also got ""that/WDT"" being marked as a subject.) (<strong>UPDATE:</strong> If I switch to 'en_core_web_md' then I <em>do</em> get ""that/WDT"" for my Trump example; that is the only difference switching from the small to the medium model makes.)</p>

<p>I can easily filter them out by looking at <code>tag_</code>; I'm more interested in the underlying reason.</p>

<p>(<strong>UPDATE:</strong> Incidentally, ""Laura Palmer"" doesn't get pulled out as a subject by this code, as the <code>dep_</code> value is ""nsubjpass"", not ""nsubj"".)</p>

<pre><code>----------
Donald Trump, legend in his own lifetime, said: ""This transaction is a continuation of our main strategy to invest in assets which offer growth potential and that are coloured pink.""
Trump/NNP
[[Donald, 'NNP'], [,, ','], [legend, 'NN'], [,, ',']]
transaction/NN
[[This, 'DT']]
which/WDT
[]
----------
The average remaining lease term is six years, and Laura Palmer was killed by Bob.
term/NN
[[The, 'DT'], [average, 'JJ'], [remaining, 'JJ'], [lease, 'NN']]
----------
Trump added he will sell up soon.
Trump/NNP
[]
he/PRP
[]
</code></pre>

<p>(By the way, the bigger picture: pronoun resolution. I want to turn PRPs into the text they refer to.)</p>
",Parsing & POS Tagging,wdt word marked sentence subject dependency parsing want report subject sentence also extract modifier e g donald trump trump average remaining lease term term test code output wondering get wdt subject model noise considered correct behaviour incidentally real sentence structure also got wdt marked subject update switch en core web md get wdt trump example difference switching small medium model make easily filter looking interested underlying reason update incidentally laura palmer get pulled subject code value nsubjpass nsubj way bigger picture pronoun resolution want turn prps text refer
How do I create a search using NLP techniques which searches an inputted named entity as well as any potential name variations it may have?,"<p>I’m currently using TextBlob to make a chatbot, and I’ve so far been extracting named entities using noun phrase extraction and finding the pos tag NNP. When entering a test user question such as ‘Will Smith’s latest single?’, I am correctly retrieving ‘Will Smith’. But I want to be able to search not only ‘will smith’ but ‘william smith’ ‘bill smith’ ‘willie smith’ ‘billy smith’ - basically other popularly known variations of the name in English language. I am using the Spotipy API as I am trying to retrieve Spotify artists. What I'm currently doing in PyCharm:</p>

<pre><code>while True:
    response = input()
    searchQuery = TextBlob(response)
    who = []
    for item, tag in searchQuery.tags:
        if tag == ""NNP"":
            for nounPhrase in searchQuery.noun_phrases:
                np = TextBlob(nounPhrase)
                if item.lower() in np.words:
                    if nounPhrase not in who:
                        who.append(nounPhrase)

    print(who)
        if who:
            for name in who:
                if spotifyObject.search(name, 50, 0, 'artist', None):
                    searchResults = spotifyObject.search(name, 50, 0, 'artist', None)
                    artists = searchResults['artists']['items']
                    for a in artists:
                        print(a['name'])
</code></pre>
",Parsing & POS Tagging,create search using nlp technique search inputted named entity well potential name variation may currently using textblob make chatbot far extracting named entity using noun phrase extraction finding po tag nnp entering test user question smith latest single correctly retrieving smith want able search smith william smith bill smith willie smith billy smith basically popularly known variation name english language using spotipy api trying retrieve spotify artist currently pycharm
How to handle selection sets of grammars that have multiple derivation trees,"<p>I need to write a parse table for a grammar that has lambda expressions and multiple derivation trees. I am having trouble finding examples of parse tables for grammars with lambda expressions. How should I start? </p>

<p>My attempt at the selection sets are as follows: </p>

<p>S-> ABCe = {e,b,c,d}. </p>

<p>(A->bB = {b}, A->lambda, B->cC ={c}, A->lambda, B->lambda, C->d = {d}, A->lambda, B-> lambda, C goes to lambda = {e}). </p>

<p>A-> bB = {b} </p>

<p>A-> lambda = {c,d,e} </p>

<p>B-> cC = {c}</p>

<p>B-> lambda = {d,e}</p>

<p>C-> d = {d} </p>

<p>C-> lambda = {d}</p>

<p>There are two problems I am having: </p>

<p>1) I don't know what to write to define lambda in the parse table or the actual parse code. </p>

<p>2) If lambda expressions are dependent on what follows in the string, then the current token would determine what is popped off, right? For example, if S goes to ABCe, and the current token is b, then would I push(e) and push(C)? I also realized something else for the lambdas just now. The selection sets for B are mutually inclusive. So, for instance, only if the current token at A is b would I push 'B,' if the current token is c,d, or e, I would just pop(). I'm really not sure how to go about writing it, this is just my thought process. But is it allowed to use the selection set in place of the actual grammar rule?</p>
",Parsing & POS Tagging,handle selection set grammar multiple derivation tree need write parse table grammar ha lambda expression multiple derivation tree trouble finding example parse table grammar lambda expression start attempt selection set follows abce e b c bb b lambda b cc c lambda b lambda c lambda b lambda c go lambda e bb b lambda c e b cc c b lambda e c c lambda two problem know write define lambda parse table actual parse code lambda expression dependent follows string current token would determine popped right example go abce current token b would push e push c also realized something else lambda selection set b mutually inclusive instance current token b would push b current token c e would pop really sure go writing thought process allowed use selection set place actual grammar rule
Using RDF to model normal sentences,"<p>I'm trying to somehow store everyday sentences -- or rather the information expressed by the sentences -- in a (semi-)structured manner. Right now, I'm exploring the feasibility of RDF for that. I'm not familiar with RDF enough to assess if this is suitable way to go. While I'm sure that there will be some form of information loss, I cannot say it it would be acceptable for practicable purposes.</p>

<p>Obviously, sentences like ""Bob ate the cake"" can be directly mapped into subject-predicate-object triple (although I'm don't know how to properly address the tense of the predicate):</p>

<pre><code>:Bob :ate :Cake .
</code></pre>

<p>I then came across reification that converts a single triple as 4-triple graph representing a statement with a subject, predicate and object node. This allows to reflect statements over statements, e.g., ""Alice thinks that Bob ate the cake."":</p>

<pre><code>_:stmt1 rdf:type rdf:Statement .
_:stmt1 rdf:subject :Bob .
_:stmt1 rdf:predicate :ate .
_:stmt1 rdf:object :Cake .
:Alice :thinks _:stmt1 .
</code></pre>

<p>So far so good. However, everyday language has so many more constructs. Here are just some of the more obvious (to me):</p>

<ul>
<li><p>Conditions: ""If Alice tells the truth, Bob ate the cake.""</p></li>
<li><p>Auxiliary verbs: ""Alice can bake cakes.""</p></li>
<li><p>Clausal complements: ""Alice likes to eat cake.""</p></li>
<li><p>Negation: Given the Open World Assumption, the absence of a triple <code>:Bob :ate :Cake .</code> is ambigious. In many practical settings I would argue that making negation explicit is important.</p></li>
<li><p>Disjunction: ""(Either) Alice or Bob ate the cake."" Most people do not reliably distinguish between OR and XOR in everyday language.</p></li>
<li><p>Quantifier: ""Most people like cake."", ""Very few people hate cake.""</p></li>
</ul>

<p>I assume some can be realized in a relatively straightforward manner. For example, ""Alice thinks that Bob did not eat the cake"" could be represented as</p>

<pre><code>_:stmt1 rdf:type rdf:Statement .
_:stmt1 rdf:subject :Bob .
_:stmt1 rdf:predicate _:predA .
_:predA :term :eat
_:predA :tense :past
_:predA :negated :true
_:stmt1 rdf:object :Cake .
:Alice :thinks _:stmt1 .
</code></pre>

<p>This might also allow to express simple adverbs (e.g. ""Bob quickly ate the cake"") by having a triple such as</p>

<pre><code>_:predA :advmod :quickly
</code></pre>

<p>On the other hand, reification alone quickly increases the number of required triples. I guess you can push it to the extreme and consider output of an dependency parser as a set of triples, e.g.:</p>

<pre><code>nsubj(thinks, Alice)
ccomp(thinks, ate)
nsubj(ate, Bob)
dobj(ate, cake)
...
</code></pre>

<p>but I cannot see this truly useful when it comes to query the graph or infer from it.</p>

<p>I read a couple of scientific papers that focus on converting text into RDF, but most of them focus the extraction of simple facts. Apart from that, I couldn't find any good resources on how useful/expressive/practical RDF is to represent knowledge beyond simple factoids.</p>
",Parsing & POS Tagging,using rdf model normal sentence trying somehow store everyday sentence rather information expressed sentence semi structured manner right exploring rdf familiar rdf enough ass suitable way go sure form information loss say would acceptable practicable purpose obviously sentence like bob ate cake directly mapped subject predicate object triple although know properly address tense predicate came across reification convert single triple triple graph representing statement subject predicate object node allows reflect statement statement e g alice think bob ate cake far good however everyday language ha many construct obvious condition alice tell truth bob ate cake auxiliary verb alice bake cake clausal complement alice like eat cake negation given open world assumption absence triple ambigious many practical setting would argue making negation explicit important disjunction either alice bob ate cake people reliably distinguish xor everyday language quantifier people like cake people hate cake assume realized relatively straightforward manner example alice think bob eat cake could represented might also allow express simple adverb e g bob quickly ate cake triple hand reification alone quickly increase number required triple guess push extreme consider output dependency parser set triple e g see truly useful come query graph infer read couple scientific paper focus converting text rdf focus extraction simple fact apart find good resource useful expressive practical rdf represent knowledge beyond simple factoid
How to find similar noun phrases in NLP?,"<p>Is there a way to identify similar noun phrases. Some suggest use pattern-based approaches, for example <code>X as Y</code> expressions:</p>
<blockquote>
<p>Usain Bolt as Sprint King</p>
<p>Liverpool as Reds</p>
</blockquote>
",Parsing & POS Tagging,find similar noun phrase nlp way identify similar noun phrase suggest use pattern based approach example expression usain bolt sprint king liverpool red
How to find the path of a node in parse tree,"<p>I used Stanford parser and got a parse tree for one sentence. How can I get the path of every node?</p>

<p>For example, here is my parser tree: </p>

<p>(ROOT</p>

<p>(S</p>

<pre><code>(NP (PRP He))
(VP (VBD entered)
  (NP (DT the) (NN door)))
(. .)))
</code></pre>

<p>When I input VP, the program gives me the path of VP node: ROOT->S->VP;
When I input PRP, the program gives me the path of PRP node: ROOT->S->NP->PRP.</p>

<p>I have no ideas about this issue. Do need your help! Thanks in advance!</p>
",Parsing & POS Tagging,find path node parse tree used stanford parser got parse tree one sentence get path every node example parser tree root input vp program give path vp node root vp input prp program give path prp node root np prp idea issue need help thanks advance
Parser trees comparison: Common sublist in two lists,"<p>My goal is to identify whether two sentences is duplicated.</p>

<p>I'm trying to compare the parser trees of the two sentences.
I have extract the tags from the parser trees in the following format</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>['ROOT', 'SBARQ', 'WHADVP', 'WRB', 'SQ', 'VP', 'VBP', 'ADJP', 'RB', 'JJ', 'NP', 'NNP', 'NP', 'NP', 'NNS', 'VP', 'VBG', 'NP', 'NP', 'NNS', 'SBAR', 'WHNP', 'WDT', 'S', 'VP', 'VBP', 'ADVP', 'RB', 'VP', 'VBN', 'PP', 'IN', 'NP', 'NNP', '.']
['ROOT', 'SBARQ', 'WHADVP', 'WRB', 'SQ', 'VBP', 'NP', 'NNS', 'VP', 'VB', 'NP', 'NP', 'NNP', 'NNS', 'SBAR', 'WHNP', 'WDT', 'S', 'VP', 'MD', 'VP', 'VB', 'VP', 'VBN', 'ADVP', 'RB', 'PP', 'IN', 'NP', 'NNP', '.']</code></pre>
</div>
</div>
</p>

<p>I want to get the length of common sublists of the two lists. In the above case, the results would be 4('ROOT', 'SBARQ', 'WHADVP', 'WRB')+5('SBAR', 'WHNP', 'WDT', 'S', 'VP')+2('ADVP', 'RB')+5('PP', 'IN', 'NP', 'NNP', '.').</p>

<p>Or do you have any other solutions can make use of the parse tree for the similarity of two sentences.
One more issue is, what is the fastest way to get the parse tree? Since I have more than 300,000 sentence pairs to compare...</p>

<p>Thanks in advance!</p>
",Parsing & POS Tagging,parser tree comparison common sublist two list goal identify whether two sentence duplicated trying compare parser tree two sentence extract tag parser tree following format want get length common sublists two list case result would root sbarq whadvp sbar whnp wdt vp advp rb pp np nnp solution make use parse tree similarity two sentence one issue fastest way get parse tree since sentence pair compare thanks advance
Examples where Dependency Parser fails,"<p>Can anyone give me few sentences on when the dependency parser fails and why they failed and what is the fix for it?</p>
",Parsing & POS Tagging,example dependency parser fails anyone give sentence dependency parser fails failed fix
POS tagger in python without NLTK,"<p>I am trying to make a POS tagger for determiners and prepositions of Sorani Kurdish. I am using the following code to put every tag after each proposition or determiner in my Kurdish text. </p>

<pre><code>import os
SOR = open(""SOR-1.txt"", ""r+"", encoding = 'utf-8')
old_text = SOR.read()
punkt = [""."", ""!"", "","", "":"", "";""]
text = """"
for i in old_text:
    if i in punkt:
        text+="" ""+i
    else:
        text += i

d = {""DET"":[""ئێمە"" , ""ئێوە"" , ""ئەم"" , ""ئەو"" , ""ئەوان"" , ""ئەوەی"", ""چەند"" ], ""PREP"":[""بۆ"",""بێ"",""بێجگە"",""بە"",""بەبێ"",""بەدەم"",""بەردەم"",""بەرلە"",""بەرەوی"",""بەرەوە"",""بەلای"",""بەپێی"",""تۆ"",""تێ"",""جگە"",""دوای"",""دەگەڵ"",""سەر"",""لێ"",""لە"",""لەبابەت"",""لەباتی"",""لەبارەی"",""لەبرێتی"",""لەبن"",""لەبەینی"",""لەبەر"",""لەدەم"",""لەرێ"",""لەرێگا"",""لەرەوی"",""لەسەر"",""لەلایەن"",""لەناو"",""لەنێو"",""لەو"",""لەپێناوی"",""لەژێر"",""لەگەڵ"",""ناو"",""نێوان"",""وەک"",""وەک"",""پاش"",""پێش"","""" ], ""punkt"":[""."", "","", ""!""]}

text = text.split()
for w in text:
    for pos in d:
        if w in d[pos]:
            SOR.write(w+""/""+pos+"" "")
SOR.close()
</code></pre>

<p>What I want to do is to add POS tags inside the text after each of the words in the defined dictionary, but the result is a separate list of words and POS tags at the end of the file.</p>
",Parsing & POS Tagging,po tagger python without nltk trying make po tagger determiner preposition sorani kurdish using following code put every tag proposition determiner kurdish text want add po tag inside text word defined dictionary result separate list word po tag end file
Applying Tfidfvectorizer on list of pos tags gives ValueError,"<p>After preprocessing, I have list of pos tags in a pandas column as below. I want to vectorize these tags and generate a matrix using Tfidfvectorizer or any other vectorizer. </p>

<blockquote>
  <p>dataset['text_posTagged']</p>
</blockquote>

<pre><code>['VBP', 'JJ', 'NNS', 'VBP', 'JJ', 'IN', 'PRP', 'VBP', 'TO', 'VB', 'PRP', 'RB', 'VBZ', 'DT', 'JJ', 'PRP$', 'NN', 'NN', 'NN', 'NN', 'VBZ', 'JJ']
['UH', 'DT', 'VB', 'VB', 'PRP$', 'NN', 'TO', 'JJ', 'IN', 'PRP', 'MD', 'VB', 'DT', 'VBZ', 'DT', 'NN', 'NN']
['NN', 'VBD', 'NN', 'NN', 'NN', 'DT', 'IN', 'IN', 'NN', 'IN', 'NN', 'NN', 'VBD', 'IN', 'JJ', 'NN', 'NN']
</code></pre>

<p>Applying Tfidfvectorizer gives <strong>ValueError: np.nan is an invalid document, expected byte or unicode string.</strong></p>

<pre><code> tfidf = TfidfVectorizer(tokenizer=identity_tokenizer, stop_words='english', lowercase=False)
 pos_tag_response = tfidf.fit_transform(dataset['text_posTagged'])
 pos_tag_matrix = pd.DataFrame(pos_tag_response.todense(), columns=tfidf.get_feature_names())
</code></pre>

<p>However, Applying Tfidfvectorizer  on the following column that has list of words works properly. </p>

<pre><code>['are', 'red', 'violets', 'are', 'blue', 'if', 'you', 'want', 'to', 'buy', 'us', 'here', 'is', 'a', 'clue', 'our', 'eye', 'amp', 'cheek', 'palette', 'is', 'al']
['is', 'it', 'too', 'late', 'now', 'to', 'say', 'sorry']
['our', 'amazonian', 'clay', 'full', 'coverage', 'foundation', 'comes', 'in', '40', 'shades', 'of', 'creamy', 'goodness']
</code></pre>

<p><strong>How can I vectorize the column with pos tags??</strong></p>
",Parsing & POS Tagging,applying tfidfvectorizer list po tag give valueerror preprocessing list po tag panda column want vectorize tag generate matrix using tfidfvectorizer vectorizer dataset text postagged applying tfidfvectorizer give valueerror np nan invalid document expected byte unicode string however applying tfidfvectorizer following column ha list word work properly vectorize column po tag
Applying literal_eval on string of lists of POS tags gives ValueError,"<p>In a pandas column I have list of POS tags as string. I thought this must be string because <code>print(dataset['text_posTagged'][0][0])</code> prints <code>[</code>. </p>

<blockquote>
  <p>dataset['text_posTagged']</p>
</blockquote>

<pre><code>['VBP', 'JJ', 'NNS', 'VBP', 'JJ', 'IN', 'PRP', 'VBP', 'TO', 'VB', 'PRP', 'RB', 'VBZ', 'DT', 'JJ', 'PRP$', 'NN', 'NN', 'NN', 'NN', 'VBZ', 'JJ']
['UH', 'DT', 'VB', 'VB', 'PRP$', 'NN', 'TO', 'JJ', 'IN', 'PRP', 'MD', 'VB', 'DT', 'VBZ', 'DT', 'NN', 'NN']
['NN', 'VBD', 'NN', 'NN', 'NN', 'DT', 'IN', 'IN', 'NN', 'IN', 'NN', 'NN', 'VBD', 'IN', 'JJ', 'NN', 'NN']
</code></pre>

<p>To convert this to an actual list I used the following. </p>

<pre><code>dataset['text_posTagged'] = dataset.text_posTagged.apply(lambda x: literal_eval(x)). 
</code></pre>

<p>However, this gives <strong>ValueError: malformed node or string: nan</strong></p>

<p>When I applied the same in a column that has list of words, it works fine. </p>

<blockquote>
  <p>dataset['text']</p>
</blockquote>

<pre><code>['are', 'red', 'violets', 'are', 'blue', 'if', 'you', 'want', 'to', 'buy', 'us', 'here', 'is', 'a', 'clue', 'our', 'eye', 'amp', 'cheek', 'palette', 'is', 'al']
['is', 'it', 'too', 'late', 'now', 'to', 'say', 'sorry']
['our', 'amazonian', 'clay', 'full', 'coverage', 'foundation', 'comes', 'in', '40', 'shades', 'of', 'creamy', 'goodness']
</code></pre>

<p>The following prints <code>are</code></p>

<pre><code>dataset['text'] = dataset.text.apply(lambda x: literal_eval(x)).
print(dataset['text'][0][0])
</code></pre>

<p><strong>What is wrong with applying literal_eval on list of POS tags?</strong> How to do it properly?</p>
",Parsing & POS Tagging,applying literal eval string list po tag give valueerror panda column list po tag string thought must string print dataset text postagged convert actual list used following however give valueerror malformed node string nan applied column ha list word work fine dataset text following print wrong applying literal eval list po tag properly
"How to pick out a the subject, predicate, and object and adjectives in a sentence","<p>I want to extract the subject, predicate, and object of a sentence and find out which adjectives go to the subject, predicate, or object with Stanford CoreNLP in java code.</p>

<p>I have tried to use the dependency parser to solve this by finding the dependency index, checking the dependency tag if it equals amod, then adding it to an ArrayList, but with this method sometimes the adjective's dependency tag is not amod and is nmod, and other tags may come up.</p>

<p>With determining the object and predicate, I have used a similar method as above. I have checked if it is det, and if it is any other tags that mean it is a predicate or object. However, sometimes different tags come up and it is not efficient to have to parse every tag that somewhat means it is a predicate pointing to the object.</p>

<p>So my question is, how to I get the subject, predicate, and object of a sentence and each's adjectives but not need to check each tag?</p>

<p>For the above mentioned attempts, I have used Stanford CoreNLP Simple API, but I am OK with the standard API if it is truly needed.</p>
",Parsing & POS Tagging,pick subject predicate object adjective sentence want extract subject predicate object sentence find adjective go subject predicate object stanford corenlp java code tried use dependency parser solve finding dependency index checking dependency tag equal amod adding arraylist method sometimes adjective dependency tag amod nmod tag may come determining object predicate used similar method checked det tag mean predicate object however sometimes different tag come efficient parse every tag somewhat mean predicate pointing object question get subject predicate object sentence adjective need check tag mentioned attempt used stanford corenlp simple api ok standard api truly needed
Data Parallelism in Python,"<p>I have written a parser that parses the sentences in a large corpus (900,000 paragraphs) into constituency parse trees. The parser works by converting each paragraph into a list of constituency parse trees. Here is a general idea of the code: </p>

<pre><code>paragraphs = load_dataset()
trees = list(map(parse_trees, paragraphs))
</code></pre>

<p>I have tried using python's <code>multiprocessing</code> and used the <code>pool</code> feature:</p>

<pre><code>paragraphs = load_dataset()
p = multiprocessing.Pool(4)
trees = p.map(parse_trees, paragraphs)
</code></pre>

<p>What I noticed is that using <code>multiprocess</code> makes the task slower than not using it.</p>

<p>Upon further research, I realized that there is a difference between task parallelism and data parallelism, and I think that in my situation, data parallelism is more suitable.</p>

<p>I cannot quite find any methods in python to do this however, and was wondering if anybody could point me in the right direction.</p>
",Parsing & POS Tagging,data parallelism python written parser par sentence large corpus paragraph constituency parse tree parser work converting paragraph list constituency parse tree general idea code tried using python used feature noticed using make task slower using upon research realized difference task parallelism data parallelism think situation data parallelism suitable quite find method python however wa wondering anybody could point right direction
Randomly generated Parse Tree using a fix set of vocabulary,"<p>I am using Python 3.2 and I have tried to build a randomly generated parse tree for a sentence.Though I am sure it generates the sentence, I am not sure how random the parse tree is and also, I do not know if there is a better/more efficient way to improve this code.
(I am new to programming and Python as such and I have recently been interested in NLP. Any advice, solution or corrections are welcome.)</p>

<pre><code> N=['man','dog','cat','telescope','park']  #noun
 P=['in','on','by','with']   #preposition
 det=['a','an','the','my']   #determinant
 V=['saw','ate','walked']    #verb
NP=['John','Mary','Bob']    #noun phrase


from random import choice
 PP=choice(NP)+' '+choice(P)   #preposition phrase
 PP=''.join(PP)
 VP=''.join(choice(V)+' '+choice(NP)) or''.join(choice(V)+' '.choice(NP)+(PP)) #verb phrase         
 VP=''.join(VP) #verb phrase 
 S=choice(NP)+' '+VP  #sentence
 print(S)
</code></pre>
",Parsing & POS Tagging,randomly generated parse tree using fix set vocabulary using python tried build randomly generated parse tree sentence though sure generates sentence sure random parse tree also know better efficient way improve code new programming python recently interested nlp advice solution correction welcome
python nltk keyword extraction from sentence,"<blockquote>
  <p>""First thing we do, let's kill all the lawyers."" - <em>William Shakespeare</em></p>
</blockquote>

<p>Given the quote above, I would like to pull out <code>""kill""</code> and <code>""lawyers""</code> as the two prominent keywords to describe the overall meaning of the sentence. I have extracted the following noun/verb POS tags:</p>

<pre><code>[[""First"", ""NNP""], [""thing"", ""NN""], [""do"", ""VBP""], [""lets"", ""NNS""], [""kill"", ""VB""], [""lawyers"", ""NNS""]]
</code></pre>

<p>The more general problem I am trying to solve is to distill a sentence to the ""most important""* words/tags to summarise the overall ""meaning""* of a sentence.</p>

<p>*note the scare quotes. I acknowledge this is a very hard problem and there is most likely no perfect solution at this point in time. Nonetheless, I am interested to see attempts at solving the specific problem (extracting <code>""kill""</code> and <code>""lawyers""</code>) and the general problem (summarising the overall meaning of a sentence in keywords/tags)</p>
",Parsing & POS Tagging,python nltk keyword extraction sentence first thing let kill lawyer william shakespeare given quote would like pull two prominent keywords describe overall meaning sentence extracted following noun verb po tag general problem trying solve distill sentence important word tag summarise overall meaning sentence note scare quote acknowledge hard problem likely perfect solution point time nonetheless interested see attempt solving specific problem extracting general problem summarising overall meaning sentence keywords tag
Parsing a sentence as many different ways as possible with shif-reduce parser in NLTK?,"<p>I am trying to finish a question and it asks me to parse a sentence using the shift-reduce parser and it's default grammar. It asks me to parse a sentence as many different ways as possible and asks for the number of different trees.</p>

<p>Can anyone know what does that mean with an example if possible please? I thought there was only one tree that would satisfy the sentence?</p>
",Parsing & POS Tagging,parsing sentence many different way possible shif reduce parser nltk trying finish question asks parse sentence using shift reduce parser default grammar asks parse sentence many different way possible asks number different tree anyone know doe mean example possible please thought wa one tree would satisfy sentence
Extracting/Parsing Pronoun-Pronoun and Verb-Noun/Pronoun Combinations from a Sentence,"<p><strong>Problem:</strong><br>
I am trying to extract a list of proper nouns from a job description, such as the following.</p>

<pre><code>text = ""Civil, Mechanical, and Industrial Engineering majors are preferred.""
</code></pre>

<p>I want to extract the following from this text:</p>

<pre><code>Civil Engineering
Mechanical Engineering
Industrial Engineering
</code></pre>

<p>This is one case of the problem, so use of application-specific information will not work. For instance, I cannot have a list of majors and then try to check if parts of the names of those majors are in the sentence along with the word ""major"" since I need this for other sentences as well.</p>

<p><strong>Attempts</strong>:<br>
1. I have looked into <strong>spacy</strong> <a href=""https://spacy.io/usage/linguistic-features#section-dependency-parse"" rel=""nofollow noreferrer"">dependency-parsing</a>, but parent-child relationships do not show up between each Engineering type (Civil,Mechanical,Industrial) and the word Engineering.<br></p>

<pre><code>import spacy

nlp = spacy.load('en_core_web_sm')
doc = nlp(u""Civil, Mechanical, and Industrial Engineering majors are preferred."")

print( ""%-15s%-15s%-15s%-15s%-30s"" % ( ""TEXT"",""DEP"",""HEAD TEXT"",""HEAD POS"",""CHILDREN"" ) )
for token in doc:
    if not token.text in ( ',','.' ):
        print( ""%-15s%-15s%-15s%-15s%-30s"" % 
          ( 
              token.text 
              ,token.dep_
              ,token.head.text
              ,token.head.pos_
              ,','.join( str(c) for c in token.children )
          ) )
</code></pre>

<p>...outputting...</p>

<pre>
TEXT           DEP            HEAD TEXT      HEAD POS       CHILDREN                      
Civil          amod           majors         NOUN           ,,Mechanical                  
Mechanical     conj           Civil          ADJ            ,,and                         
and            cc             Mechanical     PROPN                                        
Industrial     compound       Engineering    PROPN                                        
Engineering    compound       majors         NOUN           Industrial                    
majors         nsubjpass      preferred      VERB           Civil,Engineering             
are            auxpass        preferred      VERB                                         
preferred      ROOT           preferred      VERB           majors,are,.                  
</pre>

<ol start=""2"">
<li><p>I have also tried using nltk pos tagging, but I get the following...</p>

<p>import nltk
nltk.pos_tag( nltk.word_tokenize( 'Civil, Mechanical, and Industrial Engineering majors are preferred.' ) )</p></li>
</ol>

<pre>
[('Civil', 'NNP'),
 (',', ','),
 ('Mechanical', 'NNP'),
 (',', ','),
 ('and', 'CC'),
 ('Industrial', 'NNP'),
 ('Engineering', 'NNP'),
 ('majors', 'NNS'),
 ('are', 'VBP'),
 ('preferred', 'VBN'),
 ('.', '.')]
</pre>

<p>The types of engineering and the word Engineering all come up as NNP (proper nouns), so any kind of <a href=""https://www.nltk.org/api/nltk.chunk.html"" rel=""nofollow noreferrer"">RegexpParser</a> pattern I can think of does not work.</p>

<p><strong>Question:</strong><br>
Does anyone know of a way - in Python 3 - to extract these noun phrase pairings?</p>

<p><strong>EDIT:</strong> Addition Examples<br></p>

<p>The following examples are similar to the first example, except these are verb-noun / verb-propernoun versions.</p>

<pre>
text=""Experience with testing and automating API’s/GUI’s for desktop and native iOS/Android""

Extract:

testing API’s/GUI’s
automation API’s/GUI’s
</pre>

<pre>
text=""Design, build, test, deploy and maintain effective test automation solutions""

Extract:

Design test automation solutions
build test automation solutions
test test automation solutions
deploy test automation solutions
maintain test automation solutions
</pre>
",Parsing & POS Tagging,extracting parsing pronoun pronoun verb noun pronoun combination sentence problem trying extract list proper noun job description following want extract following text one case problem use application specific information work instance list major try check part name major sentence along word major since need sentence well attempt looked spacy dependency parsing parent child relationship show engineering type civil mechanical industrial word engineering outputting text dep head text head po child civil amod major noun mechanical mechanical conj civil adj cc mechanical propn industrial compound engineering propn engineering compound major noun industrial major nsubjpass preferred verb civil engineering auxpass preferred verb preferred root preferred verb major also tried using nltk po tagging get following import nltk nltk po tag nltk word tokenize civil mechanical industrial engineering major preferred civil nnp mechanical nnp cc industrial nnp engineering nnp major nns vbp preferred vbn type engineering word engineering come nnp proper noun kind regexpparser pattern think doe work question doe anyone know way python extract noun phrase pairing edit addition example following example similar first example except verb noun verb propernoun version text experience testing automating api gui desktop native io android extract testing api gui automation api gui text design build test deploy maintain effective test automation solution extract design test automation solution build test automation solution test test automation solution deploy test automation solution maintain test automation solution
Print None while comparing a list with the dataset in Python?,"<p>I have a dataset that includes noun, pronoun, proper noun and more. Here is a sample of the dataset:</p>

<pre><code>در
همین
حال
&lt;coref coref_coref_class=""set_0"" coref_mentiontype=""ne"" markable_scheme=""coref"" coref_coreftype=""ident""&gt;
نجیب
الله
خواجه
عمری
&lt;/coref&gt;
&lt;coref coref_coref_class=""set_0"" coref_mentiontype=""np"" markable_scheme=""coref"" coref_coreftype=""ident""&gt;
سرپرست
وزارت
تحصیلات
عالی
افغانستان
&lt;/coref&gt;
گفت
که
</code></pre>

<p>Now I want to check if a noun phrase is a pronoun. I compared the noun phrase with the possible list of pronouns in Dari. This feature should extract for both i_NP and j_NP. But I receive None as output. Here what I have tried.</p>

<pre><code>PRONOUNS = [""او"", ""ما"",""تو"", ""شما"", ""وی"", ""ایشان"",
""آنان"", ""آنها"", ""خود"", ""خویشتن"", ""خویش"", ""این"",
""آن"", ""اینان"", ""اینها""]

def isPronoun_feature(text):
    coref = re.findall(r'&lt;coref.*?&gt;(.*?)&lt;/coref&gt;', text, re.S)
    l = list(map(lambda x: x.replace('\n', ' '), coref))
    for i in range(0,len(l)-1,2):
        i_NP =l[i]
        j_NP = l[i+1]
        #print(i_NP, j_NP)
        if i_NP in PRONOUNS:
            return True
        elif j_NP in PRONOUNS:
            return True
</code></pre>
",Parsing & POS Tagging,print none comparing list dataset python dataset includes noun pronoun proper noun sample dataset want check noun phrase pronoun compared noun phrase possible list pronoun dari feature extract np j np receive none output tried
How to access the text inside these tags in python?,"<p>I have a dataset where I tagged the noun phrases. How to find these tags and extract the data from inside the tag. </p>

<pre><code>در
همین
حال
&lt;coref coref_coref_class=""set_0"" coref_mentiontype=""ne"" markable_scheme=""coref"" coref_coreftype=""ident""&gt;
نجیب
الله
خواجه
عمری
&lt;/coref&gt;
&lt;coref coref_coref_class=""set_0"" coref_mentiontype=""np"" markable_scheme=""coref"" coref_coreftype=""ident""&gt;
سرپرست
وزارت
تحصیلات
عالی
افغانستان
&lt;/coref&gt;
گفت
که


 def ex_feature(text):
    for w in text:
        if w.startswith(""&lt;coref"") and w.endswith(""&lt;/coref&gt;""):
            print(w)
</code></pre>
",Parsing & POS Tagging,access text inside tag python dataset tagged noun phrase find tag extract data inside tag
Querying part-of-speech tags with Lucene 7 OpenNLP,"<p>For fun and learning I am trying to build a part-of-speech (POS) tagger with OpenNLP and Lucene 7.4. The goal would be that once indexed I can actually search for a sequence of POS tags and find all sentences that match sequence. I already get the indexing part, but I am stuck on the query part. I am aware that SolR might have some functionality for this, and I already checked the code (which was not so self-expalantory after all). But my goal is to understand and implement in Lucene 7, not in SolR, as I want to be independent of any search engine on top.</p>

<p><strong>Idea</strong>
Input sentence 1: <em>The quick brown fox jumped over the lazy dogs.</em>
Applied Lucene OpenNLP tokenizer results in: [The][quick][brown][fox][jumped][over][the][lazy][dogs][.]
Next, applying Lucene OpenNLP POS tagging results in: [DT][JJ][JJ][NN][VBD][IN][DT][JJ][NNS][.]</p>

<p>Input sentence 2: <em>Give it to me, baby!</em>
Applied Lucene OpenNLP tokenizer results in: [Give][it][to][me][,][baby][!]
Next, applying Lucene OpenNLP POS tagging results in: [VB][PRP][TO][PRP][,][UH][.]</p>

<p>Query: <em>JJ NN VBD</em> matches part of sentence 1, so sentence 1 should be returned. (At this point I am only interested in exact matches, i.e. let's leave aside partial matches, wildcards etc.)</p>

<p><strong>Indexing</strong>
First, I created my own class com.example.OpenNLPAnalyzer:</p>

<pre><code>public class OpenNLPAnalyzer extends Analyzer {
  protected TokenStreamComponents createComponents(String fieldName) {
    try {

        ResourceLoader resourceLoader = new ClasspathResourceLoader(ClassLoader.getSystemClassLoader());


        TokenizerModel tokenizerModel = OpenNLPOpsFactory.getTokenizerModel(""en-token.bin"", resourceLoader);
        NLPTokenizerOp tokenizerOp = new NLPTokenizerOp(tokenizerModel);


        SentenceModel sentenceModel = OpenNLPOpsFactory.getSentenceModel(""en-sent.bin"", resourceLoader);
        NLPSentenceDetectorOp sentenceDetectorOp = new NLPSentenceDetectorOp(sentenceModel);

        Tokenizer source = new OpenNLPTokenizer(
                AttributeFactory.DEFAULT_ATTRIBUTE_FACTORY, sentenceDetectorOp, tokenizerOp);

        POSModel posModel = OpenNLPOpsFactory.getPOSTaggerModel(""en-pos-maxent.bin"", resourceLoader);
        NLPPOSTaggerOp posTaggerOp = new NLPPOSTaggerOp(posModel);

        // Perhaps we should also use a lower-case filter here?

        TokenFilter posFilter = new OpenNLPPOSFilter(source, posTaggerOp);

        // Very important: Tokens are not indexed, we need a store them as payloads otherwise we cannot search on them
        TypeAsPayloadTokenFilter payloadFilter = new TypeAsPayloadTokenFilter(posFilter);

        return new TokenStreamComponents(source, payloadFilter);
    }
    catch (IOException e) {
        throw new RuntimeException(e.getMessage());
    }              

}
</code></pre>

<p>Note that we are using a TypeAsPayloadTokenFilter wrapped around OpenNLPPOSFilter. This means, our POS tags will be indexed as payloads, and our query - however it'll look like - will have to search on payloads as well.</p>

<p><strong>Querying</strong>
This is where I am stuck. I have no clue how to query on payloads, and whatever I try does not work. Note that I am using Lucene 7, it seems that in older versions querying on payload has changed several times. Documentation is extremely scarce. It's not even clear what the proper field name is now to query - is it ""word"" or ""type"" or anything else? For example, I tried this code which does not return any search results:</p>

<pre><code>    // Step 1: Indexing
    final String body = ""The quick brown fox jumped over the lazy dogs."";
    Directory index = new RAMDirectory();
    OpenNLPAnalyzer analyzer = new OpenNLPAnalyzer();
    IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer);
    IndexWriter writer = new IndexWriter(index, indexWriterConfig);
    Document document = new Document();
    document.add(new TextField(""body"", body, Field.Store.YES));
    writer.addDocument(document);
    writer.close();


    // Step 2: Querying
    final int topN = 10;
    DirectoryReader reader = DirectoryReader.open(index);
    IndexSearcher searcher = new IndexSearcher(reader);

    final String fieldName = ""body""; // What is the correct field name here? ""body"", or ""type"", or ""word"" or anything else?
    final String queryText = ""JJ"";
    Term term = new Term(fieldName, queryText);
    SpanQuery match = new SpanTermQuery(term);
    BytesRef pay = new BytesRef(""type""); // Don't understand what to put here as an argument
    SpanPayloadCheckQuery query = new SpanPayloadCheckQuery(match, Collections.singletonList(pay));

    System.out.println(query.toString());

    TopDocs topDocs = searcher.search(query, topN);
</code></pre>

<p>Any help is very much appreciated here.</p>
",Parsing & POS Tagging,querying part speech tag lucene opennlp fun learning trying build part speech po tagger opennlp lucene goal would indexed actually search sequence po tag find sentence match sequence already get indexing part stuck query part aware solr might functionality already checked code wa self expalantory goal understand implement lucene solr want independent search engine top idea input sentence quick brown fox jumped lazy dog applied lucene opennlp tokenizer result quick brown fox jumped lazy dog next applying lucene opennlp po tagging result dt jj jj nn vbd dt jj nns input sentence give baby applied lucene opennlp tokenizer result give baby next applying lucene opennlp po tagging result vb prp prp uh query jj nn vbd match part sentence sentence returned point interested exact match e let leave aside partial match wildcards etc indexing first created class com example opennlpanalyzer note using typeaspayloadtokenfilter wrapped around opennlpposfilter mean po tag indexed payload query however look like search payload well querying stuck clue query payload whatever try doe work note using lucene seems older version querying payload ha changed several time documentation extremely even clear proper field name query word type anything else example tried code doe return search result help much appreciated
What are the common methods to determine intent,"<p>Many NLP APIs offer intent extraction like API.ai and wit.ai. However I'm unclear about their details. Do they do dependency parsing then extract relations, or simply taking out keywords from a sentence? How to parse ""check if tomorrow is going to rain""?</p>
",Parsing & POS Tagging,common method determine intent many nlp apis offer intent extraction like api ai wit ai however unclear detail dependency parsing extract relation simply taking keywords sentence parse check tomorrow going rain
How to make a tree from the output of a dependency parser?,"<p>I am trying to make a tree (nested dictionary) from the output of dependency parser. The sentence is ""I shot an elephant in my sleep"". I am able to get the output as described on the link:
<a href=""https://stackoverflow.com/questions/7443330/how-do-i-do-dependency-parsing-in-nltk"">How do I do dependency parsing in NLTK?</a></p>

<pre><code>nsubj(shot-2, I-1)
det(elephant-4, an-3)
dobj(shot-2, elephant-4)
prep(shot-2, in-5)
poss(sleep-7, my-6)
pobj(in-5, sleep-7)
</code></pre>

<p>To convert this list of tuples into nested dictionary, I used the following link:
<a href=""https://stackoverflow.com/questions/39495924/how-to-convert-python-list-of-tuples-into-tree"">How to convert python list of tuples into tree?</a></p>

<pre><code>def build_tree(list_of_tuples):
    all_nodes = {n[2]:((n[0], n[1]),{}) for n in list_of_tuples}
    root = {}    
    print all_nodes
    for item in list_of_tuples:
        rel, gov,dep = item
        if gov is not 'ROOT':
            all_nodes[gov][1][dep] = all_nodes[dep]
        else:
            root[dep] = all_nodes[dep]
    return root
</code></pre>

<p>This gives the output as follows:</p>

<pre><code>{'shot': (('ROOT', 'ROOT'),
  {'I': (('nsubj', 'shot'), {}),
   'elephant': (('dobj', 'shot'), {'an': (('det', 'elephant'), {})}),
   'sleep': (('nmod', 'shot'),
    {'in': (('case', 'sleep'), {}), 'my': (('nmod:poss', 'sleep'), {})})})}
</code></pre>

<p>To find the root to leaf path, I used the following link: <a href=""https://stackoverflow.com/questions/47302382/return-root-to-specific-leaf-from-a-nested-dictionary-tree"">Return root to specific leaf from a nested dictionary tree</a></p>

<p>[Making the tree and finding the path are two separate things]The second objective is to find the root to leaf node path like done <a href=""https://stackoverflow.com/questions/47302382/return-root-to-specific-leaf-from-a-nested-dictionary-tree]"">Return root to specific leaf from a nested dictionary tree</a>. 
But I want to get the root-to-leaf (dependency relationship path)
So, for instance, when I will call recurse_category(categories, 'an') where categories is the nested tree structure and 'an' is the word in the tree, I should get <code>ROOT-nsubj-dobj</code> (dependency relationship till root) as output.</p>
",Parsing & POS Tagging,make tree output dependency parser trying make tree nested dictionary output dependency parser sentence shot elephant sleep able get output described link give output follows find root leaf path used following link want get root leaf dependency relationship path instance call recurse category category category nested tree structure word tree get dependency relationship till root output
Approach to extract meaning from sentence NLP,"<p>What I want to know is what the best approach would be to extract meaning from a text. I gave <a href=""https://www.nltk.org/book/ch01.html"" rel=""nofollow noreferrer"">NLTK</a> a read, and it did give me some good information on the basics of NLP. </p>

<p>I'm new to nlp, so I'm having a tough time deciding what my direction should be. After reading the NLTK text, here's what I'm thinking what would solve my problem:</p>

<p>Here is my ideal goal with example sentences: </p>

<p><strong>Input:</strong></p>

<blockquote>
  <p>Do X on 8/29/2018 until 9/12/2018 (every Wednesday) and 9/10/2018 (Monday)</p>
</blockquote>

<p><strong>Output Part of Speech tag in bold (At least what I invision):</strong></p>

<blockquote>
  <p>Do X on 8/29/2018(<strong><em>Date</em></strong>) until(<strong><em>Range</em></strong>) 9/12/2018(<strong><em>Date</em></strong>) (every(<strong><em>Frequency</em></strong>) Wednesday(<strong><em>Day</em></strong>) and 9/10/2018(<strong><em>Date</em></strong>) (Monday)(<strong><em>Day</em></strong>)</p>
</blockquote>

<p>Next, I would loop through the part of speech tags and chuck the text. My hope is after chunking of the text appropriately, I would then need to do some additional processing in order to figure out what the user exactly wants. Here's what I'm thinking the output would be after a successful  meaning extraction from the sentence :</p>

<blockquote>
  <p>8/29/2018 - 9/12/2018, Wednedays</p>
  
  <p>9/10/2018, Monday</p>
</blockquote>

<p>I realize that finding days of the month, days and dates, etc can be easily found through a regular expression. </p>

<p>But my issue is that the NLTK method <strong><em>nltk.pos_tag</em></strong> method would not work for me. (For those who aren't familiar, the method is a part of speech tagger tagging words likw noun, verb etc.) I would most likely have to customize my own <strong><em>pos_tag</em></strong> method? </p>

<p>So here's my question. Is tagging each tokenized word first, then chunking the sentences from the tags considered best practice to extract meaning? </p>

<p>I'm guessing I would need some sort of AI classification to learn the chunking part so that in the future I can extracting meaning from more than one sentence. </p>

<p>Is my approach sane? Have I gone mad? :)</p>
",Parsing & POS Tagging,approach extract meaning sentence nlp want know best approach would extract meaning text gave nltk read give good information basic nlp new nlp tough time deciding direction reading nltk text thinking would solve problem ideal goal example sentence input x every wednesday monday output part speech tag bold least invision x date range date every frequency wednesday day date monday day next would loop part speech tag text hope chunking text appropriately would need additional processing order figure user exactly want thinking output would successful meaning extraction sentence wednedays monday realize finding day month day date etc easily found regular expression issue nltk method nltk po tag method would work familiar method part speech tagger tagging word likw noun verb etc would likely customize po tag method question tagging tokenized word first chunking sentence tag considered best practice extract meaning guessing would need sort ai classification learn chunking part future extracting meaning one sentence approach sane gone mad
Part-of-Speech (POS) vs Syntactic Dependency Parsing,"<p>I am using SpaCy for text analysis but I cannot understand the difference between Part-of-Speech (POS) and Syntactic Dependency Parsing. Both label the words in a sentence based on their role. But how exactly they are different?</p>
",Parsing & POS Tagging,part speech po v syntactic dependency parsing using spacy text analysis understand difference part speech po syntactic dependency parsing label word sentence based role exactly different
Dependency Parsing in python,"<p>I am new to dependecy parsing and maybe this error might be very easy to fix. I am trying to perform dependency parsing on statements in order to find out the focus of specific words. I found following links:</p>

<p><a href=""https://stackoverflow.com/questions/7443330/how-do-i-do-dependency-parsing-in-nltk"">How do I do dependency parsing in NLTK?</a></p>

<p>If I try the above link then I am getting,</p>

<pre><code>LookupError: 

===========================================================================
NLTK was unable to find the java file!
Use software specific configuration paramaters or set the JAVAHOME environment variable.
===========================================================================
</code></pre>

<p>The second link I tried was:</p>

<p><a href=""https://github.com/Lynten/stanford-corenlp"" rel=""nofollow noreferrer"">https://github.com/Lynten/stanford-corenlp</a></p>

<p>Following is my implementation:</p>

<pre><code>from pycorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP('http://localhost:9000')
sentence='I shot an elephant in my sleep'
print 'Dependency Parsing:', nlp.dependency_parse(sentence)
nlp.close()
</code></pre>

<p>But I am getting <code>AttributeError: StanfordCoreNLP instance has no attribute 'dependency_parse'</code></p>

<p>If there are other methods which are easier, then also I am open to them.
Thank you.</p>
",Parsing & POS Tagging,dependency parsing python new dependecy parsing maybe error might easy fix trying perform dependency parsing statement order find focus specific word found following link following implementation getting method easier also open thank
Dependency Parsing using Stanford Dependency Parser,"<p>i am trying to extract main verb in a sentence and i followed this <a href=""https://stackoverflow.com/questions/19751230/how-can-we-extract-the-main-verb-from-a-sentence"">question</a> , i am expecting output in this format </p>

<pre><code>nsubj(swim-4, Parrots-1)
aux(swim-4, do-2)
neg(swim-4, not-3)
root(ROOT-0, swim-4)
</code></pre>

<p>but i am getting output in this way </p>

<pre><code>[&lt;DependencyGraph with 94 nodes&gt;]
</code></pre>

<p>i did following </p>

<pre><code>  dependencyParser = stanford.StanfordDependencyParser(model_path=""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")
  print (list(dependencyParser.raw_parse(noiseLessInput)))
</code></pre>

<p>i think i am doing something wrong, how can i achieve desired ouput</p>
",Parsing & POS Tagging,dependency parsing using stanford dependency parser trying extract main verb sentence followed href expecting output format p getting output way following think something wrong achieve desired ouput
POS Tagger for a Virtual Assistant,"<p>I'm trying to build a POS Tagger for a Voise Assistant. However, the nltk's pos tagger nltk.pos_tag doesn't work well for me. For example:</p>

<pre><code>sent = 'open Youtube'
tokens = nltk.word_tokenize(sent)
nltk.pos_tag(tokens, tagset='universal')
&gt;&gt;[('open', 'ADJ'), ('Youtube', 'NOUN')]
</code></pre>

<p>In the above case I'd want the word open to be a verb and not an adjective. Similarly, it tags the word 'close' as an adverb and not a verb.</p>

<p>I have also tried using an <code>n-gram tagger</code></p>

<pre><code>train_sents = brown_tagged_sents[:size] 
test_sents = brown_tagged_sents[size:]
default_tagger = nltk.DefaultTagger('NOUN')
unigram_tagger = nltk.UnigramTagger(train_sents, backoff = default_tagger)
bigram_tagger = nltk.BigramTagger(train_sents, backoff = unigram_tagger)
trigram_tagger = nltk.TrigramTagger(train_sents, backoff = bigram_tagger)
</code></pre>

<p>I have used the brown corpus from <code>nltk</code>. But it still gives the same result.</p>

<p>So I'd like to know:</p>

<ol>
<li>Is there a better tagged corpus to train a tagger for making a voice/virtual assistant? </li>
<li>Is there a higher n-gram than trigram i.e. that looks at 4 words or more together like trigram and bigram look at 3 and 2 words together respectively. Will it improve the performance?</li>
<li>How can I fix this?</li>
</ol>
",Parsing & POS Tagging,po tagger virtual assistant trying build po tagger voise assistant however nltk po tagger nltk po tag work well example case want word open verb adjective similarly tag word close adverb verb also tried using used brown corpus still give result like know better tagged corpus train tagger making voice virtual assistant higher n gram trigram e look word together like trigram bigram look word together respectively improve performance fix
Get all possibles pos tags from a single word,"<p>I'm currently trying to get all possible pos tags of a single word using Python. 
From traditional pos taggers you get back only one tag, if you enter the single word.
Is there a way to get all possiblities?
Is it possible to search in a corpora(e.g. brown) for a specific word and not just for a category? </p>

<p>Kind regards &amp; thanks for help</p>
",Parsing & POS Tagging,get possible po tag single word currently trying get possible po tag single word using python traditional po tagger get back one tag enter single word way get possiblities possible search corpus e g brown specific word category kind regard thanks help
WordNetLemmatizer: Different handling of wn.ADJ and wn.ADJ_SAT?,"<p>I need to lemmatize text using nltk. In order to do this, I apply <code>nltk.pos_tag</code> to each sentence and then convert the resulting Penn Treebank tags (<a href=""http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""noreferrer"">http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a>) to WordNet tags. I need to do this because <code>WordNetLemmatizer.lemmatize()</code> expects both the word and its correct pos_tag as arguments, otherwise it will just assume everything is a verb.</p>

<p>I just found that there are five different tags defined in WordNet: </p>

<ul>
<li>wn.VERB</li>
<li>wn.ADV</li>
<li>wn.NOUN</li>
<li>wn.ADJ</li>
<li>wn.ADJ_SAT</li>
</ul>

<p>However, <strong>every example I found on the internet just ignores wn.ADJ_SAT</strong> when converting Treebank tags to WordNet tags. They are all just mapping Penn tags to WordNet tags like this:</p>

<ul>
<li>If Penn tag starts with J: convert to wn.ADJ</li>
<li>If Penn tag starts with V: convert to wn.VERB</li>
<li>If Penn tag starts with N: convert to wn.NOUN</li>
<li>If Penn tag starts with R: convert to wn.ADV</li>
</ul>

<p>So wn.ADJ_SAT is never used.</p>

<p><strong>My question</strong> now is if there are cases where the lemmatizer returns a different result for ADJ_SAT than for ADJ. What are examples for words that are satellite adjectives (ADJ_SAT) and no normal adjectives (ADJ)?</p>
",Parsing & POS Tagging,wordnetlemmatizer different handling wn adj wn adj sat need lemmatize text using nltk order apply sentence convert resulting penn treebank tag wordnet tag need expects word correct po tag argument otherwise assume everything verb found five different tag defined wordnet wn verb wn adv wn noun wn adj wn adj sat however every example found internet ignores wn adj sat converting treebank tag wordnet tag mapping penn tag wordnet tag like penn tag start j convert wn adj penn tag start v convert wn verb penn tag start n convert wn noun penn tag start r convert wn adv wn adj sat never used question case lemmatizer return different result adj sat adj example word satellite adjective adj sat normal adjective adj
NLTK: Auto suggestion for query completion using grammar,"<p>I want to implement <a href=""http://iswc2015.semanticweb.org/sites/iswc2015.semanticweb.org/files/93670017.pdf"" rel=""nofollow noreferrer"">Auto-Suggestion for question completion</a> (Refer Section 3.2) for my FCFG using nltk APIs. </p>

<p>E.g.
Consider the following CFG grammar:</p>

<pre><code>S -&gt; NP VP
NP -&gt; Det NN | PropN
VP -&gt; V NP | V
V -&gt; 'eats' | 'sleeps' | 'ate'
Det -&gt; 'a' | 'an' | 'the'
NN -&gt; 'police' | 'horse' | 'apple' | 'potato'
PropN -&gt; 'Robin'
</code></pre>

<p>After each word of input, what I want is a module which suggests me all possible lexicons which can follow my current input.</p>

<p>E.g. If I type <em>Robin</em>, my grammar should parse this and my module should suggest the left-most leaf from the sub-tree that just follows <em>Robin</em> parent's production i.e. <code>NP -&gt; PropN</code>. Since in this case nothing follows <code>PropN</code>, I want it to search for the parent of <code>PropN</code> i.e. <code>S -&gt; NP VP</code>. Since <code>VP</code> follows <code>NP</code>, my module should should print <code>VP</code>'s left-most leaf(for every <code>VP</code> production, in this case only <code>V</code>) i.e.: <em>eats, sleeps, ate</em>.</p>

<p>I'm using nltk and my grammar is FCFG. Any help is appreciated. </p>
",Parsing & POS Tagging,nltk auto suggestion query completion using grammar want implement auto suggestion question completion refer section fcfg using nltk apis e g consider following cfg grammar word input want module suggests possible lexicon follow current input e g type robin grammar parse module suggest left leaf sub tree follows robin parent production e since case nothing follows want search parent e since follows module print left leaf every production case e eats sleep ate using nltk grammar fcfg help appreciated
CFG gammar for interrogative statement,"<p>Using the below grammar I can chunk declarative statement which it has noun phrase and verb phrase, how can I modify the below grammar to chunk the interrogative statement from it?</p>

<pre><code>grammar = r""""""
  NP: {&lt;DT|JJ|NN.*&gt;+}         
  PP: {&lt;IN&gt;&lt;NP&gt;}               
  VP: {&lt;VB.*&gt;&lt;NP|PP|CLAUSE&gt;+$} 
  CLAUSE: {&lt;NP&gt;&lt;VP&gt;}
  """"""
</code></pre>
",Parsing & POS Tagging,cfg gammar interrogative statement using grammar chunk declarative statement ha noun phrase verb phrase modify grammar chunk interrogative statement
How to do dependency parsing in R language?,"<p>I did POSTagging. But how to do dependency parsing? Can I get the code for it in R language ?</p>
",Parsing & POS Tagging,dependency parsing r language postagging dependency parsing get code r language
Dependency parsing using spacy,"<p>I have a code for dependency parsing which gives output in the form of arcs. Is there any other way to display the parse tree for a paragraph? Because for a paragraph, the parse tree is huge. Is there a better way to display the parse tree for a paragraph?</p>
",Parsing & POS Tagging,dependency parsing using spacy code dependency parsing give output form arc way display parse tree paragraph paragraph parse tree huge better way display parse tree paragraph
Automating company information search on Google (merger acquisitions) using NLP &amp; ML,"<p>So Im trying to find out if a company has been acquired by another company or not. Lets say I search for halli labs and want to know whether its been acquired or not. If yes then I need to know the parent company name. My approach is to google search ""Halli labs parent company"". Then Ive scraped all the text on the first page, all the corresponding links, date etc. Then I can run pos tag, generate bigrams, trigrams etc and feed it to some algorithm to find if the text is about acquisition, if yes then pull out the company name.</p>

<p>The problem now is that, the name of the companies are getting tagged as ""PERSON"", is there a way I can resolve this ?</p>

<p>Also is my approach good enough ? Because thats basically how a human would find whether a company has been acquired or not ?</p>

<pre><code>nltk.ne_chunk(nltk.pos_tag(nltk.tokenize.word_tokenize(""Google has acquired Halli Labs, a four-month old start-up out of Bengaluru that is developing artificial intelligence and machine learning"")))
</code></pre>
",Parsing & POS Tagging,automating company information search google merger using nlp ml im trying find company ha acquired another company let say search halli lab want know whether acquired yes need know parent company name approach google search halli lab parent company ive scraped text first page corresponding link date etc run po tag generate bigram trigram etc feed algorithm find text yes pull company name problem name company getting tagged person way resolve also approach good enough thats basically human would find whether company ha acquired
How to write scripts to keep punctuation in Stanford dependency parser,"<p>In order to get some specific dependency information I write a java script to parse sentences rather than directly use ParserDemo.java that Stanford Parser 3.9.1 provided. But I found punctuation is missing after got typedDependencies. Is there any function to get punctuation in Stanford Parser?
<strong>I had to write a script to parse sentences myself for the reason that I need to create a SemanticGraph from a List of TypedDependencies, in order to use methods in SemanticGraph to get evey single tokens dependent information(include punctuation).</strong></p>

<pre><code>public class ChineseFileTest3 {

public static void main(String[] args){

    String modelpath = ""edu/stanford/nlp/models/lexparser/xinhuaFactored.ser.gz"";
    LexicalizedParser lp = LexicalizedParser.loadModel(modelpath);
    String textFile = ""data/chinese-onesent-unseg-utf8.txt"";
    demoDP(lp,textFile);

}
public static void demoDP(LexicalizedParser lp, String filename){

for(List&lt;HasWord&gt; sentence : new DocumentPreprocessor(filename)) {

    Tree t = lp.apply(sentence);

    ChineseGrammaticalStructure gs = new ChineseGrammaticalStructure(t);
    Collection&lt;TypedDependency&gt; tdl = gs.typedDependenciesCollapsed();
    System.out.println(tdl);

}
}
}
</code></pre>
",Parsing & POS Tagging,write script keep punctuation stanford dependency parser order get specific dependency information write java script parse sentence rather directly use parserdemo java stanford parser provided found punctuation missing got typeddependencies function get punctuation stanford parser write script parse sentence reason need create semanticgraph list typeddependencies order use method semanticgraph get evey single token dependent information include punctuation
How to get a parse in a bracketed format (without POS tags)?,"<p>I want to parse a sentence to a binary parse of this form (Format used in the <a href=""https://nlp.stanford.edu/projects/snli/"" rel=""nofollow noreferrer"">SNLI</a> corpus):</p>

<p>sentence:""A person on a horse jumps over a broken down airplane.""</p>

<p>parse: ( ( ( A person ) ( on ( a horse ) ) ) ( ( jumps ( over ( a ( broken ( down airplane ) ) ) ) ) . ) )</p>

<p>I'm unable to find a parser which does this.</p>

<p>note: This question has been asked earlier(<a href=""https://stackoverflow.com/questions/44742809/how-to-get-a-binary-parse-in-python"">How to get a binary parse in Python</a>). But the answers are not helpful. And I was unable to comment because I do not have the required reputation.</p>
",Parsing & POS Tagging,get parse bracketed format without po tag want parse sentence binary parse form format used snli corpus sentence person horse jump broken airplane parse person horse jump broken airplane unable find parser doe note question ha asked earlier href get binary parse python answer helpful wa unable comment required reputation
How to get a binary parse in Python,"<p>I have data from natural language inference corpora (<a href=""https://nlp.stanford.edu/projects/snli/"" rel=""nofollow noreferrer"">SNLI</a>, <a href=""http://www.nyu.edu/projects/bowman/multinli/"" rel=""nofollow noreferrer"">multiNLI</a>) that comes in this form:</p>

<pre><code>'( ( Two ( blond women ) ) ( ( are ( hugging ( one another ) ) ) . ) )'
</code></pre>

<p>They are supposed to be a binary trees (some are not very clean).</p>

<p>I want to parse some of my own sentences into this format. How can I do that with NLTK or similar?</p>

<p>I have found the StanfordParser, but I have not been able to find how to get this kind of a parse.</p>
",Parsing & POS Tagging,get binary parse python data natural language inference corpus snli multinli come form supposed binary tree clean want parse sentence format nltk similar found stanfordparser able find get kind parse
getDependency for file of sentences,"<p>I'm trying to have dependency parser for each sentence. 
I tried coreNLP parsing for single sentence as</p>

<pre><code> library(coreNLP)
 initCoreNLP(mem=""4g"")
 x=""I've been using Galaxy J5 for 2 years and it would never get heated while using even long.""
 anno &lt;- annotateString(x)
 ext&lt;-getDependency(anno)
 extparse&lt;-getParser(anno)
</code></pre>

<p>output: ext :
<a href=""https://i.sstatic.net/AKiu9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AKiu9.png"" alt=""image""></a></p>

<p>output:  extparse:</p>

<p>(ROOT\r\n  (S\r\n    (S\r\n      (NP (PRP I))\r\n      (VP (VBP 've)\r\n        (VP (VBN been)\r\n          (VP (VBG using)\r\n            (NP (NNP Galaxy) (NN J5))\r\n            (PP (IN for)\r\n              (NP (CD 2) (NNS years)))))))\r\n    (CC and)\r\n    (S\r\n      (NP (PRP it))\r\n      (VP (MD would)\r\n        (ADVP (RB never))\r\n        (VP (VB get)\r\n          (VP (VBN heated)\r\n            (PP (IN while)\r\n              (S\r\n                (VP (VBG using)\r\n                  (ADVP (RB even) (RB long)))))))))\r\n    (. .)))\r\n\r\n""</p>

<p>Type in dependency is giving correctly than parser But i want to implement this for a file that contains sentences ,How to implement coreNLP dependency for sentences .</p>

<p>I tried to do getdependency for multiple sentences at a time then I got it as vector form </p>

<p><a href=""https://i.sstatic.net/jr0TE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jr0TE.png"" alt=""image""></a></p>

<p>how to get dependency for multiple sentences ?? </p>

<p>here is my other question:
 how to analyse the parser data to extract all nouns and adjectives 
how to get each level of parser data separately so that each level could be processed</p>

<p>Any help is very precious as I was struct in this from many days</p>
",Parsing & POS Tagging,getdependency file sentence trying dependency parser sentence tried corenlp parsing single sentence output ext output extparse root r n r n r n np prp r n vp vbp r n vp vbn r n vp vbg using r n np nnp galaxy nn j r n pp r n np cd nns year r n cc r n r n np prp r n vp md would r n advp rb never r n vp vb get r n vp vbn heated r n pp r n r n vp vbg using r n advp rb even rb long r n r n r n type dependency giving correctly parser want implement file contains sentence implement corenlp dependency sentence tried getdependency multiple sentence time got vector form get dependency multiple sentence question analyse parser data extract noun adjective get level parser data separately level could processed help precious wa struct many day
Converting Dependency tree into sequence of Arc-eager transitions,"<p>Currently I'm trying to build syntax-aware NMT model.<br>
In this project, I need the sequence of one of three transition actions (SHIFT, REDUCE-L, REDUCE-R)</p>

<p>Similar to what is in the image 
<a href=""https://i.sstatic.net/09Ian.png"" rel=""nofollow noreferrer"">a</a></p>

<p>This chunk represents the transition-based dependency for 2 sentences(1 for 1 chunk split by empty lines) </p>

<p>I'm using <code>Syntaxnet</code> to get the dependency parse tree first, but it doesn't directly provide that transition action sequences. <br>
It's results are as follows,</p>

<p><a href=""https://i.sstatic.net/4t5IZ.png"" rel=""nofollow noreferrer"">b</a></p>

<p>Is it possible to get the action sequences similar to this image? Is it possible to convert what is achieved from this image to the original image's format.</p>
",Parsing & POS Tagging,converting dependency tree sequence arc eager transition currently trying build syntax aware nmt model project need sequence one three transition action shift reduce l reduce r similar image chunk represents transition based dependency sentence chunk split empty line using get dependency parse tree first directly provide transition action sequence result follows b possible get action sequence similar image possible convert achieved image original image format
Drawing a flatten NLTK Parse Tree with NP chunks,"<p>I want to analyze sentences with NLTK and display their chunks as a tree. NLTK offers the method <code>tree.draw()</code> to draw a tree. This following code draws a tree for the sentence <em>""the little yellow dog barked at the cat""</em>:</p>

<pre><code>import nltk 
sentence = [(""the"", ""DT""), (""little"", ""JJ""), (""yellow"", ""JJ""), (""dog"", ""NN""), (""barked"",""VBD""), (""at"", ""IN""), (""the"", ""DT""), (""cat"", ""NN"")]

pattern = ""NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}""
NPChunker = nltk.RegexpParser(pattern) 
result = NPChunker.parse(sentence)
result.draw()
</code></pre>

<p>The result is this tree:</p>

<p><a href=""https://i.sstatic.net/Id64E.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Id64E.png"" alt=""example tree""></a></p>

<p>How do i get a tree with one more level like this?</p>

<p><a href=""https://i.sstatic.net/qOuYb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qOuYb.png"" alt=""deeper tree""></a></p>
",Parsing & POS Tagging,drawing flatten nltk parse tree np chunk want analyze sentence nltk display chunk tree nltk offer method draw tree following code draw tree sentence little yellow dog barked cat result tree get tree one level like
Dependency Parse Tree Matching in python,"<p>I am working on <strong>Answer Sentence Selection Problem</strong>, and I want to <strong>compare dependency trees</strong> of two sentences. I am retrieving the dependency tree from spaCy and now I want to compare dependency trees. Is there any way or library in Python that I could use?</p>
",Parsing & POS Tagging,dependency parse tree matching python working answer sentence selection problem want compare dependency tree two sentence retrieving dependency tree spacy want compare dependency tree way library python could use
Python - Extract relation of entities (noun phrases) from unstructured-based text (NLP) using NLTK,"<p>From a unstructured text, I have extracted all necessary entities and stored it in a dictionary using stanford POS tagger. Now I want to extract the relation between them to build my own Ontology in the form of triplets (Entity1,Entity2,relation). I tried the stanford dependencies parser, but I don't know how to extract these triplets. </p>

<p>For example:
The front diffusers comprise pivotable flaps that are arranged between boundary walls of air ducts.</p>

<p>I want to have the relation (front diffusers, pivotable flaps, comprise); (pivotable flaps, boundary walls of air ducts, arrange);</p>

<p>Another example: The cargo body comprises a container having a floor, a top wall, a front wall, side walls and a rear door.</p>

<p>My expected relations are (cargo body, container, comprise); (container, floor, have); (container,top wall, have); (container, front wall, have); (container, side walls, have); (container, rear door, have). </p>

<p>What can I do with the stanford dependencies parser to achieve my goal? This means how to navigate the dependencies parse tree and get the results?</p>
",Parsing & POS Tagging,python extract relation entity noun phrase unstructured based text nlp using nltk unstructured text extracted necessary entity stored dictionary using stanford po tagger want extract relation build ontology form triplet entity entity relation tried stanford dependency parser know extract triplet example front diffuser comprise pivotable flap arranged boundary wall air duct want relation front diffuser pivotable flap comprise pivotable flap boundary wall air duct arrange another example cargo body comprises container floor top wall front wall side wall rear door expected relation cargo body container comprise container floor container top wall container front wall container side wall container rear door stanford dependency parser achieve goal mean navigate dependency parse tree get result
Extract probabilities and most likely parse tree from cyk,"<p>In order to understand cyk algorithm I've worked through example on : <a href=""https://www.youtube.com/watch?v=VTH1k-xiswM&amp;feature=youtu.be"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=VTH1k-xiswM&amp;feature=youtu.be</a> . </p>

<p>The result of which is : </p>

<p><a href=""https://i.sstatic.net/RR78n.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RR78n.png"" alt=""enter image description here""></a></p>

<p>How do I extract the probabilities associated with each parse and extract the most likely parse tree ?</p>
",Parsing & POS Tagging,extract probability likely parse tree cyk order understand cyk algorithm worked example result extract probability associated parse extract likely parse tree
How to get all VP and NP that have 2 or 3 direct children,"<p>I'm using Stanford NLP to generate parse trees of documents. I then want to iterate over these documents and store all phrases that are 2 or 3 words long that are part of a VP (verb phrase) or NP (noun phrase). What is a strategy I can use to accomplish this?</p>
",Parsing & POS Tagging,get vp np direct child using stanford nlp generate parse tree document want iterate document store phrase word long part vp verb phrase np noun phrase strategy use accomplish
Using custom POS tags for NLTK chunking?,"<p>Is it possible to use non-standard part of speech tags when making a grammar for chunking in the NLTK? For example, I have the following sentence to parse:</p>

<pre><code>complication/patf associated/qlco with/prep breast/noun surgery/diap
independent/adj of/prep the/det use/inpr of/prep surgical/diap device/medd ./pd
</code></pre>

<p>Locating the phrases I need from the text is greatly assisted by specialized tags such as ""medd"" or ""diap"". I thought that because you can use RegEx for parsing, it would be independent of anything else, but when I try to run the following code, I get an error:</p>

<pre><code>grammar = r'TEST: {&lt;diap&gt;}'
cp = nltk.RegexpParser(grammar)
cp.parse(sentence)

ValueError: Transformation generated invalid chunkstring:
&lt;patf&gt;&lt;qlco&gt;&lt;prep&gt;&lt;noun&gt;{&lt;diap&gt;}&lt;adj&gt;&lt;prep&gt;&lt;det&gt;&lt;inpr&gt;&lt;prep&gt;{&lt;diap&gt;}&lt;medd&gt;&lt;pd&gt;
</code></pre>

<p>I think this has to do with the tags themselves, because the NLTK can't generate a tree from them, but is it possible to skip that part and just get the chunked items returned? Maybe the NLTK isn't the best tool, and if so, can anyone recommend another module for chunking text? </p>

<p>I'm developing in python 2.7.6 with the Anaconda distribution.</p>

<p>Thanks in advance!</p>
",Parsing & POS Tagging,using custom po tag nltk chunking possible use non standard part speech tag making grammar chunking nltk example following sentence parse locating phrase need text greatly assisted specialized tag medd diap thought use regex parsing would independent anything else try run following code get error think ha tag nltk generate tree possible skip part get chunked item returned maybe nltk best tool anyone recommend another module chunking text developing python anaconda distribution thanks advance
What are &#39;head words&#39; and &#39;lexical head&#39; in parse trees?,"<p>In the Jurafsky and Martin's Speech and Language Processing NLP textbook, a head tag in parse trees are mentioned that in lexicalized grammar, non-terminal in the tree is annotated with its lexical head.</p>

<p>I don't actually get what lexical heads are.</p>

<p><a href=""https://i.sstatic.net/5l4Cg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5l4Cg.png"" alt=""enter image description here""></a></p>

<p>In the image attached, the word inside the parenthesis is the head word. What exactly are these and how do we determine them?</p>
",Parsing & POS Tagging,head word lexical head parse tree jurafsky martin speech language processing nlp textbook head tag parse tree mentioned lexicalized grammar non terminal tree annotated lexical head actually get lexical head image attached word inside parenthesis head word exactly determine
How to generate GloVe embeddings for POS tags? Python,"<p>For a sentence analysis task, I would like to take the sequence of POS tags associated with the sentence and feed it to my model as if the POS tags are words. </p>

<p>I am using GloVe to make representations of each word in the sentence and SpaCy to generate POS tags. However, GloVe embeddings do not make much sense for POS tags. So I will have to somehow create embeddings for each POS tag. What is the best way to do create embeddings for POS tags, so that I can feed POS sequences into my model in the same way I would feed sentences? Could anyone point to code examples of how to do this with GloVe in Python?</p>

<p><strong>Added context</strong></p>

<p>My task is a binary classification of sentence pairs, based on their resemblance (similar meaning vs different meaning). </p>

<p>I would like to use POS tags as words, so that the POS tags serve as an additional bit of information to compare the sentences. My current model does not use an LSTM as a way to predict sequences. </p>
",Parsing & POS Tagging,generate glove embeddings po tag python sentence analysis task would like take sequence po tag associated sentence feed model po tag word using glove make representation word sentence spacy generate po tag however glove embeddings make much sense po tag somehow create embeddings po tag best way create embeddings po tag feed po sequence model way would feed sentence could anyone point code example glove python added context task binary classification sentence pair based resemblance similar meaning v different meaning would like use po tag word po tag serve additional bit information compare sentence current model doe use lstm way predict sequence
nltk.pos_tag behaves inconsistently on specific words,"<p>I'm using nltk.pos_tag() on a list of unique words. Some words however get different tags on reruns. For example: 
""bighorn"" was tagged as NOUN, ADJECTIVE and VERB.</p>

<p>I don't want to enforce specific tagging but I like it to be consistent. Is there a way I could achieve this?</p>
",Parsing & POS Tagging,nltk po tag behaves inconsistently specific word using nltk po tag list unique word word however get different tag rerun example bighorn wa tagged noun adjective verb want enforce specific tagging like consistent way could achieve
Constituency parse tree after processing,"<p>I use Stanford CoreNLP to get constituency parse tree. I am wondering should I perform this after pre-processing or before pre-processing. In pre-processing I make the characters lower case, remove punctuations, remove stopwords (e.g., the, you're, ...) , remove numbers, keep just alphabets, and so on.</p>

<p>My task is getting a vector representation for each constituency parse tree by considering each leaf (i.e., token) as a vector embedding.</p>

<p>I am wondering how big the difference does it make if I get constituency parse tree after pre-processing?</p>
",Parsing & POS Tagging,constituency parse tree processing use stanford corenlp get constituency parse tree wondering perform pre processing pre processing pre processing make character lower case remove punctuation remove stopwords e g remove number keep alphabet task getting vector representation constituency parse tree considering leaf e token vector embedding wondering big difference doe make get constituency parse tree pre processing
Is this handling of ambiguities in dypgen normal or is it not?,"<p>I would like to know, if this is a bug or behavior, that is intended by the inventor.</p>

<p>Here I have a minimal example of a dypgen grammar:</p>

<pre><code>{
open Parse_tree
let dyp_merge = Dyp.keep_all
}

%start main
%layout [' ' '\t']

%%

main:
  | a  ""\n""                                                             { $1 }

a:
  | ms b                                                                { Mt ($1,$2) }
  | b &lt;Mt(_,_)&gt; kon1 b                                                  { Koo ($1, $2, $3) }
  | b &lt;Mt(_,_)&gt; kon2 b                                                  { Koo ($1, $2, $3) }
  | b                                                                   { $1 }

b:
  | k                                                                   { $1 }
  | ns b                                                                { Nt ($1,$2) } 
  /* If you comment this line out, it will work with the permutation, but I need the 'n' ! */


   /* | b &lt;Nt(_,_)&gt; kon1 b                                                 { Koo ($1, $2, $3) }
   | b &lt;Nt(_,_)&gt; kon2 b                                                 { Koo ($1, $2, $3) }*/

k:
  | k kon1 k                                                            { Koo ($1, $2, $3) }
  | k kon2 k                                                            { Koo ($1, $2, $3) }
  | ks                                                                  { $1 }

ms:
  | words &lt;M(_)&gt;                                                        { $1 }
ns:
  | words &lt;N(_)&gt;                                                        { $1 }
ks:
  | words &lt;K(_)&gt;                                                        { $1 }


kon1:
  | words &lt;U(_)&gt;                                                        { $1 }

kon2:
  | 'y'                                                                 { Y($1) }


words:
  | chain                                                               { $1 }
throw_away:
  | word  ""|"" throw_away                                                { $3 }
  | word                                                                { $1 }
chain:
  | word ""|"" throw_away                                                 { $1 }
  | word ""|"" chain                                                      { $3 }
  | word                                                                { $1 }


word:
  | ('m' ['1'-'9']?)                                                    { M ($1) }
  | ('n' ['1'-'9']?)                                                    { N ($1) }

  | ('K' ['1'-'9']?)                                                    { K ($1) }

  | ('u' ['1'-'9']?)                                                    { U ($1) }
</code></pre>

<p>The example can handle such grammars:</p>

<p>Think about the ? and * as regular expression operators, and 's' and 'm' and 'K' as lexems.</p>

<pre><code>  s = m? n* K
</code></pre>

<p>the 'K', 'm' and 'n' can also be replaced by these letters and a following number between 1-9
or they can be replaced by lists delimited by '|' as</p>

<pre><code>  m1
  n1|n2
  K|K|K or K1|K2|K3
</code></pre>

<p>and these lists could also be mixed as</p>

<pre><code>  m1|n1|K1
</code></pre>

<p>all these lists are parsed as possible ambiguities, that are globally merged – in the known sense for dypgen – with</p>

<pre><code>  let dyp_merge = Dyp.keep_all
</code></pre>

<p>If you type in:</p>

<blockquote>
<pre><code>m1|n1|K1  m1|n1|K1 m1|n1|K1      
</code></pre>
</blockquote>

<p>you get the results:</p>

<blockquote>
<pre><code>m1  &gt; n1  &gt; K1
n1  &gt; n1  &gt; K1
</code></pre>
</blockquote>

<p>If you type in</p>

<blockquote>
<pre><code> K1|K2
</code></pre>
</blockquote>

<p>you get</p>

<blockquote>
<pre><code> K1
 K2
</code></pre>
</blockquote>

<p>Now the interesting point:
In the grammar there is another feature. There is a ""koordination binding"" in the style of natural languages with 'u' or with 'y'.</p>

<p>This can bind these lists of ""phrases"" (a 'K' letter with optional fronting 'm' and a optinal number of 'n's) to somethin like ""K1 and K2"".
The grammer can parse:</p>

<pre><code> K1|K2 u K3|K4

 K1|K2 y K3|K4
</code></pre>

<p>And as I thought, it should have the same result.
But the difference between the ""koordination bindings"" is:
lexem 'u' is defined as a list of ambiguities  in the same ways as m, n, K and could also be mixed with 'K's, 'm's, 'n's
lexem 'y' is defined without this list festure.</p>

<p>And this makes a (surprising) difference:</p>

<blockquote>
<pre><code> K1|K2 u K3|K4
</code></pre>
</blockquote>

<p>is parsed as:</p>

<blockquote>
<pre><code> koo { K1 u K4 }
 koo { K2 u K4 }
</code></pre>
</blockquote>

<p>and</p>

<blockquote>
<pre><code> K1|K2 y K3|K4
</code></pre>
</blockquote>

<p>is parsed as:</p>

<blockquote>
<pre><code> koo2 { K1 y K3 }
 koo2 { K2 y K3 }
 koo2 { K1 y K4 }
 koo2 { K2 y K4 }
</code></pre>
</blockquote>

<p>In the first case the second part of the u-coordination is not permutated.
In the second case the second part of the coordination is permutated  (as dypgen does it with ambiguities normally).</p>

<p>Why this makes a difference?</p>

<p>(It must be somehow connected to the m's and n's, because if the rules for 'n's are left out, it works.)</p>

<p>Best regards and thank you for thinking about</p>

<p>gwf</p>

<hr>

<p>The minimal example is in the style of the dypgen-demos, to try, make a folder ""abc"" in the demos and put all the mentioned, fully cited files in there. 
The ""parse_tree"":</p>

<pre><code>type tree = 
  | M           of string
  | Mt          of tree * tree
  | N           of string
  | Nt          of tree * tree
  | K           of string
  | U           of string
  | Y           of string
  | Koo         of tree * tree * tree
  | Koo2        of tree * tree * tree * tree
</code></pre>

<p>A file ""printit.ml"":
    open Parse_tree </p>

<pre><code>let print_abc abc=
  let rec aux1 t = match t with
    | Koo(x1, k, x2) -&gt; (
        print_string ""\x1b[1m\x1b[31mkoo {\x1b[21m\027[0m "";
        aux1 x1;
        print_string """";
        aux1 k;
        print_string """";
        aux1 x2;
        print_string ""\x1b[1m\x1b[31m}\x1b[21m\027[0m"")
    | Koo2(k1, x1, k2, x2) -&gt; (
        print_string ""\x1b[1m\x1b[31mkoo2 {\x1b[21m\027[0m "";
        aux1 k1;
        print_string "" "";
        aux1 x1;
        print_string """";
        aux1 k2;
        print_string """";
        aux1 x2;
        print_string ""\x1b[1m\x1b[31m}\x1b[21m\027[0m"")
    | Word (w) -&gt; print_string (w ^ "" "")
    | M (w) -&gt; print_string (w ^ "" "")
    | K (w) -&gt; print_string (w ^ "" "")
    | N (w) -&gt; print_string (w ^ "" "")
    | U (w) -&gt; print_string (w ^ "" "")
    | Y (w) -&gt; print_string (w ^ "" "")
    | Nt (p, l)
    | Mt (p, l) -&gt; (
        print_string """";
        aux1 p;
        print_string "" &gt; "";
        aux1 l;)
  in
    let aux2 t = aux1 t; print_newline () in
  List.iter aux2 abc
</code></pre>

<p>and the ""main"" program:
    open Parse_tree
    open Printit</p>

<pre><code>let () = print_endline ""
please try:
  K1|K2 u K3|K4
and
  K1|K2 y K3|K4
""

let lexbuf = Dyp.from_channel (Abc_parser.pp ()) stdin

let _ =
  try
    while true do
      (Dyp.flush_input lexbuf;
      try
        let pf = Abc_parser.main lexbuf in
        print_abc (List.map (fun (x,_) -&gt; x) pf)
      with
        Dyp.Syntax_error -&gt; Printf.printf ""Syntax error\n\n""
      );
      flush stdout
    done
  with Failure _ -&gt; exit 0
</code></pre>

<p>and the ""Makefile""</p>

<pre><code>SOURCES = printit.ml abc_parser.dyp abc.ml
REP = -I ../../dyplib
CAMLC = ocamlc $(REP)
DYPGEN = ../../dypgen/dypgen --ocamlc ""-I ../../dyplib""
LIBS=dyp.cma

all: abc

SOURCES1 = $(SOURCES:.mll=.ml)
SOURCES2 = $(SOURCES1:.dyp=.ml)
OBJS = $(SOURCES2:.ml=.cmo)

abc: parse_tree.cmi $(OBJS)
    $(CAMLC) -o abc $(LIBS) $(OBJS)

.SUFFIXES: .ml .mli .cmo .cmi .dyp

.ml.cmo:
    $(CAMLC) -c $&lt;

.mli.cmi:
    $(CAMLC) -c $&lt;

.dyp.ml:
    $(DYPGEN) $&lt;
    $(CAMLC) -c $*.mli

clean:
    rm -f *.cm[iox] *~ .*~ *.o
    rm -f abc
    rm -f *.extract_type *_temp.ml
    rm -f *parser.ml *parser.mli
</code></pre>
",Parsing & POS Tagging,handling ambiguity dypgen normal would like know bug behavior intended inventor minimal example dypgen grammar example handle grammar think regular expression operator k lexems k n also replaced letter following number replaced list delimited list could also mixed list parsed possible ambiguity globally merged known sense dypgen type get result type get interesting point grammar another feature koordination binding style natural language u bind list phrase k letter optional fronting optinal number n somethin like k k grammer parse thought result difference koordination binding lexem u defined list ambiguity way n k could also mixed k n lexem defined without list festure make surprising difference parsed parsed first case second part u coordination permutated second case second part coordination permutated dypgen doe ambiguity normally make difference must somehow connected n rule n left work best regard thank thinking gwf minimal example style dypgen demo try make folder abc demo put mentioned fully cited file parse tree file printit ml open parse tree main program open parse tree open printit makefile
How to get a parse NLP Tree object from bracketed parse string with nltk or spacy?,"<p>I have a sentence ""<strong>You could say that they regularly catch a shower , which adds to their exhilaration and joie de vivre.</strong>"" and I can't achieve to get the NLP parse tree like the following example:</p>

<pre><code>(ROOT (S (NP (PRP You)) (VP (MD could) (VP (VB say) (SBAR (IN that) (S (NP (PRP they)) (ADVP (RB regularly)) (VP (VB catch) (NP (NP (DT a) (NN shower)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ adds) (PP (TO to) (NP (NP (PRP$ their) (NN exhilaration)) (CC and) (NP (FW joie) (FW de) (FW vivre))))))))))))) (. .)))
</code></pre>

<p>I want to replicate the solution to this question <a href=""https://stackoverflow.com/a/39320379"">https://stackoverflow.com/a/39320379</a> but I have a string sentence instead of the NLP tree.</p>

<p>BTW, I am using python 3</p>
",Parsing & POS Tagging,get parse nlp tree object bracketed parse string nltk spacy sentence could say regularly catch shower add exhilaration joie de vivre achieve get nlp parse tree like following example want replicate solution question href string sentence instead nlp tree p btw using python
Type Error: &#39;NoneType&#39; object is not scriptable,"<p>I am trying to Generate the possible <strong>permutations and combinations</strong> of sentences from the given raw input statement using Spacy.io. At first, I generated the <strong>Dependency Parse</strong> tree and started traversing the tree.</p>
<p>Dependency Tree for the input statement: <a href=""https://demos.explosion.ai/displacy/?text=So%20I%20am%20not%20asking%20you%20to%20decide%20anything%20right%20now%20because%20I%20just%20called%20to%20give%20you%20some%20information%20so%20that%20anytime%20may%20be%20now%20or%20%20%20in%20the%20near%20future%20if%20you%20plan%20to%20expand%20your%20business%20or%20do%20some%20renovations%20or%20purchase%20any%20equipment%20or%20get%20another%20store%20or%20anything%20like%20that%20you%20can%20contact%20us%20at%20that%20time%20okay&amp;model=en_core_web_sm&amp;cpu=0&amp;cph=0"" rel=""nofollow noreferrer"">Click here to see the Dependency Tree</a></p>
<p>The below is the sample code I used to traverse the tree.</p>
<pre><code>def traverse_the_tree(doc,root,init_track,call_from):
    generated_sentence = [[],[],[]]
    subjects = list(root.lefts)
    objects = list(root.rights)
    #traverse through LeftSubTree
    for left_ST in subjects:
       generated_sentence[0].append(left_ST)
    #append the root
    generated_sentence[1].append(root)
    #traverse through RightSubTree
    for right_ST in objects:
        #check if the SubTree
        if(check_if_has_subtree(right_ST)): #return True or False
             generated_sentence[2].append(right_ST)
             #write this outerfile
             with open(&quot;output.txt&quot;,&quot;a&quot;) as fp:
                  fp.write(&quot;{},{},{}&quot;.format(generated_sentence[0],generated_sentence[1],generated_sentence[2]))    
        else:
             traverse_the_tree(doc,right_ST,init_track,call_from)
</code></pre>
<p>This is the major syntax of the code. Though my original code stills describe a lot.I post my full code here<a href=""https://github.com/Madhivarman/Clause-IR-Extraction/blob/master/main.py"" rel=""nofollow noreferrer""> Click here for Code</a>.</p>
<p>When I run this code I get the following traceback error:</p>
<blockquote>
<p>Traceback (most recent call last):   File &quot;main.py&quot;, line 183, in
</p>
<pre><code>generate_the_sentence(doc,root,init_track,call_from)   File &quot;main.py&quot;, line 161, in generate_the_sentence

generate_the_sentence(doc,right_ST,init_track,call_from)   File &quot;main.py&quot;, line 155, in generate_the_sentence

write_to_output_file(subj_rel[0],subj_rel[1],[right_ST]) TypeError: 'NoneType' object is not subscriptable
</code></pre>
</blockquote>
<p>Sample output I want to Generate, from the input sentence. <a href=""https://docs.google.com/document/d/152hl2cMDA82i9X9Ubxman2WJVSjyXsWfYa5aIBLbEG4/edit?usp=sharing"" rel=""nofollow noreferrer"">Click here to see the Docs</a></p>
<p>Thanks!</p>
",Parsing & POS Tagging,type error nonetype object scriptable trying generate possible permutation combination sentence given raw input statement using spacy io first generated dependency parse tree started traversing tree dependency tree input statement click see dependency tree sample code used traverse tree major syntax code though original code still describe lot post full code click code run code get following traceback error traceback recent call last file main py line sample output want generate input sentence click see doc thanks
n-grams from text in python,"<p>An update to my previous <a href=""https://stackoverflow.com/questions/49064114/extracting-n-grams-from-tweets-in-python"">post</a>, with some changes:
<br>
<br>
Say that I have 100 tweets.
In those tweets, I need to extract: 1) food names, and 2) beverage names. I also need to attach type (drink or food) and an id-number (each item has a unique id) for each extraction.
<br>
<br>
I already have a lexicon with names, type and id-number:</p>

<pre><code>lexicon = {
'dr pepper': {'type': 'drink', 'id': 'd_123'},
'coca cola': {'type': 'drink', 'id': 'd_234'},
'cola': {'type': 'drink', 'id': 'd_345'},
'banana': {'type': 'food', 'id': 'f_456'},
'banana split': {'type': 'food', 'id': 'f_567'},
'cream': {'type': 'food', 'id': 'f_678'},
'ice cream': {'type': 'food', 'id': 'f_789'}}
</code></pre>

<p><br></p>

<p>Tweet example:</p>

<p>After various processing of ""tweet_1"" I have this sentences:</p>

<pre><code>sentences = [
'dr pepper is better than coca cola and suits banana split with ice cream', 
'coca cola and banana is not a good combo']
</code></pre>

<p>My requested output (can be other <em>type</em> than <em>list</em>):</p>

<pre><code>[""tweet_id_1"",
 [[[""dr pepper""], [""drink"", ""d_124""]],
  [[""coca cola""], [""drink"", ""d_234""]],
  [[""banana split""], [""food"", ""f_567""]],
  [[""ice cream""], [""food"", ""f_789""]]],

 ""tweet_id_1"",,
 [[[""coca cola""], [""drink"", ""d_234""]],
  [[""banana""], [""food"", ""f_456""]]]]
</code></pre>

<p>It's important that the output should <strong>NOT</strong> extract unigrams within ngrams (n>1):</p>

<pre><code>[""tweet_id_1"",
 [[[""dr pepper""], [""drink"", ""d_124""]],
  [[""coca cola""], [""drink"", ""d_234""]],
  [[""cola""], [""drink"", ""d_345""]],
  [[""banana split""], [""food"", ""f_567""]],
  [[""banana""], [""food"", ""f_456""]],
  [[""ice cream""], [""food"", ""f_789""]],
  [[""cream""], [""food"", ""f_678""]]],

 ""tweet_id_1"",
 [[[""coca cola""], [""drink"", ""d_234""]],
  [[""cola""], [""drink"", ""d_345""]],
  [[""banana""], [""food"", ""f_456""]]]]
</code></pre>

<p><br></p>

<p><br></p>

<p>Ideally, I would like to be able to run my sentences in various nltk filters like lemmatize() and pos_tag() <strong>BEFORE</strong> the extraction to get an output like the following. But with this regexp solution, if I do that, then all the words are split into unigrams, or they will generate 1 unigram and 1 bigram from the string ""coca cola"", which would generate the output that I did not want to have (as the example above).
The ideal output (again the <em>type</em> of the output is not important):</p>

<pre><code>[""tweet_id_1"",
 [[[(""dr pepper"", ""NN"")], [""drink"", ""d_124""]],
  [[(""coca cola"", ""NN"")], [""drink"", ""d_234""]],
  [[(""banana split"", ""NN"")], [""food"", ""f_567""]],
  [[(""ice cream"", ""NN"")], [""food"", ""f_789""]]],

 ""tweet_id_1"",
 [[[(""coca cola"", ""NN"")], [""drink"", ""d_234""]],
  [[(""banana"", ""NN"")], [""food"", ""f_456""]]]]
</code></pre>
",Parsing & POS Tagging,n gram text python update previous requested output type list important output extract unigrams within ngrams n ideally would like able run sentence various nltk filter like lemmatize po tag extraction get output like following regexp solution word split unigrams generate unigram bigram string coca cola would generate output want example ideal output type output important
Using LIBLINEAR in transition-based dependency parsing,"<p>I am going to do some work for transition-based dependency parsing using LIBLINEAR. But I am confused how to utilize it. As follows:</p>

<p>I set 3 feature templates for my training&amp;testing processes of transition-based dependency parsing: </p>

<pre><code>1. the word in the top of the stack
2. the word in the front of the queue
3. information from the current tree formed with the steps
</code></pre>

<p>And the feature defined in LIBLINEAR is:</p>

<pre><code>FeatureNode(int index, double value)
</code></pre>

<p>Some examples like:</p>

<pre><code>LABEL       ATTR1   ATTR2   ATTR3   ATTR4   ATTR5
-----       -----   -----   -----   -----   -----
1           0       0.1     0.2     0       0
2           0       0.1     0.3    -1.2     0
1           0.4     0       0       0       0
2           0       0.1     0       1.4     0.5
3          -0.1    -0.2     0.1     1.1     0.1
</code></pre>

<p>But I want to define my features like(one sentence 'I love you') at some stage:</p>

<pre><code>feature template 1: the word is 'love' 
feature template 2: the word is 'you'
feature template 3: the information is - the left son of 'love' is 'I'
</code></pre>

<p>Does it mean I must define features with LIBLINEAR like:    -------FORMAT 1
(indexes in vocabulary: 0-I, 1-love, 2-you)</p>

<pre><code>LABEL       ATTR1(template1)   ATTR2(template2)   ATTR3(template3)
-----       -----              -----              -----
SHIFT           1                 2                   0
(or LEFT-arc, 
 RIGHT-arc)
</code></pre>

<p>But I have go thought some statements of others, I seem to define feature in binary so I have to define a words vector like:
    ('I', 'love', 'you'), when 'you' appears for example, the vector will be (0, 0, 1)</p>

<p>So the features in LIBLINEAR may be:   -------FORMAT 2</p>

<pre><code>LABEL       ATTR1('I')   ATTR2('love')   ATTR3('love')
-----       -----              -----              -----
SHIFT           0                 1                   0       -&gt;denoting the feature template 1
(or LEFT-arc, 
 RIGHT-arc)
SHIFT           0                 0                   1       -&gt;denoting the feature template 2
(or LEFT-arc, 
 RIGHT-arc)
SHIFT           1                 0                   0       -&gt;denoting the feature template 3
(or LEFT-arc, 
 RIGHT-arc)
</code></pre>

<p>Which is correct between FORMAT 1 and 2?</p>

<p>Is there some something I have mistaken?</p>
",Parsing & POS Tagging,using liblinear transition based dependency parsing going work transition based dependency parsing using liblinear confused utilize follows set feature template training testing process transition based dependency parsing feature defined liblinear example like want define feature like one sentence love stage doe mean must define feature liblinear like format index vocabulary love go thought statement others seem define feature binary define word vector like love appears example vector feature liblinear may format correct format something mistaken
Creating combination of different elements in a list,"<p>I have a list of words: </p>

<pre><code>['The', 'president', 'control'...].
</code></pre>

<p>And I have different POS tags for each word. For example: </p>

<pre><code>{The: {DT}, president: {NP}, control: {NP, VB}...}.
</code></pre>

<p>Now I need to compute all combinations possible for the statement, for example:</p>

<pre><code>[The_DT, president_NP, control_NP] --&gt; Ist Combination
[The_DT, president_NP, control_VB]  --&gt; 2nd Combination
</code></pre>

<p>Any help on how can I do so?</p>
",Parsing & POS Tagging,creating combination different element list list word different po tag word example need compute combination possible statement example help
NLP - Sentence does not follow any of the grammar rule in Syntactic parsing,"<p>I have grammar extracted from Treebank dataset from nltk library. Now the parser will use these rules to parse the sentence. So for example I have grammar like below:</p>

<pre><code>A-&gt;B C
B-&gt;'b'
C-&gt;'c'
D-&gt;'d'
</code></pre>

<p>Now suppose I have sentence like <code>b c</code> so the parser will make parse tree like below</p>

<pre><code>    A
   / \
  B   C
  |   |
  b   c
</code></pre>

<p>So like this I have all the grammar from training dataset. Now for testing assume a completely new sentence is there like <code>""c d""</code></p>

<p>Parse tree for above sentence will be</p>

<pre><code>  C   D
  |   |
  c   c
</code></pre>

<p>and  the parser will stop as there is no rule for <code>A-&gt;C D</code></p>

<p>So how to parse these kind of sentences because training grammar does not have any grammar like <code>A -&gt; C D</code></p>

<p>So the question is how to parse sentence if it completely new and grammar rule is not present in training data set? I am using probabilistic grammar.</p>
",Parsing & POS Tagging,nlp sentence doe follow grammar rule syntactic parsing grammar extracted treebank dataset nltk library parser use rule parse sentence example grammar like suppose sentence like parser make parse tree like like grammar training dataset testing assume completely new sentence like parse tree sentence parser stop rule parse kind sentence training grammar doe grammar like question parse sentence completely new grammar rule present training data set using probabilistic grammar
Creating own POS Tagger,"<p>I have found the <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""noreferrer"">Stanford POS Tagger</a> pretty good, but somehow I found myself in need of creating my own POS tagger.</p>

<p>For the last two weeks, I am rambling here and there, on whether to start from parsing tree, or once we have a pos tagger than we can parse tree, using ugly CFGs and NFAs so that they can help me in creating a POS tagger and what not.</p>

<p>I am ending the question here, asking seniors, where to begin POS tagging.
(language of choice is Python, but C and JAVA won't hurt).</p>
",Parsing & POS Tagging,creating po tagger found stanford po tagger pretty good somehow found need creating po tagger last two week rambling whether start parsing tree po tagger parse tree using ugly cfgs nfas help creating po tagger ending question asking senior begin po tagging language choice python c java hurt
How to Traverse an NLTK Tree object?,"<p>Given a bracketed parse, I could convert it into a Tree object in NLTK as such:</p>

<pre><code>&gt;&gt;&gt; from nltk.tree import Tree
&gt;&gt;&gt; s = '(ROOT (S (NP (NNP Europe)) (VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends)))) (. .)))'
&gt;&gt;&gt; Tree.fromstring(s)
Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NNP', ['Europe'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('PP', [Tree('IN', ['in']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['same']), Tree('NNS', ['trends'])])])]), Tree('.', ['.'])])])
</code></pre>

<p>But when I try to traverse it, I can only access the top most Tree:</p>

<pre><code>&gt;&gt;&gt; for i in Tree.fromstring(s):
...     print i
... 
(S
  (NP (NNP Europe))
  (VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends))))
  (. .))
&gt;&gt;&gt; for i in Tree.fromstring(s):
...     print i, i.label()
... 
(S
  (NP (NNP Europe))
  (VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends))))
  (. .)) S
&gt;&gt;&gt; 
</code></pre>

<p>I could go one level deep as follows:</p>

<pre><code>&gt;&gt;&gt; for i in Tree.fromstring(s):
...     print i.subtrees()
... 
&lt;generator object subtrees at 0x7f1eb1571410&gt;
&gt;&gt;&gt; for i in Tree.fromstring(s):
...     for j in i.subtrees():
...             print j
... 
(S
  (NP (NNP Europe))
  (VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends))))
  (. .))
(NP (NNP Europe))
(NNP Europe)
(VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends))))
(VBZ is)
(PP (IN in) (NP (DT the) (JJ same) (NNS trends)))
(IN in)
(NP (DT the) (JJ same) (NNS trends))
(DT the)
(JJ same)
(NNS trends)
(. .)
</code></pre>

<p>But is there a way to traverse all subtrees depth wise?</p>

<p><strong>How should one traverse a tree in NLTK?</strong></p>

<p><strong>How to traverse all subtrees in NLTK?</strong></p>
",Parsing & POS Tagging,traverse nltk tree object given bracketed parse could convert tree object nltk try traverse access top tree could go one level deep follows way traverse subtrees depth wise one traverse tree nltk traverse subtrees nltk
Why Spacy api version and web version results are different?,"<p>I am using spacy for pos_ tags (parts of speech tags) and Here is my very simple approach :</p>

<pre><code>import spacy
model=spacy.load('en_core_web_sm')

for i in model('I want so sit on Dining table set.'):
    print((i,i.pos_))
</code></pre>

<p>output is:</p>

<pre><code>(I, 'PRON')
(want, 'VERB')
(so, 'ADV')
(sit, 'VERB')
(on, 'ADP')
(Dining, 'PROPN')
(table, 'NOUN')
(set, 'VERB')
(., 'PUNCT')
</code></pre>

<p>Now if i go to <a href=""https://demos.explosion.ai/displacy/?text=I%20want%20so%20sit%20on%20Dining%20table%20set.&amp;model=en_core_web_sm&amp;cpu=1&amp;cph=1"" rel=""nofollow noreferrer"">web version</a> of spacy and give same query then  i am getting something like this :</p>

<p><a href=""https://i.sstatic.net/QruAB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QruAB.png"" alt=""enter image description here""></a></p>

<p>Why <code>Dining</code> is <code>PROPN</code> in import version model but in web version its <code>Noun</code> .</p>

<p>My confusion is here when both model are same then why i am not getting web version result? web-version result is more correct and that is my expected output. </p>

<p>If i am getting right then the reason is web-version is going through many queries daily and that's why its updating its weights from those queries and that's why its more smart ? or my hypothesis doesn't make any sense?</p>

<p>I thought using bs4 or selenium with phanthon for using result of web-version but that's not correct way. How i can acheive web-version result ?</p>
",Parsing & POS Tagging,spacy api version web version result different using spacy po tag part speech tag simple approach output go web version spacy give query getting something like import version model web version confusion model getting web version result web version result correct expected output getting right reason web version going many query daily updating weight query smart hypothesis make sense thought using b selenium phanthon using result web version correct way acheive web version result
How can a tree be encoded as input to a neural network?,"<p>I have a tree, specifically a parse tree with tags at the nodes and strings/words at the leaves. I want to pass this tree as input into a neural network all the while preserving its structure.</p>

<p>Current approach
Assume we have some dictionary of words w1,w2.....wn
Encode the words that appear in the parse tree as n dimensional binary vectors with a 1 showing up in the ith spot whenever the word in the parse tree is wi</p>

<p>Now how about the tree structure? There are about 2^n  possible parent tags for n words that appear at the leaves So we cant set a max length of input words and then just brute force enumerate all trees.</p>

<p>Right now all i can think of is to approximate the tree by choosing the direct parent of a leaf. This can be represented by a binary vector as well with dimension equal to number of different types of tags - on the order of ~ 100 i suppose.
My input is then two dimensional. The first is just the vector representation of a word and the second is the vector representation of its parent tag</p>

<p>Except this will lose a lot of the structure in the sentence. Is there a standard/better way of solving this problem?</p>
",Parsing & POS Tagging,tree encoded input neural network tree specifically parse tree tag node string word leaf want pas tree input neural network preserving structure current approach assume dictionary word w w wn encode word appear parse tree n dimensional binary vector showing ith spot whenever word parse tree wi tree structure n possible parent tag n word appear leaf cant set max length input word brute force enumerate tree right think approximate tree choosing direct parent leaf represented binary vector well dimension equal number different type tag order suppose input two dimensional first vector representation word second vector representation parent tag except lose lot structure sentence standard better way solving problem
what&#39;s the difference between pos_tag and UnigramTagger and BigramTagger in nltk?,"<p>I am trying to get my hands dirty on nltk. I am referring <a href=""http://victoria.lviv.ua/../NaturalLanguageProcessingWithPython.pdf"" rel=""nofollow noreferrer"">http://victoria.lviv.ua/../NaturalLanguageProcessingWithPython.pdf</a>. It states that <code>nltk.pos_tag</code> function assigns parts of speech to each word in the list of words, passed to it as argument. </p>

<p>Moving ahead, I found that there's also <code>nltk.DefaultTagger</code>, <code>nltk.RegexpTagger</code>,  <code>nltk.UnigramTagger</code> and  <code>nltk.BigramTagger</code>. </p>

<p>I am confused over, why we require these taggers, since <code>nltk.pos_tag</code> is doing good job of tagging parts of speech. Moreover, which tagger does <code>nltk.pos_tag</code> uses internally for tagging.</p>

<p>Thanks in advance.</p>
",Parsing & POS Tagging,difference po tag unigramtagger bigramtagger nltk trying get hand dirty nltk referring state function assigns part speech word list word passed argument moving ahead found also confused require tagger since good job tagging part speech moreover tagger doe us internally tagging thanks advance
Does NLTK have its own Dependency Parser that gives similar results as Stanford Parser?,"<p>I want to know, does NLTK has its own Dependency Parser (I am not talking about Stanford Dependency Parser in NLTK) that produces somewhat similar results as of Stanford parser. If yes, please let me know what parser in NLTK gives similar results as NLTK has many parsers. </p>

<p>Thanks in advance :) </p>
",Parsing & POS Tagging,doe nltk dependency parser give similar result stanford parser want know doe nltk ha dependency parser talking stanford dependency parser nltk produce somewhat similar result stanford parser yes please let know parser nltk give similar result nltk ha many parser thanks advance
Stanford nlp: Parse Tree,"<p>I have this sentence: <code>My dog also likes eating sausage.</code></p>

<p>And I get the following parse tree:</p>

<pre><code>(ROOT
 (S
   (NP (PRP$ My) (NN dog))
   (ADVP (RB also))
   (VP (VBZ likes)
     (S
       (VP (VBG eating)
        (NP (NN sausage)))))
(. .)))
</code></pre>

<p>How do I get only the grammatical category, namely: NP,ADVP,VP, etc?</p>

<p>I tried with this code:</p>

<pre><code>  Tree t=sentence.get(TreeAnnotation.class);
  t.labels();
</code></pre>
",Parsing & POS Tagging,stanford nlp parse tree sentence get following parse tree get grammatical category namely np advp vp etc tried code
How to do a BFS on a verb phrase and extract all the dependent(first level or immediate dependent) noun tags in Stanford CoreNLP,"<p><a href=""https://i.sstatic.net/sio6d.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/sio6d.png"" alt=""enter image description here""></a></p>

<p>In this dependency tree I want to do one level BFS and extract a structure where VP/VBZ is the root node and the dependent NP/ NNP are the child nodes.
How can I parse the tree like this? Please help.</p>

<p>E.g, ""makes"" is the VP/VBZ then the immediate dependent Noun phrases are Bell and Products. Then suppose ""electronic"" were another verb phrase then I want to extract computer and building which are NPs.</p>
",Parsing & POS Tagging,bfs verb phrase extract dependent first level immediate dependent noun tag stanford corenlp dependency tree want one level bfs extract structure vp vbz root node dependent np nnp child node parse tree like please help e g make vp vbz immediate dependent noun phrase bell product suppose electronic another verb phrase want extract computer building np
Understanding grammar for Parse trees,"<p>I am just trying to understand a natural language interface for relational databases proposed by <a href=""http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjKo7O5rLTXAhWIIuwKHe8nA88QFggpMAA&amp;url=http%3A%2F%2Fwww.vldb.org%2Fpvldb%2Fvol8%2Fp73-li.pdf&amp;usg=AOvVaw21lOyEjhU8_uoIhbUtmmOg"" rel=""nofollow noreferrer"">Fei Li (2014)</a> (available as <a href=""https://github.com/DukeNLIDB/NLIDB"" rel=""nofollow noreferrer"">github project</a>). Specifically I don't understand the grammar they define for a ParseTree of a natural language query to a database. This question is somewhat like <a href=""https://stackoverflow.com/questions/23849261/generating-grammar-rules-for-nltk-parse-trees"">this one</a> but with a more complex grammar.</p>

<p>Background: A natural language sentence can be parsed as ParseTree (common library for this is the <a href=""https://nlp.stanford.edu/software/stanford-dependencies.shtml"" rel=""nofollow noreferrer"">Stanford Parser</a>) which describes the grammatical relations of words in a sentence. </p>

<p>The grammar for a valid ParseTree is:</p>

<ol>
<li><p>Q -> (SClause)(ComplexCondition)* </p></li>
<li><p>SClause -> SELECT + GNP</p></li>
<li><p>ComplexCondition -> ON + (leftSubtree*rightSubtree)</p></li>
<li><p>leftSubtree -> GNP</p></li>
<li><p>rightSubtree -> GNP | VN | MIN | MAX </p></li>
<li><p>GNP -> (FN + GNP) | NP</p></li>
<li><p>NP -> NN + (NN)*(condition)*</p></li>
<li><p>condition -> VN | (ON + VN)</p></li>
</ol>

<p>where </p>

<ul>
<li><code>Q</code> represents an entire query tree</li>
<li><code>+</code> a parent-child relationship </li>
<li><code>*</code> a sibling relationship</li>
<li><code>SN</code> is a SELECT node</li>
<li><code>ON</code> is an OPERATOR node (e.g. <code>=</code>, <code>&lt;=</code>)</li>
<li><code>FN</code> is a FUNCTION node (e.g. <code>AVG</code>)</li>
<li><code>NN</code> is a NAME node (e.g. a column in a db table)</li>
<li><code>VN</code> is a VALUE node (i.e. a value in a column in a db table)</li>
<li><code>ComplexCondition</code> must have one <code>ON</code> with a <code>leftSubtree</code> and <code>rightSubtree</code></li>
<li><code>NP</code> is one <code>NN</code> whose children are multiple <code>NN</code>s and conditions.</li>
</ul>

<p><b>My questions</b>:</p>

<ol>
<li><p>why is <code>condition</code> defined as <code>VN</code> or (<code>ON</code>+<code>VN</code>)? This would mean that something like the digit <code>5</code> by itself can be a condition. Would make more sense of only the latter, i.e. (<code>ON</code>+<code>VN</code>) is a condition (e.g. <code>&gt;5</code>)</p></li>
<li><p>How can a <code>rightSubtree</code> just be a function (e.g. <code>MIN</code>). Btw I am understanding the pipe <code>|</code> as logical or.</p></li>
<li><p>I understand that <code>GNP</code> is recursively defined but at a terminal node the <code>GNP</code> node must be just a <code>NP</code> node right? But a <code>NP</code> is defined as something that has children... HOW?!?!?!</p></li>
<li><p>The authors of the github project quoted above state: ""Take a Value Node (<code>VN</code>) for example, according to the grammar, it is invalid if and only if it has children. I don't know how to infer this from the grammar</p></li>
</ol>

<p>Thanks for the help</p>
",Parsing & POS Tagging,understanding grammar parse tree trying understand natural language interface relational database proposed fei li available github project specifically understand grammar define parsetree natural language query database question somewhat like stanford parser describes grammatical relation word sentence grammar valid parsetree q sclause complexcondition sclause select gnp complexcondition leftsubtree rightsubtree leftsubtree gnp rightsubtree gnp vn min max gnp fn gnp np np nn nn condition condition vn vn represents entire query tree parent child relationship sibling relationship select node operator node e g function node e g name node e g column db table value node e value column db table must one one whose child multiple condition question defined would mean something like digit condition would make sense latter e condition e g function e g btw understanding pipe logical understand recursively defined terminal node node must node right defined something ha child author github project quoted state take value node example according grammar invalid ha child know infer grammar thanks help
Stanford CoreNLP Dependency Parser Usage with Unsupported Languages,"<p>I am trying to train CoreNLP's NN based dependency parser in Turkish. I have found the command below in the documentation of the parser:</p>

<pre><code>Train a parser with CoNLL treebank data: java    edu.stanford.nlp.parser.nndep.DependencyParser -trainFile trainPath
-devFile devPath -embedFile wordEmbeddingFile -embeddingSize wordEmbeddingDimensionality -model modelOutputFile.txt.gz
</code></pre>

<p>I couldn't exactly figure out what the modelOutputFile is. It is stated in the documentation that this file is written in the training phase. Is modelOutputFile a pregenerated file that I should create or just an empty file that will be written automatically in the training phase?</p>

<p>Any help will be appreciated, thank you!</p>
",Parsing & POS Tagging,stanford corenlp dependency parser usage unsupported language trying train corenlp nn based dependency parser turkish found command documentation parser exactly figure modeloutputfile stated documentation file written training phase modeloutputfile pregenerated file create empty file written automatically training phase help appreciated thank
Combining nltk.RegexpParser grammars,"<p>As my next step towards learning more about NLP, I'm trying to implement a simple heuristic that improves results beyond simple n-grams. </p>

<p>Per the Stanford Collocations PDF linked below they mention that passing ""candidate phrases through a part of-speech filter which only lets through those patterns that are likely to be “phrases"""" will produce better results than simply using the most frequently occuring bi-grams. 
Source: Collocations, page 143 - 144: <a href=""https://nlp.stanford.edu/fsnlp/promo/colloc.pdf"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/fsnlp/promo/colloc.pdf</a> </p>

<p>The table on page 144 has 7 tag patterns. In order, the NLTK POS tag equivalent is:</p>

<p>JJ NN</p>

<p>NN</p>

<p>JJ JJ NN</p>

<p>JJ NN NN</p>

<p>NN JJ NN</p>

<p>NN NN NN</p>

<p>NN IN NN</p>

<p>In the code below, I can get the desired result when I <em>independently</em> apply each grammar below. However when I try to combine the same grammars I don't receive the desired result. </p>

<p>In my code, you can see that I uncomment one sentence, uncomment 1 grammar, run it and check the result. </p>

<p>I should be able to combine all sentences, run it through the combined grammar (just 3 of them in the code below) and get the desired results.</p>

<blockquote>
  <p>My question is, how do I correctly combine grammars?</p>
</blockquote>

<p>I'm assuming that combining grammars is like an 'OR', find this pattern, OR this pattern... </p>

<p>Thanks in advance. </p>

<pre><code>import nltk

# The following sentences are correctly grouped with &lt;JJ&gt;*&lt;NN&gt;+. 
# Should see: 'linear function', 'regression coefficient', 'Gaussian random variable' and 
# 'cumulative distribution function'
SampleSentence = ""In mathematics, the term linear function refers to two distinct, although related, notions""
#SampleSentence = ""The regression coefficient is the slope of the line of the regression equation.""
#SampleSentence = ""In probability theory, Gaussian random variable is a very common continuous probability distribution.""
#SampleSentence = ""In probability theory and statistics, the cumulative distribution function (CDF) of a real-valued random variable X, or just distribution function of X, evaluated at x, is the probability that X will take a value less than or equal to x.""

# The following sentences are correctly grouped with &lt;NN.?&gt;*&lt;V.*&gt;*&lt;NN&gt;
# Should see 'mean squared error' and # 'class probability function'. 
#SampleSentence = ""In statistics, the mean squared error (MSE) of an estimator measures the average of the squares of the errors, that is, the difference between the estimator and what is estimated.""
#SampleSentence = ""The class probability function is interesting""

# The sentence below is correctly grouped with &lt;NN.?&gt;*&lt;IN&gt;*&lt;NN.?&gt;*. 
# should see 'degrees of freedom'.
#SampleSentence = ""In statistics, the degrees of freedom is the number of values in the final calculation of a statistic that are free to vary.""

SampleSentence = SampleSentence.lower()

print(""\nFull sentence: "", SampleSentence, ""\n"")

tokens = nltk.word_tokenize(SampleSentence)
textTokens = nltk.Text(tokens)    

# Determine the POS tags.
POStagList = nltk.pos_tag(textTokens)    

# The following grammars work well *independently*
grammar = ""NP: {&lt;JJ&gt;*&lt;NN&gt;+}""
#grammar = ""NP: {&lt;NN.?&gt;*&lt;V.*&gt;*&lt;NN&gt;}""    
#grammar = ""NP: {&lt;NN.?&gt;*&lt;IN&gt;*&lt;NN.?&gt;*}""


# Merge several grammars above into a single one below. 
# Note that all 3 correct grammars above are included below. 

'''
grammar = """"""
            NP: 
                {&lt;JJ&gt;*&lt;NN&gt;+}
                {&lt;NN.?&gt;*&lt;V.*&gt;*&lt;NN&gt;}
                {&lt;NN.?&gt;*&lt;IN&gt;*&lt;NN.?&gt;*}
        """"""
'''

cp = nltk.RegexpParser(grammar)

result = cp.parse(POStagList)

for subtree in result.subtrees(filter=lambda t: t.label() == 'NP'):
    print(""NP Subtree:"", subtree)    
</code></pre>
",Parsing & POS Tagging,combining nltk regexpparser grammar next step towards learning nlp trying implement simple heuristic improves result beyond simple n gram per stanford collocation pdf linked mention passing candidate phrase part speech filter let pattern likely phrase produce better result simply using frequently occuring bi gram source collocation page table page ha tag pattern order nltk po tag equivalent jj nn nn jj jj nn jj nn nn nn jj nn nn nn nn nn nn code get desired result independently apply grammar however try combine grammar receive desired result code see uncomment one sentence uncomment grammar run check result able combine sentence run combined grammar code get desired result question correctly combine grammar assuming combining grammar like find pattern pattern thanks advance
Detecting first/second/third person pronouns,"<p>I'm looking for a way to detect whether a personal pronoun is first person (I), second person (you) or third person (they). The code is looking to see if someone is talking about themselves, but has some other applications too.</p>

<p>A python library would be fantastic, but not necessary. nltk.pos_tag will tell me what are personal pronouns, but I can't seem to get more information than that.</p>

<p>Does something like this exist?</p>
",Parsing & POS Tagging,detecting first second third person pronoun looking way detect whether personal pronoun first person second person third person code looking see someone talking ha application python library would fantastic necessary nltk po tag tell personal pronoun seem get information doe something like exist
How to train a new parser model for Stanford NLP from treebank?,"<p>I have downloaded the <a href=""http://stp.lingfil.uu.se/~mojgan/UPDT.html"" rel=""nofollow"">UPDT</a> Persian treebank (<a href=""http://stp.lingfil.uu.se/~mojgan/UPDT.html"" rel=""nofollow"">Uppsala Persian Dependency Treebank</a>) and I am trying to build a dependency parser model from it using Stanford NLP. I have tried train the model using both command line and Java code, but I get exceptions in both cases. </p>

<p>1- Train a model using command line:</p>

<pre><code>java -mx1500m edu.stanford.nlp.parser.lexparser.LexicalizedParser -train UPDT\train.conll 0 -saveToSerializedFile UPDT\updt.model.ser.gz
</code></pre>

<p>When I run the above command I will get this exception:</p>

<pre><code>done [read 26 trees]. Time elapsed: 0 ms
Options parameters:
useUnknownWordSignatures 0
smoothInUnknownsThreshold 100
smartMutation false
useUnicodeType false
unknownSuffixSize 1
unknownPrefixSize 1
flexiTag false
useSignatureForKnownSmoothing false
wordClassesFile null
parserParams edu.stanford.nlp.parser.lexparser.EnglishTreebankParserParams
forceCNF false
doPCFG true
doDep true
freeDependencies false
directional true
genStop true
distance true
coarseDistance false
dcTags true
nPrune false
Train parameters:
 smooth=false
 PA=true
 GPA=false
 selSplit=false
 (0.0)
 mUnary=0
 mUnaryTags=false
 sPPT=false
 tagPA=false
 tagSelSplit=false (0.0)
 rightRec=false
 leftRec=false
 collinsPunc=false
 markov=false
 mOrd=1
 hSelSplit=false (10)
 compactGrammar=0
 postPA=false
 postGPA=false
 selPSplit=false (0.0)
 tagSelPSplit=false (0.0)
 postSplitWithBase=false
 fractionBeforeUnseenCounting=0.5
 openClassTypesThreshold=50
 preTransformer=null
 taggedFiles=null
 predictSplits=false
 splitCount=1
 splitRecombineRate=0.0
 simpleBinarizedLabels=false
 noRebinarization=false
 trainingThreads=1
 dvKBest=100
 trainingIterations=40
 batchSize=25
 regCost=1.0E-4
 qnIterationsPerBatch=1
 qnEstimates=15
 qnTolerance=15.0
 debugOutputFrequency=0
 randomSeed=0
 learningRate=0.1
 deltaMargin=0.1
 unknownNumberVector=true
 unknownDashedWordVectors=true
 unknownCapsVector=true
 unknownChineseYearVector=true
 unknownChineseNumberVector=true
 unknownChinesePercentVector=true
 dvSimplifiedModel=false
 scalingForInit=0.5
 maxTrainTimeSeconds=0
 unkWord=*UNK*
 lowercaseWordVectors=false
 transformMatrixType=DIAGONAL
 useContextWords=false
 trainWordVectors=true
 stalledIterationLimit=12
 markStrahler=false

Using EnglishTreebankParserParams splitIN=0 sPercent=false sNNP=0 sQuotes=false
sSFP=false rbGPA=false j#=false jJJ=false jNounTags=false sPPJJ=false sTRJJ=fals
e sJJCOMP=false sMoreLess=false unaryDT=false unaryRB=false unaryPRP=false reflP
RP=false unaryIN=false sCC=0 sNT=false sRB=false sAux=0 vpSubCat=false mDTV=0 sV
P=0 sVPNPAgr=false sSTag=0 mVP=false sNP%=0 sNPPRP=false dominatesV=0 dominatesI
=false dominatesC=false mCC=0 sSGapped=0 numNP=false sPoss=0 baseNP=0 sNPNNP=0 s
TMP=0 sNPADV=0 cTags=false rightPhrasal=false gpaRootVP=false splitSbar=0 mPPTOi
IN=0 cWh=0
Binarizing trees...done. Time elapsed: 12 ms
Extracting PCFG...PennTreeReader: warning: file has extra non-matching right par
enthesis [ignored]
Exception in thread ""main"" java.lang.IllegalArgumentException: No head rule defi
ned for _ using class edu.stanford.nlp.trees.ModCollinsHeadFinder in (_
  DELM
  DELM
  DELM
  13
  punct
  _
  _
  15
  ??????
  _
  N
  N_SING
  SING
  13
  appos
  _
  _
  16
  ???????
  _
  ADJ
  ADJ
  ADJ
  15
  amod
  _
  _
  17
  ??
  _
  P
  P
  P
  15
  prep
  _
  _
  18
  ???
  _
  N
  N_SING
  SING
  17
  pobj
  _
  _
  19
  ?
  _
  CON
  CON
  CON
  18
  cc
  _
  _
  20
  ????
  _
  N
  N_SING
  SING
  18
  conj
  _
  _
  21
  ????
  _
  N
  N_SING
  SING
  20
  poss/pc
  _
  _
  22)
    at edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineNonTrivialH
ead(AbstractCollinsHeadFinder.java:242)
     at edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead(Abstra
ctCollinsHeadFinder.java:189)
     at edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead(Abstra
ctCollinsHeadFinder.java:140)
     at edu.stanford.nlp.parser.lexparser.TreeAnnotator.transformTreeHelper(T
reeAnnotator.java:145)
     at edu.stanford.nlp.parser.lexparser.TreeAnnotator.transformTree(TreeAnn
otator.java:51)
     at edu.stanford.nlp.parser.lexparser.TreeAnnotatorAndBinarizer.transform
Tree(TreeAnnotatorAndBinarizer.java:104)
     at edu.stanford.nlp.trees.CompositeTreeTransformer.transformTree(Composi
teTreeTransformer.java:30)
     at edu.stanford.nlp.trees.TransformingTreebank$TransformingTreebankItera
tor.next(TransformingTreebank.java:195)
     at edu.stanford.nlp.trees.TransformingTreebank$TransformingTreebankItera
tor.next(TransformingTreebank.java:176)
     at edu.stanford.nlp.trees.FilteringTreebank$FilteringTreebankIterator.pr
imeNext(FilteringTreebank.java:100)
     at edu.stanford.nlp.trees.FilteringTreebank$FilteringTreebankIterator.&lt;i
nit&gt;(FilteringTreebank.java:85)
     at edu.stanford.nlp.trees.FilteringTreebank.iterator(FilteringTreebank.j
ava:72)
     at edu.stanford.nlp.parser.lexparser.AbstractTreeExtractor.tallyTrees(Ab
stractTreeExtractor.java:64)
     at edu.stanford.nlp.parser.lexparser.AbstractTreeExtractor.extract(Abstr
actTreeExtractor.java:89)
     at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromTree
bank(LexicalizedParser.java:881)
     at edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(LexicalizedP
arser.java:1394)
</code></pre>

<p>2- Train the model using Java code:</p>

<pre><code>import java.io.File;
import java.io.IOException;
import java.util.Collection;
import java.util.List;

import edu.stanford.nlp.ling.HasWord;
import edu.stanford.nlp.parser.lexparser.LexicalizedParser;
import edu.stanford.nlp.parser.lexparser.Options;
import edu.stanford.nlp.process.DocumentPreprocessor;
import edu.stanford.nlp.trees.GrammaticalStructure;
import edu.stanford.nlp.trees.GrammaticalStructureFactory;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.trees.Treebank;
import edu.stanford.nlp.trees.TreebankLanguagePack;


public class FromTreeBank {

    public static void main(String[] args) throws IOException {
        // TODO Auto-generated method stub

        String treebankPathUPDT = ""src/model/UPDT.1.2/train.conll"";
        String persianFilePath  = ""src/txt/persianSentences.txt"";

        File file = new File(treebankPathUPDT);

        Options op = new Options();   
        Treebank tr = op.tlpParams.diskTreebank();
        tr.loadPath(file);    
        LexicalizedParser lpc = LexicalizedParser.trainFromTreebank(tr,op);

        //Once the lpc is trained, use it to parse a file which contains Persian text  
        //demoDP(lpc, persianFilePath);
    }


    public static void demoDP(LexicalizedParser lp, String filename) {
        // This option shows loading, sentence-segmenting and tokenizing
        // a file using DocumentPreprocessor.
        TreebankLanguagePack tlp = lp.treebankLanguagePack(); // a PennTreebankLanguagePack for English
        GrammaticalStructureFactory gsf = null;
        if (tlp.supportsGrammaticalStructures()) {
            gsf = tlp.grammaticalStructureFactory();
        }
        // You could also create a tokenizer here (as below) and pass it
        // to DocumentPreprocessor
        for (List&lt;HasWord&gt; sentence : new DocumentPreprocessor(filename)) {
            Tree parse = lp.apply(sentence);
            parse.pennPrint();
            System.out.println();
            if (gsf != null) {
                GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
                Collection tdl = gs.typedDependenciesCCprocessed();
                System.out.println(tdl);
                System.out.println();
            }
        }
    }

}
</code></pre>

<p>The Above Java program also make this exception:</p>

<pre><code>Options parameters:
useUnknownWordSignatures 0
smoothInUnknownsThreshold 100
smartMutation false
useUnicodeType false
unknownSuffixSize 1
unknownPrefixSize 1
flexiTag false
useSignatureForKnownSmoothing false
wordClassesFile null
parserParams edu.stanford.nlp.parser.lexparser.EnglishTreebankParserParams
forceCNF false
doPCFG true
doDep true
freeDependencies false
directional true
genStop true
distance true
coarseDistance false
dcTags true
nPrune false
Train parameters:
 smooth=false
 PA=true
 GPA=false
 selSplit=false
 (0.0)
 mUnary=0
 mUnaryTags=false
 sPPT=false
 tagPA=false
 tagSelSplit=false (0.0)
 rightRec=false
 leftRec=false
 collinsPunc=false
 markov=false
 mOrd=1
 hSelSplit=false (10)
 compactGrammar=0
 postPA=false
 postGPA=false
 selPSplit=false (0.0)
 tagSelPSplit=false (0.0)
 postSplitWithBase=false
 fractionBeforeUnseenCounting=0.5
 openClassTypesThreshold=50
 preTransformer=null
 taggedFiles=null
 predictSplits=false
 splitCount=1
 splitRecombineRate=0.0
 simpleBinarizedLabels=false
 noRebinarization=false
 trainingThreads=1
 dvKBest=100
 trainingIterations=40
 batchSize=25
 regCost=1.0E-4
 qnIterationsPerBatch=1
 qnEstimates=15
 qnTolerance=15.0
 debugOutputFrequency=0
 randomSeed=0
 learningRate=0.1
 deltaMargin=0.1
 unknownNumberVector=true
 unknownDashedWordVectors=true
 unknownCapsVector=true
 unknownChineseYearVector=true
 unknownChineseNumberVector=true
 unknownChinesePercentVector=true
 dvSimplifiedModel=false
 scalingForInit=0.5
 maxTrainTimeSeconds=0
 unkWord=*UNK*
 lowercaseWordVectors=false
 transformMatrixType=DIAGONAL
 useContextWords=false
 trainWordVectors=true
 stalledIterationLimit=12
 markStrahler=false

Using EnglishTreebankParserParams splitIN=0 sPercent=false sNNP=0 sQuotes=false sSFP=false rbGPA=false j#=false jJJ=false jNounTags=false sPPJJ=false sTRJJ=false sJJCOMP=false sMoreLess=false unaryDT=false unaryRB=false unaryPRP=false reflPRP=false unaryIN=false sCC=0 sNT=false sRB=false sAux=0 vpSubCat=false mDTV=0 sVP=0 sVPNPAgr=false sSTag=0 mVP=false sNP%=0 sNPPRP=false dominatesV=0 dominatesI=false dominatesC=false mCC=0 sSGapped=0 numNP=false sPoss=0 baseNP=0 sNPNNP=0 sTMP=0 sNPADV=0 cTags=false rightPhrasal=false gpaRootVP=false splitSbar=0 mPPTOiIN=0 cWh=0
Binarizing trees...done. Time elapsed: 122 ms
Extracting PCFG...PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
java.lang.IllegalArgumentException: No head rule defined for _ using class edu.stanford.nlp.trees.ModCollinsHeadFinder in (_
  DELM
  DELM
  DELM
  13
  punct
  _
  _
  15
  تلفیقی
  _
  N
  N_SING
  SING
  13
  appos
  _
  _
  16
  طنزآمیز
  _
  ADJ
  ADJ
  ADJ
  15
  amod
  _
  _
  17
  از
  _
  P
  P
  P
  15
  prep
  _
  _
  18
  اسم
  _
  N
  N_SING
  SING
  17
  pobj
  _
  _
  19
  و
  _
  CON
  CON
  CON
  18
  cc
  _
  _
  20
  شیوه
  _
  N
  N_SING
  SING
  18
  conj
  _
  _
  21
  کارش
  _
  N
  N_SING
  SING
  20
  poss/pc
  _
  _
  22)


    at edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineNonTrivialHead(AbstractCollinsHeadFinder.java:242)
    at edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead(AbstractCollinsHeadFinder.java:189)
    at edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead(AbstractCollinsHeadFinder.java:140)
    at edu.stanford.nlp.parser.lexparser.TreeAnnotator.transformTreeHelper(TreeAnnotator.java:145)
    at edu.stanford.nlp.parser.lexparser.TreeAnnotator.transformTree(TreeAnnotator.java:51)
    at edu.stanford.nlp.parser.lexparser.TreeAnnotatorAndBinarizer.transformTree(TreeAnnotatorAndBinarizer.java:104)
    at edu.stanford.nlp.trees.CompositeTreeTransformer.transformTree(CompositeTreeTransformer.java:30)
    at edu.stanford.nlp.trees.TransformingTreebank$TransformingTreebankIterator.next(TransformingTreebank.java:195)
    at edu.stanford.nlp.trees.TransformingTreebank$TransformingTreebankIterator.next(TransformingTreebank.java:176)
    at edu.stanford.nlp.trees.FilteringTreebank$FilteringTreebankIterator.primeNext(FilteringTreebank.java:100)
    at edu.stanford.nlp.trees.FilteringTreebank$FilteringTreebankIterator.&lt;init&gt;(FilteringTreebank.java:85)
    at edu.stanford.nlp.trees.FilteringTreebank.iterator(FilteringTreebank.java:72)
    at edu.stanford.nlp.parser.lexparser.AbstractTreeExtractor.tallyTrees(AbstractTreeExtractor.java:64)
    at edu.stanford.nlp.parser.lexparser.AbstractTreeExtractor.extract(AbstractTreeExtractor.java:89)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromTreebank(LexicalizedParser.java:881)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.trainFromTreebank(LexicalizedParser.java:267)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.trainFromTreebank(LexicalizedParser.java:278)
    at FromTreeBank.main(FromTreeBank.java:46)
</code></pre>

<p>Actually, I am not sure if the command-line or Java code is correct or not. I can't figure out what is missing in the command-line or the Java code, I would be the most grateful if someone tell me why I get these exceptions and what is wrong? Or suggest any better way to train a model from a treebank. </p>

<p>Thank you</p>
",Parsing & POS Tagging,train new parser model stanford nlp treebank downloaded updt persian treebank uppsala persian dependency treebank trying build dependency parser model using stanford nlp tried train model using command line java code get exception case train model using command line run command get exception train model using java code java program also make exception actually sure command line java code correct figure missing command line java code would grateful someone tell get exception wrong suggest better way train model treebank thank
Getting numbered dependency triples in python from stanford dependency parser,"<p>this question has been making me think for quite some time and i did not find a satisfying solution for it.</p>

<p>Well, right now I am using the Stanford dependency parser in Python and the following code gives me this output.</p>

<pre><code>phrase=""If there is a moose in the oven, is there also an elephant?""
dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)
test = dependency_parser.raw_parse(phrase)
dep= test.next()


list(dep.triples())
</code></pre>

<p>((u'is', u'VBZ'), u'advcl', (u'is', u'VBZ'))</p>

<p>((u'is', u'VBZ'), u'mark', (u'If', u'IN'))</p>

<p>((u'is', u'VBZ'), u'expl', (u'there', u'EX'))</p>

<p>and so on... </p>

<p>But what i actually need is some representation which includes the number of occcurence within the original sentence, because the final application will consist of long sentences with multiple occurences of the same words. 
Something like:</p>

<p>mark(is-3, If-1)</p>

<p>Thank you in advance for any idea on how to generate such an output!</p>
",Parsing & POS Tagging,getting numbered dependency triple python stanford dependency parser question ha making think quite time find satisfying solution well right using stanford dependency parser python following code give output u u vbz u advcl u u vbz u u vbz u mark u u u u vbz u expl u u ex actually need representation includes number occcurence within original sentence final application consist long sentence multiple occurences word something like mark thank advance idea generate output
Write negated string according to POS tags,"<p>Consider the following POS-tagged string:</p>

<pre><code>It/PRP was/VBD not/RB okay/JJ or/CC funny/JJ and/CC I/NN will/MD never/RB buy/VB 
from/IN them/PRP ever/RB again/RB

(It was not okay or funny and I will never buy from them ever again)
</code></pre>

<p>I want to accomplish the following:</p>

<ol>
<li>Check for negating adverbs (RB) against defined array('not', 'never')</li>
<li>When there's a match, remove the adverb</li>
<li>concatenate ""not-"" to the beginning of every subsequent adjective (JJ), adverb (RB), or verb (VB or VBN for past tense)</li>
<li>Remove all POS-tags (/XX)</li>
</ol>

<p>Thus, the desired output would be:</p>

<pre><code>It was not-okay or not-funny and I will not-buy from them not-ever not-again
</code></pre>

<p>My first thought was to do this the way I know how to: explode the string on space, then explode every word on ""/"" to [JJ => okay], then make a switch statement to treat every word (case JJ: concatenate, etc.), but this seems very sloppy. Does anybody have a more clean and / or efficient way of doing this, for instance regex? The strings have been pre-cleaned, so they will always only contain words (no punctuation, other characters than a-z, etc.).</p>

<p><em>Edit: I am aware, btw, of the very basic character of this way of treating negations, but it is good enough for what I need. There will be an error margin, but that's ok :)</em></p>
",Parsing & POS Tagging,write negated string according po tag consider following po tagged string want accomplish following check negating adverb rb defined array never match remove adverb concatenate beginning every subsequent adjective jj adverb rb verb vb vbn past tense remove po tag xx thus desired output would first thought wa way know explode string space explode every word jj okay make switch statement treat every word case jj concatenate etc seems doe anybody clean efficient way instance regex string pre cleaned always contain word punctuation character z etc edit aware btw basic character way treating negation good enough need error margin ok
Extracting key phrases from a short fragment,"<p>While there are tons of information on how to extract keywords/phrases from documents, I could not find any technique on how to extract key phrases from fragments (not necessarily sentences). Here are some examples:</p>

<ul>
<li>Art Museums and galleries in China -> Museums and galleries Naval   </li>
<li>Battles Of The Russo-Japanese War -> Naval Battles, The       Russo-Japanese War</li>
</ul>

<p>One could suggest to simply use NLP toolkit and parse the tree and extract the noun phrases. I wonder if there are any better approaches.</p>
",Parsing & POS Tagging,extracting key phrase short fragment ton information extract keywords phrase document could find technique extract key phrase fragment necessarily sentence example art museum gallery china museum gallery naval battle russo japanese war naval battle russo japanese war one could suggest simply use nlp toolkit parse tree extract noun phrase wonder better approach
Parse a penn syntax tree to extract its grammar rules,"<p>I have a PENN-Syntax-Tree and I would like to recursively get all rules that this tree contains.  </p>

<pre><code>(ROOT 
(S 
   (NP (NN Carnac) (DT the) (NN Magnificent)) 
   (VP (VBD gave) (NP ((DT a) (NN talk))))
)
)
</code></pre>

<p>my target is to get the grammar rules like:</p>

<pre><code>ROOT --&gt; S
S --&gt; NP VP
NP --&gt; NN
...
</code></pre>

<p>As I said I need to do this <strong>recursively and without the NLTK Package or any other modules or regular expression</strong>. Here's what I have so far. The parameter <code>tree</code> is a Penn-Tree splitted on each space. </p>

<pre><code>def extract_rules(tree):
    tree = tree[1:-1]
    print(""\n\n"")

    if len(tree) == 0:
        return

    root_node = tree[0]
    print(""Current Root: ""+root_node)

    remaining_tree = tree[1:]
    right_side = []

    temp_tree = list(remaining_tree)
    print(""remaining_tree: "", remaining_tree)
    symbol = remaining_tree.pop(0)

    print(""Symbol: ""+symbol)

    if symbol not in [""("", "")""]:
        print(""CASE: No Brackets"")
        print(""Rule: ""+root_node+"" --&gt; ""+str(symbol))

        right_side.append(symbol)

    elif symbol == ""("":
        print(""CASE: Opening Bracket"")
        print(""Temp Tree: "", temp_tree)
        cursubtree_end = bracket_depth(temp_tree)
        print(""Subtree ends at position ""+str(cursubtree_end)+"" and Element is ""+temp_tree[cursubtree_end])
        cursubtree_start = temp_tree.index(symbol)

        cursubtree = temp_tree[cursubtree_start:cursubtree_end+1]
        print(""Subtree: "", cursubtree)

        rnode = extract_rules(cursubtree)
        if rnode:
            right_side.append(rnode)
            print(""Rule: ""+root_node+"" --&gt; ""+str(rnode))

    print(right_side)
    return root_node


def bracket_depth(tree):
    counter = 0
    position = 0
    subtree = []

    for i, char in enumerate(tree):
        if char == ""("":
            counter = counter + 1
        if char == "")"":
            counter = counter - 1

        if counter == 0 and i != 0:
            counter = i
            position = i
            break

    subtree = tree[0:position+1]

    return position
</code></pre>

<p>Currently it works for the first subtree of <code>S</code> but all other subtrees are not getting parsed recursively. Would be glad for any help..</p>
",Parsing & POS Tagging,parse penn syntax tree extract grammar rule penn syntax tree would like recursively get rule tree contains target get grammar rule like said need recursively without nltk package module regular expression far parameter penn tree splitted space currently work first subtree subtrees getting parsed recursively would glad help
Stanford CoreNLP: nndep.DependencyParser in pipeline with geman model,"<p>i want to use the nndep in CoreNLP for dependency Parsing. So the Input is a simple german sentence and the output should be like this:</p>

<pre><code>case(Schulen-3, An-1)
amod(Schulen-3, Stuttgarter-2)
nmod(gegrüßt-13, Schulen-3)
aux(gegrüßt-13, darf-4)
case(MitschülerInnen-7, wegen-5)
amod(MitschülerInnen-7, muslimischer-6)
nmod(gegrüßt-13, MitschülerInnen-7)
neg(gegrüßt-13, nicht-8)
advmod(nicht-8, mehr-9)
case(Gott-12, mit-10)
amod(Gott-12, Grüß-11)
nmod(gegrüßt-13, Gott-12)
root(ROOT-0, gegrüßt-13)
auxpass(gegrüßt-13, werden-14)
punct(gegrüßt-13, .-15)
</code></pre>

<p>and this command is working for a single file:</p>

<pre><code>java -cp ""*"" -Xmx2g edu.stanford.nlp.parser.nndep.DependencyParser -model edu/stanford/nlp/models/parser/nndep/UD_German.gz -textFile /Users/.../input.txt
</code></pre>

<p>But I need to to this with 60.000 files. So i need the nlp.pipeline. If i execute the following command, the output is only the normal parse tree but not the parsed dependencies.</p>

<pre><code>java -Xmx6g -cp ""*:."" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP  -filelist /Users/.../filelist.txt -props StanfordCoreNLP-german.properties -outputFormat text -parse.originalDependencies
</code></pre>

<p>Can someone help?</p>
",Parsing & POS Tagging,stanford corenlp nndep dependencyparser pipeline geman model want use nndep corenlp dependency parsing input simple german sentence output like command working single file need file need nlp pipeline execute following command output normal parse tree parsed dependency someone help
How to use OpenNLP to get POS tags in R?,"<p>Here is the R Code:</p>

<pre><code>library(NLP) 
library(openNLP)
tagPOS &lt;-  function(x, ...) {
s &lt;- as.String(x)
word_token_annotator &lt;- Maxent_Word_Token_Annotator()
a2 &lt;- Annotation(1L, ""sentence"", 1L, nchar(s))
a2 &lt;- annotate(s, word_token_annotator, a2)
a3 &lt;- annotate(s, Maxent_POS_Tag_Annotator(), a2)
a3w &lt;- a3[a3$type == ""word""]
POStags &lt;- unlist(lapply(a3w$features, `[[`, ""POS""))
POStagged &lt;- paste(sprintf(""%s/%s"", s[a3w], POStags), collapse = "" "")
list(POStagged = POStagged, POStags = POStags)}
str &lt;- ""this is a the first sentence.""
tagged_str &lt;-  tagPOS(str)
</code></pre>

<p>Output is :</p>

<blockquote>
  <p>tagged_str
      $POStagged
      [1]""this/DT is/VBZ a/DT the/DT first/JJ sentence/NN ./.""</p>
</blockquote>

<p>Now I want to extract only NN word i.e sentence from the above sentence and want to store it into a variable .Can anyone help me out with this .</p>
",Parsing & POS Tagging,use opennlp get po tag r r code output tagged str postagged dt vbz dt dt first jj sentence nn want extract nn word e sentence sentence want store variable anyone help
Parse nltk chunk string to form Tree,"<p>I have a file containing Strings like </p>

<pre><code>Tree('S', [Tree('NP', [('criminal', 'JJ'), ('lawyer', 'NN')]), Tree('NP', 
[('new', 'JJ'), ('york', 'NN')])])
</code></pre>

<p>Is there a python function that parse the string to produce Tree structure again? I tried the <a href=""http://www.nltk.org/_modules/nltk/tree.html#Tree.fromstring"" rel=""nofollow noreferrer"">Tree.fromstring</a> function but it doesn't parse.</p>

<p>I generate these strings like below</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; from nltk import pos_tag
&gt;&gt;&gt; pattern = """"""NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}
... VBD: {&lt;VBD&gt;}
... IN: {&lt;IN&gt;}""""""
&gt;&gt;&gt; NPChunker = nltk.RegexpParser(pattern)
&gt;&gt;&gt; sentence = 'criminal lawyer new york'.split()
&gt;&gt;&gt; pos_tag(sentence)
[('criminal', 'JJ'), ('lawyer', 'NN'), ('new', 'JJ'), ('york', 'NN')]
&gt;&gt;&gt; result = NPChunker.parse(pos_tag(sentence))
&gt;&gt;&gt; result
Tree('S', [Tree('NP', [('criminal', 'JJ'), ('lawyer', 'NN')]), Tree('NP', 
[('new', 'JJ'), ('york', 'NN')])])
</code></pre>

<p>Thanks in advance.</p>
",Parsing & POS Tagging,parse nltk chunk string form tree file containing string like python function parse string produce tree structure tried tree fromstring function parse generate string like thanks advance
Part of speech tagging in OpenNLP vs. StanfordNLP,"<p>I'm new to part of speech (pos) taging and I'm doing a pos tagging on a text document. I'm considering using either OpenNLP or StanfordNLP for this. For StanfordNLP I'm using a <code>MaxentTagger</code> and I use <code>english-left3words-distsim.tagger</code> to train it. In OpenNLP I'm using <code>POSModel</code> and train it using <code>en-pos-maxent.bin</code>. How these two taggers (<code>MaxentTagger</code> and <code>POSTagger</code>) and the training sets (<code>english-left3words-distsim.tagger</code> and <code>en-pos-maxent.bin</code>) are different and which one is usually giving a better result.</p>
",Parsing & POS Tagging,part speech tagging opennlp v stanfordnlp new part speech po taging po tagging text document considering using either opennlp stanfordnlp stanfordnlp using use train opennlp using train using two tagger training set different one usually giving better result
Python tagging and lemmatizing,"<p>I've been reading through methods of tagging and lemmatizing for the past few days - and one issue I'm coming across is the POS tagging to lemmatization phase. </p>

<p>I've successfully managed to POS tag text using the <a href=""http://www.nltk.org/_modules/nltk/tag/perceptron.html"" rel=""nofollow noreferrer"">Perceptron tagger</a>, however when I move to lemmatize the text, due to the clear issues with POS labels, the <a href=""http://www.nltk.org/api/nltk.stem.html"" rel=""nofollow noreferrer"">Wordnet lemmatizer</a> fails. </p>

<p>I have read through SO workarounds involving relabeling tags using WordNet labels (like <a href=""https://stackoverflow.com/questions/5364493/lemmatizing-pos-tagged-words-with-nltk"">here</a> and <a href=""https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python"">also here</a>) - however I wonder about the efficiency of such a process. For tagging and then lemmatizing entire documents, or even larger texts, does the relabeling not pose an issue for slowing the whole event?</p>

<p>Is there no WordNet tagger that matches its own lemmatizer? Alternatively, are there other lemmatizers that match their tagger?</p>

<p>Any advice appreciated. </p>

<p>*edit: Perceptron tagger <a href=""https://stackoverflow.com/questions/38578100/what-is-the-tagset-for-nltk-perceptron-tagger"">pulls from treebank</a>, removed the example.</p>
",Parsing & POS Tagging,python tagging lemmatizing reading method tagging lemmatizing past day one issue coming across po tagging lemmatization phase successfully managed po tag text using perceptron tagger however move lemmatize text due clear issue po label wordnet lemmatizer fails read workarounds involving relabeling tag using wordnet label like however wonder efficiency process tagging lemmatizing entire document even larger text doe relabeling pose issue slowing whole event wordnet tagger match lemmatizer alternatively lemmatizers match tagger advice appreciated edit perceptron tagger href treebank removed example
Convert Averaged Perceptron Tagger POS to WordNet POS and Avoid Tuple Error,"<p>I have code for POS tagging with NLTK's averaged perceptron tagger:</p>

<pre><code>from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.tokenize import word_tokenize

string = 'dogs runs fast'

tokens = word_tokenize(string)
tokensPOS = pos_tag(tokens)
print(tokensPOS)
</code></pre>

<p>Result:</p>

<pre><code>[('dogs', 'NNS'), ('runs', 'VBZ'), ('fast', 'RB')]
</code></pre>

<p>I have attempted code for looping through each tagged token and lemmatizing it with the WordNet lemmatizer:</p>

<pre><code>lemmatizedWords = []
for w in tokensPOS:
       lemmatizedWords.append(WordNetLemmatizer().lemmatize(w))

print(lemmatizedWords)
</code></pre>

<p>Resulting Error:</p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-30-462d7c3bdbb7&gt;"", line 15, in &lt;module&gt;
    lemmatizedWords = WordNetLemmatizer().lemmatize(w)

  File ""C:\Users\taca\AppData\Local\Continuum\Anaconda3\lib\site-packages\nltk\stem\wordnet.py"", line 40, in lemmatize
    lemmas = wordnet._morphy(word, pos)

  File ""C:\Users\taca\AppData\Local\Continuum\Anaconda3\lib\site-packages\nltk\corpus\reader\wordnet.py"", line 1712, in _morphy
    forms = apply_rules([form])

  File ""C:\Users\taca\AppData\Local\Continuum\Anaconda3\lib\site-packages\nltk\corpus\reader\wordnet.py"", line 1692, in apply_rules
    for form in forms

  File ""C:\Users\taca\AppData\Local\Continuum\Anaconda3\lib\site-packages\nltk\corpus\reader\wordnet.py"", line 1694, in &lt;listcomp&gt;
    if form.endswith(old)]

AttributeError: 'tuple' object has no attribute 'endswith'
</code></pre>

<p>I think I have two problems here:</p>

<ol>
<li>The POS tags are not converted to tags WordNet can understand (I tried implementing something similar to this answer <a href=""https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python"">wordnet lemmatization and pos tagging in python</a> with no success)</li>
<li>The data structures are not correctly formed to be able to loop through each tuple (I couldn't find much on this error beyond <code>os</code> related code)</li>
</ol>

<p>How do I follow up POS tagging with lemmatization to avoid these errors?</p>
",Parsing & POS Tagging,convert averaged perceptron tagger po wordnet po avoid tuple error code po tagging nltk averaged perceptron tagger result attempted code looping tagged token lemmatizing wordnet lemmatizer resulting error think two problem po tag converted tag wordnet understand tried implementing something similar answer href lemmatization po tagging python success data structure correctly formed able loop tuple find much error beyond related code follow po tagging lemmatization avoid error
Dependency Parsing graph for a paragraph,"<p>I am working on a NLP project. I want to create a dependency parsing graph for an entire paragraph, instead of sentence. Is there an existing method for the same? </p>
",Parsing & POS Tagging,dependency parsing graph paragraph working nlp project want create dependency parsing graph entire paragraph instead sentence existing method
Python 3 NLTK How to Perform Operations on the type &#39;nltk.tree.Tree&#39; Chunk Extraction,"<p>Let me preface this question by saying that I may not be using the correct datatype for the type of operation that I want, so I am open to any answer.</p>

<p>I am processing some text using NLTK and I am particularly interested in extracting text if it meets a certain chunking condition.</p>

<p>If the text meets the condition of the chunk I would like to extract it into a separate list or similar datatype as a string.</p>

<p>Here is my code:</p>

<pre><code> import nltk

 from nltk.tokenize import sent_tokenize, word_tokenize



 def tokenize():
     sent_tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()
     tokenized = sent_tokenizer.tokenize('I went to the store.')
     return  tokenized

 def chunking():
     for i in (tokenize()):
        words = nltk.word_tokenize(i)
    tagged = nltk.pos_tag(words)
    chunkGram =r""""""CHUNK: {&lt;PRP&gt;&lt;VBD&gt;&lt;TO&gt;&lt;DT&gt;&lt;NN&gt;}""""""
    chunkParser = nltk.RegexpParser(chunkGram)
    chunked = chunkParser.parse(tagged)
    print(type(chunked))
    print(chunked)

 chunking()
</code></pre>

<p>In this example, I know that I am simply printing the text, but I want to the store matches the chunk gram perfectly. However, I cannot seem to do any operations on the matches and their type is of 'nltk.tree.Tree'. I have looked at some other questions on here but they seem to be more advanced for what I am trying to do. There must be some way to manipulate the tree or extract chunks.</p>

<p>I read some of the documentation of 'nltk.tree.Tree', and its pretty complicated and covers areas that I do not want to do yet.</p>

<p>The end goal is to loop though a bunch of text for chunks and if I find them output them to a list as a string (without the POS tag). </p>

<pre><code> extracted_by_chunking = ['I went to the store.', 'etc', 'etc']
</code></pre>

<p>So far I am able to pinpoint the information I need using chunking but I just cannot extract it. The task seems pretty straightforward, but I not sure where to go from here, and any help would be greatly appreciated.</p>

<p>Thanks!</p>
",Parsing & POS Tagging,python nltk perform operation type nltk tree tree chunk extraction let preface question saying may using correct datatype type operation want open answer processing text using nltk particularly interested extracting text meet certain chunking condition text meet condition chunk would like extract separate list similar datatype string code example know simply printing text want store match chunk gram perfectly however seem operation match type nltk tree tree looked question seem advanced trying must way manipulate tree extract chunk read documentation nltk tree tree pretty complicated cover area want yet end goal loop though bunch text chunk find output list string without po tag far able pinpoint information need using chunking extract task seems pretty straightforward sure go help would greatly appreciated thanks
"Replacing only pronoun, noun, verb and adjective in a sentence with its corresponding tags, how could I do it efficiently in Python?","<p>I have sentences and I want to replace only its Pronoun, nouns and adjective with its corresponding POS Tags. 
For example my sentence is :</p>

<blockquote>
  <p>""I am going to the most beautiful city, Islamabad""</p>
</blockquote>

<p>and want the result</p>

<blockquote>
  <p>""PRP am VBG to the most JJ NN, NNP"". </p>
</blockquote>
",Parsing & POS Tagging,replacing pronoun noun verb adjective sentence corresponding tag could efficiently python sentence want replace pronoun noun adjective corresponding po tag example sentence going beautiful city islamabad want result prp vbg jj nn nnp
NLTK - Remove Tags From Parsed Chunks,"<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import nltk
import re

from nltk.tree import *
from nltk.chunk.util import tagstr2tree
from nltk import word_tokenize, pos_tag

text = ""Yarın, Mehmet ile birlikte Ankara'da ki Nüfus Müdürlüğü'ne, Aziz 
Yıldırım ile birlikte, Şükrü Saraçoğlu Stadı'na gideceğiz."".decode(""utf-8"")

tagged_text = pos_tag(word_tokenize(text))
tagged_text2 = word_tokenize(text)

grammar = ""NP:{&lt;NNP&gt;+}""

cp = nltk.RegexpParser(grammar)
result = cp.parse(tagged_text)

for tree in result:
    print(tree)

wrapped = ""(ROOT ""+ str(result) + "" )""  # Add a ""root"" node at the top
trees = nltk.Tree.fromstring(wrapped, read_leaf=lambda x: x.split(""/"")[0])

for tree in trees:
    print(tree.leaves())

for tree2 in result:
    print(nltk.Tree.fromstring(str(tree2), read_leaf=lambda x: x.split(""/"")[0]))
</code></pre>

<p>The Output:</p>

<pre><code>(NP Yar\u0131n/NNP)
(u',', ',')
(NP Mehmet/NNP)
(u'ile', 'NN')
(u'birlikte', 'NN')
(NP Ankara'da/NNP ki/NNP Nufus/NNP Mudurlugu'ne/NNP)
(u',', ',')
(NP Aziz/NNP Y\u0131ld\u0131r\u0131m/NNP)
(u'ile', 'NN')
(u'birlikte', 'NN')
(u',', ',')
(NP Sukru/NNP Saracoglu/NNP Stad\u0131'na/NNP)
(u'gidece\u011fiz', 'NN')
(u'.', '.')


['Yar\\u0131n', ',', 'Mehmet', 'ile', 'birlikte', ""Ankara'da"", 'ki', 'Nufus', ""Mudurlugu'ne"", ',', 'Aziz', 'Y\\u0131ld\\u0131r\\u0131m', 'ile', 'birlikte', ',', 'Sukru', 'Saracoglu', ""Stad\\u0131'na"", 'gidecegiz', '.']


(NP Yar\u0131n)
(u',', ',')
(NP Mehmet)
(u'ile', 'NN')
(u'birlikte', 'NN')
(NP Ankara'da ki Nufus Mudurlugu'ne)
(u',', ',')
(NP Aziz Y\u0131ld\u0131r\u0131m)
(u'ile', 'NN')
(u'birlikte', 'NN')
(u',', ',')
(NP Sukru Saracoglu Stad\u0131'na)
(u'gidece\u011fiz', 'NN')
(u'.', '.')
</code></pre>

<p>I referenced from :<a href=""https://stackoverflow.com/questions/33705555/how-can-i-remove-pos-tags-before-slashes-in-nltk"">How can I remove POS tags before slashes in nltk?</a></p>

<p>I want to grouping proper names and remove the tags but when i used the solution it effects the whole text and after that my chunk parse is gone. I really tried the understand the tree structure but how can i apply the the removing function in for statement. I want my Output like:</p>

<p><strong>My desired output:</strong></p>

<pre><code>[Yar\u0131n]
[,]
[Mehmet]
[ile]
[birlikte]
[Ankara'da ki Nufus Mudurlugu'ne]
...
...
</code></pre>

<p>Also i can't deal with utf-8 as you see my output is full of non-ascii characters. How can i deal with it ?</p>

<p><strong>EDIT:</strong></p>

<pre><code>for i in range(len(tree)):
    arr.append(nltk.Tree.fromstring(str(tree[i]), read_leaf=lambda x: x.split(""/"")[0]).leaves())
    print(arr[i])
</code></pre>

<p>I found what shoul i write in the code but now i have the following error. I think i can't append punctuations on my array. </p>

<pre><code>['Yar\\u0131n']
Traceback (most recent call last):
  File ""./chunk2.py"", line 61, in &lt;module&gt;
    arr.append(nltk.Tree.fromstring(str(tree[i]), read_leaf=lambda x: x.split(""/"")[0]).leaves())
  File ""/usr/local/lib/python2.7/dist-packages/nltk/tree.py"", line 630, in fromstring
    cls._parse_error(s, match, open_b)
  File ""/usr/local/lib/python2.7/dist-packages/nltk/tree.py"", line 675, in _parse_error
    raise ValueError(msg)
ValueError: Tree.read(): expected u'(' but got ','
            at index 0.
                "",""
                 ^
</code></pre>
",Parsing & POS Tagging,nltk remove tag parsed chunk output referenced href remove po tag slash nltk want grouping proper name remove tag used solution effect whole text chunk parse gone really tried understand tree structure apply removing function statement want output like desired output also deal utf see output full non ascii character deal edit found shoul write code following error think append punctuation array
Getting index of subtree in Stanford&#39;s parse trees,"<p>I am trying to get the index of a subtree in a parsed sentence so that I can remove it.  Here is my code:</p>

<pre><code>{
    Queue&lt;Tree&gt; queue = new LinkedList&lt;Tree&gt;();
    queue.add(tree);
    while (!queue.isEmpty()) {
        Tree current = queue.poll();
        if (current.label().toString().equals(""CC"")) {
            List&lt;Tree&gt; siblings = current.siblings(tree);

            String leaf = current.getChildrenAsList().get(0).label().toString();
            Tree rightNode = siblings.get(1);
            Tree leftNode = siblings.get(0);
            if (leaf.contains(""and"") &amp;&amp;
                    rightNode.label().toString().contains(""S"")) {
                int index = rightNode.index();
                System.out.println(""index "" + index);
                System.out.println(""returning index: "" + tree.getChild(index).label().toString());  // testing to make sure it's the right node
            }
        }
        List&lt;Tree&gt; children = current.getChildrenAsList();
        for (Tree child : children) {
            if (!child.isLeaf()) {
                queue.add(child);
            }
        }
    }
}
</code></pre>

<p>I've tried every iteration of objectIndexOf that I can think of:</p>

<pre><code>int index = tree.objectIndexOf(rightNode);
int index = current.objectIndexOf(rightNode);
int index = rightNode.objectIndexOf(current);
int index = rightNode.objectIndexOf(tree);
</code></pre>

<p>but every time I get a -1.  My program is correctly hitting the conditionals so clearly the ""s"" node is in that tree as a sibling to ""CC."" But I can't get it to return an index so that I may remove it.</p>

<p>I'm not sure where I'm going wrong here.</p>
",Parsing & POS Tagging,getting index subtree stanford parse tree trying get index subtree parsed sentence remove code tried every iteration objectindexof think every time get program correctly hitting conditionals clearly node tree sibling cc get return index may remove sure going wrong
How to group sentences by edit distance?,"<p>I have a large set (36k sentence) of sentences (text list) and their POS tags (POS list), and I'd like to group/cluster the elements in the POS list using edit distance/Levenshtein:</p>

<p>(e.g Sentx POS tags= [CC DT VBZ RB JJ], Senty POS tags= [CC DT VBZ RB JJ] ) are in cluster edit distance =0, </p>

<p>while ([CC DT <strong>VBZ</strong> RB JJ], [CC DT <strong>VB</strong> RB JJ]) are in cluster edit distance =1. </p>

<p>I understand how the clustering algorithms work but I am confused how to approach such a problem in python and how to store the clusters in data structures so that I can retrieve them easily.</p>

<p>I tried to create a matrix (measuring the distance of each sentence with all the sentences in the corpus) but it takes very long to be processed.</p>
",Parsing & POS Tagging,group sentence edit distance large set k sentence sentence text list po tag po list like group cluster element po list using edit distance levenshtein e g sentx po tag cc dt vbz rb jj senty po tag cc dt vbz rb jj cluster edit distance cc dt vbz rb jj cc dt vb rb jj cluster edit distance understand clustering algorithm work confused approach problem python store cluster data structure retrieve easily tried create matrix measuring distance sentence sentence corpus take long processed
Noun Phrase Extraction Regular Expression,"<p>What is the interpretation of the RE given below</p>

<pre><code>r'KT: {(&lt;JJ&gt;* &lt;NN.*&gt;+ &lt;IN&gt;)? &lt;JJ&gt;* &lt;NN.*&gt;+}'
</code></pre>

<p>I don't know what KT is but JJ is adjective, NN is noun and IN is preposition.</p>

<p>EDIT: reposting the link
<a href=""http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/"" rel=""nofollow noreferrer"">http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/</a></p>
",Parsing & POS Tagging,noun phrase extraction regular expression interpretation given know kt jj adjective nn noun preposition edit reposting link
How to search between ranges in list of lists?,"<p>I want to find out the POS tags occurring between two ranges which are values of indices of NNP tags.</p>

<pre><code>data = [[('User', 'NNP'),
  ('is', 'VBG'),
  ('not', 'RB'),
  ('able', 'JJ'),
  ('to', 'TO'),
  ('order', 'NN'),
  ('products', 'NNS'),
  ('from', 'IN'),
  ('iShopCatalog', 'NN'),
  ('Coala', 'NNP'),
  ('excluding', 'VBG'),
  ('articles', 'NNS'),
  ('from', 'IN'),
  ('VWR', 'NNP')],
 [('Arfter', 'NNP'),
  ('transferring', 'VBG'),
  ('the', 'DT'),
  ('articles', 'NNS'),
  ('from', 'IN'),
  ('COALA', 'NNP'),
  ('to', 'TO'),
  ('SRM', 'VB'),
  ('the', 'DT'),
  ('Category', 'NNP'),
  ('S9901', 'NNP'),
  ('Dummy', 'NNP'),
  ('is', 'VBZ'),
  ('maintained', 'VBN')],
 [('Due', 'JJ'),
  ('to', 'TO'),
  ('this', 'DT'),
  ('the', 'DT'),
  ('user', 'NN'),
  ('is', 'VBZ'),
  ('not', 'RB'),
  ('able', 'JJ'),
  ('to', 'TO'),
  ('order', 'NN'),
  ('the', 'DT'),
  ('product', 'NN')],
 [('All', 'DT'),
  ('other', 'JJ'),
  ('users', 'NNS'),
  ('can', 'MD'),
  ('order', 'NN'),
  ('these', 'DT'),
  ('articles', 'NNS')],
 [('She', 'PRP'),
  ('can', 'MD'),
  ('order', 'NN'),
  ('other', 'JJ'),
  ('products', 'NNS'),
  ('from', 'IN'),
  ('a', 'DT'),
  ('POETcatalog', 'NNP'),
  ('without', 'IN'),
  ('any', 'DT'),
  ('problems', 'NNS')],
 [('Furtheremore', 'IN'),
  ('she', 'PRP'),
  ('is', 'VBZ'),
  ('able', 'JJ'),
  ('to', 'TO'),
  ('order', 'NN'),
  ('products', 'NNS'),
  ('from', 'IN'),
  ('the', 'DT'),
  ('Vendor', 'NNP'),
  ('VWR', 'NNP'),
  ('through', 'IN'),
  ('COALA', 'NNP')],
 [('But', 'CC'),
  ('articles', 'NNP'),
  ('from', 'VBG'),
  ('all', 'RB'),
  ('other', 'JJ'),
  ('suppliers', 'NNS'),
  ('are', 'NNP'),
  ('not', 'VBG'),
  ('orderable', 'RB')],
 [('I', 'PRP'),
  ('already', 'RB'),
  ('spoke', 'VBD'),
  ('to', 'TO'),
  ('anic', 'VB'),
  ('who', 'WP'),
  ('maintain', 'VBP'),
  ('the', 'DT'),
  ('catalog', 'NN'),
  ('COALA', 'NNP'),
  ('and', 'CC'),
  ('they', 'PRP'),
  ('said', 'VBD'),
  ('that', 'IN'),
  ('the', 'DT'),
  ('reason', 'NN'),
  ('should', 'MD'),
  ('be', 'VB'),
  ('the', 'DT'),
  ('assignment', 'NN'),
  ('of', 'IN'),
  ('the', 'DT'),
  ('plant', 'NN')],
 [('User', 'NNP'),
  ('is', 'VBZ'),
  ('a', 'DT'),
  ('assinged', 'JJ'),
  ('to', 'TO'),
  ('Universitaet', 'NNP'),
  ('Regensburg', 'NNP'),
  ('in', 'IN'),
  ('Scout', 'NNP'),
  ('but', 'CC'),
  ('in', 'IN'),
  ('P17', 'NNP'),
  ('table', 'NN'),
  ('YESRMCDMUSER01', 'NNP'),
  ('she', 'PRP'),
  ('is', 'VBZ'),
  ('assigned', 'VBN'),
  ('to', 'TO'),
  ('company', 'NN'),
  ('001500', 'CD'),
  ('Merck', 'NNP'),
  ('KGaA', 'NNP')],
 [('Please', 'NNP'),
  ('find', 'VB'),
  ('attached', 'JJ'),
  ('some', 'DT'),
  ('screenshots', 'NNS')]]
</code></pre>

<p>Following is my code. </p>

<pre><code>list1 = []
list4 = []
for i in data:
    list2 = []
    list3 = []
    for l,j in enumerate(i):
        if j[1] == 'NNP':
            list2.append(l)
            list3.append(j[0])
    list1.append(list2)
    list4.append(list3)
</code></pre>

<p>Output:</p>

<pre><code>list1:

[[0, 9, 13],
 [0, 5, 9, 10, 11],
 [],
 [],
 [7],
 [9, 10, 12],
 [1, 6],
 [9],
 [0, 5, 6, 8, 11, 13, 20, 21],
 [0]]

list4

[['User', 'Coala', 'VWR'],
 ['Arfter', 'COALA', 'Category', 'S9901', 'Dummy'],
 [],
 [],
 ['POETcatalog'],
 ['Vendor', 'VWR', 'COALA'],
 ['articles', 'are'],
 ['COALA'],
 ['User',
  'Universitaet',
  'Regensburg',
  'Scout',
  'P17',
  'YESRMCDMUSER01',
  'Merck',
  'KGaA'],
 ['Please']]
</code></pre>

<p>From list1 and list4 i'm able to get the strings and indices of NNP. But i want to find out, in each list of lists if VB, RB, JJ tags are present between NNP tags using the index values of NNP tags. </p>

<p>For example, in first list of lists, how to write code to search between ranges (0-9) and (9-13) whether tags with VB, RB, JJ are present or not. </p>
",Parsing & POS Tagging,search range list list want find po tag occurring two range value index nnp tag following code output list list able get string index nnp want find list list vb rb jj tag present nnp tag using index value nnp tag example first list list write code search range whether tag vb rb jj present
"spaCy Documentation for [ orth , pos , tag, lema and text ]","<p>I am new to spaCy. I added this post for documentation and make it simple for new starters as me.</p>

<pre><code>import spacy
nlp = spacy.load('en')
doc = nlp(u'KEEP CALM because TOGETHER We Rock !')
for word in doc:
    print(word.text, word.lemma, word.lemma_, word.tag, word.tag_, word.pos, word.pos_)
    print(word.orth_)
</code></pre>

<p>I am looking to understand what the meaning of orth, lemma, tag and pos ? This code print out the values also what the different between <code>print(word)</code> vs <code>print(word.orth_)</code></p>
",Parsing & POS Tagging,spacy documentation orth po tag lema text new spacy added post documentation make simple new starter looking understand meaning orth lemma tag po code print value also different v
How to filter out list of lists which doesn&#39;t contain elements from other list?,"<p>I'm trying to exclude lists which doesn't contain specific POS tags from the below small list, but couldn't do so. </p>

<pre><code>a = ['VBG', 'RB', 'NNP']
</code></pre>

<p>I only want lists which contain above tags from the below list of list of tuples in output:
(below tags may not be correct, but for representation purpose)</p>

<pre><code>  data = [[('User', 'NNP'),
      ('is', 'VBG'),
      ('not', 'RB'),
      ('able', 'JJ'),
      ('to', 'TO'),
      ('order', 'NN'),
      ('products', 'NNS'),
      ('from', 'IN'),
      ('iShopCatalog', 'NN'),
      ('Coala', 'NNP'),
      ('excluding', 'VBG'),
      ('articles', 'NNS'),
      ('from', 'IN'),
      ('VWR', 'NNP')],
     [('Arfter', 'NNP'),
      ('transferring', 'VBG'),
      ('the', 'DT'),
      ('articles', 'NNS'),
      ('from', 'IN'),
      ('COALA', 'NNP'),
      ('to', 'TO'),
      ('SRM', 'VB'),
      ('the', 'DT'),
      ('Category', 'NNP'),
      ('S9901', 'NNP'),
      ('Dummy', 'NNP'),
      ('is', 'VBZ'),
      ('maintained', 'VBN')],
     [('Due', 'JJ'),
      ('to', 'TO'),
      ('this', 'DT'),
      ('the', 'DT'),
      ('user', 'NN'),
      ('is', 'VBZ'),
      ('not', 'RB'),
      ('able', 'JJ'),
      ('to', 'TO'),
      ('order', 'NN'),
      ('the', 'DT'),
      ('product', 'NN')],
     [('All', 'DT'),
      ('other', 'JJ'),
      ('users', 'NNS'),
      ('can', 'MD'),
      ('order', 'NN'),
      ('these', 'DT'),
      ('articles', 'NNS')],
     [('She', 'PRP'),
      ('can', 'MD'),
      ('order', 'NN'),
      ('other', 'JJ'),
      ('products', 'NNS'),
      ('from', 'IN'),
      ('a', 'DT'),
      ('POETcatalog', 'NNP'),
      ('without', 'IN'),
      ('any', 'DT'),
      ('problems', 'NNS')],
     [('Furtheremore', 'IN'),
      ('she', 'PRP'),
      ('is', 'VBZ'),
      ('able', 'JJ'),
      ('to', 'TO'),
      ('order', 'NN'),
      ('products', 'NNS'),
      ('from', 'IN'),
      ('the', 'DT'),
      ('Vendor', 'NNP'),
      ('VWR', 'NNP'),
      ('through', 'IN'),
      ('COALA', 'NNP')],
     [('But', 'CC'),
      ('articles', 'NNP'),
      ('from', 'VBG'),
      ('all', 'RB'),
      ('other', 'JJ'),
      ('suppliers', 'NNS'),
      ('are', 'NNP'),
      ('not', 'VBG'),
      ('orderable', 'RB')],
     [('I', 'PRP'),
      ('already', 'RB'),
      ('spoke', 'VBD'),
      ('to', 'TO'),
      ('anic', 'VB'),
      ('who', 'WP'),
      ('maintain', 'VBP'),
      ('the', 'DT'),
      ('catalog', 'NN'),
      ('COALA', 'NNP'),
      ('and', 'CC'),
      ('they', 'PRP'),
      ('said', 'VBD'),
      ('that', 'IN'),
      ('the', 'DT'),
      ('reason', 'NN'),
      ('should', 'MD'),
      ('be', 'VB'),
      ('the', 'DT'),
      ('assignment', 'NN'),
      ('of', 'IN'),
      ('the', 'DT'),
      ('plant', 'NN')],
     [('User', 'NNP'),
      ('is', 'VBZ'),
      ('a', 'DT'),
      ('assinged', 'JJ'),
      ('to', 'TO'),
      ('Universitaet', 'NNP'),
      ('Regensburg', 'NNP'),
      ('in', 'IN'),
      ('Scout', 'NNP'),
      ('but', 'CC'),
      ('in', 'IN'),
      ('P17', 'NNP'),
      ('table', 'NN'),
      ('YESRMCDMUSER01', 'NNP'),
      ('she', 'PRP'),
      ('is', 'VBZ'),
      ('assigned', 'VBN'),
      ('to', 'TO'),
      ('company', 'NN'),
      ('001500', 'CD'),
      ('Merck', 'NNP'),
      ('KGaA', 'NNP')],
     [('Please', 'NNP'),
      ('find', 'VB'),
      ('attached', 'JJ'),
      ('some', 'DT'),
      ('screenshots', 'NNS')]]
</code></pre>

<p>My expected output is:</p>

<pre><code>data = [[('User', 'NNP'),
  ('is', 'VBG'),
  ('not', 'RB'),
  ('able', 'JJ'),
  ('to', 'TO'),
  ('order', 'NN'),
  ('products', 'NNS'),
  ('from', 'IN'),
  ('iShopCatalog', 'NN'),
  ('Coala', 'NNP'),
  ('excluding', 'VBG'),
  ('articles', 'NNS'),
  ('from', 'IN'),
  ('VWR', 'NNP')],
  [('But', 'CC'),
  ('articles', 'NNP'),
  ('from', 'VBG'),
  ('all', 'RB'),
  ('other', 'JJ'),
  ('suppliers', 'NNS'),
  ('are', 'NNP'),
  ('not', 'VBG'),
  ('orderable', 'RB')]
</code></pre>

<p>I tried to do this by writing the below code, but unable to do so:</p>

<pre><code>list1=[]
for i in data:
    list2 = []
    a = ['VBG', 'RB', 'NNP']
    for j in i:
        if all(i in j[1] for i in a):
            list2.append(j)
    list1.append(list2)
list1
</code></pre>

<p>which is returning empty list of lists.
Can anybody provide a simple understandable code to get my expected output. Thanks.</p>
",Parsing & POS Tagging,filter list list contain element list trying exclude list contain specific po tag small list want list contain tag list list tuples output tag may correct representation purpose expected output tried writing code unable returning empty list list anybody provide simple understandable code get expected output thanks
How to filter specific POS tags from list of lists to separate lists?,"<p>I have huge data of product descriptions and required to separate the product names and the intent from descriptions for which i found out separating NNP tags after tagging the text with POS tags is somewhat helpful for further cleansing. </p>

<p>I have the following similar data for which i want to only filter NNP tags and want them to be filtered in their respective list, but unable to do so. </p>

<pre><code> data = [[('User', 'NNP'),
  ('is', 'VBZ'),
  ('not', 'RB'),
  ('able', 'JJ'),
  ('to', 'TO'),
  ('order', 'NN'),
  ('products', 'NNS'),
  ('from', 'IN'),
  ('iShopCatalog', 'NN'),
  ('Coala', 'NNP'),
  ('excluding', 'VBG'),
  ('articles', 'NNS'),
  ('from', 'IN'),
  ('VWR', 'NNP')],
 [('Arfter', 'NNP'),
  ('transferring', 'VBG'),
  ('the', 'DT'),
  ('articles', 'NNS'),
  ('from', 'IN'),
  ('COALA', 'NNP'),
  ('to', 'TO'),
  ('SRM', 'VB'),
  ('the', 'DT'),
  ('Category', 'NNP'),
  ('S9901', 'NNP'),
  ('Dummy', 'NNP'),
  ('is', 'VBZ'),
  ('maintained', 'VBN')],
 [('Due', 'JJ'),
  ('to', 'TO'),
  ('this', 'DT'),
  ('the', 'DT'),
  ('user', 'NN'),
  ('is', 'VBZ'),
  ('not', 'RB'),
  ('able', 'JJ'),
  ('to', 'TO'),
  ('order', 'NN'),
  ('the', 'DT'),
  ('product', 'NN')],
 [('All', 'DT'),
  ('other', 'JJ'),
  ('users', 'NNS'),
  ('can', 'MD'),
  ('order', 'NN'),
  ('these', 'DT'),
  ('articles', 'NNS')],
 [('She', 'PRP'),
  ('can', 'MD'),
  ('order', 'NN'),
  ('other', 'JJ'),
  ('products', 'NNS'),
  ('from', 'IN'),
  ('a', 'DT'),
  ('POETcatalog', 'NNP'),
  ('without', 'IN'),
  ('any', 'DT'),
  ('problems', 'NNS')],
 [('Furtheremore', 'IN'),
  ('she', 'PRP'),
  ('is', 'VBZ'),
  ('able', 'JJ'),
  ('to', 'TO'),
  ('order', 'NN'),
  ('products', 'NNS'),
  ('from', 'IN'),
  ('the', 'DT'),
  ('Vendor', 'NNP'),
  ('VWR', 'NNP'),
  ('through', 'IN'),
  ('COALA', 'NNP')],
 [('But', 'CC'),
  ('articles', 'NNS'),
  ('from', 'IN'),
  ('all', 'DT'),
  ('other', 'JJ'),
  ('suppliers', 'NNS'),
  ('are', 'VBP'),
  ('not', 'RB'),
  ('orderable', 'JJ')],
 [('I', 'PRP'),
  ('already', 'RB'),
  ('spoke', 'VBD'),
  ('to', 'TO'),
  ('anic', 'VB'),
  ('who', 'WP'),
  ('maintain', 'VBP'),
  ('the', 'DT'),
  ('catalog', 'NN'),
  ('COALA', 'NNP'),
  ('and', 'CC'),
  ('they', 'PRP'),
  ('said', 'VBD'),
  ('that', 'IN'),
  ('the', 'DT'),
  ('reason', 'NN'),
  ('should', 'MD'),
  ('be', 'VB'),
  ('the', 'DT'),
  ('assignment', 'NN'),
  ('of', 'IN'),
  ('the', 'DT'),
  ('plant', 'NN')],
 [('User', 'NNP'),
  ('is', 'VBZ'),
  ('a', 'DT'),
  ('assinged', 'JJ'),
  ('to', 'TO'),
  ('Universitaet', 'NNP'),
  ('Regensburg', 'NNP'),
  ('in', 'IN'),
  ('Scout', 'NNP'),
  ('but', 'CC'),
  ('in', 'IN'),
  ('P17', 'NNP'),
  ('table', 'NN'),
  ('YESRMCDMUSER01', 'NNP'),
  ('she', 'PRP'),
  ('is', 'VBZ'),
  ('assigned', 'VBN'),
  ('to', 'TO'),
  ('company', 'NN'),
  ('001500', 'CD'),
  ('Merck', 'NNP'),
  ('KGaA', 'NNP')],
 [('Please', 'NNP'),
  ('find', 'VB'),
  ('attached', 'JJ'),
  ('some', 'DT'),
  ('screenshots', 'NNS')]]
</code></pre>

<p>I wrote the following code:</p>

<pre><code>def prodname(a):
    p = []
    for i in a:
        for j in range(len(i)):
            if i[j][1]=='NNP':
                p.append(i[j][0])
    return p
</code></pre>

<p>which is giving the following output:</p>

<pre><code>    ['User',
     'Coala',
     'VWR',
     'Arfter',
     'COALA',
     'Category',
     'S9901',
     'Dummy',
     'POETcatalog',
     'Vendor',
     'VWR',
     'COALA',
     'COALA',
     'User',
     'Universitaet',
     'Regensburg',
     'Scout',
     'P17',
     'YESRMCDMUSER01',
     'Merck',
     'KGaA',
     'Please']
</code></pre>

<p>The output i would like to get is:</p>

<pre><code>[['User',
  'Coala',
  'VWR']
['Arfter',
 'COALA',
 'Category',
 'S9901',
 'Dummy']
[],
[],
['POETcatalog'],
['Vendor',
 'VWR',
 'COALA'],
[],
['COALA'],
['User',
 'Universitaet',
 'Regensburg',
 'Scout',
 'P17',
 'YESRMCDMUSER01',
 'Merck',
'KGaA'],
['Please']]
</code></pre>

<p>Also tried to use <code>[[] for i in range(len(data)]</code> to append to their respective lists, but couldn't do so. </p>
",Parsing & POS Tagging,filter specific po tag list list separate list huge data product description required separate product name intent description found separating nnp tag tagging text po tag somewhat helpful cleansing following similar data want filter nnp tag want filtered respective list unable wrote following code giving following output output would like get also tried use append respective list
Part of speech tagging with Viterbi algorithm,"<p>I am working on a project where I need to use the Viterbi algorithm to do part of speech tagging on a list of sentences. For my training data I have sentences that are already tagged by word that I assume I need to parse and store in some data structure. Then I have a test data which also contains sentences where each word is tagged. </p>

<p>I'm a bit confused on how I would approach this problem. I guess part of the issue stems from the fact that I don't think I fully understand the point of the Viterbi algorithm. Am I supposed to use the Viterbi algorithm to tag my test data and compare the results to the actual data? What data structures are best to do this and represent a sentence?</p>

<p>Any help would be greatly appreciated.</p>
",Parsing & POS Tagging,part speech tagging viterbi algorithm working project need use viterbi algorithm part speech tagging list sentence training data sentence already tagged word assume need parse store data structure test data also contains sentence word tagged bit confused would approach problem guess part issue stem fact think fully understand point viterbi algorithm supposed use viterbi algorithm tag test data compare result actual data data structure best represent sentence help would greatly appreciated
Are there constituency parsers that do not aim for a full parse?,"<p>I am currently working on a set of report-styled documents, of which I want to extract information. At the moment, I am trying to divide the text body into smaller constituents, for individual classification (what kind of information do we expect in the phrase). Because of the inaccurate grammar in which the reports are written, a standard constituency parser won’t find a common root for the sentences. This obviously cries for dependency parsing. I was however interested whether there would be constituency parsers which do not aim for a full parse of the sentence. Something anlong the line of probabilistic CKY which tries to return most probable sub nodes. I am currently working in the Python nltk framework, but Java solutions would be fine as well.</p>
",Parsing & POS Tagging,constituency parser aim full parse currently working set report styled document want extract information moment trying divide text body smaller constituent individual classification kind information expect phrase inaccurate grammar report written standard constituency parser find common root sentence obviously cry dependency parsing wa however interested whether would constituency parser aim full parse sentence something anlong line probabilistic cky try return probable sub node currently working python nltk framework java solution would fine well
How to extract nouns using NLTK pos_tag()?,"<p>I am fairly new to python. I am not able to figure out the bug. I want to extract nouns using NLTK.</p>

<p>I have written the following code:</p>

<pre><code>import nltk

sentence = ""At eight o'clock on Thursday film morning word line test best beautiful Ram Aaron design""

tokens = nltk.word_tokenize(sentence)

tagged = nltk.pos_tag(tokens)


length = len(tagged) - 1

a = list()

for i in (0,length):
    log = (tagged[i][1][0] == 'N')
    if log == True:
      a.append(tagged[i][0])
</code></pre>

<p>When I run this, 'a' only has one element </p>

<pre><code>a
['detail']
</code></pre>

<p>I do not understand why?</p>

<p>When I do it without for loop, that is running</p>

<pre><code>log = (tagged[i][1][0] == 'N')
    if log == True:
      a.append(tagged[i][0])
</code></pre>

<p>by change value of 'i' manually from 0 to 'length', i get the output perfectly, but with for loop it only returns the end element. Can someone tell me what is wrong happening with for loop. </p>

<p>'a' should be as follows after the code</p>

<pre><code>['Thursday', 'film', 'morning', 'word', 'line', 'test', 'Ram' 'Aaron', 'design']
</code></pre>
",Parsing & POS Tagging,extract noun using nltk po tag fairly new python able figure bug want extract noun using nltk written following code run ha one element understand without loop running change value manually length get output perfectly loop return end element someone tell wrong happening loop follows code
Why use hidden Markov model vs. Markov model in Baum Welch algorithm,"<p>So I am trying to build the Baum Welch algorithm to do parts of speech tagging for practice. However, I am confused about using a hidden Markov Model vs. a Markov Model. Since it seems that you are losing context moving from state to state. Since the output of the last state isn't taken into account when moving to the next state. Is it just to save memory?</p>

<p>edit: added an example for clarity</p>

<p>For example if two states, A and B output a 0 or 1 there will be 4 state transitions and 2 obseravation possibilities for each state, which can be can be made into 8 transitions if you mix each pair of incoming transitions with it's state's obseravation probabilities. But my hang up is why not initially train a machine with four states {(A,1),(B,1),(A,2),(B,2)} with 16 transitions. I am quite new to nlp so I wondering if I am unaware of some algorithmic redundancy that hard to see without harder math.</p>

<p>Since it seems that one loses the information of what the transitions will be when of the last A was 1 vs. 2. But I'm wondering if the training algorithms might not need that information.</p>

<p><a href=""https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm</a></p>

<p>Thanks for the information.</p>
",Parsing & POS Tagging,use hidden markov model v markov model baum welch algorithm trying build baum welch algorithm part speech tagging practice however confused using hidden markov model v markov model since seems losing context moving state state since output last state taken account moving next state save memory edit added example clarity example two state b output state transition obseravation possibility state made transition mix pair incoming transition state obseravation probability hang initially train machine four state b b transition quite new nlp wondering unaware algorithmic redundancy hard see without harder math since seems one loses information transition last wa v wondering training algorithm might need information thanks information
mergnig POS tag by noun phrase chunk,"<p>My question is similar to this <a href=""https://stackoverflow.com/questions/33289820/noun-phrases-with-spacy"">question</a>. In <code>spacy</code>, I can do part-of-speech tagging and noun phrase identification separately e.g.</p>

<pre><code>import spacy
nlp = spacy.load('en')
sentence = 'For instance , consider one simple phenomena : 
            a question is typically followed by an answer , 
            or some explicit statement of an inability or refusal to answer .'
token = nlp(sentence)
token_tag = [(word.text, word.pos_) for word in token]
</code></pre>

<p>Output looks like:</p>

<pre><code>[('For', 'ADP'),
 ('instance', 'NOUN'),
 (',', 'PUNCT'),
 ('consider', 'VERB'),
 ('one', 'NUM'),
 ('simple', 'ADJ'),
 ('phenomena', 'NOUN'), 
 ...]
</code></pre>

<p>For Noun phrase or chunk, I can get <code>noun_chunks</code> which is a chunk of words as follows:</p>

<pre><code>[nc for nc in token.noun_chunks] # [instance, one simple phenomena, an answer, ...]
</code></pre>

<p>I'm wondering if there is a way to cluster the POS tag based on <code>noun_chunks</code> so that I get the output as</p>

<pre><code>[('For', 'ADP'),
 ('instance', 'NOUN'), # or NOUN_CHUNKS
 (',', 'PUNCT'),
 ('one simple phenomena', 'NOUN_CHUNKS'), 
 ...]
</code></pre>
",Parsing & POS Tagging,mergnig po tag noun phrase chunk question similar href part speech tagging noun phrase identification separately e g p output look like noun phrase chunk get chunk word follows wondering way cluster po tag based get output
How to use SyntaxNet parser/tagger with spaCy API?,"<p>I have been using <a href=""https://spacy.io/"" rel=""noreferrer"">spaCy</a> Python package to parse and tag text and using the resulting dependency tree and other attributes to derive meaning. Now I would like to use SyntaxNet's Parsey McParseface for parsing and dependency tagging (which seems better), but I would like to keep using spaCy API because it is so easy to use and it does many things that Parsey doesn't. SyntaxNet outputs POS tags and dependency tags/tree in a CoNLL-format: </p>

<ol>
<li>Bob    _    NOUN    NNP    _    2    nsubj    _    _</li>
<li>brought    _    VERB    VBD    _    0    ROOT    _    _</li>
<li>the    _    DET    DT    _    4    det    _    _</li>
<li>pizza    _    NOUN    NN    _    2    dobj    _    _</li>
<li>to    _    ADP    IN    _    2    prep    _    _</li>
<li>Alice    _    NOUN    NNP    _    5    pobj    _    _</li>
<li>.    _    .    .    _    2    punct    _    _</li>
</ol>

<p>and spaCy seems to be able to read CoNLL format right <a href=""https://github.com/spacy-io/spaCy/blob/master/bin/parser/conll_parse.py"" rel=""noreferrer"">here</a>. But I can't figure out where in spaCy's API does it take a CoNLL-fromatted string.</p>
",Parsing & POS Tagging,use syntaxnet parser tagger spacy api using spacy python package parse tag text using resulting dependency tree attribute derive meaning would like use syntaxnet parsey mcparseface parsing dependency tagging seems better would like keep using spacy api easy use doe many thing parsey syntaxnet output po tag dependency tag tree conll format bob noun nnp nsubj brought verb vbd root det dt det pizza noun nn dobj adp prep alice noun nnp pobj punct spacy seems able read conll format right figure spacy api doe take conll fromatted string
Using Syntaxnet POS tags in python?,"<p>I want to use the parser tags (ex: VBD ROOT, NN nsubj, etc.) of syntaxnet in python to help create a chatterbot. The input is done in the console. </p>

<p>Question: How can I get in a variable the result of only VBP ROOT and nothing else? I was able to print the ASCII version of the parse tree with this call: <code>subprocess.call([""echo 'Bob brought the pizza to Alice.' | syntaxnet/demo.sh""], shell =True)</code> but I am a bit confuse on how to get to a particular variable and store it in a python variable.</p>

<p>p.s: I started to learn python a week ago.</p>
",Parsing & POS Tagging,using syntaxnet po tag python want use parser tag ex vbd root nn nsubj etc syntaxnet python help create chatterbot input done console question get variable result vbp root nothing else wa able print ascii version parse tree call bit confuse get particular variable store python variable p started learn python week ago
Practical Earley Parsing (Aycock &amp; Horspool 2002): How to add back pointers?,"<p>I've already coded an Earley parser with back pointers but it doesn't handle nullable grammars very well. I've also implemented Aycock &amp; Horspool 2002's solution which is to make PREDICT skip over the nonterminal token if it is nullable. Unfortunately this does not tell you exactly which particular path the token needs to take to get to epsilon. </p>

<p>My idea (pretty stupid) is: </p>

<blockquote>
  <p>For each nullable nonterminal, create a list of paths for that
  nonterminal to get to epsilon. </p>
  
  <p>Every time you skip over a nullable nonterminal, add a back pointer
  called NULL.</p>
  
  <p>When you're expanding the tree, every time you encounter NULL, you
  create a list of trees, one for each path in the list for that
  nullable nonterminal. </p>
  
  <p>Finally, you go through the list of trees and get rid of duplicates.</p>
</blockquote>

<p>I think this would significantly increase the time complexity of my algorithm but I can't think of a more efficient method to generate all the possible parse trees. </p>

<p>Can anyone suggest a more efficient method of implementing Aycock &amp; Horspool 2002 to create parse trees?</p>
",Parsing & POS Tagging,practical earley parsing aycock horspool add back pointer already coded earley parser back pointer handle nullable grammar well also implemented aycock horspool solution make predict skip nonterminal token nullable unfortunately doe tell exactly particular path token need take get epsilon idea pretty stupid nullable nonterminal create list path nonterminal get epsilon every time skip nullable nonterminal add back pointer called null expanding tree every time encounter null create list tree one path list nullable nonterminal finally go list tree get rid duplicate think would significantly increase time complexity algorithm think efficient method generate possible parse tree anyone suggest efficient method implementing aycock horspool create parse tree
Head-finding rules for noun phrases,"<p>The Penn Treebank format does not annotate the internal structure of a noun phrase, e.g.</p>

<pre><code>(NP (JJ crude) (NN oil) (NNS prices))
</code></pre>

<p>or </p>

<pre><code>(NP
    (NP (DT the) (JJ big) (JJ blue) (NN house))
    (SBAR
      (WHNP (WDT that))
      (S
        (VP (VBD was)
          (VP (VBN built)
            (PP (IN near)
              (NP (DT the) (NN river)))))))
</code></pre>

<p>I would like to extract the heads (prices and house). Do you know of any tool that can do this?</p>
",Parsing & POS Tagging,head finding rule noun phrase penn treebank format doe annotate internal structure noun phrase e g would like extract head price house know tool
Python NLP dependency parser from spacy library not working well,"<p>When I give spacy parser the sentence: ""A man has been walking down the street"", it says that 'been' is the root, while walking is xcompl, like 'walking' would be in the sentence: ""A man loves walking down the street"". Which is wrong.</p>

<p>Now, this doesn't happen in displacy (<a href=""https://demos.explosion.ai/displacy/?text=a%20man%20has%20been%20walking%20in%20the%20street&amp;model=en&amp;cpu=0&amp;cph=0"" rel=""nofollow noreferrer"">https://demos.explosion.ai/displacy/?text=a%20man%20has%20been%20walking%20in%20the%20street&amp;model=en&amp;cpu=0&amp;cph=0</a>). 
Does anyone know how to download the model version that displacy uses? I would appreciate it very much.</p>
",Parsing & POS Tagging,python nlp dependency parser spacy library working well give spacy parser sentence man ha walking street say root walking xcompl like walking would sentence man love walking street wrong happen displacy doe anyone know download model version displacy us would appreciate much
Stanford Parser: can dependencies be converted to a parse tree?,"<p>Is there a way to go from dependencies:</p>

<pre><code>nmod:poss(dog-2, My-1)
nsubj(likes-4, dog-2)
advmod(likes-4, also-3)
root(ROOT-0, likes-4)
xcomp(likes-4, eating-5)
dobj(eating-5, sausage-6)
</code></pre>

<p>to the corresponding parse tree?</p>

<pre><code>(ROOT
  (S
    (NP (PRP$ My) (NN dog))
    (ADVP (RB also))
    (VP (VBZ likes)
      (S
        (VP (VBG eating)
          (NP (NN sausage)))))
    (. .)))
</code></pre>
",Parsing & POS Tagging,stanford parser dependency converted parse tree way go dependency corresponding parse tree
Is something wrong with the way that I&#39;m generating this NLTK grammar?,"<p>I wrote this simple program with NLTK that's just supposed to print out the syntax tree. However, it prints nothing out even though the <code>RecursiveDescentParser</code> is being created. What's my problem? Am I defining the grammar incorrectly? Is something wrong with the way that I'm trying to iterate through the parser? Thank you in advance.</p>

<pre><code>import nltk

'''The price of peace is rising.'''

grammar = nltk.CFG.fromstring(""""""
  S -&gt; NP VP
  VP -&gt; V NP | V NP PP
  PP -&gt; P NP
  V -&gt; ""is"" | ""rising""
  NP -&gt; Det N | Det N PP
  Det -&gt; ""the"" | ""of""
  N -&gt; ""price"" | ""peace""
  P -&gt; ""in"" | ""on"" | ""by"" | ""with""
  """""")

sentence = ""the price of peace is rising""
wordArray = sentence.split()

print(wordArray)

parser = nltk.RecursiveDescentParser(grammar)

for tree in parser.parse(wordArray):
    print(tree)
</code></pre>
",Parsing & POS Tagging,something wrong way generating nltk grammar wrote simple program nltk supposed print syntax tree however print nothing even though created problem defining grammar incorrectly something wrong way trying iterate parser thank advance
Convolutional Network for Text Classification,"<p>I am trying to train a convolutional neural network with Keras at recognizing tags for Stack Exchange questions about cooking. </p>

<p>The i-th question element of my data-set is like this:</p>

<pre><code>id                                                         2
title                    How should I cook bacon in an oven?
content    &lt;p&gt;I've heard of people cooking bacon in an ov...
tags                                 oven cooking-time bacon
Name: 1, dtype: object
</code></pre>

<p>I have removed tags with BeautifulSoup and removed punctuation too.
Since questions' content are very big I have decided to focus on titles. 
I have used sklearn CountVectorizer to vectorize words in titles. However they were more than 8000 words (excluding stop words). So I decided apply a part of speech tagging and retrieve only Nouns and Gerunds.</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(stop_words='english')
titles = dataframes['cooking']['title']
pos_titles = []
for i,title in enumerate(titles):
    pos = []
    pt_titl = nltk.pos_tag(word_tokenize(title))
    for pt in pt_titl:
        if pt[1]=='NN' or pt[1]=='NNS' or pt[1]=='VBG':# or pt[1]=='VBP' or pt[1]=='VBS':
            pos.append(pt[0])
    pos_titles.append("" "".join(pos))
</code></pre>

<p>This represents my input vector. I have vectorized tags too and extract dense matrixes for both input and tags. </p>

<pre><code>tags = ["" "".join(x) for x in dataframes['cooking']['tags']]
Xd = X.todense()

Y = vectorizer.fit_transform(tags)
Yd = Y.todense()
</code></pre>

<p>Split data into train and validation set </p>

<pre><code>from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(Xd, Yd, test_size=0.33, random_state=42)
</code></pre>

<p>Now I am trying to train a Conv1D network</p>

<pre><code>from keras.models import Sequential
from keras.layers import Dense, Activation,Flatten
from keras.layers import Conv2D, MaxPooling2D,Conv1D, Embedding,GlobalMaxPooling1D,Dropout,MaxPooling1D

model = Sequential()

model.add(Embedding(Xd.shape[1],
                    128,
                    input_length=Xd.shape[1]))
model.add(Conv1D(32,5,activation='relu'))
model.add(MaxPooling1D(100,stride=50))
model.add(Conv1D(32,5,activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(Yd.shape[1], activation ='softmax'))


model.compile(optimizer='rmsprop',
          loss='categorical_crossentropy',
          metrics=['accuracy'])
model.fit(X_train, y_train, batch_size=32,verbose=1)
</code></pre>

<p>But it gets stucked on a very low accuracy and it shows a barely increasing loss along the epochs</p>

<pre><code>Epoch 1/10
10320/10320 [==============================] - 401s - loss: 15.8098 - acc: 0.0604   
Epoch 2/10
10320/10320 [==============================] - 339s - loss: 15.5671 - acc: 0.0577   
Epoch 3/10
10320/10320 [==============================] - 314s - loss: 15.5509 - acc: 0.0578   
Epoch 4/10
10320/10320 [==============================] - 34953s - loss: 15.5493 - acc: 0.0578  
Epoch 5/10
10320/10320 [==============================] - 323s - loss: 15.5587 - acc: 0.0578   
Epoch 6/10
 6272/10320 [=================&gt;............] - ETA: 133s - loss: 15.6005 - acc: 0.0550
</code></pre>
",Parsing & POS Tagging,convolutional network text classification trying train convolutional neural network kera recognizing tag stack exchange question cooking th question element data set like removed tag beautifulsoup removed punctuation since question content big decided focus title used sklearn countvectorizer vectorize word title however word excluding stop word decided apply part speech tagging retrieve noun gerund represents input vector vectorized tag extract dense matrix input tag split data train validation set trying train conv network get stucked low accuracy show barely increasing loss along epoch
Hobbs&#39; algorithm for Coref Resolution,"<p>I have implemented Hobbs' algorithm for anaphora resolution together with Lappin &amp; Leass ranking for alternatives.</p>

<p>What bugs me is that the description of the algorithm is completely informal, and since there are sentences that are not correctly resolved by my implementation I am not sure whether the limit is on my implementation or on the actual algorithm.</p>

<p>Here is the version I have worked with, found in the Jurafsky&amp;Martin:</p>

<blockquote>
  <ol>
  <li>Begin at the noun phrase (NP) node immediately dominating the pronoun.</li>
  <li>Go up the tree to the first NP or sentence (S) node encountered. Call this node X, and call the path used to reach it p.</li>
  <li>Traverse all branches below node X to the left of path p in a left-to-right, breadth-first fashion. Propose as the antecedent any NP node that is
  encountered which has an NP or S node between it and X.</li>
  <li>If node X is the highest S node in the sentence, traverse the surface parse trees of previous sentences in the text in order of
  recency, the most recent first; each tree is traversed in a
  left-to-right, breadth-first manner, and when an NP node is
  encountered, it is proposed as antecedent. If X is not the highest S
  node in the sentence, continue to step 5.</li>
  <li>From node X, go up the tree to the first NP or S node encountered. Call this new node X, and call the path traversed to reach it p.</li>
  <li>If X is an NP node and if the path p to X did not pass through the Nominal node that X immediately dominates, propose X as the
  antecedent.</li>
  <li>Traverse all branches below node X to the left of path p in a left-to-right, breadth-
  first manner. Propose any NP node encountered as the antecedent.</li>
  <li>If X is an S node, traverse all branches of node X to the right of path p in a left-to- right, breadth-first manner, but do not go below any NP or S node
  encountered. Propose any NP node encountered as the antecedent.</li>
  <li>Go to Step4</li>
  </ol>
</blockquote>

<p>Look at step 3: ""to the left of path p"". The way I interpreted it is to iterate through the subtrees left-to-right, until I find the branch that contains the pronoun (hence part of the path from the pronoun to X). In Java:</p>

<pre><code>for (Tree relative : X.children()) {
            for (Tree candidate : relative) {
                if (candidate.contains(pronoun)) break; // I am looking to all the nodes to the LEFT (i.e. coming before) the path leading to X. contain &lt;-&gt; in the path
...
</code></pre>

<p>However, doing it this way does not process sentences like ""The house is of King Arthur himself"". This is due to the fact that ""King Arthur"" contains ""himself"" and is therefore not taken into account.</p>

<p>Is this a limit of the Hobbs algorithm or am I mistaking something here?</p>

<p>For reference, the full code in Java (using the Stanford Parser) is here:</p>

<pre><code>import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.io.PrintWriter;
import java.io.Reader;
import java.io.StringReader;
import java.io.StringWriter;
import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.Set;
import java.util.StringTokenizer;

import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;
import javax.xml.parsers.ParserConfigurationException;
import javax.xml.transform.Transformer;
import javax.xml.transform.TransformerConfigurationException;
import javax.xml.transform.TransformerException;
import javax.xml.transform.TransformerFactory;
import javax.xml.transform.dom.DOMSource;
import javax.xml.transform.stream.StreamResult;

import org.apache.commons.lang3.ArrayUtils;
import org.apache.commons.lang3.StringUtils;
import org.apache.commons.lang3.StringEscapeUtils;

import org.w3c.dom.Document;
import org.w3c.dom.Element;
import org.w3c.dom.NamedNodeMap;
import org.w3c.dom.Node;
import org.w3c.dom.NodeList;
import org.xml.sax.SAXException;

import edu.stanford.nlp.ling.HasWord;
import edu.stanford.nlp.ling.Word;
import edu.stanford.nlp.ling.Sentence;
import edu.stanford.nlp.process.DocumentPreprocessor;
import edu.stanford.nlp.process.Tokenizer;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.parser.lexparser.LexicalizedParser;

class ParseAllXMLDocuments {
    /** 
     * @throws ParserConfigurationException 
     * @throws SAXException 
     * @throws TransformerException 
     * @throws ModifyException 
     * @throws NavException 
     * @throws TranscodeException 
     * @throws ParseException 
     * @throws EntityException 
     * @throws EOFException 
     * @throws EncodingException */
    static final int MAXPREVSENTENCES = 4;
    public static void main(String[] args) throws IOException, SAXException, ParserConfigurationException, TransformerException  {
        //      File dataFolder = new File(""DataToPort"");
        //      File[] documents;
        String grammar = ""grammar/englishPCFG.ser.gz"";
        String[] options = { ""-maxLength"", ""100"", ""-retainTmpSubcategories"" };
        LexicalizedParser lp = 
                new LexicalizedParser(grammar, options);
        //
        //      if (dataFolder.isDirectory()) {
        //          documents = dataFolder.listFiles();
        //      } else {
        //          documents = new File[] {dataFolder};
        //      }
        //      int currfile = 0;
        //      int totfiles = documents.length;
        //      for (File paper : documents) {
        //          currfile++;
        //          if (paper.getName().equals("".DS_Store"")||paper.getName().equals("".xml"")) {
        //              currfile--;
        //              totfiles--;
        //              continue;
        //          }
        //          System.out.println(""Working on ""+paper.getName()+"" (file ""+currfile+"" out of ""+totfiles+"")."");
        //
        //          DocumentBuilderFactory docFactory = DocumentBuilderFactory.newInstance(); // This is for XML
        //          DocumentBuilder docBuilder = docFactory.newDocumentBuilder();
        //          Document doc = docBuilder.parse(paper.getAbsolutePath());
        //
        //          NodeList textlist = doc.getElementsByTagName(""text"");
        //          for(int i=0; i &lt; textlist.getLength(); i++) {
        //              Node currentnode = textlist.item(i);
        //              String wholetext = textlist.item(i).getTextContent();
        String wholetext = ""The house of King Arthur himself. You live in it all the day."";
        //System.out.println(wholetext);
        //Iterable&lt;List&lt;? extends HasWord&gt;&gt; sentences;
        System.out.println(wholetext);
        ArrayList&lt;Tree&gt; parseTrees = new ArrayList&lt;Tree&gt;();
        String asd = """";
        int j = 0;
        StringReader stringreader = new StringReader(wholetext);
        DocumentPreprocessor dp = new DocumentPreprocessor(stringreader);
        @SuppressWarnings(""rawtypes"")
        ArrayList&lt;List&gt; sentences = preprocess(dp);
        for (List sentence : sentences) {
            parseTrees.add( lp.apply(sentence) ); // Parsing a new sentence and adding it to the parsed tree
            ArrayList&lt;Tree&gt; PronounsList = findPronouns(parseTrees.get(j)); // Locating all pronouns to resolve in the sentence
            Tree corefedTree;
            for (Tree pronounTree : PronounsList) { 
                parseTrees.set(parseTrees.size()-1, HobbsResolve(pronounTree, parseTrees)); // Resolving the coref and modifying the tree for each pronoun
            }
            StringWriter strwr = new StringWriter();
            PrintWriter prwr = new PrintWriter(strwr);
            TreePrint tp = new TreePrint(""penn"");
            tp.printTree(parseTrees.get(j), prwr);
            prwr.flush();   
            asd += strwr.toString();
            j++;
        }
        String armando = """";
        for (Tree sentence : parseTrees) {
            for (Tree leaf : Trees.leaves(sentence))
                armando += leaf + "" "";
        }
        System.out.println(armando);
        System.out.println(""All done."");
        //              currentnode.setTextContent(asd);
        //          }
        //          TransformerFactory transformerFactory = TransformerFactory.newInstance();
        //          Transformer transformer = transformerFactory.newTransformer();
        //          DOMSource source = new DOMSource(doc);
        //          StreamResult result = new StreamResult(paper);
        //          transformer.transform(source, result);
        //
        //          System.out.println(""Done"");
        //      }
    }

    public static Tree HobbsResolve(Tree pronoun, ArrayList&lt;Tree&gt; forest) {
        Tree wholetree = forest.get(forest.size()-1); // The last one is the one I am going to start from
        ArrayList&lt;Tree&gt; candidates = new ArrayList&lt;Tree&gt;();
        List&lt;Tree&gt; path = wholetree.pathNodeToNode(wholetree, pronoun);
        System.out.println(path);
        // Step 1
        Tree ancestor = pronoun.parent(wholetree); // This one locates the NP the pronoun is in, therefore we need one more ""parenting"" !
        // Step 2
        ancestor = ancestor.parent(wholetree);
        //System.out.println(""LABEL: ""+pronoun.label().value() + ""\n\tVALUE: ""+pronoun.firstChild());
        while ( !ancestor.label().value().equals(""NP"") &amp;&amp; !ancestor.label().value().equals(""S"") )
            ancestor = ancestor.parent(wholetree);
        Tree X = ancestor;
        path = X.pathNodeToNode(wholetree, pronoun);
        System.out.println(path);
        // Step 3
        for (Tree relative : X.children()) {
            for (Tree candidate : relative) {
                if (candidate.contains(pronoun)) break; // I am looking to all the nodes to the LEFT (i.e. coming before) the path leading to X. contain &lt;-&gt; in the path
                //System.out.println(""LABEL: ""+relative.label().value() + ""\n\tVALUE: ""+relative.firstChild());
                if ( (candidate.parent(wholetree) != X) &amp;&amp; (candidate.parent(wholetree).label().value().equals(""NP"") || candidate.parent(wholetree).label().value().equals(""S"")) )
                    if (candidate.label().value().equals(""NP"")) // ""Propose as the antecedent any NP node that is encountered which has an NP or S node between it and X""
                        candidates.add(candidate);
            }
        }
        // Step 9 is a GOTO step 4, hence I will envelope steps 4 to 8 inside a while statement.
        while (true) { // It is NOT an infinite loop. 
            // Step 4
            if (X.parent(wholetree) == wholetree) {
                for (int q=1 ; q &lt; MAXPREVSENTENCES; ++q) {// I am looking for the prev sentence (hence we start with 1)
                    if (forest.size()-1 &lt; q) break; // If I don't have it, break
                    Tree prevTree = forest.get(forest.size()-1-q); // go to previous tree
                    // Now we look for each S subtree, in order of recency (hence right-to-left, hence opposite order of that of .children() ).
                    ArrayList&lt;Tree&gt; backlist = new ArrayList&lt;Tree&gt;();
                    for (Tree child : prevTree.children()) {
                        for (Tree subtree : child) {
                            if (subtree.label().value().equals(""S"")) {
                                backlist.add(child);
                                break;
                            }
                        }
                    }
                    for (int i = backlist.size()-1 ; i &gt;=0 ; --i) {
                        Tree Treetovisit = backlist.get(i);
                        for (Tree relative : Treetovisit.children()) {
                            for (Tree candidate : relative) {
                                if (candidate.contains(pronoun)) continue; // I am looking to all the nodes to the LEFT (i.e. coming before) the path leading to X. contain &lt;-&gt; in the path
                                //System.out.println(""LABEL: ""+relative.label().value() + ""\n\tVALUE: ""+relative.firstChild());
                                if (candidate.label().value().equals(""NP"")) { // ""Propose as the antecedent any NP node that you find""
                                    if (!candidates.contains(candidate)) candidates.add(candidate);
                                }
                            }
                        }
                    }
                }
                break; // It will always come here eventually
            }
            // Step 5
            ancestor = X.parent(wholetree);
            //System.out.println(""LABEL: ""+pronoun.label().value() + ""\n\tVALUE: ""+pronoun.firstChild());
            while ( !ancestor.label().value().equals(""NP"") &amp;&amp; !ancestor.label().value().equals(""S"") )
                ancestor = ancestor.parent(wholetree);
            X = ancestor;
            // Step 6
            if (X.label().value().equals(""NP"")) { // If X is an NP
                for (Tree child : X.children()) { // Find the nominal nodes that X directly dominates
                    if (child.label().value().equals(""NN"") || child.label().value().equals(""NNS"") || child.label().value().equals(""NNP"") || child.label().value().equals(""NNPS"") )
                        if (! child.contains(pronoun)) candidates.add(X); // If one of them is not in the path between X and the pronoun, add X to the antecedents
                }
            }
            // Step SETTE
            for (Tree relative : X.children()) {
                for (Tree candidate : relative) {
                    if (candidate.contains(pronoun)) continue; // I am looking to all the nodes to the LEFT (i.e. coming before) the path leading to X. contain &lt;-&gt; in the path
                    //System.out.println(""LABEL: ""+relative.label().value() + ""\n\tVALUE: ""+relative.firstChild());
                    if (candidate.label().value().equals(""NP"")) { // ""Propose as the antecedent any NP node that you find""
                        boolean contains = false;
                        for (Tree oldercandidate : candidates) {
                            if (oldercandidate.contains(candidate)) { 
                                contains=true;
                                break;
                            }
                        }
                        if (!contains) candidates.add(candidate);
                    }
                }
            }
            // Step 8
            if (X.label().value().equals(""S"")) {
                boolean right = false;
                // Now we want all branches to the RIGHT of the path pronoun -&gt; X.
                for (Tree relative : X.children()) {
                    if (relative.contains(pronoun)) {
                        right = true;
                        continue;
                    }
                    if (!right) continue;
                    for (Tree child : relative) { // Go in but do not go below any NP or S node. Go below the rest
                        if (child.label().value().equals(""NP"")) {
                            candidates.add(child);
                            break; // not sure if this means avoid going below NP but continuing with the rest of non-NP children. Should be since its DFS.
                        }
                        if (child.label().value().equals(""S"")) break; // Same
                    }
                }
            }
        }

        // Step 9 is a GOTO, so we use a while.

        System.out.println(pronoun + "": CHAIN IS "" + candidates.toString());
        ArrayList&lt;Integer&gt; scores = new ArrayList&lt;Integer&gt;();

        for (int j=0; j &lt; candidates.size(); ++j) {
            Tree candidate = candidates.get(j);
            Tree parent = null;
            int parent_index = 0;
            for (Tree tree : forest) {
                if (tree.contains(candidate)) { 
                    parent = tree;
                    break;
                }
                ++parent_index;
            }
            scores.add(0);
            if (parent_index == 0) 
                scores.set(j, scores.get(j)+100); // If in the last sentence, +100 points
            scores.set(j, scores.get(j) + syntacticScore(candidate, parent));

            if (existentialEmphasis(candidate)) // Example: ""There was a dog standing outside""
                scores.set(j, scores.get(j)+70);
            if (!adverbialEmphasis(candidate, parent))
                scores.set(j, scores.get(j)+50);
            if (headNounEmphasis(candidate, parent))
                scores.set(j, scores.get(j)+80);

            int sz = forest.size()-1;
//          System.out.println(""pronoun in sentence "" + sz + ""(sz). Candidate in sentence ""+parent_index+"" (parent_index)"");
            int dividend = 1;
            for (int u=0; u &lt; sz - parent_index; ++u)
                dividend *= 2;
            //System.out.println(""\t""+dividend);
            scores.set(j, scores.get(j)/dividend);
            System.out.println(candidate + "" -&gt; "" + scores.get(j) );
        }
        int max = -1;
        int max_index = -1;
        for (int i=0; i &lt; scores.size(); ++i) {
            if (scores.get(i) &gt; max) {
                max_index = i;
                max = scores.get(i);
            }
        }
        Tree final_candidate = candidates.get(max_index);
        System.out.println(""My decision for "" + pronoun + "" is: "" + final_candidate);
        // Decide what candidate, with both gender resolution and Lappin and Leass ranking.

        Tree pronounparent = pronoun.parent(wholetree).parent(wholetree); // 1 parent gives me the NP of the pronoun
        int pos = 0;
        for (Tree sibling : pronounparent.children()) {
            System.out.println(""Sibling ""+pos+"": "" + sibling);
            if (sibling.contains(pronoun)) break;
            ++pos;
        }
        System.out.println(""Before setchild: "" + pronounparent);
        @SuppressWarnings(""unused"")
        Tree returnval = pronounparent.setChild(pos, final_candidate);
        System.out.println(""After setchild: "" + pronounparent);

        return wholetree; // wholetree is already modified, since it contains pronounparent
    }

    private static int syntacticScore(Tree candidate, Tree root) {
        // We will check whether the NP is inside an S (hence it would be a subject)
        // a VP (direct object)
        // a PP inside a VP (an indirect obj)
        Tree parent = candidate;
        while (! parent.label().value().equals(""S"")) {
            if (parent.label().value().equals(""VP"")) return 50; // direct obj
            if (parent.label().value().equals(""PP"")) {
                Tree grandparent = parent.parent(root);
                while (! grandparent.label().value().equals(""S"")) {
                    if (parent.label().value().equals(""VP"")) // indirect obj is a PP inside a VP
                        return 40;
                    parent = grandparent;
                    grandparent = grandparent.parent(root);
                } 
            }
            parent = parent.parent(root);
        }
        return 80; // If nothing remains, it must be the subject
    }

    private static boolean existentialEmphasis(Tree candidate) {
        // We want to check whether our NP's Dets are ""a"" or ""an"".
        for (Tree child : candidate) {
            if (child.label().value().equals(""DT"")) {
                for (Tree leaf : child) {
                    if (leaf.value().equals(""a"")||leaf.value().equals(""an"")
                            ||leaf.value().equals(""A"")||leaf.value().equals(""An"") ) {
                        //System.out.println(""Existential emphasis!"");
                        return true;
                    }
                }
            }
        }
        return false;
    }

    private static boolean headNounEmphasis(Tree candidate, Tree root) {
        Tree parent = candidate.parent(root);
        while (! parent.label().value().equals(""S"")) { // If it is the head NP, it is not contained in another NP (that's exactly how the original algorithm does it)
            if (parent.label().value().equals(""NP"")) return false;
            parent = parent.parent(root);
        }
        return true;
    }

    private static boolean adverbialEmphasis(Tree candidate, Tree root) { // Like in ""Inside the castle, King Arthur was invincible"". ""Castle"" has the adv emph.
        Tree parent = candidate;
        while (! parent.label().value().equals(""S"")) {
            if (parent.label().value().equals(""PP"")) {
                for (Tree sibling : parent.siblings(root)) {
                    if ( (sibling.label().value().equals("",""))) {
                        //System.out.println(""adv Emph!"");
                        return true;
                    }
                }
            }
            parent = parent.parent(root);
        }
        return false;
    }

    public static ArrayList&lt;Tree&gt; findPronouns(Tree t) {
        ArrayList&lt;Tree&gt; pronouns = new ArrayList&lt;Tree&gt;();
        if (t.label().value().equals(""PRP"") &amp;&amp; !t.children()[0].label().value().equals(""I"") &amp;&amp; !t.children()[0].label().value().equals(""you"") &amp;&amp; !t.children()[0].label().value().equals(""You"")) {
            pronouns.add(t);
        }
        else
            for (Tree child : t.children())
                pronouns.addAll(findPronouns(child));
                    return pronouns;
    }

    @SuppressWarnings(""rawtypes"")
    public static ArrayList&lt;List&gt; preprocess(DocumentPreprocessor strarray) {
        ArrayList&lt;List&gt; Result = new ArrayList&lt;List&gt;();
        for (List&lt;HasWord&gt; sentence : strarray) {
            if (!StringUtils.isAsciiPrintable(sentence.toString())) {
                continue; // Removing non ASCII printable sentences
            }
            //string = StringEscapeUtils.escapeJava(string);
            //string = string.replaceAll(""([^A-Za-z0-9])"", ""\\s$1"");
            int nonwords_chars = 0;
            int words_chars = 0;
            for (HasWord hasword : sentence ) {
                String next = hasword.toString();
                if ((next.length() &gt; 30)||(next.matches(""[^A-Za-z]""))) nonwords_chars += next.length(); // Words too long or non alphabetical will be junk
                else words_chars += next.length();
            }
            if ( (nonwords_chars / (nonwords_chars+words_chars)) &gt; 0.5) // If more than 50% of the string is non-alphabetical, it is going to be junk
                continue;   // Working on a character-basis because some sentences may contain a single, very long word
            if (sentence.size() &gt; 100) {
                System.out.println(""\tString longer than 100 words!\t"" + sentence.toString());
                continue;
            }
            Result.add(sentence);
        }
        return Result;
    }
}
</code></pre>
",Parsing & POS Tagging,hobbs algorithm coref resolution implemented hobbs algorithm anaphora resolution together lappin lea ranking alternative bug description algorithm completely informal since sentence correctly resolved implementation sure whether limit implementation actual algorithm version worked found jurafsky martin begin noun phrase np node immediately dominating pronoun go tree first np sentence node encountered call node x call path used reach p traverse branch node x left path p left right breadth first fashion propose antecedent np node encountered ha np node x node x highest node sentence traverse surface parse tree previous sentence text order recency recent first tree traversed left right breadth first manner np node encountered proposed antecedent x highest node sentence continue step node x go tree first np node encountered call new node x call path traversed reach p x np node path p x pas nominal node x immediately dominates propose x antecedent traverse branch node x left path p left right breadth first manner propose np node encountered antecedent x node traverse branch node x right path p left right breadth first manner go np node encountered propose np node encountered antecedent go step look step left path p way interpreted iterate subtrees left right find branch contains pronoun hence part path pronoun x java however way doe process sentence like house king arthur due fact king arthur contains therefore taken account limit hobbs algorithm mistaking something reference full code java using stanford parser
How dependency parse tree is used for sentiment analysis?,"<p>With the announcement from Google on release of Parsey McParseface <a href=""https://research.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html"" rel=""nofollow noreferrer"">syntaxnet</a>
which is claimed to be the most accurate dependency parser. I want to understand how this parser can be used for more accurate sentiment analysis ? If someone can share some blogs or research papers or tutorials which can help me understand the overall flow.</p>
",Parsing & POS Tagging,dependency parse tree used sentiment analysis announcement google release parsey mcparseface syntaxnet claimed accurate dependency parser want understand parser used accurate sentiment analysis someone share blog research paper tutorial help understand overall flow
spaCy-like dependency graph navigation in CoreNLP,"<p>Is it possible to navigate the dependency parse tree in <code>CoreNLP</code> the way one does that in <code>spaCy</code> as described <a href=""https://spacy.io/docs/usage/dependency-parse#navigating"" rel=""nofollow noreferrer"">here</a>? So far I see that token attributes like lemmas, POS tags, etc. are retrievable through an index, e.g. <code>sent.lemmas(5)</code> returns the lemma of the sixth token. I am not sure this exists for dependency heads and relations. Is there an established way of using these apart from navigating the whole tree every time?</p>
",Parsing & POS Tagging,spacy like dependency graph navigation corenlp possible navigate dependency parse tree way one doe described far see token attribute like lemma po tag etc retrievable index e g return lemma sixth token sure exists dependency head relation established way using apart navigating whole tree every time
Can this sentence be parsed using this grammar?,"<p>Here is a selection of a lexicon and a grammar </p>

<blockquote>
  <p>Noun -> stench | breeze | wumpus | pits ...</p>
  
  <p>Verb -> is | feels | smells | smell | see | stinks ...</p>
  
  <p>Adjective -> right | dead | smelly | breezy ...</p>
  
  <p>Adverb -> here | ahead | nearby ...</p>
  
  <p>Pronoun -> me | you | I | it ...</p>
  
  <p>RelPro -> that | which | who | whom ...</p>
  
  <p>Name -> John | Mary | Boston ...</p>
  
  <p>Article -> the | a | an | every ...</p>
  
  <p>Prep -> to | in | on | near ...</p>
  
  <p>Conj -> and | or | but | yet ...</p>
  
  <p>Digit -> 0 | 1 | 2 | 3 | 4 ...</p>
</blockquote>

<p>Grammar rules is below:</p>

<blockquote>
  <p>S -> NP VP            | S Conj S</p>
  
  <p>NP -> Pronoun     | Namae    | Noun     | Article Noun     | Article
  Adjs Noun     | Digit Digit     | NP PP     | NP RelClause </p>
  
  <p>VP -> Verb    | VP NP    | VP Adjective    | VP PP    | VP Adverb </p>
  
  <p>Adjs -> Adjective     | Adjective Adjs </p>
  
  <p>PP -> Prep NP </p>
  
  <p>RelClause -> RelPro VP</p>
</blockquote>

<p>The Question: Is this sentence ""Mary smells the wumpus in the pit that stinks."" generated by this grammar?</p>

<p>My Answer: No, because ""pit"" was not defined in the grammar.</p>

<p>Question from me to experts at nlp: Is my logic and understanding of parse trees correct in and my given answer correct?  the reason it could not be generated because ""pit"" was not defined in the grammar. Note: I am able to create a parse tree and draw it, if i change the sentence to ""pits"".</p>
",Parsing & POS Tagging,sentence parsed using grammar selection lexicon grammar noun stench breeze wumpus pit verb feel smell smell see stink adjective right dead smelly breezy adverb ahead nearby pronoun relpro name john mary boston article every prep near conj yet digit grammar rule np vp conj np pronoun namae noun article noun article adjs noun digit digit np pp np relclause vp verb vp np vp adjective vp pp vp adverb adjs adjective adjective adjs pp prep np relclause relpro vp question sentence mary smell wumpus pit stink generated grammar answer pit wa defined grammar question expert nlp logic understanding parse tree correct given answer correct reason could generated pit wa defined grammar note able create parse tree draw change sentence pit
Where I can find edu.stanford.nlp.parser.nndep,"<p>I am trying to use Dependency Parser of Stanford's NLP in my project.
I am looking for <code>edu.stanford.nlp.parser.nndep.DependencyParser.class</code> but cannot find any jar. </p>

<p>Where I can find. Should I have to run all CoreNLP to build jar? </p>
",Parsing & POS Tagging,find edu stanford nlp parser nndep trying use dependency parser stanford nlp project looking find jar find run corenlp build jar
How to manipulate `dot` graph in python,"<p>Stanford dependency parser generates a graph in <code>dot</code> format, which looks something like this.</p>

<p>Sentence => ""John is a Computer Scientist""</p>

<pre><code>digraph G{
edge [dir=forward]
node [shape=plaintext]

0 [label=""0 (None)""]
0 -&gt; 5 [label=""root""]
1 [label=""1 (John)""]
2 [label=""2 (is)""]
3 [label=""3 (a)""]
4 [label=""4 (Computer)""]
5 [label=""5 (Scientist)""]
5 -&gt; 2 [label=""cop""]
5 -&gt; 4 [label=""compound""]
5 -&gt; 3 [label=""det""]
5 -&gt; 1 [label=""nsubj""]
}
</code></pre>

<p>I want to find terms like <code>Computer</code> and <code>Scientist</code> and combine them into one. I already have code that gives me which terms to be combined. However I am not able to find such terms in graph, and combine to generate a <code>dot</code> format output. I looked at <code>Graphviz</code> which can create new graph and plot and save files, but I could not find a way to search for nodes and combine them. How should I do this?</p>

<p>PS : I looked at <a href=""http://igraph.org/python/doc/tutorial/tutorial.html"" rel=""nofollow noreferrer"">iGraph</a>, however, it does not support reading <code>dot</code> format. I also found <code>graph-tools</code>, which I think is a very heavy utility for some simple task like this. (One of its dependency itself is about 200MB). Even in NetworkX documentation, I could not any method to search a node with a particular label. </p>
",Parsing & POS Tagging,manipulate graph python stanford dependency parser generates graph format look something like sentence john computer scientist want find term like combine one already code give term combined however able find term graph combine generate format output looked create new graph plot save file could find way search node combine p looked igraph however doe support reading format also found think heavy utility simple task like one dependency mb even networkx documentation could method search node particular label
NLP POS Tree understanding,"<p>How does tree gets formed in NLP -> parts of speech tagging. What is algorithm behind this? 
(S
   (NP Alice)
   (VP
      (V chased)
      (NP
         (Det the)
         (N rabbit))))</p>

<p>for instance how can Det ""the"" and N ""rabbit"" become NP (grouped under NP?)
What is algorithm behind tree formation and aggregation of nodes </p>
",Parsing & POS Tagging,nlp po tree understanding doe tree get formed nlp part speech tagging algorithm behind np alice vp v chased np det n rabbit instance det n rabbit become np grouped np algorithm behind tree formation aggregation node
How to get constituency-based parse tree from Parsey McParseface,"<p>Parsey McParsey returns a dependency-based parse tree by default, but is their a way to get a constituency-based parse tree from it?</p>

<p>EDIT: To clarify, by ""to get from it"" I mean from the Parsey itself. Though building a tree from ConLL output would be an option too.</p>
",Parsing & POS Tagging,get constituency based parse tree parsey mcparseface parsey mcparsey return dependency based parse tree default way get constituency based parse tree edit clarify get mean parsey though building tree conll output would option
Stanford CorenNLP Phrase POS tags and lemmatization explanation,"<p>I have this result during lemmatization of the phrase:</p>

<blockquote>
  <p>Gathered requirements</p>
</blockquote>

<p>Using the <a href=""http://corenlp.run/"" rel=""nofollow noreferrer"">corenlp online tool</a>, POS-tagging and lemmatization of this phrase results in:</p>

<p><a href=""https://i.sstatic.net/XQGIz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XQGIz.png"" alt=""enter image description here""></a></p>

<p>For some reason ""Gathered"" is given a POS-tag of ""JJ"" (""adjective""), which presumably results in the lemma being ""gathered"" rather than ""gather"".</p>

<p>If the input phrase is <code>gathered requirements</code> (i.e. lower-cased), then the POS tag is correctly identified as a verb, and the lemmatization result is what I expected:</p>

<p><a href=""https://i.sstatic.net/lgwR8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lgwR8.png"" alt=""enter image description here""></a></p>

<p>Why is CoreNLP identifying <code>Gathered</code> as an adjective rather than a verb?</p>
",Parsing & POS Tagging,stanford corennlp phrase po tag lemmatization explanation result lemmatization phrase gathered requirement using corenlp online tool po tagging lemmatization phrase result reason gathered given po tag jj adjective presumably result lemma gathered rather gather input phrase e lower cased po tag correctly identified verb lemmatization result expected corenlp identifying adjective rather verb
How to translate syntatic parse to a dependency parse tree?,"<p>Using <a href=""http://abiword.org/projects/link-grammar/"" rel=""nofollow noreferrer"">Link Grammar</a> I can have the syntaxic parse of sentences something like the following:</p>

<pre><code>    +-------------------Xp------------------+
    +-------&gt;WV-------&gt;+------Ost------+    |
    +-----Wd----+      |  +----Ds**x---+    |
    |     +Ds**c+--Ss--+  +-PHc+---A---+    |
    |     |     |      |  |    |       |    |
LEFT-WALL a  koala.n is.v a cute.a animal.n . 

    +---------------------Xp--------------------+
    +-------&gt;WV------&gt;+---------Osm--------+    |
    +-----Wd----+     |  +------Ds**x------+    |
    |     +Ds**c+--Ss-+  +--PHc-+-----A----+    |
    |     |     |     |  |      |          |    |
LEFT-WALL a  wolf.n is.v a dangerous.a animal.n . 

    +--------------------Xp--------------------+
    +-------&gt;WV------&gt;+--------Ost--------+    |
    +-----Wd----+     |  +------Ds**x-----+    |
    |     +Ds**c+--Ss-+  +--PHc-+----A----+    |
    |     |     |     |  |      |         |    |
LEFT-WALL a   dog.n is.v a faithful.a animal.n . 

    +-----------------------Xp----------------------+
    +-------&gt;WV-------&gt;+----------Osm----------+    |
    +-----Wd----+      |   +-------Ds**x-------+    |
    |     +Ds**c+--Ss--+   +--PHv--+-----A-----+    |
    |     |     |      |   |       |           |    |
LEFT-WALL a monkey.n is.v an independant.a animal.n . 
</code></pre>

<p>The problem with this that it's not possible AFAIK to make sens
of that output programmatically; It seems like the way to go
is to convert that syntaxic output to a dependency parse tree
how can I achieve that?</p>
",Parsing & POS Tagging,translate syntatic parse dependency parse tree using link grammar syntaxic parse sentence something like following problem possible afaik make sen output programmatically seems like way go convert syntaxic output dependency parse tree achieve
How to use bag of words or tf-idf to classify text,"<p>I have a general question regarding classifying using bag of words or similar methods.</p>

<p>I have text that I am trying to classify.The classes are known to me and I know that each sentence of the text belongs to one type of sentences.For example sentence 1 should be an order,Sentence 2 should be news, etc.</p>

<p>So what I was thinking is to use n-gram generation for feature extraction and my idea is that n-grams of words can be helpful for the machine to find the right category.But implementing the idea using Python is not easy for me.I can not connect concepts with impelementation. For example I am not sure if I have to supply all possible chunks of POS tags that can belong to each category or the machine can find them.Also, I feel that n-grams can be helpful in this kind of analysis.But I don't know how.</p>

<p>It would be great if can give me some ideas or tell me the steps I should take to do this kind of classification.</p>

<p>Best</p>
",Parsing & POS Tagging,use bag word tf idf classify text general question regarding classifying using bag word similar method text trying classify class known know sentence text belongs one type sentence example sentence order sentence news etc wa thinking use n gram generation feature extraction idea n gram word helpful machine find right category implementing idea using python easy connect concept impelementation example sure supply possible chunk po tag belong category machine find also feel n gram helpful kind analysis know would great give idea tell step take kind classification best
Finding POS-TAG Frequency in sentences of a corpus,"<p>Q. Finding pos-tag frequency/ sentence. Forgive me I am new to python started about 4 months ago. I was able to figure out how to apply pos-tags to the words in the document.</p>

<pre><code>        train_text = SOME TEXT 1
        sample_text = SOME TEXT 2

        custom_sent_tokenizer = PunktSentenceTokenizer(train_text)

        tokenized = custom_sent_tokenizer.tokenize(sample_text)

def process_content():
    try:
       for i in tokenized:
          words = nltk.word_tokenize(i)
          tagged = nltk.pos_tag(words)
          print(tagged)

    except Exception as e:
      print(str(e))
 process_content()
</code></pre>

<p>Now here is where I have no idea to proceed. At this point I am interested in calculating the frequency of certain POS_TAGs per sentence. Then when I do that, I want to plot the sentence length (number of words) in relation to the number of the POS-TAGs I identified. When I tried to do this I was only able to find the frequency of the post-tags in the whole document in relation to all the words in the document. Even though I tokenized already I am still getting the whole document when I analyze. Help this is driving my nuts!</p>
",Parsing & POS Tagging,finding po tag frequency sentence corpus q finding po tag frequency sentence forgive new python started month ago wa able figure apply po tag word document idea proceed point interested calculating frequency certain po tag per sentence want plot sentence length number word relation number po tag identified tried wa able find frequency post tag whole document relation word document even though tokenized already still getting whole document analyze help driving nut
How to extract the primary subject and object phrases from a complex sentence?,"<p>In the documentation for the Stanford Parser, the following example sentence is given: </p>

<blockquote>
  <p>The strongest rain ever recorded in India shut down the financial hub
  of Mumbai, snapped communication lines, closed airports and forced
  thousands of people to sleep in their offices or walk home during the
  night, officials said today.</p>
</blockquote>

<p>This produces the parse tree:</p>

<blockquote>
  <p>[ROOT [S [S [NP [NP [DT The] [JJS strongest] [NN rain] ] [VP [ADVP [RB
  ever] ] [VBN recorded][PP [IN in] [NP [NNP India] ] ] ] ] [VP [VP [VBD
  shut] [PRT [RP down] ] [NP [NP [DT the] [JJ financial] [NN hub] ] [PP
  [IN of] [NP [NNP Mumbai] ] ] ] ] [, ,] [VP [VBD snapped] [NP [NN
  communication] [NNS lines] ] ] [, ,] [VP [VBD closed] [NP [NNS
  airports] ] ] [CC and] [VP [VBD forced] [NP [NP [NNS thousands] ] [PP
  [IN of] [NP [NNS people] ] ] ] [S [VP [TO to] [VP [VP [VB sleep] [PP
  [IN in] [NP [PRP$ their] [NNS offices] ] ] ] [CC or] [VP [VB walk] [NP
  [NN home] ] [PP [IN during] [NP [DT the] [NN night] ] ] ] ] ] ] ] ] ]
  [, ,] [NP [NNS officials] ] [VP [VBD said] [NP-TMP [NN today] ] ] [.
  .] ] ]</p>
</blockquote>

<p>(see <a href=""https://i.sstatic.net/yjnJT.png"" rel=""nofollow"">https://i.sstatic.net/yjnJT.png</a>).</p>

<p><strong>What sort of NLP tool would be able to output the sentential subject and object from the above complex sentence example?</strong> Desired output:</p>

<pre><code>sentence_subj_phrase = ""the strongest rain ever recorded in India""
sentence_obj_phrase = ""the financial hub of Mumbai""
</code></pre>

<p><strong>FROM ORIGINAL OP's POST</strong> (It's just details about what he's thinks doesn't work):</p>

<p>A naive way of extracting the subject and object in a sentence is to find the noun phrases immediately preceding and succeeding the verb. In complex sentences, however, there are multiple verbs, and thus multiple subjects and objects. It is possible to consider complex sentences like this as multiple sentences (using the first part of the independent clause as the ""root"", and replacing the second part with each of the dependent clauses), but usually the first clause is the most important and could be considered the main ""topic"" of the sentence.</p>

<p>Doing a simple BFS to find the first NP prior to a verb will result in ""officials"" being the subject, since it is at the lowest depth level. This doesn't capture the intuition of the first clause containing the subject. One approach I tried was searching for the NPs in the first ""base"" S node (i.e., lowest level subtree rooted at an S node), but in this case that would capture nodes rooted at S<sub>3</sub>.</p>
",Parsing & POS Tagging,extract primary subject object phrase complex sentence documentation stanford parser following example sentence given strongest rain ever recorded india shut financial hub mumbai snapped communication line closed airport forced thousand people sleep office walk home night official said today produce parse tree root np np dt jjs strongest nn rain vp advp rb ever vbn recorded pp np nnp india vp vp vbd shut prt rp np np dt jj financial nn hub pp np nnp mumbai vp vbd snapped np nn communication nns line vp vbd closed np nns airport cc vp vbd forced np np nns thousand pp np nns people vp vp vp vb sleep pp np prp nns office cc vp vb walk np nn home pp np dt nn night np nns official vp vbd said np tmp nn today see sort nlp tool would able output sentential subject object complex sentence example desired output original op post detail think work naive way extracting subject object sentence find noun phrase immediately preceding succeeding verb complex sentence however multiple verb thus multiple subject object possible consider complex sentence like multiple sentence using first part independent clause root replacing second part dependent clause usually first clause important could considered main topic sentence simple bfs find first np prior verb result official subject since lowest depth level capture intuition first clause containing subject one approach tried wa searching np first base node e lowest level subtree rooted node case would capture node rooted
How to convert Earley recognizer to Earley Parser,"<p>I have implemented an Earley Recognizer algorithm. I am having trouble figuring out how to get the parse tree from the chart. I have back pointers pointing to the rule that ""generated"" the rule being added to the chart, but I am taking this quite literally in that, for the current rule <code>R1</code>:</p>

<p>1) if it has a terminal after the dot, check the terminal against the input sentence, add the rule <code>R2</code> to the next column where <code>R2</code> is the same as <code>R1</code> but with the dot shifted. The back pointer of <code>R2</code> = the back pointer of <code>R1</code>. </p>

<p>2) if there is a non terminal after the dot, add new rules to the current column and the back pointers for each of the new rules point to <code>R1</code></p>

<p>3) if there is nothing after the dot (completed rule), then <code>R1</code> is complete, so we scan in the column (less than curr column) associated with <code>R1</code>, find all rules <code>Rj</code> that have the left hand side of <code>R1</code> after the dot, add <code>Rj</code> to the current column but shift the dot, make the backpointer of <code>Rj</code> point to <code>R1</code>.</p>

<p>I don't think I am getting the right output, so I'm wondering if it's a problem with my logic. What needs to be done to the Earley recognizer to convert it to an Earley parser? </p>

<p>I have a <code>print_parse</code> method which recurses on the back pointers of the rules, but I don't think it produces the correct output. For the sentence</p>

<p><code>Papa ate the caviar</code></p>

<p>with (ignoring probabilities) grammar</p>

<pre><code>1   ROOT    S
1   S   NP VP
0.8 NP  Det N
0.1 NP  NP PP
0.7 VP  V NP
0.3 VP  VP PP
1   PP  P NP
0.1 NP  Papa
0.5 N   caviar
0.5 N   spoon
1   V   ate
1   P   with
0.5 Det the
0.5 Det a
</code></pre>

<p>it generates:</p>

<pre><code>(ROOT ['S'])(S ['NP', 'VP'])(NP ['Papa'])(S ['NP', 'VP'])(VP ['V', 'NP'])(V ['ate'])(VP ['V', 'NP'])(NP ['Det', 'N'])(Det ['the'])(NP ['Det', 'N'])(N ['caviar'])(ROOT ['S'])
</code></pre>

<p>I know the parse chart is correct however as I've checked it against doing the parse table manually (by hand). 
I know other questions have been asked about this, but they all point to papers and frankly they are difficult. I would really appreciate some help. </p>
",Parsing & POS Tagging,convert earley recognizer earley parser implemented earley recognizer algorithm trouble figuring get parse tree chart back pointer pointing rule generated rule added chart taking quite literally current rule ha terminal dot check terminal input sentence add rule next column dot shifted back pointer back pointer non terminal dot add new rule current column back pointer new rule point nothing dot completed rule complete scan column le curr column associated find rule left hand side dot add current column shift dot make backpointer point think getting right output wondering problem logic need done earley recognizer convert earley parser method recurses back pointer rule think produce correct output sentence ignoring probability grammar generates know parse chart correct however checked parse table manually hand know question asked point paper frankly difficult would really appreciate help
An Algorithm to Determine How Similar Two Sentences Are,"<p>A friend of mine had an idea to make a speed reading program that displays words one by one (much like currently existing speed reading programs). However, the program would filter out words that aren't completely necessary to the meaning (if you want to skim something). </p>

<p>I have starting to implement this program, but I'm not quite sure on what the algorithm to get rid of ""unimportant"" words should be. </p>

<p>My idea is to parse the sentence (I'm currently using Stanford Parser) and somehow assign weights based on how important that word is to the sentence's meaning to each word then start removing words with the with the lowest weights. I will continue to do this, check how ""different"" the original tree and the new tree is. I will continue to remove the word with the lowest weight until the two trees are too different (I will determine some constant via a ""calibration"" process that each user goes through once). Finally, I will go through each word of the shortened sentence and try to replace it with a simpler or shorter synonym for that word (again while still trying to retain value).</p>

<p>As well, there will be special cases for very common words like ""the,"" ""a,"" and ""of.""</p>

<p>For example:</p>

<p>""Billy said to Jane, 'Do you want to go out?'""</p>

<p>Would become:</p>

<p>""Billy told Jane 'want go out?'""</p>

<p>This would retain basically all of the meaning of the sentence but shortened it significantly. </p>

<p><strong>Is this a good idea for an algorithm and if so how would I assign the weights, what tree comparison algorithm should I use, and is inserting the synonyms done in a good place (i.e. should it be done before I try to remove any words)?</strong></p>
",Parsing & POS Tagging,algorithm determine similar two sentence friend mine idea make speed reading program display word one one much like currently existing speed reading program however program would filter word completely necessary meaning want skim something starting implement program quite sure algorithm get rid unimportant word idea parse sentence currently using stanford parser somehow assign weight based important word sentence meaning word start removing word lowest weight continue check different original tree new tree continue remove word lowest weight two tree different determine constant via calibration process user go finally go word shortened sentence try replace simpler shorter synonym word still trying retain value well special case common word like example billy said jane want go would become billy told jane want go would retain basically meaning sentence shortened significantly good idea algorithm would assign weight tree comparison algorithm use inserting synonym done good place e done try remove word
How to keep punctuation in Stanford dependency parser,"<p>I am using Stanford CoreNLP (01.2016 version) and I would like to keep the punctuation in the dependency relations. I have found some ways for doing that when you run it from command line, but I didn't find anything regarding the java code which extracts the dependency relations.</p>

<p>Here is my current code. It works, but no punctuation is included:</p>

<pre><code>Annotation document = new Annotation(text);

        Properties props = new Properties();

        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, parse"");

        props.setProperty(""ssplit.newlineIsSentenceBreak"", ""always"");

        props.setProperty(""ssplit.eolonly"", ""true"");

        props.setProperty(""pos.model"", modelPath1);

        props.put(""parse.model"", modelPath );

        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        pipeline.annotate(document);

        LexicalizedParser lp = LexicalizedParser.loadModel(modelPath + lexparserNameEn,

                ""-maxLength"", ""200"", ""-retainTmpSubcategories"");

        TreebankLanguagePack tlp = new PennTreebankLanguagePack();

        GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();

        List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

        for (CoreMap sentence : sentences) {

            List&lt;CoreLabel&gt; words = sentence.get(CoreAnnotations.TokensAnnotation.class);               

            Tree parse = lp.apply(words);

            GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
            Collection&lt;TypedDependency&gt; td = gs.typedDependencies();

            parsedText += td.toString() + ""\n"";
</code></pre>

<p>Any kind of dependency relation is OK for me, basic, typed, collapsed, etc.
I just want to include the punctuation marks.</p>

<p>Thanks in advance,</p>
",Parsing & POS Tagging,keep punctuation stanford dependency parser using stanford corenlp version would like keep punctuation dependency relation found way run command line find anything regarding java code extract dependency relation current code work punctuation included kind dependency relation ok basic typed collapsed etc want include punctuation mark thanks advance
How to programmatically determine the Parts of Speech tag of a word?,"<p>Been wondering how to determine the POS tag of a word accurately. I played with POS taggers such as Stanford NLP etc, but they hit and miss as a word like ""respond"" is sometimes tagged as a NN (noun) when it is a verb (VB).</p>

<p>Would querying wordnet, or a dictionary dump be more accurate? Eg the word ""respond"" is a verb, and can also be a noun. Or perhaps infer from ngrams or add in a frequency based sanity check?</p>
",Parsing & POS Tagging,programmatically determine part speech tag word wondering determine po tag word accurately played po tagger stanford nlp etc hit miss word like respond sometimes tagged nn noun verb vb would querying wordnet dictionary dump accurate eg word respond verb also noun perhaps infer ngrams add frequency based sanity check
Stanford Dependency Parser - how to get spans?,"<p>I'm doing dependency parsing with the Stanford library in Java.
Is there any way to get back the indices within my original string of a dependency?
I have tried to call the getSpans() method, but it returns null for every token:</p>

<pre><code>LexicalizedParser lp = LexicalizedParser.loadModel(
        ""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"",
        ""-maxLength"", ""80"", ""-retainTmpSubcategories"");
TreebankLanguagePack tlp = new PennTreebankLanguagePack();
GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
Tree parse = lp.apply(text);
GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
Collection&lt;TypedDependency&gt; tdl = gs.typedDependenciesCollapsedTree();
for(TypedDependency td:tdl)
{
      td.gov().getSpan()  // it's null!
      td.dep().getSpan()  // it's null!
}
</code></pre>

<p>Any idea?</p>
",Parsing & POS Tagging,stanford dependency parser get span dependency parsing stanford library java way get back index within original string dependency tried call getspans method return null every token idea
Spacy NLP - Chunking with Regular Expressions,"<p>Spacy includes the <code>noun_chunks</code> functionality to retrieve set of Noun -Phrases. 
The function <code>english_noun_chunks</code> (attached below) uses <code>word.pos == NOUN</code></p>

<pre><code>def english_noun_chunks(doc):
    labels = ['nsubj', 'dobj', 'nsubjpass', 'pcomp', 'pobj',
              'attr', 'root']
    np_deps = [doc.vocab.strings[label] for label in labels]
    conj = doc.vocab.strings['conj']
    np_label = doc.vocab.strings['NP']
    for i in range(len(doc)):
        word = doc[i]
        if word.pos == NOUN and word.dep in np_deps:
            yield word.left_edge.i, word.i+1, np_label
        elif word.pos == NOUN and word.dep == conj:
            head = word.head
            while head.dep == conj and head.head.i &lt; head.i:
                head = head.head
            # If the head is an NP, and we're coordinated to it, we're an NP
            if head.dep in np_deps:
                yield word.left_edge.i, word.i+1, np_label
</code></pre>

<p>I would like to get chunks from a sentence that maintain some regular expression. For example, I phrase of zero or more adjectives followed by one or more nouns.</p>

<pre><code>{(&lt;JJ&gt;)*(&lt;NN | NNS | NNP&gt;)+}
</code></pre>

<p>Is it possible w/o overriding the <code>english_noun_chunks</code> function? </p>
",Parsing & POS Tagging,spacy nlp chunking regular expression spacy includes functionality retrieve set noun phrase function attached us would like get chunk sentence maintain regular expression example phrase zero adjective followed one noun possible w overriding function
Using the Stanford Dependency Parser on a previously tagged sentence,"<p>I'm currently using the Twitter POS tagger <a href=""http://www.ark.cs.cmu.edu/TweetNLP/"" rel=""nofollow"">available here</a> to tag out tweets into the Penn-Tree Bank tags.  </p>

<p>Here is that code:</p>

<pre><code>import java.util.List;

import cmu.arktweetnlp.Tagger;
import cmu.arktweetnlp.Tagger.TaggedToken;

/* Tags the tweet text */
List&lt;TaggedToken&gt; tagTweet(String text) throws IOException {

    // Loads Penn Treebank POS tags
    tagger.loadModel(""res/model.ritter_ptb_alldata_fixed.txt"");

    // Tags the tweet text
    taggedTokens = tagger.tokenizeAndTag(text);

    return taggedTokens;
}
</code></pre>

<p>Now I need to identify where the direct objects are in these tags.  After some searching, I've discovered that the Stanford Parser can do this, by way of the Stanford Typed Dependencies, (<a href=""http://nlp.stanford.edu:8080/parser/"" rel=""nofollow"">online example</a>).  By using the dobj() call, I should be able to get what I need.</p>

<p>However, I have not found any good documentation about how to feed already-tagged sentences into this tool.  From what I understand, before using the Dependency Parser I need to create a tree from the sentence's tokens/tags. How is this done? I have not been able to find any example code.</p>

<p>The Twitter POS Tagger contains an instance of the Stanford NLP Tools, so I'm not far off, however I am not familiar enough with the Stanford tools to feed my POS-tagged text into it in order to get the dependency parser to work properly.  <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#f"" rel=""nofollow"">The FAQ does mention this functionality</a>, but without any example code to go off of, I'm a bit stuck.</p>
",Parsing & POS Tagging,using stanford dependency parser previously tagged sentence currently using twitter po tagger available tag tweet penn tree bank tag code need identify direct object tag searching discovered stanford parser way stanford typed dependency online example using dobj call able get need however found good documentation feed already tagged sentence tool understand using dependency parser need create tree sentence token tag done able find example code twitter po tagger contains instance stanford nlp tool far however familiar enough stanford tool feed po tagged text order get dependency parser work properly faq doe mention functionality without example code go bit stuck
How to get a tree visualization for google nl api?,"<p>How can you develop a tree for the syntax analysis in google nl api. Like stanford corenlp uses brat annotation tool to generate the tree. Can we use something like that to generate a dependency parse tree for the json response for google nl api, if so how? </p>

<p>Thanks in advance :)</p>
",Parsing & POS Tagging,get tree visualization google nl api develop tree syntax analysis google nl api like stanford corenlp us brat annotation tool generate tree use something like generate dependency parse tree json response google nl api thanks advance
How do I interpret the results of a sentence&#39;s parse tree built using Spacy in Python?,"<p>I'm trying to build and interpret the results of a parse tree of a sentence using Spacy in Python.
I've used the below code for the same : </p>

<pre><code>from spacy.en import English
nlp=English()
example = ""The angry bear chased the frightened little squirrel""
parsedEx = nlp(unicode(example))
for token in parsedEx:
   print(""Head:"", token.head, "" Left:"",token.left_edge, "" Right:"",token.right_edge ,"" Relationship:"",token.dep_)
</code></pre>

<p>The code gave the below result.Can someone tell me how to interpret it? Thanks in advance!</p>

<pre><code> ('Head:', bear, ' Left:', The, ' Right:', The, ' Relationship:', u'det')
   ('Head:', bear, ' Left:', angry, ' Right:', angry, ' Relationship:', u'amod')
   ('Head:', chased, ' Left:', The, ' Right:', bear, ' Relationship:', u'nsubj')
   ('Head:', chased, ' Left:', The, ' Right:', squirrel, ' Relationship:', u'ROOT')
   ('Head:', squirrel, ' Left:', the, ' Right:', the, ' Relationship:', u'det')
   ('Head:', squirrel, ' Left:', frightened, ' Right:', frightened, ' Relationship:', u'amod')
   ('Head:', squirrel, ' Left:', little, ' Right:', little, ' Relationship:', u'amod')
   ('Head:', chased, ' Left:', the, ' Right:', squirrel, ' Relationship:', u'dobj')
</code></pre>
",Parsing & POS Tagging,interpret result sentence parse tree built using spacy python trying build interpret result parse tree sentence using spacy python used code code gave result someone tell interpret thanks advance
Create another train.txt to train sentiment model for other domains,"<p>I find that data to train the sentiment model in train.txt is the PTB format looks like this.</p>

<pre><code>(3 (2 Yet) (3 (2 (2 the) (2 act)) (3 (4 (3 (2 is) (3 (2 still) (4 charming))) (2 here)) (2 .))))
</code></pre>

<p>which the real sentence should be </p>

<pre><code>Yet the act is still charming here.
</code></pre>

<p>But after parse I got the different structure</p>

<pre><code>(ROOT (S (CC Yet) (NP (DT the) (NN act)) (VP (VBZ is) (ADJP (RB still) (JJ charming)) (ADVP (RB here))) (. .)))
</code></pre>

<p>Follows my code:</p>

<pre><code>public static void main(String args[]){
    // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit,parse"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // read some text in the text variable
    String text = ""Yet the act is still charming here ."";// Add your text here!

    // create an empty Annotation just with the given text
    Annotation annotation = new Annotation(text);

    // run all Annotators on this text

    pipeline.annotate(annotation);

    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
    List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);

    // int sentiment = 0;
    for(CoreMap sentence: sentences) {
        // traversing the words in the current sentence
        Tree tree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
        System.out.println(tree);
        // System.out.println(tree.yield());
        tree.pennPrint(System.out);
        // Tree tree = sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class);
        // sentiment = RNNCoreAnnotations.getPredictedClass(tree);
    }

    // System.out.print(sentiment);
}
</code></pre>

<p>Then two questions appear when I use my own sentences to create train.txt.</p>

<p>1.My tree is different than that in train.txt,I know the number in the latter one is the sentiment polarity.But it seems the tree structure is different, I want to get a binarized parse tree which might look like this</p>

<pre><code>((Yet) (((the) (act)) ((((is) ((still) (charming))) (here)) (.))))
</code></pre>

<p>Once I get the sentiment number I can fill it in to get my own train.txt</p>

<p>2.How to get all phrases at each node of the binarized parse tree, in this example, I should get </p>

<pre><code>Yet
the 
act
the act
is
still 
charming 
still charming 
is still charming
here
is still charming here
.
is still charming here .
the act is still charming here .
Yet the act is still charming here.
</code></pre>

<p>Once I got them I can spend money annotating them by human annotators.</p>

<p>Actually I googled them a lot,but couldn't work them out,so I post here.Any useful answers would appreciated!</p>
",Parsing & POS Tagging,create another train txt train sentiment model domain find data train sentiment model train txt ptb format look like real sentence parse got different structure follows code two question appear use sentence create train txt tree different train txt know number latter one sentiment polarity seems tree structure different want get binarized parse tree might look like get sentiment number fill get train txt get phrase node binarized parse tree example get got spend money annotating human annotator actually googled lot work post useful answer would appreciated
How to get the Stanford-style parse tree (with &quot;noun phrases&quot; and &quot;verb phrases&quot;) in spaCy?,"<p>spaCy provides POS tagging and dependency trees. Is it possible to get what Stanford calls the ""Parse"" tree from it? The difference between these two trees can be seen at the Stanford parser demo at <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow"">http://nlp.stanford.edu:8080/parser/index.jsp</a></p>

<p>Stanford ""Parse"" tree:</p>

<pre><code>(ROOT
  (S
    (NP (NNP John))
    (VP (VBZ likes)
      (NP (PRP him)))
    (. .)))
</code></pre>

<p>Dependency tree: (Provided by spaCy and Stanford parser)</p>

<pre><code>nsubj(likes-2, John-1)
root(ROOT-0, likes-2)
dobj(likes-2, him-3)
</code></pre>

<p>Is it possible to deduce or directly get the parse tree in spaCy?
I have gone through the documentation and I couldn't find any direct API's.</p>
",Parsing & POS Tagging,get stanford style parse tree noun phrase verb phrase spacy spacy provides po tagging dependency tree possible get stanford call parse tree difference two tree seen stanford parser demo stanford parse tree dependency tree provided spacy stanford parser possible deduce directly get parse tree spacy gone documentation find direct api
Get certain nodes out of a Parse Tree,"<p>I am working on a project involving anaphora resolution via Hobbs algorithm. I have parsed my text using the Stanford parser, and now I would like to manipulate the nodes in order to implement my algorithm.</p>

<p>At the moment, I don't understand how to:</p>

<ul>
<li><p>Access a node based on its POS tag (e.g. I need to start with a pronoun - how do I get all pronouns?).</p></li>
<li><p>Use visitors. I'm a bit of a noob of Java, but in C++ I needed to implement a Visitor functor and then work on its hooks. I could not find much for the Stanford Parser's Tree structure though. Is that jgrapht? If it is, could you provide me with some pointers at code snippets?</p></li>
</ul>
",Parsing & POS Tagging,get certain node parse tree working project involving anaphora resolution via hobbs algorithm parsed text using stanford parser would like manipulate node order implement algorithm moment understand access node based po tag e g need start pronoun get pronoun use visitor bit noob java c needed implement visitor functor work hook could find much stanford parser tree structure though jgrapht could provide pointer code snippet
Parse Tree for a proper structured sentence using OpenNLP,"<p>I have an NLP task where I need to make sure that a paragraph of multiple sentences include at least one well structured question, I'm using OpenNLP to generate the parse trees in the paragraph. My questions are:</p>

<p>1-Is there a way to get a list of possible parse trees for a properly structured question.</p>

<p>2- How can I compare two parse trees </p>

<p>Thanks</p>
",Parsing & POS Tagging,parse tree proper structured sentence using opennlp nlp task need make sure paragraph multiple sentence include least one well structured question using opennlp generate parse tree paragraph question way get list possible parse tree properly structured question compare two parse tree thanks
Does MaltParser actually provide an option for returning probabilities of Parse trees?,"<p>While looking at the source code of Malt Parser which actually has class LibLinear.java(jar file) and calls the java version of the liblinear toolkit; I don't find any option/way to return probability despite the information that, in principle training the model using liblinear(by default in malt parser) with Logistic regression(-s 0) should produce probability score of parsed trees.</p>

<p>The main concern is: Do the integration of Liblinear and Malt Parser working smoothly without affecting each other expected operations?</p>

<p>Working separately with Liblinear does give me probability output for the datasets.</p>

<pre><code>liblinear-train -s 0 train_scale 
</code></pre>

<p>//training data using logistic regression model</p>

<pre><code>liblinear-predict -b 1 test_scale train_scale.model test_scale_output 
</code></pre>

<p>//labels and classes and probability outputs.  Here -b 1 does extract out probabilities of each datasets.</p>

<p>Reference: <a href=""https://stackoverflow.com/questions/28791352/how-to-get-probability-score-of-parsed-sentences-using-malt-parser"">https://stackoverflow.com/questions/28791352/how-to-get-probability-score-of-parsed-sentences-using-malt-parser</a></p>
",Parsing & POS Tagging,doe maltparser actually provide option returning probability parse tree looking source code malt parser actually ha class liblinear java jar file call java version liblinear toolkit find option way return probability despite information principle training model using liblinear default malt parser logistic regression produce probability score parsed tree main concern integration liblinear malt parser working smoothly without affecting expected operation working separately liblinear doe give probability output datasets training data using logistic regression model label class probability output b doe extract probability datasets reference href
NLP - Extracting &quot;Correct&quot; Noun Phrases,"<p>I'm stuck with a problem related to NLP and I was hoping I could get some advice to help solve it. I am currently writing a program in which given a sentence that contains a monetary number, the program will be able to return the noun phrase that corresponds to that number. For example, given this sentence:</p>

<p>""That bike costs $100.""</p>

<p>I am matching the noun phrase ""bike"" to ""$100"". I am using Stanford NLP's parser to find the noun phrases in a sentence, and as you know it is possible to have noun phrases within noun phrases. For example, in the sentence:</p>

<p>""The purchase price for these goods was $50.""</p>

<p>The two noun phrases ""purchase price"" and ""goods"" are both contained under the noun phrase ""The purchase price for these goods"", as identified by the Stanford parser. Thus, it would make sense when matching to only take into account what I refer to as base noun phrases, or noun phrases that don't have any other noun phrases contained within it. But this doesn't always work, because in this sentence for example:</p>

<p>""He incurred $23 thousand of costs related to office space and $10 thousand of costs related to office supplies.""</p>

<p>The base noun phrases identified by the parser would be ""costs"" x2, ""office supplies"", and ""office space"". But none of these noun phrases properly describe the numbers ""$23 thousand"" and ""10 thousand"". We really need are the more general noun phrases ""costs of office supplies"" and ""costs of office space"" to adequately describe the numbers.</p>

<p>So I'm stuck in trying to figure out a method to extract noun phrases that aren't too broad or too specific, as shown by the last two examples. But so far, it just doesn't seem possible to develop a method that will work for all the different sentences out there. If someone could offer any advice on how to approach this problem, it would be greatly appreciated.</p>

<p>Thanks in advance for your time.</p>
",Parsing & POS Tagging,nlp extracting correct noun phrase stuck problem related nlp wa hoping could get advice help solve currently writing program given sentence contains monetary number program able return noun phrase corresponds number example given sentence bike cost matching noun phrase bike using stanford nlp parser find noun phrase sentence know possible noun phrase within noun phrase example sentence purchase price good wa two noun phrase purchase price good contained noun phrase purchase price good identified stanford parser thus would make sense matching take account refer base noun phrase noun phrase noun phrase contained within always work sentence example incurred thousand cost related office space thousand cost related office supply base noun phrase identified parser would cost x office supply office space none noun phrase properly describe number thousand thousand really need general noun phrase cost office supply cost office space adequately describe number stuck trying figure method extract noun phrase broad specific shown last two example far seem possible develop method work different sentence someone could offer advice approach problem would greatly appreciated thanks advance time
Why is Stanford NLP nesting other phrases inside verb phrases?,"<p>I noticed a full parse nests other phrases inside verb phrases like here (although Noun Phrases seem to be standalone)</p>

<pre><code>(ROOT\n  (S\n    (NP (DT The) (JJ quick) (JJ brown) (NN fox))\n    (VP (VBD jumped)\n      (PP (IN over)\n        (NP (DT the) (JJ lazy) (NN dog.))))))
</code></pre>

<p>When I run a simple chunking via Apache OpenNLP, verb phrases are standalone like here</p>

<pre><code>[NP The_DT quick_JJ brown_JJ fox_NN ] [VP jumped_VBD ] [PP over_IN ] [NP the_DT lazy_JJ dog_NN ] ._.
</code></pre>

<p>Although, Apache OpenNLPs full parse is nesting verb phrases too. (<a href=""https://opennlp.apache.org/documentation/1.5.2-incubating/manual/opennlp.html#tools.parser.parsing.cmdline"" rel=""nofollow"">https://opennlp.apache.org/documentation/1.5.2-incubating/manual/opennlp.html#tools.parser.parsing.cmdline</a>)</p>

<pre><code>(TOP (NP (NP (DT The) (JJ quick) (JJ brown) (NN fox) (NNS jumps)) (PP (IN over) (NP (DT the) (JJ lazy) (NN dog))) (. .)))
</code></pre>

<p>Is it correct that Stanford NLP nests other phrases inside Verb Phrases?</p>
",Parsing & POS Tagging,stanford nlp nesting phrase inside verb phrase noticed full parse nest phrase inside verb phrase like although noun phrase seem standalone run simple chunking via apache opennlp verb phrase standalone like although apache opennlps full parse nesting verb phrase correct stanford nlp nest phrase inside verb phrase
Converting output of dependency parsing to tree,"<p>I am using <code>Stanford dependency parser</code> and the I get the following output of the sentence</p>

<blockquote>
  <p>I shot an elephant in my sleep</p>
</blockquote>

<pre><code>python dep_parsing.py 
[((u'shot', u'VBD'), u'nsubj', (u'I', u'PRP')), ((u'shot', u'VBD'), u'dobj', (u'elephant', u'NN')), ((u'elephant', u'NN'), u'det', (u'an', u'DT')), ((u'shot', u'VBD'), u'nmod', (u'sleep', u'NN')), ((u'sleep', u'NN'), u'case', (u'in', u'IN')), ((u'sleep', u'NN'), u'nmod:poss', (u'my', u'PRP$'))]
</code></pre>

<p><strong>I want to convert this into a graph with nodes being each token and edges being the relation between them.</strong></p>

<p>I need the graph structure for further processing hence it would help if modification to it are easy and also must be easily representable.</p>

<p>Here is my code till now.</p>

<pre><code>from nltk.parse.stanford import StanfordDependencyParser
stanford_parser_dir = 'stanford-parser/'
eng_model_path = stanford_parser_dir  + ""stanford-parser-models/edu/stanford/nlp/models/lexparser/englishRNN.ser.gz""
my_path_to_models_jar = stanford_parser_dir  + ""stanford-parser-3.5.2-models.jar""
my_path_to_jar = stanford_parser_dir  + ""stanford-parser.jar""

dependency_parser = StanfordDependencyParser(path_to_jar=my_path_to_jar, path_to_models_jar=my_path_to_models_jar)

result = dependency_parser.raw_parse('I shot an elephant in my sleep')
dep = result.next()
a = list(dep.triples())
print a
</code></pre>

<p>How can I make such a graph structure?</p>
",Parsing & POS Tagging,converting output dependency parsing tree using get following output sentence shot elephant sleep want convert graph node token edge relation need graph structure processing hence would help modification easy also must easily representable code till make graph structure
"How to alter Stanford NLP output, so that it is compatible with Apache Open NLP format for POS tagging and Chunking?","<p>Apache OpenNLP formats POS and Chunks somewhat differently than Stanford NLP as per <a href=""https://opennlp.apache.org/documentation/1.6.0/manual/opennlp.html#tools.chunker"" rel=""nofollow"">https://opennlp.apache.org/documentation/1.6.0/manual/opennlp.html#tools.chunker</a></p>

<p>Using ""The quick brown fox jumped over the lazy dog.""</p>

<pre><code>Apache OpenNLP POS: The_DT quick_JJ brown_JJ fox_NN jumped_VBD over_IN the_DT lazy_JJ dog_NN ._.

Apache OpenNLP Chunk:  [NP The_DT quick_JJ brown_JJ fox_NN ] [VP jumped_VBD ] [PP over_IN ] [NP the_DT lazy_JJ dog_NN ] ._.
</code></pre>

<p>I noticed the Stanford Parser does provide Chunks and POS in a full parse and looks like this:</p>

<pre><code>(ROOT\n  (S\n    (NP (DT The) (JJ quick) (JJ brown) (NN fox))\n    (VP (VBD jumped)\n      (PP (IN over)\n        (NP (DT the) (JJ lazy) (NN dog.))))))
</code></pre>

<p>How can we modify the output, so that it becomes compatible with Apache OpenNLP?</p>

<p>I do notice that Stanford NLP uses nested phrases, eg the verb phrase contains a Prepositional as well as a noun phrase...</p>
",Parsing & POS Tagging,alter stanford nlp output compatible apache open nlp format po tagging chunking apache opennlp format po chunk somewhat differently stanford nlp per using quick brown fox jumped lazy dog noticed stanford parser doe provide chunk po full parse look like modify output becomes compatible apache opennlp notice stanford nlp us nested phrase eg verb phrase contains prepositional well noun phrase
Implicit Aspect identification from text Reviews,"<p>I have to identify explicit and implicit aspect from a movie review. </p>

<p>The explicit aspect is the one that expressed in a review directly such as <code>the price of the camera is reasonable</code>. Here, the aspect is <code>price</code> and the opinion word is <code>reasonable</code> which are mentioned directly here. However, when someone says <code>when I watched this film, I Hope it ended as soon as possible</code>, here the writer means that the movie is very boring. However, there is neither <strong>opinion</strong> word nor <strong>aspect</strong> word is mentioned here. we considered here that the sentence hold feature-opinion pair <code>movie-boring</code>.</p>

<p>Now, the question is that how we can identify <code>implicit</code> aspects from unstructured text? While searching I have found that dependency parser will help to make a tree structure of sentence. The next question is which classifiers should be used to measure the accuracy of our system by using dependency parser?</p>
",Parsing & POS Tagging,implicit aspect identification text review identify explicit implicit aspect movie review explicit aspect one expressed review directly aspect opinion word mentioned directly however someone say writer mean movie boring however neither opinion word aspect word mentioned considered sentence hold feature opinion pair question identify aspect unstructured text searching found dependency parser help make tree structure sentence next question classifier used measure accuracy system using dependency parser
How to reverse engineer an NLP parse tree to arrive at the original sentence?,"<p>Given a parse tree (obtained with <a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow"">http://nlp.stanford.edu:8080/corenlp/process</a> pretty print option)</p>

<pre><code>(ROOT (S (NP (PRP You)) (VP (MD could) (VP (VB say) (SBAR (IN that) (S (NP (PRP they)) (ADVP (RB regularly)) (VP (VB catch) (NP (NP (DT a) (NN shower)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ adds) (PP (TO to) (NP (NP (PRP$ their) (NN exhilaration)) (CC and) (NP (FW joie) (FW de) (FW vivre))))))))))))) (. .)))
</code></pre>

<p>How could I arrive at the original sentence?</p>

<pre><code>You could say that they regularly catch a shower, which adds to their exhilaration and joie de vivre.
</code></pre>

<p>I am thinking of using some regexp magic, but I wonder if Stanford NLP has a built in feature to do this task?</p>
",Parsing & POS Tagging,reverse engineer nlp parse tree arrive original sentence given parse tree obtained pretty print option could arrive original sentence thinking using regexp magic wonder stanford nlp ha built feature task
Getting the root word using the Wordnet Lemmatizer,"<p>I need to find a common root word matched for all related words for a keyword extractor.</p>

<p>How to convert words into the same root using the python nltk lemmatizer? </p>

<ul>
<li>Eg: 

<ol>
<li>generalized, generalization -> general </li>
<li>optimal, optimized -> optimize (maybe) </li>
<li>configure, configuration, configured -> configure</li>
</ol></li>
</ul>

<p>The python nltk lemmatizer gives 'generalize', for 'generalized' and 'generalizing' when part of speech(pos) tag parameter is used but not for 'generalization'.</p>

<p>Is there a way to do this?</p>
",Parsing & POS Tagging,getting root word using wordnet lemmatizer need find common root word matched related word keyword extractor convert word root using python nltk lemmatizer eg generalized generalization general optimal optimized optimize maybe configure configuration configured configure python nltk lemmatizer give generalize generalized generalizing part speech po tag parameter used generalization way
Natural language processing parse tree abbreviations,"<p>I've been working on a project that I have to learn to use NLP tools for. I'm writing it in C#, and am thus using the SharpNLP library (based off of OpenNLP), which also includes a WordNet access library. I'm doing some practice with the chunking feature because my program is going to use that a lot to modify some words in a sentence but not others, depending on the role they play in the sentence.</p>

<p>The chunker outputs phrases with parse-tree abbreviation tags attached to them, as well as attached to the individual words (which is more important to me for my situation), but the problem is that I don't know what half of them mean, and I can't seem to find a full list of what all the abbreviations mean; all the parse tree tutorials I find just list the ones that exist on that parse tree. I know things like NP = Noun Phrase, PP = Preposition Phrase, VP = Verb Phrase, and I think that DT = Determiner (I saw it abbreviated as D once). There's a couple more that I know, but I'm sure there's a lot that I don't know (JJ, NNS, NN, etc) so I'm wondering if there is a list somewhere that has all of them listed, as well as ideally a description/examples of each item.</p>
",Parsing & POS Tagging,natural language processing parse tree abbreviation working project learn use nlp tool writing c thus using sharpnlp library based opennlp also includes wordnet access library practice chunking feature program going use lot modify word sentence others depending role play sentence chunker output phrase parse tree abbreviation tag attached well attached individual word important situation problem know half mean seem find full list abbreviation mean parse tree tutorial find list one exist parse tree know thing like np noun phrase pp preposition phrase vp verb phrase think dt determiner saw abbreviated couple know sure lot know jj nns nn etc wondering list somewhere ha listed well ideally description example item
Use SyntaxNet for chunking?,"<p>In an app I'm developing I need to know whether a string of words is a noun phrase, verb phrase, etc. I understand that NP and VP are neither dependencies nor POS. I also understand that to do this I probably need some sort of chunking tool but I couldn't find any open source ones.</p>

<p>In the output of SyntaxNet for the sentence ""She really likes cute black dog"":</p>

<pre><code>likes VBZ ROOT
 +-- She PRP nsubj
 +-- really RB advmod
 +-- dog NN dobj
     +-- cute JJ amod
     +-- black JJ amor
</code></pre>

<p>I noticed that the NP ""cute black dog"" has been put in its own tree node:</p>

<pre><code> +-- dog NN dobj
     +-- cute JJ amod
     +-- black JJ amor
</code></pre>

<p>So I'm wondering if there is anyway I can use SyntaxNet as a chunker? </p>
",Parsing & POS Tagging,use syntaxnet chunking app developing need know whether string word noun phrase verb phrase etc understand np vp neither dependency po also understand probably need sort chunking tool find open source one output syntaxnet sentence really like cute black dog noticed np cute black dog ha put tree node wondering anyway use syntaxnet chunker
Using Stanford CoreNLP Python Parser for specific output,"<p>I'm using <a href=""https://github.com/dasmith/stanford-corenlp-python"" rel=""nofollow"">SCP</a> to get the parse CFG tree for English sentences. </p>

<pre><code>from corenlp import *
corenlp = StanfordCoreNLP()
corenlp.parse(""Every cat loves a dog"")
</code></pre>

<p>My expected output is a tree like this: </p>

<pre><code>(S (NP (DET Every) (NN cat)) (VP (VT loves) (NP (DET a) (NN dog))))
</code></pre>

<p>But what i got is: </p>

<pre><code>(ROOT (S (NP (DT Every) (NN cat)) (VP (VBZ loves) (NP (DT a) (NN dog)))))
</code></pre>

<p>How to change the POS tag as expected and remove the ROOT node?</p>

<p>Thanks</p>
",Parsing & POS Tagging,using stanford corenlp python parser specific output using scp get parse cfg tree english sentence expected output tree like got change po tag expected remove root node thanks
On using Lingua-TreeTagger-0.06,"<p>I am very, very new to NLP and the like. Therefore, I have a very basic question. I want to POS-tag an corpus of files with TreeTagger using a <code>Mac OSX 10.6.8</code>. I have installed <code>TreeTagger</code> by using the instructions provided at <a href=""http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/"" rel=""nofollow"">http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/</a></p>

<p>[<em>I installed it in /Applications</em>]</p>

<p>Then I have installed <code>Lingua-TreeTagger-0.06</code> for '<em>for calling the TreeTagger and manipulating its output</em>'. This took a lot of effort to do. </p>

<p>[<em>I installed it in /Applications</em>]</p>

<p>I think I have succeeded in the previous steps. Now what? I mean how do I call Tree-Tagger? </p>

<p>Thanks to anybody who could help me? </p>

<p>mc</p>
",Parsing & POS Tagging,using lingua treetagger new nlp like therefore basic question want po tag corpus file treetagger using installed using instruction provided installed application installed calling treetagger manipulating output took lot effort installed application think succeeded previous step mean call tree tagger thanks anybody could help mc
Does NLTK have a tool for dependency parsing?,"<p>I'm building a NLP application and have been using the Stanford Parser for most of my parsing work, but I would like to start using Python.</p>

<p>So far, NLTK seems like the best bet, but I cannot figure out how to parse grammatical dependencies.  I.e. this is an example from the Stanford Parser.  I want to be able to produce this in NTLK using Python from the original sentence ""I am switching to Python."":</p>

<pre><code>nsubj(switching-3, I-1)
aux(switching-3, am-2)
prep_to(switching-3, Python-5)
</code></pre>

<p>Can anyone give me a shove in the right direction to parse grammatical dependencies?</p>
",Parsing & POS Tagging,doe nltk tool dependency parsing building nlp application using stanford parser parsing work would like start using python far nltk seems like best bet figure parse grammatical dependency e example stanford parser want able produce ntlk using python original sentence switching python anyone give shove right direction parse grammatical dependency
Separating a compound sentence with multiple subjects into multiple sentences with one subject each?,"<p>Is there a way to separate a compound sentence with multiple subjects like:</p>

<ul>
<li>I am a human, and you are a human. => I am a human. You are a human.</li>
<li>The dog sometimes eats cheese, and it is a labrador. => The dog sometimes eats cheese. It is a labrador.</li>
</ul>

<p>I was thinking about separating it using a parse tree and identifying multiple subjects, but I'm not sure of how to create an algorithm that will do this generally and reliably.</p>
",Parsing & POS Tagging,separating compound sentence multiple subject multiple sentence one subject way separate compound sentence multiple subject like human human human human dog sometimes eats cheese labrador dog sometimes eats cheese labrador wa thinking separating using parse tree identifying multiple subject sure create algorithm generally reliably
Convert to parse tree with POS tag,"<p>I used <a href=""https://github.com/emilmont/pyStatParser"" rel=""nofollow"">pystatParser</a> to generate each parse tree for a English sentence.
Example output (just an example): </p>

<pre><code>(NP+S (NP (DT every) (NN cat)) (VP (VB loves) (NP (DT a) (NN dog))))
</code></pre>

<p>How to convert every output into the below style:</p>

<pre><code>(S (NP (DET Every) (NN cat)) (VP (VT loves) (NP (DET a) (NN dog))))
</code></pre>

<p>Thanks!</p>
",Parsing & POS Tagging,convert parse tree po tag used pystatparser generate parse tree english sentence example output example convert every output style thanks
How to encode categorical Variables to pass to SVM,"<p>I am doing some NLP tasks. A feature in my list is the POS tag of a sentence. How can i pass the POS tags as a feature to SVM as it expects numerical data. </p>
",Parsing & POS Tagging,encode categorical variable pas svm nlp task feature list po tag sentence pas po tag feature svm expects numerical data
How to calculate confidence score from dependency parse tree?,"<p>Is there any way to get confidence score or any score from dependency parse tree of a sentence using ntlk or something else?</p>

<p>Any advice and suggestions will be greatly appreciated!</p>
",Parsing & POS Tagging,calculate confidence score dependency parse tree way get confidence score score dependency parse tree sentence using ntlk something else advice suggestion greatly appreciated
Custom tagger NLTK 3,"<p>I am working with nltk's default tagger to get a POS tag of the word but I am not getting the expected results:</p>

<pre><code>&gt;&gt;&gt; nltk.pos_tag(nltk.tokenize.word_tokenize(""I want a watch""))
[('I', 'PRP'), ('want', 'VBP'), ('a', 'DT'), ('watch', 'NN')]
&gt;&gt;&gt; nltk.pos_tag(nltk.tokenize.word_tokenize(""Lets watch a movie""))
[('Lets', 'NNS'), ('watch', 'VBP'), ('a', 'DT'), ('movie', 'NN')]
</code></pre>

<p>As you can see above, the <code>pos_tag</code> function correctly tags the word <code>watch</code>. But in the below case:</p>

<pre><code>&gt;&gt;&gt; nltk.pos_tag(nltk.tokenize.word_tokenize(""I want to read a book""))
[('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('read', 'VB'), ('a', 'DT'), ('book', 'NN')]

&gt;&gt;&gt; nltk.pos_tag(nltk.tokenize.word_tokenize(""I want to book a ticket""))
[('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('book', 'NN'), ('a', 'DT'), ('ticket', 'NN')]
</code></pre>

<p>It incorrectly predicts the tag for the word <code>book</code>. 
I know we can build a custom tagger but I would not prefer build a tagger from scratch just for one word. I am looking to improve the accuracy of the tagger for the word <code>book</code>. I referred to this <a href=""https://stackoverflow.com/questions/5919355/custom-tagging-with-nltk"">answer</a> but the latest version doesn't seem to have the method <code>nltk.tag._POS_TAGGER</code>.</p>

<p>Is there any possible workaround for this?</p>
",Parsing & POS Tagging,custom tagger nltk working nltk default tagger get po tag word getting expected result see function correctly tag word case incorrectly predicts tag word know build custom tagger would prefer build tagger scratch one word looking improve accuracy tagger word referred href latest version seem method p possible workaround
How can use the xml file outputed by CoreNLP,"<p>I intended first POS tagging a document and then extract nouns or noun phrases, then find the frequent nouns or noun pharsess. What language should I use? is there some package or lib I can use? </p>
",Parsing & POS Tagging,use xml file outputed corenlp intended first po tagging document extract noun noun phrase find frequent noun noun pharsess language use package lib use
Can not figure out how Stanford Dependencies works,"<p>My task is to parse text and find out the main characters in sentences. I need a Stanford Dependencies Parser, but i can't figure out, how and where can i get it. I downloaded CoreNLP as SD is a part of it. What should I do next? Didn't find any tutorials about how SDP works.
I will be very grateful if someone explains me, what i should do.
Thanks!</p>
",Parsing & POS Tagging,figure stanford dependency work task parse text find main character sentence need stanford dependency parser figure get downloaded corenlp sd part next find tutorial sdp work grateful someone explains thanks
reading matrices cotaining dictionaries in python,"<p>I have two matrices (I want them for part of speech tagging). The first one contains the pos tags probabilities and the second contains the words probabilities. I need to extract numbers and sum the matrices. The problem is when I call each cell the string part appears, too. But I need the numbers. How can I call them. (Is this a correct way of making matrices? if not, how can I correct it with tags in heads of rows and columns?)</p>

<pre><code>import numpy as np
A = np.array([[{'ARTART':0}],[{'ARTN':1}],[{'ARTV':0}],[{'ARTP':0}],
          [{'NART':0}],[{'NN':0.13}],[{'NV':0.43}],[{'NP':0.44}],
          [{'VART':0.65}],[{'VN':0.35}],[{'VV':0}],[{'VP':0}],
          [{'PART':0.74}],[{'PN':0.26}],[{'PV':0}],[{'PP':0}],
          [{'NULLART':0.71}],[{'NULLN':0.29}],[{'NULLV':0}],[{'NULLP':0}]]).reshape(5,4)
          #print (A)

B = np.array([[{'ARTflies':0}],[{'ARTlike':0}],[{'ARTa':0.36}],[{'ARTflower':0}],
          [{'Nflies':0.025}],[{'Nlike':0.012}],[{'Na':0.001}],[{'Nflower':0.063}],
          [{'Vflies':0.076}],[{'Vlike':0.1}],[{'Va':0}],[{'Vflower':0.05}],
          [{'Pflies':0}],[{'Plike':0.068}],[{'Pa':0}],[{'Pflower':0}]]).reshape(4,4)
#print (B)
#print (A[4][0])
</code></pre>
",Parsing & POS Tagging,reading matrix cotaining dictionary python two matrix want part speech tagging first one contains po tag probability second contains word probability need extract number sum matrix problem call cell string part appears need number call correct way making matrix correct tag head row column
Extracting phrase n-grams from a sentence corresponding to the main verb,"<p>I am working on a project which needs me to extract similarity between sentences. So given a sentence, I need the phrase n-gram of that sentence, which is 'a combination of the main verb and the noun phrase left and right of the verb'. Any idea how to extract this? I am given the dependency and constituency parse trees of the sentence. I am using Python.</p>

<pre><code>Sample Sentence: My dog also likes eating sausage.
Constituency Parse Tree:
ROOT
(S
    (NP (PRP$ My) (NN dog))
    (ADVP (RB also))
    (VP (VBZ likes)
      (S
        (VP (VBG eating)
          (NP (NN sausage)))))
    (. .)))

Dependency Graph:
nmod:poss(dog-2, My-1)
nsubj(likes-4, dog-2)
advmod(likes-4, also-3)
root(ROOT-0, likes-4)
xcomp(likes-4, eating-5)
dobj(eating-5, sausage-6)
</code></pre>

<p>Main verb : likes</p>

<p>Left Noun Phrase(NP) : My dog</p>

<p>Right Noun Phrase : sausage.</p>
",Parsing & POS Tagging,extracting phrase n gram sentence corresponding main verb working project need extract similarity sentence given sentence need phrase n gram sentence combination main verb noun phrase left right verb idea extract given dependency constituency parse tree sentence using python main verb like left noun phrase np dog right noun phrase sausage
Parsing a sentence with SharpNL &amp; en-parser-chunking.bin,"<p>Using <a href=""https://github.com/knuppe/SharpNL/blob/33b55c10891ddc354251b394b721152f294a2798/src/SharpNL.Tests/Parser/TreeInsert/ParserTest.cs"" rel=""nofollow"">SharpNL</a> and <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow"">OpenNLP</a>'s <code>en-parser-chunking.bin</code>, I'm attempting to parse a sentence into a tree. One of SharpNL's tests shows that, given a model, you can parse a sentence as follows:</p>

<pre><code>var model = SharpNL.Parser.TreeInsert.Parser.Train(""en"", parseSamples, headRules, 100, 0);

var parser = ParserFactory.Create(model);

// Tests parsing to make sure the code does not has
// a bug which fails always with a runtime exception
var p = parser.Parse(Parse.ParseParse(""She was just another freighter from the "" +
        ""States and she seemed as commonplace as her name .""));
</code></pre>

<p>So I downloaded the en-parser-chunking.bin file, created a model from it as well as a parser and attempted to parse the same input:</p>

<pre><code>var parserModelStream = new FileStream(@""en-parser-chunking.bin"", FileMode.Open, FileAccess.Read);
var parserModel = new ParserModel(parserModelStream);
var parser = ParserFactory.Create(parserModel);

var p = parser.Parse(Parse.ParseParse(""She was just another freighter from the "" +
        ""States and she seemed as commonplace as her name .""));
</code></pre>

<p>This code runs, but when I analyze <code>p</code> in the debugger, it has a Head of TOP and no children. Is this an issue with what model I'm using? Or how I'm using it?</p>
",Parsing & POS Tagging,parsing sentence sharpnl en parser chunking bin using sharpnl opennlp attempting parse sentence tree one sharpnl test show given model parse sentence follows downloaded en parser chunking bin file created model well parser attempted parse input code run analyze debugger ha head top child issue model using using
SyntaxNet creating tree to root verb,"<p>I am new to Python and the world of NLP.  The recent announcement of Google's Syntaxnet intrigued me. However I am having a lot of trouble understanding documentation around both syntaxnet and related tools (nltk, etc.)</p>

<p><strong>My goal:</strong> given an input such as ""Wilbur kicked the ball"" I would like to extract the root verb (kicked) and the object it pertains to ""the ball"".</p>

<p>I stumbled across ""spacy.io"" and <a href=""https://spacy.io/demos/displacy?share=8251221623493000114"" rel=""nofollow noreferrer"">this visualization</a> seems to encapsulate what I am trying to accomplish: POS tag a string, and load it into some sort of tree structure so that I can start at the root verb and traverse the sentence.</p>

<p>I played around with the syntaxnet/demo.sh, and as suggested in <a href=""https://stackoverflow.com/questions/37219598/how-to-get-dependency-parse-output-from-syntaxnet"">this thread</a> commented out the last couple lines to get conll output.</p>

<p>I then loaded this input in a python script (kludged together myself, probably not correct):</p>

<pre><code>import nltk
from nltk.corpus import ConllCorpusReader
columntypes = ['ignore', 'words', 'ignore', 'ignore', 'pos']
corp = ConllCorpusReader('/Users/dgourlay/development/nlp','input.conll', columntypes)
</code></pre>

<p>I see that I have access to corp.tagged_words(), but no relationship between the words. Now I am stuck!  How can I load this corpus into a tree type structure?  </p>

<p>Any help is much appreciated!</p>
",Parsing & POS Tagging,syntaxnet creating tree root verb new python world nlp recent announcement google syntaxnet intrigued however lot trouble understanding documentation around syntaxnet related tool nltk etc goal given input wilbur kicked ball would like extract root verb kicked object pertains ball stumbled across spacy io visualization seems encapsulate trying accomplish po tag string load sort tree structure start root verb traverse sentence played around syntaxnet demo sh suggested href thread commented last couple line get conll output loaded input python script kludged together probably correct see access corp tagged word relationship word stuck load corpus tree type structure help much appreciated
How to parse Penn Tree Bank and get all the child trees using stanford NLP?,"<p>Is there a way to parse the PTB tree below to get all the child trees
for example:</p>

<pre><code>Text   :  Today is a nice day.
PTB : (3 (2 Today) (3 (3 (2 is) (3 (2 a) (3 (3 nice) (2 day)))) (2 .)))
</code></pre>

<p>Need All child trees possible</p>

<pre><code>Output  : 
(3 (2 Today) (3 (3 (2 is) (3 (2 a) (3 (3 nice) (2 day)))) (2 .)))
(2 Today)
(3 (3 (2 is) (3 (2 a) (3 (3 nice) (2 day)))) (2 .))
(3 (2 is) (3 (2 a) (3 (3 nice) (2 day))))
(3 (2 is) (3 (2 a) (3 (3 nice) (2 day))))
(2 is)
(3 (2 a) (3 (3 nice) (2 day)))
(2 a)
(3 (3 nice) (2 day))
(3 nice)
(2 day)
(2 .)
</code></pre>
",Parsing & POS Tagging,parse penn tree bank get child tree using stanford nlp way parse ptb tree get child tree example need child tree possible
PTB treebank from CoNLL-X,"<p>I have a CoNLL-X format treebank and the corresponding binary parse tree for each sentence and I want to convert it into a PTB format. Is there any converters or can anyone shed light on the PTB format?</p>
",Parsing & POS Tagging,ptb treebank conll x conll x format treebank corresponding binary parse tree sentence want convert ptb format converter anyone shed light ptb format
Stanford Dependency Parser - How to get phrase vectors?,"<p>In the <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/parser/nndep/DependencyParser.java"" rel=""nofollow"">DependencyParser.java repository</a>, I can see it’s using recursive neural networks.
And from the open lecture (<a href=""http://cs224d.stanford.edu"" rel=""nofollow"">http://cs224d.stanford.edu</a>), I learned that these networks calculate phrase vectors at each node of the parse tree.</p>

<p>I'm trying to make the Parser to output phrase vectors so that I can plot them on the 2-d plane but so far I haven't figured it out. - Can someone please point me to the java object and line numbers where they are calculated? (I suspect that they would be in line 765~)</p>

<pre><code> private void setupClassifierForTraining(List&lt;CoreMap&gt; trainSents, List&lt;DependencyTree&gt; trainTrees, String embedFile, String preModel) {
    double[][] E = new double[knownWords.size() + knownPos.size() + knownLabels.size()][config.embeddingSize];
    double[][] W1 = new double[config.hiddenSize][config.embeddingSize * config.numTokens];
    double[] b1 = new double[config.hiddenSize];
    double[][] W2 = new double[system.numTransitions()][config.hiddenSize];
</code></pre>

<p>And if this is not the correct place to be looking for phrase vectors, I'd really appreciate it if you could point me to the code in the <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow"">CoreNLP project</a> I should be looking at.</p>
",Parsing & POS Tagging,stanford dependency parser get phrase vector dependencyparser java repository see using recursive neural network open lecture learned network calculate phrase vector node parse tree trying make parser output phrase vector plot plane far figured someone please point java object line number calculated suspect would line correct place looking phrase vector really appreciate could point code corenlp project looking
Optimizing function computation in a pandas column?,"<p>Let's assume that I have the following pandas dataframe:</p>

<pre><code>id |opinion
1  |Hi how are you?
...
n-1|Hello!
</code></pre>

<p>I would like to create a new pandas <a href=""https://en.wikipedia.org/wiki/Part-of-speech_tagging"" rel=""nofollow"">POS-tagged</a> column like this:</p>

<pre><code>id|     opinion   |POS-tagged_opinions
1 |Hi how are you?|hi\tUH\thi
                  how\tWRB\thow
                  are\tVBP\tbe
                  you\tPP\tyou
                  ?\tSENT\t?

.....

n-1|     Hello    |Hello\tUH\tHello
                   !\tSENT\t!
</code></pre>

<p>From the documentation a tutorial, I tried several approaches. Particularly:</p>

<pre><code>df.apply(postag_cell, axis=1)
</code></pre>

<p>and</p>

<pre><code>df['content'].map(postag_cell)
</code></pre>

<p>Therefore, I created this POS-tag cell function:</p>

<pre><code>import pandas as pd

df = pd.read_csv('/Users/user/Desktop/data2.csv', sep='|')
print df.head()


def postag_cell(pandas_cell):
    import pprint   # For proper print of sequences.
    import treetaggerwrapper
    tagger = treetaggerwrapper.TreeTagger(TAGLANG='en')
    #2) tag your text.
    y = [i.decode('UTF-8') if isinstance(i, basestring) else i for i in [pandas_cell]]
    tags = tagger.tag_text(y)
    #3) use the tags list... (list of string output from TreeTagger).
    return tags



#df.apply(postag_cell(), axis=1)

#df['content'].map(postag_cell())




df['POS-tagged_opinions'] = (df['content'].apply(postag_cell))

print df.head()
</code></pre>

<p>The above function return the following:</p>

<pre><code>user:~/PycharmProjects/misc_tests$ time python tagging\ with\ pandas.py



id|     opinion   |POS-tagged_opinions
1 |Hi how are you?|[hi\tUH\thi
                  how\tWRB\thow
                  are\tVBP\tbe
                  you\tPP\tyou
                  ?\tSENT\t?]

.....

n-1|     Hello    |Hello\tUH\tHello
                   !\tSENT\t!

--- 9.53674316406e-07 seconds ---

real    18m22.038s
user    16m33.236s
sys 1m39.066s
</code></pre>

<p>The problem is that with large number of <a href=""http://www.mediafire.com/download/5bowh8tzff91fvd/new_corpus.csv"" rel=""nofollow"">opinions</a> it get takes a lot of time:</p>

<p><strong>How to perform pos-tagging more efficiently and in a more pythonic way with pandas and treetagger?</strong>. I believe that this issue is due my pandas limited knowledge, since I tagged very quickly the opinions just with treetagger, out of a pandas dataframe.</p>
",Parsing & POS Tagging,optimizing function computation panda column let assume following panda dataframe would like create new panda po tagged column like documentation tutorial tried several approach particularly therefore created po tag cell function function return following problem large number opinion get take lot time perform po tagging efficiently pythonic way panda treetagger believe issue due panda limited knowledge since tagged quickly opinion treetagger panda dataframe
Stanford NLP parse tree format,"<p>This may be a silly question, but how does one iterate through a parse tree as an output of an NLP parser (like Stanford NLP)? It's all nested brackets, which is neither an <code>array</code> nor a <code>dictionary</code> or any other collection type I've used.</p>

<pre><code>(ROOT\n  (S\n    (PP (IN As)\n      (NP (DT an) (NN accountant)))\n    (NP (PRP I))\n    (VP (VBP want)\n      (S\n        (VP (TO to)\n          (VP (VB make)\n            (NP (DT a) (NN payment))))))))
</code></pre>
",Parsing & POS Tagging,stanford nlp parse tree format may silly question doe one iterate parse tree output nlp parser like stanford nlp nested bracket neither collection type used
What is the state-of-art in extracting Noun Phrases from a textual content?,"<p>I am looking at Stanford NLP Parser for extracting noun phrases from a text. Is there something better that can be used for free? </p>
",Parsing & POS Tagging,state art extracting noun phrase textual content looking stanford nlp parser extracting noun phrase text something better used free
What tools can provide frequency-sorted lists of interesting multi-word terms in documents?,"<p>I have a set of nearly half a million text documents. For each document I want to generate a list of interesting 1-, 2-, and 3-word phrases (i.e. unigrams, bigrams, and trigrams.) By interesting, I mean phrases that occur frequently in the document but relatively infrequently in the entire corpus and phrases that are not junky phrases (e.g. ""such that."") Ideally, the phrases are characteristic of the contents of the document, i.e. those phrases would be helpful in word clouds. And, if there's a way to incorporate part of speech tagging to help identify meaningful phrases that would be a plus.</p>

<p>I'm interested to know what tools and approaches I should consider.</p>

<p>Here's what I am aware of:</p>

<ol>
<li>Elasticsearch (maybe with the Python elasticsearch-py library). Elasticsearch has very sophisticated full-search capabilities. Elasticsearch does have a ""significant terms aggregation"" feature. [1],[2] And, Elasticsearch can rank using TF/IDF. [3]</li>
<li>NLTK. </li>
<li>Scikit-learn.</li>
</ol>

<p>Aside from what tool or set of tools to use, I need to consider my approach. I imagine I'll be experimenting with different tokenizers to split my text into words, removing stop words, then building and indexing the n-grams. I may or may not want to stem words. I'll have to experiment and see. Then, for each document, I'll want to get a frequency-sorted list of n-grams and for each n-gram of each document I'll want to score the n-gram based on its frequency in the document and its frequency in the corpus and provide a scored list of n-grams for each document.</p>

<p>My guess is that a Python-based toolkit and approach would work well as Python has a rich set of text mining tools and the power and flexibility to let me experiment with different parts of my processing pipeline.</p>

<p>I'd appreciate your thoughts on how to proceed and links to similar efforts.</p>

<p>[1]: Elasticsearch and significant terms aggregation <a href=""https://www.elastic.co/blog/significant-terms-aggregation"" rel=""nofollow"">https://www.elastic.co/blog/significant-terms-aggregation</a></p>

<p>[2]: Tag clouds and significant term aggregation
<a href=""http://www.buzzcapture.com/en/2014/04/how-to-create-tag-clouds/"" rel=""nofollow"">http://www.buzzcapture.com/en/2014/04/how-to-create-tag-clouds/</a></p>

<p>[3]: Elasticsearch and TF/IDF <a href=""https://www.elastic.co/guide/en/elasticsearch/guide/current/scoring-theory.html#tfidf"" rel=""nofollow"">https://www.elastic.co/guide/en/elasticsearch/guide/current/scoring-theory.html#tfidf</a></p>
",Parsing & POS Tagging,tool provide frequency sorted list interesting multi word term document set nearly half million text document document want generate list interesting word phrase e unigrams bigram trigram interesting mean phrase occur frequently document relatively infrequently entire corpus phrase junky phrase e g ideally phrase characteristic content document e phrase would helpful word cloud way incorporate part speech tagging help identify meaningful phrase would plus interested know tool approach consider aware elasticsearch maybe python elasticsearch py library elasticsearch ha sophisticated full search capability elasticsearch doe significant term aggregation feature elasticsearch rank using tf idf nltk scikit learn aside tool set tool use need consider approach imagine experimenting different tokenizers split text word removing stop word building indexing n gram may may want stem word experiment see document want get frequency sorted list n gram n gram document want score n gram based frequency document frequency corpus provide scored list n gram document guess python based toolkit approach would work well python ha rich set text mining tool power flexibility let experiment different part processing pipeline appreciate thought proceed link similar effort elasticsearch significant term aggregation tag cloud significant term aggregation elasticsearch tf idf
A syntactic annotation tool used to create treebanks,"<p>Can you recommend a good syntactic annotation tool? I want to create a small (phrase structure) treebank.</p>

<p>I know there are some tools used for dependency treebanks, <a href=""http://brat.nlplab.org/"" rel=""nofollow"">brat</a> being one of them.
<a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">Stanford Parser</a> comes with parse tree viewer, but you cannot create or edit parse trees, as far as I know.</p>

<p>Thank you in advance!</p>

<p>PS: I also tried <a href=""http://wordfreak.sourceforge.net/"" rel=""nofollow"">WorkFreak</a>, which looks fine, but it welcomes me with some runtime exceptions :)</p>
",Parsing & POS Tagging,syntactic annotation tool used create treebanks recommend good syntactic annotation tool want create small phrase structure treebank know tool used dependency treebanks brat one stanford parser come parse tree viewer create edit parse tree far know thank advance p also tried workfreak look fine welcome runtime exception
How to extract elements from NLP Tree?,"<p>I am using the <strong>NLP</strong> package to parse sentences.  How can I extract an element from the <code>Tree</code> output that is created?  For example I'd like to grab the Noun Phrases (<code>NP</code>) from the example below:</p>

<pre><code>library(NLP)
library(openNLP)

s &lt;- c(
    ""Really, I like chocolate because it is good."", 
    ""Robots are rather evil and most are devoid of decency""
)
s &lt;- as.String(s)


sent_token_annotator &lt;- Maxent_Sent_Token_Annotator()
word_token_annotator &lt;- Maxent_Word_Token_Annotator()
a2 &lt;- annotate(s, list(sent_token_annotator, word_token_annotator))

parse_annotator &lt;- Parse_Annotator()

p &lt;- parse_annotator(s, a2)
ptexts &lt;- sapply(p$features, `[[`, ""parse"")
ptexts

ptrees &lt;- lapply(ptexts, Tree_parse)

ptrees

## [[1]]
## (TOP
##   (S
##     (S
##       (S
##         (ADVP (RB Really))
##         (, ,)
##         (NP (PRP I))
##         (VP
##           (VBP like)
##           (NP (NN chocolate))
##           (SBAR (IN because) (S (NP (PRP it)) (VP (VBZ is) (ADJP (JJ good)))))))
##       (. .)
##       (, ,)
##       (NP (NNP Robots))
##       (VP (VBP are) (ADJP (RB rather) (JJ evil))))
##     (CC and)
##     (S (NP (RBS most)) (VP (VBP are) (ADJP (JJ devoid) (PP (IN of) (NP (NN decency))))))))
</code></pre>

<p>I'd like to grab pieces from the <code>Tree</code> but can't figure out from the documentation for <code>Tree_parse</code>.  Using <code>str</code> indicates it should be easy to do but I can't achieve it.  </p>

<p>I'd like it to return something like:</p>

<pre><code>[1] ""I""      ""Robots""
</code></pre>

<p>Or as a <code>list</code> rather  than a vector.</p>

<p>This likely requires having <strong>openNLPmodels.en</strong> installed from: <a href=""http://datacube.wu.ac.at/src/contrib/"" rel=""nofollow"">http://datacube.wu.ac.at/src/contrib/</a></p>

<p>Download and run </p>

<pre><code>install.packages(
    ""http://datacube.wu.ac.at/src/contrib/openNLPmodels.en_1.5-1.tar.gz"",  
    repos=NULL, 
    type=""source""
)
              `
</code></pre>

<p>If it's helpful folks can source the <code>Tree</code> directly in using the <strong>curl</strong> package from my Dropbox:</p>

<pre><code>library(curl)
ptrees &lt;- source(curl(""https://dl.dropboxusercontent.com/u/61803503/Errors/tree.R""))[[1]]
</code></pre>
",Parsing & POS Tagging,extract element nlp tree using nlp package parse sentence extract element output created example like grab noun phrase example like grab piece figure documentation using indicates easy achieve like return something like rather vector likely requires opennlpmodels en installed download run helpful folk source directly using curl package dropbox
How to use both the lexicalized and the dependency parser in the StanfordCoreNLP pipeline?,"<p>Suppose I have defined the following StanfordCoreNLP pipeline:</p>

<pre><code>Properties props = new Properties();

props.put(""language"", ""english"");
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, depparse"");
props.put(""depparse.model"", ""edu/stanford/nlp/models/parser/nndep/english_SD.gz"");
props.put(""parse.originalDependencies"", true);

StanfordCoreNLP pipeline =  new StanfordCoreNLP(props);
</code></pre>

<p>Now, this code will give me the tokens, sentence splitter, POS tags, lemmas, NER and depdencency parse (the NN model). Now, I also want to have a lexicalized parse tree as well. </p>

<p>How can I put this information in the pipeline? Or maybe I have to do it otherwise? What is the optimal way to do this?</p>
",Parsing & POS Tagging,use lexicalized dependency parser stanfordcorenlp pipeline suppose defined following stanfordcorenlp pipeline code give token sentence splitter po tag lemma ner depdencency parse nn model also want lexicalized parse tree well put information pipeline maybe otherwise optimal way
How to use a phrase chunker in the StanfordCoreNLP pipeline?,"<p>Suppose I have defined the following StanfordCoreNLP pipeline:</p>

<pre><code>Properties props = new Properties();

props.put(""language"", ""english"");
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, depparse"");
props.put(""depparse.model"", ""edu/stanford/nlp/models/parser/nndep/english_SD.gz"");
props.put(""parse.originalDependencies"", true);

StanfordCoreNLP pipeline =  new StanfordCoreNLP(props);
</code></pre>

<p>Now, I would like to use the Stanford chunker as well in the pipeline so I could discover NPs, VPs, ... (I guess with the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/pipeline/LabeledChunkIdentifier.html"" rel=""nofollow"">Chunk Identifier</a>). Also, I wouldn't want to do lexicalized parsing, just a dependency parse (as specified in the properties). Is this possible? If so, could you provide an example of how to do this? </p>
",Parsing & POS Tagging,use phrase chunker stanfordcorenlp pipeline suppose defined following stanfordcorenlp pipeline would like use stanford chunker well pipeline could discover np vps guess chunk identifier also want lexicalized parsing dependency parse specified property possible could provide example
issue in implementing stylometry features (POS tagging using stanford pos tagger),"<p>I have found frequency of POS tags e.g (Np, JJ..) for each token. How can i find POS tag bigrams and trigrams distributions ? (I am using stanford POS tagger java i.e <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tagger.shtml</a>) 
code for uni-gram is</p>

<pre><code>MaxentTagger tagger = new MaxentTagger(""taggers/english-left3words-distsim.tagger"");

// The sample string
String sample = ""This is a sample text"";

// The tagged string
String tagged = tagger.tagString(s);
final String tagged = s;
int nouns = 0;
int adjectives = 0;
int adverbs = 0;
int verbs = 0;
int cd = 0;  // Cardinal number
int preposition = 0;
int fw = 0;
int particle = 0;
int symbol = 0;
int conjuction = 0;
int Determiner = 0;
int interrogative = 0;
int prp$ = 0; //Possessive pronoun
final String[] tokens = tagged.split("" "");

for (final String token : tokens) {
    //System.out.println(token);
    final int lastUnderscoreIndex = token.lastIndexOf(""_"");
    final String realToken = token.substring(lastUnderscoreIndex + 1);
    if (""NN"".equals(realToken) || ""NNS"".equals(realToken) || ""NNP"".equals(realToken) || ""NNPS"".equals(realToken)) {
        nouns++;
    }
    if (""JJ"".equals(realToken) || ""JJR"".equals(realToken) || ""JJR"".equals(realToken)) {
        adjectives++;
    }
    if (""RB"".equals(realToken) || ""RBS"".equals(realToken) || ""RBR"".equals(realToken)) {
        adverbs++;
    }
    if (""VB"".equals(realToken) || ""VBD"".equals(realToken) || ""VBG"".equals(realToken)
            || ""VBN"".equals(realToken) || ""VBP"".equals(realToken) || ""VBZ"".equals(realToken)) {
        verbs++;
    }
    if (""CD"".equals(realToken)) {
        cd++;
    }
    if (""IN"".equals(realToken) || ""TO"".equals(realToken)) {
        preposition++;
    }
    if (""RP"".equals(realToken)) {
        particle++;
    }
    if (""SYM"".equals(realToken)) {
        symbol++;
    }
    if (""CC"".equals(realToken)) {
        conjuction++;
    }
    if (""DT"".equals(realToken)) {
        Determiner++;
    }
    if (""WDT"".equals(realToken) || ""WP"".equals(realToken) || ""WRB"".equals(realToken)) {

        interrogative++;
    }
    if (""FW"".equals(realToken)) {
        fw++;
    }
    if (""PRP$"".equals(realToken)) {
        prp$++;
    }
}
</code></pre>
",Parsing & POS Tagging,issue implementing stylometry feature po tagging using stanford po tagger found frequency po tag e g np jj token find po tag bigram trigram distribution using stanford po tagger java e code uni gram
Is there a way to get typed dependencies using OpenNlp?,"<p>I am trying to extract the subject, object and predicate triplets of a sentence. I am required to use the typed dependencies in order to find out the relationship between the triplets. I am using OpenNlp to do this. I know Stanford Nlp provides typed dependency parser but I would like to do it with OpenNlp</p>
",Parsing & POS Tagging,way get typed dependency using opennlp trying extract subject object predicate triplet sentence required use typed dependency order find relationship triplet using opennlp know stanford nlp provides typed dependency parser would like opennlp
Regex to extract Noun Phrases from a Part of Speech parse tree?,"<p>I am trying to extract all three word noun phrases from a Stanford POS Parse Tree. Basically, anything that looks like:</p>



<pre class=""lang-none prettyprint-override""><code>(NP (TAG WORD) (TAG WORD) (TAG WORD))
</code></pre>

<p>Or:</p>

<pre class=""lang-none prettyprint-override""><code>(NP (TAG WORD) (TAG (TAG WORD) (TAG WORD)))
</code></pre>

<p>This is what a parse tree can look like:</p>

<pre class=""lang-none prettyprint-override""><code>(ROOT (SQ (VBZ Is) (NP (DT this)) (NP (DT an) (NN asthma) (NN attack)) (. ?)))
</code></pre>

<p>When I do this regex, it extracts the correct 3 word noun phrase:</p>

<pre class=""lang-ruby prettyprint-override""><code>threeWordNounPhrases = full.scan(/\(NP \([^()]+ [^()]+\) \([^()]+ [^()]+\)\)/)
# =&gt; ""(NP (DT an) (NN asthma) (NN attack))""
</code></pre>

<p>However, this does not work for something like:</p>

<pre class=""lang-none prettyprint-override""><code>(ROOT (SQ (NNP Should) (NP (PRP I)) (VP (VB watch) (NP (NP (NNP Game)) (PP (IN of) (NP (NNP Thrones)))) ) (. ?)))
</code></pre>

<p>Which should return:</p>

<pre class=""lang-none prettyprint-override""><code>(NP (NP (NNP Game)) (PP (IN of) (NP (NNP Thrones))))
</code></pre>
",Parsing & POS Tagging,regex extract noun phrase part speech parse tree trying extract three word noun phrase stanford po parse tree basically anything look like parse tree look like regex extract correct word noun phrase however doe work something like return
Why is pos_tag in NLTK tagging &quot;please&quot; as NN?,"<p>I have a serious problem: I have downloaded last version of NLTK
and I got a strange POS output:</p>

<pre><code>import nltk
import re

sample_text=""start please with me""
tokenized = nltk.sent_tokenize(sample_text)  

for i in tokenized:
            words=nltk.word_tokenize(i)
            tagged=nltk.pos_tag(words)
            chunkGram=r""""""Chank___Start:{&lt;VB|VBZ&gt;*}  """"""                           
            chunkParser=nltk.RegexpParser(chunkGram)
            chunked=chunkParser.parse(tagged)
            print(chunked) 
</code></pre>

<p>[out]:</p>

<blockquote>
  <blockquote>
    <p>(S start/JJ please/NN with/IN me/PRP)</p>
  </blockquote>
</blockquote>

<p>I do not know why ""start"" is tagged as <code>JJ</code> and ""please"" as <code>NN</code>?</p>
",Parsing & POS Tagging,po tag nltk tagging please nn serious problem downloaded last version nltk got strange po output start jj please nn prp know start tagged please
How to get POS tagging using Stanford Parser,"<p>I'm using Stanford Parser to parse the dependence relations between pair of words, but I also need the tagging of words. However, in the ParseDemo.java, the program only output the Tagging Tree. I need each word's tagging like this:</p>

<pre><code>My/PRP$ dog/NN also/RB likes/VBZ eating/VBG bananas/NNS ./.
</code></pre>

<p>not like this:</p>

<pre><code>(ROOT
  (S
    (NP (PRP$ My) (NN dog))
    (ADVP (RB also))
    (VP (VBZ likes)
      (S
        (VP (VBG eating)
          (S
            (ADJP (NNS bananas))))))
    (. .)))
</code></pre>

<p>Who can help me? thanks a lot.</p>
",Parsing & POS Tagging,get po tagging using stanford parser using stanford parser parse dependence relation pair word also need tagging word however parsedemo java program output tagging tree need word tagging like like help thanks lot
NLP finds a program that uses conll format,"<p>So I am doing a project here I need to process the information of a text. I used opennlp and experimented with freeling and both got me good results (lemmas, divide by sentences, divide by phrases and POS). But then I trained maltparser with this CONLL (<a href=""http://www.linguateca.pt/floresta/CoNLL-X/"" rel=""nofollow"">http://www.linguateca.pt/floresta/CoNLL-X/</a>) file, and the POS tags that malparser uses is different from opennlp and freeling. I know that a way of doing this is converting the POS tag form opennlp (or freeling) to be accepted by the malparser. What I wanted to know is if is there any program that uses the CONLL format to train his algorithm, so that the POS and lemmas that I got are known by the malparser. If possible a program that work on Java and windows.</p>

<p>Ty</p>
",Parsing & POS Tagging,nlp find program us conll format project need process information text used opennlp experimented freeling got good result lemma divide sentence divide phrase po trained maltparser conll file po tag malparser us different opennlp freeling know way converting po tag form opennlp freeling accepted malparser wanted know program us conll format train algorithm po lemma got known malparser possible program work java window ty
Python: map NLTK Stanford POS tags to WordNet POS tags,"<p>I'm reading a list of sentences and tagging each word with NLTK's Stanford POS tagger.  I get outputs like so:</p>

<pre><code>wordnet_sense = []

for o in output:
    a = st.tag(o)
    wordnet_sense.append(a)
</code></pre>

<p>outputs: <code>[[(u'feel', u'VB'), (u'great', u'JJ')], [(u'good', u'JJ')]]</code></p>

<p>I want to map these words with their POS, so that they are recognised in WordNet.</p>

<p>I've attempted this:</p>

<pre><code>sense = []

for i in wordnet_sense:
    tmp = []

    for tok, pos in i:
        lower_pos = pos[0].lower()

        if lower_pos in ['a', 'n', 'v', 'r', 's']:
            res = wn.synsets(tok, lower_pos)
            if len(res) &gt; 0:
                a = res[0]
        else:
            a = ""[{0}, {1}]"".format(tok, pos)

        tmp.append(a)

    sense.append(tmp)

print sense
</code></pre>

<p>outputs: <code>[Synset('feel.v.01'), '[great, JJ]'], ['[good, JJ]']]</code></p>

<p>So <code>feel</code> is recognised as a verb, but <code>great</code> and <code>good</code> are not recognised as adjectives.  I've also checked if <code>great</code> and <code>good</code> actually belong in Wordnet because I thought they weren't being mapped if they weren't there, but they are.  Can anyone help?</p>
",Parsing & POS Tagging,python map nltk stanford po tag wordnet po tag reading list sentence tagging word nltk stanford po tagger get output like output want map word po recognised wordnet attempted output recognised verb recognised adjective also checked actually belong wordnet thought mapped anyone help
How to use stanford dependency parser to extract aspect terms from text?,"<p>I'm currently working on an aspect-level sentiment analysis project using online travel reviews.</p>

<p>I'm using <code>Stanford CoreNLP</code> to get things done. So far, I have managed to pre-process the data by POS tagging and lemmatizing the review content.</p>

<p>I read several papers related to sentiment analysis and it looks like the next step is to extract aspect terms from the review text, along with their sentiment polarity.I have seen a video tutorial in Python NLTK where regex were used to find relationships between POS tagged words to find Noun phrases etc. I want to do the same using Stanford Dependency parser.</p>

<p>Unfortunately, I do not understand how to use the output of Stanford Dependency Parser to write such rules to identify aspect terms.</p>

<p>For two days, I have looked for a sample Java code that would explain how exactly I could accomplish this task. But so far, no luck. </p>

<p>Would really appreciate if someone could point me to a tutorial/sample code where I could take a look and understand the procedure. </p>

<p>Say I have an output similar to following;</p>

<pre><code>(ROOT
  (S
    (NP (PRP It))
    (VP (VBZ is) (RB not)
      (NP
        (NP (DT a) (NN museum))
        (PP (CC but)
          (NP
            (NP (DT a) (VBG living) (JJ historic) (NN town))
            (PP (IN with)
              (NP (JJ wonderful) (NNS places)))
            (S
              (VP (TO to)
                (VP
                  (VP (VB eat)
                    (NP (NN drink)))
                  (CC and)
                  (VP (VB do)
                    (NP (NN shopping))))))))))
</code></pre>

<p>How can I extract <strong>museum</strong>, <strong>eat</strong>, <strong>drink</strong>, <strong>shopping</strong> as aspects? </p>

<p>Any help is greatly appreciated.</p>
",Parsing & POS Tagging,use stanford dependency parser extract aspect term text currently working aspect level sentiment analysis project using online travel review using get thing done far managed pre process data po tagging lemmatizing review content read several paper related sentiment analysis look like next step extract aspect term review text along sentiment polarity seen video tutorial python nltk regex used find relationship po tagged word find noun phrase etc want using stanford dependency parser unfortunately understand use output stanford dependency parser write rule identify aspect term two day looked sample java code would explain exactly could accomplish task far luck would really appreciate someone could point tutorial sample code could take look understand procedure say output similar following extract museum eat drink shopping aspect help greatly appreciated
Coreference resolution using Stanford CoreNLP,"<p>I am new to the Stanford CoreNLP toolkit and trying to use it for a project to resolve coreferences in news texts. In order to use the Stanford CoreNLP coreference system, we would usually create a pipeline, which requires tokenization, sentence splitting, part-of-speech tagging, lemmarization, named entity recoginition and parsing. For example:</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

// read some text in the text variable
String text = ""As competition heats up in Spain's crowded bank market, Banco Exterior de Espana is seeking to shed its image of a state-owned bank and move into new activities."";

// create an empty Annotation just with the given text
Annotation document = new Annotation(text);

// run all Annotators on this text
pipeline.annotate(document);
</code></pre>

<p>Then we can easily get the sentence annotations with:</p>

<pre><code>List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
</code></pre>

<p>However, I am using other tools for for preprocessing and just need a stand-alone coreference resolution system. It is pretty easy to create tokens and parse tree annotations and set them to the annotation:</p>

<pre><code>// create new annotation
Annotation annotation = new Annotation();

// create token annotations for each sentence from the input file
List&lt;CoreLabel&gt; tokens = new ArrayList&lt;&gt;();
for(int tokenCount = 0; tokenCount &lt; parsedSentence.size(); tokenCount++) {

        ArrayList&lt;String&gt; parsedLine = parsedSentence.get(tokenCount);
        String word = parsedLine.get(1);
        String lemma = parsedLine.get(2);
        String posTag = parsedLine.get(3);
        String namedEntity = parsedLine.get(4); 
        String partOfParseTree = parsedLine.get(6);

        CoreLabel token = new CoreLabel();
        token.setWord(word);
        token.setWord(lemma);
        token.setTag(posTag);
        token.setNER(namedEntity);
        tokens.add(token);
    }

// set tokens annotations to annotation
annotation.set(TokensAnnotation.class, tokens);

// set parse tree annotations to annotation
Tree stanfordParseTree = Tree.valueOf(inputParseTree);
annotation.set(TreeAnnotation.class, stanfordParseTree);
</code></pre>

<p>However, creating sentence annotations is pretty tricky, because to my knowledge there is no document to explain it in full detail. I am able to create the data structure for the sentence annotations and set it to the annotation:</p>

<pre><code>List&lt;CoreMap&gt; sentences = new ArrayList&lt;CoreMap&gt;();
annotation.set(SentencesAnnotation.class, sentences);
</code></pre>

<p>I am sure it cannot be that difficult, but there is no documentation on how to create sentence annotation from tokens annotations, i.e. how to fill the ArrayList with actual sentence annotations.</p>

<p>Any ideas?</p>

<p>Btw, if I use the tokens and parse tree annotations provided by my processing tools and only use the sentence annotations provided by the StanfordCoreNLP pipeline and apply the StanfordCoreNLP stand-alone coreference resolution system I am getting the correct results. So the only part missing for a complete stand-alone coreference resolution system is the ability to create the sentence annotations from the tokens annotations.</p>
",Parsing & POS Tagging,coreference resolution using stanford corenlp new stanford corenlp toolkit trying use project resolve coreference news text order use stanford corenlp coreference system would usually create pipeline requires tokenization sentence splitting part speech tagging lemmarization named entity recoginition parsing example easily get sentence annotation however using tool preprocessing need stand alone coreference resolution system pretty easy create token parse tree annotation set annotation however creating sentence annotation pretty tricky knowledge document explain full detail able create data structure sentence annotation set annotation sure difficult documentation create sentence annotation token annotation e fill arraylist actual sentence annotation idea btw use token parse tree annotation provided processing tool use sentence annotation provided stanfordcorenlp pipeline apply stanfordcorenlp stand alone coreference resolution system getting correct result part missing complete stand alone coreference resolution system ability create sentence annotation token annotation
Persian Dependency Parser for Stanford Dependency Parser from a Persian dependency treebank.,"<p>I have downloaded the <a href=""http://stp.lingfil.uu.se/~mojgan/UPDT.html"" rel=""nofollow"">UPDT</a> Persian treebank (<a href=""http://stp.lingfil.uu.se/~mojgan/UPDT.html"" rel=""nofollow"">Uppsala Persian Dependency Treebank</a>) and I am trying to build a dependency parser model using <a href=""http://www.maltparser.org/mco/mco.html"" rel=""nofollow"">MaltParser</a>, but I am very new to <a href=""https://en.wikipedia.org/wiki/Natural_language_processing"" rel=""nofollow"">NLP</a> field and need some help.</p>

<p>Does anyone know how to use <a href=""http://www.maltparser.org/mco/mco.html"" rel=""nofollow"">MaltParser</a> to build the dependency parser model? and can the new created model be used with the <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">Stanford Dependency Parser</a> in my Java program?</p>

<p>Thank you</p>
",Parsing & POS Tagging,persian dependency parser stanford dependency parser persian dependency treebank downloaded updt persian treebank uppsala persian dependency treebank trying build dependency parser model using maltparser new nlp field need help doe anyone know use maltparser build dependency parser model new created model used stanford dependency parser java program thank
Saving python NLTK parse tree to image file,"<p>This might replicate this stackoverflow <a href=""https://stackoverflow.com/questions/23429117/saving-nltk-drawn-parse-tree-to-image-file"">question</a> . However, i'm facing a different problem. This is my working code.</p>

<pre><code>import nltk 
from textblob import TextBlob
with open('test.txt', 'rU') as ins:
    array = []
    for line in ins:
        array.append(line)
for i in array:
    wiki = TextBlob(i)
    a=wiki.tags
    sentence = a
    pattern = """"""NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}
    VBD: {&lt;VBD&gt;}
    IN: {&lt;IN&gt;}""""""
    NPChunker = nltk.RegexpParser(pattern)
    result = NPChunker.parse(sentence)

    result.draw()
</code></pre>

<p><a href=""https://i.sstatic.net/FaCKL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FaCKL.png"" alt=""enter image description here""></a>
This produce parse trees one by one for all the sentences. Actually in my ""test.txt"" i have more than 100 sentences. therefore, it's really hard to save each file into .ps files manually. How could i modify my code to save this trees to single .ps or .png files with a label (something like: 1.png,2.png ...).That mean i need to get more than one image files.
thanks in advance.</p>
",Parsing & POS Tagging,saving python nltk parse tree image file might replicate stackoverflow produce parse tree one one sentence actually test txt sentence therefore really hard save file p file manually could modify code save tree single p png file label something like png png mean need get one image file thanks advance
"How to get morphological variation, given a lemma (java preferred)","<p>I'm currently using Stanford Core NLP to get word lemmas from text. Is there any way within the Stanford suite to run this ""in reverse""? For example, suppose I have the lemma ""come"" and I want to get the gerund form ('VBG' POS tag) ""coming"".</p>

<p>If this isn't available in the Stanford suite, can anyone recommend a different Java tool that can accomplish this?</p>
",Parsing & POS Tagging,get morphological variation given lemma java preferred currently using stanford core nlp get word lemma text way within stanford suite run reverse example suppose lemma come want get gerund form vbg po tag coming available stanford suite anyone recommend different java tool accomplish
Numerical POS tag training in nltk (python),"<p>To create a natural language calculator, I tried TrigramTagger from nltk. I want to tag multiplication and 2 numbers in given sentences. 
For example: ""What is product of 5 and 7"", here 'product' is 'binary.multiply', '5' is 'num-1', and '7' is 'num-2'. Once I can tag these 3, I can easily calculate answer.</p>

<p>But as you can see in the output below, I am not able to train the tagger about the 2 numbers. The number has to be exactly same as it was trained, otherwise it defaults to ""CD"".  Using regex or otherwise, how do I train tagger about the num-1 and num-2?</p>

<pre><code>import nltk.tag, nltk.data
from nltk import word_tokenize
default_tagger = nltk.data.load(nltk.tag._POS_TAGGER)

def evaluate(tagger, sentences):
    good,total = 0,0.
    for sentence in sentences:
        tags = tagger.tag(nltk.word_tokenize(sentence))
        print tags

train_sents = [
    [('product', 'binary.multiply'), ('of', 'IN'), ('5', 'num-1'), ('and', 'CC'), ('7', 'num-2'), ('?', '.')],
    [ ('what', 'WP'), ('is', 'VBZ'),  ('product', 'binary.multiply'), ('of', 'IN'), ('5', 'num-1'), ('and', 'CC'), ('7', 'num-2'), ('?', '.')],
    [('what', 'WP'), ('happens', 'NNS'), ('when', 'WRB'), ('I', 'PRP'), ('multiply', 'binary.multiply'), ('5', 'num-1'), ('with', 'IN'), ('7', 'num-2'), ('?', '.')],
    [('5', 'num-1'), ('*', 'binary.multiply'), ('3.2','CD')],
    [('is', 'NNP'), ('it', 'PRP'), ('possible', 'JJ'), ('to', 'TO'), ('multiply', 'binary.multiply'), ('5', 'num-1'), ('with', 'IN'), ('7', 'num-2'), ('?', '.')],
    [('what', 'WP'), ('is', 'VBZ'), ('5', 'num-1'), ('times', 'binary.multiply'), ('7', 'num-2'), ('?', '.')]
]

sentences = [
    ('product of 5 and 7?'),
    ('what is product of 3 and 2.7?'),
    ('what happens when I multiply 0.1 with 5.21?'),
    ('9.1 * 3.2'),
    ('is it possible to multiply 5 with 7?'),
    ('what is 5 times 7?')
]

tagger = nltk.TrigramTagger(train_sents, backoff=default_tagger)
evaluate(tagger, sentences)
#model = tagger._context_to_tag
</code></pre>

<p>The output of this program does not recognize different numbers as num-1 and num-2, how to make it recognize it?</p>

<pre><code>[('product', 'binary.multiply'), ('of', 'IN'), ('4', 'CD'), ('and', 'CC'), ('2', 'CD'), ('?', '.')]
[('what', 'WP'), ('is', 'VBZ'), ('product', 'binary.multiply'), ('of', 'IN'), ('3', 'CD'), ('and', 'CC'), ('2.7', 'CD'), ('?', '.')]
[('what', 'WP'), ('happens', 'NNS'), ('when', 'WRB'), ('I', 'PRP'), ('multiply', 'binary.multiply'), ('0.1', 'CD'), ('with', 'IN'), ('5.21', 'CD'), ('?', '.')]
[('9.1', 'CD'), ('*', '-NONE-'), ('3.2', 'CD')]
[('is', 'NNP'), ('it', 'PRP'), ('possible', 'JJ'), ('to', 'TO'), ('multiply', 'binary.multiply'), ('2', 'CD'), ('with', 'IN'), ('77', 'CD'), ('?', '.')]
[('what', 'WP'), ('is', 'VBZ'), ('15', 'CD'), ('times', 'NNS'), ('72', 'CD'), ('?', '.')]
</code></pre>
",Parsing & POS Tagging,numerical po tag training nltk python create natural language calculator tried trigramtagger nltk want tag multiplication number given sentence example product product binary multiply num num tag easily calculate answer see output able train tagger number number ha exactly wa trained otherwise default cd using regex otherwise train tagger num num output program doe recognize different number num num make recognize
Lingua::TreeTagger tagging only first word in Parts Of Speech tagging,"<p>I'm using <a href=""http://search.cpan.org/dist/Lingua-TreeTagger/lib/Lingua/TreeTagger.pm"" rel=""nofollow"">Lingua::TreeTagger</a> for POS tagging, but its tagging just first word of string.</p>

<pre><code>my $tagger = Lingua::TreeTagger-&gt;new(
    'language' =&gt; 'english',
    'options'  =&gt; [ qw( -token -lemma -no-unknown ) ],
);
$text_to_tag = 'I another yet sample text I.';
my $tagged_text = $tagger-&gt;tag_text( \$text_to_tag );
print Dumper $tagged_text;
</code></pre>

<p>Output of the above dumper is as below:</p>

<pre><code>'sequence' =&gt; [
                                 bless( {
                                          'is_SGML_tag' =&gt; 0,
                                          'original' =&gt; 'I',
                                          'tag' =&gt; 'PP',
                                          'lemma' =&gt; 'I'
                                        }, 'Lingua::TreeTagger::Token' )
                               ],
</code></pre>

<p>Please note that only <code>I</code> is tagged here, but I want to tag whole sentence. In my actual code, I want to tag contents of a file. <strong>How can I tag all words of the sentence?</strong> Any help is appreciated.</p>
",Parsing & POS Tagging,lingua treetagger tagging first word part speech tagging using lingua treetagger po tagging tagging first word string output dumper please note tagged want tag whole sentence actual code want tag content file tag word sentence help appreciated
match POS tag and sequence of words,"<p>I have the following two strings with their POS tags: </p>

<p><strong>Sent1</strong>: ""<em>something like how writer pro or phraseology works would be really cool.</em>""</p>

<blockquote>
  <p>[('something', 'NN'), ('like', 'IN'), ('how', 'WRB'), ('writer',
  'NN'), ('pro', 'NN'), ('or', 'CC'), ('phraseology', 'NN'), ('works',
  'NNS'), ('would', 'MD'), ('be', 'VB'), ('really', 'RB'), ('cool',
  'JJ'), ('.', '.')]</p>
</blockquote>

<p><strong>Sent2</strong>: ""<em>more options like the syntax editor would be nice</em>"" </p>

<blockquote>
  <p>[('more', 'JJR'), ('options', 'NNS'), ('like', 'IN'), ('the', 'DT'),
  ('syntax', 'NN'), ('editor', 'NN'), ('would', 'MD'), ('be', 'VB'),
  ('nice', 'JJ')]</p>
</blockquote>

<p>I am looking for a way to detect (return True) if there is the sequence: ""would"" + be"" + adjective (regardless of the position of the adjective, as long as its after ""would"" ""be"") in these strings. In the second string the adjective, ""nice"" immediately follows ""would be"" but that is not the case in the first string.</p>

<p>The trivial case (no other word before the adjective; <em>""would be nice"")</em> was solved in an earlier question of mine: <a href=""https://stackoverflow.com/questions/34672986/detecting-pos-tag-pattern-along-with-specified-words"">detecting POS tag pattern along with specified words</a></p>

<p>I am now looking for a more general solution where optional words may occur before the adjective.  I am new to NLTK and Python.</p>
",Parsing & POS Tagging,match po tag sequence word following two string po tag sent something like writer pro phraseology work would really cool something nn like writer nn pro nn cc phraseology nn work nns would md vb really rb cool jj sent option like syntax editor would nice jjr option nns like dt syntax nn editor nn would md vb nice jj looking way detect return true sequence would adjective regardless position adjective long would string second string adjective nice immediately follows would case first string trivial case word adjective would nice wa solved earlier question mine href po tag pattern along specified word looking general solution optional word may occur adjective new nltk python
detecting POS tag pattern along with specified words,"<p>I need to identify certain POS tags before/after certain specified words, for example the following tagged sentence: </p>

<pre><code>[('This', 'DT'), ('feature', 'NN'), ('would', 'MD'), ('be', 'VB'), ('nice', 'JJ'), ('to', 'TO'), ('have', 'VB')]
</code></pre>

<p>can be abstracted to the form ""would be"" + Adjective</p>

<p>Similarly: </p>

<pre><code>[('I', 'PRP'), ('am', 'VBP'), ('able', 'JJ'), ('to', 'TO'), ('delete', 'VB'), ('the', 'DT'), ('group', 'NN'), ('functionality', 'NN')]
</code></pre>

<p>is of the form ""am able to"" + Verb </p>

<p>How can I go about checking for these type of a pattern in sentences. I am using NLTK. </p>
",Parsing & POS Tagging,detecting po tag pattern along specified word need identify certain po tag certain specified word example following tagged sentence abstracted form would adjective similarly form able verb go checking type pattern sentence using nltk
Numeral in POS tag &quot;whnp-1&quot; mean?,"<p>I am trying to understand an already annotated the sentence <em>When this happens, the two KIMs show a magnetism that causes the first KIM to move toward the second KIM.</em></p>

<p>What does number <em>1</em> in POS tag WHADVP-1 for <em>When</em> mean/signify?</p>

<p>Similarly what does number <em>1</em> in POS tag WHNP-1 for <em>that</em>  mean/signify?</p>

<p>I think I understand well POS tags, after reading <a href=""http://web.mit.edu/6.863/www/PennTreebankTags.html"" rel=""nofollow"">http://web.mit.edu/6.863/www/PennTreebankTags.html</a> and
notes by Andrew McIntyre.</p>
",Parsing & POS Tagging,numeral po tag whnp mean trying understand already annotated sentence happens two kims show magnetism cause first kim move toward second kim doe number po tag whadvp mean signify similarly doe number po tag whnp mean signify think understand well po tag reading note andrew mcintyre
How do I check whether a given string is a valid geographical location or not?,"<p>I have a list of strings (noun phrases) and I want to filter out all valid geographical locations from them. Most of these (unwanted location names) are country or city or state names. What would be a way to do this? Is there any open-source lookup table available which contains all country, states, cities of the world?</p>

<p>Example desired output:
<em>TREC4</em>: false,   <em>Vienna</em>: <strong>true</strong>,   <em>Ministry</em>: false,   <em>IBM</em>: false,   <em>Montreal</em>: <strong>true</strong>,   <em>Singapore</em>: <strong>true</strong></p>

<p>Unlike this post: <a href=""https://stackoverflow.com/questions/12042452/verify-user-input-location-string-is-a-valid-geographic-location"">Verify user input location string is a valid geographic location?</a>
I have a high number of strings like these (~0.7 million) so <em>google geolocation API</em> is probably not an option for me.</p>
",Parsing & POS Tagging,check whether given string valid geographical location list string noun phrase want filter valid geographical location unwanted location name country city state name would way open source lookup table available contains country state city world example desired output trec false vienna true false ibm false montreal true singapore true unlike post href user input location string valid geographic location high number string like million google geolocation api probably option
Converting stanfordNLP parse tree into DOT format,"<p>Is there a way/method to convert the parse tree generated by stanford parser into DOT format. I'm aware of the method toDotFormat() that converts stanford dependency output to DOT. But I want to convert edu.stanford.nlp.trees.Tree to DOT.
Thanks </p>
",Parsing & POS Tagging,converting stanfordnlp parse tree dot format way method convert parse tree generated stanford parser dot format aware method todotformat convert stanford dependency output dot want convert edu stanford nlp tree tree dot thanks
dependency tree sentiment stanford nlp,"<p>I have a dependency Tree as String. however i convert to Tree and do sentiment analysis. It gives me only -1 (not working).
I able to print the tree successfully using parse.pennPrint(), and it looks fine to me .</p>

<pre><code>    String sentence=""(TOP (S (S (NP (NNP china)) (VP (VBD experimented) (PP (IN in) (NP (DT the) (NN past))) (PP (IN with) (NP (NP (JJ various) (JJ political) (NNS systems,)) (VP (VBG including) (NP (JJ multi-party) (NN democracy,))))))) (CC but) (S (NP (PRP it)) (VP (VBD did) (RB not) (VP (VB work,) (S (NP (NN president) (NN xi) (NN jinping)) (VP (VBD said) (PP (IN during) (NP (NP (DT a) (NN visit)) (S (VP (TO to) (VP (VB europe,) (VP (VBG warning) (SBAR (IN that) (S (S (VP (VBG copying) (NP (JJ foreign) (JJ political) (CC or) (NN development) (NNS models)))) (VP (MD could) (VP (VB be) (ADJP (JJ catastrophic)))))))))))))))))))"";
    int sentiment_score =0;
    try{

        Tree parse =  Tree.valueOf(sentence);
        parse.pennPrint();


        sentiment_score = RNNCoreAnnotations.getPredictedClass(parse);
        System.out.println(""input tree, score: ""+sentiment_score);  
    }
    catch(Exception e){
        e.printStackTrace();
    }
</code></pre>

<p>Printed Tree using parse.pennPrint():</p>

<p>(TOP
  (S
    (S
      (NP (NNP china))
      (VP (VBD experimented)
        (PP (IN in)
          (NP (DT the) (NN past)))
        (PP (IN with)
          (NP
            (NP (JJ various) (JJ political) (NNS systems,))
            (VP (VBG including)
              (NP (JJ multi-party) (NN democracy,)))))))
    (CC but)
    (S
      (NP (PRP it))
      (VP (VBD did) (RB not)
        (VP (VB work,)
          (S
            (NP (NN president) (NN xi) (NN jinping))
            (VP (VBD said)
              (PP (IN during)
                (NP
                  (NP (DT a) (NN visit))
                  (S
                    (VP (TO to)
                      (VP (VB europe,)
                        (VP (VBG warning)
                          (SBAR (IN that)
                            (S
                              (S
                                (VP (VBG copying)
                                  (NP (JJ foreign) (JJ political)
                                    (CC or)
                                    (NN development) (NNS models))))
                              (VP (MD could)
                                (VP (VB be)
                                  (ADJP (JJ catastrophic)))))))))))))))))))</p>
",Parsing & POS Tagging,dependency tree sentiment stanford nlp dependency tree string however convert tree sentiment analysis give working able print tree successfully using parse pennprint look fine printed tree using parse pennprint top np nnp china vp vbd experimented pp np dt nn past pp np np jj various jj political nns system vp vbg including np jj multi party nn democracy cc np prp vp vbd rb vp vb work np nn president nn xi nn jinping vp vbd said pp np np dt nn visit vp vp vb europe vp vbg warning sbar vp vbg copying np jj foreign jj political cc nn development nns model vp md could vp vb adjp jj catastrophic
Where to find domain-specific corpus for a text mining task?,"<p>I am working on a text mining project which focus on the computer technology documents. So there're many jargons. Tasks like part-of-speech tagging require some training data to built a pos-tagger. And I think this training data should be from the same domain with words like "".NET, COM, JAVA"" correctly tagged.</p>

<p>So where can I find such corpus? Or is there any work around? Or can we tune an existing tagger to handle domain specific task?</p>
",Parsing & POS Tagging,find domain specific corpus text mining task working text mining project focus computer technology document many jargon task like part speech tagging require training data built po tagger think training data domain word like net com java correctly tagged find corpus work around tune existing tagger handle domain specific task
Python: How to count pos tags from from a sentence?,"<p>I have code from this <a href=""https://stackoverflow.com/questions/10674832/count-verbs-nouns-and-other-parts-of-speech-with-pythons-nltk"">link</a>. It returns the the POS tags along with their occurrence number.
How would I achieve a code where instead of entering a tag I enter a sentence and it returns the words from it and the different pos tags per word based on the corpus (in this case the Brown corpus). </p>

<pre><code>def findtags(tag_prefix, tagged_text):
    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text
                                  if tag.startswith(tag_prefix))
    return dict((tag, cfd[tag].keys()[:5]) for tag in cfd.conditions())

tagdictNNS = findtags('NNS', nltk.corpus.brown.tagged_words())

for tag in sorted(tagdictNNS):
    print tag, tagdictNNS[tag]

for k,v in tagdictNNS.items():
        new[k] = len(tagdictNNS[k])

print new
</code></pre>
",Parsing & POS Tagging,python count po tag sentence code href return po tag along occurrence number would achieve code instead entering tag enter sentence return word different po tag per word based corpus case brown corpus p
How to generate Universal Dependency Relations using Stanford NLP tools using Java?,"<p>We have created the parse tree from the command line using this command :</p>

<pre><code>java -mx1g -cp ""*"" edu.stanford.nlp.parser.lexparser.LexicalizedParser -sentences newline -tokenized -tagSeparator / -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerMethod newCoreLabelTokenizerFactory edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz data/for.txt
</code></pre>

<p>and upto this we got the correct output of the input file as tree format.</p>

<p>But we want the output like : <a href=""http://nlp.stanford.edu:8080/parser/"" rel=""nofollow"">http://nlp.stanford.edu:8080/parser/</a></p>

<p><strong>Universal dependencies</strong>
nmod:poss(dog-2, My-1)
nsubj(likes-4, dog-2)
advmod(likes-4, also-3)
root(ROOT-0, likes-4)
xcomp(likes-4, eating-5)
dobj(eating-5, sausage-6)
<strong>Universal dependencies, enhanced</strong>
nmod:poss(dog-2, My-1)
nsubj(likes-4, dog-2)
advmod(likes-4, also-3)
root(ROOT-0, likes-4)
xcomp(likes-4, eating-5)
dobj(eating-5, sausage-6)</p>

<p>Kindly share the JAVA code to produce this output.</p>
",Parsing & POS Tagging,generate universal dependency relation using stanford nlp tool using java created parse tree command line using command upto got correct output input file tree format want output like universal dependency nmod po dog nsubj like dog advmod like also root root like xcomp like eating dobj eating sausage universal dependency enhanced nmod po dog nsubj like dog advmod like also root root like xcomp like eating dobj eating sausage kindly share java code produce output
Stanford Dependency Parser and NLTK - Setup,"<p>I am trying to get Stanford Dependency Parser setup. However I am getting an Import Error - <strong><em>'ImportError: cannot import name StanfordDependencyParser</em></strong>'. I tried following instructions from answers to previous posts such as -  <a href=""https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk"">Stanford Parser and NLTK</a>  and <a href=""https://stackoverflow.com/questions/34053021/stanford-dependency-parser-setup-and-nltk"">Stanford Dependency Parser Setup and NLTK</a>. Nothing seems to be working.</p>

<p>My code is as follows: </p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; import os
&gt;&gt;&gt; os.environ['STANFORD_PARSER'] = 'C:\jars'
&gt;&gt;&gt; os.environ['STANFORD_MODELS'] = 'C:\jars'
&gt;&gt;&gt; os.environ['JAVAHOME'] = 'C:\Program Files (x86)\Java\jre1.8.0_66\bin'
&gt;&gt;&gt; from nltk.parse.stanford import StanfordDependencyParser
Traceback (most recent call last):
 File ""&lt;pyshell#23&gt;"", line 1, in &lt;module&gt;
   from nltk.parse.stanford import StanfordDependencyParser
ImportError: cannot import name StanfordDependencyParser
</code></pre>

<p>I am not sure what the issue is. I have updated NLTK to the latest version. I have also tried removing the 'import nltk' in case the error was because I was importing an entity twice. </p>

<p>I'm pretty lost as to how to proceed. I am very new to NLTK and python. Any help would be appreciated. Thank you in advance. </p>
",Parsing & POS Tagging,stanford dependency parser nltk setup trying get stanford dependency parser setup however getting import error importerror import name stanforddependencyparser tried following instruction answer previous post nothing seems working code follows sure issue updated nltk latest version also tried removing import nltk case error wa wa importing entity twice pretty lost proceed new nltk python help would appreciated thank advance
Stanford Dependency Parser Setup and NLTK,"<p>So I got the ""standard"" Stanford Parser to work thanks to danger89's answers to this previous post, <a href=""https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk"">Stanford Parser and NLTK</a>.</p>

<p>However, I am now trying to get the dependency parser to work and it seems the method highlighted in the previous link no longer works. Here is my code:</p>

<pre><code>import nltk
import os
java_path = ""C:\\Program Files\\Java\\jre1.8.0_51\\bin\\java.exe"" 
os.environ['JAVAHOME'] = java_path


from nltk.parse import stanford
os.environ['STANFORD_PARSER'] = 'path/jar'
os.environ['STANFORD_MODELS'] = 'path/jar'
parser = stanford.StanfordDependencyParser(model_path=""path/jar/englishPCFG.ser.gz"")

sentences = parser.raw_parse_sents(nltk.sent_tokenize(""The iPod is expensive but pretty.""))
</code></pre>

<p>I get the following error: <strong><em>'module' object has no attribute 'StanfordDependencyParser'</em></strong></p>

<p>The only thing I changed was ""StanfordDependencyParser"" from ""StanfordParser"". Any ideas how I can get this to work?</p>

<p>I also tried the Stanford Neural Dependency parser by importing it as shown in the documentation here: <a href=""http://www.nltk.org/_modules/nltk/parse/stanford.html"" rel=""nofollow noreferrer"">http://www.nltk.org/_modules/nltk/parse/stanford.html</a></p>

<p>This one didn't work either.</p>

<p>Pretty new to NLTK. Thanks in advance for any helpful input.</p>
",Parsing & POS Tagging,stanford dependency parser setup nltk got standard stanford parser work thanks danger answer previous post also tried stanford neural dependency parser importing shown documentation one work either pretty new nltk thanks advance helpful input
Transformation-Based Part-of-Speech Tagging(Brill Tagging),"<p>What are the weaknesses and strengths of the Brill Tagger? Can you suggest some possible improvements for the tagger?</p>
",Parsing & POS Tagging,transformation based part speech tagging brill tagging weakness strength brill tagger suggest possible improvement tagger
Semantic head finder with priority given to noun/noun phrases,"<p>Does anyone know of an implementation of a semantic head finder which gives priority to N*/NP's instead of V*/VP's?</p>

<p>I've been able to use the Stanford Tregex package to extract heads of NPs with the pattern <code>__ &gt;&gt;# NP</code> but doing the same for <code>ROOT</code> will give a verb as the head phrase.</p>
",Parsing & POS Tagging,semantic head finder priority given noun noun phrase doe anyone know implementation semantic head finder give priority n np instead v vp able use stanford tregex package extract head np pattern give verb head phrase
Part of speech tagging : tagging unknown words,"<p>In the part of speech tagger, the best probable tags for the given sentence is determined using HMM by</p>

<pre><code>    P(T*) = argmax P(Word/Tag)*P(Tag/TagPrev)
              T
</code></pre>

<p>But when 'Word' did not appear in the training corpus, P(Word/Tag) produces ZERO for given all possible tags, this leaves no room for choosing the best. </p>

<p>I have tried few ways, </p>

<p>1) Assigning small amount of probability for all unknown words, P(UnknownWord/AnyTag)~Epsilon... means this completely ignores the P(Word/Tag) for unknowns word by assigning the constant probability.. So decision making on unknown word is by prior probabilities.. As expected it is not producing good result. </p>

<p>2) Laplace Smoothing 
I confused with this. I don't know what is difference between (1) and this.   My way of understanding Laplace Smoothing adds the constant probability(lambda) to all unknown &amp; Known words.. So the All Unknown words will get constant probability(fraction of lambda) and Known words probabilities will be the same relatively since all word's prob increased by Lambda.
Is the Laplace Smoothing same as the previous one ?</p>

<p>*)Is there any better way of dealing with unknown words ?</p>
",Parsing & POS Tagging,part speech tagging tagging unknown word part speech tagger best probable tag given sentence determined using hmm word appear training corpus p word tag produce zero given possible tag leaf room choosing best tried way assigning small amount probability unknown word p unknownword anytag epsilon mean completely ignores p word tag unknown word assigning constant probability decision making unknown word prior probability expected producing good result laplace smoothing confused know difference way understanding laplace smoothing add constant probability lambda unknown known word unknown word get constant probability fraction lambda known word probability relatively since word prob increased lambda laplace smoothing previous one better way dealing unknown word
Part of speech for unknown and known words,"<p>what are the different between part of speech tagging for unknown words and part of speech tagging for known words. Is there any tool that can predict part of speech tagging for the words ..</p>
",Parsing & POS Tagging,part speech unknown known word different part speech tagging unknown word part speech tagging known word tool predict part speech tagging word
POS accuracy of known and unknown words,"<p>How do I calculate the accuracy of known and unknown words in part of speech tagging? For example for known words, is it dividing the correctly tagged known words by all the known words ? Any other ways ?</p>
",Parsing & POS Tagging,po accuracy known unknown word calculate accuracy known unknown word part speech tagging example known word dividing correctly tagged known word known word way
extract NP-VP-NP from Stanford dependency parse tree,"<p>I need to extract triplets of the form <code>NP-VP-NP</code> from the dependency parse tree produced as the output of lexalized parsing in Stanford Parser.</p>

<p>Whats the best way to do this.
e.g. If the parse tree is as follows:</p>

<pre><code>(ROOT
  (S
    (S
      (NP (NNP Exercise))
      (VP (VBZ reduces)
        (NP (NN stress)))
      (. .))
    (NP (JJ Regular) (NN exercise))
    (VP (VBZ maintains)
      (NP (JJ mental) (NN fitness)))
    (. .)))
</code></pre>

<p>I need to extract 2 triplets: </p>

<ol>
<li>Exercise-reduces-stress and  </li>
<li>Regular Exercise-maintains-mental fitness</li>
</ol>

<p>Any ideas?</p>
",Parsing & POS Tagging,extract np vp np stanford dependency parse tree need extract triplet form dependency parse tree produced output lexalized parsing stanford parser whats best way e g parse tree follows need extract triplet exercise reduces stress regular exercise maintains mental fitness idea
How can I remove POS tags before slashes in nltk?,"<p>This is part of my project where I need to represent the output after phrase detection like this - (a,x,b) where a, x, b are phrases. I constructed the code and got the output like this:</p>

<pre><code>(CLAUSE (NP Jack/NNP) (VP loved/VBD) (NP Peter/NNP))
(CLAUSE (NP Jack/NNP) (VP stayed/VBD) (NP in/IN London/NNP))
(CLAUSE (NP Tom/NNP) (VP is/VBZ) (NP in/IN Kolkata/NNP))
</code></pre>

<p>I want to make it just like the previous representation which means I have to remove 'CLAUSE', 'NP', 'VP', 'VBD', 'NNP' etc tags.</p>

<p>How to do that? </p>

<h2>What I tried</h2>

<p>First wrote this in a text file, tokenize and used <code>list.remove('word')</code>. But that is not at all helpful.
I am clarifying a bit more.</p>

<h2>My Input</h2>

<p><code>(CLAUSE (NP Jack/NNP) (VP loved/VBD) (NP Peter/NNP))
(CLAUSE (NP Jack/NNP) (VP stayed/VBD) (NP in/IN London/NNP))</code></p>

<h2>Output will be</h2>

<p>[Jack,loved,Peter], [Jack,stayed,in London] 
The output is just according to the braces and without the tags.</p>
",Parsing & POS Tagging,remove po tag slash nltk part project need represent output phrase detection like x b x b phrase constructed code got output like want make like previous representation mean remove clause np vp vbd nnp etc tag tried first wrote text file tokenize used helpful clarifying bit input output jack loved peter jack stayed london output according brace without tag
Stanford NNDep parser: features used,"<p>In regards to Stanford’s neural network dependency parser<a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow"">*̦</a> which features are used during training and testing phases? In practice, which columns in a CONLL<a href=""http://universaldependencies.github.io/docs/format"" rel=""nofollow"">ᶸ</a><a href=""http://ilk.uvt.nl/conll/#dataformat"" rel=""nofollow"">ˣ</a> formatted data set could be substituted with <strong>_</strong> without the parser loosing any accuracy when training? Which columns are never read?</p>

<p>Certainly <code>ID</code>, <code>FORM</code> and <code>HEAD</code> (columns # <strong>1</strong>, <strong>2</strong> &amp; <strong>7</strong>) are a must, as most likely are <code>U/C-POSTAG</code> (# <strong>4</strong>) and <code>DEPREL</code> (# <strong>8</strong>). But how about the columns <code>LEMMA</code>, <code>(X)-POSTAG</code> and <code>FEATS</code> (# <strong>3</strong>, <strong>5</strong> &amp; <strong>6</strong>)? Do they help while training, or whether the treebank contains any information in these is irrelevant for the parser?</p>
",Parsing & POS Tagging,stanford nndep parser feature used regard stanford neural network dependency parser feature used training testing phase practice column conll formatted data set could substituted without parser loosing accuracy training column never read certainly column must likely column help training whether treebank contains information irrelevant parser
Get parse tree of a sentence using OpenNLP. Getting stuck with example.,"<p>OpenNLP is an Apache project on Natural Language Processing. One of the aims of an NLP program is to parse a sentence giving a tree of its grammatical structure. For example, the sentence ""The sky is blue."" might be parsed as</p>

<pre><code>      S
     / \
   NP   VP
  / \    | \
The sky is blue.
</code></pre>

<p>where <code>S</code> is Sentence, <code>NP</code> is Noun-phrase, and <code>VP</code> is Verb-phrase. Equivalently the above tree can be written down as a parenthesized string like this: <code>S(NP(The sky) VP(is blue.))</code></p>

<p>I am trying to be able to get the parenthesized strings from sentences using OpenNLP, but I can't get the example code to work. </p>

<p>In particular, I am following along <a href=""http://www.programcreek.com/2012/05/opennlp-tutorial/#parser"" rel=""nofollow"">the last part of this tutorial</a> and my code gets stuck at initializing <code>ParserModel</code>.</p>

<p>I have downloaded the appropriate binaries from <a href=""http://opennlp.apache.org/cgi-bin/download.cgi"" rel=""nofollow"">here</a> and added <code>opennlp-tools-1.5.3.jar</code> (which includes classes for all of the following objects) as a library to my IntelliJ project. Also, I moved <code>en-parser-chunking.bin</code> to my ""user.dir.""</p>

<p>The following is the code which should give me a parse tree, but it runs indefinitely at creating the <code>ParserModel</code> object. </p>

<pre><code>    InputStream is = new FileInputStream(""en-parser-chunking.bin"");
    ParserModel model = new ParserModel(is);
    Parser parser = ParserFactory.create(model);
    String sentence = ""The sky is blue."";
    Parse topParses[] = ParserTool.parseLine(sentence, parser, 1);
    for (Parse p : topParses)
        p.show();
    is.close();
</code></pre>

<p>It's my first day of working with OpenNLP, but I can't even get this simple example to work. </p>
",Parsing & POS Tagging,get parse tree sentence using opennlp getting stuck example opennlp apache project natural language processing one aim nlp program parse sentence giving tree grammatical structure example sentence sky blue might parsed sentence noun phrase verb phrase equivalently tree written parenthesized string like trying able get parenthesized string sentence using opennlp get example code work particular following along last part tutorial code get stuck initializing downloaded appropriate binary added includes class following object library intellij project also moved user dir following code give parse tree run indefinitely creating object first day working opennlp even get simple example work
Idf score for an unknown word?,"<p>My task is to extract keywords from a text. What i did is following:</p>

<p>I'm using the tf-idf ""algorithm"". For the idf part i'm crawling wikipedia articles and extract the noun phrases (opennlp) and store them in a database.</p>

<p>So when i analyze a text i just have to calculate the tf part and get the idf part from the database.</p>

<p>The results so far are very appealing. My only problem is -> since the texts i have to analyze differ from the wikipedia corpus, some words have a high tf but no idf value (it was not found in the wiki corpus). But sometimes these words are still very important (an example for this could be a new company which is not listed on wikipedia yet).</p>

<p>What should i take as an idf value if it wasn't found in the db(corpus)? (average idf is probably not so a good idea)</p>
",Parsing & POS Tagging,idf score unknown word task extract keywords text following using tf idf algorithm idf part crawling wikipedia article extract noun phrase opennlp store database analyze text calculate tf part get idf part database result far appealing problem since text analyze differ wikipedia corpus word high tf idf value wa found wiki corpus sometimes word still important example could new company listed wikipedia yet take idf value found db corpus average idf probably good idea
POS tagging using nltk takes time,"<p>I am trying to get POS tags using nltk, i think it should take less then or around 1 sencond for processing small text. But 2-3 sentence it takes 20-25 second.</p>

<pre><code>import nltk,re, time
def findPos( text):
    start_time = time.time()
    try:
        tokens = nltk.word_tokenize(text)
        pos_tags = nltk.pos_tag(tokens)
        print [ x[0] for x in pos_tags if x[1] == ""NN"" or ""NNP""]
    except Exception:
        import traceback
        traceback.format_exc()
    print(""--- %s seconds ---"" % (time.time() - start_time))

findPos(raw_input())
</code></pre>

<p>Any suggestion how to reduce time ere?</p>
",Parsing & POS Tagging,po tagging using nltk take time trying get po tag using nltk think take le around sencond processing small text sentence take second suggestion reduce time ere
How to develop a grammar correcting program using NLP techniques?,"<p>As part of my NLP course project I have decided to write a program that can correct the grammatical and semantic errors in a sentence with an idea in my mind that I can convert this program to an mobile app in future which can help in faster typing in mobile devices . For example my program should be able to detect and fix following sentences which contains a combination of grammatical and semantic errors.</p>

<pre><code>FROM[I *am reach* home] -&gt; TO[I am *reaching* home OR I *reached* home OR I have *reached* home ]
FROM[Dog is *barring* in the street] -&gt; TO[dog is *barking* in the street ]  
FROM[She *died* not *seen* to be understanding] -&gt; TO[she *does* not *seem* to understanding ]
FROM[I am not interested in *then*] -&gt; TO[i am not interested in *them* 
FROM[I would really *live* to meet you.] -&gt; TO[i would really *like* to meet you OR I would really *love* to meet you]  
FROM[He *if* a good boy.] -&gt; TO[He *is* a good boy.]
</code></pre>

<p><a href=""https://www.languagetool.org"" rel=""nofollow noreferrer"">https://www.languagetool.org</a> and <a href=""http://www.polishmywriting.com"" rel=""nofollow noreferrer"">http://www.polishmywriting.com</a> are doing a poor job in finding issues with above sentences whereas the grammar correction tools at </p>

<p><a href=""https://www.paperrater.com/"" rel=""nofollow noreferrer"">https://www.paperrater.com/</a>, <a href=""http://spellcheckplus.com"" rel=""nofollow noreferrer"">http://spellcheckplus.com</a>, <a href=""http://www.reverso.net/spell-checker/english-spelling-grammar/"" rel=""nofollow noreferrer"">http://www.reverso.net/spell-checker/english-spelling-grammar/</a> are good.  </p>

<p>Initially I had a rough idea in my mind about solving this problem which I thought will work to some extent and started with this project but  I am completely lost now. Even after couple of days of lot of browsing I am not able to get faintest idea about how I can solve this problem. I am new to NLP but I know about  parse trees , POS tagging, NLTK module in python etc., but I don't know how I can put this concepts together in use to solve my problem. For instance I can use rule based POS tagging to find that a 'verb form' is needed in sentence a like 'I reaching home' but how can I fix a semantic error or remove a totally out-of-context word from a sentence using POS tagging or parse tree or using lexical dictionary like WordNet?</p>

<p>I would like to at least write a  small application for now that can fix grammatical and semantic error to some extent and build  this over a period of time in future. 
Can anyone guide me about how I can approach this problem? What NLP techniques should I use and how should they be put together to solve this issue. Thanks.</p>

<p>Edit:</p>

<p>I have looked at </p>

<p><a href=""https://stackoverflow.com/questions/10252448/how-to-check-whether-a-sentence-is-correct-simple-grammar-check-in-python?rq=1"">How to check whether a sentence is correct (simple grammar check in Python)?</a></p>

<p>but I don't understand how to use parse tree for my application.</p>
",Parsing & POS Tagging,develop grammar correcting program using nlp technique part nlp course project decided write program correct grammatical semantic error sentence idea mind convert program mobile app future help faster typing mobile device example program able detect fix following sentence contains combination grammatical semantic error poor job finding issue sentence whereas grammar correction tool good initially rough idea mind solving problem thought work extent started project completely lost even couple day lot browsing able get faintest idea solve problem new nlp know parse tree po tagging nltk module python etc know put concept together use solve problem instance use rule based po tagging find verb form needed sentence like reaching home fix semantic error remove totally context word sentence using po tagging parse tree using lexical dictionary like wordnet would like least write small application fix grammatical semantic error extent build period time future anyone guide approach problem nlp technique use put together solve issue thanks edit looked href check whether sentence correct simple grammar check python understand use parse tree application
extracting special nodes from dependency parser,"<p>I want to find some nodes in the Stanford dependency parser, for example:</p>

<p>Sentence: <code>Microsoft ad says that Macs are too cool for its customers.</code></p>

<p>Dependencies:</p>

<pre><code> - compound(ad-2, Microsoft-1)
 - nsubj(says-3, ad-2)
 - root(ROOT-0, says-3)
 - mark(cool-8, that-4)
 - nsubj(cool-8, Macs-5)
 - cop(cool-8, are-6)
 - advmod(cool-8, too-7)
 - ccomp(says-3, cool-8)
 - case(customers-11, for-9)
 - nmod:poss(customers-11, its-10)
 - nmod:for(cool-8, customers-11)
</code></pre>

<p>I'd like to capture the following constructs:</p>

<pre><code>p1={Node with two outgoing edges with labels ""nsubj"" and ""ccomp""},

In its dependency tree, `says` satisfies this condition, so p1={says}
</code></pre>

<p>and</p>

<pre><code>s1={ n1={Node that connected to the p1 by an edge with label ""nsubj""},
Node connected to n1 by an edge with label ""nn"" or ""quantmod""} 

In its dependency tree s1={n1=ad, Microsoft}
</code></pre>

<p>I don't know how can I extract these nodes, I tried this structure for extracting ad, but it extracts Macs too!. I have no idea for extracting other nodes! Any help would be greatly appreciated. </p>

<blockquote>
  <p>typedDependency.reln().getShortName().equals(""nsubj"")</p>
</blockquote>

<p>Here is my code:</p>

<pre><code>Tree tree = sentence.get(TreeAnnotation.class);
        // Get dependency tree
        TreebankLanguagePack tlp = new PennTreebankLanguagePack();
        GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
        GrammaticalStructure gs = gsf.newGrammaticalStructure(tree);
        Collection&lt;TypedDependency&gt; td = gs.typedDependenciesCollapsed();
        System.out.println(td);

        Object[] list = td.toArray();
        System.out.println(list.length);
        TypedDependency typedDependency;
        for (Object object : list) {
        typedDependency = (TypedDependency) object;
        System.out.println(""Depdency Name  ""+typedDependency.dep().toString()+ "" :: ""+ ""Node  ""+typedDependency.reln());



        if (typedDependency.reln().getShortName().equals(""nsubj"")) {

                ????

}
         }
        }
    }
    }
</code></pre>
",Parsing & POS Tagging,extracting special node dependency parser want find node stanford dependency parser example sentence dependency like capture following construct know extract node tried structure extracting ad extract mac idea extracting node help would greatly appreciated typeddependency reln getshortname equal nsubj code
How to use NNDEP parser in Stanford parser to process Chinese data,"<p>We are currently using the NNDEP parser in Stanford parser to process Chinese data, expecting to obtain useful syntax trees. Below is what we used to set the parameters: </p>

<pre><code>java -cp ""./*"" edu.stanford.nlp.parser.nndep.DependencyParser -language chinese -model edu/stanford/nlp/models/parser/nndep/CTB_CoNLL_params.txt.gz -tagger.model edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger -escaper edu.stanford.nlp.trees.international.pennchinese.ChineseEscaper -textFile INPUT_FILE
</code></pre>

<p>However, the output is not as same as the grammar relations described in the paper <em><a href=""http://www.aclweb.org/anthology/W09-2307"" rel=""nofollow"">Discriminative reordering with Chinese grammatical relations features</a></em>. If we have two sentences: 1. 我把他打了, 2. 我打了他， the result we obtained is as following:</p>

<pre><code>   SUB(把-2, 我-1)
   root(ROOT-0, 把-2)
   SUB(打了。-4, 他-3)
   VMOD(把-2, 打了。-4)


   SUB(打了-2, 我-1)
   root(ROOT-0, 打了-2)
   OBJ(打了-2, 他。-3)
</code></pre>

<p>which is similar to the result outputted from the default English parser.</p>

<p>We referred to the manual and read the <a href=""https://github.com/stanfordnlp/CoreNLP/blob/8b5f36aa7b83e5954bd94974de934156af325403/src/edu/stanford/nlp/parser/nndep/DependencyParser.java#L998"" rel=""nofollow"">source code</a>, and we could not find any clue. Therefore, could anyone please let us know how to set the right parameter to process Chinese data in a right way? Many thanks!</p>
",Parsing & POS Tagging,use nndep parser stanford parser process chinese data currently using nndep parser stanford parser process chinese data expecting obtain useful syntax tree used set parameter however output grammar relation described paper discriminative reordering chinese grammatical relation feature two sentence result obtained following similar result outputted default english parser referred manual read source code could find clue therefore could anyone please let u know set right parameter process chinese data right way many thanks
Dependency parser evaluation with or without punctuation,"<p>I want to evaluate a dependency parser taking into consideration punctuation and not taking into consideration punctuation. How should I define the input data if I do not want to take into consideration punctuation? Should I use the same input data (normal sentences with punctuation) as input, the parser defines all the dependencies incuding punctuation. During evaluation I exclude all dependencies related to periods and commas, etc. Or should I remove punctuation in the input sentences?
Why is the punctuation often not included (CONLL-X) when evaluating a dependency parser?     </p>
",Parsing & POS Tagging,dependency parser evaluation without punctuation want evaluate dependency parser taking consideration punctuation taking consideration punctuation define input data want take consideration punctuation use input data normal sentence punctuation input parser defines dependency incuding punctuation evaluation exclude dependency related period comma etc remove punctuation input sentence punctuation often included conll x evaluating dependency parser
How to get Coarse-grained Part of Speech Tags?,"<p>I have a data set which is annotated by Collins parser. Right now, I am keeping the POS of each word in the data set as a feature. The problem is that I don't need fine-grained POS. So, I have combined some of the tags. For example, I assume all VBD,VBP,VBZ,VBG under the category of ""Verb"". And for nouns, I assume NNP and NNS as ""Noun"" category.</p>

<p>So, here is the list of POS tags that I have after doing all combinations:</p>

<blockquote>
  <p>VB, NN, TO, JJ, IN, EX, RB, WP, PRP, MD, UH, WRB, WDT, RP, CD, POS, DT, PRP$, WP$, CC, RBR</p>
</blockquote>

<p>Now, my question is where can I find a list of coarse-grained POS tags? Is there any standard coarse-grained POS tag list?</p>

<p>In my system, If I don't combine other POS tags, I can get better results. I am wondering if I am allowed to keep my current list? Or should I combine them as well?</p>

<p>Thanks in advance,</p>
",Parsing & POS Tagging,get coarse grained part speech tag data set annotated collins parser right keeping po word data set feature problem need fine grained po combined tag example assume vbd vbp vbz vbg category verb noun assume nnp nns noun category list po tag combination vb nn jj ex rb wp prp md uh wdt rp cd po dt prp wp cc rbr question find list coarse grained po tag standard coarse grained po tag list system combine po tag get better result wondering allowed keep current list combine well thanks advance
Tree node mapping to GrammaticalStructure dependency,"<p>I'm using the Stanford Core NLP framework 3.4.1 to construct syntactic parse trees of wikipedia sentences. After which I would like to extract out of each parse tree all of the tree fragments of certain length (i.e. at most 5 nodes), but I am having a lot of trouble figuring out how to do that without creating a new GrammaticalStructure for each sub-tree.</p>

<p>This is what I am using to construct the parse tree, most of the code is from TreePrint.printTreeInternal() for conll2007 format which I modified to suit my output needs:</p>

<pre><code>    DocumentPreprocessor dp = new DocumentPreprocessor(new StringReader(documentText));

    for (List&lt;HasWord&gt; sentence : dp) {
        StringBuilder plaintexSyntacticTree = new StringBuilder();
        String sentenceString = Sentence.listToString(sentence);

        PTBTokenizer tkzr = PTBTokenizer.newPTBTokenizer(new StringReader(sentenceString));
        List toks = tkzr.tokenize();
        // skip sentences smaller than 5 words
        if (toks.size() &lt; 5)
            continue;
        log.info(""\nTokens are: ""+PTBTokenizer.labelList2Text(toks));
        LexicalizedParser lp = LexicalizedParser.loadModel(
        ""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"",
        ""-maxLength"", ""80"");
        TreebankLanguagePack tlp = new PennTreebankLanguagePack();
        GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
        Tree parse = lp.apply(toks);
        GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
        Collection&lt;TypedDependency&gt; tdl = gs.allTypedDependencies();
        Tree it = parse.deepCopy(parse.treeFactory(), CoreLabel.factory());
        it.indexLeaves();

        List&lt;CoreLabel&gt; tagged = it.taggedLabeledYield();
        // getSortedDeps
        List&lt;Dependency&lt;Label, Label, Object&gt;&gt; sortedDeps = new ArrayList&lt;Dependency&lt;Label, Label, Object&gt;&gt;();
        for (TypedDependency dep : tdl) {
            NamedDependency nd = new NamedDependency(dep.gov().label(), dep.dep().label(), dep.reln().toString());
            sortedDeps.add(nd);
        }
        Collections.sort(sortedDeps, Dependencies.dependencyIndexComparator());

        for (int i = 0; i &lt; sortedDeps.size(); i++) {
          Dependency&lt;Label, Label, Object&gt; d = sortedDeps.get(i);

          CoreMap dep = (CoreMap) d.dependent();
          CoreMap gov = (CoreMap) d.governor();

          Integer depi = dep.get(CoreAnnotations.IndexAnnotation.class);
          Integer govi = gov.get(CoreAnnotations.IndexAnnotation.class);

          CoreLabel w = tagged.get(depi-1);

          // Used for both course and fine POS tag fields
          String tag = PTBTokenizer.ptbToken2Text(w.tag());

          String word = PTBTokenizer.ptbToken2Text(w.word());

          if (plaintexSyntacticTree.length() &gt; 0)
              plaintexSyntacticTree.append(' ');
          plaintexSyntacticTree.append(word+'/'+tag+'/'+govi);
        }
        log.info(""\nTree is: ""+plaintexSyntacticTree);
    }
</code></pre>

<p>In the output I need to get something of this format: word/Part-Of-Speech-tag/parentID which is compatible with the output of the <a href=""http://commondatastorage.googleapis.com/books/syntactic-ngrams/index.html"" rel=""nofollow noreferrer"" title=""Google Syntactic n-grams"">Google Syntactic N-Grams</a></p>

<p>I can't see to figure out, how I could get the POS tag and parentID from the original syntactic parse tree (stored in the GrammaticalStructure as a dependency list as far as I understand) for only a subset of nodes from the original tree.</p>

<p>I have also seen some mentions about the <a href=""https://stackoverflow.com/questions/19431754/using-stanford-parsercorenlp-to-find-phrase-heads/22841952#22841952"">HeadFinder</a> but as far as I understand that is only useful to construct the GrammaticalStructure, whereas I am trying to use the existing one. 
I have also seen a somwewhat similar issue about <a href=""https://stackoverflow.com/questions/32565312/cast-from-grammaticalstructure-to-tree"">converting GrammaticalStructure to Tree</a> but that is still an open issue and it does not tackle the issue of sub-trees or creating a custom output. Instead of creating a tree from the GrammaticalStructure I was thinking that I could just use the node reference from the tree to get the information I need, but I am basically missing an equivalent of getNodeByIndex() which can get index by node from GrammaticalStructure. </p>

<p><strong>UPDATE:</strong> I have manage to get all of the required information by using the SemanticGraph as suggested in the answer. Here is a basic snippet of code that does that:</p>

<pre><code>    String documentText = value.toString();
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize,ssplit,pos,depparse"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    Annotation annotation = new Annotation(documentText);
    pipeline.annotate(annotation);
    List&lt;CoreMap&gt; sentences =  annotation.get(CoreAnnotations.SentencesAnnotation.class);

    if (sentences != null &amp;&amp; sentences.size() &gt; 0) {
        CoreMap sentence = sentences.get(0);
        SemanticGraph sg = sentence.get(SemanticGraphCoreAnnotations.CollapsedDependenciesAnnotation.class);
        log.info(""SemanticGraph: ""+sg.toDotFormat());
       for (SemanticGraphEdge edge : sg.edgeIterable()) {
           int headIndex = edge.getGovernor().index();
           int depIndex = edge.getDependent().index();
           log.info(""[""+headIndex+""]""+edge.getSource().word()+""/""+depIndex+""/""+edge.getSource().get(CoreAnnotations.PartOfSpeechAnnotation.class));
       }
    }
</code></pre>
",Parsing & POS Tagging,tree node mapping grammaticalstructure dependency using stanford core nlp framework construct syntactic parse tree wikipedia sentence would like extract parse tree tree fragment certain length e node lot trouble figuring without creating new grammaticalstructure sub tree using construct parse tree code treeprint printtreeinternal conll format modified suit output need output need get something format word part speech tag parentid compatible output google syntactic n gram see figure could get po tag parentid original syntactic parse tree stored grammaticalstructure dependency list far understand subset node original tree also seen mention still open issue doe tackle issue sub tree creating custom output instead creating tree grammaticalstructure wa thinking could use node reference tree get information need basically missing equivalent getnodebyindex get index node grammaticalstructure update manage get required information using semanticgraph suggested answer basic snippet code doe
Lemmatizing words after POS tagging produces unexpected results,"<p>I am using python3.5 with the nltk pos_tag function and the WordNetLemmatizer. My goal is to flatten words in our database to classify text. I am trying to test using the lemmatizer and I encounter strange behavior when using the POS tagger on identical tokens. In the example below, I have a list of three strings and when running them in the POS tagger every other element is returned as a noun(NN) and the rest are return as verbs (VBG). </p>

<p>This affects the lemmatization. The out put looks like this:  </p>

<pre><code>pos Of token: v
lemmatized token: skydive
pos Of token: n
lemmatized token: skydiving
pos Of token: v
lemmatized token: skydive
</code></pre>

<p>If I add more elements to the list of identical strings this same pattern continues. The code in full I am using is this:</p>

<pre><code>tokens = ['skydiving', 'skydiving', 'skydiving']
lmtzr=WordNetLemmatizer()

def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return 'a'
    elif treebank_tag.startswith('V'):
        return 'v'
    elif treebank_tag.startswith('N'):
        return 'n'
    elif treebank_tag.startswith('R'):
        return 'r'
    elif treebank_tag.startswith('S'):
        return ''
    else:
        return ''

numTokens = (len(tokens))
for i in range(0,numTokens):
    tokens[i]=tokens[i].replace("" "","""")

noSpaceTokens = pos_tag(tokens)

for token in noSpaceTokens:
    tokenStr = str(token[1])
    noWhiteSpace = token[0].replace("" "", """")
    preLemmed = get_wordnet_pos(tokenStr)
    print(""pos Of token: "" + preLemmed)
    lemmed = lmtzr.lemmatize(noWhiteSpace,preLemmed)
    print(""lemmatized token: "" + lemmed)
</code></pre>
",Parsing & POS Tagging,lemmatizing word po tagging produce unexpected result using python nltk po tag function wordnetlemmatizer goal flatten word database classify text trying test using lemmatizer encounter strange behavior using po tagger identical token example list three string running po tagger every element returned noun nn rest return verb vbg affect lemmatization put look like add element list identical string pattern continues code full using
How to assign a score to each chunk in a sentence?,"<p>I'm working on a keyword extraction task in which I'd like to extract phrases instead of words. In order to chunk each sentence into meaningful parts, I do a part of speech tagging first and them based on linguistic rule extract only the Noun Phrases. Each noun phrase is a potential keyword to be extracted. However as I only need to extract 'k' keywords for each given document, I need a good way to rank the extracted noun phrases. A simple way is to calculate the TDIDF score for each term (within each noun phrase) and then the score of each noun phrase would be the multiplication of its constituent terms' TDIDF score. I wonder to know whether anyone has a better approach or any idea on my simple naive solution?</p>
",Parsing & POS Tagging,assign score chunk sentence working keyword extraction task like extract phrase instead word order chunk sentence meaningful part part speech tagging first based linguistic rule extract noun phrase noun phrase potential keyword extracted however need extract k keywords given document need good way rank extracted noun phrase simple way calculate tdidf score term within noun phrase score noun phrase would multiplication constituent term tdidf score wonder know whether anyone ha better approach idea simple naive solution
Feature extraction for text,"<p>I am stuck at this issue and am not able to find relevant literature. Not sure, if this is coding question to begin with.</p>

<p>I have articles related to some disaster. I want to do a temporal classification of the text. Hereby, I want to get the sentences/ phrases related to the infoamtion before the event. I want know about classification from background in ml. But I have no idea about relevant extraction. 
I have tried tokening the words and get relevant frequencies and also tried pos tagging using max ent.</p>

<p>I guess the problem reduces to analyzing the manually classified text and constructing features. But I am not sure how to extract patterns using the pos tags. I also am how to know the exhaustive set if features.</p>
",Parsing & POS Tagging,feature extraction text stuck issue able find relevant literature sure coding question begin article related disaster want temporal classification text hereby want get sentence phrase related infoamtion event want know classification background ml idea relevant extraction tried tokening word get relevant frequency also tried po tagging using max ent guess problem reduces analyzing manually classified text constructing feature sure extract pattern using po tag also know exhaustive set feature
How to get Stanford Deep Sentiment Leaf Sentiment,"<p>I'm using the Stanford sentiment analysis part of the CoreNLP Java library. I figured out how to extract the sentiment of nodes using the answer to this question (stackoverflow.com/questions/23729829/sentiment-ranked-nodes-in-dependency-parse-with-stanford-corenlp/25935721). I need the sentiment of each node, including leaves and I'm getting values of -1 for the sentiment of leaves (which should be in the range [0,4]).</p>

<p>Is there a way to get the sentiment of the leaves? I saw in the online demo that use to be online that the leaf nodes of the trees had sentiment value. I feel like the RNTN should be able to output sentiment values. I'm using the RNNCoreAnnotations.getPredictedClass function.</p>

<p>Thanks</p>
",Parsing & POS Tagging,get stanford deep sentiment leaf sentiment using stanford sentiment analysis part corenlp java library figured extract sentiment node using answer question stackoverflow com question sentiment ranked node dependency parse stanford corenlp need sentiment node including leaf getting value sentiment leaf range way get sentiment leaf saw online demo use online leaf node tree sentiment value feel like rntn able output sentiment value using rnncoreannotations getpredictedclass function thanks
Using Core Annotation vs. GrammaticalStructure / headWordNode,"<p>I am using the Stanford CoreNLP to parse sentences. I am trying to create lexicalized grammar with sentiment and other language annotations.</p>

<p>I am getting a parse + sentiment + dep using:</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, 
                  lemma, ner, parse, depparse, sentiment"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

Annotation annotation = pipeline.process(mySentence);
</code></pre>

<p>Since I would like to use also dependencies in my grammar, I've been using <code>GrammaticalStructureFactory</code> and its <code>newGrammaticalStructure()</code> method to get a grammatical structure instance. I noticed that the 'TreeGraphNode' has a 'headWordNode' and thus, I traverse both trees in synchronization, e.g.</p>

<pre><code>public void BuildSomething(Tree tree, TreeGraphNode gsNode) {
    List&lt;Tree&gt; gsNodes = gsNode.getChildrenAsList();
    List&lt;Tree&gt; constNodes = tree.getChildrenAsList();

    for (int i = 0; i &lt; constNodes.size(); i++) {
        Tree childTree = constNodes.get(i);
        TreeGraphNode childGsNode = (TreeGraphNode) gsNodes.get(i);

        // build for this ""sub-tree""            
        BuildSomething(childTree, childGsNode);
    }

    String headWord = gsNode.headWordNode().label().value();
    // do something with the head-word...

    // do other stuff...
}
</code></pre>

<p>And I call this function with the tree annotation and the grammatical structure root:</p>

<pre><code>for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
    Tree tree = sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class);
     GrammaticalStructure gs = gsf.newGrammaticalStructure(tree);

     BuildSomething(tree, gs.root);
}
</code></pre>

<p>Using something like the above code, I notice that the GrammaticalStructure I am getting does not necessarily matches the structure of the Tree (should it?) and also, I am getting sometimes a ""weird"" head selection (the determiner selected as head, seems incorrect):
<a href=""https://i.sstatic.net/R7i6P.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/R7i6P.png"" alt=""A parse with determiner as the head""></a></p>

<p>Is my usage of <code>GrammaticalStructure</code>/<code>headWordNode</code> is incorrect?</p>

<p>P.S. I've seen some code that uses 'HeadFinder' and 'node.headTerminal(hf, parent)' (<a href=""https://stackoverflow.com/questions/19431754/using-stanford-parsercorenlp-to-find-phrase-heads/22841952#22841952"">here</a>) but I thought that the other approach would be faster... so I am not sure if I can use the above or must use this approach</p>
",Parsing & POS Tagging,using core annotation v grammaticalstructure headwordnode using stanford corenlp parse sentence trying create lexicalized grammar sentiment language annotation getting parse sentiment dep using since would like use also dependency grammar using method get grammatical structure instance noticed treegraphnode ha headwordnode thus traverse tree synchronization e g call function tree annotation grammatical structure root using something like code notice grammaticalstructure getting doe necessarily match structure tree also getting sometimes weird head selection determiner selected head seems incorrect usage incorrect p seen code us headfinder node headterminal hf parent href thought approach would faster sure use must use approach p
NLP techniques for document classification?,"<p>I was wondering if there are any NLP techniques for document classification.  I was wondering if statistics of n-grams from part-of-speech tagging could be useful?  I can't seem to find too much in the literature on the topic..</p>

<p>Has anyone found any nlp technique that enhanced their document classification efforts?  If you know of any surveys on this topic that would be awesome.</p>

<p>Note.  I saw <a href=""https://stackoverflow.com/questions/5499448/part-of-speech-pos-tag-feature-selection-for-text-classification"">this question</a>, but my corpus is way too large for the only solution there to be practical.</p>
",Parsing & POS Tagging,nlp technique document classification wa wondering nlp technique document classification wa wondering statistic n gram part speech tagging could useful seem find much literature topic ha anyone found nlp technique enhanced document classification effort know survey topic would awesome note saw href question corpus way large solution practical
Cast from GrammaticalStructure to Tree,"<p>I am trying out the new NN Dependency Parser from Stanford. According to the demo they have provided, this is how the parsing is done:</p>

<pre><code>import edu.stanford.nlp.process.DocumentPreprocessor;
import edu.stanford.nlp.trees.GrammaticalStructure;
import edu.stanford.nlp.parser.nndep.DependencyParser;

...

GrammaticalStructure gs = null;

DocumentPreprocessor tokenizer = new DocumentPreprocessor(new StringReader(sentence));
for (List&lt;HasWord&gt; sent : tokenizer) {
    List&lt;TaggedWord&gt; tagged = tagger.tagSentence(sent);
    gs = parser.predict(tagged);

    // Print typed dependencies
    System.out.println(Grammatical structure: "" + gs);
} 
</code></pre>

<p>Now, what I want to do is this object <code>gs</code>, which is of class <code>GrammaticalStructure</code>, to be casted as a <code>Tree</code> object from <code>edu.stanford.nlp.trees.Tree</code>.</p>

<p>I naively tried out with simple casting:</p>

<pre><code>Tree t = (Tree) gs;
</code></pre>

<p>but, this is not possible (the IDE gives an error: Cannot cast from <code>GrammaticalStructure</code> to <code>Tree</code>). </p>

<p>How do I do this?</p>
",Parsing & POS Tagging,cast grammaticalstructure tree trying new nn dependency parser stanford according demo provided parsing done want object class casted object naively tried simple casting possible ide give error cast
How to get POS tags of compound words with stanford,"<p>I used Stanford POS Tagger to tag parts of speech in a sentence, i used the following code:</p>

<pre><code>private static MaxentTagger tagger = new MaxentTagger("".../english-left3words-distsim.tagger"");
String tags= tagger.tagString(st);   //st is a string 
</code></pre>

<p>That gives a result when words are not compound. But what I want is to get the POS Tag of compound words like ""go back"", computer science"", ""picking up"".</p>

<p>Any ideas?</p>
",Parsing & POS Tagging,get po tag compound word stanford used stanford po tagger tag part speech sentence used following code give result word compound want get po tag compound word like go back computer science picking idea
"Handling (, ,) and (. .) and other punctuation when processing natural language parse trees with Lisp","<p>My question has to do with post-processing of part-of-speech tagged and parsed natural language sentences.  Specifically, I am writing a component of a Lisp post-processor that takes as input a sentence parse tree (such as, for example, one produced by the Stanford Parser), extracts from that parse tree the phrase structure rules invoked to generate the parse, and then produces a table of rules and rule counts.  An example of input and output would be the following:</p>

<p>(1)  Sentence:</p>

<pre><code>John said that he knows who Mary likes
</code></pre>

<p>(2)  Parser output:</p>

<pre><code>(ROOT
  (S
    (NP (NNP John))
    (VP (VBD said)
      (SBAR (IN that)
        (S
          (NP (PRP he))
          (VP (VBZ knows)
            (SBAR
              (WHNP (WP who))
              (S
                (NP (NNP Mary))
                (VP (VBZ likes))))))))))
</code></pre>

<p>(3)  My Lisp program post-processor output for this parse tree:</p>

<pre><code>(S --&gt; NP VP)             3
(NP --&gt; NNP)              2
(VP --&gt; VBZ)              1
(WHNP --&gt; WP)             1
(SBAR --&gt; WHNP S)         1
(VP --&gt; VBZ SBAR)         1
(NP --&gt; PRP)              1
(SBAR --&gt; IN S)           1
(VP --&gt; VBD SBAR)         1
(ROOT --&gt; S)              1
</code></pre>

<p>Note the lack of punctuation in sentence (1).  That's intentional.  I am having trouble parsing the punctuation in Lisp -- precisely because some punctuation (commas, for example) are reserved for special purposes.  But parsing sentences without punctuation changes the distribution of the parse rules as well as the symbols contained in those rules, as illustrated by the following:</p>

<p>(4)  Input sentence:</p>

<pre><code>I said no and then I did it anyway
</code></pre>

<p>(5)  Parser output:</p>

<pre><code>(ROOT
  (S
    (NP (PRP I))
    (VP (VBD said)
      (ADVP (RB no)
        (CC and)
        (RB then))
      (SBAR
        (S
          (NP (PRP I))
          (VP (VBD did)
            (NP (PRP it))
            (ADVP (RB anyway))))))))
</code></pre>

<p>(6)  Input sentence (with punctuation):</p>

<pre><code>I said no, and then I did it anyway.
</code></pre>

<p>(7)  Parser output:</p>

<pre><code> (ROOT
   (S
     (S
       (NP (PRP I))
       (VP (VBD said)
         (INTJ (UH no))))
     (, ,)
     (CC and)
     (S
       (ADVP (RB then))
       (NP (PRP I))
       (VP (VBD did)
         (NP (PRP it))
         (ADVP (RB anyway))))
     (. .)))
</code></pre>

<p>Note how including punctuation completely rearranges the parse tree and also involves different POS tags (and thus, implies that different grammar rules were invoked to produce it)  So including punctuation is important, at least for my application.</p>

<p>What I need is to discover a way to include punctuation in rules, so that I can produce rules like the following, which would appear, for example, in the table like (3), as follows:</p>

<p>(8)  Desired rule:</p>

<pre><code>S --&gt; S , CC S .
</code></pre>

<p>Rules like (8) are in fact desired for the specific application I am writing.</p>

<p>But I am finding that doing this in Lisp is difficult:  In (7), for example, we observe the appearance of (, ,) and (. .) , both of which are problematic to handle in Lisp.</p>

<p>I have included my relevant Lisp code below.  Please note that I'm a neophyte Lisp hacker and so my code isn't particularly pretty or efficient.  If someone could suggest how I might modify my below code such that I can parse (7) to produce a table like (3) that includes a rule like (8), I would be most appreciative.</p>

<p>Here is my Lisp code relevant to this task:</p>

<pre><code>(defun WRITE-RULES-AND-COUNTS-SORTED (sent)
  (multiple-value-bind (rules-list counts-list)
      (COUNT-RULES-OCCURRENCES sent)
    (setf comblist (sort (pairlis rules-list counts-list) #'&gt; :key #'cdr))
    (format t ""~%"")
    (do ((i 0 (incf i)))
        ((= i (length comblist)) NIL)
      (format t ""~A~26T~A~%"" (car (nth i comblist)) (cdr (nth i comblist))))
    (format t ""~%"")))


 (defun COUNT-RULES-OCCURRENCES (sent)
   (let* ((original-rules-list (EXTRACT-GRAMMAR sent))
          (de-duplicated-list (remove-duplicates original-rules-list :test #'equalp))
          (count-list nil))
     (dolist (i de-duplicated-list)
       (push (reduce #'+ (mapcar #'(lambda (x) (if (equalp x i) 1 0)) original-rules-list) ) count-list))
     (setf count-list (nreverse count-list))
    (values de-duplicated-list count-list)))


 (defun EXTRACT-GRAMMAR (sent &amp;optional (rules-stack nil))
   (cond ((null sent) 
          NIL)
         ((and (= (length sent) 1)
               (listp (first sent))
               (= (length (first sent)) 2)
               (symbolp (first (first sent)))
               (symbolp (second (first sent))))
          NIL)
         ((and (symbolp (first sent)) 
               (symbolp (second sent)) 
               (= 2 (length sent)))
          NIL)
         ((symbolp (first sent))
          (push (EXTRACT-GRAMMAR-RULE sent) rules-stack)
          (append rules-stack (EXTRACT-GRAMMAR (rest sent)   )))
         ((listp (first sent))
          (cond ((not (and (listp (first sent)) 
                           (= (length (first sent)) 2) 
                           (symbolp (first (first sent))) 
                           (symbolp (second (first sent)))))
                 (push (EXTRACT-GRAMMAR-RULE (first sent)) rules-stack)
                 (append rules-stack (EXTRACT-GRAMMAR (rest (first sent))) (EXTRACT-GRAMMAR (rest sent) )))
               (t (append rules-stack (EXTRACT-GRAMMAR (rest sent)  )))))))


(defun EXTRACT-GRAMMAR-RULE (sentence-or-phrase)
  (append (list (first sentence-or-phrase))
          '(--&gt;)
          (mapcar #'first (rest sentence-or-phrase))))
</code></pre>

<p>The code is invoked as follows (using (1) as input, producing (3) as output):</p>

<pre><code>(WRITE-RULES-AND-COUNTS-SORTED  '(ROOT
  (S
    (NP (NNP John))
    (VP (VBD said)
      (SBAR (IN that)
        (S
          (NP (PRP he))
          (VP (VBZ knows)
            (SBAR
              (WHNP (WP who))
              (S
                (NP (NNP Mary))
                (VP (VBZ likes)))))))))))
</code></pre>
",Parsing & POS Tagging,handling punctuation processing natural language parse tree lisp question ha post processing part speech tagged parsed natural language sentence specifically writing component lisp post processor take input sentence parse tree example one produced stanford parser extract parse tree phrase structure rule invoked generate parse produce table rule rule count example input output would following sentence parser output lisp program post processor output parse tree note lack punctuation sentence intentional trouble parsing punctuation lisp precisely punctuation comma example reserved special purpose parsing sentence without punctuation change distribution parse rule well symbol contained rule illustrated following input sentence parser output input sentence punctuation parser output note including punctuation completely rearranges parse tree also involves different po tag thus implies different grammar rule invoked produce including punctuation important least application need discover way include punctuation rule produce rule like following would appear example table like follows desired rule rule like fact desired specific application writing finding lisp difficult example observe appearance problematic handle lisp included relevant lisp code please note neophyte lisp hacker code particularly pretty efficient someone could suggest might modify code parse produce table like includes rule like would appreciative lisp code relevant task code invoked follows using input producing output
Convert two lists to dictionary with values as list,"<p>I can convert two lists to dictionary</p>

<pre><code>&gt;&gt;&gt; keys = ['a', 'b', 'c']
&gt;&gt;&gt; values = [1, 2, 3]
&gt;&gt;&gt; dictionary = dict(zip(keys, values))
&gt;&gt;&gt; print dictionary
</code></pre>

<p>How to convert it to dictionary with keys but values as list.</p>

<pre><code>keys = ['a', 'b', 'c' ,'a']
values=[1, 2, 3, 4]
</code></pre>

<p>Output:</p>

<pre><code>{'a': [1,4], 'c': [3], 'b': [2]}
</code></pre>

<p>I am using this in dependency parser to get corresponding adjectives for nouns in text.
Note I have to do this for huge text so efficency matters.</p>

<p>Please state the computational time of approach as well.</p>
",Parsing & POS Tagging,convert two list dictionary value list convert two list dictionary convert dictionary key value list output using dependency parser get corresponding adjective noun text note huge text efficency matter please state computational time approach well
Stanford NN dependency parser: Unrecoverable error while loading a tagger model,"<p>I am trying to test the <a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow"">new Stanford Dependency parser</a> which works with Neural Networks. I am trying to run the demos which are included in the zip file. The files <code>ParserDemo.java</code> and <code>ParserDemo2.java</code> work fine. However the file <code>DependencyParserDemo.java</code>:</p>

<pre><code>import edu.stanford.nlp.ling.HasWord;
import edu.stanford.nlp.ling.TaggedWord;
import edu.stanford.nlp.parser.nndep.DependencyParser;
import edu.stanford.nlp.process.DocumentPreprocessor;
import edu.stanford.nlp.tagger.maxent.MaxentTagger;
import edu.stanford.nlp.trees.GrammaticalStructure;

import java.io.StringReader;
import java.util.List;

/**
 * Demonstrates how to first use the tagger, then use the NN dependency
 * parser. Note that the parser will not work on untagged text.
 *
 * @author Jon Gauthier
 */
public class DependencyParserDemo {
  public static void main(String[] args) {
    String modelPath = DependencyParser.DEFAULT_MODEL;
    String taggerPath = ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"";

    for (int argIndex = 0; argIndex &lt; args.length; ) {
      switch (args[argIndex]) {
        case ""-tagger"":
          taggerPath = args[argIndex + 1];
          argIndex += 2;
          break;
        case ""-model"":
          modelPath = args[argIndex + 1];
          argIndex += 2;
          break;
        default:
          throw new RuntimeException(""Unknown argument "" + args[argIndex]);
      }
    }

    String text = ""I can almost always tell when movies use fake dinosaurs."";

    MaxentTagger tagger = new MaxentTagger(taggerPath);
    DependencyParser parser = DependencyParser.loadFromModelFile(modelPath);

    DocumentPreprocessor tokenizer = new DocumentPreprocessor(new StringReader(text));
    for (List&lt;HasWord&gt; sentence : tokenizer) {
      List&lt;TaggedWord&gt; tagged = tagger.tagSentence(sentence);
      GrammaticalStructure gs = parser.predict(tagged);

      // Print typed dependencies
      System.err.println(gs);
    }
  }
}
</code></pre>

<p>Throws an error:</p>

<pre><code>Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:769)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:297)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:262)
    at DependencyParserDemo.main(DependencyParserDemo.java:40)
Caused by: java.io.IOException: Unable to resolve ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as either class path, filename or URL
    at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:448)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:764)
    ... 3 more
</code></pre>

<p>Could someone tell me what am I doing wrong?</p>
",Parsing & POS Tagging,stanford nn dependency parser unrecoverable error loading tagger model trying test new stanford dependency parser work neural network trying run demo included zip file file work fine however file throw error could someone tell wrong
Stanford Universal Dependencies on Python NLTK,"<p>Is there any way I can get the Universal dependencies using python, or nltk?I can only produce the parse tree.</p>

<p>Example:</p>

<p>Input sentence:</p>

<pre><code>My dog also likes eating sausage.
</code></pre>

<p>Output:</p>

<pre><code>Universal dependencies

nmod:poss(dog-2, My-1)
nsubj(likes-4, dog-2)
advmod(likes-4, also-3)
root(ROOT-0, likes-4)
xcomp(likes-4, eating-5)
dobj(eating-5, sausage-6)
</code></pre>
",Parsing & POS Tagging,stanford universal dependency python nltk way get universal dependency using python nltk produce parse tree example input sentence output
Is there a way to get the subject of a sentence using OpenNLP?,"<p>Is there a way to get the subject of a sentence using OpenNLP? 
I'm trying to identify the most important part of a users sentence.  Generally, users will be submitting sentences to our ""engine"" and we want to know exactly what the core topic is of that sentence.</p>

<p>Currently we are using openNlp to:</p>

<ol>
<li>Chunk the sentence</li>
<li>Identify the noun-phrase, verbs, etc of the sentence</li>
<li>Identify all  ""topics"" of the sentence</li>
<li>(NOT YET DONE!) Identify the ""core topic"" of the sentence</li>
</ol>

<p>Please let me know if you have any bright ideas..</p>
",Parsing & POS Tagging,way get subject sentence using opennlp way get subject sentence using opennlp trying identify important part user sentence generally user submitting sentence engine want know exactly core topic sentence currently using opennlp chunk sentence identify noun phrase verb etc sentence identify topic sentence yet done identify core topic sentence please let know bright idea
How to extract nouns using OpenNLP in Scala?,"<p>I want to extract noun phrases using openNLP. I have the following Scala code for it.</p>

<pre><code>object ParserTest1 extends Serializable {

  def apply(line: String): (String) = {
    val is = new FileInputStream(""/home/akshat/en-parser-chunking.bin"")
    val model = new ParserModel(is)
    val parser = ParserFactory.create(model)
    var nounPhrases = """"
    val topParses = ParserTool.parseLine(line, parser, 1)
    for (p &lt;- topParses) {
    getNounPhrases(p)
    }
  def getNounPhrases(p: Parse) {
    if (p.getType == ""NN"" || p.getType == ""NNS"" || p.getType == ""NNP"" || 
      p.getType == ""NNPS"") {
      nounPhrases += p.getCoveredText + "" ""
    }    

    for (child &lt;- p.getChildren) {
      getNounPhrases(child)
    }
}
(nounPhrases)
}
}
</code></pre>

<p>My code is working fine but I have to extract only the first two nouns from the line and stop looking for other nouns after that, basically stop the loop after finding the first two nouns.
What changes should be made in the code for this ?</p>
",Parsing & POS Tagging,extract noun using opennlp scala want extract noun phrase using opennlp following scala code code working fine extract first two noun line stop looking noun basically stop loop finding first two noun change made code
Two diffident output on Stanford parser,"<p>I parse a sentence ""The man swimming in the lake is my father."" with deduction ""...who is..."". ""swimming"" in this sentence is a verb. </p>

<p>The tree structure shows that ""swimming"" is a verb.
<a href=""https://i.sstatic.net/z4Ppm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/z4Ppm.png"" alt=""Tree parsing""></a></p>

<p>However, in dependency parsing (DP), ""swimming"" is a noun.</p>

<pre><code>(ROOT
  (S
    (NP
      (NP (DT The) (NN man) (NN swimming))
      (PP (IN in)
        (NP (DT the) (NN lake))))
    (VP (VBZ is)
      (NP (PRP$ my) (NN father)))
    (. .)))

det(swimming-3, The-1)
compound(swimming-3, man-2)
nsubj(father-9, swimming-3)
case(lake-6, in-4)
det(lake-6, the-5)
nmod:in(swimming-3, lake-6)
cop(father-9, is-7)
nmod:poss(father-9, my-8)
root(ROOT-0, father-9)
</code></pre>

<p>I am confusing about DP output, is it correct?</p>
",Parsing & POS Tagging,two diffident output stanford parser parse sentence man swimming lake father deduction swimming sentence verb tree structure show swimming verb however dependency parsing dp swimming noun confusing dp output correct
LaTeX and Text Statistics,"<p>with LaTeX is there the possibility to generate a table with some statistics about the text written?</p>

<p>For exemple, one table with the number of nouns, phrases, paragraph, and some index about the variability or frequency of the words used.</p>

<p>Anyway, if there is not a package like this, how can I build one? I am very interested about this.</p>

<p>Thank you in advance</p>
",Parsing & POS Tagging,latex text statistic latex possibility generate table statistic text written exemple one table number noun phrase paragraph index variability frequency word used anyway package like build one interested thank advance
Faster alternatives to Stanford CoreNLP for obtaining parse trees?,"<p>I love <a href=""http://www-nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">Stanford CoreNLP</a>, by now it is pretty accurate for my NLP needs. The problem is that analyzing massive amounts of text (let's say <em>millions</em> of sentences) takes <em>days</em>.</p>

<p>Are there alternative Java implementations that sacrifice some accuracy for efficiency while (ideally) providing the same API?</p>
",Parsing & POS Tagging,faster alternative stanford corenlp obtaining parse tree love stanford corenlp pretty accurate nlp need problem analyzing massive amount text let say million sentence take day alternative java implementation sacrifice accuracy efficiency ideally providing api
How to Interpret NLTK Brill Tagger Rules,"<p>For the generated Brill Tagger Rule:</p>

<pre><code>Rule('016', 'CS', 'QL', [(Word([1, 2, 3]),'as')])
</code></pre>

<p>I know:
<code>'CS'</code> is subordinating conjunction
<code>'QL'</code> is qualifier</p>

<p>I guess:
<code>[(Word([1, 2, 3]),'as')]</code> means the condition of the rule. It stands for the word <code>'as'</code> appear as the first, second or third position before the target word. Target word is word that is going to be tagged by POS tag.</p>

<p>I do not know:
What is the meaning for <code>'016'</code>?
How to interpret the rule as a whole?</p>
",Parsing & POS Tagging,interpret nltk brill tagger rule generated brill tagger rule know subordinating conjunction qualifier guess mean condition rule stand word appear first second third position target word target word word going tagged po tag know meaning interpret rule whole
How to create a GrammaticalRelation in Stanford CoreNLP,"<p>I have recently upgraded to the latest version of Stanford CoreNLP.  The code I previously used to get the subject or object in a sentence was </p>

<pre><code>System.out.println(""subject: ""+dependencies.getChildWithReln(dependencies.getFirstRoot(), EnglishGrammaticalRelations.NOMINAL_SUBJECT));
</code></pre>

<p>but this now returns <code>null</code>.</p>

<p>I have tried creating a relation with </p>

<pre><code>GrammaticalRelation subjreln =
                edu.stanford.nlp.trees.GrammaticalRelation.valueOf(""nsubj"");
</code></pre>

<p>without success.  If I extract a relation using code like</p>

<pre><code>GrammaticalRelation target = (dependencies.childRelns(dependencies.getFirstRoot())).iterator().next();
</code></pre>

<p>Then run the same request, </p>

<pre><code>System.out.println(""target: ""+dependencies.getChildWithReln(dependencies.getFirstRoot(), target));
</code></pre>

<p>then I get the desired result, confirming that the parsing worked fine (I also know this from printing out the full dependencies).</p>

<p>I suspect my problem has to do with the switch to universal dependencies, but I don't know how to create the GrammaticalRelation from scratch in a way that will match what the dependency parser found.</p>
",Parsing & POS Tagging,create grammaticalrelation stanford corenlp recently upgraded latest version stanford corenlp code previously used get subject object sentence wa return tried creating relation without success extract relation using code like run request get desired result confirming parsing worked fine also know printing full dependency suspect problem ha switch universal dependency know create grammaticalrelation scratch way match dependency parser found
Stanford nndep to get parse trees,"<p>Using Stanford CoreNLP, I am trying to parse text using the neural nets dependency parser. It runs really fast (that's why I want to use this and not the <code>LexicalizedParser</code>), and produces high-quality dependency relations. I am also interested in retrieving the parse trees (Penn-tree style) from that too. So, given the GrammaticalStructure, I am getting the root of that (using <code>root()</code>), and then trying to print it out using the <code>toOneLineString()</code> method. However, <code>root()</code> returns the root node of the tree, with an empty/null list of children. I couldn't find anything on this in the instructions or FAQs. </p>

<pre><code> GrammaticalStructure gs = parser.predict(tagged);

  // Print typed dependencies
  System.err.println(gs);

  // get the tree and print it out in the parenthesised form
  TreeGraphNode tree = gs.root();
  System.err.println(tree.toOneLineString());
</code></pre>

<p>The output of this is:</p>

<pre><code>  ROOT-0{CharacterOffsetBeginAnnotation=-1, CharacterOffsetEndAnnotation=-1, PartOfSpeechAnnotation=null, TextAnnotation=ROOT}Typed Dependencies: 
  [nsubj(tell-5, I-1), aux(tell-5, can-2), advmod(always-4, almost-3), advmod(tell-5, always-4), root(ROOT-0, tell-5), advmod(use-8, when-6), nsubj(use-8, movies-7), advcl(tell-5, use-8), amod(dinosaurs-10, fake-9), dobj(use-8, dinosaurs-10), punct(tell-5, .-11)]
  ROOT-0
</code></pre>

<p>How can I get the parse tree too?</p>
",Parsing & POS Tagging,stanford nndep get parse tree using stanford corenlp trying parse text using neural net dependency parser run really fast want use produce high quality dependency relation also interested retrieving parse tree penn tree style given grammaticalstructure getting root using trying print using method however return root node tree empty null list child find anything instruction faq output get parse tree
How to get noun phrase with multiple words inside it using Stanford NLP Tregex?,"<p>I'm trying to figure out if it is possible to efficently extract NP using condition with multiple words. This is my current code:</p>

<pre><code>public static List&lt;Tree&gt; getNounPhrasesWithMultipleKeywords(Annotation doc,
        List&lt;String&gt; tags) {
    StringBuilder sb = new StringBuilder();
    boolean firstWord = true;

    for (int i = 0; i &lt; tags.size(); i++) {
        String word = tags.get(i);
        String[] splitted = word.split("" "");
        for (String splitWord : splitted) {
            if (!firstWord) {
                sb.append("" &amp;"");
            }
            sb.append("" &lt;&lt; "" + splitWord);
            firstWord = false;
        }

    }
    // sb.append("")"");

    TregexPattern pattern = TregexPattern.compile(""NP &lt; (__""
            + sb.toString() + "")"");

    return getTreeWithPattern(doc, pattern);
}
</code></pre>

<p>Now, lets say that input phrase has got this tree:</p>

<pre><code>(ROOT (S (NP (ADJP (RB Poorly) (VBN controlled)) (NN asthma)) (VP (VBZ is) (NP (DT a) (JJ vicious) (NN disease))) (. .)))
</code></pre>

<p>I want to get only those NP, which contains tags specified in function argument, e.g. for input [""controlled"", ""asthma""] it should return </p>

<pre><code>(NP (ADJP (RB Poorly) (VBN controlled)) (NN asthma))
</code></pre>

<p>But when input is [""injection"", ""controlled"", ""asthma""] it should return nothing.</p>

<p>As you can see, if one of input strings is ""multiple words"", then program splits it into words. I think that there should be better solution for it, but I don't know how it should work.</p>
",Parsing & POS Tagging,get noun phrase multiple word inside using stanford nlp tregex trying figure possible efficently extract np using condition multiple word current code let say input phrase ha got tree want get np contains tag specified function argument e g input controlled asthma return input injection controlled asthma return nothing see one input string multiple word program split word think better solution know work
How do I extract the offset of a WordNet synset give a synset in Python NLTK?,"<p>A sense offset in WordNet is an 8 digit number followed by a POS tag. For example, the offset for the synset 'dog.n.01' is '02084071-n'. I have tried the following code:</p>

<pre><code>    from nltk.corpus import wordnet as wn

    ss = wn.synset('dog.n.01')
    offset = str(ss.offset)
    print (offset)
</code></pre>

<p>However, I get this output:</p>

<pre><code>    &lt;bound method Synset.offset of Synset('dog.n.01')&gt;
</code></pre>

<p>How do I get the actual offset in this format: '02084071-n'?</p>
",Parsing & POS Tagging,extract offset wordnet synset give synset python nltk sense offset wordnet digit number followed po tag example offset synset dog n n tried following code however get output get actual offset format n
How to get dependency parse output exactly as online demo?,"<p>How can I programmatically get the same dependency parse using stanford corenlp as seen in the online demo?</p>

<p>I am using the corenlp package to obtain the dependency parse for the following sentence.</p>

<p><strong>Second healthcare worker in Texas tests positive for Ebola , authorities say .</strong></p>

<p>I try to obtain the parse programmatically using the code below</p>

<pre><code>            Properties props = new Properties();
            props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
            StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

            String text = ""Second healthcare worker in Texas tests positive for Ebola , authorities say .""; // Add your text here!
            Annotation document = new Annotation(text);
            pipeline.annotate(document);
            String[] myStringArray = {""SentencesAnnotation""};
            List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
            for(CoreMap sentence: sentences) {
                SemanticGraph dependencies = sentence.get(BasicDependenciesAnnotation.class);
                IndexedWord root = dependencies.getFirstRoot();
                System.out.printf(""root(ROOT-0, %s-%d)%n"", root.word(), root.index());
                for (SemanticGraphEdge e : dependencies.edgeIterable()) {
                    System.out.printf (""%s(%s-%d, %s-%d)%n"", e.getRelation().toString(), e.getGovernor().word(), e.getGovernor().index(), e.getDependent().word(), e.getDependent().index());
                }
            }

    }
</code></pre>

<p>I get the following output using the stanford corenlp 3.5.0 package.</p>

<pre><code>root(ROOT-0, worker-3)
amod(worker-3, Second-1)
nn(worker-3, healthcare-2)
prep(worker-3, in-4)
amod(worker-3, positive-7)
dep(worker-3, say-12)
pobj(in-4, tests-6)
nn(tests-6, Texas-5)
prep(positive-7, for-8)
pobj(for-8, ebola-9)
nsubj(say-12, authorities-11)
</code></pre>

<p>But the online demo gives a different answer that marks say as the root and has other relationships like ccomp between words in the parse.</p>

<pre><code>amod(worker-3, Second-1)
nn(worker-3, healthcare-2)
nsubj(tests-6, worker-3)
prep(worker-3, in-4)
pobj(in-4, Texas-5)
ccomp(say-12, tests-6)
acomp(tests-6, positive-7)
prep(positive-7, for-8)
pobj(for-8, Ebola-9)
nsubj(say-12, authorities-11)
root(ROOT-0, say-12)
</code></pre>

<p>How can I resolve my output to match with the online demo?</p>
",Parsing & POS Tagging,get dependency parse output exactly online demo programmatically get dependency parse using stanford corenlp seen online demo using corenlp package obtain dependency parse following sentence second healthcare worker texas test positive ebola authority say try obtain parse programmatically using code get following output using stanford corenlp package online demo give different answer mark say root ha relationship like ccomp word parse resolve output match online demo
How to match the single/plural forms and tense of the sentences in NLP?,"<p>I am trying to mix sentences together.</p>

<p>For example:</p>

<pre><code>We are nice guys.
John is a rich boy.
</code></pre>

<p>Using stanford parser (wrapped by NLTK in python), I can get the syntax tree, and concat the noun phrase and the verbal phrase together. like this:</p>

<pre><code>We is a rich boy. 
John are nice guys.
</code></pre>

<p>I want to do at least some verbal morphing, to turn them into <code>""We are ""</code> and <code>""John is""</code>. It would be harder to match the later nouns to ""boys""/""guy"".</p>

<p>Also matching tense would be nice, but it doesn't cause as much trouble as the single/plural form matching.</p>

<p>I have searched the Internet, and cannot find good resources to deal with this.</p>
",Parsing & POS Tagging,match single plural form tense sentence nlp trying mix sentence together example using stanford parser wrapped nltk python get syntax tree concat noun phrase verbal phrase together like want least verbal morphing turn would harder match later noun boy guy also matching tense would nice cause much trouble single plural form matching searched internet find good resource deal
How to extract derivation rules from a bracketed parse tree?,"<p>I have a lot of parse trees like this:</p>
<pre><code>( S ( NP-SBJ ( PRP I  )  )  ( INODE@S ( VP ( VBP have  )  ( NP ( DT a  )  ( INODE@NP ( NN savings  )  ( NN account  )  )  )  )  ( . .  )  )  )
</code></pre>
<p>for a sentence like this: &quot;I have a savings account .&quot;</p>
<p>I need to extract all derivation rules from these trees.
The derivation rules like:</p>
<pre><code>S -&gt; NP-SBJ INODE@S
NP-SBJ -&gt; PRP 
PRP -&gt; I
INODE@S -&gt; VP NP
and so on.
</code></pre>
<p>Is there any prepared code (preferably in java) or pseudo code for this purpose?</p>
<p><strong>Edit:</strong></p>
<p>I think this problem is very general and common in many areas. The simplified problem is to find each parent and it's children from a parenthesis tree.</p>
",Parsing & POS Tagging,extract derivation rule bracketed parse tree lot parse tree like sentence like saving account need extract derivation rule tree derivation rule like prepared code preferably java pseudo code purpose edit think problem general common many area simplified problem find parent child parenthesis tree
How can I find grammatical relations of a noun phrase using Stanford Parser or Stanford CoreNLP,"<p>I am using stanford CoreNLP to try to find grammatical relations of noun phrases.</p>

<p>Here is an example: </p>

<p>Given the sentence ""The fitness room was dirty."" </p>

<p>I managed to identify ""The fitness room"" as my target noun phrase. I am now looking for a way to find that the ""dirty"" adjective has a relationship to ""the fitness room"" <strong>and not only to</strong> ""room"".</p>

<p>example code:</p>

<pre><code>private static void doSentenceTest(){
    Properties props = new Properties();
    props.put(""annotators"",""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
    StanfordCoreNLP stanford = new StanfordCoreNLP(props);

    TregexPattern npPattern = TregexPattern.compile(""@NP"");

    String text = ""The fitness room was dirty."";


    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);
    // run all Annotators on this text
    stanford.annotate(document);

    List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);
    for (CoreMap sentence : sentences) {

        Tree sentenceTree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
        TregexMatcher matcher = npPattern.matcher(sentenceTree);

        while (matcher.find()) {
            //this tree should contain ""The fitness room"" 
            Tree nounPhraseTree = matcher.getMatch();
            //Question : how do I find that ""dirty"" has a relationship to the nounPhraseTree


        }

        // Output dependency tree
        TreebankLanguagePack tlp = new PennTreebankLanguagePack();
        GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
        GrammaticalStructure gs = gsf.newGrammaticalStructure(sentenceTree);
        Collection&lt;TypedDependency&gt; tdl = gs.typedDependenciesCollapsed();

        System.out.println(""typedDependencies: ""+tdl); 

    }

}
</code></pre>

<p>I used the Stanford CoreNLP on the sentence extracted the root Tree object of it. On this tree object I managed to extract Noun Phrases using a TregexPattern and a TregexMatcher. This gives me a child Tree that contains the actual noun phrase. What I would like to do know is find modifiers of the noun phrase in the original sentence. </p>

<p>The typedDependecies ouptut gives me the following : </p>

<pre><code>typedDependencies: [det(room-3, The-1), nn(room-3, fitness-2), nsubj(dirty-5, room-3), cop(dirty-5, was-4), root(ROOT-0, dirty-5)]
</code></pre>

<p>where I can see <strong>nsubj(dirty-5, room-3)</strong> but I dont have the full noun phrase as dominator.</p>

<p>I hope I am clear enough.
Any help appreciated.</p>
",Parsing & POS Tagging,find grammatical relation noun phrase using stanford parser stanford corenlp using stanford corenlp try find grammatical relation noun phrase example given sentence fitness room wa dirty managed identify fitness room target noun phrase looking way find dirty adjective ha relationship fitness room room example code used stanford corenlp sentence extracted root tree object tree object managed extract noun phrase using tregexpattern tregexmatcher give child tree contains actual noun phrase would like know find modifier noun phrase original sentence typeddependecies ouptut give following see nsubj dirty room dont full noun phrase dominator hope clear enough help appreciated
Spark partition issue with Stanford NLP,"<p>I am trying to process millions of data with spark/scala integrated with stanford NLP (3.4.1).
Since I am using social media data I have to use NLP for the themes generation (pos tagging) and Sentiment calculation.</p>

<p>I have to deal with Twitter data and NON Twitter data separately.So I have two classes that deal with Twitter/Non Twitter</p>

<p>I am using lasy val initialization from each class for loading the stanfordNLP</p>

<pre><code>features: Seq[String] = Seq(""tokenize"",""ssplit"",""pos"",""parse"",""sentiment"")
  val props = new Properties()
  props.put(""annotators"", features.mkString("", ""))
  props.put(""pos.model"", ""tagger/gate-EN-twitter.model"")
  props.put(""parse.model"", ""tagger/englishSR.ser.gz"");
  val pipeline = new StanfordCoreNLP(props)
</code></pre>

<p>Note: For above Twitter I am using different pos model and shift reduce parse model for parsing. The reason I use shift reduce parser is for some of the junk
data at rum time the default PCFG model takes lot of time for processing and getting some Exception.Shift reduce parser will take around 15 seconds at load time and its faster at run time while processing the 
data.</p>

<p>NonTwitter class</p>

<pre><code> features: Seq[String] = Seq(""tokenize"",""ssplit"",""pos"",""parse"",""sentiment"")
    val props = new Properties()
    props.put(""annotators"", features.mkString("", ""))
    props.put(""parse.model"", ""tagger/englishSR.ser.gz"");
</code></pre>

<p>Here I am using the default pos model and shift reduce parser</p>

<p>Problem:</p>

<p>Currently we am running with 8 Nodes with 6 cores and I can run with 48 partition. For to process millions of data
with the above configuration with lesser partition it works fine for me.</p>

<p>8 Nodes and 6 cores we have almost 48 partition and if I ran with 42 Number of partition it takes around 1 hr to finish the processing.</p>

<p>with the current configuration I need to scale it to 200 partition</p>

<p>8 Nodes and 6 cores we have almost 48 partition and if we ran with 200 Number of partition it takes around 2 hr and finally throwing some exception saying the one node is lost
or java.lang.IllegalArgumentException: annotator ""sentiment"" requires annotator ""binarized_trees"" etc etc. </p>

<p>The problem is only if we scale up the number of partition to 200 with 8 Nodes and 6 cores which we have only 48 cores.</p>

<p>I have the suspect that its because of loading the shift reduce parser loading at each partition. I thought of loading this class at one time and then do the Broadcast but standforndNLP class is not searializable so I cannot broadcast.</p>

<p>The reason we need to scale to 200 partition is it will run quickly with lesser time to process this data.</p>
",Parsing & POS Tagging,spark partition issue stanford nlp trying process million data spark scala integrated stanford nlp since using social medium data use nlp theme generation po tagging sentiment calculation deal twitter data non twitter data separately two class deal twitter non twitter using lasy val initialization class loading stanfordnlp note twitter using different po model shift reduce parse model parsing reason use shift reduce parser junk data rum time default pcfg model take lot time processing getting exception shift reduce parser take around second load time faster run time processing data nontwitter class using default po model shift reduce parser problem currently running node core run partition process million data configuration lesser partition work fine node core almost partition ran number partition take around hr finish processing current configuration need scale partition node core almost partition ran number partition take around hr finally throwing exception saying one node lost java lang illegalargumentexception annotator sentiment requires annotator binarized tree etc etc problem scale number partition node core core suspect loading shift reduce parser loading partition thought loading class one time broadcast standforndnlp class searializable broadcast reason need scale partition run quickly lesser time process data
What is the acl tag in Stanford dependency parsing?,"<p>An <code>acl</code> tag appears in Stanford dependency parses with no explanation in the manual. For example, a sentence like ""are you going there"" you gives something like:</p>

<pre><code>root(ROOT-0, are-1)
nsubj(are-1, you-2)
acl(you-2, going-3)     &lt;--
advmod(going-3, there-4)
</code></pre>

<p>Can someone explain what this tag is?</p>
",Parsing & POS Tagging,acl tag stanford dependency parsing tag appears stanford dependency par explanation manual example sentence like going give something like someone explain tag
compare TypedDependencies from Stanford NLP dependency parser tree,"<p>I am trying a semantic match between two sentences by comparing the dependencies.<br>
I am getting two Stanford dependency trees from two different sentences. I want to compare and get a score for the semantic match between the sentences.<br></p>

<pre><code>for(TypedDependency td1 : dependencyList1)
    {
        for(TypedDependency td2 : dependencyList2)
        {
            score = td1.compareTo(td2);
        }
    }
</code></pre>

<p><code>dependencyList1</code> and <code>dependencyList2</code> are the list of all dependencies from sentences1 and sentence 2 respectively.
  I am using a <code>compareTo</code> function which gives out scores of <code>-1,0,1</code>.<br>
I then average out the scores to come up with a final score.<br>
I don't know how these scores are calculated.<br> Is there a better way to compare and identify similar dependencies.<br>
Any help would be appreciated.</p>
",Parsing & POS Tagging,compare typeddependencies stanford nlp dependency parser tree trying semantic match two sentence comparing dependency getting two stanford dependency tree two different sentence want compare get score semantic match sentence list dependency sentence sentence respectively using function give score average score come final score know score calculated better way compare identify similar dependency help would appreciated
NLTK syntax parsing,"<p>I need to parse raw text into syntax tree and find relation between parsed parts  using NLTK without defining special patterns (regex patterns included) and rules as NLTK books proposes.
Is there any opportunity to set this process automatically? 
Or without installing additional tools for integrating Stanford parser it is impossible to do in NLTK?</p>
",Parsing & POS Tagging,nltk syntax parsing need parse raw text syntax tree find relation parsed part using nltk without defining special pattern regex pattern included rule nltk book proposes opportunity set process automatically without installing additional tool integrating stanford parser impossible nltk
How to deserialize a CoNLL format dependency tree with ClearNLP?,"<p>Dependency parsing using ClearNLP creates a <code>DEPTree</code> object. I have parsed a large corpus and serialized all the data in CoNLL format (e.g., <a href=""https://code.google.com/p/clearnlp/wiki/NLPComponents#Data_Format"" rel=""nofollow"">this ClearNLP page on Google code</a>).</p>

<p>But I can't figure out how to deserialize them. ClearNLP provides a <code>DEPTree#toStringCoNLL()</code> method (scroll down <a href=""http://code.google.com/p/clearnlp/source/browse/src/main/java/com/googlecode/clearnlp/dependency/DEPTree.java?r=cac4fff0433b157d31f504753aaec8f06eb70cf0"" rel=""nofollow"">this page</a> to see it). I am looking for something to read a CoNLL format parse tree and create a <code>DEPTree</code> object. I tried to reverse-engineer it, but didn't really understand the inner workings of the code.</p>

<p>I have, instead, created my own dependency tree class to handle the basic functionalities I need, but I would really like to know how to get a <code>DEPTree</code> object instead. So far, I haven't found any method in their API which does this.</p>
",Parsing & POS Tagging,deserialize conll format dependency tree clearnlp dependency parsing using clearnlp creates object parsed large corpus serialized data conll format e g clearnlp page google code figure deserialize clearnlp provides method scroll page see looking something read conll format parse tree create object tried reverse engineer really understand inner working code instead created dependency tree class handle basic functionality need would really like know get object instead far found method api doe
What does the dependency-parse output of TurboParser mean?,"<p>I have been trying to use the dependency parse trees generated by <a href=""http://www.ark.cs.cmu.edu/TurboParser/"" rel=""noreferrer"">CMU's TurboParser</a>. It works flawlessly. The problem, however, is that there is very little documentation. I need to precisely understand the output of their parser. For example, the sentence ""<em>I solved the problem with statistics.</em>"" generates the following output:</p>

<pre><code>1   I           _   PRP PRP _   2   SUB
2   solved      _   VBD VBD _   0   ROOT
3   the         _   DT  DT  _   4   NMOD
4   problem     _   NN  NN  _   2   OBJ
5   with        _   IN  IN  _   2   VMOD
6   statistics  _   NNS NNS _   5   PMOD
7   .           _   .   .   _   2   P
</code></pre>

<p>I haven't found any documentation that can help understand what the various columns stand for, and how the indices in the second-last column (2, 0, 4, 2, ... ) are created. Also, I have no idea why there are two columns devoted to part-of-speech tags. Any help (or link to external documentation) will be of great help.</p>

<p>P.S. If you want to try out their parser, <a href=""http://demo.ark.cs.cmu.edu/parse"" rel=""noreferrer"">here is their online demo</a>.</p>

<p>P.P.S. Please do not suggest using Stanford's dependency parse output. I am interested in linear programming algorithms, which is not what Stanford's NLP system does.</p>
",Parsing & POS Tagging,doe dependency parse output turboparser mean trying use dependency parse tree generated cmu turboparser work flawlessly problem however little documentation need precisely understand output parser example sentence solved problem statistic generates following output found documentation help understand various column stand index second last column created also idea two column devoted part speech tag help link external documentation great help p want try parser online demo p p please suggest using stanford dependency parse output interested linear programming algorithm stanford nlp system doe
How to properly navigate an NLTK parse tree?,"<p>NLTK is driving me nuts again.</p>

<p>How do I properly navigate through an NLTK tree (or ParentedTree)?
I would like to identify a certain leaf with the parent node ""VBZ"", then I would like to move from there further up the tree and to the left to identify the NP node.</p>

<p>How do I do this? The NLTK tree class does not seem to be thought through... Or I am too stupid...</p>

<p>Thanks for your help!</p>

<p><img src=""https://i.sstatic.net/a1ANO.png"" alt=""Tree""></p>
",Parsing & POS Tagging,properly navigate nltk parse tree nltk driving nut properly navigate nltk tree parentedtree would like identify certain leaf parent node vbz would like move tree left identify np node nltk tree class doe seem thought stupid thanks help
NLTK: how can I list all pairs of adjacent subtrees (rooted in specific nonterminal) of a parse tree,"<p>How can I efficiently list all pairs of subtrees (rooted in specific nonterminal) of a parse tree? For example, I have the following tree:</p>

<blockquote>
  <p>(S (S (S (S (X (PRO pro))) (X (V v))) (X (ADJ adj))) (X (N n))) </p>
</blockquote>

<p>You can see the image on <a href=""https://i.sstatic.net/I9Tzk.png"" rel=""nofollow"">this link</a>.</p>

<p>I want to list all adjacent instances of the symbol X expanding to other symbols, i. e.:</p>

<ol>
<li><code>(X (PRO pro)) and (X (V v))</code></li>
<li><code>(X (V v)) and (X (ADJ adj))</code></li>
<li><code>(X (ADJ adj)) and (X (N n))</code></li>
</ol>
",Parsing & POS Tagging,nltk list pair adjacent subtrees rooted specific nonterminal parse tree efficiently list pair subtrees rooted specific nonterminal parse tree example following tree x pro pro x v v x adj adj x n n see image link want list adjacent instance symbol x expanding symbol e
CoreNLP for Original Dependencies with Neural Network Dependency Parsing,"<p>I use Stanford CoreNLP v 3.5.2 in order to get Neural Network Dependency Parsing.</p>

<p>The problem is I don't need Universal Dependencies I need original Dependencies.</p>

<p>Properties file can have <strong>parse.originalDependencies</strong> attribute, however there two questions</p>

<p>NN DepParsing is <strong>depparse</strong> option, and the attributes of <strong>depparse</strong> are not described in <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/corenlp.shtml</a></p>

<p>I am not sure that <strong>depparse.originalDependencies</strong> exists.</p>

<p>In addition, how exactly should I define it in properties file</p>

<pre><code>depparse.originalDependencies = 1
or 
depparse.originalDependencies = true
</code></pre>

<p>Thank you</p>
",Parsing & POS Tagging,corenlp original dependency neural network dependency parsing use stanford corenlp v order get neural network dependency parsing problem need universal dependency need original dependency property file parse originaldependencies attribute however two question nn depparsing depparse option attribute depparse described sure depparse originaldependencies exists addition exactly define property file thank
Stanford Neural Network Dependency Parser -filelist,"<p>I am a newbie in using Stanford DepParser</p>

<p>I want to proceed several sentences with Stanford DepParser.
If there any way instead of <strong>-textFile</strong> to use the list of files like <strong>-filelist</strong>. in CoreNLP</p>

<p>The usual run for command line parameters doesn't give anything</p>

<pre><code>java -cp stanford-parser.jar edu.stanford.nlp.parser.nndep.DependencyParser

EMPTY
</code></pre>
",Parsing & POS Tagging,stanford neural network dependency parser filelist newbie using stanford depparser want proceed several sentence stanford depparser way instead textfile use list file like filelist corenlp usual run command line parameter give anything
I am having problems doing Word Sense Disambiguation in Python using Lesk algorithm,"<p>I am new to Python and NLTK so please bear with me. I wish to find the sense of a word in the context of a sentence. I am using the Lesk WSD algorithm but it is giving different outputs every time I run it. I know that Lesk has some level of inaccuracy. But, I think a POS tag will increase accuracy. </p>

<p>The Lesk algorithm takes a POS tag as an argument, but it takes 'n','s','v' as an input and not 'NN','VBP' or other POS tags which are outputted by the pos_tag() function. I would like to know how to tag words in the form of 'n','s','v', or if there is a method in which I can convert the 'NN','VBP' and other tags into 'n','s','v', so I can give them as an input to the lesk(context_sentence,word,pos_tag) function. </p>

<p>I am calculating the sentiment score of every word using SentiWordNet afterwards. </p>

<pre><code>    from nltk.wsd import lesk
    from nltk import word_tokenize
    import nltk, re, pprint
    from nltk.corpus import sentiwordnet as swn

    def word_sense():

        sent = word_tokenize(""He should be happy."")
        word = ""be""
        pos = ""v""
        score = lesk(sent,word,pos)
        print(score)
        print (str(score),type(score))
        set1 = re.findall(""'([^']*)'"",str(score))[0]
        print (set1)
        bank = swn.senti_synset(str(set1))
        print (bank)

    word_sense()
</code></pre>
",Parsing & POS Tagging,problem word sense disambiguation python using lesk algorithm new python nltk please bear wish find sense word context sentence using lesk wsd algorithm giving different output every time run know lesk ha level inaccuracy think po tag increase accuracy lesk algorithm take po tag argument take n v input nn vbp po tag outputted po tag function would like know tag word form n v method convert nn vbp tag n v give input lesk context sentence word po tag function calculating sentiment score every word using sentiwordnet afterwards
Stanford Dependency Parser Jython,"<p>I am using the code from 
<a href=""http://blog.gnucom.cc/2010/using-the-stanford-parser-with-jython/"" rel=""nofollow"">http://blog.gnucom.cc/2010/using-the-stanford-parser-with-jython/</a> 
to generate the dependency parse.</p>

<pre class=""lang-jython prettyprint-override""><code>import sys
sys.path.append('/path/to/jar/stanford-parser-2008-10-26.jar')

from java.io import CharArrayReader
from edu.stanford.nlp import *

lp = parser.lexparser.LexicalizedParser('/path/to/englishPCFG.ser.gz')
tlp = trees.PennTreebankLanguagePack()
lp.setOptionFlags([""-maxLength"", ""80"", ""-retainTmpSubcategories""])

sentence = 'One of my favorite features of functional programming \
languages is that you can treat functions like values.'

toke = tlp.getTokenizerFactory().getTokenizer(CharArrayReader(sentence));
wordlist = toke.tokenize()

if (lp.parse(wordlist)):
    parse = lp.getBestParse()

gsf = tlp.grammaticalStructureFactory()
gs = gsf.newGrammaticalStructure(parse)
tdl = gs.typedDependenciesCollapsed()

print parse.toString() 
print tdl
</code></pre>

<p>It gives a list which contains tuples of the type:</p>

<pre><code>&lt;type 'edu.stanford.nlp.trees.TypedDependency'&gt;
</code></pre>

<p>How can I access the individual tuples to use the dependency parse?</p>
",Parsing & POS Tagging,stanford dependency parser jython using code generate dependency parse give list contains tuples type access individual tuples use dependency parse
Extract Noun phrase using stanford NLP,"<p>I am trying to find the Theme/Noun phrase from a sentence using Stanford NLP</p>

<p>For eg: the sentence ""the white tiger""  I would love to get</p>

<p>Theme/Nound phrase as  : white tiger.</p>

<p>For this I used pos tagger. My sample code is below.</p>

<p>Result I am getting is  ""tiger"" which is not correct. Sample code I used to run is</p>

<pre><code>public static void main(String[] args) throws IOException {
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,parse"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        Annotation annotation = new Annotation(""the white tiger)"");
        pipeline.annotate(annotation);
        List&lt;CoreMap&gt; sentences = annotation
                .get(CoreAnnotations.SentencesAnnotation.class);
        System.out.println(""the size of the senetence is......""
                + sentences.size());
        for (CoreMap sentence : sentences) {
            System.out.println(""the senetence is..."" + sentence.toString());
            Tree tree = sentence.get(TreeAnnotation.class);
            PrintWriter out = new PrintWriter(System.out);
            out.println(""The first sentence parsed is:"");
            tree.pennPrint(out);
            System.out.println(""does it comes here.....1111"");
            TregexPattern pattern = TregexPattern.compile(""@NP"");
            TregexMatcher matcher = pattern.matcher(tree);
            while (matcher.find()) {
                Tree match = matcher.getMatch();
                List&lt;Tree&gt; leaves1 = match.getChildrenAsList();
                StringBuilder stringbuilder = new StringBuilder();
                for (Tree tree1 : leaves1) {
                    String val = tree1.label().value();
                    if (val.equals(""NN"") || val.equals(""NNS"")
                            || val.equals(""NNP"") || val.equals(""NNPS"")) {
                        Tree nn[] = tree1.children();
                        String ss = Sentence.listToString(nn[0].yield());
                        stringbuilder.append(ss).append("" "");

                    }
                }
                System.out.println(""the final stringbilder is ....""
                        + stringbuilder);
            }

        }

    }
</code></pre>

<p>Any help is really appreciated.Any other thoughts to get this achieved.</p>
",Parsing & POS Tagging,extract noun phrase using stanford nlp trying find theme noun phrase sentence using stanford nlp eg sentence white tiger would love get theme nound phrase white tiger used po tagger sample code result getting tiger correct sample code used run help really appreciated thought get achieved
Is there a Python wrapper for Stanford Neural Net based dependency parser?,"<p>I know about the Python wrappers for Stanford CoreNLP package but this package does not seem to contain neural net based dependency parser model. Rather it is present in Stanford-parser-full-****-<strong>-</strong> package for which I can't find any Python wrapper. <strong>My Question: Is there a Python wrapper that would parse using Stanford Neural Net based dependency parser?</strong> Any suggestions or directions would be helpful. Thanks!</p>
",Parsing & POS Tagging,python wrapper stanford neural net based dependency parser know python wrapper stanford corenlp package package doe seem contain neural net based dependency parser model rather present stanford parser full package find python wrapper question python wrapper would parse using stanford neural net based dependency parser suggestion direction would helpful thanks
stanford nlp pos tagging,"<p>Im doing my POS (part of speech tagging) from this tagger. But when i combine that part to my maven project it doesnt work. Is there a way where i can user stanford directly to do pos without using a seperate tagger? I want the output as same as this. </p>

<pre><code> MaxentTagger tagger = new MaxentTagger(""taggers/left3words-wsj-0-18.tagger"");
        String sample = ""Im so happy about my marks"";
        String tagged = tagger.tagString(sample);
        System.out.println(tagged);
</code></pre>

<p>output:Im/NNP so/RB happy/JJ about/IN my/PRP$ marks/NNS </p>
",Parsing & POS Tagging,stanford nlp po tagging im po part speech tagging tagger combine part maven project doesnt work way user stanford directly po without using seperate tagger want output output im nnp rb happy jj prp mark nns
Delete leaves in a tree with regex (Python),"<p>I have a syntax tree, saved in a text file in a ""LISP-style"", with open and closed brackets that show relations. I want to delete all leaves. For example, I have "" (Det the)"" that I want to become "" Det"". I'm not expert of regex, so I wonder how I could handle this behaviour in a more complex structure, with nested brackets. An example of tree (in my file is in one row, is indented just for a simpler visualization):</p>

<pre><code>(S
  (NP I)
  (VP
    (VP (V shot) (NP (Det an) (N elephant)))
    (PP (P in) (NP (Det my) (N pajamas)))))
</code></pre>

<p>I would have something like:</p>

<pre><code>(S NP
  (VP
    (VP V (NP Det N))
    (PP P (NP Det N))))
</code></pre>
",Parsing & POS Tagging,delete leaf tree regex python syntax tree saved text file lisp style open closed bracket show relation want delete leaf example det want become det expert regex wonder could handle behaviour complex structure nested bracket example tree file one row indented simpler visualization would something like
NLTK parses but doesn&#39;t display a sentence diagram?,"<p>I want to parse <em>dextromethorphan abuse is underreported</em> using a feature-based context-free grammar in NLTK 3.0. The next to last line of the console output seems to indicate that the FCFG successfully parsed this sentence. However <code>tree.draw()</code> does nothing. On further inspection the generator <code>trees</code> contains no elements. </p>

<p>This console output doesn't look different (to me) from other outputs that also generate a tree. I have generated trees with other FCFGs with this NLTK configuration. What gives?</p>

<p><strong>My NLTK Code</strong></p>

<pre><code>weasel_words = set(['moreover','likely'])
punkt = set(string.punctuation)

def deweasel(tokens):
return [token for token in tokens if token.lower() not in weasel_words
    and not token.lower().isdigit()]


cp = load_parser('./bip-grammar.fcfg',trace=2)
for tree in cp.parse(deweasel([token for token in word_tokenize(sentence) if token not in punkt])):
    tree.draw()
</code></pre>

<p>I've included <code>deweasel</code> for completeness. The error occurs whether <code>deweasel</code> is there or not.</p>

<p><strong>bip-grammar.fcfg</strong></p>

<pre><code> %start NP
 #Feature based context-free grammar
 #Base start is sentence

 S[NUM=?n,SEM=&lt;?vp(?np)&gt;] -&gt; NP[NUM=?n, SEM=?np] VP[NUM=?n,SEM=?vp] 


 NP[NUM=?n,SEM=?np] -&gt; N[NUM=?n, SEM=?np] 
 NP[NUM=?n, SEM=&lt;\P.P(?np(?adj))&gt;] -&gt; ADJ[SEM=?adj] N[NUM=?n, SEM=?np]
 NP[NUM=?n, SEM=?np] -&gt; ADJ[SEM=?np] 

 VP[NUM=?n,SEM=&lt;?np(?vp)&gt;] -&gt; BV[NUM=?n, SEM=?vp] PP[SEM=?np] PP
 VP[NUM=?n,SEM=?vp] -&gt; LV[NUM=?n] NP[SEM=?vp]

 PP[SEM=?np] -&gt; Prep NP[SEM=?np]

 N[NUM=sg, SEM=&lt;\P.P(abuse)&gt;] -&gt; 'abuse'
 N[NUM=pl, SEM=&lt;\P.P(reasons)&gt;] -&gt; 'reasons'
 BV[NUM=sg,SEM=&lt;\y x.agree(x,y)&gt;] -&gt; 'agree'
 LV[NUM=sg] -&gt; 'is'

 Prep -&gt; 'with' | 'for'

 ADJ[SEM=&lt;\P.P(DXM)&gt;] -&gt; 'dextromethorphan'
 ADJ[SEM=&lt;\P.P(underreported)&gt;] -&gt; 'underreported'
</code></pre>

<p><strong>Console Output</strong></p>

<pre><code> |.dex.abu. is.und.|
 Leaf Init Rule:
 |[---]   .   .   .| [0:1] 'dextromethorphan'
 |.   [---]   .   .| [1:2] 'abuse'
 |.   .   [---]   .| [2:3] 'is'
 |.   .   .   [---]| [3:4] 'underreported'
 Feature Bottom Up Predict Combine Rule:
 |[---]   .   .   .| [0:1] ADJ[SEM=&lt;\P.P(DXM)&gt;] -&gt; 'dextromethorphan' *
 Feature Bottom Up Predict Combine Rule:
 |[---&gt;   .   .   .| [0:1] NP[NUM=?n, SEM=&lt;\P.P(?np(?adj))&gt;] -&gt; ADJ[SEM=?adj] * N[NUM=?n, SEM=?np] {?adj: &lt;LambdaExpression \P.P(DXM)&gt;}
 |[---]   .   .   .| [0:1] NP[NUM=?n, SEM=&lt;\P.P(DXM)&gt;] -&gt; ADJ[SEM=&lt;\P.P(DXM)&gt;] *
 Feature Bottom Up Predict Combine Rule:
 |[---&gt;   .   .   .| [0:1] S[NUM=?n, SEM=&lt;?vp(?np)&gt;] -&gt; NP[NUM=?n, SEM=?np] * VP[NUM=?n, SEM=?vp] {?n2: Variable('?n'), ?np: &lt;LambdaExpression \P.P(DXM)&gt;}
 Feature Bottom Up Predict Combine Rule:
 |.   [---]   .   .| [1:2] N[NUM='sg', SEM=&lt;\P.P(abuse)&gt;] -&gt; 'abuse' *
 Feature Bottom Up Predict Combine Rule:
 |.   [---]   .   .| [1:2] NP[NUM='sg', SEM=&lt;\P.P(abuse)&gt;] -&gt; N[NUM='sg', SEM=&lt;\P.P(abuse)&gt;] *
 Feature Single Edge Fundamental Rule:
 |[-------]   .   .| [0:2] NP[NUM='sg', SEM=&lt;\P.P(abuse(DXM))&gt;] -&gt; ADJ[SEM=&lt;\P.P(DXM)&gt;] N[NUM='sg', SEM=&lt;\P.P(abuse)&gt;] *
 Feature Bottom Up Predict Combine Rule:
 |[-------&gt;   .   .| [0:2] S[NUM=?n, SEM=&lt;?vp(?np)&gt;] -&gt; NP[NUM=?n, SEM=?np] * VP[NUM=?n, SEM=?vp] {?n: u'sg', ?np: &lt;LambdaExpression \P.P(abuse(DXM))&gt;} 
 Feature Bottom Up Predict Combine Rule:
 |.   [---&gt;   .   .| [1:2] S[NUM=?n, SEM=&lt;?vp(?np)&gt;] -&gt; NP[NUM=?n, SEM=?np] * VP[NUM=?n, SEM=?vp] {?n: u'sg', ?np: &lt;LambdaExpression \P.P(abuse)&gt;}
 Feature Bottom Up Predict Combine Rule:
 |.   .   [---]   .| [2:3] LV[NUM='sg'] -&gt; 'is' *
 Feature Bottom Up Predict Combine Rule:
 |.   .   [---&gt;   .| [2:3] VP[NUM=?n, SEM=?vp] -&gt; LV[NUM=?n] * NP[SEM=?vp] {?n: u'sg'}
 Feature Bottom Up Predict Combine Rule:
 |.   .   .   [---]| [3:4] ADJ[SEM=&lt;\P.P(underreported)&gt;] -&gt; 'underreported' *
 Feature Bottom Up Predict Combine Rule:
 |.   .   .   [---&gt;| [3:4] NP[NUM=?n, SEM=&lt;\P.P(?np(?adj))&gt;] -&gt; ADJ[SEM=?adj] * N[NUM=?n, SEM=?np] {?adj: &lt;LambdaExpression \P.P(underreported)&gt;}
 |.   .   .   [---]| [3:4] NP[NUM=?n, SEM=&lt;\P.P(underreported)&gt;] -&gt; ADJ[SEM=&lt;\P.P(underreported)&gt;] *
 Feature Bottom Up Predict Combine Rule:
 |.   .   .   [---&gt;| [3:4] S[NUM=?n, SEM=&lt;?vp(?np)&gt;] -&gt; NP[NUM=?n, SEM=?np] * VP[NUM=?n, SEM=?vp] {?n2: Variable('?n'), ?np: &lt;LambdaExpression \P.P(underreported)&gt;}
 Feature Single Edge Fundamental Rule:
 |.   .   [-------]| [2:4] VP[NUM='sg', SEM=&lt;\P.P(underreported)&gt;] -&gt; LV[NUM='sg'] NP[SEM=&lt;\P.P(underreported)&gt;] *
 Feature Single Edge Fundamental Rule:
 |[===============]| [0:4] S[NUM='sg', SEM=&lt;underreported(abuse(DXM))&gt;] -&gt; NP[NUM='sg', SEM=&lt;\P.P(abuse(DXM))&gt;] VP[NUM='sg', SEM=&lt;\P.P(underreported)&gt;] *
  |.   [-----------]| [1:4] S[NUM='sg', SEM=&lt;underreported(abuse)&gt;] -&gt; NP[NUM='sg', SEM=&lt;\P.P(abuse)&gt;] VP[NUM='sg', SEM=&lt;\P.P(underreported)&gt;] *
</code></pre>
",Parsing & POS Tagging,nltk par display sentence diagram want parse dextromethorphan abuse underreported using feature based context free grammar nltk next last line console output seems indicate fcfg successfully parsed sentence however doe nothing inspection generator contains element console output look different output also generate tree generated tree fcfgs nltk configuration give nltk code included completeness error occurs whether bip grammar fcfg console output
Annotating a treebank with lexical information (Head Words) in JAVA,"<p>I have a treebank with syntactic parse tree for each sentence as given below:</p>

<p><strong>(S (NP (DT The) (NN government)) (VP (VBZ charges) (SBAR (IN that) (S (PP (IN between) (NP (NNP July) (CD 1971)) (CC and) (NP (NNP July) (CD 1992))) (, ,) (NP (NNP Rostenkowski)) (VP (VBD placed) (NP (CD 14) (NNS people)) (PP (IN on) (NP (NP (PRP$ his) (JJ congressional) (NN payroll)) (SBAR (WHNP (WP who)) (S (VP (VBD performed) (NP (NP (JJ personal) (NNS services)) (PP (IN for) (NP (NP (PRP him)) (CC and) (NP (PRP$ his) (NN family))))))))))))))</strong></p>

<p>I want to annotate the parse tree with lexical information like <strong>headwords</strong> for each node in the parse tree.</p>

<p>Can I do that using StanfordCoreNLP? Please guide me in the right direction. I would prefer a solution that can be implemented in JAVA as I am familiar with JAVA.</p>

<p>Thanks a lot!</p>
",Parsing & POS Tagging,annotating treebank lexical information head word java treebank syntactic parse tree sentence given np dt nn government vp vbz charge sbar pp np nnp july cd cc np nnp july cd np nnp rostenkowski vp vbd placed np cd nns people pp np np prp jj congressional nn payroll sbar whnp wp vp vbd performed np np jj personal nns service pp np np prp cc np prp nn family want annotate parse tree lexical information like headword node parse tree using stanfordcorenlp please guide right direction would prefer solution implemented java familiar java thanks lot
Find a path in an nltk.tree.Tree,"<p>I'm using <code>nltk.tree.Tree</code> in order to read a constituency-based parse tree. I need to find the path of nodes I need to move through to get from one specific word in the tree, to another. </p>

<p>A quick example:</p>

<p>This is the parse tree of the sentece ""saw the dog"":</p>

<pre><code>(VP (VERB saw) (NP (DET the) (NOUN dog)))
</code></pre>

<p>If I want the path  between the word <code>the</code> and <code>dog</code>, it would  be: <code>DET, NP, NOUN</code>.</p>

<p>I'm not even sure how to start: how do I find the values of the leaves of the tree? How could I find a leave's/node's parent?</p>

<p>Thanks.</p>
",Parsing & POS Tagging,find path nltk tree tree using order read constituency based parse tree need find path node need move get one specific word tree another quick example parse tree sentece saw dog want path word would even sure start find value leaf tree could find leave node parent thanks
"Possible approach to sentiment analysis (I apologize, I&#39;m very new to NLP)","<p>So I have an idea for classifying sentiments of sentences talking about a given brand product (in this case, pepsi). Basically, let's say I wanted to figure out how people feel about the taste of pepsi. Given this problem, I want to construct abstract sentence templates, basically possible sentence structures that would indicate an opinion about the taste of pepsi. Here's one example for a three word sentence:</p>

<pre><code>[Pepsi] [tastes] [good, bad, great, horrible, etc.]
</code></pre>

<p>I then look through my database of sentences, and try to find ones that match this particular structure. Once I have this, I can simply extract the third component and get a sentiment regarding this particular aspect (taste) of this particular entity (pepsi).</p>

<p>The application for this would be looking at tweets, so this might yield a few tweets from the past year or so, but it wouldn't be enough to get an accurate read on the general sentiment, so I would create other possible structures, like:</p>

<pre><code>[I] [love, hate, dislike, like, etc.] [the taste of pepsi]
[I] [love, hate, dislike, like, etc.] [the way pepsi tastes]
[I] [love, hate, dislike, like, etc.] [how pepsi tastes]
</code></pre>

<p>And so on and so forth.</p>

<p>Of course most tweets won't be this simple, there would be possible words that would mean the same as pepsi, or words in between the major components, etc - deviations that it would not be practical to account for. </p>

<p>What I'm looking for is just a general direction, or a subfield of sentiment analysis that discusses this particular problem. I have no problem coming up with a large list of possible structures, it's just the deviations from the structures that I'm worried about. I know this is something like a syntax tree, but most of what I've read about them has just been about generating text - in this case I'm trying to match a sentence to a structure, and pull out the entity, sentiment, and aspect components to get a basic three word answer.</p>
",Parsing & POS Tagging,possible approach sentiment analysis apologize new nlp idea classifying sentiment sentence talking given brand product case pepsi basically let say wanted figure people feel taste pepsi given problem want construct abstract sentence template basically possible sentence structure would indicate opinion taste pepsi one example three word sentence look database sentence try find one match particular structure simply extract third component get sentiment regarding particular aspect taste particular entity pepsi application would looking tweet might yield tweet past year enough get accurate read general sentiment would create possible structure like forth course tweet simple would possible word would mean pepsi word major component etc deviation would practical account looking general direction subfield sentiment analysis discus particular problem problem coming large list possible structure deviation structure worried know something like syntax tree read ha generating text case trying match sentence structure pull entity sentiment aspect component get basic three word answer
StanfordCoreNLP: why two different data structures for cons. parse and dependency parse?,"<p>Why Stanford:CoreNLP is using different data structures to represent its  trees (e.g. dep. trees with 'BasicDependenciesAnnotation' and cons. tree with 'TreeAnnotation')? 
It seems like these annotation are representable with the same data structure (like a DAG with labels). Is there any mechanism to cast these to each other? (at least for some of them)</p>

<p>LINK: <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/corenlp.shtml</a></p>
",Parsing & POS Tagging,stanfordcorenlp two different data structure con parse dependency parse stanford corenlp using different data structure represent tree e g dep tree basicdependenciesannotation con tree treeannotation seems like annotation representable data structure like dag label mechanism cast least link
wordnet lemmatizer in NLTK is not working for adverbs,"<pre><code>from nltk.stem import WordNetLemmatizer
x = WordNetLemmatizer()   
x.lemmatize(""angrily"", pos='r')
Out[41]: 'angrily'
</code></pre>

<p>Here is reference documnetation for pos tags in nltk wordnet, <a href=""http://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html"" rel=""nofollow"">http://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html</a></p>

<p>I may be missing some basic things. Please let me know</p>
",Parsing & POS Tagging,wordnet lemmatizer nltk working adverb reference documnetation po tag nltk wordnet may missing basic thing please let know
Swapping in Berkley parser in Stanford corenlp,"<p>I was using stanford nlp stack in my experiment and it was working nice until stanford PCFG parser started behaving weird for some of the sentences. I found <a href=""http://tomato.banatao.berkeley.edu:8080/parser/parser.html"" rel=""nofollow"">http://tomato.banatao.berkeley.edu:8080/parser/parser.html</a> the berkley parser giving correct parse tree for the sentences in my dataset. How could i swap in stanford pos tagger by bekley parser and continue using stanford dependency parser. I found here <a href=""http://brenocon.com/blog/2011/09/end-to-end-nlp-packages/"" rel=""nofollow"">http://brenocon.com/blog/2011/09/end-to-end-nlp-packages/</a> that it could be done, but not sure how.</p>

<p>Thanks in advance</p>

<hr>

<p>I have used following configuration for this purpose:</p>

<pre>

    props.put(""parse.type"",""charniak"");
    props.put(""parse.executable"",""src/main/resources/berkeley.bat"");
    props.put(""parse.model"","""");

    /*File: berkeley.bat*/

    @ECHO OFF
    java -jar C:\Users\Arindam\Downloads\berkeleymy.jar -gr C:/Users/Arindam/Downloads/eng_sm6.gr  -inputFile %4  -maxLength 399 -sentence_likelihood -kbest 2

</pre>
",Parsing & POS Tagging,swapping berkley parser stanford corenlp wa using stanford nlp stack experiment wa working nice stanford pcfg parser started behaving weird sentence found berkley parser giving correct parse tree sentence dataset could swap stanford po tagger bekley parser continue using stanford dependency parser found could done sure thanks advance used following configuration purpose prop put parse type charniak prop put parse executable src main resource berkeley bat prop put parse model file berkeley bat echo java jar c user arindam downloads berkeleymy jar gr c user arindam downloads eng sm gr inputfile maxlength sentence likelihood kbest
Where can I get the CFG used in Stanford Parser?,"<p>I am trying to identify the key phrase(s) in a question along with the type of answer expected. I am using Stanford Parser to generate the parse tree of the question. I need to traverse this parse tree and make choices at each node whether it is a key phrase or not based on some heuristics.
If only I had access to the complete CFG used in Stanford Parser, I could expand the heuristics to cover all the children of a node that might appear in the tree.</p>

<p><a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">The Stanford Parser: A statistical parser</a></p>
",Parsing & POS Tagging,get cfg used stanford parser trying identify key phrase question along type answer expected using stanford parser generate parse tree question need traverse parse tree make choice node whether key phrase based heuristic access complete cfg used stanford parser could expand heuristic cover child node might appear tree stanford parser statistical parser
Cutting down on Stanford parser&#39;s time-to-parse by pruning the sentence,"<p>We are already aware that the parsing time of Stanford Parser increases as the length of a sentence increases. I am interested in finding creative ways in which we prune the sentence such that the parsing time decreases without compromising on accuracy. For e.g. we can replace known noun phrases with one word nouns. Similarly can there be some other smart ways of guessing a subtree before hand, let's say, using the POS Tag information? We have a huge corpus of unstructured text at our disposal. So we wish to learn some common patterns that can ultimately reduce the parsing time. Also some references to publicly available literature in this regards will also be highly appreciated.</p>

<p>P.S. We already are aware of how to multi-thread using Stanford Parser, so we are not looking for answers from that point of view.</p>
",Parsing & POS Tagging,cutting stanford parser time parse pruning sentence already aware parsing time stanford parser increase length sentence increase interested finding creative way prune sentence parsing time decrease without compromising accuracy e g replace known noun phrase one word noun similarly smart way guessing subtree hand let say using po tag information huge corpus unstructured text disposal wish learn common pattern ultimately reduce parsing time also reference publicly available literature regard also highly appreciated p already aware multi thread using stanford parser looking answer point view
StanfordCoreNLP: Why multiple roots for SemanticGraph (e.g. dependency parsing),"<p>In the the definition of the SemanticGraph class which is being used for Dependency Parsing. </p>

<p>Here is the definition of the variable ""roots"" as a collection of vertices: </p>

<pre><code>private final Collection&lt;IndexedWord&gt; roots;
</code></pre>

<p>My question is why <em>collection</em>? In what cases we would need more than one vertex as the root? </p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/semgraph/SemanticGraph.java"" rel=""nofollow"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/semgraph/SemanticGraph.java</a></p>
",Parsing & POS Tagging,stanfordcorenlp multiple root semanticgraph e g dependency parsing definition semanticgraph class used dependency parsing definition variable root collection vertex question collection case would need one vertex root
How to compare complexities of corpora?,"<p>I would like to compare how complex (varied or predictable) are my three corpora. They are from different topics, so some vocabulary is different, some is the same. Looking at one of the data sets it's clear that the syntax is more difficult than in the other two, sentences are longer, etc. I built word N-Gram language models using the SRILM toolkit (I'm new to language modelling) with the idea that I can then compare these models. One measure mentioned in relation to language models is perplexity. I'm confused about the following question: Can I just use perplexities of the three LMs directly as a measure of how varied are the corpora? The vocabulary and the sizes of the corpora are different, so now I think that this won't be a good comparison. I also built LMs from POS-Tags but the quality of the POS-Tagging result is not good because the language is from fora, has spelling mistakes, ungrammatical sentences and so on. What measures could be used to compare complexity of corpora from different domains? I'd appreciate your advise.
[I'm new to Stackexchange. I posted this on Crossvalidated, but I think maybe here is more appropriate forum.]</p>
",Parsing & POS Tagging,compare complexity corpus would like compare complex varied predictable three corpus different topic vocabulary different looking one data set clear syntax difficult two sentence longer etc built word n gram language model using srilm toolkit new language modelling idea compare model one measure mentioned relation language model perplexity confused following question use perplexity three lm directly measure varied corpus vocabulary size corpus different think good comparison also built lm po tag quality po tagging result good language forum ha spelling mistake ungrammatical sentence measure could used compare complexity corpus different domain appreciate advise new stackexchange posted crossvalidated think maybe appropriate forum
Activate makeCopulaHead in Stanford CoreNLP parser,"<p>I want to use Stanford CoreNLP Parser to parse a sentence with the flag ""makeCopulaHead"" activated.</p>

<p>In my file input.txt, I have the following sentence:</p>

<pre><code>I am tall.
</code></pre>

<p>The objective is to not have a copula relation (cop) in the output dependency tree.</p>

<p>I tried:</p>

<pre><code>java -cp ""*"" -mx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -makeCopulaHead -file input.txt 
</code></pre>

<p>The .xml file contains cop relation :(</p>

<p>I also tried (a bug with xml-output: <a href=""https://mailman.stanford.edu/pipermail/java-nlp-user/2013-January/002959.html"" rel=""nofollow"">https://mailman.stanford.edu/pipermail/java-nlp-user/2013-January/002959.html</a> ?): </p>

<pre><code>java -cp ""*"" -mx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -makeCopulaHead -file input.txt -outputFormat text 
</code></pre>

<p>But it's the same thing...</p>
",Parsing & POS Tagging,activate makecopulahead stanford corenlp parser want use stanford corenlp parser parse sentence flag makecopulahead activated file input txt following sentence objective copula relation cop output dependency tree tried xml file contains cop relation also tried bug xml output thing
Treebank-style tree parser python,"<p>Recently i have been trying to parse syntactic trees returned by the <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">stanford parser</a> in python. I have been trying to do that with nltk <code>tree = Tree.parse(result['sentences'][0]['parsetree'])</code> and the parsing succeeds but the tree class of nltk offers very few processing methods. I will need methods like <code>tree.isPrePreTerminal()</code> which are not included from what i understand. I found <a href=""http://nlp.stanford.edu/~mcclosky/software/PyInputTree/"" rel=""nofollow"">this</a> alternative but it seems that it doesnt like 64bit architectures and it gives me this error <code>ImportError: InputTree/_InputTree.so: wrong ELF class: ELFCLASS32</code> even though i compiled with the <code>-m64</code> flag. 
I have been looking into this the last 2 days, if you know a way to make the above module to work with 64bit systems or an alternate library or at least a good nltk.tree documentation so that i implement the methods myself please let me know.</p>
",Parsing & POS Tagging,treebank style tree parser python recently trying parse syntactic tree returned stanford parser python trying nltk parsing succeeds tree class nltk offer processing method need method like included understand found alternative seems doesnt like bit architecture give error even though compiled flag looking last day know way make module work bit system alternate library least good nltk tree documentation implement method please let know
Stanford NLP pos tag PRN,"<p>I am using Stanford's CoreNLP pipeline. 
I am getting following TreeAnnotation:</p>

<pre><code>(ROOT (S (NP (PRP I)) (VP (VP (VBP love) (NP (NN hepling))) (PRN (CC and) (ADJP (JJ willing) (S (VP (TO to) (VP (VB help)))))))))
</code></pre>

<p>Note: the sentence is grammatically incorrect, but this is intensional.</p>

<p><strong>What is ""PRN"" tag in TreeAnnotation?</strong></p>

<p>I did not find such tag in <a href=""http://www.ling.upenn.edu/courses/Fall_2007/ling001/penn_treebank_pos.html"" rel=""nofollow"">Penn tags</a> nor in <a href=""http://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""nofollow"">Stanford dependency tags</a>.</p>
",Parsing & POS Tagging,stanford nlp po tag prn using stanford corenlp pipeline getting following treeannotation note sentence grammatically incorrect intensional prn tag treeannotation find tag penn tag stanford dependency tag
How to combine features of different types more effectively with python,"<p>I am doing a text classification task(7000 texts evenly distributed by 10 labels). I set character, word and POS as colume names and stored all characters words and POS tags of each text into <code>Sqlite3</code> database. What I want to do is to combine character 2-gram, word 2-gram and POS 2-gram into a training vector for <code>SVM</code>. My code is below:</p>

<pre><code>def dbConnect(db_name,sql):
    conn = sqlite3.connect(db_name)
    cursor = conn.cursor()
    n = cursor.execute(sql)
    res1 = cursor.fetchall()
    return res1

def createtrainingVector(item,n,count1,res1):
        ''' append all items(character/word/POS n-gram) that occurs at least count1 times in the text into a training Vector used for SVM'''

def featureComb(item,count1,res1):
    '''combine features into a vector'''
    if item == ""word"":
        train_vector1 = createtrainingVector(item,1,count1,res1)
        train_vector2 = createtrainingVector(item,2,count1,res1)
    elif item == ""character"":
        train_vector1 = createtrainingVector(item,1,count1,res1)
        train_vector2 = createtrainingVector(item,2,count1,res1)
    elif item == ""pos"":
        train_vector1 = createtrainingVector(item,1,count1,res1)
        train_vector2 = createtrainingVector(item,2,count1,res1)
    return sorted(train_vector1+train_vector2)

def svmParaComb(item,train_vector,label_vector,X,y,res1):
    matrix = []
    for res in res1:            
        y.append(label_vector[res[0]])

        if item == ""word"":
            word_dic1 = wordNgrams(res[1], 1)
            word_dic2 = wordNgrams(res[1], 2)
        elif lexical == ""pos"":
            word_dic1 = lemmaNgrams(json.loads(res[1]), 1)
            word_dic2 = lemmaNgrams(json.loads(res[1]), 2)
        elif lexical == ""character"":
            word_dic1 = characterNgrams(res[1], 1)
            word_dic2 = characterNgrams(res[1], 2)
        X.append([1 if gram in (word_dic1 or word_dic2) else 0 for gram in train_vecter)

db = 'text.db'
'''Access to database and Extract label and essay'''
sql_dict1 = {""word"": ""select label,word from main.training_data"",
        ""pos"" : ""select label,pos from main.training_data"",
        ""character"" : ""select label,character from main.training_data"",
        }
sql_dict2 = {""word"": ""select label,word from main.test_data"",
        ""pos"" : ""select label,pos from main.test_data"", 
        ""character"" : ""select label,character from main.test_data"",
        }

if i == 1:
        sql1 = sql_dict1[""word""]
        sql2 = sql_dict2[""word""]
elif i == 2:
        sql1 = sql_dict1[""character""]
        sql2 = sql_dict2[""character""]
elif i == 3:
        sql1 = sql_dict1[""pos""]
        sql2 = sql_dict2[""pos""]
res1 = dbConnect(db, sql1)
    X = []
    y = []
    train_vector_combine = featureComb(item, 2,res1)
    svmParaComb(item, train_vector_combine, label_vector, X, y, res1)

    clf1 = svm.LinearSVC()
    clf1.fit(X, y)

    res2 = dbConnect(db, sql2)

    '''Parameter used for predicting'''
    X_test = []
    y_true = []
    svmCombinePredict(item, train_vector, label_vector, X_test, y_true, res2)  

    clf1.predict(X_test)
    print clf1.score(X_test,y_true)
</code></pre>

<p>However, it can only combine features of the same type. I do not know how to deal with the combination of different types features, because res1 represents only one colume word/character/POS. therefore, if I want to combine character 2-gram, word 2-gram and POS 2-gram into a training vector, what should I do? Could anyone help me with my code?</p>
",Parsing & POS Tagging,combine feature different type effectively python text classification task text evenly distributed label set character word po colume name stored character word po tag text database want combine character gram word gram po gram training vector code however combine feature type know deal combination different type feature represents one colume word character po therefore want combine character gram word gram po gram training vector could anyone help code
How to print the parse tree of Stanford JavaNLP,"<p>I am trying to get all the noun phrases using the <code>edu.stanford.nlp.*</code> package. I got all the subtrees of label value ""NP"", but I am not able to get the normal original <code>String</code> format (not Penn Tree format). </p>

<p>E.g. for the <code>subtree.toString()</code> gives <code>(NP (ND all)(NSS times)))</code> but I want the string ""all times"". Can anyone please help me. Thanks in advance.</p>
",Parsing & POS Tagging,print parse tree stanford javanlp trying get noun phrase using package got subtrees label value np able get normal original format penn tree format e g give want string time anyone please help thanks advance
How to do semantic analysis from POS tag?,"<p>suppose sentence is, ""Vehicle does not start in cold weather and need to change windshield blades."" I'm interested to find out what part of car is affected, and what is the reason for that. From above sentence, we can not infer that windshield blades does not start. In addition single sentence can contain multiple car parts. How to tackle this problem?</p>
",Parsing & POS Tagging,semantic analysis po tag suppose sentence vehicle doe start cold weather need change windshield blade interested find part car affected reason sentence infer windshield blade doe start addition single sentence contain multiple car part tackle problem
concepts extraction from stanford parsing tree nlp,"<p>I am nlp research beginner
I want to extract the concepts from text
for example ""The Thing Things albums"" the concept is ""The Thing Things""</p>

<p>I am using parsing tree for noun phrases <strong>but</strong> in this example the tree extracts ""The Thing Things"" &amp;""albums""
and another example 
""Who started the handset alliance?"" I expected that ""handset alliance"" but the noun phrase is""the handset alliance""</p>

<p>how can I solve them ?</p>
",Parsing & POS Tagging,concept extraction stanford parsing tree nlp nlp research beginner want extract concept text example thing thing album concept thing thing using parsing tree noun phrase example tree extract thing thing album another example started handset alliance expected handset alliance noun phrase handset alliance solve
Punctuation in Noun Phrase Extraction,"<p>I am using OpenNLP to extract noun phrases from documents. In reviewing the output, I discovered that the phrase chunker ignores commas, leading to noun phrases that combine, for instance, multiple elements of a list into one phrase or two clauses in a sentence into one noun phrase. As a dummy example:</p>

<pre><code>public class TestTokenizer {
    public static void main(String[] args) throws IOException {
        String content = ""dog, cat, fish, rat"";
        String[] tokens = NLPToolsControllerOpenNLP.getInstance().getTokeniser().tokenize(content);
        String[] pos = NLPToolsControllerOpenNLP.getInstance().getPosTagger().tag(tokens);
        String[] phrases = NLPToolsControllerOpenNLP.getInstance().getPhraseChunker().chunk(tokens, pos);
        for(int i = 0; i&lt;tokens.length; i++) {
            System.out.println(""Token: "" + tokens[i] + "" and POS: "" + phrases[i]);
        }
        List&lt;String&gt; candidates = new ArrayList&lt;String&gt;();
        String phrase = """";
        for (int n = 0; n &lt; tokens.length; n++) {
            if (phrases[n].equals(""B-NP"")) {
                phrase = tokens[n];
                for (int m = n + 1; m &lt; tokens.length; m++) {
                    if (phrases[m].equals(""I-NP"")) {
                        phrase = phrase + "" "" + tokens[m];
                    } else {
                        n = m;
                        break;
                    }
                }
                phrase = phrase.replaceAll(""\\s+"", "" "").trim();
                System.out.println(""phrase: "" + phrase);
    }
}
</code></pre>

<p>outputs:</p>

<pre><code>Token: dog and POS: B-NP
Token: , and POS: I-NP
Token: cat and POS: I-NP
Token: , and POS: I-NP
Token: fish and POS: I-NP
Token: , and POS: O
Token: rat and POS: B-NP
phrase: dog , cat , fish
phrase: rat
</code></pre>

<p>Parentheses have the same issue: because the chunker tags them with I-NP, I end up with noun phrases containing them.</p>

<p>The OpenNLP documentation says that ""The <strong>OpenNLP Sentence Detector</strong> can detect that a punctuation character marks the end of a sentence or not."" As such, I am a bit surprised that the phrase detector cannot detect the use of a comma or a parenthesis to mark the beginning or end of a phrase. Is there something I am missing here? Is there another approach that I should use? I am trying to avoid dealing with these issues on a case-by-case basis in a large corpus.</p>
",Parsing & POS Tagging,punctuation noun phrase extraction using opennlp extract noun phrase document reviewing output discovered phrase chunker ignores comma leading noun phrase combine instance multiple element list one phrase two clause sentence one noun phrase dummy example output parenthesis issue chunker tag np end noun phrase containing opennlp documentation say opennlp sentence detector detect punctuation character mark end sentence bit surprised phrase detector detect use comma parenthesis mark beginning end phrase something missing another approach use trying avoid dealing issue case case basis large corpus
Clause Extraction using Stanford parser,"<p>I have a complex sentence and I  need to separate it into main and dependent clause.
For example for the sentence<br>
ABC cites the fact that chemical additives are banned in many countries and feels they may be banned in this state too.<br>
The split required</p>

<pre><code>1)ABC cites the fact   
2)chemical additives are banned in many countries   
3)ABC feels they may be banned in this state too.    
</code></pre>

<p>I think I could use the Stanford Parser tree or dependencies, but I am not  sure how to proceed from here.  </p>

<p>The tree </p>

<pre>
(ROOT
  (S
    (NP (NNP ABC))
    (VP (VBZ cites)
      (NP (DT the) (NN fact))
      (SBAR (IN that)
        (S
          (NP (NN chemical) (NNS additives))
          (VP
            (VP (VBP are)
              (VP (VBN banned)
                (PP (IN in)
                  (NP (JJ many) (NNS countries)))))
            (CC and)
            (VP (VBZ feels)
              (SBAR
                (S
                  (NP (PRP they))
                  (VP (MD may)
                    (VP (VB be)
                      (VP (VBN banned)
                        (PP (IN in)
                          (NP (DT this) (NN state)))
                        (ADVP (RB too))))))))))))
    (. .)))
</pre>

<p>and the collapsed dependency parse  </p>

<pre>
nsubj(cites-2, ABC-1)  
root(ROOT-0, cites-2)  
det(fact-4, the-3)   
dobj(cites-2, fact-4)  
mark(banned-9, that-5)  
nn(additives-7, chemical-6)  
nsubjpass(banned-9, additives-7)   
nsubj(feels-14, additives-7)   
auxpass(banned-9, are-8)   
ccomp(cites-2, banned-9)   
amod(countries-12, many-11)  
prep_in(banned-9, countries-12)   
ccomp(cites-2, feels-14)    
conj_and(banned-9, feels-14)    
nsubjpass(banned-18, they-15)   
aux(banned-18, may-16)    
auxpass(banned-18, be-17)    
ccomp(feels-14, banned-18)   
det(state-21, this-20)    
prep_in(banned-18, state-21)    
advmod(banned-18, too-22)   
</pre>
",Parsing & POS Tagging,clause extraction using stanford parser complex sentence need separate main dependent clause example sentence abc cite fact chemical additive banned many country feel may banned state split required think could use stanford parser tree dependency sure proceed tree root np nnp abc vp vbz cite np dt nn fact sbar np nn chemical nns additive vp vp vbp vp vbn banned pp np jj many nns country cc vp vbz feel sbar np prp vp md may vp vb vp vbn banned pp np dt nn state advp rb collapsed dependency parse nsubj cite abc root root cite det fact dobj cite fact mark banned nn additive chemical nsubjpass banned additive nsubj feel additive auxpass banned ccomp cite banned amod country many prep banned country ccomp cite feel conj banned feel nsubjpass banned aux banned may auxpass banned ccomp feel banned det state prep banned state advmod banned
Sentiment-ranked nodes in dependency parse with Stanford CoreNLP?,"<p>I'd like to perform a dependency parse on a group of sentences and look at the sentiment ratings of individual nodes, as in the Stanford Sentiment Treebank (<a href=""http://nlp.stanford.edu/sentiment/treebank.html"" rel=""nofollow"">http://nlp.stanford.edu/sentiment/treebank.html</a>). </p>

<p>I'm new to the CoreNLP API, and after fiddling around I still have no idea how I'd go about getting a dependency parse with ranked nodes. Is this even possible with CoreNLP, and if so, does anyone have experience doing it?</p>
",Parsing & POS Tagging,sentiment ranked node dependency parse stanford corenlp like perform dependency parse group sentence look sentiment rating individual node stanford sentiment treebank new corenlp api fiddling around still idea go getting dependency parse ranked node even possible corenlp doe anyone experience
How to convert a verb to its (derived) noun form?,"<p>I am working on a project related to NLP, in which I would like to identify main verb (I can do that with a dependency parser) from a sentence and then convert the verb to its noun form (or we can say noun derived from verb), for example <code>define to definition</code> or <code>sensitive to sensitivity</code> whenever possible. Are there any resources similar to wordnet or verbnet that provides this? </p>
",Parsing & POS Tagging,convert verb derived noun form working project related nlp would like identify main verb dependency parser sentence convert verb noun form say noun derived verb example whenever possible resource similar wordnet verbnet provides
Get k-best dependency parse trees,"<p>For a project I need a dependency parse tree of a sentence. This works quite well using Stanford NLP:</p>

<pre><code>import edu.stanford.nlp.pipeline.StanfordCoreNLP;
// ...
public void parse(String sentence) {
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, parse"");
    Annotation document = new Annotation(sentence);
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

    CoreMap firstSentence = sentences.get(0);
    SemanticGraph dependencies = firstSentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
    // ...
}
</code></pre>

<p>However I now also need other possible parse trees, i.e. the <strong>k-best parse trees</strong>. I can later pick out the tree that makes most sense. I have looked around but not found any code on the web that does this using a Java API.</p>

<p>Has anyone done something like this?</p>

<p>Cheers!</p>
",Parsing & POS Tagging,get k best dependency parse tree project need dependency parse tree sentence work quite well using stanford nlp however also need possible parse tree e k best parse tree later pick tree make sense looked around found code web doe using java api ha anyone done something like cheer
NLTK RegEx Chunker - Wildcard match any POS tag?,"<p>I'm using NLTK's RegexpParser to get phrases from POS-tagged words. For example: </p>

<pre><code>grammar = """"""
        FOUND:{&lt;NNP&gt;+&lt;CD&gt;+&lt;,&gt;+&lt;CD&gt;}
        ...
        """"""

pos_tagged_words = [('February', 'NNP'), ('14', 'CD'), (',', ','), ('1993', 'CD')]

result = nltk.RegexpParser(grammar).parse(pos_tagged_words)
</code></pre>

<p>Is there any way to match a wildcard tag? If this worked, I'd be looking for something like this:</p>

<pre><code>FOUND:{&lt;NNP&gt;?&lt;.&gt;*&lt;VBZ&gt;}
</code></pre>

<p>Where &lt;.> is the wildcard.</p>

<p>EDIT:<br>
Found a pretty bad way to do it that doesnt really include all characters. Would still appreciate a dedicated wildcard char.</p>

<pre><code>FOUND:{&lt;NNP&gt;?&lt;[A-Z]+|[:punct:]+&gt;*&lt;VBZ&gt;}
</code></pre>
",Parsing & POS Tagging,nltk regex chunker wildcard match po tag using nltk regexpparser get phrase po tagged word example way match wildcard tag worked looking something like wildcard edit found pretty bad way doesnt really include character would still appreciate dedicated wildcard char
Representing a Syntax Tree with QTreeview,"<p>I'm studying on my master thesis and I'm to implement an application finding deep syntactic relations (in future we hope to use these to generate semantic relations) for Turkish texts.</p>

<p>I've prepared an interface with PyQt for our application and we need a visualization module representing a syntax tree for each sentence in our corpus, in a specified format.</p>

<p>A hand drown example syntax tree with morphological features for each word is:</p>

<p><a href=""https://i.sstatic.net/Zhj7D.png"" rel=""nofollow"">https://i.sstatic.net/Zhj7D.png</a></p>

<p>Since already using Qt gui toolkit, I think, a quick way can be making use of Qtreeview for this purpose. But apperently, classic Qtreeview for filesystem needs some customizations.</p>

<p>Minimum expectations for tree items are:</p>

<ol>
<li>a label for each connection to a node (if not possible, this can be added in node icons)</li>
<li>an icon for each intermediate node and root node</li>
<li>(an icon + a morphological features box) for each leave node </li>
</ol>

<p>My question is how it can be implement such a representation with QTreeview. Especially for leave nodes, how can we generate a morphological features box? </p>

<p>If you have an idea guys, and give a small example, I'd be happy ^_^
Thanks for everyone read and think about this question...</p>
",Parsing & POS Tagging,representing syntax tree qtreeview studying master thesis implement application finding deep syntactic relation future hope use generate semantic relation turkish text prepared interface pyqt application need visualization module representing syntax tree sentence corpus specified format hand drown example syntax tree morphological feature word since already using qt gui toolkit think quick way making use qtreeview purpose apperently classic qtreeview filesystem need customizations minimum expectation tree item label connection node possible added node icon icon intermediate node root node icon morphological feature box leave node question implement representation qtreeview especially leave node generate morphological feature box idea guy give small example happy thanks everyone read think question
How to take the suffix in smoothing of Part of speech tagging,"<p>I am making a ""Part of speech Tagger"". I am handling the unknown word with the suffix.</p>

<p>But the main issue is that how would i decide the number of suffix... should it be pre-decided (like Weischedel approach) or I have to take the last few alphabets of the words(like Samuelsson approach).</p>

<p>Which approach would be better......</p>
",Parsing & POS Tagging,take suffix smoothing part speech tagging making part speech tagger handling unknown word suffix main issue would decide number suffix pre decided like weischedel approach take last alphabet word like samuelsson approach approach would better
Extracting Noun Phrases with OpenNLP from Labelled Words,"<p>I'm using OpenNLP to extract noun phrases from a chunk of text. Unfortunately, the documentation for OpenNLP is extremely confusing.</p>

<p>At the moment,I have two arrays: one with the tokenized text and another with the POS tags for the tokenized text. I fed these two strings into the chunker function, but the chunker just labels the words in the text as O, B-PP, B-NP, I-NP, etc.</p>

<p>What I'd like to do is have an array of strings that contains only the noun phrases from the text, rather than an array of strings that labels the tokenized text as different phrases. Is there already some sort of function in OpenNLP that can return the noun phrases in an array of strings (or even in a data structure other than an array)?</p>

<p>This looked like a related post, but I don't think we're doing the same thing, as they're using the parsing tree to accomplish their goals.
<a href=""https://stackoverflow.com/questions/14708047/how-to-extract-the-noun-phrases-using-open-nlps-chunking-parser"">How to extract the noun phrases using Open nlp&#39;s chunking parser</a></p>

<p>Any help would be greatly appreciated. Thanks in advance!</p>
",Parsing & POS Tagging,extracting noun phrase opennlp labelled word using opennlp extract noun phrase chunk text unfortunately documentation opennlp extremely confusing moment two array one tokenized text another po tag tokenized text fed two string chunker function chunker label word text b pp b np np etc like array string contains noun phrase text rather array string label tokenized text different phrase already sort function opennlp return noun phrase array string even data structure array looked like related post think thing using parsing tree accomplish goal href extract noun phrase using open nlp chunking parser help would greatly appreciated thanks advance
NLP framework for .NET,"<p>I have found references to SharpNLP (a port of the Java-based OpenNLP), and Antelope by Proxem.  I'm looking to create a full parse tree of a sentence (part-of-speech tagging), along with name-finding for dates/times and locations.</p>

<p>The SharpNLP library appears to be inactive since 2007, and it appears that the Antelope library was last updated in 2009.  The former is LGPL, the latter appears to require a commercial license but the installer self-describes the license as ""underspecified"".  </p>

<p>Is there a modern NLP library for .NET?  Is there a better platform choice for NLP?  (it's more important to me to have a great NLP implementation than to stick with a platform choice).</p>
",Parsing & POS Tagging,nlp framework net found reference sharpnlp port java based opennlp antelope proxem looking create full parse tree sentence part speech tagging along name finding date time location sharpnlp library appears inactive since appears antelope library wa last updated former lgpl latter appears require commercial license installer self describes license underspecified modern nlp library net better platform choice nlp important great nlp implementation stick platform choice
"Are there open source deep parsers for English which take &lt;tokens, POS tags&gt; as input and produce the parse tree?","<p>I'm wondering if there are open source probabilistic deep parsers for English which take as input a sequence of tokens and their corresponding parts of speech (POS tags) as input, and produce the parse tree as results. The parsers I am aware of take only token sequences as input, and produce as output the POS tags as well as the parse tree. In my case, I have a specific tokenizer and corresponding (hacked) POS tagger with Penn tagset already, and want to generate only the parse tree based on these tags and the corresponding tokens.</p>
",Parsing & POS Tagging,open source deep parser english take token po tag input produce parse tree wondering open source probabilistic deep parser english take input sequence token corresponding part speech po tag input produce parse tree result parser aware take token sequence input produce output po tag well parse tree case specific tokenizer corresponding hacked po tagger penn tagset already want generate parse tree based tag corresponding token
Can collins parser give me the dependency relations?,"<p>I would like to obtain dependency tree by <a href=""http://www.cs.columbia.edu/~mcollins/code.html"" rel=""nofollow"">Collins parser</a>. I am wondering if such a thing exist.</p>

<p>I know that there are many other parsers which can provide me the dependency tree, but I am using a dataset which uses collins parser to parse the text. So, I would like to get the dependency tree by the same parser.</p>

<p>But, unfortunately, I can't see anything about how to get dependency relations in Collins parser's readme file.</p>

<p>I would be thankful if anybody helps me.</p>

<p>Thanks in advance,</p>
",Parsing & POS Tagging,collins parser give dependency relation would like obtain dependency tree collins parser wondering thing exist know many parser provide dependency tree using dataset us collins parser parse text would like get dependency tree parser unfortunately see anything get dependency relation collins parser readme file would thankful anybody help thanks advance
Quick NLTK parse into syntax tree,"<p>I am trying to parse several hundreds of sentences into their syntax trees and i need to do that fast, the problem is that if i use NLTK then i need to define a grammar, and i cant know that i only know its gonna be english. I tried using <a href=""https://github.com/emilmont/pyStatParser"" rel=""noreferrer"">this</a> statistical parser, and it works great for my purposes however the speed could be a lot better, is there a way to use nltk parsing without a grammar?
In this snippet i am using a processing pool to do the processing in ""parallel"" but the speed leaves a lot to be desired.</p>

<pre><code>import pickle
import re
from stat_parser.parser import Parser
from multiprocessing import Pool
import HTMLParser
def multy(a):
    global parser
    lst=re.findall('(\S.+?[.!?])(?=\s+|$)',a[1])
    if len(lst)==0:
        lst.append(a[1])
    try:
        ssd=parser.norm_parse(lst[0])
    except:
        ssd=['NNP','nothing']
    with open('/var/www/html/internal','a') as f:
        f.write(""[[ss"")
        pickle.dump([a[0],ssd], f)
        f.write(""ss]]"")
if __name__ == '__main__':
    parser=Parser()
    with open('/var/www/html/interface') as f:
        data=f.read()
    data=data.split(""\n"")
    p = Pool(len(data))
    Totalis_dict=dict()
    listed=list()
    h = HTMLParser.HTMLParser()
    with open('/var/www/html/internal','w') as f:
        f.write("""")
    for ind,each in enumerate(data):
        listed.append([str(ind),h.unescape(re.sub('[^\x00-\x7F]+','',each))])
    p.map(multy,listed)
</code></pre>
",Parsing & POS Tagging,quick nltk parse syntax tree trying parse several hundred sentence syntax tree need fast problem use nltk need define grammar cant know know gon na english tried using statistical parser work great purpose however speed could lot better way use nltk parsing without grammar snippet using processing pool processing parallel speed leaf lot desired
NLP creating a model out of POS tags,"<p>I am trying to create a knowledgebase based on text mining. I am using Genia Corpus to tag the words by their Parts of speech. Given two terms from the text, how do i create a model that finds out its relation?</p>

<p>Eg Text:</p>

<p>HIF1A gene is involved in Hypoxic regulation. Hypoxia also up regulates BRCA1 gene expression which is mainly associated in breast cancer. </p>

<p>I have the POS tagged out. </p>

<pre><code>Word     Base Form  Part-Of-Speech   
HIF1A    HIF1A          NN  
gene     gene           NN  
is           be         VBZ 
involved     involve    VBN 
in           in         IN  
Hypoxic  Hypoxic    JJ  
regulation   regulation NN  
.            .          .   
Hypoxia  Hypoxia        NN  
also     also           RB  
regulates    regulate   VBZ 
BRCA1    BRCA1          NN  
gene     gene           NN  
which    which          WDT 
is           be         VBZ 
mainly   mainly         RB  
associated   associate  VBN 
in           in         IN  
breast   breast         NN  
cancer   cancer         NN
</code></pre>

<p>I am writing a web interface that when queried BRCA1 and Hypoxia should tell that there is positive regulation between them. when queried HIF1A and Hypoxia it should tell that there is a positive regulation based on these sentences. </p>

<p>Now that i have the POS tagged I dont know how to proceed in creating a model that would come up with identifying the relation between them. This is just an example. I want to do it for general biomedical terms and texts.</p>

<p>Anyone any suggestions?</p>
",Parsing & POS Tagging,nlp creating model po tag trying create knowledgebase based text mining using genia corpus tag word part speech given two term text create model find relation eg text hif gene involved hypoxic regulation hypoxia also regulates brca gene expression mainly associated breast cancer po tagged writing web interface queried brca hypoxia tell positive regulation queried hif hypoxia tell positive regulation based sentence po tagged dont know proceed creating model would come identifying relation example want general biomedical term text anyone suggestion
Changing Stanford Dependency Parser POS tags Label,"<p>I'm using Stanford.NLP.NET installed as IKVM nugget in my current C# project. From which I'm extracting PoS tags from dependency tree. But for some reasons I want to aggregate various types of noun, adjective, verb and adverb tags labels.</p>

<p>For example,</p>

<p><strong>""n"" label for all noun types</strong></p>

<p>NN  Noun, singular or mass</p>

<p>NNS     Noun, plural</p>

<p>NNP     Proper noun, singular</p>

<p>NNPS    Proper noun, plural</p>

<p><strong>""a"" label for all adjective types</strong></p>

<p>JJ  Adjective</p>

<p>JJR     Adjective, comparative</p>

<p>JJS     Adjective, superlative</p>

<p><strong>""r"" label for all adverb types</strong></p>

<p>RB  Adverb</p>

<p>RBR     Adverb, comparative</p>

<p>RBS     Adverb, superlative</p>

<p><strong>""v"" label for all verb types</strong> </p>

<p>VBD     Verb, past tense</p>

<p>VBG     Verb, gerund or present participle</p>

<p>VBN     Verb, past participle</p>

<p>VBP     Verb, non-3rd person singular present</p>

<p>VBZ     Verb, 3rd person singular present</p>

<p>Where and what change should I make?</p>
",Parsing & POS Tagging,changing stanford dependency parser po tag label using stanford nlp net installed ikvm nugget current c project extracting po tag dependency tree reason want aggregate various type noun adjective verb adverb tag label example n label noun type nn noun singular mass nns noun plural nnp proper noun singular nnps proper noun plural label adjective type jj adjective jjr adjective comparative jjs adjective superlative r label adverb type rb adverb rbr adverb comparative rb adverb superlative v label verb type vbd verb past tense vbg verb gerund present participle vbn verb past participle vbp verb non rd person singular present vbz verb rd person singular present change make
Finding adverbs and what they modify using Stanford Parser,"<p>I am working with the Stanford Parser and NLP software for the first time. </p>

<p>In my project, I would like to find all adverbs (such as ""not"" and ""very"") and determine they modify in a sentence.</p>

<p>For example, for the sentence ""The book is not interesting"", I can find the vertex for ""not"" in the parse tree. I would then like to find that ""not"" is modifying ""interesting"". Can the parser (or other Stanford corenlp software) provide that information?</p>
",Parsing & POS Tagging,finding adverb modify using stanford parser working stanford parser nlp software first time project would like find adverb determine modify sentence example sentence book interesting find vertex parse tree would like find modifying interesting parser stanford corenlp software provide information
Uses/Applications of Part-of-speech-tagging (POS Tagging),"<p>I understand the implicit value of part-of-speech tagging and have seen mentions about its use in parsing, text-to-speech conversion, etc.</p>

<p>Could you tell me how is the output of a PoS tagger formated ?
Also, could you explain how is such an output used by other tasks/parts of an NLP system?</p>
",Parsing & POS Tagging,us application part speech tagging po tagging understand implicit value part speech tagging seen mention use parsing text speech conversion etc could tell output po tagger formated also could explain output used task part nlp system
Horizontal and Vertical Markovization,"<p>I have a sentence along with the grammar in a tree form. I need to train a Probabilistic Context Free Grammar from it so that I can give the best possible parse for it.
I am using Viterbi CKY algorithm to get the best parse.
The sentences are in the following tree format:
(TOP (S (NP (DT The) (NN flight)) (VP (MD should) (VP (VB be) (NP (NP (CD eleven) (RB a.m)) (NP (NN tomorrow)))))) (PUNC .))</p>

<p>I have built a system which from the ATIS section of the Penn Treebank has learnt a probabilistic grammar and now can give a possible parse output for the above sentence.</p>

<p>I read about Horizontal and Vertical Markovization techniques which can help increase the accuracy by using annotations.
I am a little confused as to how they work.
Can someone guide me to some explanatory examples or illustrate how they work and how they effect the accuracy.</p>
",Parsing & POS Tagging,horizontal vertical markovization sentence along grammar tree form need train probabilistic context free grammar give best possible parse using viterbi cky algorithm get best parse sentence following tree format top np dt nn flight vp md vp vb np np cd eleven rb np nn tomorrow punc built system atis section penn treebank ha learnt probabilistic grammar give possible parse output sentence read horizontal vertical markovization technique help increase accuracy using annotation little confused work someone guide explanatory example illustrate work effect accuracy
Converting NLTK phrase structure trees to BRAT .ann standoff,"<p>I'm trying to annotate a corpus of plain text. I'm working with systemic functional grammar, which is fairly standard in terms of part-of-speech annotation, but differs in terms of phrases/chunks.</p>

<p>Accordingly, I've POS tagged my data with NLTK defaults, and made a regex chunker with <code>nltk.RegexpParser</code>. Basically, the output now is an NLTK-style phrase structure tree:</p>

<blockquote>
  <p>Tree('S', [Tree('Clause', [Tree('Process-dependencies',
  [Tree('Participant', [('This', 'DT')]), Tree('Verbal-group', [('is',
  'VBZ')]), Tree('Participant', [('a', 'DT'), ('representation',
  'NN')]), Tree('Circumstance', [('of', 'IN'), ('the', 'DT'),
  ('grammar', 'NN')])])]), ('.', '.')])</p>
</blockquote>

<p>There is some stuff I want to manually annotate on top of this, however: the systemic grammar breaks down participants and verbal groups into sub-types that probably can't be automatically annotated. So, I was hoping to convert the parse tree format into something an annotation tool (preferably BRAT) could handle, and then go through the text and specify the sub-types manually, as in (one possible solution):</p>

<p><img src=""https://i.sstatic.net/09EzH.png"" alt=""BRAT annotation""></p>

<p>Perhaps the solution would be sort of tricking BRAT into treating the phrase structure like dependencies? I could modify the chunking regex if need be. Are there any converters out there? (Brat provides ways of converting from CONLL2000 and Stanford Core NLP, so if I could get the phrase structure into either of those forms it would be acceptable too.)</p>

<p>Thanks!</p>
",Parsing & POS Tagging,converting nltk phrase structure tree brat ann standoff trying annotate corpus plain text working systemic functional grammar fairly standard term part speech annotation differs term phrase chunk accordingly po tagged data nltk default made regex chunker basically output nltk style phrase structure tree tree tree clause tree process dependency tree participant dt tree verbal group vbz tree participant dt representation nn tree circumstance dt grammar nn stuff want manually annotate top however systemic grammar break participant verbal group sub type probably automatically annotated wa hoping convert parse tree format something annotation tool preferably brat could handle go text specify sub type manually one possible solution perhaps solution would sort tricking brat treating phrase structure like dependency could modify chunking regex need converter brat provides way converting conll stanford core nlp could get phrase structure either form would acceptable thanks
How start with UIMA and simple NLP tasks?,"<p>I've recently found out about UIMA (<a href=""http://uima.apache.org/"" rel=""nofollow"">http://uima.apache.org/</a>). It looks promising for simple NLP tasks, such as tokenizing, sentence splitting, part-of-speech tagging etc.</p>

<p>I've managed to get my hands on an already configured minimal java sample that is using OpenNLP components for its pipeline.</p>

<p>The code looks like this:</p>

<pre><code>public void ApplyPipeline() throws IOException, InvalidXMLException,
        ResourceInitializationException, AnalysisEngineProcessException {

    XMLInputSource in = new XMLInputSource(
            ""opennlp/OpenNlpTextAnalyzer.xml"");
    ResourceSpecifier specifier = UIMAFramework.getXMLParser()
            .parseResourceSpecifier(in);

    AnalysisEngine ae = UIMAFramework.produceAnalysisEngine(specifier);

    JCas jcas = ae.newJCas();
    jcas.setDocumentText(""This is my text."");

    ae.process(jcas);
    this.doSomethingWithResults(jcas);

    jcas.reset();
    ae.destroy();
}

private void doSomethingWithResults(JCas jcas) {
    AnnotationIndex&lt;Annotation&gt; idx = jcas.getAnnotationIndex();
    FSIterator&lt;Annotation&gt; it = idx.iterator();

    while (it.hasNext()) {
        System.out.println(it.next().toString());
    }

}
</code></pre>

<p>Excerpt from OpenNlpTextAnalyzer.xml:</p>

<pre><code>&lt;delegateAnalysisEngine key=""SentenceDetector""&gt;
    &lt;import location=""SentenceDetector.xml"" /&gt;
&lt;/delegateAnalysisEngine&gt;
&lt;delegateAnalysisEngine key=""Tokenizer""&gt;
    &lt;import location=""Tokenizer.xml"" /&gt;
&lt;/delegateAnalysisEngine&gt;
</code></pre>

<p>The java code produces output like this:</p>

<pre><code>Token
   sofa: _InitialView
   begin: 426
   end: 435
   pos: ""NNP""
</code></pre>

<p>I'm trying to get the same information from each Annotation object that the toString() method uses. I've already looked into UIMA's source code to understand where the values are coming from. My attempts to retrieve them sort of works, but they aren't smart in any way.</p>

<p>I'm struggling to find easy examples that, extract information out of the JCas objects.</p>

<p>I'm looking for a way to get for instance all Annotations produces by my PosTagger or by the SentenceSplitter for further usage. </p>

<p>I guess</p>

<pre><code>List&lt;Feature&gt; feats = it.next().getType().getFeatures();
</code></pre>

<p>is a start to get values, but due to UIMA owns classes for primitive types, even the source code  of the  toString method in the annotation class reads like a slap in the face.</p>

<p>Where do I find java code that uses basic UIMA stuff and where are good tutorials (except javadoc from the framework itself)?</p>
",Parsing & POS Tagging,start uima simple nlp task recently found uima look promising simple nlp task tokenizing sentence splitting part speech tagging etc managed get hand already configured minimal java sample using opennlp component pipeline code look like excerpt opennlptextanalyzer xml java code produce output like trying get information annotation object tostring method us already looked uima source code understand value coming attempt retrieve sort work smart way struggling find easy example extract information jcas object looking way get instance annotation produce postagger sentencesplitter usage guess start get value due uima owns class primitive type even source code tostring method annotation class read like slap face find java code us basic uima stuff good tutorial except javadoc framework
algorithm to get topic / focus of sentence out of words in sentence,"<p>Are there any well-know or successful algorithms for obtaining the topic and / or focus of a sentence ( question ) out of the words in the sentence question?</p>

<p>If not, how would I got about getting the topic / focus of the question. It seems that the topic / focus of the questions is usually a noun or a noun-phrase.</p>

<p>So the first thing I would do is determine the nouns by Part Of Speech tagging the question. but then how do I know if I should get just the nouns or the noun(s) and a adjective before it, or the noun and the adverb before it, or the noun(s) and verb?</p>

<p>For example:</p>

<p>In ' did the quick brown fox jump over the lazy dog ', get ' quick brown fox ', ' jump ', and ' lazy dog '.</p>

<p>In ' what is the population of japan ', get ' population ' and ' japan '</p>

<p>In ' what color is milk ' get ' color ' and ' milk '</p>

<p>In ' What is the height of Mt. Everest ' get ' Mt. Everst ' and ' Height '.</p>

<p><strong>While writing these I guess the easiest way is removing stop words.</strong></p>
",Parsing & POS Tagging,algorithm get topic focus sentence word sentence well know successful algorithm obtaining topic focus sentence question word sentence question would got getting topic focus question seems topic focus question usually noun noun phrase first thing would determine noun part speech tagging question know get noun noun adjective noun adverb noun verb example quick brown fox jump lazy dog get quick brown fox jump lazy dog population japan get population japan color milk get color milk height mt everest get mt everst height writing guess easiest way removing stop word
Stanford CoreNLP ExhaustivePCFGParser Initialization Query,"<p>I am a NLP student and relatively new to Stanford CoreNLP. I wanted to try PCFG parser for my sentence because I wanted to get the probability score for the best parse tree the parser fetches. I was able to successfully use LexicalizedParser as </p>

<pre><code>    LexicalizedParser myParser = LexicalizedParser.loadModel();
    myParser.parse(tokenLabels); // where List&lt;CoreLabel&gt; tokenLabels
</code></pre>

<p>But I don't see any loadModel method for ExhaustivePCFGParser so am confused as to how to initialize and start using parse function for it. I can see the constructor defined for it but have no idea what the parameters means ? Could you please provide some guidance for this.</p>
",Parsing & POS Tagging,stanford corenlp exhaustivepcfgparser initialization query nlp student relatively new stanford corenlp wanted try pcfg parser sentence wanted get probability score best parse tree parser fetch wa able successfully use lexicalizedparser see loadmodel method exhaustivepcfgparser confused initialize start using parse function see constructor defined idea parameter mean could please provide guidance
Using Stanford Parser(CoreNLP) to find phrase heads,"<p>I am going to use Stanford Corenlp 2013 to find phrase heads. I saw <a href=""https://stackoverflow.com/questions/10768038/is-there-any-phrase-head-finder"">this thread</a>.</p>

<p>But, the answer was not clear to me and I couldn't add any comment to continue that thread. So, I'm sorry for duplication.</p>

<p>What I have at the moment is the parse tree of a sentence (using Stanford Corenlp) (I also tried with CONLL format which is created by Stanford Corenlp). And what I need is exactly the head of noun phrases.</p>

<p>I don't know how I can use dependencies and the parse tree to extract heads of nounphrases.
What I know is that if I have <code>nsubj (x, y)</code>, y is the head of the subject. If I have <code>dobj(x,y)</code>, y is the head of the direct object. f I have <code>iobj(x,y)</code>, y is the head of the indirect object.</p>

<p>However, I am not sure if this way is the correct way to find all phrase heads. If it is, which rules I should add to get all heads of noun phrases?</p>

<p>Maybe, it is worth saying that I need the heads of noun phrases in a java code.</p>
",Parsing & POS Tagging,using stanford parser corenlp find phrase head going use stanford corenlp find phrase head saw href thread answer wa clear add comment continue thread sorry duplication moment parse tree sentence using stanford corenlp also tried conll format created stanford corenlp need exactly head noun phrase know use dependency parse tree extract head nounphrases know head subject head direct object f head indirect object however sure way correct way find phrase head rule add get head noun phrase maybe worth saying need head noun phrase java code
Is there any algorithm or solution for term identification?,"<p>I want to extract terminological units from a corpus of specialized documents. Is there any algorithm or out-of-box solution for this? Can nltk do this?</p>

<p>It seems this thread addressed my question.
<a href=""https://stackoverflow.com/questions/11320776/extracting-terms-with-contextual-relevance-noun-phrases-from-text-in-a-net-pr"">Extracting terms with contextual relevance (noun phrases) from text in a .NET project</a></p>
",Parsing & POS Tagging,algorithm solution term identification want extract terminological unit corpus specialized document algorithm box solution nltk seems thread addressed question href term contextual relevance noun phrase text net project
How to get a node level with Stanford dependency parser,"<p>I would like to know if there is any method that allows to give the node level of the parse given by Stanford dependency parser. I haven't found the method that gives the node level. Thanks for the help.</p>
",Parsing & POS Tagging,get node level stanford dependency parser would like know method allows give node level parse given stanford dependency parser found method give node level thanks help
python NLTK POS tagger not behaving as expected,"<p>I ran pos_tag function on below text,it shows output with battery as 'RB'. As battery is noun, it should show as 'NN'.</p>

<pre><code>nltk.pos_tag(nltk.word_tokenize('Camera picture quality was fair but speed was an issue and also battery life was not that good'))
</code></pre>

<p>Output:</p>

<blockquote>
  <p>[('Camera', 'NNP'), ('picture', 'NN'), ('quality', 'NN'), ('was',
  'VBD'), ('fair', 'JJ'), ('but', 'CC'), ('speed', 'NN'), ('was',
  'VBD'), ('an', 'DT'), ('issue', 'NN'), ('and', 'CC'), ('also', 'RB'),
  ('battery', 'RB'), ('life', 'NN'), ('was', 'VBD'), ('not', 'RB'),
  ('that', 'IN'), ('good', 'JJ')]</p>
</blockquote>

<p>While if I POS tagged the same statement by this tagger <a href=""http://cst.dk/online/pos_tagger/uk/"" rel=""nofollow"">http://cst.dk/online/pos_tagger/uk/</a> , it shows battery as 'NN' and gives following output:</p>

<blockquote>
  <p>Camera/NNP picture/NN quality/NN was/VBD fair/JJ but/CC speed/NN
  was/VBD an/DT issue/NN and/CC also/RB battery/NN life/NN was/VBD
  not/RB that/IN good/JJ</p>
</blockquote>

<p><strong>Edit</strong>:</p>

<p>With statement as :</p>

<blockquote>
  <p>""Camera picture quality was fair but speed was an issue <strong>but</strong>
  battery life was not that good""</p>
</blockquote>

<p>the NLTK tagger gives following output:</p>

<blockquote>
  <p>[('Camera', 'NNP'), ('picture', 'NN'), ('quality', 'NN'), ('was',
  'VBD'), ('fair', 'JJ'), ('but', 'CC'), ('speed', 'NN'), ('was',
  'VBD'), ('an', 'DT'), ('issue', 'NN'), ('but', 'CC'), ('battery',
  'NN'), ('life', 'NN'), ('was', 'VBD'), ('not', 'RB'), ('that', 'IN'),
  ('good', 'JJ')]</p>
</blockquote>

<p>Please explain!</p>
",Parsing & POS Tagging,python nltk po tagger behaving expected ran po tag function text show output battery rb battery noun show nn output camera nnp picture nn quality nn wa vbd fair jj cc speed nn wa vbd dt issue nn cc also rb battery rb life nn wa vbd rb good jj po tagged statement tagger show battery nn give following output camera nnp picture nn quality nn wa vbd fair jj cc speed nn wa vbd dt issue nn cc also rb battery nn life nn wa vbd rb good jj edit statement camera picture quality wa fair speed wa issue battery life wa good nltk tagger give following output camera nnp picture nn quality nn wa vbd fair jj cc speed nn wa vbd dt issue nn cc battery nn life nn wa vbd rb good jj please explain
Extract noun phrases containing a specific word using Stanford Parser,"<p>How can I extract the noun phrase containing a specific word using the Stanford Parser. I can extract the Noun phrases using the code written in this post: </p>

<p><a href=""https://stackoverflow.com/questions/21652202/get-noun-phrase-of-subject-in-sentence-stanford-parser"">https://stackoverflow.com/questions/21652202/get-noun-phrase-of-subject-in-sentence-stanford-parser</a></p>

<p>However, I need to get the noun phrase that contains a specific word, However this is not as simple as doing a string search because that word can appear twice in the sentence. So I need to extract the noun phrase that contains the specific word at a specific order of the sentence. So suppose I have the sentence:</p>

<pre><code>  String some_sentence = ""The dog ran after the intruding bigger dog""; 
</code></pre>

<p>dog appears twice, the first time as the second word in the sentence, and the second time as the last word in the sentence. How can I extract the noun phrases that contain the first occurrence of dog? </p>
",Parsing & POS Tagging,extract noun phrase containing specific word using stanford parser extract noun phrase containing specific word using stanford parser extract noun phrase using code written post href p however need get noun phrase contains specific word however simple string search word appear twice sentence need extract noun phrase contains specific word specific order sentence suppose sentence dog appears twice first time second word sentence second time last word sentence extract noun phrase contain first occurrence dog
"How to do part-of-speech tagging of texts, containing mathematical expressions?","<p>The goal is a syntactic parsing of scientific texts. And first I need to make part-of-speech tagging of sentences of such texts. Texts are from arxiv.org. So they are originally in LaTeX. When extracting text from LaTeX documents, math expressions can be converted into MathML (or maybe some other format, but I prefer MathML cause this work is being done to create a specific web-app, and MathML is a convenient tool for this).</p>

<p>The only idea I have is to substitute mathematical expressions with some phrases of natural language and then use some implemented algorithm for pos-tagging. So the question is how to implement this substitutions or, in general, how to implement pos-tagging of texts with mathematics in them?</p>
",Parsing & POS Tagging,part speech tagging text containing mathematical expression goal syntactic parsing scientific text first need make part speech tagging sentence text text arxiv org originally latex extracting text latex document math expression converted mathml maybe format prefer mathml cause work done create specific web app mathml convenient tool idea substitute mathematical expression phrase natural language use implemented algorithm po tagging question implement substitution general implement po tagging text mathematics
Maxent POS tag table,"<p>I use <code>nltk.pos_tag</code> for part-of-speech tagging which use <code>maxent part of speech tagger</code>. I need a table of all available tags.</p>

<p>My ultimate aim is to extract just the adverbs and adjectives from a text.</p>

<p>Any help is appreciated.</p>

<p>Thanks</p>
",Parsing & POS Tagging,maxent po tag table use part speech tagging use need table available tag ultimate aim extract adverb adjective text help appreciated thanks
What is a good Java library for Parts-Of-Speech tagging?,"<p>I'm looking for a good open source <a href=""http://en.wikipedia.org/wiki/Part-of-speech_tagging"" rel=""noreferrer"">POS Tagger</a> in Java. Here's what I have come up with so far.</p>

<ul>
<li><a href=""http://alias-i.com/lingpipe/"" rel=""noreferrer"">LingPipe</a></li>
<li><a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""noreferrer"">Stanford</a></li>
<li><a href=""http://l2r.cs.uiuc.edu/~cogcomp/asoftware.php?skey=FLBJPOS"" rel=""noreferrer"">LBJ</a></li>
<li><a href=""http://www.markwatson.com/opensource/"" rel=""noreferrer"">FastTag</a></li>
</ul>

<p>Anybody got any recommendations?</p>
",Parsing & POS Tagging,good java library part speech tagging looking good open source po tagger java come far lingpipe stanford lbj fasttag anybody got recommendation
python parsing syntax tree in nlp,"<p>I wonder whether there is a convenient module for working with <a href=""http://en.wikipedia.org/wiki/Parse_tree"" rel=""nofollow"">parse tree</a> in python</p>

<p>for example, this is a part of the parse tree</p>

<pre><code>(ROOT
          (S
          (NP
          (NP (NN sent1534))
          (: :)
          (NP (NNP Lancaster))
          (, ,)
          (NP
          (NP (DT a) (NN farming) (NN town))
          (PP (IN of)
          (NP (CD 3))))
          (, ,)
          (NP (CD 400))
</code></pre>

<p>I am interested in extracting the path on the tree from one specific word to another specific word, in addition it would be great two know whether two words share the same NP, VP, etc.</p>

<p>If you know any good module for parsing parse tree, please let me know.</p>
",Parsing & POS Tagging,python parsing syntax tree nlp wonder whether convenient module working parse tree python example part parse tree interested extracting path tree one specific word another specific word addition would great two know whether two word share np vp etc know good module parsing parse tree please let know
Lucene searching using payload and NLP tags,"<p>I have already indexed the documents with each word having  payload that contains the Part of speech (POS) tag.
I want to search only those documents for which the search query words have that  POS tag.
E.g. 'access google' has google as Noun. It should show only docs with google as noun.
Can writing a custom analyser help?
How can i access the Term when Payload is being accessed in Similarity class?</p>
",Parsing & POS Tagging,lucene searching using payload nlp tag already indexed document word payload contains part speech po tag want search document search query word po tag e g access google ha google noun show doc google noun writing custom analyser help access term payload accessed similarity class
stanford nlp tool pos tagger property file arch parameter,"<p>I'm using the left3words model provided by stanford nlp tool. in the props file, the arch parameter has indicated some directories which i am unable to locate. can anyone help on this? thanks alot.</p>

<p>arch = left3words,naacl2003unknowns,wordshapes(-1,1),distsim(/u/nlp/data/pos_tags_are_useless/egw4-reut.512.clusters,-1,1),distsimconjunction(/u/nlp/data/pos_tags_are_useless/egw4-reut.512.clusters,-1,1)</p>
",Parsing & POS Tagging,stanford nlp tool po tagger property file arch parameter using left word model provided stanford nlp tool prop file arch parameter ha indicated directory unable locate anyone help thanks alot arch left word naacl unknown wordshapes distsim u nlp data po tag useless egw reut cluster distsimconjunction u nlp data po tag useless egw reut cluster
Models for classify Noun Phrase?,"<p>I need a model for the following tasks:</p>

<p>a sequence of words, with its POS tags. I want to judge whether this sequence of words is a Noun Phrase or not.</p>

<p>One model I can think of is HMM.</p>

<p>For those sequences which are noun phrase, we train a HMM (HMM+). For those are not noun phrase, we try an HMM(HMM-). And when we do prediction for a sequence, we can calculate P(sequence| HMM+) and P(sequence|HMM-). If the former is larger, we think this phrase is a noun phrase, otherwise it's not.</p>

<p>What do you think of it? and do you have any other models suited for this question?  </p>
",Parsing & POS Tagging,model classify noun phrase need model following task sequence word po tag want judge whether sequence word noun phrase one model think hmm sequence noun phrase train hmm hmm noun phrase try hmm hmm prediction sequence calculate p sequence hmm p sequence hmm former larger think phrase noun phrase otherwise think model suited question
POS tagging in Scala,"<p>I tried to POS tag a sentence in Scala using Stanford parser like below</p>

<pre><code>val lp:LexicalizedParser = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"");
lp.setOptionFlags(""-maxLength"", ""50"", ""-retainTmpSubcategories"")
val s = ""I love to play""
val parse :Tree =  lp.apply(s)
val taggedWords = parse.taggedYield()
println(taggedWords)
</code></pre>

<p>I got an error <strong>type mismatch; found : java.lang.String required: java.util.List[_ &lt;: edu.stanford.nlp.ling.HasWord]</strong> in the line <strong>val parse :Tree =  lp.apply(s)</strong></p>

<p>I don't know whether this is the right way of doing it or not. Are there any other easy ways of POS tagging a sentence in Scala?</p>
",Parsing & POS Tagging,po tagging scala tried po tag sentence scala using stanford parser like got error type mismatch found java lang string required java util list edu stanford nlp ling hasword line val parse tree lp apply know whether right way easy way po tagging sentence scala
"extracting nouns,noun phrases,adjectives verbs from text file corpus using visual c#","<p>i am doing a project wherein i have to extract nouns adjectives noun phrases and verbs from text files(.doc) format.
i have a corpus of around 75 such files. i have accessed net to find about it and i came across POS tagging in python using nltk.
as my project is in c# (using visual studio 2008) i need a code to do so.
i have tried wordnet api for the same and even sharpnlp but as i am a newbie i found these tough to integrate with my project.
can anybody please suggest me simpler code to do so using something like vocabulary etc. plz help me guys.
thanx.</p>
",Parsing & POS Tagging,extracting noun noun phrase adjective verb text file corpus using visual c project wherein extract noun adjective noun phrase verb text file doc format corpus around file accessed net find came across po tagging python using nltk project c using visual studio need code tried wordnet api even sharpnlp newbie found tough integrate project anybody please suggest simpler code using something like vocabulary etc plz help guy thanx
Incorrect tagging of verb in NLTK,"<p>I used the a tagger with the treebank corpus to POS tag a sentence using NLTK. The sentence is ""The cat ate the dog"". For some reason, it tags ""ate"" as NN while it should be a verb. Any clues?</p>

<pre><code>from nltk.tag import pos_tag  
from nltk.tokenize import word_tokenize 
pos_tag(word_tokenize(""The cat ate the dog""))
</code></pre>

<p><strong>[out]:</strong>
    <code>[('The', 'DT'), ('cat', 'NN'), ('ate', 'NN'), ('the', 'DT'), ('dog', 'NN')]</code></p>
",Parsing & POS Tagging,incorrect tagging verb nltk used tagger treebank corpus po tag sentence using nltk sentence cat ate dog reason tag ate nn verb clue
Extract the noun words &amp; original sentence from POS Tag,"<p>I want to extract the nouns from the sentence and get back the original sentence from the POS Tag  </p>

<pre><code> //Extract the words before _NNP &amp; _NN from below  and also how to get back the original sentence from the Pos TAG. 
 Original Sentence:Hi. How are you? This is Mike·
 POSTag: Hi._NNP How_WRB are_VBP you?_JJ This_DT is_VBZ Mike._NN
</code></pre>

<p>I tried something like this </p>

<pre><code>    String txt = ""Hi._NNP How_WRB are_VBP you?_JJ This_DT is_VBZ Mike._NN"";


    String re1 = ""((?:[a-z][a-z0-9_]*))"";   // Variable Name 1
    String re2 = "".*?""; // Non-greedy match on filler
    String re3 = ""(_)""; // Any Single Character 1
    String re4 = ""(NNP)"";   // Word 1

    Pattern p = Pattern.compile(re1 + re2 + re3 + re4, Pattern.CASE_INSENSITIVE | Pattern.DOTALL);
    Matcher m = p.matcher(txt);
    if (m.find()) {
        String var1 = m.group(1);
        System.out.print(  var1.toString()  );
    }
}
</code></pre>

<p>output: Hi 
But I need a list of all the nouns in the sentence.</p>
",Parsing & POS Tagging,extract noun word original sentence po tag want extract noun sentence get back original sentence po tag tried something like output hi need list noun sentence
How to generate multiple parse trees for an ambiguous sentence in NLTK?,"<p>I have the following code in Python.</p>

<pre><code>sent = [(""very"",""ADJ""),(""colourful"",""ADJ""),(""ice"",""NN""),(""cream"",""NN""),(""van"",""NN"")] 
patterns= r""""""
  NP:{&lt;ADJ&gt;*&lt;NN&gt;+}  

""""""
NPChunker=nltk.RegexpParser(patterns) # create chunk parser
for s in NPChunker.nbest_parse(sent):
    print s.draw()
</code></pre>

<p>The output is:</p>

<pre><code>(S (NP very/ADJ colourful/ADJ ice/NN cream/NN van/NN))
</code></pre>

<p>But the output should have another 2 parse trees.</p>

<pre><code>(S (NP very/ADJ colourful/ADJ ice/NN) (NP cream/NN) (NP van/NN))
(S (NP very/ADJ colourful/ADJ ice/NN cream/NN) van/NN)
</code></pre>

<p>The problem is that only the first regular expression is taken by the RegexpParser. How can I generate all possible parse trees at once?</p>
",Parsing & POS Tagging,generate multiple parse tree ambiguous sentence nltk following code python output output another parse tree problem first regular expression taken regexpparser generate possible parse tree
Is there a reasonably accurate heuristic for detecting the subject and object of an English sentence?,"<p>I realize that flawlessly separating subject noun phrases and object noun phrases from a sentence is an open research problem and is not easily explained here, but is there a smart way of doing this (assuming I already have a POS-tagged sentence) which works for most sentences, or at least relatively simple ones? I know that simply assuming the first noun phrase is the subject is a pretty good approximation, but in sentences beginning with prepositional phrases (e.g., ""Across the clearing and through the stream ran the frightened deer.""), this fails. Ideally, I'd like something which also recognizes the subject in this case.</p>

<p>For reference, that example sentence gives the following parse tree with the Stanford Parser:</p>

<p><code>[ROOT [S [PP [IN Across] [NP [NP [DT the] [NN clearing] ] [CC and] [NP [IN through] ] ] ] [NP [DT the] [NN stream] ] [VP [VBD ran] [NP [DT the] [ADJP [JJ frightened] ] [NNS deer] ] ] [. .] ] ]</code></p>

<p>My current strategy is as follows:</p>

<ol>
<li><p>Subject: Do a BFS on the tree, looking for the first NP.</p></li>
<li><p>Verb: Do a BFS on the tree, looking for the first VP. On this subtree, do a BFS looking for a VB(D|G|N|P|Z).</p></li>
<li><p>Object: Do a BFS on the VP subtree found above, looking for a NP.</p></li>
</ol>

<p>This strategy results in the following for my example:
<code>SUBJECT: (NP (DT the) (NN stream) ) , VERB: (VBD ran) , OBJECT: (NP (DT the) (ADJP (JJ frightened) ) (NNS deer) )</code>.</p>

<p>If possible, I'd like to modify my strategy to not fail on these cases, and eventually even more complicated cases.</p>
",Parsing & POS Tagging,reasonably accurate heuristic detecting subject object english sentence realize flawlessly separating subject noun phrase object noun phrase sentence open research problem easily explained smart way assuming already po tagged sentence work sentence least relatively simple one know simply assuming first noun phrase subject pretty good approximation sentence beginning prepositional phrase e g across clearing stream ran frightened deer fails ideally like something also recognizes subject case reference example sentence give following parse tree stanford parser current strategy follows subject bfs tree looking first np verb bfs tree looking first vp subtree bfs looking vb g n p z object bfs vp subtree found looking np strategy result following example possible like modify strategy fail case eventually even complicated case
Mixing words and PoS tags in NLTK parser grammars,"<p>I've been playing with NLTK for awhile already and am at the point to define custom parser grammar for special chunking. I am following the description in <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html"" rel=""nofollow"">http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html</a> but what I am interested to do is slightly different than what is described in the chapter. For instance in example 7.10 instead using the following for the verb phase:
VP: {&lt;VB.*&gt;&lt;NP|PP|CLAUSE&gt;+$}
I would like to  just match sentences that use one particular verb and not any verb. Something like:
VP: {go&lt;NP|PP|CLAUSE&gt;+$}</p>

<p>In other words I would like to match the actual word and not the PoS tag for the word and mix and match actual words and PoS tags in the regular expression.</p>

<p>Is this possible?</p>
",Parsing & POS Tagging,mixing word po tag nltk parser grammar playing nltk awhile already point define custom parser grammar special chunking following description interested slightly different described chapter instance example instead using following verb phase vp vb np pp clause would like match sentence use one particular verb verb something like vp go np pp clause word would like match actual word po tag word mix match actual word po tag regular expression possible
Why there is a difference in parse tree output generated from api and GUI provided in stanfordNLP,"<p>I am using 'stanford-corenlp-full-2013-06-20' api to generate parse tree as given below</p>

<pre><code>private String text= ""Heart attack causes reduced lifespan average"";
Annotation annotation = new Annotation(text);
coreNLP.annotate(annotation);
List&lt;CoreMap&gt; sentences = annotation.get(SentencesAnnotation.class);
for (CoreMap sentence : sentences) {
    Tree tree = sentence.get(TreeAnnotation.class);
    tree.pennPrint();
}
</code></pre>

<p>It is showing sub sentence 'S' as shown below </p>

<pre><code>(ROOT (**S** (NP (NNP Heart) (NN attack))
             (VP (VBZ causes)
                 (**S** (NP (VBN reduced) (NN lifespan) (NN average))))))
</code></pre>

<p>But When I try to parse the same sentence using the GUI provided by 'stanford-parser-full-2013-06-20' it is giving a different tree (It seems right one) as given below</p>

<pre><code>(ROOT (**S** (NP (NNP Heart) (NN attack))
             (VP (VBZ causes)
                 (VP (VBN reduced) (NP (NN lifespan) (NN average))))))
</code></pre>

<p>Can some one point out why they both are showing two different outputs though they both belong to same version.</p>
",Parsing & POS Tagging,difference parse tree output generated api gui provided stanfordnlp using stanford corenlp full api generate parse tree given showing sub sentence shown try parse sentence using gui provided stanford parser full giving different tree seems right one given one point showing two different output though belong version
Any FSM/FSA Based Tagger,"<p>There are several good taggers around. I even asked a question creating own <a href=""https://stackoverflow.com/questions/15921417/creating-own-pos-tagger"">tagger</a>,
I have got another requirement now. In Python I was using <a href=""https://pypi.python.org/pypi/topia.termextract/"" rel=""nofollow noreferrer"">topia</a> and it seemed a great choice for job (fast and concise). But there is no such alternative in Java,I could find.<br>
Now, I have three questions related to this : </p>

<p>1)Is there any term extractor/pos tagger in Java which is based on FSM?</p>

<p>2) Is FSM tagger ""CAN BE"" more efficient (I know it is way faster, but accuracy) than corpus based taggers?</p>

<p>3) How Do I start building One in Java? Any basic guide creating machine extracting pos tags from sentence :- ""Einstein was a great scientist."" ? Just a start ?</p>
",Parsing & POS Tagging,fsm fsa based tagger several good tagger around even asked question creating topia seemed great choice job fast concise alternative java could find three question related term extractor po tagger java based fsm fsm tagger efficient know way faster accuracy corpus based tagger start building one java basic guide creating machine extracting po tag sentence einstein wa great scientist start
Regular expressions in POS tagged NLTK corpus,"<p>I'm loading a POS-tagged corpus in NLTK, and I would like to find certain patterns involving POS tags. These patterns can be quite complex, including a lot of different combinations of POS tags.
Example input string: </p>

<blockquote>
  <p>We/PRP spent/VBD some/DT time/NN reading/NN about/IN the/DT
  historical/JJ importance/NN of/IN tea/NN <strong>in/IN Korea/NNP  and/CC
  China/NNP</strong> and/CC then/RB tasted/VBD the/DT  most/JJS expensive/JJ
  green/JJ tea/NN I/PRP have/VBP ever/RB seen/VBN ./.</p>
</blockquote>

<p>In this case the POS pattern is something like: <code>(IN) (THE)? (NNP) (CC)? (NNP)</code> ...</p>

<p>I'm loading my corpus with:</p>

<pre><code> reader = TaggedCorpusReader(corpus_dir, r'.*\.pos')
</code></pre>

<p>Clearly, I can do this using Python's <code>re</code> package, but such regular expressions quickly become hard to understand, debug, and update for other developers.</p>

<p>What is the most elegant way of doing this in NLTK? Are there helper functions to find patterns in POS-tagged text more readable than usual regex?</p>

<p>Thanks</p>
",Parsing & POS Tagging,regular expression po tagged nltk corpus loading po tagged corpus nltk would like find certain pattern involving po tag pattern quite complex including lot different combination po tag example input string prp spent vbd dt time nn reading nn dt historical jj importance nn tea nn korea nnp cc china nnp cc rb tasted vbd dt jjs expensive jj green jj tea nn prp vbp ever rb seen vbn case po pattern something like loading corpus clearly using python package regular expression quickly become hard understand debug update developer elegant way nltk helper function find pattern po tagged text readable usual regex thanks
getting text from the class &quot;Parse&quot; opennlp,"<p>I have a parse tree, which is an object of the class <code>Parse</code> class in opennlp. I want to print the text from the parse tree. For example consider the sentence: <code>Wikipedia is a collaboratively edited, multilingual, free Internet encyclopedia supported by the non-profit Wikimedia Foundation.</code>. I have identified the noun phrases in the sentence: </p>

<pre><code> (NP (NNP Wikipedia))
 (NP (NP (DT a) (RB collaboratively) (JJ edited,) (JJ multilingual,) (JJ free) (NNP    Internet) (NN encyclopedia)) (VP (VBN supported) (PP (IN by) (NP (DT the) (JJ non-profit) (NNP Wikimedia) (NNP Foundation.)))))
 (NP (DT a) (RB collaboratively) (JJ edited,) (JJ multilingual,) (JJ free) (NNP Internet)  (NN encyclopedia))
 (NP (DT the) (JJ non-profit) (NNP Wikimedia) (NNP Foundation.))
</code></pre>

<p>I want to output the list <code>[""Wikipedia"", ""collaboratively edited..."", ""non profit wikimedia foundation""]</code>. The <code>getText</code> method in <code>Parse</code> returns the whole sentence, instead of just the string associated with the parse tree. Is there a way to do that directly in OpenNLP?    </p>
",Parsing & POS Tagging,getting text class parse opennlp parse tree object class class opennlp want print text parse tree example consider sentence identified noun phrase sentence want output list method return whole sentence instead string associated parse tree way directly opennlp
Different xsubj dependency output from CoreNLP and stanford dependency parser,"<p>I am using latest CoreNLP 1.3.5 . For the example sentence of xsubj dependency ""Tom likes to eat fish"", I am not getting a dependency xsubj(eat, Tom). Although I get a dependency xcomp(like, eat).</p>

<p>But I can see the dependency xsubj in the latest stanford dependecy parser 2.0.5 output for same sentence.
I tried copying some classes for parser, semgraph and Trees into the coreNLP jar from stanford parser jar. But still I could not get this outut.
Can anybody please guide me , how can I determine this relation.</p>
",Parsing & POS Tagging,different xsubj dependency output corenlp stanford dependency parser using latest corenlp example sentence xsubj dependency tom like eat fish getting dependency xsubj eat tom although get dependency xcomp like eat see dependency xsubj latest stanford dependecy parser output sentence tried copying class parser semgraph tree corenlp jar stanford parser jar still could get outut anybody please guide determine relation
Parser tags for OpenNLP,"<p>Is there any documentation about the meaning of parser tags in OpenNLP? I know that the POS tag types follows the TreeBank convention, but unfortunately I haven't found any information about the parser tags, such as ""SBAR"", etc.</p>

<p>Does this documentation exist somewhere or I have to figure it out myself?</p>
",Parsing & POS Tagging,parser tag opennlp documentation meaning parser tag opennlp know po tag type follows treebank convention unfortunately found information parser tag sbar etc doe documentation exist somewhere figure
replacing pronoun with its antecedent using python2.7 and nltk,"<p>As the title shows I am trying to look for pronouns in a string and replace it with it's antecedent  like: </p>

<pre><code>[in]: ""the princess looked from the palace, she was happy"".
[out]: ""the princess looked from the palace, the princess was happy"". 
</code></pre>

<p>I use pos tag to return pronouns  and nouns. I need to know  how to replace without knowing the sentence, meaning how to specify the subject in the sentence to replace the pronoun with it. Any suggestions?</p>
",Parsing & POS Tagging,replacing pronoun antecedent using python nltk title show trying look pronoun string replace antecedent like use po tag return pronoun noun need know replace without knowing sentence meaning specify subject sentence replace pronoun suggestion
Discriminatively trained supervised part of speech tagging,"<p>I have to implement a Discriminatively trained supervised part of speech tagger, and I have been looking at a couple of techniques including Maximum likelihood, perceptron and the large margin (SVM). Finally after reading through some experimental results quoted in a couple of research papers i have come down to using SVMs for it. I have been studying it for some time and a couple of things in theory seem a little confusing.
Can someone please point me to some relevant reading material to a practical implementation or just more clarification on how to implement it using Viterbi Algorithm.</p>

<p>P.S. : I am not asking for the solution, but just need some guidance.</p>
",Parsing & POS Tagging,discriminatively trained supervised part speech tagging implement discriminatively trained supervised part speech tagger looking couple technique including maximum likelihood perceptron large margin svm finally reading experimental result quoted couple research paper come using svms studying time couple thing theory seem little confusing someone please point relevant reading material practical implementation clarification implement using viterbi algorithm p asking solution need guidance
Does NLTK parts of speech tagger use global information or just the word that is being tagged?,"<p>I am currently doing some parts of speech tagging using NLTK's ""nltk.pos_tag"". I was wondering does NLTK's tagger use information beyond the word that is currently being tagged to determine the POS of the word?</p>

<p>If not does NLTK have a tagger that would do this?</p>

<p>Thanks in advance for any info!</p>
",Parsing & POS Tagging,doe nltk part speech tagger use global information word tagged currently part speech tagging using nltk nltk po tag wa wondering doe nltk tagger use information beyond word currently tagged determine po word doe nltk tagger would thanks advance info
Java CFG parser that supports ambiguities,"<p>I'm looking for a CFG parser implemented with Java. The thing is I'm trying to parse a natural language. And I need all possible parse trees (ambiguity) not only one of them. I already researched many NLP parsers such as Stanford parser. But they mostly require statistical data (a treebank which I don't have) and it is rather difficult and poorly documented to adapt them in to a new language.
I found some parser generators such as ANTRL or JFlex but I'm not sure that they can handle ambiguities.  So which parser generator or java library is best for me?
Thanks in advance</p>
",Parsing & POS Tagging,java cfg parser support ambiguity looking cfg parser implemented java thing trying parse natural language need possible parse tree ambiguity one already researched many nlp parser stanford parser mostly require statistical data treebank rather difficult poorly documented adapt new language found parser generator antrl jflex sure handle ambiguity parser generator java library best thanks advance
Calculate frequency of function words,"<p>I would like to calculate the frequency of <a href=""http://en.wikipedia.org/wiki/Function_word"" rel=""nofollow"">function words</a> in Python/NLTK. I see two ways to go about it : </p>

<ul>
<li>Use Part-Of-Speech tagger and sum up on POS tags which constitute to function words</li>
<li>Create a list of function words and perform a simple look up</li>
</ul>

<p>The catch in the first case is that, my data is noisy and I don't know(for sure) which POS tags constitute as function words. The catch in the second case is I don't have a list and since my data is noisy the lookup won't be accurate. </p>

<p>I would prefer the first to the second or any other example which would throw me more accurate results.</p>
",Parsing & POS Tagging,calculate frequency function word would like calculate frequency function word python nltk see two way go use part speech tagger sum po tag constitute function word create list function word perform simple look catch first case data noisy know sure po tag constitute function word catch second case list since data noisy lookup accurate would prefer first second example would throw accurate result
checking if a sentence is grammatically correct using stanford parser,"<p>Is there any method to check if a sentence is grammatically correct or not using stanford parser? As of now am able to get the parse tree of a sentence using stanford parser. I got stuck here and don't know how to proceed further.</p>
",Parsing & POS Tagging,checking sentence grammatically correct using stanford parser method check sentence grammatically correct using stanford parser able get parse tree sentence using stanford parser got stuck know proceed
Using context to improve part-of-speech tagging,"<p>Are there some common or recommended techniques for using the context of word to improve the accuracy of part-of-speech tagging?</p>

<p>For example if I had the sentence:</p>

<blockquote>
  <p>I played golf on a links.</p>
</blockquote>

<p>The word ""links"" could be either singular (a golf course) or plural. I tried this sentence in several grammar checkers and they all correctly recognized the sentence as valid.</p>

<p>The problem is they also thought that this sentence was valid:</p>

<blockquote>
  <p>I clicked on a links.</p>
</blockquote>

<p>Is there a good way to use the context (clicked vs played golf) to infer the correct part-of-speech?</p>

<p>Thanks!</p>
",Parsing & POS Tagging,using context improve part speech tagging common recommended technique using context word improve accuracy part speech tagging example sentence played golf link word link could either singular golf course plural tried sentence several grammar checker correctly recognized sentence valid problem also thought sentence wa valid clicked link good way use context clicked v played golf infer correct part speech thanks
CFG using POS tags in NLTK,"<p>I am trying to check if a given sentence is <em>grammatical</em> using NLTK.</p>

<p>Ex: </p>

<blockquote>
  <p>OK :   The whale licks the sadness</p>
  
  <p>NOT OK :   The best I ever had</p>
</blockquote>

<p>I know that I could do POS tagging, then use a CFG parser and check that way, but I have yet to find a CFG that uses POS tags instead of actual words as terminal branches. </p>

<p>Is there a CFG that anyone can recommend? I think that making my own is silly, because I am not a linguist and will probably leave out important structures.</p>

<p>Also, my application is such that the system would ideally reject many sentences and only approve sentences it is extremely sure of.</p>

<p>Thanks :D</p>
",Parsing & POS Tagging,cfg using po tag nltk trying check given sentence grammatical using nltk ex ok whale lick sadness ok best ever know could po tagging use cfg parser check way yet find cfg us po tag instead actual word terminal branch cfg anyone recommend think making silly linguist probably leave important structure also application system would ideally reject many sentence approve sentence extremely sure thanks
stanford parser or an alternative to it in dotnet,"<p>i'm working on a Q&amp;A project in the dotnet framework and i need something that do the NLP processes like , Part of speech tagging , and generating parse trees </p>

<p>i know Stanford parser but am a little bit confused about finding an interface to it in C# 
i searched for a good alternative and i found : </p>

<p>1- <a href=""http://nlpdotnet.com/Services/Introduction.aspx"" rel=""nofollow"">http://nlpdotnet.com/Services/Introduction.aspx</a>   (but it doesn't provide parse trees )</p>

<p>i wonder if there are some good alternatives as well </p>
",Parsing & POS Tagging,stanford parser alternative dotnet working q project dotnet framework need something nlp process like part speech tagging generating parse tree know stanford parser little bit confused finding interface c searched good alternative found provide parse tree wonder good alternative well
Extracting noun phrases from a text file using stanford typed parser,"<p>I have a text which I want to extract the noun phrases from it. I can easily get the typed parser for the text that i have, but wondering how i can extract the noun phrases in the text ?</p>
",Parsing & POS Tagging,extracting noun phrase text file using stanford typed parser text want extract noun phrase easily get typed parser text wondering extract noun phrase text
How to Extract the Noun Phrases using Open NLP,"<p>I am new to Open NLP and need some help in extracting noun phrases using it. I have generated a tree structure which contains the pos tags with the text. But from the tree structure I am not able to extract the noun phrases. Here is the code I am using:</p>

<pre><code>InputStream is = new FileInputStream(""en-parser-chunking.bin"");
ParserModel model = new ParserModel(is);
Parser parser = ParserFactory.create(model);
Parse topParses[] = ParserTool.parseLine(line, parser, 1);
    for (Parse p : topParses){
             p.show();} 
</code></pre>

<p>Here <code>p.show()</code> prints the tree structure and it does not return anything. How can I use <code>p.show()</code> to extract the noun phrases or is there any other way to get them from  the tree structure?</p>

<p>Please help me on this.</p>

<p>Thanks in Advance</p>

<p>Gouse.</p>
",Parsing & POS Tagging,extract noun phrase using open nlp new open nlp need help extracting noun phrase using generated tree structure contains po tag text tree structure able extract noun phrase code using print tree structure doe return anything use extract noun phrase way get tree structure please help thanks advance gouse
How does Facebook Graph Search work?,"<p>I'm guessing I'm on thin ice with regards to a question that can be answered instead of just discussed as from my research <a href=""https://www.facebook.com/about/graphsearch"" rel=""nofollow"">Facebook's Graph Search</a> seems to be in stealth mode with nothing much officially shared by Facebook, except <a href=""http://www.facebook.com/notes/facebook-engineering/under-the-hood-building-graph-search-beta/10151240856103920"" rel=""nofollow"">this.</a></p>

<p>I'm interested in how facebook are getting this to work. So I've been playing with OpenNLP and the POS Tagger (parts of speech). I've got that far, but then it became obvious how many nuances exist with natural language, kinda knew that but didn't respect it, until I hacked something together.</p>

<p>So I got this far - </p>

<pre><code>[Debug] Question :: Friends from France who like England
[Debug] Token =  Friends :: POS = NNS
[Debug] Token =  from :: POS = IN
[Debug] Token =  France :: POS = NNP
[Debug] Token =  who :: POS = WP
[Debug] Token =  like :: POS = VBP
[Debug] Token =  England :: POS = NNP
</code></pre>

<p>Where POS is part of speech as described <a href=""http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""nofollow"">here</a> </p>

<p>I guess from there I can take the POS tags and where I have a rule set look for similar terms in my schema? Rules seem a brittle approach. Perhaps I should have an ontology to have thing related to a friend in there some how? The problem seems ""easier"" for a triple store, where the semantics are already tagged e.g. ""James"",""likes"",""England"" and I have another triple where ""James"",""comes from"",""France"" it's essentially just a just a join in SQL.</p>

<p>This is purely academic, a little hack for the evenings. I'm just interested in how this works for them. This google <a href=""https://www.google.co.uk/search?client=ubuntu&amp;channel=fs&amp;q=Penn+English+Treebank+POS+tags&amp;ie=utf-8&amp;oe=utf-8&amp;gl=uk&amp;redir_esc=&amp;ei=fQMBUZ_UDqiQ0QWP_ID4DA#hl=en&amp;safe=off&amp;client=ubuntu&amp;hs=a7z&amp;tbo=d&amp;channel=fs&amp;gl=uk&amp;sclient=psy-ab&amp;q=natural+language+to+sql&amp;oq=natural+language+to+&amp;gs_l=serp.3.1.0l4j0i30l2j0i8i30l2j0i22l2.304621.308285.1.310750.20.14.0.0.0.0.1020.3678.1j9j1j0j1j1j0j1.14.0.les%3B..0.0...1c.1.mqFfKFhPqOY&amp;psj=1&amp;bav=on.2,or.r_gc.r_pw.r_cp.r_qf.&amp;bvm=bv.41248874,d.d2k&amp;fp=399618ef21ce39da&amp;biw=1024&amp;bih=656"" rel=""nofollow"">search</a> throws up some things of interest.</p>

<p>So specific questions are, how does graph search work? How is natural language mapped to what ever query/ datastore they use? Other than Wolfram Alpha are there any other good examples of this type of thing? Is there anything open source? What computer science concepts are used here that I should go and read up on?</p>

<p>Cheers</p>

<p>David   </p>
",Parsing & POS Tagging,doe facebook graph search work guessing thin ice regard question answered instead discussed research facebook graph search seems stealth mode nothing much officially shared facebook except interested facebook getting work playing opennlp po tagger part speech got far became obvious many nuance exist natural language kinda knew respect hacked something together got far po part speech described guess take po tag rule set look similar term schema rule seem brittle approach perhaps ontology thing related friend problem seems easier triple store semantics already tagged e g james like england another triple james come france essentially join sql purely academic little hack evening interested work google search throw thing interest specific question doe graph search work natural language mapped ever query datastore use wolfram alpha good example type thing anything open source computer science concept used go read cheer david
How to get PoS tag of individual words in a sentence?,"<p>I have been using <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">Stanford POS Tagger</a> to tag parts of speech in a sentence. What I have learnt from the documentation and other questions on the topic on stackoverflow is that following code gives POS Tagging of a sentence </p>

<pre><code>            MaxentTagger tagger = new MaxentTagger(""./left3words-wsj-0-18.tagger"");
                     String tags=tagger.tagString(Summary);
</code></pre>

<p>but what I want is to get the POS Tag of individual words. One possible way of doing this is to break up the input string and give input to the Tagger word by word but it does not seem elegant and I think that it increases the chances of errors. <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/tagger/maxent/MaxentTagger.html#apply%28java.util.List%29"" rel=""nofollow"">Javadoc</a> says that apply function is available that returns the ArrayList of TaggedWords but I have not been able to call this function successfully.
Any idea how can I get the POS tagging of individual words of a string using Stanford POS 
Tagger either by apply function or some other function??</p>
",Parsing & POS Tagging,get po tag individual word sentence using stanford po tagger tag part speech sentence learnt documentation question topic stackoverflow following code give po tagging sentence want get po tag individual word one possible way break input string give input tagger word word doe seem elegant think increase chance error javadoc say apply function available return arraylist taggedwords able call function successfully idea get po tagging individual word string using stanford po tagger either apply function function
Understanding structured perceptron for POS tagging,"<p>I have some trouble understanding exactly how one should implement the structured perceptron for part-of-speech tagging. Could you please confirm or correct my thoughts, and/or fill in any gaps missing?</p>

<p>So, basically the structured perceptron is a variant of the multiclass perceptron, except for how you implement collecting the best score. A first-order Markov assumption is made, saying that the current sequence index only depends on the previous index. The input is an entire sequence of words, instead of just one word as would be in a non-structured case, as well as a vector of all possible labels (y). The function f(x,y) returns a guessed label sequence for the given word sequence.</p>

<p>In a multiclass perceptron, getting the best score is easily done through iteration since we only deal with classifying one label to one instance. The problem with classifying entire sequences is that it results in an exponential growth of the number of possible labelings. This is where the Viterbi algorithm is needed, which recursively finds the best path using two feature sets; one for determining how likely a given POS tag is to a certain word, and one for determining how likely a certain POS tag is coming directly after another POS tag. The score from each of these feature sets are multiplied with a unique weight for each state. If the chosen path is wrong, each weight in the states of the wrong path are punished, and the weights in the correct path are awarded.</p>

<p>This is about how far I have (hopefully) understood. My biggest questions right now is how the features are structured (is the previous tag sequence a part of the features?), and how to actually implement the Viterbi algorithm. Also, is there an implementation of a POS tagger using structured perceptron anywhere I could analyze (preferably in Java)? </p>

<p>I would be very grateful if you could give me some hints! </p>
",Parsing & POS Tagging,understanding structured perceptron po tagging trouble understanding exactly one implement structured perceptron part speech tagging could please confirm correct thought fill gap missing basically structured perceptron variant multiclass perceptron except implement collecting best score first order markov assumption made saying current sequence index depends previous index input entire sequence word instead one word would non structured case well vector possible label function f x return guessed label sequence given word sequence multiclass perceptron getting best score easily done iteration since deal classifying one label one instance problem classifying entire sequence result exponential growth number possible labelings viterbi algorithm needed recursively find best path using two feature set one determining likely given po tag certain word one determining likely certain po tag coming directly another po tag score feature set multiplied unique weight state chosen path wrong weight state wrong path punished weight correct path awarded far hopefully understood biggest question right feature structured previous tag sequence part feature actually implement viterbi algorithm also implementation po tagger using structured perceptron anywhere could analyze preferably java would grateful could give hint
Specifying the frequencies of nouns in text using java,"<p>I have a file contains the words attached with POS tags such as </p>

<pre><code>Tom/NNP went/VBP to/IN the/DT stadium/NN ....etc
</code></pre>

<p>I need to know the frequency of all nouns contains in this file. So, the output might be </p>

<pre><code>stadium     12 
football    20
player      13
</code></pre>

<p>where these numbers are the times of numbers these nouns occur in the text. How can I do this in java? </p>
",Parsing & POS Tagging,specifying frequency noun text using java file contains word attached po tag need know frequency noun contains file output might number time number noun occur text java
NLP - subject of sentence,"<p>I am trying to get the main subject of a sentence, i.e what a sentence is talking about (not the grammatical subject which may be different).</p>

<p>So far I have got </p>

<p>1.) OpenNLP in Java which is giving me sentence detection, POS tagging, parsing, tokenizer and Name Finder.</p>

<p>2.) MatlParser,stanford Parser - which can give the grammatical subject of a simple sentence by dependency parsing.</p>

<p>I think a noun or a noun phrase will always be subject in more general sense,but a sentence can have many nouns and noun phrases.
Any help would be much appreciated.</p>
",Parsing & POS Tagging,nlp subject sentence trying get main subject sentence e sentence talking grammatical subject may different far got opennlp java giving sentence detection po tagging parsing tokenizer name finder matlparser stanford parser give grammatical subject simple sentence dependency parsing think noun noun phrase always subject general sense sentence many noun noun phrase help would much appreciated
extracting the text from output parse Tree,"<p>I am new to nlp, I am trying to use stanford parser to extract the (NP ) sentence from a text,  I want to retrieve the parts of the text where it's tagged (NP )</p>

<p>if a part is tagged (NP ) and a smaller part inside it is also tagged (NP ) I want to take the smaller part.</p>

<p>till now I managed to do what I wanted in the following method:</p>

<pre><code>private static ArrayList&lt;Tree&gt; extract(Tree t) 
{
    ArrayList&lt;Tree&gt; wanted = new ArrayList&lt;Tree&gt;();
   if (t.label().value().equals(""NP"") )
    {
       wanted.add(t);
        for (Tree child : t.children())
        {
            ArrayList&lt;Tree&gt; temp = new ArrayList&lt;Tree&gt;();
            temp=extract(child);
            if(temp.size()&gt;0)
            {
                int o=-1;
                o=wanted.indexOf(t);
                if(o!=-1)
                    wanted.remove(o);
            }
            wanted.addAll(temp);
        }
    }

    else
        for (Tree child : t.children())
            wanted.addAll(extract(child));
    return wanted;
}
</code></pre>

<p>The return type of this method is a list of trees, When I do the following:</p>

<pre><code>     LexicalizedParser parser = LexicalizedParser.loadModel();
        x = parser.apply(""Who owns club barcelona?"");
     outs=extract(x);
    for(int i=0;i&lt;outs.size();i++){System.out.println(""tree #""+i+"": ""+outs.get(i));}
</code></pre>

<p>is :</p>

<pre><code>tree #0: (NP (NN club) (NN barcelona))
</code></pre>

<p>I want the output to be <code>""club barcelona""</code> right away, without the tags, I tried the <code>.labels();</code> property and <code>.label().value();</code> they return the tags instead</p>
",Parsing & POS Tagging,extracting text output parse tree new nlp trying use stanford parser extract np sentence text want retrieve part text tagged np part tagged np smaller part inside also tagged np want take smaller part till managed wanted following method return type method list tree following want output right away without tag tried property return tag instead
Combining nn tags to form noun phrases using Java,"<p>How to combine all nn tags to a phrase tag using Java </p>

<pre><code>nsubj(martyrdom-4, Today-1)
cop(martyrdom-4, is-2)
det(martyrdom-4, the-3)
root(ROOT-0, martyrdom-4)
nn(Mukherjee-7, Dr-6)
prep_of(martyrdom-4, Mukherjee-7)
det(founder-9, the-8)
dep(tribute-17, founder-9)

prep_of(founder-9, Jan-11)
nn(body-15, Sangh-12)
nn(body-15, BJP-13)
nn(body-15, parent-14)
dep(tribute-17, body-15)
poss(tribute-17, My-16)
dep(martyrdom-4, tribute-17)
prep_to(tribute-17, him-19)
</code></pre>

<p>I want to get a noun phrase:</p>

<pre><code>prep_of(founder-9,Jan-11)
nn(body-15, Sangh-12)
nn(body-15, BJP-13)
nn(body-15, parent-14)
</code></pre>

<p>The output should be----------> jan sangh BJP parent</p>
",Parsing & POS Tagging,combining nn tag form noun phrase using java combine nn tag phrase tag using java want get noun phrase output jan sangh bjp parent
NLP text annotation storage and access,"<p>I have a large corpus of text (10 million sentences or so) which I'd like to preprocess with various NLP tools (POS tagger, Syntax parser, Dependency Parser, etc). I need to store the various annotation layers created by these tools somehow, and access them on-the-fly from within my Java code (perhaps by providing the start and end index of the text span in the corpus, and the type of annotation).</p>

<p>Does a software system already exist to store and access these annotations quickly? If not, what would be the best way to store and access these annotations? Speed of access would be most important.</p>
",Parsing & POS Tagging,nlp text annotation storage access large corpus text million sentence like preprocess various nlp tool po tagger syntax parser dependency parser etc need store various annotation layer created tool somehow access fly within java code perhaps providing start end index text span corpus type annotation doe software system already exist store access annotation quickly would best way store access annotation speed access would important
How to combine different NLP features for machine learning?,"<p>I'm trying to do some KNN learning using different NLP features.
For example, I want to use bag-of-words and local POS tags.</p>

<p>Separately, I have some idea of how to calculate similarity with a single feature. Like using cosine similarity with counts (for bag-of-words vectors), or using perhaps Hamming distance for POS tags.</p>

<p>However, I don't know how to combine the two. How do people in this area normally do this?
Could anyone help me with that?</p>

<p>Thanks in advance. </p>
",Parsing & POS Tagging,combine different nlp feature machine learning trying knn learning using different nlp feature example want use bag word local po tag separately idea calculate similarity single feature like using cosine similarity count bag word vector using perhaps hamming distance po tag however know combine two people area normally could anyone help thanks advance
Change a synonym lemma according to a POS Tag value,"<p>I need to replace a a word with its synonym in a sentence.
For that, I POS Tag each word in the sentence and for the word I wish to replace I find the best matching synonym Wordnet synset.
Now, what I am missing is the ability to take the synonym lemma and change it according to the original word POS Tag value.
I was wondering whether anyone knows of a library in C++ or Python that I can take a lemma and the POS Tag value and change the lemma according to the input POS Tag...
For instance, the sentence:
""The grand jurry commented""</p>

<p>The word 'commented' was pos tagged as 'VBD' (past tense)
I can take the synonym: 'remark' and need to change it to 'remarked' -> as 'VBD' is a past tense of the verb</p>
",Parsing & POS Tagging,change synonym lemma according po tag value need replace word synonym sentence po tag word sentence word wish replace find best matching synonym wordnet synset missing ability take synonym lemma change according original word po tag value wa wondering whether anyone know library c python take lemma po tag value change lemma according input po tag instance sentence grand jurry commented word commented wa po tagged vbd past tense take synonym remark need change remarked vbd past tense verb
Lowest Common Ancestor of multiple nodes in a n-ary tree,"<p>I am trying to implement LCA of multiple nodes in an n-ary tree in java. I am working with parse trees of sentences, So its reasonable to assume that number of children of a node &lt;= 6. Multiple nodes here are two phrases(continuous word sequence) in a sentence. Let k be the number of nodes involved.</p>

<p>One way is to find the LCA of two nodes for k/2 pairs and we will get k/2 nodes. Now recurse on these k/2 nodes. The order will be O(nlog k), where O(n) is the complexity of linear LCA finding algorithms. Can I do it more efficiently ?</p>
",Parsing & POS Tagging,lowest common ancestor multiple node n ary tree trying implement lca multiple node n ary tree java working parse tree sentence reasonable assume number child node multiple node two phrase continuous word sequence sentence let k number node involved one way find lca two node k pair get k node recurse k node order nlog k n complexity linear lca finding algorithm efficiently
JAVA Separate the a sentence contains Part of Speech(POS) tag into free of POS tag sentence and just POS tag sentence?,"<p>Assume the sentences is:</p>

<blockquote>
  <p>It/pps urged/vbd that/cs the/at next/ap Legislature/nn-tl <code>/</code> provide/vb enabling/vbg funds/nns and/cc re-set/vb the/at effective/jj date/nn so/cs that/cs an/at orderly/jj implementation/nn of/in the/at law/nn may/md be/be effected/vbn ''/'' ./.</p>
</blockquote>

<p>The sentence above is take from brown corpus. How can I get the sentence free of all these POS tag and print and another sentence is just POS tag.</p>

<p>The sentence free of POS tag result like as below:</p>

<blockquote>
  <p>It urged that the next Legislature `` provide enabling funds and re-set the effective date so that an orderly implementation of the law may be effected '' .</p>
</blockquote>

<p>The sentence just POS tag result like as below:</p>

<blockquote>
  <p>pps vbd cs at ap nn-tl `` vb vbg nns cc vb at jj nn cs cs at jj nn in at nn md be vbn '' .</p>
</blockquote>
",Parsing & POS Tagging,java separate sentence contains part speech po tag free po tag sentence po tag sentence assume sentence pps urged vbd c next ap legislature nn tl provide vb enabling vbg fund nns cc set vb effective jj date nn c c orderly jj implementation nn law nn may md effected vbn sentence take brown corpus get sentence free po tag print another sentence po tag sentence free po tag result like urged next legislature provide enabling fund set effective date orderly implementation law may effected sentence po tag result like pps vbd c ap nn tl vb vbg nns cc vb jj nn c c jj nn nn md vbn
How to get relationship between noun phrases in a sentence?,"<p>I have this pattern to get cause-effect relationship between noun phrases in a sentence:</p>

<pre><code>&lt;NP I&gt; * have * effect/impact on/in &lt;NP II&gt;
</code></pre>

<p>NP is Noun Phrase.</p>

<p>If I have a sentence:</p>

<pre><code>Technology can have negative impact on social interactions
</code></pre>

<p>then based on above pattern, <strong>NP I</strong> match with <strong>Technology</strong> and <strong>NP II</strong> match with <strong>social interactions</strong></p>

<p>The question: what is appropriate algorithm to get NP I and NP II?</p>

<p>Thanks</p>
",Parsing & POS Tagging,get relationship noun phrase sentence pattern get cause effect relationship noun phrase sentence np noun phrase sentence based pattern np match technology np ii match social interaction question appropriate algorithm get np np ii thanks
Render linguistic syntax tree in browser,"<p>The input is <strong>either:</strong></p>

<p>(1) a bracketed representation of a tree with labeled internal nodes such as:</p>

<pre><code>(S (N John) (VP (V hit) (NP (D the) (N ball))))
</code></pre>

<p>with output:</p>

<p><img src=""https://i.sstatic.net/ZD8S2.jpg"" alt=""enter image description here""></p>

<p>(Whether the lines are dashed and whether the caption is present are not significant.)</p>

<p><strong>Or the input could be:</strong></p>

<p>(2) a bracketing on words without labels e.g.:</p>

<pre><code>((John) ((hit) ((the) (ball))))
</code></pre>

<p>with output same as above (no internal labels this time, just the tree structure).</p>

<p>Another component of the input is whether the tree is labeled as in (1) or unlabeled as in (2).</p>

<hr>

<p><strong>My question:</strong> What is the best way (fastest development time) to render these trees in the browser in javascript? Everything should happen on the client side.</p>

<p>I'm imagining a simple interface with just a textbox (and a radio button specifying whether it is a labeled tree or not), that, when changed, triggers a tree to render (if the input does not have any syntax errors). </p>
",Parsing & POS Tagging,render linguistic syntax tree browser input either bracketed representation tree labeled internal node output whether line dashed whether caption present significant input could bracketing word without label e g output internal label time tree structure another component input whether tree labeled unlabeled question best way fastest development time render tree browser javascript everything happen client side imagining simple interface textbox radio button specifying whether labeled tree changed trigger tree render input doe syntax error
Extracting information from context-free phrase structure output from Stanford Parser,"<p>The Stanford Parser (http://nlp.stanford.edu/software/lex-parser.shtml) gives context-free phrase structure trees as following. What is the best way to extract things like all the Noun Phrases(NP) and Verb Phrases(NP) in the tree? Is there any Python (or Java) library that can allow me to read structures like these? Thank you.</p>

<pre><code>(ROOT
  (S
    (S
      (NP
        (NP (DT The) (JJS strongest) (NN rain))
        (VP
          (ADVP (RB ever))
          (VBN recorded)
          (PP (IN in)
            (NP (NNP India)))))
      (VP
        (VP (VBD shut)
          (PRT (RP down))
          (NP
            (NP (DT the) (JJ financial) (NN hub))
            (PP (IN of)
              (NP (NNP Mumbai)))))
        (, ,)
        (VP (VBD snapped)
          (NP (NN communication) (NNS lines)))
        (, ,)
        (VP (VBD closed)
          (NP (NNS airports)))
        (CC and)
        (VP (VBD forced)
          (NP
            (NP (NNS thousands))
            (PP (IN of)
              (NP (NNS people))))
          (S
            (VP (TO to)
              (VP
                (VP (VB sleep)
                  (PP (IN in)
                    (NP (PRP$ their) (NNS offices))))
                (CC or)
                (VP (VB walk)
                  (NP (NN home))
                  (PP (IN during)
                    (NP (DT the) (NN night))))))))))
    (, ,)
    (NP (NNS officials))
    (VP (VBD said)
      (NP-TMP (NN today)))
    (. .)))
</code></pre>
",Parsing & POS Tagging,extracting information context free phrase structure output stanford parser stanford parser give context free phrase structure tree following best way extract thing like noun phrase np verb phrase np tree python java library allow read structure like thank
How do I manipulate parse trees?,"<p>I've been playing around with natural language parse trees and manipulating them in various ways. I've been using Stanford's Tregex and Tsurgeon tools but the code is a mess and doesn't fit in well with my mostly Python environment (those tools are Java and aren't ideal for tweaking). I'd like to have a toolset that would allow for easy hacking when I need more functionality. Are there any other tools that are well suited for doing pattern matching on trees and then manipulation of those matched branches?</p>

<p>For example, I'd like to take the following tree as input:</p>

<pre><code>(ROOT
  (S
    (NP
      (NP (NNP Bank))
      (PP (IN of)
        (NP (NNP America))))
    (VP (VBD used)
      (S
        (VP (TO to)
          (VP (VB be)
            (VP (VBN called)
              (NP
                (NP (NNP Bank))
                (PP (IN of)
                  (NP (NNP Italy)))))))))))
</code></pre>

<p>and (this is a simplified example):</p>

<ol>
<li>Find any node with the label NP that has a first child with the label NP and some descendent named ""Bank"", and a second child with the label PP.</li>
<li>If that matches, then take all of the children of the PP node and move them to end of the matched NP's children.</li>
</ol>

<p>For example, take this part of the tree:</p>

<pre><code>(NP
  (NP (NNP Bank))
  (PP (IN of)
    (NP (NNP America))))
</code></pre>

<p>and turn it into this:</p>

<pre><code>(NP
  (NP (NNP Bank) (IN of) (NP (NNP America))))
</code></pre>

<p>Since my input trees are S-expressions I've considered using Lisp (embedded into my Python program) but it's been so long that I've written anything significant in Lisp that I have no idea where to even start.</p>

<p>What would be a good way to describe the patterns? What would be a good way to describe the manipulations? What's a good way to think about this problem?</p>
",Parsing & POS Tagging,manipulate parse tree playing around natural language parse tree manipulating various way using stanford tregex tsurgeon tool code mess fit well mostly python environment tool java ideal tweaking like toolset would allow easy hacking need functionality tool well suited pattern matching tree manipulation matched branch example like take following tree input simplified example find node label np ha first child label np descendent named bank second child label pp match take child pp node move end matched np child example take part tree turn since input tree expression considered using lisp embedded python program long written anything significant lisp idea even start would good way describe pattern would good way describe manipulation good way think problem
using Dependency Parser in Stanford coreNLP,"<p>I am using the Stanford coreNLP ( <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/corenlp.shtml</a> ) in order to parse sentences and extract dependencies between the words.</p>

<p>I have managed to create the dependencies graph like in the example in the supplied link, but I don't know how to work with it. I can print the entire graph using the <code>toString()</code> method, but the problem I have is that the methods that search for certain words in the graph, such as <code>getChildList</code>, require an IndexedWord object as a parameter. Now, it is clear why they do because the nodes of the graph are of IndexedWord type, but it's not clear to me how I create such an object in order to search for a specific node.</p>

<p>For example: I want to find the children of the node that represents the word ""problem"" in my sentence. How I create an IndexWord object that represents the word ""problem"" so I can search for it in the graph?</p>
",Parsing & POS Tagging,using dependency parser stanford corenlp using stanford corenlp order parse sentence extract dependency word managed create dependency graph like example supplied link know work print entire graph using method problem method search certain word graph require indexedword object parameter clear node graph indexedword type clear create object order search specific node example want find child node represents word problem sentence create indexword object represents word problem search graph
Clustering phrases around a theme,"<p>I have encountered a very unusual problem. I have a set of phrases (noun phrases) extracted from a large corpus of documents. These phrases are >=2 and &lt;=3 words of length. There is a need to cluster these phrases because the number of phrases extracted are very large in number and showing them as a simple list might not be useful for the user. </p>

<p>We are thinking of nice very simple ways of clustering these. Is there a quick tool/software/method that I could use to cluster these so that all phrases inside a cluster belong to a particular theme/topic, if I keep the number of topics as a fixed initially? I don't have any training set or any other clusters that I can use as a training set.</p>
",Parsing & POS Tagging,clustering phrase around theme encountered unusual problem set phrase noun phrase extracted large corpus document phrase word length need cluster phrase number phrase extracted large number showing simple list might useful user thinking nice simple way clustering quick tool software method could use cluster phrase inside cluster belong particular theme topic keep number topic fixed initially training set cluster use training set
When would we extract verb phrases from a text?,"<p>I have come across plenty of material on extracting noun phrases from text. Noun phrases were defined as adjacent NN/NNS/NNP/NNPS modified by an optional JJ. It is easy to note that noun phrases are extracted to get a sense of what the text is all about and to may be generate a tag/cloud of words, or to display the distribution of noun phrases for a text corpus.</p>

<p>On the otherhand, what are the scenarios when a verb phrase would need to be extracted? What business problems exists that necessitate for the extraction of verb phrases?</p>

<p>Thanks
Abhishek S</p>
",Parsing & POS Tagging,would extract verb phrase text come across plenty material extracting noun phrase text noun phrase defined adjacent nn nns nnp nnps modified optional jj easy note noun phrase extracted get sense text may generate tag cloud word display distribution noun phrase text corpus otherhand scenario verb phrase would need extracted business problem exists necessitate extraction verb phrase thanks abhishek
"In NLTK pos_tag, why &quot;hello&quot; is classified as Noun?","<p>I've tried</p>

<pre><code>text = nltk.word_tokenize(""hello, my name is John"")
words = nltk.pos_tag(text)

for w in words:
    print ""%s = %s"" % (w[0], w[1])
</code></pre>

<p>And I got:</p>

<pre><code>hello = NN
, = ,
my = PRP$
name = NN
is = VBZ
John = NNP
</code></pre>
",Parsing & POS Tagging,nltk po tag hello classified noun tried got
POS Pattern Filter?,"<p>I'm writing some code that iterates a set of POS tags (generated by pos_tag in NLTK) to search for POS patterns. Matching sets of POS tags are stored in a list for later processing. Surely a regex-style pattern filter already exists for a task like this, but a couple of initial google searches didn't give me anything.</p>

<p>Are there any code snippets out there that can do my POS pattern filtering for me?</p>

<p>Thanks,
Dave</p>

<p>EDIT: Complete solution (using RegexParser, and where messages is any string)</p>

<pre><code>text = nltk.word_tokenize(message)
tags = nltk.pos_tag(text)
grammar = r""""""
    RULE_1: {&lt;JJ&gt;+&lt;NNP&gt;*&lt;NN&gt;*}
    """"""
chunker = nltk.RegexpParser(grammar)
chunked = chunker.parse(tags)
def filter(tree):
    return (tree.node == ""RULE_1"")
for s in chunked.subtrees(filter):
    print s
</code></pre>

<p>Check out <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html"" rel=""nofollow"">http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html</a> and <a href=""http://www.regular-expressions.info/reference.html"" rel=""nofollow"">http://www.regular-expressions.info/reference.html</a> for more on creating the rules.</p>
",Parsing & POS Tagging,po pattern filter writing code iterates set po tag generated po tag nltk search po pattern matching set po tag stored list later processing surely regex style pattern filter already exists task like couple initial google search give anything code snippet po pattern filtering thanks dave edit complete solution using regexparser message string check creating rule
How to calculate tag-wise precision and recall for POS tagger?,"<p>I am using some rule-based and statistical POS taggers to tag a corpus(of around <strong>5000 sentences</strong>) with Parts of Speech(POS). Following is a snippet of my test corpus where each word is seperated by its respective POS tag by '/'.</p>

<pre><code>No/RB ,/, it/PRP was/VBD n't/RB Black/NNP Monday/NNP ./.
But/CC while/IN the/DT New/NNP York/NNP Stock/NNP Exchange/NNP did/VBD n't/RB fall/VB apart/RB Friday/NNP as/IN the/DT Dow/NNP Jones/NNP Industrial/NNP Average/NNP plunged/VBD 190.58/CD points/NNS --/: most/JJS of/IN it/PRP in/IN the/DT final/JJ hour/NN --/: it/PRP barely/RB managed/VBD *-2/-NONE- to/TO stay/VB this/DT side/NN of/IN chaos/NN ./.
Some/DT ``/`` circuit/NN breakers/NNS ''/'' installed/VBN */-NONE- after/IN the/DT October/NNP 1987/CD crash/NN failed/VBD their/PRP$ first/JJ test/NN ,/, traders/NNS say/VBP 0/-NONE- *T*-1/-NONE- ,/, *-2/-NONE- unable/JJ *-3/-NONE- to/TO cool/VB the/DT selling/NN panic/NN in/IN both/DT stocks/NNS and/CC futures/NNS ./.
</code></pre>

<p>After tagging the corpus, it looks like this:</p>

<pre><code>No/DT ,/, it/PRP was/VBD n't/RB Black/NNP Monday/NNP ./. 
But/CC while/IN the/DT New/NNP York/NNP Stock/NNP Exchange/NNP did/VBD n't/RB fall/VB apart/RB Friday/VB as/IN the/DT Dow/NNP Jones/NNP Industrial/NNP Average/JJ plunged/VBN 190.58/CD points/NNS --/: most/RBS of/IN it/PRP in/IN the/DT final/JJ hour/NN --/: it/PRP barely/RB managed/VBD *-2/-NONE- to/TO stay/VB this/DT side/NN of/IN chaos/NNS ./. 
Some/DT ``/`` circuit/NN breakers/NNS ''/'' installed/VBN */-NONE- after/IN the/DT October/NNP 1987/CD crash/NN failed/VBD their/PRP$ first/JJ test/NN ,/, traders/NNS say/VB 0/-NONE- *T*-1/-NONE- ,/, *-2/-NONE- unable/JJ *-3/-NONE- to/TO cool/VB the/DT selling/VBG panic/NN in/IN both/DT stocks/NNS and/CC futures/NNS ./. 
</code></pre>

<p>I need to calculate the tagging accuracy(<strong>Tag wise- Recall &amp; Precision</strong>), therefore need to find an error(if any) in tagging for each word-tag pair. </p>

<p>The approach I am thinking of is to loop through these 2 text files and store them in a list and later compare the 'two' lists element by element. </p>

<p>The approach seems really crude to me, so would like you guys to suggest some better solution to the above problem.</p>

<p>From the <a href=""http://en.wikipedia.org/wiki/Precision_and_recall"" rel=""noreferrer"">wikipedia</a> page:</p>

<blockquote>
  <p>In a classification task, the
  <strong>precision</strong> for a class is the number of
  true positives (i.e. the number of
  items correctly labeled as belonging
  to the positive class) divided by the
  total number of elements labeled as
  belonging to the positive class (i.e.
  the sum of true positives and false
  positives, which are items incorrectly
  labeled as belonging to the class).
  <strong>Recall</strong> in this context is defined as
  the number of true positives divided
  by the total number of elements that
  actually belong to the positive class
  (i.e. the sum of true positives and
  false negatives, which are items which
  were not labeled as belonging to the
  positive class but should have been).</p>
</blockquote>
",Parsing & POS Tagging,calculate tag wise precision recall po tagger using rule based statistical po tagger tag corpus around sentence part speech po following snippet test corpus word seperated respective po tag tagging corpus look like need calculate tagging accuracy tag wise recall precision therefore need find error tagging word tag pair approach thinking loop text file store list later compare two list element element approach seems really crude would like guy suggest better solution problem wikipedia page classification task precision class number true positive e number item correctly labeled belonging positive class divided total number element labeled belonging positive class e sum true positive false positive item incorrectly labeled belonging class recall context defined number true positive divided total number element actually belong positive class e sum true positive false negative item labeled belonging positive class
Parsing a Syntax Tree with Perl Regex,"<p>Perhaps regex is not the best way to parse this, tell me if I it is not. Anyway, here are some examples of what the syntax tree looks like:</p>

<pre><code>(S (CC and))  
(SBARTMP (IN once) (NP otherstuff))   
(S (S (NP blah (VP blah)) (CC then) (NP blah (VP blah (PP blah))) ))   
</code></pre>

<p>Anyway, what I am trying to do is pull the connective out (and, then, once, etc) and its corresponding head (CC,IN,CC), which I already know for each syntax tree so it can act as an anchor, and I also need to retrieve its parent (in the first it is S, second SBARTMP, and third it is S), and its siblings, if there are any (in the first none, in the second left hand side sibling, and third left-hand-side and right-hand-side sibling). Anything higher than the parent is not included  </p>

<pre><code>my $pos = ""(\\\w|-)*"";  
my $sibling = qr{\s*(\\((?:(?&gt;[^()]+)|(?1))*\\))\s*};  
my $connective = ""once"";  
my $re = qr{(\(\w*\s*$sibling*\s*\\(IN\s$connective\\)\s*$sibling*\s*\))};  
</code></pre>

<p>This code works for things like:  </p>

<pre><code>my $test1 = ""(X (SBAR-TMP (IN once) (S sdf) (S sdf)))"";  
my $test2 = ""(X (SBAR-TMP (IN once))"";  
my $test3 = ""(X (SBAR-TMP (IN once) (X as))"";  
my $test4 = ""(X (SBAR-TMP (X adsf) (IN once))"";  
</code></pre>

<p>It will throw away the X on top and keep everything else, however, once the siblings have stuff embedded in them then it does not match because the regex does not go deeper.</p>

<pre><code>my $test = ""(X (SBAR-TMP (IN once) (MORE stuff (MORE stuff))))"";  
</code></pre>

<p>I am not sure how to account for this. I am kind of new to the extended patterns for Perl, just started learning it. To clarify a bit about what the regex is doing: it looks for the connective within two parentheses and the capital-letter/- combo, looks for a complete parent of the same format closing with two parentheses and then should look for any number of siblings that have all their parentheses paired off.</p>
",Parsing & POS Tagging,parsing syntax tree perl regex perhaps regex best way parse tell anyway example syntax tree look like anyway trying pull connective etc corresponding head cc cc already know syntax tree act anchor also need retrieve parent first second sbartmp third sibling first none second left hand side sibling third left hand side right hand side sibling anything higher parent included code work thing like throw away x top keep everything else however sibling stuff embedded doe match regex doe go deeper sure account kind new extended pattern perl started learning clarify bit regex look connective within two parenthesis capital letter combo look complete parent format closing two parenthesis look number sibling parenthesis paired
nltk custom tokenizer and tagger,"<p>Here is my requirement. I want to tokenize and tag a paragraph in such a way that it allows me to achieve following stuffs.</p>

<ul>
<li>Should identify date and time in the paragraph and Tag them as DATE and TIME</li>
<li>Should identify known phrases in the paragraph and Tag them as CUSTOM</li>
<li>And rest content should be tokenized should be tokenized by the default nltk's word_tokenize and pos_tag functions?</li>
</ul>

<p><strong>For example</strong>, following sentense</p>

<pre><code>""They all like to go there on 5th November 2010, but I am not interested.""
</code></pre>

<p>should be tagged and tokenized as follows in case of that custom phrase is <strong>""I am not interested""</strong>.</p>

<pre><code>[('They', 'PRP'), ('all', 'VBP'), ('like', 'IN'), ('to', 'TO'), ('go', 'VB'), 
('there', 'RB'), ('on', 'IN'), ('5th November 2010', 'DATE'), (',', ','), 
('but', 'CC'), ('I am not interested', 'CUSTOM'), ('.', '.')]
</code></pre>

<p><strong>Any suggestions would be useful.</strong></p>
",Parsing & POS Tagging,nltk custom tokenizer tagger requirement want tokenize tag paragraph way allows achieve following stuff identify date time paragraph tag date time identify known phrase paragraph tag custom rest content tokenized tokenized default nltk word tokenize po tag function example following sentense tagged tokenized follows case custom phrase interested suggestion would useful
Dependency parsing,"<p>I particularly like the transduce feature offered by agfl in their EP4IR 
<a href=""http://www.agfl.cs.ru.nl/EP4IR/english.html"" rel=""nofollow noreferrer"">http://www.agfl.cs.ru.nl/EP4IR/english.html</a></p>

<p>The download page is here:
<a href=""http://www.agfl.cs.ru.nl/download.html"" rel=""nofollow noreferrer"">http://www.agfl.cs.ru.nl/download.html</a></p>

<p>Is there any way i can make use of this in a c# program? Do I need to convert classes to c#?</p>

<p>Thanks
:)</p>
",Parsing & POS Tagging,dependency parsing particularly like transduce feature offered agfl ep ir download page way make use c program need convert class c thanks
Parsing expressions with an undefined number of arguments,"<p>I'm trying to parse a string in a self-made language into a sort of tree, e.g.:</p>

<pre><code># a * b1 b2 -&gt; c * d1 d2 -&gt; e # f1 f2 * g
</code></pre>

<p>should result in:</p>

<pre><code># a
  * b1 b2
    -&gt; c
  * d1 d2
    -&gt; e
# f1 f2
  * g
</code></pre>

<p>#, * and -> are symbols. a, b1, etc. are texts.</p>

<p>Since the moment I know only rpn method to evaluate expressions, and my current solution is as follows. If I allow only a single text token after each symbol I can easily convert expression first into RPN notation (b = b1 b2; d = d1 d2; f = f1 f2) and parse it from here:</p>

<p>a b c -> * d e -> * # f g * #</p>

<p>However, merging text tokens and whatever else comes seems to be problematic. My idea was to create marker tokens (M), so RPN looks like:</p>

<p>a M b2 b1 M c -> * M d2 d1 M e -> * # f2 f1 M g * #</p>

<p>which is also parseable and seems to solve the problem.</p>

<p>That said:</p>

<ol>
<li>Does anyone have experience with something like that and can say it is or it is not a viable solution for the future?</li>
<li>Are there better methods for parsing expressions with undefined arity of operators?</li>
<li>Can you point me at some good resources?</li>
</ol>

<p>Note. Yes, I know this example very much resembles Lisp prefix notation and maybe the way to go would be to add some brackets, but I don't have any experience here. However, the source text must not contain any artificial brackets and also I'm not sure what to do about potential infix mixins like # a * b -> [if value1 =  value2] c -> d.</p>

<p>Thanks for any help.</p>

<p>EDIT: It seems that what I'm looking for are sources on postfix notation with a variable number of arguments.</p>
",Parsing & POS Tagging,parsing expression undefined number argument trying parse string self made language sort tree e g result symbol b etc text since moment know rpn method evaluate expression current solution follows allow single text token symbol easily convert expression first rpn notation b b b f f f parse b c e f g however merging text token whatever else come seems problematic idea wa create marker token rpn look like b b c e f f g also parseable seems solve problem said doe anyone experience something like say viable solution future better method parsing expression undefined arity operator point good resource note yes know example much resembles lisp prefix notation maybe way go would add bracket experience however source text must contain artificial bracket also sure potential infix mixins like b value value c thanks help edit seems looking source postfix notation variable number argument
