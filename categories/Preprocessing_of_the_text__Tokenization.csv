Title,Description,category,combined_text
How to implement a &quot;related&quot; degree measure algorithm?,"<p>I was going to Ask a Question earlier today when I was presented to a surprising functionality in Stackoverflow. When I wrote my question title stackoverflow suggested me several related questions and I found out that there was already two similar questions. That was stunning! </p>

<p>Then I started thinking how I would implement such function. How I would order questions by relatedness:</p>

<ol>
<li>Question that have higher number of
words matchs with the new question</li>
<li>If the number of matchs are the
same, the order of words is considered</li>
<li>Words that appears in the title has
higher relevancy</li>
</ol>

<p>That would be a simple workflow or a complex score algortithm?
Some stemming to increase the recall, maybe?
Is there some library the implements this function?
What other aspects would you consider?
Maybe Jeff could answer himself! How did you implemented this in Stackoverflow? :)</p>
",Preprocessing of the text & Tokenization,implement related degree measure algorithm wa going ask question earlier today wa presented surprising functionality stackoverflow wrote question title stackoverflow suggested several related question found wa already two similar question wa stunning started thinking would implement function would order question relatedness question higher number word match new question number match order word considered word appears title ha higher relevancy would simple workflow complex score algortithm stemming increase recall maybe library implement function aspect would consider maybe could answer implemented stackoverflow
Creating regular expression(s) which finds capitalization errors,"<blockquote>
<p>This is a Sentence which contains<br/>
Some capitalization errors.</p>
</blockquote>
<p>So far I have this: <code>(?&lt;![.!?]\s)(?&lt;!^)(?&lt;!\sI\s)(?!I['’][a-z])(?!\b(?:Dr|Mr|Mrs)\.[\s\r\n])\b(?!I\b)[A-Z]\w*</code></p>
<p>It will find &quot;Sentence&quot; in the above. It avoids hitting on I and I' contractions, and Dr. / Mr. / Mrs.</p>
<p>What I can't get it to do is find &quot;Some&quot; in the above.</p>
<p>I feel maybe a second expression might be better for document scanning, as the first expression is quite long and probably not optimized.</p>
<p>I need the expression to be PCRE compliant that avoids non fixed width errors and such.</p>
<p>Just can't solve this on my own unfortunately. As expected AI is no help here... the best models struggle with regular expressions unless they are more simple.</p>
<p>Tried many different RegEx's to match the word &quot;Some&quot; in the above. It should NOT match a preceding line that ends with a period, question mark, or exclamation point. It should also NOT match on I and I' contractions, or on Dr. / Mr. / Mrs.</p>
",Preprocessing of the text & Tokenization,creating regular expression find capitalization error sentence contains capitalization error far find sentence avoids hitting contraction dr mr mr get find feel maybe second expression might better document scanning first expression quite long probably optimized need expression pcre compliant avoids non fixed width error solve unfortunately expected ai help best model struggle regular expression unless simple tried many different regex match word match preceding line end period question mark exclamation point also match contraction dr mr mr
How can I run an input file on the porter stemmer java code?,"<p>How can I run an input file
on the porter stemmer java code?</p>
",Preprocessing of the text & Tokenization,run input file porter stemmer java code run input file porter stemmer java code
Clustering of news articles,"<p>My scenario is pretty straightforwrd: I have a bunch of news articles (~1k at the moment) for which I know that some cover the same story/topic. I now would like to group these articles based on shared story/topic, i.e., based on their similarity.</p>

<p>What I did so far is to apply basic NLP techniques including stopword removal and stemming. I also calculated the tf-idf vector for each article, and with this can also calculate the, e.g., cosine similarity based on these tf-idf-vectors. But now with the grouping of the articles I struggles a bit. I see two principle ways -- probably related -- to do it:</p>

<p>1) Machine Learning / Clustering: I already played a bit with existing clustering libraries, with more or less success; see <a href=""https://stackoverflow.com/questions/25217065/scikit-learn-clustering-text-documents-using-dbscan"">here</a>. On the one hand, algorithms such as k-means require the number of clusters as input, which I don't know. Other algorithms require parameters that are also not intuitive to specify (for me that is).</p>

<p>2) Graph algorithms: I can represent my data as a graph with the articles being the nodes and weighted adges representing the pairwise (cosine) similarity between the articles. With that, for example, I can first remove all edges that fall below a certain threshold and then might apply graph algorithms to look for strongly-connected subgraphs.</p>

<p>In short, I'm not sure where best to go from here -- I'm still pretty new in this area. I wonder if there some best practices for that, or some kind of guidelines which methods / algorithms can (not) be applied in certain scenarios.</p>

<p>(EDIT: forgot to link to related question of mine)</p>
",Preprocessing of the text & Tokenization,clustering news article scenario pretty straightforwrd bunch news article k moment know cover story topic would like group article based shared story topic e based similarity far apply basic nlp technique including stopword removal stemming also calculated tf idf vector article also calculate e g cosine similarity based tf idf vector grouping article struggle bit see two principle way probably related machine learning clustering already played bit existing clustering library le success see href one hand algorithm k mean require number cluster input know algorithm require parameter also intuitive specify p graph algorithm represent data graph article node weighted adges representing pairwise cosine similarity article example first remove edge fall certain threshold might apply graph algorithm look strongly connected subgraphs short sure best go still pretty new area wonder best practice kind guideline method algorithm applied certain scenario edit forgot link related question mine
nltk add or remove some abbreviations for the specific project not working,"<p>When tokenizing paragraphs in the Czech language, I am observing that some abbreviations are not treated as abbreviations. The paragraph is stored in the file as one long line. The nltk is of the version 3.9.1, the <code>nltk_data</code> are shared -- stored in <code>c:\nltk_data\</code>; freshly downloaded (30. 1. 2025). Python 3.12 on Windows 10 was used.</p>
<p>Firstly, the example with using the <code>.sent_tokenize</code> method, and the (should be equivalent) code with <code>PunktTokenizer</code> used explicitly -- that should add the the two other Czech abbreviations. The script is stored as <code>test2.py</code> file, using the UTF-8 encoding without BOM:</p>
<pre><code>import nltk

text = '''Věta číslo 1. Věta č. 2. Toto je začátek další věty [pozn. překl. nějaká poznámka překladatele], která definuje pojem „kružnice“.'''

lst = nltk.tokenize.sent_tokenize(text, language='czech')

for n, s in enumerate(lst, 1):
    print(f'{n}: {s}')
print('---------------------------')

# The same tokenizer with added abbreviations.
tokenizer = nltk.tokenize.PunktTokenizer('czech')
tokenizer._params.abbrev_types.update('pozn', 'překl')  # adding the two abbreviatios
lst = tokenizer.tokenize(text)

for n, s in enumerate(lst, 1):
    print(f'{n}: {s}')
</code></pre>
<p><a href=""https://i.sstatic.net/tqWk7cyf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tqWk7cyf.png"" alt=""enter image description here"" /></a></p>
<p>The third sentence contains the (human) translator's note in square brackets -- the abbreviation at the beginning is <code>pozn. překl.</code>.</p>
<p>The added abbreviations are not recognized. However, when I manually add the two abbreviations into the <code>c:\nltk_data\tokenizers\punkt_tab\czech\abbrev_types.txt</code>, the tokenizer works as expected:</p>
<p><a href=""https://i.sstatic.net/045vItCY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/045vItCY.png"" alt=""enter image description here"" /></a></p>
<p>Where is the bug? How should I add the extra abbreviations in the situation when I am not allowed (or do not want) to modify the shared nltk data?</p>
<p>P.S. I am very new to nltk.</p>
",Preprocessing of the text & Tokenization,nltk add remove abbreviation specific project working tokenizing paragraph czech language observing abbreviation treated abbreviation paragraph stored file one long line nltk version shared stored freshly downloaded python window wa used firstly example using method equivalent code used explicitly add two czech abbreviation script stored file using utf encoding without bom third sentence contains human translator note square bracket abbreviation beginning added abbreviation recognized however manually add two abbreviation tokenizer work expected bug add extra abbreviation situation allowed want modify shared nltk data p new nltk
word reduction of a list of words,"<p>Besides using a chatbot like o1 mini, is there more local way to reduce a list of words from 10 words down to 3 words? I feel that due to context, word association, semantic meanings of the words, a there has to be some form of extrapolation to reduce the list but so far my attempts have not been nearly as good.</p>
<p>The ultimate goal is to reduce a list to something that can be searchable.</p>
<p>example using chatgpt 1o mini</p>
<p>reduce the list of words to 3 terms able to used in search. Combine words if they make sense in context to all other words in the list. Each term should combine words meaningfully, and no word should be repeated across the terms. return in list form</p>
<pre><code>[&quot;rpg&quot;, &quot;role playing&quot;, &quot;fantasy&quot;, &quot;monster&quot;, &quot;Dungeons&quot;, &quot;dragons&quot;, &quot;master&quot;, &quot;monster&quot;, &quot;job&quot;, &quot;class&quot;]
</code></pre>
<blockquote>
<p>result: Dungeons Dragons, Role Playing, Monster Class</p>
</blockquote>
<p>here are the attempts I tried using python</p>
<p>attempt 1: UMAP, Cosine similarity</p>
<pre><code>from sklearn.metrics.pairwise import cosine_similarity

def concatenate_overlap(list_of_list):
    pooled = [set(subList) for subList in list_of_list]
    merging = True
    while merging:
        merging = False
        for i, group in enumerate(pooled):
            merged = next((g for g in pooled[i + 1:] if g.intersection(group)), None)
            if not merged: 
                continue
            group.update(merged)
            pooled.remove(merged)
            merging = True
    return [list(x) for x in pooled]


text_list = [&quot;rpg&quot;, &quot;role playing&quot;, &quot;fantasy&quot;, &quot;monster&quot;, &quot;Dungeons&quot;, &quot;dragons&quot;, &quot;master&quot;, &quot;monster&quot;, &quot;job&quot;, &quot;class&quot;]

MODEL_NAME = 'Alibaba-NLP/gte-multilingual-base'
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)
batch_dict = tokenizer(text_list, max_length=8192, padding=True, truncation=True, return_tensors='pt')
outputs = model(**batch_dict)
embeddings = outputs.last_hidden_state[:, 0][:768]
similarity_matrix = cosine_similarity(embeddings.detach())
np.fill_diagonal(similarity_matrix, 0)
to_merge_list: List[List[int]] = []
for idx, topic_similarity_scores in enumerate(similarity_matrix):
    similar_words = list(np.where(0.8&gt; min_similarity)[0])
    similar_words.append(idx)
    to_merge_list.append(similar_words)

to_concat = concatenate_overlap(to_merge_list)
words = [text_list[a[0]] for a in to_concat]

</code></pre>
<blockquote>
<p>result: ['rpg', 'monster', 'Dungeons', 'dragons', 'master', 'monster', 'job', 'class']</p>
</blockquote>
<p>attempt 2: summurization model</p>
<pre><code>summarizer = pipeline(&quot;summarization&quot;, model=&quot;facebook/bart-large-cnn&quot;)

word_list = [&quot;rpg&quot;, &quot;role playing&quot;, &quot;fantasy&quot;, &quot;monster&quot;, &quot;Dungeons&quot;, &quot;dragons&quot;, &quot;master&quot;, &quot;monster&quot;, &quot;job&quot;, &quot;class&quot;]
text = &quot;, &quot;.join(word_list)
summarizer(text, max_length=50, do_sample=False)
</code></pre>
<blockquote>
<p>result: rpgs, role playing, fantasy, monster, Dungeons, dragons, master, monster. job, class. rpg, roleplaying, fantasy,. monster, dungeons, dragons,. master, monsters, master. job,. class</p>
</blockquote>
<p>edit: the criteria is simple, shrink the word list to a 3 or 4 word phrase that can be searchable. In the case of chatgpt 01, it gave me 3 multi word expressions for searching. For the two examples. Ideally it would give me something also as small.</p>
<p>I plan to take word lists and boil them down to searchable prompts</p>
<p>edit 2:
to provide more context to what the expected outcome should be here is a unit test and explanation.</p>
<pre><code>input: [&quot;bad&quot;, &quot;horrible&quot;, &quot;unfun&quot;, &quot;waste&quot;, &quot;big&quot;, &quot;mutha&quot;, &quot;truckers&quot;, &quot;racing&quot;, &quot;incomplete&quot;, &quot;big mutha&quot;]

output: [&quot;big mutha truckers&quot;, &quot;unfun&quot;, &quot;incomplete&quot;]
</code></pre>
<p>contextually, the list of 10 words is focusing on a game called &quot;big mutha truckers&quot; and as such this would get pulled from the word list. The second would be all the synonyms which refer to the game being not good, so &quot;unfun&quot; or &quot;bad&quot; or even &quot;horrible racing&quot;. Lastly &quot;incomplete&quot; would get pulled as it isn't a close synonym of bad and is is descriptive</p>
<p>with these 3 words, I can throw it into google and ideally search results of negative reviews for big mutha truckers should appear.</p>
",Preprocessing of the text & Tokenization,word reduction list word besides using chatbot like mini local way reduce list word word word feel due context word association semantic meaning word ha form extrapolation reduce list far attempt nearly good ultimate goal reduce list something searchable example using chatgpt mini reduce list word term able used search combine word make sense context word list term combine word meaningfully word repeated across term return list form result dungeon dragon role playing monster class attempt tried using python attempt umap cosine similarity result rpg monster dungeon dragon master monster job class attempt summurization model result rpgs role playing fantasy monster dungeon dragon master monster job class rpg roleplaying fantasy monster dungeon dragon master monster master job class edit criterion simple shrink word list word phrase searchable case chatgpt gave multi word expression searching two example ideally would give something also small plan take word list searchable prompt edit provide context expected outcome unit test explanation contextually list word focusing game called big mutha trucker would get pulled word list second would synonym refer game good unfun bad even horrible racing lastly incomplete would get pulled close synonym bad descriptive word throw google ideally search result negative review big mutha trucker appear
Getting all leaf words (reverse stemming) into one Python List,"<p>On the same lines as the solution provided <a href=""https://stackoverflow.com/questions/65559962/get-all-leaf-words-for-a-stemmed-keyword"">in this link</a>, I am trying to get all leaf words of one stem word. I am using the community-contributed (@Divyanshu Srivastava) package <code>get_word_forms</code></p>
<p>Imagine I have a shorter sample word list as follows:</p>
<pre><code>my_list = [' jail', ' belief',' board',' target', ' challenge', ' command']
</code></pre>
<p>If I work it manually, I do the following (which is go word-by-word, which is very time-consuming if I have a list of 200 words):</p>
<pre><code>get_word_forms(&quot;command&quot;)
</code></pre>
<p>and get the following output:</p>
<pre><code>{'n': {'command',
  'commandant',
  'commandants',
  'commander',
  'commanders',
  'commandership',
  'commanderships',
  'commandment',
  'commandments',
  'commands'},
 'a': set(),
 'v': {'command', 'commanded', 'commanding', 'commands'},
 'r': set()}
</code></pre>
<p>'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb.</p>
<p>If I try to reverse-stem the entire list in one go:</p>
<pre><code>[get_word_forms(word) for word in sample]
</code></pre>
<p>I fail at getting any output:</p>
<pre><code>[{'n': set(), 'a': set(), 'v': set(), 'r': set()},
 {'n': set(), 'a': set(), 'v': set(), 'r': set()},
 {'n': set(), 'a': set(), 'v': set(), 'r': set()},
 {'n': set(), 'a': set(), 'v': set(), 'r': set()},
 {'n': set(), 'a': set(), 'v': set(), 'r': set()},
 {'n': set(), 'a': set(), 'v': set(), 'r': set()},
 {'n': set(), 'a': set(), 'v': set(), 'r': set()}]
</code></pre>
<p>I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb:</p>
<p>something like:</p>
<pre><code>['command','commandant','commandants',  'commander', 'commanders', 'commandership',
'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on. 
</code></pre>
",Preprocessing of the text & Tokenization,getting leaf word reverse stemming one python list line solution provided href link trying get leaf word one stem word using community contributed divyanshu srivastava package imagine shorter sample word list follows work manually following go word word time consuming list word get following output n noun adjective v verb r adverb try reverse stem entire list one go fail getting output think failing saving output dictionary eventually would like output list without breaking noun adjective adverb verb something like
How to apply semantic tokenize on sentence in java by NLP?,"<p>Can an NLP model be used to tokenize a sentence based on its semantic meaning?</p>
<p>For example,
for the sentence: <strong>If the driver's age is more than 20</strong>,
the tokens would be:</p>
<p>Token1: if</p>
<p>Token2: driver age</p>
<p>Token3: more than</p>
<p>Token4: 20</p>
",Preprocessing of the text & Tokenization,apply semantic tokenize sentence java nlp nlp model used tokenize sentence based semantic meaning example sentence driver age token would token token driver age token token
Counting the Frequency of Some Words within some other Key Words in Text,"<p>I have two sets of word lists - first one I called <code>search words</code> and the second one I called <code>key words</code>. My goal is to calculate the frequency of <code>search words</code> within 10 words of <code>key words</code>. For example, assume that the word - <strong>acquire</strong> - is in <code>key words</code> list, then I will look for the words in <code>search words</code> list within 10 words of <strong>acquire</strong>. Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement.</p>
<p>Below is my <code>search word</code> and <code>key word</code> lists -</p>
<pre><code>search_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',
 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',
 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',
 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',
 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',
 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', 
 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',
 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', 
 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', 
 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',
 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',
 'Securion', 'security event management', 'security information and event management', 
 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',
 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', 
 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']

key_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',
 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',
 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', 
 'integrate', 'invest', 'lease',
 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',
  'upgrade', 'use']
</code></pre>
<p>A small Example -</p>
<pre><code>text_dict = {
    'ITEM7':[&quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process \
    of adopting Symantec. We believe these technologies will improve our access control. \
        Moreover, we also integrated data security diagnostic program.&quot;,
        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;]
}

df = pd.DataFrame(text_dict)
</code></pre>
<p>My expected outcome is -</p>
<pre><code>                 ITEM7                          Frequency
Last year, from AVG we have acquired Alibaba S...   6
We are planning to install end-point security,...   2
</code></pre>
<p>For the first row in <code>df</code>, we see the word <code>AVG</code> and <code>Alibaba Security</code> are from <code>search_words</code> list and around the word <strong>acquired</strong>, the base form of which - <strong>acquire</strong> - is in the <code>key_words</code> list. Similarly, <code>Symantec</code>, <code>Access Control</code>, <code>data security</code>, <code>diagnostic program</code> are from <code>search_words</code> list and these words are within 10 words of <code>adopting</code>, <code>improve</code>, <code>integrated</code> from <code>key_words</code> list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the <code>Frequency</code> column of <code>df</code>, the value is 6.</p>
<p>Please note that the words in <code>key_words</code> are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.</p>
",Preprocessing of the text & Tokenization,counting frequency word within key word text two set word list first one called second one called goal calculate frequency within word example assume word acquire list look word list within word acquire within word mean word forward key word word backward key word meaning forward backward movement list small example expected outcome first row see word list around word acquired base form acquire list similarly list word within word list total search word avg alibaba security symantec access control data security diagnostic program therefore column value please note word basically base form variation like adopted adopting counted key word also
Bert models show tokenizing statistics,"<p>Is there any built-in way to request some tokenizing statistics when using BertTokenizer.from_pretrained('bert-base-uncased') and BertModel.from_pretrained('bert-base-uncased') to understand how efficiently my texts are being processed?</p>
<p>I am using texts that are not very large, but their length differentiates from 4 to 250 characters, depending on the training image. The texts sometimes may contain some weird and unpopular words. I am afraid that due to such conditions, the tokenizing process may not be very effective.</p>
<p>I am looking for some way of checking tokenizing statistics when processing all my text images.</p>
<p>I tried using this code, which was founded on Github, but there are a lot of errors inside it and it's not very verbal:</p>
<pre><code>from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

texts = [&quot;This is a sample text.&quot;, &quot;Another text with some uncommon words.&quot;]

encoded_texts = tokenizer(texts, return_tensors='pt')

# Vocabulary Coverage
vocab_size = len(tokenizer.vocab)
total_tokens = sum([len(text) for text in encoded_texts['input_ids']])
oov_tokens = sum([1 for token in encoded_texts['input_ids'].flatten() if token not in tokenizer.vocab])
vocab_coverage = 1 - (oov_tokens / total_tokens)

# Average Token Length
token_lengths = [len(text) for text in encoded_texts['input_ids']]
average_token_length = sum(token_lengths) / len(token_lengths)

print(f&quot;Vocabulary Coverage: {vocab_coverage:.2f}&quot;)
print(f&quot;Average Token Length: {average_token_length:.2f}&quot;)
</code></pre>
",Preprocessing of the text & Tokenization,bert model show tokenizing statistic built way request tokenizing statistic using berttokenizer pretrained bert base uncased bertmodel pretrained bert base uncased understand efficiently text processed using text large length differentiates character depending training image text sometimes may contain weird unpopular word afraid due condition tokenizing process may effective looking way checking tokenizing statistic processing text image tried using code wa founded github lot error inside verbal
Text Classification + NLP + Data-mining + Data Science: Should I do stop word removal and stemming before applying tf-idf?,"<p>I am working on a text classification problem. The problem is explained below:</p>

<p>I have a dataset of events which contains three columns - name of the event, description of the event, category of the event. There are about 32 categories in the dataset, such as, travel, sport, education, business etc. I have to classify each event to a category depending on its name and description.</p>

<p>What I understood is this particular task of classification is highly dependent on keywords, rather than, semantics. I am giving you two examples:</p>

<p>If the word 'football' is found either in the name or description or in both, it is highly likely that the event is about sport.</p>

<p>If the word 'trekking' is found either in the name or description or in both, it is highly likely that the event is about travel.</p>

<p>We are not considering multiple categories for an event(however, that's a plan for future !! )</p>

<p>I hope applying tf-idf before Multinomial Naive Bayes would lead to decent result for this problem. My question is:</p>

<p>Should I do stop word removal and stemming before applying tf-idf or should I apply tf-idf just on raw text? Here text means entries in name of event and description columns.</p>
",Preprocessing of the text & Tokenization,text classification nlp data mining data science stop word removal stemming applying tf idf working text classification problem problem explained dataset event contains three column name event description event category event category dataset travel sport education business etc classify event category depending name description understood particular task classification highly dependent keywords rather semantics giving two example word football found either name description highly likely event sport word trekking found either name description highly likely event travel considering multiple category event however plan future hope applying tf idf multinomial naive bayes would lead decent result problem question stop word removal stemming applying tf idf apply tf idf raw text text mean entry name event description column
Apertium + Python: POS-tagger not providing surface form,"<p>I'm trying to POS-tag some sentences in Italian with Apertium's tagger.
While according to the <a href=""https://github.com/apertium/apertium-python"" rel=""nofollow noreferrer"">Apertium GitHub page</a> I am supposed to get as output also the surface form in addition to the morphological analysis, I only get the analysis. I want also the surface form. I cannot infer it since the tagger doesn't necessarily tag a single token, so I cannot simply tokenize the original sentence and loop over it or zip it with the tagger's output.</p>
<p>According to the GitHub page:</p>
<pre><code>In [1]: import apertium
In [2]: tagger = apertium.Tagger('ita')
In [3]: tagger.tag('gatti').
Out[3]: [gatti/gatto&lt;n&gt;&lt;m&gt;&lt;pl&gt;]
</code></pre>
<p>What I got:</p>
<pre><code>In [1]: import apertium
In [2]: tagger = apertium.Tagger('ita')
In [3]: tagger.tag('gatti') # 'gatti' is the surface form
Out[3]: [gatto&lt;n&gt;&lt;m&gt;&lt;pl&gt;]
</code></pre>
<p>How can I get the surface form? If I provided one token at a time this would not be a problem since I would know what the token is. But in a sentence I cannot know how the tagger creates chunks.</p>
",Preprocessing of the text & Tokenization,apertium python po tagger providing surface form trying po tag sentence italian apertium tagger according apertium github page supposed get output also surface form addition morphological analysis get analysis want also surface form infer since tagger necessarily tag single token simply tokenize original sentence loop zip tagger output according github page got get surface form provided one token time would problem since would know token sentence know tagger creates chunk
How to search a string for different tenses?,"<p>I can use Stemmers, Filters etc. No problem. </p>

<p>But what about this case, for example the source text contains the phrase: </p>

<p>The fox made a jump.</p>

<p>User has entered: fox AND make
Results = 0;</p>

<p>The question is how to process irregular forms of words? </p>
",Preprocessing of the text & Tokenization,search string different tense use stemmer filter etc problem case example source text contains phrase fox made jump user ha entered fox make result question process irregular form word
My LSTM model returns empty with no output and no parameters captured,"<p>I am building an LSTM model for sentiment analysis using a hotel review dataset. However, the model always returns empty with empty output and parameters each time I run the code.</p>
<p>I have followed many processes from cleaning to tokenization, vectorization, encoding, and padding.</p>
<p><a href=""https://i.sstatic.net/oVNsQBA4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oVNsQBA4.png"" alt=""enter image description here"" /></a></p>
<p>See attached image.</p>
<p>I have followed the normal steps from cleaning, stopwords, tokenization, lemmatization, vectorization, padding.</p>
<p>I used the below code for the final model design:</p>
<pre><code># Design the model
model = Sequential()
model.add(Embedding(input_dim=len(vocab_int), output_dim=embedding_vector_length, input_length=max_sentence_length))
model.add(SimpleRNN(256, return_sequences=True, dropout=dropout, recurrent_dropout=dropout))
model.add(SimpleRNN(256, dropout=dropout, recurrent_dropout=dropout))
model.add(Dense(2, activation='softmax'))

model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

print(model.summary())
</code></pre>
<p>I was expecting the built model with output layers and parameters but got nothing.</p>
",Preprocessing of the text & Tokenization,lstm model return empty output parameter captured building lstm model sentiment analysis using hotel review dataset however model always return empty empty output parameter time run code followed many process cleaning tokenization vectorization encoding padding see attached image followed normal step cleaning stopwords tokenization lemmatization vectorization padding used code final model design wa expecting built model output layer parameter got nothing
UDPipe 2 Models in R,"<p>I am not entirely happy with udpipe 2.5. Latin models available through the library <code>udpipe</code>, and looking for a way to use newer (2.12 or 2.14) models listed <a href=""https://ufal.mff.cuni.cz/udpipe/2/models#universal_dependencies_212_models"" rel=""nofollow noreferrer"">here</a>.</p>
<p>However, it seems that they are only available in Python:</p>
<blockquote>
<p>Compared to UDPipe 1, it is Python-only, it does not perform tokenization, and the models require more computation power (<a href=""https://ufal.mff.cuni.cz/udpipe/2"" rel=""nofollow noreferrer"">here</a>).</p>
</blockquote>
<p>I am not a Python user at all, is there a way to use these models in R?</p>
<p>So far, I have been able to download a huge archive with the newer conllu-files (from <a href=""https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-5502"" rel=""nofollow noreferrer"">here</a>) and trying to train a model as described <a href=""https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-train.html"" rel=""nofollow noreferrer"">here</a>. However, it does not lead to a significantly better result.</p>
",Preprocessing of the text & Tokenization,udpipe model r entirely happy udpipe latin model available library looking way use newer model listed however seems available python compared udpipe python doe perform tokenization model require computation power python user way use model r far able download huge archive newer conllu file trying train model described however doe lead significantly better result
Removing bi-grams after tokenization for TfidfVectorizer,"<p>I'm attempting to remove bi-grams that are created by <code>TfidfVectorizer</code>.  I'm using <code>text.TfidfVectorizer</code> so that I can use my own preprocessor function.</p>
<p>Test strings and preprocessor function:</p>
<pre><code>doc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', 
        'another that has aa aa and start date and hurricane hitting south carolina']

def remove_bigrams(doc):
    gram_2 = ['past performance', 'start date', 'aa aa']
    res = []
    for record in doc:
        the_string = record
        for phrase in gram_2:
            the_string = the_string.replace(phrase, &quot;&quot;)
        res.append(the_string)
    return res

remove_bigrams(doc2)
</code></pre>
<p>My <code>TfidfVectorizer</code> instantiation and <code>fit_transform</code>:</p>
<pre><code>from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import text

custom_stop_words = [i for i in stop_words]

vec = text.TfidfVectorizer(stop_words=custom_stop_words,
                           analyzer='word',
                           ngram_range=(2, 2),
                           preprocessor=remove_bigrams,
                          )

features = vec.fit_transform(doc2)
</code></pre>
<p>Here is my error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [49], in &lt;cell line: 5&gt;()
      3 #t3_cv = CountVectorizer(t2, stop_words = stop_words)
      4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams)
----&gt; 5 features = vec.fit_transform(doc2)

File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y)
   2072 self._check_params()
   2073 self._tfidf = TfidfTransformer(
   2074     norm=self.norm,
   2075     use_idf=self.use_idf,
   2076     smooth_idf=self.smooth_idf,
   2077     sublinear_tf=self.sublinear_tf,
   2078 )
-&gt; 2079 X = super().fit_transform(raw_documents)
   2080 self._tfidf.fit(X)
   2081 # X is already a transformed view of raw_documents so
   2082 # we set copy to False

File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y)
   1330             warnings.warn(
   1331                 &quot;Upper case characters found in&quot;
   1332                 &quot; vocabulary while 'lowercase'&quot;
   1333                 &quot; is True. These entries will not&quot;
   1334                 &quot; be matched with any documents&quot;
   1335             )
   1336             break
-&gt; 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)
   1340 if self.binary:
   1341     X.data.fill(1)

File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab)
   1207 for doc in raw_documents:
   1208     feature_counter = {}
-&gt; 1209     for feature in analyze(doc):
   1210         try:
   1211             feature_idx = vocabulary[feature]

File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)
    111     doc = preprocessor(doc)
    112 if tokenizer is not None:
--&gt; 113     doc = tokenizer(doc)
    114 if ngrams is not None:
    115     if stop_words is not None:

TypeError: expected string or bytes-like object
</code></pre>
<p>How to resolve it?</p>
",Preprocessing of the text & Tokenization,removing bi gram tokenization tfidfvectorizer attempting remove bi gram created using use preprocessor function test string preprocessor function instantiation error resolve
C# english word stemming and lemmatizing using Catalyst - how to do that,"<p>I added Catalyst nuget package to my C# project, to help me lemmatize english words. However its documentation is not clear, and lacks of examples, I tried to lemmatize/stem only one word:</p>
<pre><code>Catalyst.Models.English.Register();

Storage.Current = new DiskStorage(&quot;catalyst-models&quot;);
var nlp = Pipeline.For(Language.English);

var doc = new Document(&quot;described&quot;, Language.English);
Console.WriteLine(doc.Value); // outputs &quot;described&quot;, expected &quot;describe&quot;
</code></pre>
<p>But it does nothing for me, with any words I added to it - returned the same value.</p>
<p>Is it capable of lemmatize one word? How? If not - how to do so in C#? In Python using nltk I can do it, but now I need to do so in C#!</p>
",Preprocessing of the text & Tokenization,c english word stemming lemmatizing using catalyst added catalyst nuget package c project help lemmatize english word however documentation clear lack example tried lemmatize stem one word doe nothing word added returned value capable lemmatize one word c python using nltk need c
How to remove stop words in Power Query Editor in Power BI,"<p>I've loaded an Excel file with reviews into Power BI Desktop. I want to do Sentiment Analysis on the file. I've already turned everything in the reviews column(named Column3) to lowercase and removed punctuation marks using the Power Query Editor by inserting a new step where I have the M code for that. I next want to remove Stop Words from Column3. I inserted a new step where I created a list of stop words. But I'm having issues proceeding. How do I run Column3 through the list(named StopWords) and store the results in a new step?</p>
",Preprocessing of the text & Tokenization,remove stop word power query editor power bi loaded excel file review power bi desktop want sentiment analysis file already turned everything review column named column lowercase removed punctuation mark using power query editor inserting new step code next want remove stop word column inserted new step created list stop word issue proceeding run column list named stopwords store result new step
bert-tokenizer to tokenize the sentence,"<p>how can i overcome to remove # signs when I am using ber-tokenizer
​
Tokens length: 200
Tokens: ['[CLS]', 'educational', 'background', 'computer', 'applications', 'masters', 'degree', 'software', 'along', 'strong', 'skills', 'p', '##yt', '##hon', 'sq', '##l', 'j', '##ava', 'l', '##in', '##ux', 'g', '##it', 'certification', 'cloud', 'computing', 'well', '##e', '##qui', '##pped', 'successful', 'career', 'tech', 'industry', 'given', 'interest', 'cloud', 'computing', 'could', 'ex', '##cel', 'roles', 'cloud', 'solutions', 'architect', 'cloud', 'engineer', 'de', '##vo', '##ps', 'engineer', 'leverage', 'skills', 'cloud', 'technologies', 'design', 'implement', 'manage', 'cloud', 'infrastructure', 'organizations', 'recommend', 'continuing', 'enhance', 'expertise', 'cloud', 'computing', 'pursuing', 'advanced', 'certification', '##s', 'like', 'a', '##ws', 'certified', 'solutions', 'architect', 'micro', '##so', '##ft', 'certified', 'a', '##zure', 'solutions', 'architect', 'expert', 'stand', 'competitive', 'tech', 'market', 'additionally', 'gaining', 'experience', 'real', '##world', 'cloud', 'projects', 'internship', '##s', 'freelance', 'opportunities', 'boost', 'profile', 'keep', 'networkin</p>
<p>I am tring this code using bert-tokenize
seq_length=200</p>
<pre><code>tokens= tokenizer(df['answer'].tolist(),
                 max_length=seq_length,
                truncation=True,
                padding='max_length',
                add_special_tokens=True,
                return_tensors='np')
</code></pre>
<p>tokens have input_ids where i have seen some tokens are divided to subtokens and it may effect the whole results,
I want to expect only one token instead of sub tokens</p>
",Preprocessing of the text & Tokenization,bert tokenizer tokenize sentence overcome remove sign using ber tokenizer token length token cl educational background computer application master degree software along strong skill p yt hon sq l j ava l g certification cloud computing well e qui pped successful career tech industry given interest cloud computing could ex cel role cloud solution architect cloud engineer de vo p engineer leverage skill cloud technology design implement manage cloud infrastructure organization recommend continuing enhance expertise cloud computing advanced certification like w certified solution architect micro ft certified zure solution architect expert stand competitive tech market additionally gaining experience real world cloud project internship freelance opportunity boost profile keep networkin tring code using bert tokenize seq length token input id seen token divided subtokens may effect whole result want expect one token instead sub token
Calculating weighted cosine similarity between vectors of words,"<p>I have two word lists, where each word makes up a topic, and has a tf-idf weight for that topic:</p>
<pre><code>topic1 = [('blue',.1), ('red',.05), ('sky',.01)]
topic2 = [('water',.5), ('fire',.1), ('earth',.02)]
</code></pre>
<p>I am trying to calculate the cosine similarity between the vectors, but also account for the tf-idf weighting of each word.</p>
<p>Is there a commonly accepted way to account for individual weightings when doing vector similarity? How would I implement this?</p>
",Preprocessing of the text & Tokenization,calculating weighted cosine similarity vector word two word list word make topic ha tf idf weight topic trying calculate cosine similarity vector also account tf idf weighting word commonly accepted way account individual weighting vector similarity would implement
Filtering stop words out of a multiple text files (using a list of stop words),"<p>I have a folder named <strong>cleaned_texts</strong>. The folder contains text files(a.txt, b.txt, c.txt etc) and each text file contains tokenized words in this format:<strong>['Rise', 'of', 'e-health', 'and', 'its', 'Germany', 'dollar']</strong>.</p>
<p>Example:</p>
<p>a.txt contains <strong>['Rise', 'of', 'e-health', 'and', 'its', 'Thailand', 'YEN', 'India']</strong> and</p>
<p>b.txt contains <strong>['PESO', 'Man', 'development', 'never', 'Japan', 'year', 'date', 'Canada']</strong>.</p>
<p>I also have another folder named <strong>StopWords</strong> which also contains text files and each text file contains a stop word. The text files are named in this format (currency.txt, names.txt, geographic.txt etc).</p>
<p>Example:</p>
<p>currency.txt contains names of currencies <strong>(Eg: BAHT | Thailand, PESO  | Mexico, YEN | Japan etc)</strong>.</p>
<p>geographic.txt contains names of countries <strong>(Eg: Canada, China, India, Germany etc)</strong>.</p>
<p>I want to filter all the stop words contained in the text files inside the StopWords folder, from all the text files in the cleaned_texts folder.</p>
<p>I looped through the stop words folder, Combined all the stop words and converted it to a list. My challenge is how to filter the stop words from my cleaned_texts files. I have been on it for days now but i couldn't figure out how to do it.</p>
<p>Here is my script:</p>
<pre><code>import glob
import codecs
import os

#Cleaned texts
os.getcwd()
clean_texts_folder =  os.path.join(os.getcwd(), 'cleaned_texts')

clean_text_data = []
for root, folders, files in os.walk(clean_texts_folder):
    for file in files:
        path = os.path.join(root, file)
        with codecs.open(path, encoding='utf-8', errors='ignore') as info:
            clean_text_data.append(info.read())


#Stop Words
stopwords_folder_path = &quot;StopWords&quot;
stopwords_files = glob.glob(os.path.join(stopwords_folder_path, '*.txt'))

for file in stopwords_files:
    with open(file, 'r') as w:
        stop_words = w.read()
        
        map_dict = {'|': ''}
        res = ''.join(
            idx if idx not in map_dict else map_dict[idx] for idx in stop_words)
        new_list = res.split()

#new_list Output= ['SMITH', 'Surnames', 'from', '1990', 'Thailand', 'YEN', 'India', 'PESO', 'Japan', 'Canada']


#Trying to save the filtered texts
folder_name = &quot;new_texts&quot;
Path(folder).mkdir(parents=True, exist_ok=True)
filtered_sentence = []
for index, word in enumerate(clean_text_data):
    if word not in new_list:
        #print(filtered_sentence.append(word))
        file_path = Path(folder_name, f&quot;{index}.txt&quot;)
        with pathlib.Path.open(file_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
           f.write(f&quot;{filtered_sentence }&quot;)

</code></pre>
<p><strong>Actual/Resulting Output:</strong>
&quot;None&quot; is printing in all the text files.</p>
<p>a.txt = None</p>
<p>b.txt = None</p>
<p>c.txt = None</p>
<p><strong>Expected Output:</strong></p>
<p>a.txt = ['Rise', 'of', 'e-health', 'and', 'its']</p>
<p>b.txt = ['Man', 'development', 'never','year', 'date']</p>
",Preprocessing of the text & Tokenization,filtering stop word multiple text file using list stop word folder named cleaned text folder contains text file txt b txt c txt etc text file contains tokenized word format rise e health germany dollar example txt contains rise e health thailand yen india b txt contains peso man development never japan year date canada also another folder named stopwords also contains text file text file contains stop word text file named format currency txt name txt geographic txt etc example currency txt contains name currency eg baht thailand peso mexico yen japan etc geographic txt contains name country eg canada china india germany etc want filter stop word contained text file inside stopwords folder text file cleaned text folder looped stop word folder combined stop word converted list challenge filter stop word cleaned text file day figure script actual resulting output none printing text file txt none b txt none c txt none expected output txt rise e health b txt man development never year date
Why is my word lemmatization not working as expected?,"<p>Hi stackoverflow community!
Long-time reader but first-time poster. I'm currently trying my hand at NLP and after reading a few forum posts touching upon this topic, I can't seem to get the lemmatizer to work properly (function pasted below). Comparing my original text vs preprocessed text, all the cleaning steps work as expected, except the lemmatization. I've even tried specifying the part of speech : 'v' to not default the word as noun, and still get the base form of the verb (ex: turned -&gt; turn , are -&gt; be, reading -&gt; read) ... however this doesn't seem to be working.</p>
<p>Appreciate another set of eyes and feedback - thanks!</p>
<pre><code># key imports

import pandas as pd
import numpy as np
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from string import punctuation
from nltk.stem import WordNetLemmatizer
import contractions


# cleaning functions

def to_lower(text):
    '''
    Convert text to lowercase
    '''
    return text.lower()

def remove_punct(text):
    return ''.join(c for c in text if c not in punctuation)

def remove_stopwords(text):
    '''
    Removes stop words which don't have meaning (ex: is, the, a, etc.)
    '''
    additional_stopwords = ['app']

    stop_words = set(stopwords.words('english')) - set(['not','out','in']) 
    stop_words = stop_words.union(additional_stopwords)
    return ' '.join([w for w in nltk.word_tokenize(text) if not w in stop_words])

def fix_contractions(text):
    '''
    Expands contractions
    '''
    return contractions.fix(text)



# preprocessing pipeline

def preprocess(text):
    # convert to lower case
    lower_text = to_lower(text)
    sentence_tokens = sent_tokenize(lower_text)
    word_list = []      
            
    for each_sent in sentence_tokens:
        # fix contractions
        clean_text = fix_contractions(each_sent)
        # remove punctuation
        clean_text = remove_punct(clean_text)
        # filter out stop words
        clean_text = remove_stopwords(clean_text)
        # get base form of word
        wnl = WordNetLemmatizer()
        for part_of_speech in ['v']:
            lemmatized_word = wnl.lemmatize(clean_text, part_of_speech)
        # split the sentence into word tokens
        word_tokens = word_tokenize(lemmatized_word)
        for i in word_tokens:
            word_list.append(i)                     
    return word_list

# lemmatize not properly working to get base form of word
# ex: 'turned' still remains 'turned' without returning base form 'turn'
# ex: 'running' still remains 'running' without getting base form 'run'



sample_data = posts_with_text['post_text'].head(5)
print(sample_data)
sample_data.apply(preprocess)
</code></pre>
",Preprocessing of the text & Tokenization,word lemmatization working expected hi stackoverflow community long time reader first time poster currently trying hand nlp reading forum post touching upon topic seem get lemmatizer work properly function pasted comparing original text v preprocessed text cleaning step work expected except lemmatization even tried specifying part speech v default word noun still get base form verb ex turned turn reading read however seem working appreciate another set eye feedback thanks
tokenize multilple files python,"<p>I am currently trying to attempting to tokenize large text, however I have a lot of files in the directory that I want to tokenize as this is very time consuming to do 1 by 1.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch


tokenizer = AutoTokenizer.from_pretrained(&quot;joeddav/distilbert-base-uncased-go-emotions-student&quot;)
model = AutoModelForSequenceClassification.from_pretrained(&quot;joeddav/distilbert-base-uncased-go-emotions-student&quot;)

txt=&quot;....&quot;

words_input_dir = &quot;/content/sample_data/&quot;

for filename in os.listdir(words_input_dir):
    if filename.endswith(&quot;.txt&quot;):
        with open(filename, &quot;r&quot;) as input_file:
            input_tokens = word_tokensize(input_file.read())

tokens = tokenizer.encode_plus(input_file.read(), add_special_tokens = False, return_tensors = 'pt')

print(len(tokens))
</code></pre>
<p>tokens</p>
<p>before I added the loop the original read</p>
<pre><code>tokens = tokenizer.encode_plus(txt, add_special_tokens = False, return_tensors = 'pt')
</code></pre>
<p>Regards,</p>
<p>I tried to loop the tokens function however it appears to only be taking specific printed text.</p>
",Preprocessing of the text & Tokenization,tokenize multilple file python currently trying attempting tokenize large text however lot file directory want tokenize time consuming token added loop original read regard tried loop token function however appears taking specific printed text
I cannot get past data(stop_words) to analyze text in text mining,"<p>It's my first attempt at text mining and I have run into a wall. This is what I have done thus far:</p>
<pre class=""lang-r prettyprint-override""><code>library(tm)
library(tidytext)
library(dplyr)
library(ggplot2)

text1 &lt;- c(&quot;Dear land of Guyana, of rivers and plains,
Made rich by the sunshine, and lush by the rains,
Set gem-like and fair between mounts and sea-
Your children salute you. dear land of the free.
Green land of Guyana, our heroes of yore,
Both bondsman and free, laid their bones on your shore,
This soil so they hallowed, and from them are we,
All sons of one mother, Guyana the free
Great land of Guyana, diverse though our strains,
We are born of their sacrifice, heirs of their pains,
And ours is the glory their eyes did not see –
One Land of six peoples, united and free.
Dear Land of Guyana, to you will we give
Our homage, our service each day that we live;
God guard you, great Mother, and make us to be
More worthy our heritage – land of the free.&quot;)

text1 
newtext1 &lt;- data_frame(line = 1:16, text = text1)
newtext1

newtext1 %&gt;%
  unnest_tokens(word, text)

data(stop_words)

newtext1 &lt;- newtext1 %&gt;%
  anti_join(newtext1)

newtext1 %&gt;%
  count(newtext1, sort = TRUE)
</code></pre>
<p>I have not been able to move forward from <code>data(stop_words)</code>. Thanks in advance.</p>
<p>Rohan</p>
",Preprocessing of the text & Tokenization,get past data stop word analyze text text mining first attempt text mining run wall done thus far able move forward thanks advance rohan
R Tidymodels textrecipes - tokenizing with spacyR - how to remove punctuations from produced list of tokens,"<p>I would like to tokenize my text by using the step_tokenize with the spacyR engine before proceeding to lemmatisation using step_lemma. Following that, i would like to remove for example punctuations from the list of tokens.</p>
<p>When using the default tokenizers::tokenize_words you can pass this option through a list of options in step_tokenize().</p>
<p>However, my understanding is that step_tokenize uses spacy_parse on the backend which does not provide such an option.</p>
<p>Is there a way to remove for e.g. punctuations or numeric tokens from the tokens produced after lemmatisation using step_lemma()?</p>
<p>A reprex:</p>
<pre><code>library(tidyverse)
library(tidymodels)
library(textrecipes)
library(spacyr)

text = &quot;It was a day, Tuesday. It wasn't Thursday!&quot;

df &lt;- tibble(text)

spacyr::spacy_initialize(entity = FALSE)

lexicon_features_tokenized_lemmatised &lt;-
  recipe(~ text, data = df%&gt;%head(1)) %&gt;%
  step_tokenize(text, engine = &quot;spacyr&quot;) %&gt;%
  step_lemma(text) %&gt;%
  prep() %&gt;%
  bake(new_data = NULL) 

lexicon_features_tokenized_lemmatised %&gt;% pull(text) %&gt;%textrecipes:::get_tokens()
</code></pre>
<p>Output:
&quot;it&quot;, &quot;be&quot;, &quot;a&quot;, &quot;day&quot;, &quot;,&quot;, &quot;Tuesday&quot;, &quot;.&quot;, &quot;it&quot;, &quot;be&quot;, &quot;not&quot;, &quot;Thursday&quot;, &quot;!&quot;</p>
<p>Desired output (Removal of &quot;!&quot;, &quot;,&quot; and &quot;.&quot;):
&quot;it&quot;, &quot;be&quot;, &quot;a&quot;, &quot;day&quot;, &quot;Tuesday&quot;, &quot;it&quot;, &quot;be&quot;, &quot;not&quot;, &quot;Thursday&quot;</p>
",Preprocessing of the text & Tokenization,r tidymodels textrecipes tokenizing spacyr remove punctuation produced list token would like tokenize text using step tokenize spacyr engine proceeding lemmatisation using step lemma following would like remove example punctuation list token using default tokenizers tokenize word pas option list option step tokenize however understanding step tokenize us spacy parse backend doe provide option way remove e g punctuation numeric token token produced lemmatisation using step lemma reprex output day tuesday thursday desired output removal day tuesday thursday
Out of RAM while tokenize a 12GB xml with SpaCy,"<p>I'm trying to tokenize a 12GB of text in xml. The file contains only &quot;content words&quot; with no stopwords. I am trying to implement a function in order to tokenize by chunks of text and clean the RAM. (I have a core i7 and 32GB of RAM)</p>
<p>I have tried the following without success;</p>
<pre class=""lang-py prettyprint-override""><code>from spacy.util import minibatch
import spacy

nlp = spacy.load(&quot;es_core_news_sm&quot;)
nlp.disable_pipes('tok2vec', 'morphologizer', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner')

def tokenizer(corpus_file):
    all_tokens = []

    with open(corpus_file, 'r', encoding='utf-8') as file:
        for batch in minibatch(file, size=100):
            docs = nlp.pipe(batch)
            for doc in docs:
                tokens = [token.text for token in doc if not token.is_space]
                all_tokens.extend(tokens)
    return all_tokens
</code></pre>
",Preprocessing of the text & Tokenization,ram tokenize gb xml spacy trying tokenize gb text xml file contains content word stopwords trying implement function order tokenize chunk text clean ram core gb ram tried following without success
How to create table of contents using unstructured (the python package),"<h2>tl;dr</h2>
<p>How can I extract a clean table of contents from a pdf document that has hierarchical section headers using the <a href=""https://unstructured.io/"" rel=""nofollow noreferrer""><code>unstructured</code></a> package?</p>
<h2>Some more details</h2>
<p>I have a pdf document that is multiple pages long. The text in the document is organised into multiple sections, each with a header/title. Each of these sections are potentially split up into subsections with their own header/title. These subsections can have subsubsections, etc.</p>
<p>The document does not have a table of contents page. How can I use the <a href=""https://unstructured.io/"" rel=""nofollow noreferrer""><code>unstructured</code></a> package to automatically extract a table of contents from my document? The table of contents should have the same hierarchy as the sections and subsections in my document.</p>
<h2>Example</h2>
<p>If my document looks like this:</p>
<blockquote>
<p><strong>This is the title of section 1</strong></p>
<p>Bla bla bla.</p>
<p><strong>This is the title of subsection 1.1</strong></p>
<p>More bla bla bla.</p>
<p><strong>This is the title of subsubsection 1.1.1</strong></p>
<p>More bla bla bla.</p>
<p><strong>This is the title of subsection 1.2</strong></p>
<p>More bla bla bla.</p>
<p><strong>This is the title of section 2</strong></p>
<p>Even more bla bla.</p>
</blockquote>
<p>Then I would like to extract a table of contents from this that includes the hierarchy of headers. For example:</p>
<pre class=""lang-py prettyprint-override""><code>{
    &quot;This is the title of section 1&quot;: 0,
    &quot;This is the title of subsection 1.1&quot;: 1,
    &quot;This is the title of subsubsection 1.1.1&quot;: 2,
    &quot;This is the title of subsection 1.2&quot;: 1,
    &quot;This is the title of section 2&quot;: 0,
}
</code></pre>
<p>Where the number indicates the level of the header in the hierarchy.</p>
",Preprocessing of the text & Tokenization,create table content using unstructured python package tl dr extract clean table content pdf document ha hierarchical section header using package detail pdf document multiple page long text document organised multiple section header title section potentially split subsection header title subsection subsubsections etc document doe table content page use package automatically extract table content document table content hierarchy section subsection document example document look like title section bla bla bla title subsection bla bla bla title subsubsection bla bla bla title subsection bla bla bla title section even bla bla would like extract table content includes hierarchy header example number indicates level header hierarchy
Cleaning text data that has extra whitespaces between letters of a word,"<p>Using R, I have read text from PDFs, and some words were read in with a space within the word, and I can't find any way to clean it.</p>
<p>For example, I need to turn &quot;Then I cre ated a c h a r t using Excel&quot; into &quot;Then I created a chart using Excel&quot;</p>
<p>The problem is that text analysis tools first separate words into tokens (separate rows) based on where spaces occur with the assumption that a space denotes a new word.</p>
<pre><code>library(tidytext)
library(dplyr)

## Example dataframe of text strings with erroneous spaces within words
txt &lt;- structure(list(id = 1:3, 
                      data = c(&quot;Then I cre ated a c h a r t using Excel&quot;, 
                               &quot;other p r inc ip le is that when a person&quot;, 
                               &quot;M R . C O O K S O N : Mr . Speaker , on behal f of&quot;)), 
                 class = &quot;data.frame&quot;, 
                 row.names = c(NA, -3L))


## Unnest text data so each word becomes a separate row in the dataframe
result &lt;- tidytext::unnest_tokens(tbl = txt,
                        output = text, 
                        input = data, 
                        token = &quot;words&quot;, 
                        to_lower = F)

print(result)

   id    text
1   1    Then
2   1       I
3   1     cre
4   1    ated
5   1       a
6   1       c
7   1       h
8   1       a
9   1       r
10  1       t
11  1   using
12  1   Excel
...
</code></pre>
<p>Is there a way to collapse all words with extra whitespaces <em><strong>within</strong></em> them without collapsing white space that occurs <em><strong>between</strong></em> words? All I can think is some kind of function that would:</p>
<ul>
<li><p>find occurrences of single characters with whitespace on either side (besides &quot;a&quot; or &quot;I&quot;)</p>
</li>
<li><p>remove whitespace from either side of the character, but not if that space is between the character and a whole word (i.e., a string of two or more characters)</p>
</li>
</ul>
<p>Maybe something like: if you find a letter besides &quot;a&quot; or &quot;I&quot; with a space on both sides, which is followed by a single letter that is not &quot;a&quot; or &quot;I&quot; followed by a space, then remove that first space?</p>
<p>I tried:</p>
<pre><code>txt &lt;- structure(list(id = 1:3, 
                      data = c(&quot;Then I cre ated a c h a r t using Excel&quot;, 
                               &quot;other p r inc ip le is that when a person&quot;, 
                               &quot;M R . C O O K S O N : Mr . Speaker , on behal f of&quot;)), 
                 class = &quot;data.frame&quot;, 
                 row.names = c(NA, -3L))

df1 &lt;- txt %&gt;%
  mutate(revised = gsub(&quot;([A-Za-z])\\s(?=[A-Za-z]\\b)&quot;, &quot;\\1&quot;, data, perl = TRUE))

df2 &lt;- txt %&gt;%
  mutate(revised = gsub(&quot;(?&lt;=\\b\\w)\\s(?=\\w\\b)&quot;, &quot;&quot;, data, perl=T))
</code></pre>
",Preprocessing of the text & Tokenization,cleaning text data ha extra whitespaces letter word using r read text pdfs word read space within word find way clean example need turn cre ated c h r using excel created chart using excel problem text analysis tool first separate word token separate row based space occur assumption space denotes new word way collapse word extra whitespaces within without collapsing white space occurs word think kind function would find occurrence single character whitespace either side besides remove whitespace either side character space character whole word e string two character maybe something like find letter besides space side followed single letter followed space remove first space tried
How to prevent DataCollatorForLanguageModelling from using input_ids as labels in CLM tasks?,"<p>How to instruct<code>DataCollatorForLanguageModeling</code> to not use shifted inputs as labels but my own labels?</p>
<p>Here's a MWE:</p>
<pre><code>data = {
'sources': [&quot;This is some text&quot;, &quot;Another text athta ljdlsfjsdlf&quot;, &quot;Also some bulshit type text who knows wtf?&quot;],
'targets': [&quot;Some potential target.&quot;, &quot;The answer is JoLo!&quot;, &quot;Who killed margaret and what was the motive and poential causes!&quot;]
}

tokenizer = AutoTokenizer.from_pretrained(&quot;openai-community/gpt2&quot;)
config = AutoConfig.from_pretrained(&quot;openai-community/gpt2&quot;)
gpt2model = AutoModelForCausalLM.from_config(config)
tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

&gt;&gt; &quot;Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained&quot;


tokenized_data = tokenizer(data['sources'])
with tokenizer.as_target_tokenizer():
    tokenized_data['labels'] = tokenizer(data['targets'])

&gt;&gt; tokenized_data
{'input_ids': [[1212, 318, 617, 2420], [6610, 2420, 379, 4352, 64, 300, 73, 67, 7278, 69, 8457, 67, 1652], [7583, 617, 4807, 16211, 2099, 2420, 508, 4206, 266, 27110, 30]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],
'labels': {'input_ids': [[4366, 2785, 2496, 13], [464, 3280, 318, 5302, 27654, 0], [8241, 2923, 6145, 8984, 290, 644, 373, 262, 20289, 290, 745, 1843, 5640, 0]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}}


tokenized_labels = tokenized_data.pop('labels')

outputs = data_collator(tokenized_data)

&gt;&gt; outputs
{'input_ids': tensor([[ 1212,   318,   617,  2420, 50257, 50257, 50257, 50257, 50257, 50257,
         50257, 50257, 50257],
        [ 6610,  2420,   379,  4352,    64,   300,    73,    67,  7278,    69,
          8457,    67,  1652],
        [ 7583,   617,  4807, 16211,  2099,  2420,   508,  4206,   266, 27110,
            30, 50257, 50257]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]), 'labels': tensor([[ 1212,   318,   617,  2420,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100],
        [ 6610,  2420,   379,  4352,    64,   300,    73,    67,  7278,    69,
          8457,    67,  1652],
        [ 7583,   617,  4807, 16211,  2099,  2420,   508,  4206,   266, 27110,
            30,  -100,  -100]])}

</code></pre>
<p>Now the <code>outputs['labels']</code> are just the shifted <code>outputs['input_ids']</code> which is happening automatically from the <code>DataColaltorForLanguageModeling</code>.</p>
<p>The question is since I do have proper labels for the data, in this case <code>tokenized_data['labels']</code> or the variable <code>tokenized_labels</code>, how do I use that in the Trainer class?</p>
<p>So, the <code>dataset.map(...)</code> will tokenize the whole dataset and will return tokens for both <code>text</code> and <code>labels</code>.</p>
<p>Then using <code>data_collator</code> will create <code>labels</code> by shifting the <code>input_ids</code> and feed that to the model.</p>
<p>How do I tell <code>Trainer</code> or <code>DataCollator</code> to use my <code>tokenized_labels</code> instead of creating them based on the inputs?</p>
",Preprocessing of the text & Tokenization,prevent datacollatorforlanguagemodelling using input id label clm task instruct use shifted input label label mwe shifted happening automatically question since proper label data case variable use trainer class tokenize whole dataset return token using create shifting feed model tell use instead creating based input
Extracting difficult words from text,"<p>I need to identify difficult words from input text. I do not want to use common word lists because the level of difficulty will need to be set for children. Is there a scoring mechanism that computes the difficulty level of each word? I can use a threshold for score to separate difficult words from easy ones. The end objective is to provide word meanings for all such difficult words.</p>
<p>There are several ways in which overall text can be scored for complexity or difficulty level, eg. The Dale–Chall formula, The Gunning fog formula, etc. However, these are used for defining &quot;readability&quot; i.e. the ease with which a reader can understand the written text. My requirement is related to the difficulty level of individual words in a text.</p>
<p>There are a few methods that I have come across for defining difficult words such words with more than 2 syllables or any word not appearing in the 10000 most common words, etc. However, none of these methods are useful for me. I am trying to build an application that can identify difficult words, and provides relevant dictionary meanings for only those words. Is there a scoring mechanism that will allow me to use a threshold to separate the difficult words from the easy ones?</p>
",Preprocessing of the text & Tokenization,extracting difficult word text need identify difficult word input text want use common word list level difficulty need set child scoring mechanism computes difficulty level word use threshold score separate difficult word easy one end objective provide word meaning difficult word several way overall text scored complexity difficulty level eg dale chall formula gunning fog formula etc however used defining readability e ease reader understand written text requirement related difficulty level individual word text method come across defining difficult word word syllable word appearing common word etc however none method useful trying build application identify difficult word provides relevant dictionary meaning word scoring mechanism allow use threshold separate difficult word easy one
What is the difference between lemmatization vs stemming?,"<p>When do I use each ?</p>

<p>Also...is the NLTK lemmatization dependent upon Parts of Speech?
Wouldn't it be more accurate if it was?</p>
",Preprocessing of the text & Tokenization,difference lemmatization v stemming use also nltk lemmatization dependent upon part speech accurate wa
Error in unit testing on pre-processing raw data,"<pre><code>import pandas as pd
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
import nltk

nlp = spacy.load(&quot;en_core_web_md&quot;)

class fileread:
    def readfile(self):
        file_path = 'C:\\Users\\Documents\\Emails\\DEP72303-SYSOUT.txt'

        with open(file_path, 'r') as text:
            return text.read()
class preprocess:
    fi=fileread()
    def remove(self):
        return re.sub('^[\sA-Za-z0-9]', '',self.fi.readfile())

    def preprocess(self):
        doc = nlp(self.remove())
        filtered = []
        for token in doc:
            if token.is_stop or token.is_punct:
                continue

            filtered.append(token.lemma_)

        return &quot; &quot;.join(filtered)
pre=preprocess()
output=pre.preprocess()
tokens=output.split(&quot;\n&quot;)
print(tokens)
</code></pre>
<p>This code is take a reads a file which has raw data and pre-processing is done using regex expression, removing stop words and punctuation and in the last lemmatization is done.</p>
<h1>Unit testing on the above program is:</h1>
<pre><code>import unittest
import os  # Make sure this line is present
from preprocessing import preprocess

class MockFileRead:
    # Mocking the file read class to use the temporary file
    def readfile(self):
        with open('test_input.txt', 'r') as text:
            return text.read()

class TestPreprocess(unittest.TestCase):
    def test_preprocess(self):
        # Assuming you have a sample input file with known content for testing
        sample_input = &quot;Sample text content for testing purposes.&quot;

        # Create a temporary file for testing
        with open('test_input.txt', 'w') as temp_file:
            temp_file.write(sample_input)

        preprocess_instance = preprocess()
        preprocess_instance.fi = MockFileRead()

        # Expected output after preprocessing
        expected_output = &quot;sample text content test purpose&quot;

        self.assertEqual(preprocess_instance.preprocess(), expected_output)

        # Clean up: Remove the temporary file
        os.remove('test_input.txt')

if __name__ == 'main':
    unittest.main()

</code></pre>
<p>In the above program my expected output is: sample text content test purpose
but the output I got is: ample text content testing purpose</p>
<p>How do I resolve the code and get the correct output?</p>
",Preprocessing of the text & Tokenization,error unit testing pre processing raw data code take read file ha raw data pre processing done using regex expression removing stop word punctuation last lemmatization done unit testing program program expected output sample text content test purpose output got ample text content testing purpose resolve code get correct output
Identifying near-identical sections of text across documents in Python,"<p>I need a method to clean up an arbitrary set of emails by identifying identical or near-identical footers that occur in many of them. These could be signatures, company boilerplate, etc. Before I write something up myself, I was wondering if there any off-the-shelf solution? Most of the deduplication libraries I've found are document-level.</p>
",Preprocessing of the text & Tokenization,identifying near identical section text across document python need method clean arbitrary set email identifying identical near identical footer occur many could signature company boilerplate etc write something wa wondering shelf solution deduplication library found document level
How to remove stop phrases/stop ngrams (multi-word strings) using pandas/sklearn?,"<p>I want to prevent certain phrases for creeping into my models. For example, I want to prevent 'red roses' from entering into my analysis. I understand how to add individual stop words as given in <a href=""https://stackoverflow.com/questions/24386489/adding-words-to-scikit-learns-countvectorizers-stop-list"">Adding words to scikit-learn&#39;s CountVectorizer&#39;s stop list</a> by doing so:</p>

<pre><code>from sklearn.feature_extraction import text
additional_stop_words=['red','roses']
</code></pre>

<p>However, this also results in other ngrams like 'red tulips' or 'blue roses' not being detected. </p>

<p>I am building a TfidfVectorizer as part of my model, and I realize the processing I need might have to be entered after this stage but I am not sure how to do this.</p>

<p>My eventual aim is to do topic modelling on a piece of text. Here is the piece of code (borrowed almost directly from <a href=""https://de.dariah.eu/tatom/topic_model_python.html#index-0"" rel=""nofollow noreferrer"">https://de.dariah.eu/tatom/topic_model_python.html#index-0</a> ) that I am working on:</p>

<pre><code>from sklearn import decomposition

from sklearn.feature_extraction import text
additional_stop_words = ['red', 'roses']

sw = text.ENGLISH_STOP_WORDS.union(additional_stop_words)
mod_vectorizer = text.TfidfVectorizer(
    ngram_range=(2,3),
    stop_words=sw,
    norm='l2',
    min_df=5
)

dtm = mod_vectorizer.fit_transform(df[col]).toarray()
vocab = np.array(mod_vectorizer.get_feature_names())
num_topics = 5
num_top_words = 5
m_clf = decomposition.LatentDirichletAllocation(
    n_topics=num_topics,
    random_state=1
)

doctopic = m_clf.fit_transform(dtm)
topic_words = []

for topic in m_clf.components_:
    word_idx = np.argsort(topic)[::-1][0:num_top_words]
    topic_words.append([vocab[i] for i in word_idx])

doctopic = doctopic / np.sum(doctopic, axis=1, keepdims=True)
for t in range(len(topic_words)):
    print(""Topic {}: {}"".format(t, ','.join(topic_words[t][:5])))
</code></pre>

<p><strong>EDIT</strong></p>

<p>Sample dataframe (I have tried to insert as many edge cases as possible),  df:</p>

<pre><code>   Content
0  I like red roses as much as I like blue tulips.
1  It would be quite unusual to see red tulips, but not RED ROSES
2  It is almost impossible to find blue roses
3  I like most red flowers, but roses are my favorite.
4  Could you buy me some red roses?
5  John loves the color red. Roses are Mary's favorite flowers.
</code></pre>
",Preprocessing of the text & Tokenization,remove stop phrase stop ngrams multi word string using panda sklearn want prevent certain phrase creeping model example want prevent red rose entering analysis understand add individual stop word given working edit sample dataframe tried insert many edge case possible df
"slurm didn&#39;t execute my Python code after running a few lines but also didn&#39;t stop, whereas it worked well on my local Linux","<p><strong>my code:</strong></p>
<pre><code>from datasets import load_dataset
MAX_LEN = 512
dataset = load_dataset(&quot;glue&quot;,&quot;mrpc&quot;)
from transformers import AutoTokenizer
from transformers import RobertaTokenizerFast

#tokenizer =AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
tokenizer = RobertaTokenizerFast.from_pretrained(&quot;/data/home//raw_roberta/Roberta_Tokenizer&quot;, max_length=MAX_LEN, padding='max_length', return_tensors='pt')

print(&quot;mapped_dataset&quot;)

mapped_dataset = dataset.map(lambda x: tokenizer(x[&quot;sentence1&quot;], x[&quot;sentence2&quot;], max_length = MAX_LEN, truncation=True, padding='max_length', return_tensors='pt'), batched=True)

print(&quot;completeed mapped_dataset&quot;)

from transformers import DataCollatorWithPadding
data_collator= DataCollatorWithPadding(tokenizer=tokenizer)

from transformers import AutoModelForSequenceClassification
from transformers import RobertaForMaskedLM

#model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;,num_labels = 2)
#model = AutoModelForSequenceClassification.from_pretrained(&quot;data/home//raw_roberta/Roberta_Model/checkpoint-90000&quot;, num_labels = 2)
#model = AutoModelForSequenceClassification.from_pretrained(&quot;data/home//raw_roberta/Roberta_Model/checkpoint-90000&quot;)
base_model = RobertaForMaskedLM.from_pretrained('/data/home//raw_roberta/Roberta_Model/checkpoint-90000').roberta

from transformers import TrainingArguments
print(base_model.config)
</code></pre>
<p><strong>I run above code on my local linux,it only takes about 2 minutes to execute.the log:</strong></p>
<p>The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization.
The tokenizer class you load from this checkpoint is 'BertTokenizer'.
The class this function is called from is 'RobertaTokenizer'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization.
The tokenizer class you load from this checkpoint is 'BertTokenizer'.
The class this function is called from is 'RobertaTokenizerFast'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</p>
<pre><code>mapped_dataset
completeed mapped_dataset
RobertaConfig {
  &quot;_name_or_path&quot;: &quot;/data/home//raw_roberta/Roberta_Model/checkpoint-90000&quot;,
  &quot;architectures&quot;: [
    &quot;RobertaForMaskedLM&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;bos_token_id&quot;: 0,
  &quot;classifier_dropout&quot;: null,
  &quot;eos_token_id&quot;: 2,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 514,
  &quot;model_type&quot;: &quot;roberta&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;pad_token_id&quot;: 1,
  &quot;position_embedding_type&quot;: &quot;absolute&quot;,
  &quot;torch_dtype&quot;: &quot;float32&quot;,
  &quot;transformers_version&quot;: &quot;4.33.2&quot;,
  &quot;type_vocab_size&quot;: 1,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 52000
}

</code></pre>
<p><strong>but when I upload my code to slurm, it run for 4 hours,only get these log:</strong></p>
<pre><code>/data/home//anaconda3/envs/py38v1/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.24.4)
  warnings.warn(f&quot;A NumPy version &gt;={np_minversion} and &lt;{np_maxversion} is required for this version of &quot;
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'BertTokenizer'. 
The class this function is called from is 'RobertaTokenizer'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'BertTokenizer'. 
The class this function is called from is 'RobertaTokenizerFast'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
slurmstepd: error: *** JOB xxxxx ON compute-9-0 CANCELLED AT 2024-02-05T08:35:30 DUE TO TIME LIMIT ***
</code></pre>
<p><strong>this problem truly confuses me,anyone know how to fix it? thanks!</strong></p>
",Preprocessing of the text & Tokenization,slurm execute python code running line also stop whereas worked well local linux code run code local linux take minute execute log tokenizer class load checkpoint type class function called may result unexpected tokenization tokenizer class load checkpoint berttokenizer class function called robertatokenizer special token added vocabulary make sure associated word embeddings fine tuned trained tokenizer class load checkpoint type class function called may result unexpected tokenization tokenizer class load checkpoint berttokenizer class function called robertatokenizerfast special token added vocabulary make sure associated word embeddings fine tuned trained upload code slurm run hour get log problem truly confuses anyone know fix thanks
How to de-normalize text in Python?,"<p>I am currently working on a Python project using text semantic to match similarities. At the end, my goal is to have a dataset column where all my interesting words are in order to be searched in by a dumb searchbar.</p>
<p>Currently I have in a column my original text and my normalized text. Is there a way to create all denormalized forms of each word (or at least for each noun) ?</p>
",Preprocessing of the text & Tokenization,de normalize text python currently working python project using text semantic match similarity end goal dataset column interesting word order searched dumb searchbar currently column original text normalized text way create denormalized form word least noun
POS-Tagger is incredibly slow,"<p>I am using <code>nltk</code> to generate n-grams from sentences by first removing given stop words. However, <code>nltk.pos_tag()</code> is extremely slow taking up to 0.6 sec on my CPU (Intel i7).</p>

<p>The output:</p>

<pre><code>['The first time I went, and was completely taken by the live jazz band and atmosphere, I ordered the Lobster Cobb Salad.']
0.620481014252
[""It's simply the best meal in NYC.""]
0.640982151031
['You cannot go wrong at the Red Eye Grill.']
0.644664049149
</code></pre>

<p>The code:</p>

<pre><code>for sentence in source:

    nltk_ngrams = None

    if stop_words is not None:   
        start = time.time()
        sentence_pos = nltk.pos_tag(word_tokenize(sentence))
        print time.time() - start

        filtered_words = [word for (word, pos) in sentence_pos if pos not in stop_words]
    else:
        filtered_words = ngrams(sentence.split(), n)
</code></pre>

<p>Is this really that slow or am I doing something wrong here?</p>
",Preprocessing of the text & Tokenization,po tagger incredibly slow using generate n gram sentence first removing given stop word however extremely slow taking sec cpu intel output code really slow something wrong
why does binary pair encoding (BPE) not work on binary data but only on natural languages?,"<p>I know that BPE is used on sentences that can generate some sort of tokenization and pairings, with spare bytes being used to create such mappings. Why doesn't it work on BPE?</p>
<p>What would be the best guess/way to actually try to perform BPE as a form of compression of binary data?</p>
",Preprocessing of the text & Tokenization,doe binary pair encoding bpe work binary data natural language know bpe used sentence generate sort tokenization pairing spare byte used create mapping work bpe would best guess way actually try perform bpe form compression binary data
Is there any best practice to prepare features for text-based classification?,"<p>We have many feedback and issue reports from customers. And they are plain texts. We are trying to build a auto classifier for these docs so <strong>future</strong> feedback/issues could be auto routed to the correct support team. Besides the text itself, I think we should include things like customer profile, case submit region, etc into the classifier. I think this could provide more clues for classifier to make better predictions.</p>
<p>Currently, all the features selected for training are based on the text content. How to include the above mentioned meta-features?</p>
<h2>ADD 1</h2>
<p>My current approach is to first do some typical pre-processing to the raw text (including title and body), such as remove the stop words, POS-tagging and extract significant words. Then I convert the title and body into a list of words and store them in some sparse format as below:</p>
<blockquote>
<p>instance 1:   word1:word1 count,  word2: word2 count, ....</p>
<p>instance 2:   wordX:word1 count,  wordY: word2 count, ....</p>
</blockquote>
<p>And for the other non-text features, I am planning to add them as new columns after the word columns. So a final instance will look like:</p>
<blockquote>
<p>instance 1: word1:word1 count, ... , feature X:value, feature Y:value</p>
</blockquote>
",Preprocessing of the text & Tokenization,best practice prepare feature text based classification many feedback issue report customer plain text trying build auto classifier doc future feedback issue could auto routed correct support team besides text think include thing like customer profile case submit region etc classifier think could provide clue classifier make better prediction currently feature selected training based text content include mentioned meta feature add current approach first typical pre processing raw text including title body remove stop word po tagging extract significant word convert title body list word store sparse format instance word word count word word count instance wordx word count wordy word count non text feature planning add new column word column final instance look like instance word word count feature x value feature value
What do spaCy&#39;s part-of-speech and dependency tags mean?,"<p>spaCy tags up each of the <code>Token</code>s in a <code>Document</code> with a part of speech (in two different formats, one stored in the <code>pos</code> and <code>pos_</code> properties of the <code>Token</code> and the other stored in the <code>tag</code> and <code>tag_</code> properties) and a syntactic dependency to its <code>.head</code> token (stored in the <code>dep</code> and <code>dep_</code> properties).</p>

<p>Some of these tags are self-explanatory, even to somebody like me without a linguistics background:</p>

<pre><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; en_nlp = spacy.load('en')
&gt;&gt;&gt; document = en_nlp(""I shot a man in Reno just to watch him die."")
&gt;&gt;&gt; document[1]
shot
&gt;&gt;&gt; document[1].pos_
'VERB'
</code></pre>

<p>Others... are not:</p>

<pre><code>&gt;&gt;&gt; document[1].tag_
'VBD'
&gt;&gt;&gt; document[2].pos_
'DET'
&gt;&gt;&gt; document[3].dep_
'dobj'
</code></pre>

<p>Worse, the <a href=""https://spacy.io/docs/"" rel=""noreferrer"">official docs</a> don't contain even a list of the possible tags for most of these properties, nor the meanings of any of them. They sometimes mention what tokenization standard they use, but these claims aren't currently entirely accurate and on top of that the standards are tricky to track down.</p>

<p>What are the possible values of the <code>tag_</code>, <code>pos_</code>, and <code>dep_</code> properties, and what do they mean?</p>
",Preprocessing of the text & Tokenization,spacy part speech dependency tag mean spacy tag part speech two different format one stored property stored property syntactic dependency token stored property tag self explanatory even somebody like without linguistics background others worse official doc contain even list possible tag property meaning sometimes mention tokenization standard use claim currently entirely accurate top standard tricky track possible value property mean
why Argument of type &quot;Dataset&quot; cannot be assigned to parameter &quot;train_dataset&quot; of type &quot;Dataset[Unknown]?,"<blockquote>
<p>I am trying to finetune a large language model. and when i give train and test dataset to the model it shows the above error and there is red line under train_dataset and test_dataset.<strong>Following is the lines where i have problem</strong></p>
</blockquote>
<pre><code>trainer = Trainer(
model=base_model,
args=training_args,
train_dataset=tokenized_train_data,
eval_dataset=tokenized_test_data
  )
</code></pre>
<blockquote>
<p><strong>This is how i am mapping and preprocess the data</strong>
<em>My dataset is in dataframe so</em></p>
</blockquote>
<pre><code>train_dataset, test_dataset = train_test_split(concatinate_dataset_refined, test_size=0.2, random_state=132)
</code></pre>
<blockquote>
<p>Converting into Datasets.</p>
</blockquote>
<pre><code>train_dataset = Dataset.from_pandas(train_dataset)
test_dataset = Dataset.from_pandas(test_dataset)
</code></pre>
<blockquote>
<p>Tokenising</p>
</blockquote>
<pre><code>def tokenize_function(example):
merged = example[&quot;title&quot;] + &quot; &quot; + example[&quot;story&quot;]
batch = tokenizer(merged, padding='max_length', truncation=True, max_length=128)
batch[&quot;labels&quot;] = batch[&quot;input_ids&quot;].copy()
return batch  

# Apply it to our dataset, and remove the text columns
</code></pre>
<p>tokenized_train_data = train_dataset.map(tokenize_function, remove_columns=[&quot;title&quot;, &quot;story&quot;])</p>
<p>tokenized_test_data = test_dataset.map(tokenize_function, remove_columns=[&quot;title&quot;, &quot;story&quot;])</p>
<p><strong>Even i run the code it model runs okay but does not generate any texts, just the commas.</strong></p>
<blockquote>
<p>Thank you for your time guys.</p>
</blockquote>
",Preprocessing of the text & Tokenization,argument type dataset assigned parameter train dataset type dataset unknown trying finetune large language model give train test dataset model show error red line train dataset test dataset following line problem mapping preprocess data dataset dataframe converting datasets tokenising tokenized train data train dataset map tokenize function remove column title story tokenized test data test dataset map tokenize function remove column title story even run code model run okay doe generate text comma thank time guy
How to get token ids using spaCy (I want to map a text sentence to sequence of integers),"<p>I want to use spacy to tokenize sentences to get a sequence of integer token-ids that I can use for downstream tasks. I expect to use it something like below. Please fill in <code>???</code></p>
<pre><code>import spacy

# Load English tokenizer, tagger, parser, NER and word vectors
nlp = spacy.load('en_core_web_lg')

# Process whole documents
text = (u&quot;When Sebastian Thrun started working on self-driving cars at &quot;)

doc = nlp(text)

idxs = ???

print(idxs)
</code></pre>
<p>I want the output to be something like:</p>
<blockquote>
<p>array([ 8045, 70727, 24304, 96127, 44091, 37596, 24524, 35224, 36253])</p>
</blockquote>
<p>Preferably the integers refers to some special embedding id in <code>en_core_web_lg</code>..</p>
<p>spacy.io/usage/vectors-similarity does not give a hint what attribute in doc to look for.</p>
<p>I asked this on <a href=""https://stats.stackexchange.com/questions/376011/how-to-get-token-ids-using-spacy-i-want-to-map-a-text-sentence-to-sequence-of-i"">crossvalidated</a> but it was determined as OT. Proper terms for googling/describing this problem is also helpful.</p>
",Preprocessing of the text & Tokenization,get token id using spacy want map text sentence sequence integer want use spacy tokenize sentence get sequence integer token id use downstream task expect use something like please fill want output something like array preferably integer refers special embedding id spacy io usage vector similarity doe give hint attribute doc look asked href wa determined ot proper term googling describing problem also helpful p
How do I remove stop words from an arraylist of strings in python?,"<p>I would like to remove stop words from the arraylist named arrayList1, which is stored in the data variable. 
I try the below method but it does not work. Please help me check the below codes and improve the codes.Thanks.    </p>

<pre><code>import Retrieve_ED_Notes
from nltk.corpus import stopwords

data = Retrieve_ED_Notes.arrayList1

stop_words = set(stopwords.words('english'))


def remove_stopwords(data):
 data = [word for word in data if word not in stop_words]
 return data

for i in range(0, len(remove_stopwords(data))):
  print(remove_stopwords(data[i]))
</code></pre>

<p>Console output of the arrayList1:</p>

<pre><code>1|I really love writing journals
2|The mat is very comfortable and I will buy it again likes
3|The mousepad is smooth
1|I really love writing journals
4|This pen is very special to me.
4|This pencil is very special to me.
5|Meaningful novels
4|It brights up my day like a lighter and makes me higher.
6|School foolscap
7|As soft as my heart.lovey
</code></pre>
",Preprocessing of the text & Tokenization,remove stop word arraylist string python would like remove stop word arraylist named arraylist stored data variable try method doe work please help check code improve code thanks console output arraylist
How to extract the footnote from a PDF file,"<p><img src=""https://i.sstatic.net/MyoIC.png"" alt=""enter image description here"" /></p>
<p>How do I identify and extract the footnote portion of a PDF in Python? especially when part of the footnote jumps to the second page.</p>
<p>I try to use regular expression but there is nothing that i could go off with.</p>
",Preprocessing of the text & Tokenization,extract footnote pdf file identify extract footnote portion pdf python especially part footnote jump second page try use regular expression nothing could go
NLP sentence summarization techniques with python,"<p>I am trying to make a python script that takes a sentence and summarize it in 5-7 words with keywords and details.</p>
<p>So far, I have used the nltk library to first remove any symbols and numbers, then remove all word types except nouns and verbs. I also included a function to remove all stopwords(words without value like, 'the', 'it).</p>
<p>The code I have is extremely basic and the output isn't grammatically correct or understandable. My main objective is to take a sentence like:</p>
<p>&quot;Drug stability refers to the ability of a pharmaceutical product to retain its quality, safety, and efficacy over time&quot;</p>
<p>...and turn it into:</p>
<p>&quot;Drug stability is ability to retain quality, safety, and efficacy&quot;</p>
<p>But when i run the code I get &quot;Drug stability refers ability pharmaceutical product retain quality, safety, efficacy time&quot; which isn't bad but I want to make the system able to produce more grammatically correct while still retaining major keywords. I am aware of libraries like gensin or nltk summarize but these libraries only take the important sentences of a paragraph through word frequency but this doesn't simplify single sentences. are there any other methods for sentence summarization?</p>
<p>Here is the code I have so far:</p>
<pre><code>def shortenSentence(sentence):
    #sentence = &quot;%^Regulatory scientists must take measures to guarantee that the drug remains consistent and safe from the moment of production through packaging, storage, and shipping.907&quot;
    clean_sentence = re.sub(r'[^a-zA-Z\s]', '', sentence)  # Added 0-9 and period (.)
    #print(clean_sentence)


    def remove_adj_adv(sentence):
        words = word_tokenize(sentence)
        pos_tags = pos_tag(words)
        shortened = [word for word, tag in pos_tags if tag in ['NN', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]
        return ' '.join(shortened)
    shortened = remove_adj_adv(clean_sentence)
    #print(shortened)



    words = word_tokenize(shortened)
    # Get the list of English stopwords
    stop_words = set(stopwords.words('english'))
    # Remove stopwords from the list of words
    filtered_words = [word for word in words if word.lower() not in stop_words]
    # Join the filtered words back into a sentence
    filtered_sentence = ' '.join(filtered_words)
    #print(filtered_sentence)
    return(filtered_sentence)
</code></pre>
",Preprocessing of the text & Tokenization,nlp sentence summarization technique python trying make python script take sentence summarize word keywords detail far used nltk library first remove symbol number remove word type except noun verb also included function remove stopwords word without value like code extremely basic output grammatically correct understandable main objective take sentence like drug stability refers ability pharmaceutical product retain quality safety efficacy time turn drug stability ability retain quality safety efficacy run code get drug stability refers ability pharmaceutical product retain quality safety efficacy time bad want make system able produce grammatically correct still retaining major keywords aware library like gensin nltk summarize library take important sentence paragraph word frequency simplify single sentence method sentence summarization code far
nltk python library word tokenizatio error,"<p>I am trying to tokenize a file.</p>
<pre><code>`AttributeError                            Traceback (most recent call last)
&lt;ipython-input-8-81ae6f78b554&gt; in &lt;cell line: 4&gt;()
      2 robert = open('Robert Frost.txt', 'r')
      3 robert_poem = robert.read()
----&gt; 4 robert_poem_words = nltk.word_tokenization(robert_poem)

# AttributeError: module 'nltk' has no attribute 'word_tokenization'`
</code></pre>
<p>The error is in the word_tokenization function.</p>
",Preprocessing of the text & Tokenization,nltk python library word tokenizatio error trying tokenize file error word tokenization function
.tokenize() behaviour difference in Tokenizer using various pre-trained models in HuggingFace on Chinese sentences,"<p>I am testing the functionality of Tokenizer using various pre-trained models on Chinese sentences. Here are my codes:</p>
<pre><code>from transformers import BartTokenizer, BertTokenizer

text_eng = 'I go to school by train.'
text_can = '我乘搭火車上學。'
text_chi = '我搭火車返學。'

tokenizer_bartchinese = BertTokenizer.from_pretrained('fnlp/bart-base-chinese')
tokenizer_bertchinese = BertTokenizer.from_pretrained('bert-base-chinese')
tokenizer_fb = BartTokenizer.from_pretrained('facebook/bart-large') 

# BART Chinese
print(tokenizer_bartchinese.tokenize(text_eng))
print(tokenizer_bartchinese.tokenize(text_can))
print(tokenizer_bartchinese.tokenize(text_chi))

# BERT Chinese
print(tokenizer_bertchinese.tokenize(text_eng))
print(tokenizer_bertchinese.tokenize(text_can))
print(tokenizer_bertchinese.tokenize(text_chi))

# BART Large
print(tokenizer_fb.tokenize(text_eng))
print(tokenizer_fb.tokenize(text_can))
print(tokenizer_fb.tokenize(text_chi))
</code></pre>
<p>Here are the results:</p>
<pre><code>['I', 'go', 'to', 'school', 'by', 'train', '.']
['我', '乘', '搭', '火', '車', '上', '學', '。']
['我', '搭', '火', '車', '返', '學', '。']
['[UNK]', 'go', 'to', 'school', 'by', 't', '##rain', '.']
['我', '乘', '搭', '火', '車', '上', '學', '。']
['我', '搭', '火', '車', '返', '學', '。']
['I', 'Ġgo', 'Ġto', 'Ġschool', 'Ġby', 'Ġtrain', '.']
['æĪ', 'ĳ', 'ä¹', 'ĺ', 'æ', 'Ĳ', 'Ń', 'ç', 'ģ«', 'è»', 'Ĭ', 'ä¸Ĭ', 'åŃ', '¸', 'ãĢĤ']
['æĪ', 'ĳ', 'æ', 'Ĳ', 'Ń', 'ç', 'ģ«', 'è»', 'Ĭ', 'è¿', 'Ķ', 'åŃ', '¸', 'ãĢĤ']
</code></pre>
<p>Should the Tokenizer recognize vocabulary in Chinese, such that the Tokenizer will segment the vocabulary instead of splitting each character? For example, 火車 (train) in Chinese is a vocabulary; and it should not be split into 火(fire) and 車(car).</p>
<p>My expected behaviour:</p>
<pre><code>['我', '乘搭', '火車', '上', '學', '。']
</code></pre>
<p>which translates (just for reference):</p>
<pre><code>['I', 'ride', 'train', 'to', 'school', '.']
</code></pre>
<p>Also, I noticed that <code>facebook/bart-large</code> generates weird characters (the last 3 lines of the code output). Is this a normal behaviour?</p>
",Preprocessing of the text & Tokenization,tokenize behaviour difference tokenizer using various pre trained model huggingface chinese sentence testing functionality tokenizer using various pre trained model chinese sentence code result tokenizer recognize vocabulary chinese tokenizer segment vocabulary instead splitting character example train chinese vocabulary split fire car expected behaviour translates reference also noticed generates weird character last line code output normal behaviour
Using a Word Counter in Python is understating results,"<p>As a complete preface, I am a beginner and learning. But, here's the sample schema of my products review table.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Record_ID</th>
<th style=""text-align: center;"">Product_ID</th>
<th style=""text-align: right;"">Review Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1234</td>
<td style=""text-align: center;"">89847457</td>
<td style=""text-align: right;"">I love this product it was shipped fast and is comfortable</td>
</tr>
</tbody>
</table>
</div>
<p>And here is my code. It gives me a total word count for all of the reviews, as well as another count of phrases to try and get more context...i.e. ('flimsy', 'tight') if the fit of the shirt was tight and quality was flimsy.  The script writes a new Excel doc with the counts for both.</p>
<pre><code>import pandas as pd
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
import string
from collections import Counter
from nltk.util import ngrams
import nltk
nltk.download('punkt')

df = pd.read_excel('productsvydata.xlsx')

def preprocess_text(text):
    translator = str.maketrans('', '', string.punctuation)
    text = text.lower() 
    text = text.translate(translator)
    return text

word_counts = {}
phrase_counts = {}

unique_product_ids = df[&quot;Product_ID&quot;].unique()

# Set the number of top words and phrases you want to keep
top_n = 100

for selected_product_id in unique_product_ids:
    selected_comments_df = df[df[&quot;Product_ID&quot;] == selected_product_id]
    selected_comments = ' '.join(selected_comments_df[&quot;Product Review Comment&quot;].astype(str))
    selected_comments = preprocess_text(selected_comments)
    if not selected_comments.strip():
        continue
    tokenized_words = nltk.word_tokenize(selected_comments)
    stop_words = set(ENGLISH_STOP_WORDS)
    filtered_words = [word for word in tokenized_words if word not in stop_words]
    lemmatizer = nltk.WordNetLemmatizer()
    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]
    max_phrase_length = 4
    phrases = [phrase for n in range(2, max_phrase_length + 1) for phrase in ngrams(lemmatized_words, n)]
    word_counter = Counter(lemmatized_words)
    phrase_counter = Counter(phrases)

    # Get the top N words and phrases
    top_words = dict(word_counter.most_common(top_n))
    top_phrases = dict(phrase_counter.most_common(top_n))

    # Extract record_id for each Product_ID
    record_ids = selected_comments_df[&quot;record_id&quot;].values[0]

    word_counts[(selected_product_id, record_ids)] = top_words
    phrase_counts[(selected_product_id, record_ids)] = top_phrases

word_result_data = []
phrase_result_data = []

for (product_id, record_id), top_words in word_counts.items():
    for word, count in top_words.items():
        word_result_data.append([product_id, record_id, word, count])
for (product_id, record_id), top_phrases in phrase_counts.items():
    for phrase, count in top_phrases.items():
        phrase_result_data.append([product_id, record_id, phrase, count])

word_df = pd.DataFrame(word_result_data, columns=['Product_ID', 'record_id', 'Word', 'Count'])
phrase_df = pd.DataFrame(phrase_result_data, columns=['Product_ID', 'record_id', 'Phrase', 'Count'])

word_df.to_csv('top_words_counts.csv', index=False)
phrase_df.to_csv('top_phrases_counts.csv', index=False)
</code></pre>
<p>I used top_n = 100 to just get around the top 100 words in the export because there's over 20,000 rows of data and if I Do all of the words and phrases, the thing will not run. It needs to both use product id and record id because that's what it joins onto in my work tool.</p>
<p>The issue is I feel the results are very understated. I am wondering if it has to do with tokenization. For instance, right now I have 9 instances of the word 'customer' in our data in this export. And in the phrase count, ('customer', 'service') comes up even less. If I just control F through the raw Product Review Comments in the original document, there's way more instances of people speaking about customer service. Something's going wrong in the processing, but I don't know what.</p>
<p>Would anyone be able to help suggest ways to better optimize this code as well as yield a larger number of results? It's pretty basic NLP but again, I'm new, I want to learn, but I've hit a blocker in my output.</p>
",Preprocessing of the text & Tokenization,using word counter python understating result complete preface beginner learning sample schema product review table record id product id review comment love product wa shipped fast comfortable code give total word count review well another count phrase try get context e flimsy tight fit shirt wa tight quality wa flimsy script writes new excel doc count used top n get around top word export row data word phrase thing run need use product id record id join onto work tool issue feel result understated wondering ha tokenization instance right instance word customer data export phrase count customer service come even le control f raw product review comment original document way instance people speaking customer service something going wrong processing know would anyone able help suggest way better optimize code well yield larger number result pretty basic nlp new want learn hit blocker output
When to remove stop words when using bigram_measures like PMI?,"<p>I need to verify an overall approach to dealing with bigram stop words that are returned from bigram_measures such as PMI. Why deal with these stop words? Well, they're noise and don’t add any additional value past a certain point.</p>
<p>I've seen several specific examples of how to use bigram_measures. However, I'm wondering WHEN it's best to remove stop word in the overall process of cleaning data, expansion, lemmatizing/stemming, etc.</p>
<p>And yes, I am using a corpus that is sufficiently large. I remember the size of your corpus will also affect the quality of the bigram_measures result.</p>
<p>Based on the accepted answer in this post (<a href=""https://stackoverflow.com/questions/19145332/nltk-counting-frequency-of-bigram"">NLTK - Counting Frequency of Bigram</a>) it seems that stop words could be removed after PMI or other bigram_measures are used on the corpus.</p>
<blockquote>
<p>&quot;Imagine that if filtering collocations was simply deleting them, then there were many probability measures such as liklihood ratio or the PMI itself (that compute probability of a word relative to other words in a corpus) which would not function properly after deleting words from random positions in the given corpus. By deleting some collocations from the given list of words, many potential functionalities and computations would be disabled...&quot;</p>
</blockquote>
<p>Therefore, I believe the best process is:</p>
<ol>
<li>Clean the text and remove garbage chars like HTML tags, etc.</li>
<li>Expand contractions (e.g.: they're -&gt; they are)</li>
<li>Lemmatize or stem to normalize the words</li>
<li>Calculate bigrams using bigram_measures like PMI. You can calculate bigrams using other methods, but this is what I'm using.</li>
<li>Apply a frequency filter like &quot;apply_freq_filter(N)&quot; to get the bigrams that occur above your threshold. Note this will still return some bigrams with stop words mixed in with valuable bigrams.</li>
<li>Check to see if BOTH words are stop words. If yes, then don't include that bigram in the final results but leave them in the corpus for the reasons quoted above.</li>
</ol>
<p>Is this a correct overall approach to dealing with bigram stop words mixed in with valuable bigrams?</p>
",Preprocessing of the text & Tokenization,remove stop word using bigram measure like pmi need verify overall approach dealing bigram stop word returned bigram measure pmi deal stop word well noise add additional value past certain point seen several specific example use bigram measure however wondering best remove stop word overall process cleaning data expansion lemmatizing stemming etc yes using corpus sufficiently large remember size corpus also affect quality bigram measure result based accepted answer post href counting frequency bigram seems stop word could removed pmi bigram measure used corpus imagine filtering collocation wa simply deleting many probability measure liklihood ratio pmi compute probability word relative word corpus would function properly deleting word random position given corpus deleting collocation given list word many potential functionality computation would disabled therefore believe best process clean text remove garbage char like html tag etc expand contraction e g lemmatize stem normalize word calculate bigram using bigram measure like pmi calculate bigram using method using apply frequency filter like apply freq filter n get bigram occur threshold note still return bigram stop word mixed valuable bigram check see word stop word yes include bigram final result leave corpus reason quoted correct overall approach dealing bigram stop word mixed valuable bigram
Can stop-words be found automatically?,"<p>In NLP, stop-words removal is a typical pre-processing step. And it is typically done in an empirical way based on what we think stop-words should be.</p>

<p>But in my opinion, we should generalize the concept of stop-words. And the stop-words could vary for corpora from different domains. I am wondering if we can define the stop-words mathematically, such as by its statistical characteristics. And then can we automatically extract stop-words from a corpora for a specific domain.</p>

<p>Is there any similar thought and progress on this? Could anyone shed some light?</p>
",Preprocessing of the text & Tokenization,stop word found automatically nlp stop word removal typical pre processing step typically done empirical way based think stop word opinion generalize concept stop word stop word could vary corpus different domain wondering define stop word mathematically statistical characteristic automatically extract stop word corpus specific domain similar thought progress could anyone shed light
Tokenizing and summarizing Textual data by group efficiently in Python,"<p>I have a dataset in Python that look like this one:</p>
<pre><code>data = pd.DataFrame({
    'ID': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],
    'TEXT': [
        &quot;Mouthwatering BBQ ribs cheese, and coleslaw.&quot;,
        &quot;Delicious pizza with pepperoni and extra cheese.&quot;,
        &quot;Spicy Thai curry with cheese and jasmine rice.&quot;,
        &quot;Tiramisu dessert topped with cocoa powder.&quot;,
        &quot;Sushi rolls with fresh fish and soy sauce.&quot;,
        &quot;Freshly baked chocolate chip cookies.&quot;,
        &quot;Homemade lasagna with layers of cheese and pasta.&quot;,
        &quot;Gourmet burgers with all the toppings and extra cheese.&quot;,
        &quot;Crispy fried chicken with mashed potatoes and extra cheese.&quot;,
        &quot;Creamy tomato soup with a grilled cheese sandwich.&quot;
    ],
    'DATE': [
        '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-02',
        '2023-02-02', '2023-02-01', '2023-02-01', '2023-02-02', '2023-02-02'
    ]
})
</code></pre>
<p>What I'd like to do is group by DATE and get the frequency of each token after removing punctuation. I'm very new to the Python environment; I come from R, and I have been looking into the gensim library for further reference. It looks quite complicated to me. My desired output would look like this: for each group (DATE), we'll have the frequency of each unique token.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>TOKEN</th>
<th>SUBTOTAL</th>
<th>DATE</th>
</tr>
</thead>
<tbody>
<tr>
<td>cheese</td>
<td>5</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>and</td>
<td>5</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>with</td>
<td>5</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>extra</td>
<td>2</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>mouthwatering</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>bbq</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>ribs</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>coleslaw</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>delicious</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>pizza</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>pepperoni</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
</tbody>
</table>
</div>
<p>In R this can be done very easy with quanteda like this:</p>
<pre><code>corpus_food&lt;-corpus(data,
                  docid_field = &quot;ID&quot;,
                  text_field = &quot;TEXT&quot;)

corpus_food %&gt;%
  tokens(remove_punct = TRUE) %&gt;% 
  dfm() %&gt;% 
  textstat_frequency(groups = lubridate::date(DATE)) 
</code></pre>
<p>Which only creates a corpus and then tokenizes to remove punctuation. Later, it creates a document-term matrix and finally summarizes the tokens and their frequencies by group.</p>
<p>I am in no way comparing the two languages, Python and R. They are amazing, but at the moment, I'm interested in a very straightforward and fast method to achieve my results in Python. If perhaps you don't use the gensim library, I'd still be interested in a way to achieve what I'm looking for in a faster and more efficient way in Python. I'm new to Python.</p>
",Preprocessing of the text & Tokenization,tokenizing summarizing textual data group efficiently python dataset python look like one like group date get frequency token removing punctuation new python environment come r looking gensim library reference look quite complicated desired output would look like group date frequency unique token token subtotal date cheese extra mouthwatering bbq rib coleslaw delicious pizza pepperoni r done easy quanteda like creates corpus tokenizes remove punctuation later creates document term matrix finally summarizes token frequency group way comparing two language python r amazing moment interested straightforward fast method achieve result python perhaps use gensim library still interested way achieve looking faster efficient way python new python
Lemmatizing Italian sentences for frequency counting,"<p>I would like to lemmatize some Italian text in order to perform some frequency counting of words and further investigations on the output of this lemmatized content.</p>

<p>I am preferring lemmatizing than stemming because I could extract the word meaning from the context in the sentence (e.g. distinguish between a verb and a noun) and obtain words that exist in the language, rather than roots of those words that don't usually have a meaning.</p>

<p>I found out this library called <code>pattern</code> (<code>pip2 install pattern</code>) that should complement <code>nltk</code> in order to perform lemmatization of the <strong>Italian language</strong>, however I am not sure the approach below is correct because each word is lemmatized by itself, not in the context of a sentence.</p>

<p>Probably I should give <code>pattern</code> the responsibility to tokenize a sentence (so also annotating each word with the metadata regarding verbs/nouns/adjectives etc), then retrieving the lemmatized word, but I am not able to do this and I am not even sure it is possible at the moment?</p>

<p>Also: in Italian some articles are rendered with an apostrophe so for example ""l'appartamento"" (in English ""the flat"") is actually 2 words: ""lo"" and ""appartamento"". Right now I am not able to find a way to split these 2 words with a combination of <code>nltk</code> and <code>pattern</code> so then I am not able to count the frequency of the words in the correct way.</p>

<pre><code>import nltk
import string
import pattern

# dictionary of Italian stop-words
it_stop_words = nltk.corpus.stopwords.words('italian')
# Snowball stemmer with rules for the Italian language
ita_stemmer = nltk.stem.snowball.ItalianStemmer()

# the following function is just to get the lemma
# out of the original input word (but right now
# it may be loosing the context about the sentence
# from where the word is coming from i.e.
# the same word could either be a noun/verb/adjective
# according to the context)
def lemmatize_word(input_word):
    in_word = input_word#.decode('utf-8')
    # print('Something: {}'.format(in_word))
    word_it = pattern.it.parse(
        in_word, 
        tokenize=False,  
        tag=False,  
        chunk=False,  
        lemmata=True 
    )
    # print(""Input: {} Output: {}"".format(in_word, word_it))
    the_lemmatized_word = word_it.split()[0][0][4]
    # print(""Returning: {}"".format(the_lemmatized_word))
    return the_lemmatized_word

it_string = ""Ieri sono andato in due supermercati. Oggi volevo andare all'ippodromo. Stasera mangio la pizza con le verdure.""

# 1st tokenize the sentence(s)
word_tokenized_list = nltk.tokenize.word_tokenize(it_string)
print(""1) NLTK tokenizer, num words: {} for list: {}"".format(len(word_tokenized_list), word_tokenized_list))

# 2nd remove punctuation and everything lower case
word_tokenized_no_punct = [string.lower(x) for x in word_tokenized_list if x not in string.punctuation]
print(""2) Clean punctuation, num words: {} for list: {}"".format(len(word_tokenized_no_punct), word_tokenized_no_punct))

# 3rd remove stop words (for the Italian language)
word_tokenized_no_punct_no_sw = [x for x in word_tokenized_no_punct if x not in it_stop_words]
print(""3) Clean stop-words, num words: {} for list: {}"".format(len(word_tokenized_no_punct_no_sw), word_tokenized_no_punct_no_sw))

# 4.1 lemmatize the words
word_tokenize_list_no_punct_lc_no_stowords_lemmatized = [lemmatize_word(x) for x in word_tokenized_no_punct_no_sw]
print(""4.1) lemmatizer, num words: {} for list: {}"".format(len(word_tokenize_list_no_punct_lc_no_stowords_lemmatized), word_tokenize_list_no_punct_lc_no_stowords_lemmatized))

# 4.2 snowball stemmer for Italian
word_tokenize_list_no_punct_lc_no_stowords_stem = [ita_stemmer.stem(i) for i in word_tokenized_no_punct_no_sw]
print(""4.2) stemmer, num words: {} for list: {}"".format(len(word_tokenize_list_no_punct_lc_no_stowords_stem), word_tokenize_list_no_punct_lc_no_stowords_stem))

# difference between stemmer and lemmatizer
print(
    ""For original word(s) '{}' and '{}' the stemmer: '{}' '{}' (count 1 each), the lemmatizer: '{}' '{}' (count 2)""
    .format(
        word_tokenized_no_punct_no_sw[1],
        word_tokenized_no_punct_no_sw[6],
        word_tokenize_list_no_punct_lc_no_stowords_stem[1],
        word_tokenize_list_no_punct_lc_no_stowords_stem[6],
        word_tokenize_list_no_punct_lc_no_stowords_lemmatized[1],
        word_tokenize_list_no_punct_lc_no_stowords_lemmatized[1]
    )
)
</code></pre>

<p>Gives this output:</p>

<pre><code>1) NLTK tokenizer, num words: 20 for list: ['Ieri', 'sono', 'andato', 'in', 'due', 'supermercati', '.', 'Oggi', 'volevo', 'andare', ""all'ippodromo"", '.', 'Stasera', 'mangio', 'la', 'pizza', 'con', 'le', 'verdure', '.']
2) Clean punctuation, num words: 17 for list: ['ieri', 'sono', 'andato', 'in', 'due', 'supermercati', 'oggi', 'volevo', 'andare', ""all'ippodromo"", 'stasera', 'mangio', 'la', 'pizza', 'con', 'le', 'verdure']
3) Clean stop-words, num words: 12 for list: ['ieri', 'andato', 'due', 'supermercati', 'oggi', 'volevo', 'andare', ""all'ippodromo"", 'stasera', 'mangio', 'pizza', 'verdure']
4.1) lemmatizer, num words: 12 for list: [u'ieri', u'andarsene', u'due', u'supermercato', u'oggi', u'volere', u'andare', u""all'ippodromo"", u'stasera', u'mangiare', u'pizza', u'verdura']
4.2) stemmer, num words: 12 for list: [u'ier', u'andat', u'due', u'supermerc', u'oggi', u'vol', u'andar', u""all'ippodrom"", u'staser', u'mang', u'pizz', u'verdur']
For original word(s) 'andato' and 'andare' the stemmer: 'andat' 'andar' (count 1 each), the lemmatizer: 'andarsene' 'andarsene' (count 2)
</code></pre>

<ul>
<li>How to effectively lemmatize some sentences with <code>pattern</code> using their tokenizer? (assuming lemmas are recognized as nouns/verbs/adjectives etc.)</li>
<li>Is there a python alternative to <code>pattern</code> to use for Italian lemmatization with <code>nltk</code>?  </li>
<li>How to split articles that are bound to the next word using apostrophes?</li>
</ul>
",Preprocessing of the text & Tokenization,lemmatizing italian sentence frequency counting would like lemmatize italian text order perform frequency counting word investigation output lemmatized content preferring lemmatizing stemming could extract word meaning context sentence e g distinguish verb noun obtain word exist language rather root word usually meaning found library called complement order perform lemmatization italian language however sure approach correct word lemmatized context sentence probably give responsibility tokenize sentence also annotating word metadata regarding verb noun adjective etc retrieving lemmatized word able even sure possible moment also italian article rendered apostrophe example l appartamento english flat actually word lo appartamento right able find way split word combination able count frequency word correct way give output effectively lemmatize sentence using tokenizer assuming lemma recognized noun verb adjective etc python alternative use italian lemmatization split article bound next word using apostrophe
How to get Enhanced++ dependency labels with a java command line in the terminal?,"<p>I don't really know java, but I was just trying to use the documentation of the Stanford NLP parser to get the Enhanced++ dependency labels. This is the line I ran:</p>
<pre><code>java -cp &quot;*&quot; -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators &quot;tokenize,ssplit,pos,lemma,depparse&quot; -file input.txt
</code></pre>
<p>And indeed I get an output. But I don't get the labels that I expect. For example, in the input.txt file there is a sentence &quot;The older couple is picnicking with wine&quot;, and the dependency between picnicking and wine should be nmod, but instead it is obl:with.
Another sentence is &quot;What do you call it?&quot;, where I expect a dobj relationship between &quot;call&quot; and &quot;it&quot;, but instead I get &quot;obj&quot;.</p>
<p>What should I fix to get the labels of the enhanced universal dependencies?</p>
<p>(Also, do I really need to specify the options &quot;tokenize,ssplit,pos,lemma&quot; if I am interested only in &quot;depparse&quot;?)</p>
<p>Thank you.</p>
",Preprocessing of the text & Tokenization,get enhanced dependency label java command line terminal really know java wa trying use documentation stanford nlp parser get enhanced dependency label line ran indeed get output get label expect example input txt file sentence older couple picnicking wine dependency picnicking wine nmod instead obl another sentence call expect dobj relationship call instead get obj fix get label enhanced universal dependency also really need specify option tokenize ssplit po lemma interested depparse thank
What is Stanford CoreNLP&#39;s recipe for tokenization?,"<p>Whether you're using Stanza or Corenlp (now deprecated) python wrappers, or the original Java implementation, the tokenization rules that StanfordCoreNLP follows is super hard for me to figure out from the code in the original codebases.</p>
<p>The implementation is very verbose and the tokenization approach is not really documented. Do they consider this proprietary? On their website, they say that &quot;CoreNLP splits texts into tokens with an elaborate collection of rules, designed to follow UD 2.0 specifications.&quot;</p>
<p>I'm looking for where to find those rules, and ideally, to replace CoreNLP (a massive codebase!) with just a regex or something much simpler to mimic their tokenization strategy. Please assume in your responses that Stanford's tokenization approach is the goal. I am not looking for alternative tokenization solutions, but I also very much do not want to include and ship a code base that requires a massive java library as a dependency.</p>
<p>The answer should address the following behavior:</p>
<ul>
<li>Word hyphenation should be disabled (someone with a hyphenated last name should not be split, e.g., Marie Illonig-Alberts should tokenize as [&quot;Marie&quot;, &quot;Illonig-Alberts&quot;]. Similarly, compound words like &quot;well-intentioned&quot; should not be split.</li>
<li>Plural apostrophes should be tokenized (e.g., all boys' shoes are red to [&quot;all&quot;, &quot;boys&quot;, &quot;'&quot;, &quot;shoes&quot;, &quot;are&quot;, red&quot;])</li>
<li>Apostrophes for single ownership (e.g., my aunt's favorite to [&quot;my&quot;, &quot;aunt&quot;, &quot;'s&quot;, &quot;favorite&quot;]</li>
<li>Mr./Mrs. should not be [&quot;Mr&quot;, &quot;.&quot;] / [&quot;Mrs&quot;, &quot;.&quot;]</li>
<li>Normal punctuation should be their own tokens (end of sentence periods, commas, quotes for direct quotes or to denote sarcasm, question marks, semicolon and colons, and dashes). Double dashes should not be separated (e.g., -- is [&quot;--&quot;] NOT [&quot;-&quot;, &quot;-&quot;]</li>
<li>Wouldn't should tokenize to [&quot;would&quot;, &quot;n't&quot;]</li>
<li>&quot;and/or&quot; should not tokenize</li>
<li>Contractions should tokenize (e.g., I'm to [&quot;I&quot;, &quot;'m&quot;]</li>
<li>I also see weird tokens that correspond to POS tags sometimes like &quot;-LRB-&quot; and &quot;:-RRB-&quot;, which I do not understand.</li>
</ul>
",Preprocessing of the text & Tokenization,stanford corenlp recipe tokenization whether using stanza corenlp deprecated python wrapper original java implementation tokenization rule stanfordcorenlp follows super hard figure code original codebases implementation verbose tokenization approach really documented consider proprietary website say corenlp split text token elaborate collection rule designed follow ud specification looking find rule ideally replace corenlp massive codebase regex something much simpler mimic tokenization strategy please assume response stanford tokenization approach goal looking alternative tokenization solution also much want include ship code base requires massive java library dependency answer address following behavior word hyphenation disabled someone hyphenated last name split e g marie illonig albert tokenize marie illonig albert similarly compound word like well intentioned split plural apostrophe tokenized e g boy shoe red boy shoe red apostrophe single ownership e g aunt favorite aunt favorite mr mr mr mr normal punctuation token end sentence period comma quote direct quote denote sarcasm question mark semicolon colon dash double dash separated e g tokenize would n tokenize contraction tokenize e g also see weird token correspond po tag sometimes like lrb rrb understand
How can I get the longest word and it&#39;s length from a dictionary along with the most common word?,"<p>I have a text file which is as follows:</p>
<pre><code>['To be or not to be,',
 'that is the question.']
</code></pre>
<p>I populated the text into a dictionary called hamlet_dict so the key is the word and the value is the number of times it appears in the text. I am trying to write a code so I can get the longest word in the dictionary as well as the length of the longest word. I also want the most common word as well as the count of the most common word.</p>
<p>I created a variable and a list to use for the longest word.</p>
<pre><code>length = 0
longest = []

for k in hamlet_dict.keys(): 
    if (len(k) &gt; length):
        length = len(k)
</code></pre>
<p>I want to append the length of the longest word to the list longest. This is the starting point but I don't know where to go from here.</p>
<pre><code>for k in hamlet_dict.keys():
</code></pre>
<p>Finally, I want to print the statement
print(f'There are {length} characters in the longest word. The word is {longest}.')</p>
<p>So the output should be:</p>
<pre><code>There are 8 characters in the longest word. The word is question.
</code></pre>
<p>But I am getting:</p>
<p>There are 8 characters in the longest word. The word is [].</p>
<p>I have a similar problem with the most common word and the count of the most common word.</p>
<p>I have a variable and a list:</p>
<pre><code>common = []
count = 0

</code></pre>
<p>For the count, I have:</p>
<pre><code>for i in hamlet_dict.values():
    if i &gt; count:
        count = i
</code></pre>
<p>How can I add the matching words of the variable to the list?</p>
<p>I want to print:</p>
<p>print(f'{common} is the most common word(s). It showed up {count} times.')</p>
<p>The output I am getting is:</p>
<p>[] is the most common word(s). It showed up 2 times.</p>
<p>But I want:</p>
<pre><code>to, be is the most common word(s). It showed up 2 times. 

</code></pre>
<p>Any help is appreciated!</p>
",Preprocessing of the text & Tokenization,get longest word length dictionary along common word text file follows populated text dictionary called hamlet dict key word value number time appears text trying write code get longest word dictionary well length longest word also want common word well count common word created variable list use longest word want append length longest word list longest starting point know go finally want print statement print f length character longest word word longest output getting character longest word word similar problem common word count common word variable list count add matching word variable list want print print f common common word showed count time output getting common word showed time want help appreciated
Snowball Stemmer token,"<p>The following lines of code do not work of the module SnowballStemmer from nltk</p>
<pre><code>def fun(text):
    stemmer.stem(text)
</code></pre>
<pre><code>TypeError: SnowballStemmer.stem() missing 1 required positional argument: 'token'
</code></pre>
",Preprocessing of the text & Tokenization,snowball stemmer token following line code work module snowballstemmer nltk
"Regex to detect words based on the words Action, Object, Sumbject, etc in the middle of a text","<p>I have the following text and I would like to detect the words after the subject, action and capabilities using regular expressions:</p>
<pre><code>For this text:
T1  Subject num num xxx
T2  Action num num  xxx
A1  Capability T2 xxx
</code></pre>
<p>I have created the following regex but it's not correct:</p>
<pre><code># Regular expressions for pattern matching
action_pattern = r'^T\d+\tAction \d+ \d+\t(.+)$'
subject_pattern = r'^T\d+\tSubject \d+ \d+;?\d+? \d+\t(.+)$'
object_pattern = r'^T\d+\tObject \d+ \d+;?\d+? \d+\t(.+)$'
capability_pattern = r'^A\d+\tCapability T\d+ (.+)$'
</code></pre>
",Preprocessing of the text & Tokenization,regex detect word based word action object sumbject etc middle text following text would like detect word subject action capability using regular expression created following regex correct
&#39;list&#39; object has no attribute &#39;lower&#39;&#39;list&#39; object has no attribute &#39;lower&#39; in TfidfVectorizer,"<p>i have csv file about some arabic tweet i did token and stemming and clean the text</p>
<pre><code>
`data = pd.read_csv(r&quot;tweet.csv&quot; ,dtype=str, encoding=&quot;utf-8&quot;)

    
def preprocessing(text):
\#remove stop words
stop_words = set(stopwords.words('arabic'))
\#takenazation
tokens = word_tokenize(text.lower())
result = \[i for i in tokens if not i  in stop_words\]
\#stemming
als=ArabicLightStemmer();
word_list = \[als.light_stem(w) for w in  result\]
return word_list

data\['text'\]  = data\['text'\].apply(preprocessing)

vectorizer=TfidfVectorizer(binary=False,norm='l2',use_idf=True,smooth_idf=True,lowercase=True,min_df=1,
max_df=1.0,max_features=None,ngram_range=(1,1))
vectorizer.fit(data\[&quot;text&quot;\])
x=vectorizer.transformv(data\[&quot;text&quot;\])

</code></pre>
<p>i have error in TfidfVectorizer it said that  'list' object has no attribute 'lower'</p>
<p>i want to do vetorization to my text</p>
",Preprocessing of the text & Tokenization,list object ha attribute lower list object ha attribute lower tfidfvectorizer csv file arabic tweet token stemming clean text error tfidfvectorizer said list object ha attribute lower want vetorization text
regarding nlp stemming and stop-words,"<p>I just got started with NLP in machine learning.
I was dealing with sentiment analysis for the Twitter dataset
the part where I am confused is if we remove stop-words before stemming the words will it not affect the negative reviews like -<strong>&quot;i food was not good&quot;</strong> in these cases, even if make an exception for not what about the other words like <strong>wasn't don't</strong>
so what do I do?</p>
<p>I taught about stemming the words and then removing the stop words but doesnt seem like good idea</p>
",Preprocessing of the text & Tokenization,regarding nlp stemming stop word got started nlp machine learning wa dealing sentiment analysis twitter dataset part confused remove stop word stemming word affect negative review like food wa good case even make exception word like taught stemming word removing stop word doesnt seem like good idea
How nltk.TweetTokenizer different from nltk.word_tokenize?,"<p>I am unable to understand the difference between the two. Though, I come to know that word_tokenize uses Penn-Treebank for tokenization purposes. But nothing on TweetTokenizer is available. For which sort of data should I be using TweetTokenizer over word_tokenize?</p>
",Preprocessing of the text & Tokenization,nltk tweettokenizer different nltk word tokenize unable understand difference two though come know word tokenize us penn treebank tokenization purpose nothing tweettokenizer available sort data using tweettokenizer word tokenize
How can I edit Turkish sentence to get better and consistent sentiment analysis from a pre trained model?,"<pre><code># Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline(&quot;text-classification&quot;, model=&quot;savasy/bert-base-turkish-sentiment-cased&quot;)

sentence = &quot;Bakan Varank Milli elektrikli tren 29 Mayıs'ta test edilmeye başlanacak. Sanayi ve Teknoloji Bakanı Mustafa Varank yaptığı son dakika açıklamasında Milli elektrikli tren, 29 Mayıs'ta raylara indirilip test edilmeye başlanacak. Testlere göre, eylül ayında bu trenler vatandaşlarımızca kullanılmaya başlanacak dedi.&quot;

sentiment_result = pipe(sentence)
print(sentiment_result)
</code></pre>
<p>it prints this:
<code>[{'label': 'negative', 'score': 0.6390795707702637}]</code></p>
<p>It should have been positive. What preprocessing can I do to take better score and label ?
Would it be better if I tokenize Turkish sentence or apply other things ?</p>
",Preprocessing of the text & Tokenization,edit turkish sentence get better consistent sentiment analysis pre trained model print positive preprocessing take better score label would better tokenize turkish sentence apply thing
Multi-class text classification where classification depends on other columns beside text column,"<p>I want to do text classification in Python where I have a df with 3 columns: scenario, text and label. The label depends on the column text but also on the column scenario. There are 6 different scenarios.</p>
<p>Would it be correct to make a new column where I would merge columns scenario and text into one string and then do the tokenization and the rest of the model training with just this new column?</p>
",Preprocessing of the text & Tokenization,multi class text classification classification depends column beside text column want text classification python df column scenario text label label depends column text also column scenario different scenario would correct make new column would merge column scenario text one string tokenization rest model training new column
How to remove a word from a dataset in R? NLP,"<p>I'm very new in this world of programming.</p>
<p>Ok so I am making an analysis of a text in R. I am using this to get rid of stop words:</p>
<pre><code>kant_palavras &lt;- kant_palavras %&gt;% anti_join(get_stopwords(language = 'pt'))
</code></pre>
<p>BUT after, in the counting of words, the most common is &quot;no&quot;. This is not useful for my analysis and I want to remove it, but I do not know how to do it.</p>
<p>I tried</p>
<pre><code>kant_palavras &lt;- kant_palavras %&gt;% anti_join(&quot;no&quot;)
</code></pre>
<p>and</p>
<pre><code>palavras_a_remover &lt;- c(&quot;no&quot;) 

kant_palavras &lt;- kant_palavras %&gt;% anti_join(data.frame(palavra = palavras_a_remover))
</code></pre>
<p>and</p>
<pre><code>palavras_a_remover &lt;- c(&quot;no&quot;)

kant_palavras &lt;- kant_palavras %&gt;% 
  filter(!palavra %in% palavras_a_remover)
</code></pre>
<p>neither worked to get rid of that no!</p>
<p>--</p>
<p>full code before (all works):</p>
<pre><code>dados_kant &lt;- read.csv(&quot;kant2.csv&quot;)

dados_kant2 &lt;- as_tibble(dados_kant)

Encoding(dados_kant2$texto.do.kant) &lt;- &quot;ASCII&quot;

for (i in 1:nrow(dados_kant2))
{
  dados_kant2$texto.do.kant[i] &lt;- iconv(dados_kant2$texto.do.kant[i], to = &quot;ASCII//TRANSLIT&quot;)
}

kant_palavras &lt;- dados_kant2 %&gt;%  unnest_tokens(word, texto.do.kant)

kant_palavras &lt;- kant_palavras %&gt;% anti_join(get_stopwords(language = 'pt'))
</code></pre>
",Preprocessing of the text & Tokenization,remove word dataset r nlp new world programming ok making analysis text r using get rid stop word counting word common useful analysis want remove know tried neither worked get rid full code work
How to get Attentions Part from the output of a Bert model?,"<h4>I am using <code>Bert-Model</code> for  Query Expansion and I am trying to extract the keywords from the Document I have</h4>
<pre><code>tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)
sentence=&quot;This is a sentence&quot;
tokens = tokenizer.tokenize(sentence)
print(tokens,&quot;-tokens&quot;)
input_ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(\[input_ids\])
print(input_ids)
with torch.no_grad():
output = model(input_ids)
attention_scores = output.attentions
print(attention_scores)   #this prints None
</code></pre>
<p>this is my code I am using a Simple Sentence and trying to extract keywords from it In order to Do that I need attention of the output part of <code>Bert-Model</code></p>
<p>I tried with Different tokenizer methods(tokenize,encode,encode_plus) to tokenize and tried with
different bert variants (bert-large-uncased)</p>
<p>I want to extract the attention Part form the model output but I am not able to do that</p>
<p>I get None in that place I am not able to get any value in the attention part of  the output</p>
",Preprocessing of the text & Tokenization,get attention part output bert model using query expansion trying extract keywords document code using simple sentence trying extract keywords order need attention output part tried different tokenizer method tokenize encode encode plus tokenize tried different bert variant bert large uncased want extract attention part form model output able get none place able get value attention part output
Lemmatization and Pos tagging from scratch,"<p>I am NLP enthusiast and i plan to write some basic nlp models for my language(Azerbaijani), which do not have any good opportunities on Spacy\NLTK. Can you tell me please &quot;roadmap&quot; to realize this goal?</p>
<p>Firstly, i am going to write pos tagger with helped of Hidden Markov Model(HMM) and approximately 1000 tagged texts and after use this model like a tagger for further model training. And after that, write typical functions like token.pos_, token.tag_.</p>
<p>After Pos-tagger, i want to write <code>lemmatize()</code> function. But i don't have idea how to write it. Can you please tell some steps for achieving this goal?</p>
",Preprocessing of the text & Tokenization,lemmatization po tagging scratch nlp enthusiast plan write basic nlp model language azerbaijani good opportunity spacy nltk tell please roadmap realize goal firstly going write po tagger helped hidden markov model hmm approximately tagged text use model like tagger model training write typical function like token po token tag po tagger want write function idea write please tell step achieving goal
How can I train a model for specific information extraction,"<p>I have a dataset of verdict sentences texts that looks like this
&quot;The defendant has been found guilty of armed robbery and is hereby sentenced to 10 years in prison, and sentencing him to a fine of five dollars and fees. After considering the defendant's cooperation with the investigation and lack of prior convictions, the sentence is reduced to 5 years in prison starting from the day this ruling becomes final.&quot;</p>
<p>how can I train/tune a model/(which models are best) and preprocess the data to understand the text and give me only the reduced sentence &quot;5 years in prison starting from the day this ruling becomes final.&quot; as output</p>
",Preprocessing of the text & Tokenization,train model specific information extraction dataset verdict sentence text look like defendant ha found guilty armed robbery hereby sentenced year prison sentencing fine five dollar fee considering defendant cooperation investigation lack prior conviction sentence reduced year prison starting day ruling becomes final train tune model model best preprocess data understand text give reduced sentence year prison starting day ruling becomes final output
Splitting a word into two words with spaCy,"<p>I'm facing an issue where I need to split a single 'word' into two words due to missing spaces or new lines in the received text. My intention is to establish a pipeline (spaCy 3.5.4) for this task prior to utilizing more advanced NLP techniques.</p>
<p>The criteria for identifying the words to be split are as follows:</p>
<ul>
<li>The word should have <code>.is_oov </code>set to <code>True</code>.</li>
<li>The <code>.shape_</code> of the word should be in the format of <code>XxxXxx</code> or something similar, such as having a number in front or after.</li>
</ul>
<p>Here's an illustrative example:</p>
<pre><code>for token in doc:
  if token.is_oov ==True:
    print (token.text, token.shape_)
    [split here...]
</code></pre>
<p>with the following output:</p>
<pre><code>62.000Unbefristeter dd.dddXxxxx
KundenFlexible XxxxxXxxxx
WorkDiverse XxxxXxxxx
RabatteFlache XxxxxXxxxx
ArbeitsumfeldIhre XxxxxXxxx
Stellenauswahl Xxxxx
</code></pre>
<p>While the last example is a false positive, the remaining instances need to be split into two words, and it's essential to retain the text for subsequent processing.</p>
<p>I'm familiar with regular expressions, but I'm curious if there is a simpler approach using spaCy.</p>
",Preprocessing of the text & Tokenization,splitting word two word spacy facing issue need split single word two word due missing space new line received text intention establish pipeline spacy task prior utilizing advanced nlp technique criterion identifying word split follows word set word format something similar number front illustrative example following output last example false positive remaining instance need split two word essential retain text subsequent processing familiar regular expression curious simpler approach using spacy
C3W2_Assignment in coursera Deep Learning course NLP,"<h1>grader-required-cell</h1>
<h1>GRADED FUNCTION: tokenize_labels</h1>
<p>def tokenize_labels(all_labels, split_labels):
&quot;&quot;&quot;
Tokenizes the labels</p>
<pre><code>Args:
    all_labels (list of string): labels to generate the word-index from
    split_labels (list of string): labels to tokenize

Returns:
    label_seq_np (array of int): tokenized labels
&quot;&quot;&quot;

### START CODE HERE

# Instantiate the Tokenizer (no additional arguments needed)
label_tokenizer = Tokenizer()

# Fit the tokenizer on all the labels
label_tokenizer.fit_on_texts(all_labels)

# Convert labels to sequences
label_seq = label_tokenizer.texts_to_sequences(split_labels)

# Convert sequences to a numpy array. Don't forget to substact 1 from every entry in the array!
label_seq_np = np.array(label_seq)-1 &lt;----------- i don't know what is wrong


### END CODE HERE

return label_seq_np
</code></pre>
<p>the output is:</p>
<blockquote>
<p>TypeError: unsupported operand type(s) for -: 'list' and 'int'</p>
</blockquote>
",Preprocessing of the text & Tokenization,c w assignment coursera deep learning course nlp grader required cell graded function tokenize label def tokenize label label split label tokenizes label output typeerror unsupported operand type list int
"How to resolve this : Expected a string, Doc, or bytes as input, but got: &lt;class &#39;float&#39;&gt;","<p>My main objective was to remove stop words and puntuations from a large amount of text within the dataset. However after 5 minutes of the code running i got this error.</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-121-c761affbd958&gt; in &lt;cell line: 2&gt;()
      1 # Apply preprocess funtion to text data in dataset
----&gt; 2 df_balanced['Preprocessed_text'] = df_balanced['text'].apply(tf_preprocess)

6 frames
/usr/local/lib/python3.10/dist-packages/spacy/language.py in _ensure_doc(self, doc_like)
   1122         if isinstance(doc_like, bytes):
   1123             return Doc(self.vocab).from_bytes(doc_like)
-&gt; 1124         raise ValueError(Errors.E1041.format(type=type(doc_like)))
   1125 
   1126     def _ensure_doc_with_context(

ValueError: [E1041] Expected a string, Doc, or bytes as input, but got: &lt;class 'float'&gt;
</code></pre>
<p>My preprocessing code is as follows:</p>
<pre><code># Further Preprocessing
# remove stopwords and puntuations
tf_nlp = spacy.load(&quot;en_core_web_sm&quot;)
def tf_preprocess(text):
  doc_15 = tf_nlp(text)
  filtered_data = []
  for token in doc_15:
    if token.is_stop or token.is_punct:
      continue
    filtered_data.append(token.lemma_)
  return &quot; &quot;.join(filtered_data)
</code></pre>
<p>I would like to know how to resolve that error.
Thank you for your time and suggestions :)</p>
",Preprocessing of the text & Tokenization,resolve expected string doc byte input got class float main objective wa remove stop word puntuations large amount text within dataset however minute code running got error preprocessing code follows would like know resolve error thank time suggestion
How to use a text dataset that is al ready tokenized at character level?,"<p>I have a text dataset that contains already tokenized text, at character level with characters separated by space, space replaced by _ and unknown characters replaced by a #. The goal is to use this dataset to train a seq2seq or transformer model for correcting spellings. I am having trouble using this dataset. I am unclear on how to feed this dataset to the model. Do I have to add the [start] and [end] tags to the target sentences on the already tokenized text? How do I build the vocabulary as previously I would preprocess the text myself and have only done word level tokenization. I have tried hard but haven't been able to find much useful information. If you guys can direct me to the right material/tutorial I would really appreciate that. I am very new to deep learning for NLP.</p>
",Preprocessing of the text & Tokenization,use text dataset al ready tokenized character level text dataset contains already tokenized text character level character separated space space replaced unknown character replaced goal use dataset train seq seq transformer model correcting spelling trouble using dataset unclear feed dataset model add start end tag target sentence already tokenized text build vocabulary previously would preprocess text done word level tokenization tried hard able find much useful information guy direct right material tutorial would really appreciate new deep learning nlp
How to tokenize persian string and save that into txt file,"<p>I'm trying to tokenize the A.txt and save that into B.txt file
the string i'm trying to process is persian and i want to save that word by word in persian, this is my code</p>

<p>this is main.py</p>

<pre><code>import LevelOne
import save_file
import nltk

original_data = "" "".join(open(""A.txt""))print('Processing')
save_file.saving(LevelOne.spliter(original_data))
print('Done')
</code></pre>

<p>this is LevelOne</p>

<pre><code>import re 
import persian
import stop_word


def spliter(text):
    data = re.split(r'\W+',text)
    tokenized = [word for word in data if word not in 
    stop_word.stop_words]
    return tokenized
</code></pre>

<p>and this is saving part</p>

<pre><code># -*- coding: utf-8 -*-

def saving(infile):
    outfile = open('B.txt', 'w')
    replacements = {'پ':'\u067e',
          'چ':'\u0686','ج':'\u062c', 'ح':'\u062d','خ':'\u062e', 
          'ه':'\u0647','ع':'\u0639', 'غ':'\u063a','ف':'\u0641',                                           
          'ق':'\u0642','ث':'\u062b', 'ص':'\u0635','ض':'\u0636', 
          'گ':'\u06af','ک':'\u06a9', 'م':'\u0645','ن':'\u0646', 
          'ت':'\u062a','ا':'\u0627', 'ل':'\u0644','ب':'\u0628', 
          'ي':'\u06cc','س':'\u0633', 'ش':'\u0634','و':'\u0648', 
          'ئ':'\u0626','د':'\u062f', 'ذ':'\u0630','ر':'\u0631', 
          'ز':'\u0632','ط':'\u0637', 'ظ':'\u0638','ژ':'\u0698', 
          'آ':'\u0622','ی':'\u064a', '؟':'\u061f'}
    data = "" "".join(infile)
    print(data)
    for line in data:
        for src, target in replacements.items() :
            line = line.replace(src, target)
            outfile.write(line)
    outfile.close()
</code></pre>

<p>but when i open the B.text file , i See this </p>

<pre><code>Ú Ù Ù¾Ø³Ø Ø³Ù Ø Ù Ø ÙˆØ ÛŒ Ú Ù Ø Ø Ø ØŸ
</code></pre>

<p>the original file look like this</p>

<pre><code>گل پسر
سلام خوبی چه خبر؟
</code></pre>
",Preprocessing of the text & Tokenization,tokenize persian string save txt file trying tokenize txt save b txt file string trying process persian want save word word persian code main py levelone saving part open b text file see original file look like
How to fix text encoding in python,"<p>I have some text which is not in proper UTF-8 encoding. So when BERT tokenizer tokenize it, it failed.
Here is example</p>
<pre><code>from transformers import AutoTokenizer
s =&quot;𝗜𝘀 𝗽а𝘀𝘀𝗶𝘃е 𝗶𝗻𝗰о𝗺е 𝘄𝗵𝗮𝘁 𝘆𝗼𝘂'𝗿𝗲 𝗹𝗼𝗼𝗸𝗶𝗻𝗴 𝗳𝗼𝗿? \n⚡️𝗜𝗻𝘃𝗲𝘀𝘁𝗺𝗲𝗻𝘁 𝗽𝗹𝗮𝘁𝗳𝗼𝗿𝗺 𝗳𝗼𝗿 𝗯𝗲𝗴𝗶𝗻𝗻𝗲𝗿𝘀! \n⚡️ 𝗝𝘂𝘀𝘁 𝗼𝗻𝗲 𝘀𝗺𝗮𝗿𝘁 𝗶𝗻𝘃𝗲𝘀𝘁𝗺𝗲𝗻𝘁 𝗰𝗮𝗻 𝗺𝗼𝗱𝗶𝗳𝘆 𝗲𝘃𝗲𝗿𝘆𝘁𝗵𝗶𝗻𝗴! \n𝗗𝘂𝗲 𝘁𝗼 𝘁𝗵𝗲 𝗹𝗮𝘁𝗲𝘀𝘁 𝘀𝗮𝗻𝗰𝘁𝗶𝗼𝗻𝘀 𝗶𝗻 𝘁𝗵𝗲 𝘄𝗼𝗿𝗹𝗱, 𝘁𝗵𝗶𝘀 𝗶𝘀 𝘆𝗼𝘂𝗿 𝗰𝗵𝗮𝗻𝗰𝗲 𝗳𝗼𝗿 𝘀𝘁𝗮𝗯𝗶𝗹𝗶𝘁𝘆 𝗶𝗻 𝗳𝗶𝗳𝘁𝗵 𝗺𝗶𝗻𝘂𝘁𝗲𝘀. 𝗝𝘂𝘀𝘁 𝗿𝗲𝗴𝗶𝘀𝘁𝗲𝗿 𝗮𝗻𝗱 𝗱𝗶𝘀𝗰𝗼𝘃𝗲𝗿 𝗼𝘂𝘁 𝗺𝗼𝗿𝗲.&quot;
model_name = &quot;bert-base-multilingual-cased&quot;

tokenizer = AutoTokenizer.from_pretrained(model_name)
print(tokenizer.tokenize(s))
</code></pre>
<p>Output</p>
<pre><code>['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', &quot;'&quot;, '[UNK]', '[UNK]', '[UNK]', '?', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '!', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '!', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', ',', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '.', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '.']
</code></pre>
<p>How to fix that?</p>
",Preprocessing of the text & Tokenization,fix text encoding python text proper utf encoding bert tokenizer tokenize failed example output fix
How can I split multiple joined words?,"<p>I have an array of 1000 or so entries, with examples below:</p>

<pre><code>wickedweather
liquidweather
driveourtrucks
gocompact
slimprojector
</code></pre>

<p>I would like to be able to split these into their respective words, as:</p>

<pre><code>wicked weather
liquid weather
drive our trucks
go compact
slim projector
</code></pre>

<p>I was hoping a regular expression my do the trick.  But, since there is no boundary to stop on, nor is there any sort of capitalization that I could possibly key on, I am thinking, that some sort of reference to a dictionary might be necessary?  </p>

<p>I suppose it could be done by hand, but why - when it can be done with code! =)  But this has stumped me.  Any ideas?  </p>
",Preprocessing of the text & Tokenization,split multiple joined word array entry example would like able split respective word wa hoping regular expression trick since boundary stop sort capitalization could possibly key thinking sort reference dictionary might necessary suppose could done hand done code ha stumped idea
Error using the nlp-id package in Jupyter Notebook,"<p>I am working on an Indonesian NLP project and planning to use the <a href=""https://pypi.org/project/nlp-id/"" rel=""nofollow noreferrer"">nlp-id package</a> for removing stop words.</p>
<p>However, when I tried the code as explained on the website:</p>
<pre><code>!pip install nlp-id
from nlp_id.tokenizer import Tokenizer 
tokenizer = Tokenizer() 
tokenizer.tokenize('Lionel Messi pergi ke pasar di daerah Jakarta Pusat.') 
</code></pre>
<p>It gave me the following error:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
Input In [28], in &lt;cell line: 1&gt;()
----&gt; 1 from nlp_id.stopword import StopWord 
      2 text = &quot;Lionel Messi pergi Ke pasar di area Jakarta Pusat&quot;
      3 stopword = StopWord() 

ModuleNotFoundError: No module named 'nlp_id'
</code></pre>
<p>Can you help me troubleshoot the &quot;ModuleNotFoundError&quot; related to the missing 'nlp_id' module in my Indonesian NLP project?</p>
",Preprocessing of the text & Tokenization,error using nlp id package jupyter notebook working indonesian nlp project planning use nlp id package removing stop word however tried code explained website gave following error help troubleshoot modulenotfounderror related missing nlp id module indonesian nlp project
I am trying to download stop words from nltk and NLTK Downloader shows WinError 10060 (Error connecting to server) and says connection attempt failed,"<p>I am typing the following in both jupyter notebook and cmd(Adminstrator mode):
<code>import nltk nltk.download()</code></p>
<p>Both give the same result, which is this:</p>
<p><a href=""https://i.sstatic.net/vlB9U.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vlB9U.png"" alt=""This dialog box shows up when I run nltk.download() and the NLTK Downloader also shows up"" /></a></p>
<p><a href=""https://i.sstatic.net/AoE0A.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AoE0A.png"" alt=""The downloader shows empty. I don't understand why this is happening"" /></a></p>
<p>Next when I clock OK on the dialog box, the NLTK downloader goes to Not Responding.</p>
<p>The same WinError 10060 comes when I try nltk.download('all) or even nltk.download('punkt).</p>
<p>Can somebody please explain why this is happening and what do I have to do for downloading NLTK Stopwords?</p>
",Preprocessing of the text & Tokenization,trying download stop word nltk nltk downloader show winerror error connecting server say connection attempt failed typing following jupyter notebook cmd adminstrator mode give result next clock ok dialog box nltk downloader go responding winerror come try nltk download even nltk download punkt somebody please explain happening downloading nltk stopwords
Execute nltk.stem.SnowballStemmer in pandas,"<p>I have a four column DataFrame with two columns of tokenized words that have had stop words removed and converted to lower case and am now attempting to stem.  </p>

<p><a href=""https://i.sstatic.net/emq5l.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/emq5l.png"" alt=""enter image description here""></a></p>

<p>I'm not sure if the <code>apply()</code> method accesses the series plus its individual cells or if I need another way of stepping into each record so tried both (I think!)</p>

<p><code>from nltk.stem import SnowballStemmer</code><br>
<code>stemmer = nltk.stem.SnowballStemmer('english')</code></p>

I've tried:

<p><code>df_2['Headline'] = df_2['Headline'].apply(lambda x: stemmer.stem(item) for item in x)</code></p>

<blockquote>
  <p>--------------------------------------------------------------------------- TypeError                                 Traceback (most recent call
  last)  in ()
  ----> 1 df_2['Headline__'] = df_2['Headline'].apply(lambda x: stemmer.stem(item) for item in x)</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\envs\learn-env\lib\site-packages\pandas\core\series.py
  in apply(self, func, convert_dtype, args, **kwds)    3192<br>
  else:    3193                 values = self.astype(object).values
  -> 3194                 mapped = lib.map_infer(values, f, convert=convert_dtype)    3195     3196         if len(mapped) and
  isinstance(mapped[0], Series):</p>
  
  <p>pandas/_libs/src\inference.pyx in pandas._libs.lib.map_infer()</p>
  
  <p>TypeError: 'generator' object is not callable</p>
</blockquote>

<p>I believe this TypeError is similar to the one that says 'List' object is not callable and fixed that one with the <code>apply()</code> method and out of ideas here.  </p>

<p><code>df_2['Headline'] = df_2['Headline'].apply(lambda x: stemmer.stem(x))</code></p>

<blockquote>
  <p>--------------------------------------------------------------------------- AttributeError                            Traceback (most recent call
  last)  in ()
  ----> 1 df_2['Headline'] = df_2['Headline'].apply(lambda x: stemmer.stem(x))
        2 
        3 df_2.head()</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\envs\learn-env\lib\site-packages\pandas\core\series.py
  in apply(self, func, convert_dtype, args, **kwds)    3192<br>
  else:    3193                 values = self.astype(object).values
  -> 3194                 mapped = lib.map_infer(values, f, convert=convert_dtype)    3195     3196         if len(mapped) and
  isinstance(mapped[0], Series):</p>
  
  <p>pandas/_libs/src\inference.pyx in pandas._libs.lib.map_infer()</p>
  
  <p> in (x)
  ----> 1 df_2['Headline'] = df_2['Headline'].apply(lambda x: stemmer.stem(x))
        2 
        3 df_2.head()</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\envs\learn-env\lib\site-packages\nltk\stem\snowball.py
  in stem(self, word)    1415     1416         """"""
  -> 1417         word = word.lower()    1418     1419         if word in self.stopwords or len(word) &lt;= 2:</p>
  
  <p>AttributeError: 'list' object has no attribute 'lower'</p>
</blockquote>
",Preprocessing of the text & Tokenization,execute nltk stem snowballstemmer panda four column dataframe two column tokenized word stop word removed converted lower case attempting stem sure method access series plus individual cell need another way stepping record tried think tried typeerror traceback recent call last df headline df headline apply lambda x stemmer stem item item x appdata local continuum anaconda envs learn env lib site package panda core series py apply self func convert dtype args kwds else value self astype object value mapped lib map infer value f convert convert dtype len mapped isinstance mapped series panda libs src inference pyx panda libs lib map infer typeerror generator object callable believe typeerror similar one say list object callable fixed one method idea attributeerror traceback recent call last df headline df headline apply lambda x stemmer stem x df head appdata local continuum anaconda envs learn env lib site package panda core series py apply self func convert dtype args kwds else value self astype object value mapped lib map infer value f convert convert dtype len mapped isinstance mapped series panda libs src inference pyx panda libs lib map infer x df headline df headline apply lambda x stemmer stem x df head appdata local continuum anaconda envs learn env lib site package nltk stem snowball py stem self word word word lower word self stopwords len word attributeerror list object ha attribute lower
Hugging face - Efficient tokenization of unknown token in GPT2,"<p>I am trying to train a dialog system using GPT2. For tokenization, I am using the following configuration for adding the special tokens.</p>
<pre><code>from transformers import (
     AdamW,
     AutoConfig,
     AutoTokenizer,
     PreTrainedModel,
     PreTrainedTokenizer,
     get_linear_schedule_with_warmup,
)

SPECIAL_TOKENS = {
    &quot;bos_token&quot;: &quot;&lt;|endoftext|&gt;&quot;,
    &quot;eos_token&quot;: &quot;&lt;|endoftext|&gt;&quot;,
    &quot;pad_token&quot;: &quot;[PAD]&quot;,
    &quot;additional_special_tokens&quot;: [&quot;[SYS]&quot;, &quot;[USR]&quot;, &quot;[KG]&quot;, &quot;[SUB]&quot;, &quot;[PRED]&quot;, &quot;[OBJ]&quot;, &quot;[TRIPLE]&quot;, &quot;[SEP]&quot;, &quot;[Q]&quot;,&quot;[DOM]&quot;]
}
tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)
tokenizer.add_special_tokens(SPECIAL_TOKENS)
</code></pre>
<p>Next, when I am trying to tokenize a sequence(dialog's utterance) and later convert into ids, some of the most important tokens in my sequence are getting mapped as unknown tokens, since the ids of these important tokens becomes the same as bos and eos as they all map to &lt;|endoftext|&gt; as in the GPT2's <a href=""https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/gpt2/tokenization_gpt2.py#L104"" rel=""nofollow noreferrer"">source code</a>.</p>
<p>Here is a working example -</p>
<pre><code>tokenized_sequence = ['[PRED]', 'name', '[SUB]', 'frankie_and_bennys', '[PRED]', 'address', '[SUB]', 'cambridge_leisure_park_clifton_way_cherry_hinton', '[PRED]', 'area', '[SUB]', 'south', '[PRED]', 'food', '[SUB]', 'italian', '[PRED]', 'phone', '[SUB]', '01223_412430', '[PRED]', 'pricerange', '[SUB]', 'expensive', '[PRED]', 'postcode', '[SUB]', 'cb17dy']
important_tokens = ['frankie_and_bennys','cambridge_leisure_park_clifton_way_cherry_hinton','italian','postcode', 'cb17dy']
tokens_to_ids = [50262, 3672, 50261, 50256, 50262, 21975, 50261, 50256, 50262, 20337, 50261, 35782, 50262, 19425, 50261, 50256, 50262, 4862, 50261, 50256, 50262, 50256, 50261, 22031, 50262, 50256, 50261, 50256]
ids_to_tokens = [PRED]name[SUB]&lt;|endoftext|&gt;[PRED]address[SUB]&lt;|endoftext|&gt;[PRED]area[SUB]south[PRED]food[SUB]&lt;|endoftext|&gt;[PRED]phone[SUB]&lt;|endoftext|&gt;[PRED]&lt;|endoftext|&gt;[SUB]expensive[PRED]&lt;|endoftext|&gt;[SUB]&lt;|endoftext|&gt;
</code></pre>
<p>As you can see the important_tokens are being mapped to the id  50256 (that is to |endoftext|), the model fails to see and learn these important tokens and hence generate very poor and often hallucinated responses.</p>
<p>What could be a quick and efficient fix for this issue?</p>
",Preprocessing of the text & Tokenization,hugging face efficient tokenization unknown token gpt trying train dialog system using gpt tokenization using following configuration adding special token next trying tokenize sequence dialog utterance later convert id important token sequence getting mapped unknown token since id important token becomes bos eos map endoftext gpt source code working example see important token mapped id endoftext model fails see learn important token hence generate poor often hallucinated response could quick efficient fix issue
Efficient way to check if a large list of words exists in millions of search queries,"<ol>
<li>I have a list of strings containing 50 million search queries. [1-500+ words in each query]. </li>
<li>I also have a list of strings containing 500 words and phrases 
I need to return indices of search queries (1) that contain any word or phrase (2). </li>
</ol>

<p>The goal is to only keep queries related to a certain topic (movies) and then use NLP to cluster these filtered queries (stemming -> tf_idf -> pca -> kmeans).</p>

<p>I tried to filter queries using nested loops, but it would take more than 10 hours to finish. </p>

<pre><code>filtered = []
with open('search_logs.txt', 'r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        query, timestamp = line.strip().split('\t')
        for word in key_words:
            if word in query:
                filtered.append(i)
</code></pre>

<p>I looked into solutions which use regex (word1|word2|...|wordN), but the problem is that i cannot combine queries into a large string since i need to filter irrelevant queries.</p>

<p>UPDATE: examples of logs and keywords</p>

<pre><code>search_logs.txt
'query  timestamp\n'
'the dark knight    2019-02-17 19:05:12\n'
'how to do a barrel roll    2019-02-17 19:05:13\n'
'watch movies   2019-02-17 19:05:13\n'
'porn   2019-02-17 19:05:13\n'
'news   2019-02-17 19:05:14\n'
'rami malek 2019-02-17 19:05:14\n'
'Traceback (most recent call last): File ""t.py"" 2019-02-17 19:05:15\n'
.......... # millions of other search queries
</code></pre>

<pre><code>key_words = [
    'movie',
    'movies',
    'cinema',
    'oscar',
    'oscars',
    'george lucas',
    'ben affleck',
    'netflix',
    .... # hundreds of other words and phrases
]
</code></pre>
",Preprocessing of the text & Tokenization,efficient way check large list word exists million search query list string containing million search query word query also list string containing word phrase need return index search query contain word phrase goal keep query related certain topic movie use nlp cluster filtered query stemming tf idf pca kmeans tried filter query using nested loop would take hour finish looked solution use regex word word wordn problem combine query large string since need filter irrelevant query update example log keywords
nltk : How to prevent stemming of proper nouns,"<p>I am trying to wrote a keyword extraction program using Stanford POS taggers and NER. For keyword extraction, i am only interested in proper nouns. Here is the basic approach</p>

<ol>
<li>Clean up the data by removing anything but alphabets</li>
<li>Remove stopwords</li>
<li>Stem each word</li>
<li>Determine POS tag of each word</li>
<li>If the POS tag is a noun then feed it to the NER</li>
<li>The NER will then determine if the word is a person, organization or location</li>
</ol>

<p>sample code</p>

<pre><code>docText=""'Jack Frost works for Boeing Company. He manages 5 aircraft and their crew in London""

words = re.split(""\W+"",docText) 

stops = set(stopwords.words(""english""))

#remove stop words from the list
words = [w for w in words if w not in stops and len(w) &gt; 2]

# Stemming
pstem = PorterStemmer()

words = [pstem.stem(w) for w in words]    

nounsWeWant = set(['NN' ,'NNS', 'NNP', 'NNPS'])

finalWords = []

stn = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') 
stp = StanfordPOSTagger('english-bidirectional-distsim.tagger') 

for w in words:
    if stp.tag([w.lower()])[0][1] not in nounsWeWant:
        finalWords.append(w.lower())
    else:
        finalWords.append(w)

finalString = "" "".join(finalWords)
print finalString

tagged = stn.tag(finalWords)
print tagged
</code></pre>

<p>which gives me</p>

<pre><code>Jack Frost work Boe Compani manag aircraft crew London
[(u'Jack', u'PERSON'), (u'Frost', u'PERSON'), (u'work', u'O'), (u'Boe', u'O'), (u'Compani', u'O'), (u'manag', u'O'), (u'aircraft', u'O'), (u'crew', u'O'), (u'London', u'LOCATION')]
</code></pre>

<p>so clearly, i did not want Boeing to be stemmed. nor Company. I need to stem the words as my input might contain terms like <code>Performing</code>. I have seen that a word like <code>Performing</code> will be picked up by the NER as a proper noun and hence could be categorized as  <code>Organization</code>. Hence, first i stem all the words and convert to lower case. Then i check to see if the POS tag of the word is a noun. If so, i keep it as is. If not, i convert the word to lower case and add it to the final word list that will be passed to the NER.</p>

<p>Any idea on how to avoid stemming proper nouns?</p>
",Preprocessing of the text & Tokenization,nltk prevent stemming proper noun trying wrote keyword extraction program using stanford po tagger ner keyword extraction interested proper noun basic approach clean data removing anything alphabet remove stopwords stem word determine po tag word po tag noun feed ner ner determine word person organization location sample code give clearly want boeing stemmed company need stem word input might contain term like seen word like picked ner proper noun hence could categorized hence first stem word convert lower case check see po tag word noun keep convert word lower case add final word list passed ner idea avoid stemming proper noun
"Using Natural Language Processing, how can we add our own Stop Words to a list?","<p>I am testing the library below, based on this code sample:</p>
<pre><code>import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from collections import Counter

df_new = pd.DataFrame(['okay', 'yeah', 'thank', 'im'])
stop_words = text.ENGLISH_STOP_WORDS.union(df_new)
#stop_words

w_counts = Counter(w for w in ' '.join(df['text_without_stopwords']).split() if w.lower() not in stop_words)


df_words = pd.DataFrame.from_dict(w_counts, orient='index').reset_index()
df_words.columns = ['word','count']


import seaborn as sns
# selecting top 20 most frequent words
d = df_words.nlargest(columns=&quot;count&quot;, n = 25) 
plt.figure(figsize=(20,5))
ax = sns.barplot(data=d, x= &quot;word&quot;, y = &quot;count&quot;)
ax.set(ylabel = 'Count')
plt.show()
</code></pre>
<p>I'm seeing this chart.</p>
<p><a href=""https://i.sstatic.net/CJpwt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CJpwt.png"" alt=""enter image description here"" /></a></p>
<p>I'm trying to add these words to stop words: 'okay', 'yeah', 'thank', 'im'</p>
<p>But...they are all coming through!! What's wrong here??</p>
",Preprocessing of the text & Tokenization,using natural language processing add stop word list testing library based code sample seeing chart trying add word stop word okay yeah thank im coming wrong
How to stop spaCy tokenizer from tokenizing words enclosed within brackets,"<p>I'm trying to make the spaCy tokenizer avoid certain words enclosed by brackets, like <code>[intervention]</code>. However, no matter what I try, I cannot get the right code to include a rule or an exception.  Here is an example of some input and the expected output:</p>
<p><strong>Example input:</strong></p>
<blockquote>
<p>A randomized, prospective study of [intervention]endometrial resection[intervention] to prevent [condition]recurrent endometrial polyps[condition]</p>
</blockquote>
<p><strong>Example output:</strong></p>
<pre class=""lang-none prettyprint-override""><code>A
randomized
,
prospective
study
of
[intervention]
endometrial
resection
[intervention]
to
prev

... et cetera
</code></pre>
<p>I've tried adding a special case like this:</p>
<pre class=""lang-py prettyprint-override""><code>nlp = spacy.load('en_core_web_sm')

case = [{ORTH: &quot;[intervention]&quot;}]
    
nlp.tokenizer.add_special_case('[intervention]',case)
    
# Test the custom tokenizer
text = combined_abstracts
doc = nlp(text)
for token in doc:
    print(token.text)
</code></pre>
<p>But the tokenizer is still splitting the word. Does anybody know a way to either make the tokenizer ignore certain words or all of the words enclosed by brackets? Either option would be useful to me.</p>
",Preprocessing of the text & Tokenization,stop spacy tokenizer tokenizing word enclosed within bracket trying make spacy tokenizer avoid certain word enclosed bracket like however matter try get right code include rule exception example input expected output example input randomized prospective study intervention endometrial resection intervention prevent condition recurrent endometrial polyp condition example output tried adding special case like tokenizer still splitting word doe anybody know way either make tokenizer ignore certain word word enclosed bracket either option would useful
Spacy tokenizer with only &quot;Whitespace&quot; rule,"<p>I would like to know if the spacy tokenizer could tokenize words only using the &quot;space&quot; rule.
For example:</p>
<pre><code>sentence= &quot;(c/o Oxford University )&quot;
</code></pre>
<p>Normally, using the following configuration of spacy:</p>
<pre><code>nlp = spacy.load(&quot;en_core_news_sm&quot;)
doc = nlp(sentence)
for token in doc:
   print(token)
</code></pre>
<p>the result would be:</p>
<pre><code> (
 c
 /
 o
 Oxford
 University
 )
</code></pre>
<p>Instead, I would like an output like the following (using spacy):</p>
<pre><code>(c/o 
Oxford 
University
)
</code></pre>
<p>Is it possible to obtain a result like this using spacy?</p>
",Preprocessing of the text & Tokenization,spacy tokenizer whitespace rule would like know spacy tokenizer could tokenize word using space rule example normally using following configuration spacy result would instead would like output like following using spacy possible obtain result like using spacy
Finding the original form of a word after stemming,"<p>I am stemming a list of words and making a dataframe from it. The original data is as follow:</p>
<pre><code>original = 'The man who flies the airplane dies in an air crash. His wife died a couple of weeks ago.'
df = pd.DataFrame({'text':[original]})
</code></pre>
<p>the functions I've used for lemmatisation and stemming are:</p>
<pre><code># lemmatize &amp; stemmed.
def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))

def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS:
            result.append(lemmatize_stemming(token))
    return result
</code></pre>
<p>The output will come from running <code>df['text'].map(preprocess)[0]</code> for which I get:</p>
<pre><code>['man',
 'fli',
 'airplan',
 'die',
 'air',
 'crash',
 'wife',
 'die',
 'coupl',
 'week',
 'ago']
</code></pre>
<p>I wonder how can I return the output to the original tokens? for instance I have die which is from died and dies.</p>
",Preprocessing of the text & Tokenization,finding original form word stemming stemming list word making dataframe original data follow function used lemmatisation stemming output come running get wonder return output original token instance die died dy
"Sentence tokenization splits sentences where there is dialogue followed by &#39;he/she said&#39;, how can I fix this?","<p>I am extracting sentences from pdfs into an excel spreadsheet. It handles most cases fine, but when there is a dialogue which ends in ? or ! and is followed by something like 'he said.' it splits the quote and the 'he said' into separate sentences.</p>
<p>For example: <em>&quot;Ingaba ibhasi isishiyile?&quot; babebuza.</em> which means <em>&quot;Has the bus left?&quot; they asked.</em>
gets split into <em>&quot;Ingaba ibhasi isishiyile?&quot;</em> and <em>babebuza.</em></p>
<p>How can I fix my code so it doesn't split these type of sentences?
Note that it's not just in the case of <em>babebuza</em> (they asked) it happens with all cases where the quotes ends in ? or !</p>
<p>This is what I have tried but no luck so far.</p>
<p>I tried to code for instances where a sentence ends in ?&quot; or !&quot; and is followed by a word/sentence that starts with a lower case letter- so that these get treated as one sentence, but this didn't give the desired outcome.</p>
<pre><code>import pdfplumber
import pandas as pd
import nltk
import re
import glob

def exclude_page_numbers(page_text, page_elements, exclusion_regions):
    filtered_text = &quot;&quot;

    for element in page_elements:
        text = element[&quot;text&quot;]
        x, y = element[&quot;x0&quot;], element[&quot;top&quot;]  # Get the x and y coordinates of the element

        # Check if the element falls within any exclusion region
        exclude = False
        for region in exclusion_regions:
            x_min, y_min, x_max, y_max = region
            if x_min &lt;= x &lt;= x_max and y_min &lt;= y &lt;= y_max:
                exclude = True
                break

        if not exclude and not any(char.isdigit() for char in text) and not text.strip().isdigit():
            filtered_text += text + &quot; &quot;  # Add a space after each element

    return filtered_text


def extract_sentences(text):
    # Replace “ and ” with normal double quotes &quot;
    text = text.replace(&quot;“&quot;, '&quot;').replace(&quot;”&quot;, '&quot;')
       
    # Replace ‘ and ’ with normal single quotes '
    text = text.replace(&quot;‘&quot;, &quot;'&quot;).replace(&quot;’&quot;, &quot;'&quot;)

    # Split text on occurrences of `.`, `!`, or `?` while considering quotation marks within the sentence
    sentences = re.split(r'(?&lt;=[.!?])(?=(?:[^&quot;]*&quot;[^&quot;]*&quot;)*[^&quot;]*$)', text)
    sentences = [sentence.strip() for sentence in sentences]

    # Merge the closing quotation mark with the preceding sentence
    merged_sentences = []
    i = 0
    while i &lt; len(sentences):
        if sentences[i].endswith('&quot;'):
            if i + 1 &lt; len(sentences):
                merged_sentence = sentences[i] + ' ' + sentences[i + 1]
                merged_sentences.append(merged_sentence)
                i += 2
            else:
                merged_sentences.append(sentences[i])
                i += 1
        else:
            merged_sentences.append(sentences[i])
            i += 1

    # Join adjacent sentences if the first sentence ends with ?&quot; or !&quot; and the second sentence starts with a lowercase letter
    final_sentences = []
    i = 0
    while i &lt; len(merged_sentences):
        if i + 1 &lt; len(merged_sentences):
            first_sentence = merged_sentences[i]
            second_sentence = merged_sentences[i + 1]
            if re.search(r'[?!]&quot;?\s*$', first_sentence) and re.search(r'^[a-z]', second_sentence):
                joined_sentence = first_sentence + ' ' + second_sentence
                final_sentences.append(joined_sentence)
                i += 2
                continue
        final_sentences.append(merged_sentences[i])
        i += 1

    # Tokenize final sentences using nltk.sent_tokenize
    tokenized_sentences = []
    for sentence in final_sentences:
        tokenized_sentences.extend(nltk.sent_tokenize(sentence))

    return tokenized_sentences

def extract_text_from_pdf(pdf_file_path):
    with pdfplumber.open(pdf_file_path) as pdf:
        num_pages = len(pdf.pages)
        text = &quot;&quot;
        for page in range(3, num_pages - 2):
            page_obj = pdf.pages[page]
            page_text = page_obj.extract_text()
            page_elements = page_obj.extract_words()  # Obtain the text elements and their positions
            filtered_text = exclude_page_numbers(page_text, page_elements, exclusion_regions)
   def scrape_sentences_to_excel(pdf_file_paths, excel_file_path):
    sentences = []
    for pdf_file_path in pdf_file_paths:
        # Extract text from PDF
        pdf_text = extract_text_from_pdf(pdf_file_path)

        # Extract sentences from the text
        extracted_sentences = extract_sentences(pdf_text)
        sentences.extend(extracted_sentences)

    # Create a pandas DataFrame with a single column for sentences
    df = pd.DataFrame({'Sentences': sentences})

    # Write the DataFrame to an Excel file
    df.to_excel(excel_file_path, index=False)

    print(&quot;Sentences extracted and saved to Excel successfully!&quot;)

pdf_folder_path = 'C:/Users/Paige Cox/Desktop/Little Zebra Books/pdfs'  # Folder path containing PDFs
excel_file_path = 'C:/Users/Paige Cox/Desktop/Little Zebra Books/pdfs/sentence_corpus_xhosa.xlsx'  # Output Excel file path

# Specify the exclusion regions as (x_min, y_min, x_max, y_max)
exclusion_regions = [(42, 558, 359, 559)]  # Add your exclusion regions here

# Get a list of PDF files in the folder
pdf_file_paths = glob.glob(pdf_folder_path + '/*.pdf')

# Scrape sentences from PDFs and save to Excel
scrape_sentences_to_excel(pdf_file_paths, excel_file_path)

</code></pre>
",Preprocessing of the text & Tokenization,sentence tokenization split sentence dialogue followed said fix extracting sentence pdfs excel spreadsheet handle case fine dialogue end followed something like said split quote said separate sentence example ingaba ibhasi isishiyile babebuza mean ha bus left asked get split ingaba ibhasi isishiyile babebuza fix code split type sentence note case babebuza asked happens case quote end tried luck far tried code instance sentence end followed word sentence start lower case letter get treated one sentence give desired outcome
How to Sentence Tokenize a List of Strings while maintaining the information of what strings constitute each sentence?,"<p>I have a list of strings as below(found from an OCR on a pdf) , and for each string in the list, I also have the co-ordinates of their position in the pdf</p>
<pre><code>[&quot;Much of Singapore's infrastructure had been destroyed during the war&quot;,  
&quot;including those needed to supply utilities.&quot;
&quot;A shortage of food led to malnutrition, disease,&quot;  
&quot; and rampant crime and violence.&quot;   
&quot;A series of strikes in 1947 caused massive&quot;,    
&quot;stoppages in public transport and other services.&quot; ]
</code></pre>
<p>I want to sentence tokenize the &quot;concatenated list&quot; (the blob of text created if I concatenate each item in the list) , but I also want to keep the info of which items in the list constitute each sentence.</p>
<p>Currently, I am simply concatenating each item in the list --&gt; feeding it to nltk/spacy --&gt; getting sentences.  I get the below result:</p>
<pre><code>[&quot;Much of Singapore's infrastructure had been destroyed during the war,‘including those needed to supply utilities.&quot;,  
&quot;A shortage of food led to malnutrition, disease, and rampant crime and violence.&quot;,  
&quot;A series of strikes in 1947 caused massive stoppages in public transport and other services.&quot;]
</code></pre>
<p>But I am not finding a way to know which Items from the list constitute the sentence..<br />
I basically need a mapping like <code>{sentence 1: (line 1 ,line 2) ,  sentence 2: (line 3,line 4)}</code></p>
<p>I need such a mapping for retaining the co-ordinate info (mentioned above) when I go from lines--&gt;sentences</p>
<p>The OCR service I am using is azure form recognizer</p>
",Preprocessing of the text & Tokenization,sentence tokenize list string maintaining information string constitute sentence list string found ocr pdf string list also co ordinate position pdf want sentence tokenize concatenated list blob text created concatenate item list also want keep info item list constitute sentence currently simply concatenating item list feeding nltk spacy getting sentence get result finding way know item list constitute sentence basically need mapping like need mapping retaining co ordinate info mentioned go line sentence ocr service using azure form recognizer
Stop spacy from deleting stopwords in split strings,"<p>I'm trying to use spacy to remove stopwords from a panda dataframe created from a csv.
My issue is that I'm trying to account for words that might have a mix of words and numbers.</p>
<p><strong>My issue:</strong></p>
<p>If a number separates a word so that it contains a stop word,
it will delete that portion of the word.</p>
<pre><code>    Ex. With stop word at the end
        Input: 'co555in'
        Breaks up the word, separating it in 'co'+ 555 + 'in'
        Removes 'in' because it is a stop word.
        Output: 'co555'

    Ex. Without stop word at the end
        Input: 'co555inn'
        Breaks up the word, separating it in 'co'+ 555 + 'inn'
        Will not remove 'inn' because it is not a stop word.
        Output: 'co555inn'
</code></pre>
<p><strong>Current implementation:</strong></p>
<pre><code>    df[col] = df[col].apply(lambda text: 
            &quot;&quot;.join(token.lemma_ for token in nlp(text) 
            if not token.is_stop))
</code></pre>
<p>So what I'd like is to be able to account for numbers and words mixed without spacy filtering out the portion of the word if the number separates the string so that it contains a stopword.</p>
<p>UPDATE: According to the devs this is a feature and not a bug. So a workaround, like the answer below, is necessary to account for these edge cases.</p>
",Preprocessing of the text & Tokenization,stop spacy deleting stopwords split string trying use spacy remove stopwords panda dataframe created csv issue trying account word might mix word number issue number separate word contains stop word delete portion word current implementation like able account number word mixed without spacy filtering portion word number separate string contains stopword update according devs feature bug workaround like answer necessary account edge case
Is there a way to tokenize sentences with Longformer?,"<p>I have forked the Multimodal Transformers package and created a new version with Longformer support here --&gt; <a href=""https://github.com/jtfields/Multimodal-Toolkit-Longformer/tree/master"" rel=""nofollow noreferrer"">https://github.com/jtfields/Multimodal-Toolkit-Longformer/tree/master</a>.  Georgian.io maintains the Multimodal Transformers package and here are their comments on the error message I'm receiving:</p>
<blockquote>
<p>&quot;Hey @jtfields, I haven't had a chance to look at your code but
judging by the error, it sounds like longformers might have an
additional step required. Specifically, your forward() method returns
a different than expected shape.</p>
<p>Looking at this in particular: RuntimeError: stack expects each tensor
to be equal size, but got [32, 2, 768] at entry 0 and [32, 43] at
entry 1</p>
<p>It looks like your outputs are of the shape (batch_size,
sequence_length, embedding_dim). This corresponds to having an
embedding for every word in the output I.E., word embeddings. However,
what we want is a sentence embedding where we have one embedding for
every sentence (or paragraph). So instead, the shape we want is
(batch_size, embedding_dim).</p>
<p>Unfortunately there's no ready answer I have on how to get that.
Different models have different best practices. BERT-based models use
the embedding of the [CLS] token to get sentence embeddings, while
others such as XLM use an additional layer to do this task (see the
sequence_summary bits in
multimodal_transformers/model/tabular_transformers.py). I'm not
familiar with longformers so I can't tell you exactly what to do, but
I'm sure that there's a standard method people use for it.&quot;</p>
</blockquote>
<p>Does anyone have any suggestions for how to change the tokenization to work with Longformer in this package?</p>
<p>I'm not sure if changing the code below in the notebook is the best approach or changing the code in tabular_transformers.py:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer_path_or_name = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path
print('Specified tokenizer: ', tokenizer_path_or_name)
tokenizer = AutoTokenizer.from_pretrained(
    tokenizer_path_or_name,
    cache_dir=model_args.cache_dir,
)
</code></pre>
",Preprocessing of the text & Tokenization,way tokenize sentence longformer forked multimodal transformer package created new version longformer support georgian io maintains multimodal transformer package comment error message receiving hey jtfields chance look code judging error sound like longformers might additional step required specifically forward method return different expected shape looking particular runtimeerror stack expects tensor equal size got entry entry look like output shape batch size sequence length embedding dim corresponds embedding every word output e word embeddings however want sentence embedding one embedding every sentence paragraph instead shape want batch size embedding dim unfortunately ready answer get different model different best practice bert based model use embedding cl token get sentence embeddings others xlm use additional layer task see sequence summary bit multimodal transformer model tabular transformer py familiar longformers tell exactly sure standard method people use doe anyone suggestion change tokenization work longformer package sure changing code notebook best approach changing code tabular transformer py
Extract tables from text using regular expression,"<p>I want to extract table from the example text as follows but my out put is not correct. My sample text, code, current output and expected output is as follows. any help is appreciated.</p>
<blockquote>
<p>dic = {'ID': ':  ID\nRespondent ID\nBased upon 3,302 valid cases out
of 3,302 total cases.\n•Mean: 54361.66\n•Minimum: 10005.00\n•Maximum:
99992.00\n•Standard Deviation: 25723.74\nLocation: 1-5 (width: 5; decimal: 0)\nVariable Type:  numeric \n',  'VIS': ':  visit\nSwan
study visit number\nValue Label Unweighted\nFrequency%\n00- 3302 100.0
%\n Total 3,302 100%\nBased upon 3,302 valid cases out of 3,302 total
cases.\nLocation: 6-7 (width: 2; decimal: 0)\nVariable Type:
character \n',  'INT': ':  day\nDate form completed\nValue Label
Unweighted\nFrequency%\n0- 3302 100.0 %\n Total 3,302 100%\nBased upon
3,302 valid cases out of 3,302 total cases.\n•Mean: 0.00\n•Median:
0.00\n•Mode: 0.00\n•Minimum: 0.00\n•Maximum: 0.00\n•Standard Deviation: 0.00\nLocation: 8-8 (width: 1; decimal: 0)\nVariable Type:
numeric \n',  'AG0': ': Age in months\n- 2 -Value Label
Unweighted\nFrequency%\n42- 367 11.1 %\n43- 421 12.7 %\n44- 416 12.6
%\n45- 389 11.8 %\n46- 400 12.1 %\n47- 392 11.9 %\n48- 299 9.1 %\n49-
255 7.7 %\n50- 168 5.1 %\n51- 115 3.5 %\n52- 71 2.2 %\n53- 40.1 %\n
Missing Data   \n.- 50.2 %\n Total 3,302 100%\nBased upon 3,297 valid
cases out of 3,302 total cases.\n•Mean: 45.85\n•Median: 46.00\n•Mode:
43.00\n•Minimum: 42.00\n•Maximum: 53.00\n•Standard Deviation: 2.69\nLocation: 9-10 (width: 2; decimal: 0)\nVariable Type:  numeric \n',  'PRE0': ': Currently preant?\nAre you currently pnant?\nValue
Label Unweighted\nFrequency%\n1No 3295 99.8 %\n2Yes 00.0 %\n Missing
Data   \n-9Missing 70.2 %\n Total 3,302 100%\nBased upon 3,295 valid
cases out of 3,302 total cases.\n•Minimum: 1.00\n•Maximum:
1.00\nLocation: 11-12 (width: 2; decimal: 0)\nVariable Type:  numeric \n- 3 -(Range of) Missing Values:  -9 , -8 , -7 , -1\n',}</p>
</blockquote>
<pre><code>import re
import pandas as pd

data = {}
for key, value in dic.items():
    regex = r'(Value\s+Label\s+Unweighted\s+Frequency%\s+(?:Missing\s+Data\s+)?[\s\S]+?)\n(?=\S)'
    match = re.search(regex, value, flags=re.DOTALL)
    if match:
        rows = [re.split(r&quot;\s+&quot;, row.strip()) for row in match.group(1).strip().split(&quot;\n&quot;)]
        df = pd.DataFrame(rows, columns=[&quot;Value&quot;, &quot;Label&quot;, &quot;Unweighted&quot;,&quot; Frequency%&quot;])
        df[&quot;Variable&quot;] = key
        data[key] = df

data[&quot;VISIT&quot;]
</code></pre>
<p>the current output:</p>
<pre><code>    Value       Label   Unweighted  Frequency%  Variable
0   Value       Label   Unweighted  None        VISIT
1   Frequency%  None    None        None        VISIT
2   00-         3302    100.0       %           VISIT
3   Total      3,302    100%        None        VISIT
</code></pre>
<p>Expected output:
This is only an example for few keys. I need this for all keys of dictionary</p>
<pre><code>dic = {id:&quot;no_table&quot;,VIS:Value  Label  Unweighted       Frequency%
                         00     -            3302            100.0 %
                                Total        3,302           100%
                     ,INT:Value Label  Unweighted       Frequency%
                         00     -            3302            100.0 %
                                Total        3,302           100% 
                      ,}
  
</code></pre>
<p>Update :</p>
<pre><code>import re
import pandas as pd

# Define the regular expression pattern to match the table rows
pattern = r'(?P&lt;Value&gt;[\w.-]+)\s+(?P&lt;Label&gt;.+?)\s+(?P&lt;Unweighted_Frequency&gt;.+%)'


# Initialize an empty list to store the rows
rows = []

# Loop through the dictionary
for key, text in dic.items():
    # Find all the matches of the pattern in the text
    matches = re.findall(pattern, text)
    if matches:
        # Add the matches to the rows list with the key as the first column
        rows.extend([(key,) + match for match in matches])

# Create a pandas DataFrame from the rows list with the desired column names
df = pd.DataFrame(rows, columns=['Variable', 'Value', 'Label', 'Unweighted Frequency%'])
</code></pre>
<p>current out put is close but I dont know how to fix it :
1- what is under Unweighted Frequency% should be under only %
2- what is under Label should be under Unweighted Frequency
3 - what is under value is correct except for when the value is 1No , 1 should be under value and No should be under Label.
4- the last row is missing</p>
<p>This is not the full output only a sample.
Variable  Value   Label   Unweighted Frequency%</p>
<pre><code>7           AG0     42-     367      11.1 %
8           AG0     43-     421      12.7 %
9           AG0     44-     416      12.6 %
10          AG0     45-     389      11.8 %
18          AG0 Missing Data    .-   50.2 %
19          AG0     Total   3,302    100%
20          PRE0    Value   Label   Unweighted  Frequency%
21          PRE0    1No     3295     99.8 %
22          PRE0    Missing Data    -9Missing 70.2 %
23          PRE0    Total   3,302    100%'''
</code></pre>
",Preprocessing of the text & Tokenization,extract table text using regular expression want extract table example text follows put correct sample text code current output expected output follows help appreciated dic id id nrespondent id nbased upon valid case total case n mean n minimum n maximum n standard deviation nlocation width decimal nvariable type numeric n vi visit nswan study visit number nvalue label unweighted nfrequency n n total nbased upon valid case total case nlocation width decimal nvariable type character n int day ndate form completed nvalue label unweighted nfrequency n n total nbased upon valid case total case n mean n median n mode n minimum n maximum n standard deviation nlocation width decimal nvariable type numeric n ag age month n value label unweighted nfrequency n n n n n n n n n n n n n missing data n n total nbased upon valid case total case n mean n median n mode n minimum n maximum n standard deviation nlocation width decimal nvariable type numeric n pre currently preant nare currently pnant nvalue label unweighted nfrequency n n yes n missing data n missing n total nbased upon valid case total case n minimum n maximum nlocation width decimal nvariable type numeric n range missing value n current output expected output example key need key dictionary update current put close dont know fix unweighted frequency label unweighted frequency value correct except value value label last row missing full output sample variable value label unweighted frequency
Tokenize list of strings without comma separation,"<p>I'm still new to Python and want to know how I can tokenize a list of strings without every word being separated by a comma.</p>
<p>For example, starting from a list like ['I have to get groceries.','I need some bananas.','Anything else?'], I want to obtain a list like this: ['I have to get groceries .', 'I need some bananas .', 'Anything else ?']. The point is thus not to create a list with separate tokens necessarily, but to create a list with sentences in which all words and punctuation marks are separated from each other.</p>
<p>Any ideas? I only managed to create a list of comma-separated tokens, using this code:</p>
<pre class=""lang-py prettyprint-override""><code>import nltk
nltk.download('punkt')
from nltk import word_tokenize 
tokenized = []
for line in unique:
      tokenized.append(word_tokenize(line))
</code></pre>
",Preprocessing of the text & Tokenization,tokenize list string without comma separation still new python want know tokenize list string without every word separated comma example starting list like get grocery need banana anything else want obtain list like get grocery need banana anything else point thus create list separate token necessarily create list sentence word punctuation mark separated idea managed create list comma separated token using code
spaCY lemmatizer different results on repeated words,"<p>The lemmatization of the following sentence</p>
<ul>
<li>&quot;Not finished! Not finished! A gem for our adopted daughter, Kiri, - - born of Grace's avatar, - - and whose conception was a complete mystery.&quot;*</li>
</ul>
<p>has given a little bit confusing result</p>
<p>['not', 'finished', 'not', 'finish', 'a', 'gem', 'for', 'our', 'adopt', 'daughter', 'bear', 'of', 'avatar', 'and', 'whose', 'conception', 'be', 'a', 'complete', 'mystery']</p>
<p>The thing is that first word &quot;finished&quot; and &quot;second&quot; word &quot;finished&quot; were detected as different POS.</p>
<p>['PART', 'AUX', 'PART', 'VERB', 'DET', 'NOUN', 'ADP', 'PRON', 'VERB', 'NOUN', 'PROPN', 'VERB', 'ADP', 'PROPN', 'NOUN', 'CCONJ', 'DET', 'NOUN', 'AUX', 'DET', 'ADJ', 'NOUN']</p>
<p>First one was detected as an auxilary, and the second one was detected as a verb, as had been expected.</p>
<p>Adding third &quot;Not finished!&quot; gave the followinf result:</p>
<p>['not', 'finish', 'not', 'finish', 'not', 'finish', 'a', 'gem', 'for', 'our', 'adopt', 'daughter', 'bear', 'of', 'avatar', 'and', 'whose', 'conception', 'be', 'a', 'complete', 'mystery']</p>
<p>Removing one of them</p>
<p>['not', 'finish', 'a', 'gem', 'for', 'our', 'adopt', 'daughter', 'bear', 'of', 'avatar', 'and', 'whose', 'conception', 'be', 'a', 'complete', 'mystery']</p>
<p>Even four repetition let to get the expected result</p>
<p>['not', 'finish', 'not', 'finish', 'not', 'finish', 'not', 'finish', 'a', 'gem', 'for', 'our', 'adopt', 'daughter', 'bear', 'of', 'avatar', 'and', 'whose', 'conception', 'be', 'a', 'complete', 'mystery']</p>
<p>I find it difficult to find both a logical explanation and a workaround that will allow solving the problem not only with a specific example.</p>
",Preprocessing of the text & Tokenization,spacy lemmatizer different result repeated word lemmatization following sentence finished finished gem adopted daughter kiri born grace avatar whose conception wa complete mystery ha given little bit confusing result finished finish gem adopt daughter bear avatar whose conception complete mystery thing first word finished second word finished detected different po part aux part verb det noun adp pron verb noun propn verb adp propn noun cconj det noun aux det adj noun first one wa detected auxilary second one wa detected verb expected adding third finished gave followinf result finish finish finish gem adopt daughter bear avatar whose conception complete mystery removing one finish gem adopt daughter bear avatar whose conception complete mystery even four repetition let get expected result finish finish finish finish gem adopt daughter bear avatar whose conception complete mystery find difficult find logical explanation workaround allow solving problem specific example
Removing html code from text data in Spark,"<p>i'm working with Stack Exchange data dumps and I want to clean body of a question from code which occurs really often.</p>
<p>I tried using BeautifulSoup but it leaves the text which occurs inside the code and I want to remove everything between  &lt; pre &gt;&lt; code&gt;  and &lt; /code &gt; &lt; /pre &gt;. Maybe it can be done with regular expressions? Preferably with Spark.</p>
",Preprocessing of the text & Tokenization,removing html code text data spark working stack exchange data dump want clean body question code occurs really often tried using beautifulsoup leaf text occurs inside code want remove everything pre code code pre maybe done regular expression preferably spark
lemmatization or normalization using a dictionary and list of variations,"<p>I have a pandas data frame with string column which is a transaction string column. I am trying to some manual lemmatization. I have manually created a dictionary which has the main word as the key and a list of variations of the words as the values. I would like to substitute the words in the list with the main word.</p>
<p>here is the example code of the data I have.</p>
<pre><code>import pandas as pd
list1 = ['0412 UBER TRIP HELP.UBER.COMCA',
'0410 UBER TRIP HELP.UBER.COMCA',
'MOBILE PURCHASE 0410 VALENCIA WHOLE FOODS SAN FRANCISCOCA',
'WHOLEFDS WBG#1 04/13 PURCHASE WHOLEFDS WBG#104 BROOKLYN NY',
'0414 LYFT *CITI BIKE BIK LYFT.COM CA',
'0421 WALGREENS.COM 877-250-5823 IL',
'0421 Rapha Racing PMT LLC XXX-XX72742 OR',
'0422 UBER EATS PAYMENT HELP.UBER.COMCA',
'0912 WHOLEFDS NOE 10379 SAN FRANCISCOCA',
'PURCHASE 1003 CAVIAR*JUNOON WWW.DOORDASH.CA']
df = pd.DataFrame(list1, columns = ['feature'])

map1 = {'payment':['pmts','pmnt','pmt','pmts','pyment','pymnts'],
'account':['acct'],
 'pharmacy':['walgreens','walgreen','riteaid','cvs','pharm'],
 'food_delivery':['uber eats','doordash','seamless','grubhub','caviar'],
 'ride_share':['uber','lyft'],
 'whole_foods':['wholefds','whole foods','whole food']
}
</code></pre>
<p>I know how to do it one word at a time using <code>df['feature'].str.replace('variation','main word')</code>. However, this is laborious and time consuming. Is there a faster way to do this? Thank you.</p>
",Preprocessing of the text & Tokenization,lemmatization normalization using dictionary list variation panda data frame string column transaction string column trying manual lemmatization manually created dictionary ha main word key list variation word value would like substitute word list main word example code data know one word time using however laborious time consuming faster way thank
A machine Learning model to find similarities between two words in python,"<p>I have 2 lists of words. The first list contains 5 words. The second list contains 1000s of words.</p>
<p>I am looking for a ML model that will help me find the best match between the words in the first list to the words in the second list, by assigning a score between all possible pairs from the first list to the second list. Highest score means best match. score of 1.0 means perfect match.</p>
<p>For example, list A has the word <code>Light Classic</code>. List B has <code>Classical Music</code>, <code>Rock</code> and <code>Opera</code>.</p>
<ol>
<li>Score between <code>Light Classic</code> and <code>Classical Music</code> is 0.82</li>
<li>Score between <code>Light Classic</code> and <code>Rock</code> is 0.23</li>
<li>Score between <code>Light Classic</code> and <code>Opera</code> is 0.54</li>
</ol>
<p>Therefore the best match for <code>Light Classic</code> is <code>Classical Music</code></p>
<p>This image shows more examples: <a href=""https://i.sstatic.net/Wwh21.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Wwh21.png"" alt=""Mapping between List A and List B"" /></a></p>
<p>Currently I am using <code>sentence_transformers</code> and <code>all-mpnet-base-v2</code> to find these scores. For scores, I am using <code>cosine score</code>. The code I am using is shown below:</p>
<pre><code>from tqdm import tqdm
from sentence_transformers import SentenceTransformer, util

model_name = 'all-mpnet-base-v2'
model = SentenceTransformer(model_name)

mapping = {}
for word_a in list_a:
    word_a = word_a.lower()
    mapping[word_a] = {
        'score': 0
        'list_b': ''
    }
    embedding_word_a = model.encode(word_a, convert_to_tensor = True)

    for word_b in list_b:
        word_b = word_b.lower()

        if (word_a == word_b):
            mapping[word_a]['score'] = 1.0
            mapping[word_a]['list_b'] = word_b
            break

        embedding_word_b = model.encode(word_b, convert_to_tensor = True)
            cosine_score = round((util.cos_sim(embedding_word_a, embedding_word_b)).item(), 2)
            if (cosine_score &gt; mapping[word_a]['score']):
                mapping[word_a]['score'] = cosine_score
                mapping[word_a]['list_b'] = word_b
print(mapping)
</code></pre>
<p>While this works fine, I have two questions:</p>
<ol>
<li>The model I am using has an average performance of 63.30 <a href=""https://www.sbert.net/docs/pretrained_models.html"" rel=""nofollow noreferrer"">here</a>. Is there a better approach or model or method that I can use?</li>
<li>This is pretty slow, as I am comparing each of the 5 words in list a to all the 1000+ words in list b, thus it is slow. Is there a faster approach or model?</li>
</ol>
<p>I am on <code>python v3.9.16</code></p>
",Preprocessing of the text & Tokenization,machine learning model find similarity two word python list word first list contains word second list contains word looking ml model help find best match word first list word second list assigning score possible pair first list second list highest score mean best match score mean perfect match example list ha word list b ha score score score therefore best match image show example currently using find score score using code using shown work fine two question model using ha average performance better approach model method use pretty slow comparing word list word list b thus slow faster approach model
How do I do word Stemming or Lemmatization?,"<p>I've tried PorterStemmer and Snowball but both don't work on all words, missing some very common ones. </p>

<p>My test words are: ""<strong>cats running ran cactus cactuses cacti community communities</strong>"", and both get less than half right.</p>

<p><strong>See also:</strong></p>

<ul>
<li><a href=""https://stackoverflow.com/questions/190775"">Stemming algorithm that produces real words</a></li>
<li><a href=""https://stackoverflow.com/questions/595110"">Stemming - code examples or open source projects?</a></li>
</ul>
",Preprocessing of the text & Tokenization,word stemming lemmatization tried porterstemmer snowball work word missing common one test word cat running ran cactus cactus cactus community community get le half right see also
Fuzzy string matching in Python for structured strings?,"<p>I have a Python implementation of fuzzy matching using the Levenshtein similarity. I'm pretty happy with it but I feel I'm leaving a lot on the table by not considering the structure of the strings.</p>
<p>Here are some examples of matches that are clearly good, but not captured well by Levenshtein :</p>
<ul>
<li><code>The Hobbit</code>  / <code>Hobbit/The</code></li>
<li><code>Charlies Angles</code> / <code>Charlie's Angels</code></li>
<li><code>Apples &amp; Pairs</code> / <code>Apples and Pairs</code></li>
</ul>
<p>I think some normalization ahead of using Levenshtein would be good - eg. replace all <code>&amp;</code> with <code>and</code>, remove punctuation, etc...  not sure I want to jump straight to stop-word removal and lematization, but something along those line</p>
<p>To avoid re-inventing the wheel, is there any easy way to do this? Or an alternative to levenshtine that addresses these issues (short of some Bert embeddings)</p>
",Preprocessing of the text & Tokenization,fuzzy string matching python structured string python implementation fuzzy matching using levenshtein similarity pretty happy feel leaving lot table considering structure string example match clearly good captured well levenshtein think normalization ahead using levenshtein would good eg replace remove punctuation etc sure want jump straight stop word removal lematization something along line avoid inventing wheel easy way alternative levenshtine address issue short bert embeddings
How to get the feature names on the shap plot from an XGBoost Model?,"<p>I have a dataset with a text column (&quot;description&quot;), 2 categorical columns (&quot;type&quot; and &quot;action&quot;) and a binary variable (&quot;score&quot;).</p>
<pre><code>y = project['score']
X = project(['type', 'action','description'], axis=1)
</code></pre>
<p>After processing the 'description' field (removing accents, punctuation, stop words, and stemming), I am vectorizing it with TfidfVectorizer and creating a sparse matrix.</p>
<pre><code>tfidf = TfidfVectorizer(lowercase=False, max_features=50) 
vectorizer = TfidfVectorizer(binary=True, ngram_range=(1, 2)) 
y = project[&quot;score&quot;].values.astype(np.float32) 
X = sp.sparse.hstack((vectorizer.fit_transform(project.description), project[['type','action']].values), dtype=float, format='csr') 
X_columns=vectorizer.get_feature_names_out()
</code></pre>
<p>Then, I split into training and test.</p>
<pre><code>train, test, y_train, y_test = train_test_split(X, y, random_state=42) 
</code></pre>
<p>After that, I create an XGBoost model to predict the project score (0,1):</p>
<pre><code>xgb_model = xgb.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=100, objective='binary:logistic', random_state=42) 
xgb_model.fit(train, y_train) y_pred_xgb = xgb_model.predict_proba(test)[:, 1] 
xgb_performance = roc_auc_score(y_test, y_pred_xgb) 
</code></pre>
<p>Later, I am trying to plot the SHAP graph:</p>
<pre><code>explainer = shap.Explainer(xgb_model) 
shap_values = explainer(test) 
shap.summary_plot(shap_values, features=test, feature_names=X_columns) 
</code></pre>
<p>I need shap to show me the feature names on the plot, instead of the number of the features, as it is showing now:</p>
<p><a href=""https://i.sstatic.net/FCla6.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Here is some sample data:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>description</th>
<th>type</th>
<th>action</th>
<th>score</th>
</tr>
</thead>
<tbody>
<tr>
<td>tecnolog cinacalcet indicaca hiperparatireoid</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>tecnolog medic gefitinib iress indicaca linh</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>tecnolog citogene hibridizaca situ fluorescenc</td>
<td>2</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>tecnolog paricalcitol zempl indicaca trat prev</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>tecnolog canaquinumab ilaril indicaca sindrom</td>
<td>0</td>
<td>2</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>I have already tried feature_names=X_columns, but I get an error &quot;index 77810 is out of bounds for axis 0 with size 77810&quot;.</p>
",Preprocessing of the text & Tokenization,get feature name shap plot xgboost model dataset text column description categorical column type action binary variable score processing description field removing accent punctuation stop word stemming vectorizing tfidfvectorizer creating sparse matrix split training test create xgboost model predict project score later trying plot shap graph need shap show feature name plot instead number feature showing enter image description sample data description type action score tecnolog cinacalcet indicaca hiperparatireoid tecnolog medic gefitinib ire indicaca linh tecnolog citogene hibridizaca situ fluorescenc tecnolog paricalcitol zempl indicaca trat prev tecnolog canaquinumab ilaril indicaca sindrom already tried feature name x column get error index bound axis size
How to correctly remove stop words using tidytext package in R?,"<p>I am using stopwords dataset in <code>tidytext</code> package in <code>R</code> to remove stopwords. I am using following code:</p>
<pre><code>library(tidyverse)
library(tidytext)
library(dplyr)

data(stop_words)
example_words &lt;- c(&quot;the&quot;, &quot;quick&quot;, &quot;brown&quot;, &quot;fox&quot;, &quot;jumps&quot;, &quot;over&quot;, &quot;the&quot;, &quot;lazy&quot;, &quot;dog&quot;,&quot;i'm&quot;,&quot;don’t&quot;,&quot;it’s&quot;,&quot;i’ve&quot;)
filtered_words &lt;- example_words[!example_words %in% stop_words$word]
filtered_words 
</code></pre>
<p>The final output is as follows:</p>
<pre><code>&gt; filtered_words
[1] &quot;quick&quot; &quot;brown&quot; &quot;fox&quot;   &quot;jumps&quot; &quot;lazy&quot;  &quot;dog&quot;   &quot;don’t&quot; &quot;it’s&quot;  &quot;i’ve&quot; 
</code></pre>
<p>We can see the stop words like &quot;don’t&quot; &quot;it’s&quot;  &quot;i’ve&quot; still presented in the filtered output. But those stop words are actually presented in the stop word dataset and somehow not get removed. So could anyone help me to figure out why is it not removing some of these words that are presented in the stop words dataset?</p>
",Preprocessing of the text & Tokenization,correctly remove stop word using tidytext package r using stopwords dataset package remove stopwords using following code final output follows see stop word like still presented filtered output stop word actually presented stop word dataset somehow get removed could anyone help figure removing word presented stop word dataset
Extract ONLY valid bi-grams from a sentence,"<p>Looking for some ideas on how to tackle this problem that we have ..
we want to validate if a particular BI-Gram phrase is a valid phrase or not ..
say we have a sentence like &quot;Xyz,inc is heavily investing into Digital Transformation&quot; ...after stop word removal say we are left with &quot;Xyz inc heavily investing Digital Transformation&quot;</p>
<p>if we extract BI-Grams from this cleaned sentence we are having</p>
<ul>
<li>Xyz,inc heavily</li>
<li>heavily Investing</li>
<li>investing Digital</li>
<li>Digital Transformation</li>
</ul>
<p>In these combinations ONLY &quot;Digital Transformation&quot; makes sense ..</p>
<p>Any idea on how we can filter other bi-gram phrases extracted</p>
",Preprocessing of the text & Tokenization,extract valid bi gram sentence looking idea tackle problem want validate particular bi gram phrase valid phrase say sentence like xyz inc heavily investing digital transformation stop word removal say left xyz inc heavily investing digital transformation extract bi gram cleaned sentence xyz inc heavily heavily investing investing digital digital transformation combination digital transformation make sense idea filter bi gram phrase extracted
Fixing spacing after removing stopwords,"<p>I have run a code to remove stopwords, but the output has removed the space between the words.</p>
<p>The initial input before the stopword removal is like this:</p>
<pre><code>form server happen
</code></pre>
<p>The word output looks like this:</p>
<pre><code>formserverhappen 
</code></pre>
<p>Code:</p>
<pre><code>#Removing stopwords

def remove_stopwords(text,nlp,custom_stop_words=None,remove_small_tokens=True,min_len=2):
    # if custom stop words are provided, then add them to default stop words list
    if custom_stop_words:
        nlp.Defaults.stop_words |= custom_stop_words
    
    filtered_sentence = [] 
    doc=nlp(text)
    for token in doc:
        
        if token.is_stop == False: 
            
            # if small tokens have to be removed, then select only those which are longer than the min_len 
            if remove_small_tokens:
                if len(token.text)&gt;min_len:
                    filtered_sentence.append(token.text)
            else:
                filtered_sentence.append(token.text)
    # if after the stop word removal, words are still left in the sentence, then return the sentence as a string else return null 
    return '  '.join(filtered_sentence) if len(filtered_sentence)&gt;0 else None
#creating a spaCy object. 
nlp = spacy.load(&quot;en_core_web_sm&quot;, disable=[&quot;parser&quot;, &quot;ner&quot;])

#removing stop-words and short words from every row
negative.ctstring=negative.ctstring.apply(lambda x:remove_stopwords(x,nlp,{&quot;mask&quot;,&quot;mandates&quot;,}))
</code></pre>
<p><a href=""https://i.sstatic.net/4MbQX.png"" rel=""nofollow noreferrer"">Picture of the code</a></p>
<p>I looked that join but I can't figure out why the change is occuring.</p>
",Preprocessing of the text & Tokenization,fixing spacing removing stopwords run code remove stopwords output ha removed space word initial input stopword removal like word output look like code picture code looked join figure change occuring
Removal of Stop Words and Stemming/Lemmatization for BERTopic,"<p>For Topic Modelling, I'm trying out the BERTopic: <a href=""https://maartengr.github.io/BERTopic/index.html"" rel=""nofollow noreferrer"">Link</a></p>
<p>I'm little confused here, I am trying out the BERTopic on my custom Dataset. <br />
Since BERT was trained in such a way that it holds the semantic meaning of the text/document,
Should I be removing the stop words and stem/lemmatize my documents before passing it onto BERTopic?
Because I'm afraid if these stopwords might land into my topics as salient terms which they are not</p>
<p>Suggestions and Advices please!</p>
",Preprocessing of the text & Tokenization,removal stop word stemming lemmatization bertopic topic modelling trying bertopic link little confused trying bertopic custom dataset since bert wa trained way hold semantic meaning text document removing stop word stem lemmatize document passing onto bertopic afraid stopwords might land topic salient term suggestion advice please
pyLDAvis | Could I get &quot;Top-30 Most Relevant Terms for Topic&quot;?,"<p>During the Topicmodeling visualization through LDAvis, I found that Slide to adjust relevance metric varies depending on the topic and lambda values. Is there a way to get this word list?
I want to get the representative words that vary depending on the lambda value.</p>
",Preprocessing of the text & Tokenization,pyldavis could get top relevant term topic topicmodeling visualization ldavis found slide adjust relevance metric varies depending topic lambda value way get word list want get representative word vary depending lambda value
Creating an elbow plot with k-means clustering,"<p>I am clustering many texts using K-means. Now I am trying to decide on the optimal number of clusters by creating an elbow plot. However, I did not succeed yet.</p>
<p>My code looks like this, where corpus</p>
<pre><code>corpus = df['content'].tolist()
language = 'dutch'
corpus = processCorpus(corpus, language)
</code></pre>
<p>Followed by multiple functions in order to remove stopwords, words with less than 2 letters, stemming etc. Consequently, this is followed by:</p>
<pre><code>
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)
x_norm = normalize(X)
tf_idf = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names())

final_df = tf_idf
final_df.to_excel(&quot;test_defensie.xlsx&quot;)

print(&quot;{} rows&quot;.format(final_df.shape[0]))
final_df.T.nlargest(5, 0)

def run_KMeans(max_k, data):
    max_k += 1
    kmeans_results = dict()
    for k in range(2 , max_k):
        kmeans = cluster.KMeans(n_clusters = k
                               , init = 'k-means++'
                               , n_init = 10
                               , tol = 0.0001
                               , n_jobs = -1
                               , random_state = 1
                               , algorithm = 'elkan')

        kmeans_results.update( {k : kmeans.fit(data)} )
        
    return kmeans_results
def printAvg(avg_dict):
    for avg in sorted(avg_dict.keys(), reverse=True):
        print(&quot;Avg: {}\tK:{}&quot;.format(avg.round(4), avg_dict[avg]))
# Running Kmeans
k = 12
kmeans_results = run_KMeans(k, final_df)
best_result = 8
kmeans = kmeans_results.get(best_result)

final_df_array = final_df.to_numpy()
prediction = kmeans.predict(final_df)
n_feats = 20
dfs = get_top_features_cluster(final_df_array, prediction, n_feats)
</code></pre>
<p>I tried to add the following code but I get the following error: PCA does not support sparse input. See TruncatedSVD for a possible alternative</p>
<pre><code>number_clusters = range(1, 10)
sklearn_pca = PCA(n_components = 2)
Y_sklearn = sklearn_pca.fit_transform(x_norm_array)
kmeans = [KMeans(n_clusters=i, max_iter = 600) for i in number_clusters]
kmeans
score = [kmeans[i].fit(Y_sklearn).score(Y_sklearn) for i in range(len(kmeans))]
score
plt.plot(number_clusters, score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Method')
plt.show()

</code></pre>
",Preprocessing of the text & Tokenization,creating elbow plot k mean clustering clustering many text using k mean trying decide optimal number cluster creating elbow plot however succeed yet code look like corpus followed multiple function order remove stopwords word le letter stemming etc consequently followed tried add following code get following error pca doe support sparse input see truncatedsvd possible alternative
Natural Language Processing Database Querying,"<p>I need to develop natural language querying tool for a structured database. I tried two approaches.</p>

<ul>
<li>using Python nltk (Natural Language Toolkit for python) using</li>
<li>Javascript and JSON (for data source)</li>
</ul>

<p>In the first case I did some NLP steps to format the natural query by doing removing stop words, stemming, finally mapping keywords using featured grammar mapping. This methodology works for simple scenarios.</p>

<p>Then I moved to second approach. Finding the data in JSON and getting corresponding column name and table name , then building a sql query. For this one,  I also implemented removing stop words, stemming using javascript.</p>

<p>Both of these techniques have limitations.I want to implement semantic search approach.</p>

<p>Please can anyone suggest me better approach to do this..</p>
",Preprocessing of the text & Tokenization,natural language processing database querying need develop natural language querying tool structured database tried two approach using python nltk natural language toolkit python using javascript json data source first case nlp step format natural query removing stop word stemming finally mapping keywords using grammar mapping methodology work simple scenario moved second approach finding data json getting corresponding column name table name building sql query one also implemented removing stop word stemming using javascript technique limitation want implement semantic search approach please anyone suggest better approach
changing the output of text_tokens function in R,"<p>I have a question redarding text mining with the <code>corpus package</code> and the function <code>text_tokens()</code>. I want to use the function for stemming and deleting stop words. I have a huge amount of data (almost 1.000.000 comments) where I want to use it for. But I've problems with the output, the function <code>text_tokens</code> produces. So here is a basic example of my data and code:</p>
<pre><code>library(tidyverse)
library(corpus)
library(stopwords)

text &lt;- data.frame(comment_id = 1:2,
                   comment_content = c(&quot;Hallo mein Name ist aaron&quot;,&quot;Vielen Lieben Dank für das Video&quot;))


tmp &lt;- text_tokens(text$comment_content, 
                   text_filter(stemmer = &quot;de&quot;,drop = stopwords(&quot;german&quot;)))
</code></pre>
<p>My problem now is, that I want a <code>data.frame</code> as output with the comment_id in the first column and word_token in the column. So the output I would like to have looks as followed:</p>
<pre><code>df &lt;- data.frame(comment_id = c(1,1,1,2,2,2),
                 comment_tokens = c(&quot;hallo&quot;,&quot;nam&quot;,&quot;aaron&quot;,&quot;lieb&quot;,&quot;dank&quot;,&quot;video&quot;))
</code></pre>
<p><a href=""https://i.sstatic.net/MgBkw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MgBkw.png"" alt=""output I need"" /></a></p>
<p>I tried different <code>do.calls</code> (cbind/rbind), but they don't give me the result I need. So what is the function I'm looking for, is it <code>map()</code> from the tidyverse?</p>
<p>Thank you in advance.</p>
<p>Cheers,</p>
<p>Aaron</p>
",Preprocessing of the text & Tokenization,changing output text token function r question redarding text mining function want use function stemming deleting stop word huge amount data almost comment want use problem output function produce basic example data code problem want output comment id first column word token column output would like look followed tried different cbind rbind give result need function looking tidyverse thank advance cheer aaron
Python NLP processing if statement not in stop words list,"<p>I'm working with NLP <code>spacy</code> library and I created a function to return a list of token from a text.</p>
<pre><code>import spacy    
def preprocess_text_spacy(text):
    stop_words = [&quot;a&quot;, &quot;the&quot;, &quot;is&quot;, &quot;are&quot;]
    nlp = spacy.load('en_core_web_sm')
    tokens = set()
    doc = nlp(text)
    for word in doc:
        if word.is_currency:
            tokens.add(word.lower_)
        elif len(word.lower_) == 1:
            if word.is_digit and float(word.text) == 0:
                tokens.add(word.text)
        elif not word.is_punct and not word.is_space and not word.is_quote and not word.is_bracket and not in stop_words:
            tokens.add(word.lower_)
    return list(tokens)
</code></pre>
<p>This function is not correct because removing stop words not working.
Everything is ok only if I delete the last condition <code>and not in stop_words</code>.</p>
<p>How to upgrade this function to remove stop words according a defined list in addition to all other condition statement?</p>
",Preprocessing of the text & Tokenization,python nlp processing statement stop word list working nlp library created function return list token text function correct removing stop word working everything ok delete last condition upgrade function remove stop word according defined list addition condition statement
Byte pair encoding when most bytes are already used,"<p>Byte pair encoding is apparently sometimes used to compress/tokenize text prior to running machine learning algorithms on it.</p>
<p>According to <a href=""https://en.wikipedia.org/wiki/Byte_pair_encoding"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Byte_pair_encoding</a> the basic step is one in which</p>
<blockquote>
<p>the most common pair of contiguous bytes of data in a sequence are replaced with a byte that does not occur within the sequence</p>
</blockquote>
<p>I can see how this works for ASCII, which typically leaves about 160 possible bytes unused.</p>
<p>It would seem to be inapplicable for binary data, which in general would use all the possible byte values.</p>
<p>What about Unicode? That uses a lot more of the possible byte values than ASCII. Does the algorithm work less well here, does Unicode use fewer byte values than I am taking into account, or is there something else I am missing?</p>
",Preprocessing of the text & Tokenization,byte pair encoding byte already used byte pair encoding apparently sometimes used compress tokenize text prior running machine learning algorithm according basic step one common pair contiguous byte data sequence replaced byte doe occur within sequence see work ascii typically leaf possible byte unused would seem inapplicable binary data general would use possible byte value unicode us lot possible byte value ascii doe algorithm work le well doe unicode use fewer byte value taking account something else missing
Removing punctuation using spaCy; AttributeError,"<p>Currently I'm using the following code to lemmatize and calculate TF-IDF values for some text data using spaCy:</p>

<pre><code>lemma = []

for doc in nlp.pipe(df['col'].astype('unicode').values, batch_size=9844,
                        n_threads=3):
    if doc.is_parsed:
        lemma.append([n.lemma_ for n in doc if not n.lemma_.is_punct | n.lemma_ != ""-PRON-""])
    else:
        lemma.append(None)

df['lemma_col'] = lemma

vect = sklearn.feature_extraction.text.TfidfVectorizer()
lemmas = df['lemma_col'].apply(lambda x: ' '.join(x))
vect = sklearn.feature_extraction.text.TfidfVectorizer()
features = vect.fit_transform(lemmas)

feature_names = vect.get_feature_names()
dense = features.todense()
denselist = dense.tolist()

df = pd.DataFrame(denselist, columns=feature_names)
df = pd.DataFrame(denselist, columns=feature_names)
lemmas = pd.concat([lemmas, df])
df= pd.concat([df, lemmas])
</code></pre>

<p>I need to strip out proper nouns, punctuation, and stop words but am having some trouble doing that within my current code.  I've read some <a href=""https://nicschrading.com/project/Intro-to-NLP-with-spaCy/"" rel=""nofollow noreferrer"">documentation</a> and <a href=""https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/"" rel=""nofollow noreferrer"">other resources</a>, but am now running into an error:</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-21-e924639f7822&gt; in &lt;module&gt;()
      7     if doc.is_parsed:
      8         tokens.append([n.text for n in doc])
----&gt; 9         lemma.append([n.lemma_ for n in doc if not n.lemma_.is_punct or n.lemma_ != ""-PRON-""])
     10         pos.append([n.pos_ for n in doc])
     11     else:

&lt;ipython-input-21-e924639f7822&gt; in &lt;listcomp&gt;(.0)
      7     if doc.is_parsed:
      8         tokens.append([n.text for n in doc])
----&gt; 9         lemma.append([n.lemma_ for n in doc if not n.lemma_.is_punct or n.lemma_ != ""-PRON-""])
     10         pos.append([n.pos_ for n in doc])
     11     else:

AttributeError: 'str' object has no attribute 'is_punct'
</code></pre>

<p>Is there an easier way to strip this stuff out of the text, without having to drastically change my approach?</p>

<p>Full code available <a href=""https://github.com/LizMGagne/TIP_code/blob/master/TIP%20Stuff%20(8).ipynb"" rel=""nofollow noreferrer"">here</a>.</p>
",Preprocessing of the text & Tokenization,removing punctuation using spacy attributeerror currently using following code lemmatize calculate tf idf value text data using spacy need strip proper noun punctuation stop word trouble within current code read documentation resource running error easier way strip stuff text without drastically change approach full code available
SpaCy STOP_WORDS for keywords still returning unwanted words,"<p>I am trying to filter out keywords from spaCy AI's STOP_WORDS using the below code, which I believe should work. However words contained in both the spaCy word list and my own when added are still returned to the user? Can you spot where I have go wrong as I am blind to it, please!?</p>
<pre><code>import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from pdfminer.high_level import extract_pages, extract_text
from sklearn.feature_extraction.text import TfidfVectorizer
import sqlite3
import streamlit as st

# Load the pre-trained spaCy model
nlp = spacy.load(&quot;en_core_web_sm&quot;)

# Define a set of custom stop words
STOP_WORDS |= {&quot;stop1&quot;,&quot;wordzz&quot;}

# Define a function to segment the document into smaller segments
def segment_document(doc, min_sentence_length=10, max_sentence_length=50, max_segment_length=220):
    segments = []
    current_segment = &quot;&quot;
    current_length = 0
    
    for sentence in doc.sents:
        if len(sentence) &lt; min_sentence_length:
            continue
        if len(sentence) &gt; max_sentence_length:
            continue
        
        if current_length + len(sentence) &gt; max_segment_length:
            segments.append(current_segment.strip())
            current_segment = &quot;&quot;
            current_length = 0
        
        current_segment += sentence.text
        current_length += len(sentence)
    
    if current_segment:
        segments.append(current_segment.strip())
    
    return segments

# Define a function to extract keywords and phrases from a text segment
def extract_keywords(segment, num_keywords=15):
    doc = nlp(segment)
   
    # Extract named entities and noun chunks
    entities = [entity.text for entity in doc.ents if entity.label_ != &quot;DATE&quot;]
    noun_chunks = [chunk.text for chunk in doc.noun_chunks]
    
    # Combine entities and noun chunks into a single list
    keywords = entities + noun_chunks
    
    # Filter out stop words
    filtered_keywords = [keyword for keyword in keywords if keyword not in STOP_WORDS]
    
    # Create a tf-idf vectorizer and fit it to the filtered keywords
    vectorizer = TfidfVectorizer()
    vectorizer.fit_transform(filtered_keywords)
    
    # Get the feature names and tf-idf scores for each keyword
    feature_names = vectorizer.get_feature_names_out()
    tfidf_scores = vectorizer.idf_
    
    # Combine the feature names and scores into a dictionary and sort by score
    keyword_scores = {}
    for i in range(len(feature_names)):
        keyword_scores[feature_names[i]] = tfidf_scores[i]
    
    sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)
    
    # Return the top num_keywords
    top_keywords = [keyword[0] for keyword in sorted_keywords[:num_keywords]]
    
    return top_keywords

    

# Define the Streamlit app
def app():
    st.title(&quot;Document Segmenter&quot;)
    st.write(&quot;This app segments a long multi-page document into smaller, coherent segments of text and extracts important keywords and phrases from each segment.&quot;)
    
    # Allow the user to upload a file
    doc_file = st.file_uploader(&quot;Upload a document&quot;, type=[&quot;pdf&quot;, &quot;txt&quot;])
    
    
    if doc_file is not None:
        # Load the document using spaCy
        text_input = extract_text(doc_file)
        doc = nlp(text_input)

        
        # Segment the document into smaller segments
        segments = segment_document(doc)
        
        # Extract keywords and phrases from each segment and display the results to the user
        for i, segment in enumerate(segments):
            st.write(f&quot;Segment {i+1}:&quot;)
            st.write(segment)
            st.write(&quot;---&quot;)
            st.write(&quot;Keywords and phrases:&quot;)
            keywords = extract_keywords(segment)
            for keyword in keywords:
                st.write(keyword)
            st.write(&quot;===&quot;)
        
if __name__ == &quot;__main__&quot;:
    app()

</code></pre>
<p>Words initially identified as keywords which can be found in the STOP_WORDS list should be dropped and not returned to the user.</p>
",Preprocessing of the text & Tokenization,spacy stop word keywords still returning unwanted word trying filter keywords spacy ai stop word using code believe work however word contained spacy word list added still returned user spot go wrong blind please word initially identified keywords found stop word list dropped returned user
Cleaning Up (or Avoiding) Extra Whitespace with PyPDF2,"<p>I've been extracting text from PDFs using PyPDF2. However it seems to be inputting erroneous white space in between words. Does anyone know of way to avoid this, or clean it after the fact? Here is an example:</p>

<blockquote>
  <p>'IN THE MATTER OF  an application submitted by 1113 York Avenue Realty
  Company,  L.L.C. and 60th Street Devel opment LLC pursuant to Sections
  197-c and 201 of the New York  City Charter for an amendment of th e
  Zoning Map, Section Nos. 8c and 8d:'</p>
</blockquote>

<p>Here ""development"" is spelt ""devel opment"" and ""the"" is the spelt ""th e"". I'd like to correct this.</p>

<p>Here is <a href=""http://www1.nyc.gov/assets/planning/download/pdf/about/cpc/000198.pdf"" rel=""nofollow noreferrer"">PDF</a>. The example text is from list item number 1, on the first page.</p>
",Preprocessing of the text & Tokenization,cleaning avoiding extra whitespace pypdf extracting text pdfs using pypdf however seems inputting erroneous white space word doe anyone know way avoid clean fact example matter application submitted york avenue realty company l l c th street devel opment llc pursuant section c new york city charter amendment th e zoning map section c development spelt devel opment spelt th e like correct pdf example text list item number first page
How to stem tokens using list comprehension?,"<p>I extracted a series of texts from an xml file (with BeautifulSoup) storing them in a list of strings(each string is a text). Now I want to modify that  list of strings with list comprehension so that it becomes a list of lists where each list-item contains the lowered stemmed words of the text without punctuation.</p>
<p>The problem is threefold:</p>
<p>a) I can't remove the <code>&quot; &quot;</code> element (I tried using <code>if word != &quot; &quot;</code> but did not have any effect)</p>
<p>b) when I use the <code>string library</code> to remove punctuation things like 26-year-old turn into 26yearold. How can I avoid that while removing punctuation (with string)</p>
<p>c) wasn't turn into wasnt</p>
<p>This is the list that I am storing everything. I want to remove the <code>&quot; &quot;</code>
element and find a way to parse better the phrases with <code>&quot;-&quot;</code></p>
<pre><code>list_of_texts = [[stem(word.lower().translate (word.maketrans('', '', string.punctuation)).replace(&quot;\n&quot;,  &quot; &quot;))  for word in text.split()] for text in list_of_texts]
</code></pre>
",Preprocessing of the text & Tokenization,stem token using list comprehension extracted series text xml file beautifulsoup storing list string string text want modify list string list comprehension becomes list list list item contains lowered stemmed word text without punctuation problem threefold remove element tried using effect b use remove punctuation thing like year old turn yearold avoid removing punctuation string c turn wasnt list storing everything want remove element find way parse better phrase
How to do text pre-processing using spaCy?,"<p>How to do preprocessing steps like Stopword removal , punctuation removal , stemming and lemmatization in spaCy using python.</p>

<p>I have text data in csv file like paragraphs and sentences. I want to do text cleaning. </p>

<p>Kindly give example by loading csv in pandas dataframe </p>
",Preprocessing of the text & Tokenization,text pre processing using spacy preprocessing step like stopword removal punctuation removal stemming lemmatization spacy using python text data csv file like paragraph sentence want text cleaning kindly give example loading csv panda dataframe
Gensim Word2Vec Error: ValueError: missing section header before line #0,"<p>I am new to Gensim Word2Vec. I was trying to use Word2Vec to build word vectors for some raw html files. So I first convert the html file into txt file.</p>

<h3>My First Question:</h3>

<p>When I train the word2vec model, everything is fine. But when I want to test the accuracy of the model by doing</p>

<pre><code>model.accuracy(file_name)
</code></pre>

<p>it produced error: </p>

<pre><code>Traceback (most recent call last):
  File ""build_w2v.py"", line 82, in &lt;module&gt;
    main()
  File ""build_w2v.py"", line 77, in main
    gen_w2v_model()
  File ""build_w2v.py"", line 71, in gen_w2v_model
    accuracy = model.accuracy(target)
  File ""/home/k/shankai/app/anaconda2/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 1330, in accuracy
    return self.wv.accuracy(questions, restrict_vocab, most_similar, case_insensitive)
  File ""/home/k/shankai/app/anaconda2/lib/python2.7/site-packages/gensim/models/keyedvectors.py"", line 679, in accuracy
    raise ValueError(""missing section header before line #%i in %s"" % (line_no, questions))
ValueError: missing section header before line #0
</code></pre>

<p>Below is the sample file:</p>

<pre><code>zGR='ca-about-health_js';var ziRfw=0;zobt="" Vision Ads"";zOBT="" Ads"";function zIpSS(u){zpu(0,u,280,375,""ssWin"")}function zIlb(l,t,f){zT(l,'18/1Pp/wX')}


zWASL=1;zGRH=1
#rs{margin:0 0 10px}#rs #n5{font-weight:bold}#rs a{padding:7px;text-transform:capitalize}Poking Eyelashes - Poking Eyelashes Problem


&lt;!--
zGOW=0;xd=0;zap="""";zAth='25752';zAthG='25752';zTt='11';zir='';zBTS=0;zBT=0;zSt='';zGz=''
ch='health';gs='vision';xg=""Vision"";zcs=''
zFDT='0'
zFST='0'
zOr='BA15WT26OkWA0O1b';zTbO=zRQO=1;zp0=zp1=zp2=zp3=zfs=0;zDc=1;
zSm=zSu=zhc=zpb=zgs=zdn='';zFS='BA110BA0110B00101';zFD='BA110BA0110B00101'
zDO=zis=1;zpid=zi=zRf=ztp=zpo=0;zdx=20;zfx=100;zJs=0;
zi=1;zz=';336280=2-1-1299;72890=2-1-1299;336155=2-1-12-1;93048=2-1-12-1;30050=2-1-12-1';zx='100';zde=15;zdp=1440;zds=1440;zfp=0;zfs=66;zfd=100;zdd=20;zaX=new Array(11, new Array(100,1051,8192,2,'336,300'),7, new Array(100,284,8196,12,'336,400'));zDc=1;;zDO=1;;zD336=1;zhc='';;zGTH=1;
zGo=0;zG=17;zTac=2;zDot=0;
zObT=""Vision"";zRad=5;var tp="" primedia_""+(zBT?"""":""non_"")+""site_targeting"";if(!this.zGCID)zGCID=tp
else zGCID+=tp;
if(zBT&gt;0){zOBR=1}
if(!this.uy)uy='about.com';if(typeof document.domain!=""undefined"")document.domain=uy;//--&gt;


function zob(p){if(!this.zOfs)return;var a=zOfs,t,i=0,l=a.length;if(l){w('&lt;div id=""oF""&gt;&lt;b&gt;'+(this.zobt?zobt:xg+' Ads')+'&lt;/b&gt;&lt;ul&gt;');while((i&lt;l)&amp;&amp;i&lt;zRad){t=a[i++].line1;w('&lt;li&gt;&lt;a href=""/z/js/o'+(p?p:'')+'.htm?k='+zUriS(t.toLowerCase())+(this.zobr?zobr:'')+'&amp;d='+zUriS(t)+'&amp;r='+zUriS(zWl)+'"" target=""_'+(this.zOBNW?'new'+zr(9999):'top')+'""&gt;'+t+'&lt;/a&gt;&lt;/li&gt;');}w('&lt;/ul&gt;&lt;/div&gt;')}}function rb600(){if(gEI('bb'))gEI('bb').height=600}zJs=10
zJs=11
zJs=12
zJs=13
zc(5,'jsc',zJs,9999999,'')
zDO=0
</code></pre>

<p>So This file actually begins with many (I don't know) space or \n. When I open in the vim.<a href=""https://i.sstatic.net/jOz4T.png"" rel=""nofollow noreferrer"">It looks like this</a>.</p>

<p><strong>So what is the problem here?</strong></p>

<h3>My second question:</h3>

<p>Also, I am doing text classification of some biomedical papers. The files I was given are all raw html files in either Japanese or English. After I do the ascii conversion and some stop_words cleaning, there are still many HTML code left in the file. </p>

<p>When I try to clean these files and restrict the characters to [a-zA-Z0-9], I found some medical terms like [4protein...] or something get not properly cleaned as well.</p>

<p><strong>Are there any suggestions in how to clean up these files?</strong></p>
",Preprocessing of the text & Tokenization,gensim word vec error valueerror missing section header line new gensim word vec wa trying use word vec build word vector raw html file first convert html file txt file first question train word vec model everything fine want test accuracy model produced error sample file file actually begin many know space n open vim look like problem second question also text classification biomedical paper file wa given raw html file either japanese english ascii conversion stop word cleaning still many html code left file try clean file restrict character za z found medical term like protein something get properly cleaned well suggestion clean file
TFIDFVectorizer making concatenated word tokens,"<p>I am using the <a href=""https://ir.dcs.gla.ac.uk/resources/test_collections/cran/"" rel=""nofollow noreferrer"">Cranfield Dataset</a> to make an Indexer and Query Processor. For that purpose I am using TFIDFVectorizer to tokenize the data. But after using TFIDFVectorizer when I check the vocabulary,there were lot of tokens formed using a concatenation of two words.</p>
<p>I am using the following code to achieve it:</p>
<pre><code>import re
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from nltk import word_tokenize          
from nltk.stem import WordNetLemmatizer
#reading the data
with open('cran.all', 'r') as f:
    content_string=&quot;&quot;
    content = [line.replace('\n','') for line in f]
    content =  content_string.join(content)
    doc=re.split('.I\s[0-9]{1,4}',content)
    f.close()
#some data cleaning
doc = [line.replace('.T',' ').replace('.B',' ').replace('.A',' ').replace('.W',' ') for line in doc]
del doc[0]
doc= [ re.sub('[^A-Za-z]+', ' ', lines) for lines in doc]




vectorizer = TfidfVectorizer(analyzer ='word', ngram_range=(1,1), stop_words=text.ENGLISH_STOP_WORDS,lowercase=True)
X = vectorizer.fit_transform(doc)
print(vectorizer.vocabulary_)
</code></pre>
<p>I have attached below a few examples I obtain when I print vocabulary:</p>
<p><code>'freevibration': 7222, 'slendersharp': 15197, 'frequentlyapproximated': 7249, 'notapplicable': 11347, 'rateof': 13727, 'itsvalue': 9443, 'speedflow': 15516, 'movingwith': 11001, 'speedsolution': 15531, 'centerof': 3314, 'hypersoniclow': 8230, 'neice': 11145, 'rutkowski': 14444, 'chann': 3381, 'layerapproximations': 9828, 'probsteinhave': 13353, 'thishypersonic': 17752</code></p>
<p>When I use with small data, it does not happen. How to prevent this from happening?</p>
",Preprocessing of the text & Tokenization,tfidfvectorizer making concatenated word token using cranfield dataset make indexer query processor purpose using tfidfvectorizer tokenize data using tfidfvectorizer check vocabulary lot token formed using concatenation two word using following code achieve attached example obtain print vocabulary use small data doe happen prevent happening
How do I merge rows of data with the same ID but different data in other columns and remove duplicate contiunous words?,"<p>I have a set of data in a pandas dataframe</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Date</th>
<th>Location</th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr>
<td>123456</td>
<td>01/01/23</td>
<td>Paris</td>
<td>Reported Problem:</td>
</tr>
<tr>
<td>123456</td>
<td>01/01/23</td>
<td>Paris</td>
<td>Reported Problem: One Plus One Is Not Three</td>
</tr>
<tr>
<td>123456</td>
<td>01/01/23</td>
<td>Paris</td>
<td>Reported Problem: One Plus One Is Not Three</td>
</tr>
<tr>
<td>123456</td>
<td>01/01/23</td>
<td>Paris</td>
<td>Reported Pro</td>
</tr>
<tr>
<td>123456</td>
<td>01/01/23</td>
<td>Paris</td>
<td>Reported Problem: One Plus One Is Not Three. Potato Grows On Tree</td>
</tr>
</tbody>
</table>
</div>
<p>I want to merge them all together so that they are only one row, and the text to merge together and delete the duplicate text. The desired result is</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Date</th>
<th>Location</th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr>
<td>123456</td>
<td>01/01/23</td>
<td>Paris</td>
<td>Reported Problem:\nReported Problem: One Plus One Is Not Three. Potato Grows On Tree</td>
</tr>
</tbody>
</table>
</div>
<p>I used groupby to merge the rows together, but there are duplicate that needs to be remove</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Date</th>
<th>Location</th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr>
<td>123456</td>
<td>01/01/23</td>
<td>Paris</td>
<td>Reported Problem:\nReported Problem: One Plus One Is Not Three\nReported Problem: One Plus One Is Not Three\nReported Pro\nReported Problem: One Plus One Is Not Three. Potato Grows On Tree</td>
</tr>
</tbody>
</table>
</div>
<p>I tried converting the merged data to a list and tokenize and segment the sentences using Stanza, using the newline to separate the sentences</p>
<p>But I'm not sure where to go from here
I'm using Stanza to split the sentences that way I can hopefully get sentences I need and put it into a PowerPoint with Python-pptx</p>
<pre><code>import pandas as pd

df = pd.read_csv('Samplepool.csv')

g = df['ID'].ffill()

d = df.groupby(g, sort=False).first()

d['Text'] = df['Text'].dropna().groupby(g).agg('!'.join)
d = d.reset_index(drop=True)

</code></pre>
<pre><code>import stanza
#stanza.download('en') # download English model
npl = stanza.Pipeline(lang='en', processors={'tokenize': 'TokenizeProcessor',},tokenize_no_ssplit=True)

sample2=d['Text'].to_list()
doc = npl(sample2)
#word = ['Report Problem']
for i, sentence in enumerate(doc.sentences):
    print(f'====== Sentence {i+1} tokens =======')
    print(*[token.text for token in sentence.tokens], sep='\n')



</code></pre>
",Preprocessing of the text & Tokenization,merge row data id different data column remove duplicate contiunous word set data panda dataframe id date location text paris reported problem paris reported problem one plus one three paris reported problem one plus one three paris reported pro paris reported problem one plus one three potato grows tree want merge together one row text merge together delete duplicate text desired result id date location text paris reported problem nreported problem one plus one three potato grows tree used groupby merge row together duplicate need remove id date location text paris reported problem nreported problem one plus one three nreported problem one plus one three nreported pro nreported problem one plus one three potato grows tree tried converting merged data list tokenize segment sentence using stanza using newline separate sentence sure go using stanza split sentence way hopefully get sentence need put powerpoint python pptx
NotImplementedError: The lemmatize parameter is no longer supported,"<p>I have run the code for my own similar gpt2 model, but the below Error was got it. How to solve this implement error in python.</p>
<pre><code>corpus = WikiCorpus(file_path, lemmatize=False, lower=False, tokenizer_func=tokenizer_func)
  File &quot;C:\Rayi\python\text-generate\text-gene\lib\site-packages\gensim\corpora\wikicorpus.py&quot;, line 619, in __init__
    raise NotImplementedError(
NotImplementedError: The lemmatize parameter is no longer supported. If you need to lemmatize, use e.g. &lt;https://github.com/clips/pattern&gt;. Perform lemmatization as part of your tokenization function and pass it as the tokenizer_func parameter to this initializer.
</code></pre>
<pre><code>import tensorflow as tf
from gensim.corpora import WikiCorpus
import os
import argparse

# lang = 'bn'

def store(corpus, lang):
    base_path = os.getcwd()
    store_path = os.path.join(base_path, '{}_corpus'.format(lang))
    if not os.path.exists(store_path):
        os.mkdir(store_path)
    file_idx=1
    for text in corpus.get_texts():
        current_file_path = os.path.join(store_path, 'article_{}.txt'.format(file_idx))
        with open(current_file_path, 'w' , encoding='utf-8') as file:
            file.write(bytes(' '.join(text), 'utf-8').decode('utf-8'))
        #endwith
        file_idx += 1
    #endfor

def tokenizer_func(text: str, token_min_len: int, token_max_len: int, lower: bool) -&gt; list:
    return [token for token in text.split() if token_min_len &lt;= len(token) &lt;= token_max_len]

def run(lang):
    origin='https://dumps.wikimedia.org/{}wiki/latest/{}wiki-latest-pages-articles.xml.bz2'.format(lang,lang)
    fname='{}wiki-latest-pages-articles.xml.bz2'.format(lang)
    file_path = tf.keras.utils.get_file(origin=origin, fname=fname, untar=False, extract=False)
    corpus = WikiCorpus(file_path, lemmatize=True, lower=False, tokenizer_func=tokenizer_func)
    store(corpus, lang)

if __name__ == '__main__':
    ARGS_PARSER = argparse.ArgumentParser()
    ARGS_PARSER.add_argument(
        '--lang',
        default='en',
        type=str,
        help='language code to download from wikipedia corpus'
    )
    ARGS = ARGS_PARSER.parse_args()
    run(**vars(ARGS))
</code></pre>
",Preprocessing of the text & Tokenization,notimplementederror lemmatize parameter longer supported run code similar gpt model error wa got solve implement error python
Trace various patterns strategically - python regex,"<p>I have to extract information from PDF documents, all documents have the same structure. I use the following regular expressions:</p>
<pre><code>regex_objetivos = r&quot;Objetivo([\s\S]*)(?=3\s*\.\s*Justi)&quot;
regex_claves = r&quot;Palabras\s+clave([\s\S]*?)(?:Intro|Introduc|1\s*.)&quot;
regex_resumen = r&quot;Resumen([\s\S]*?)(?=\s*Palabras\s*clave)&quot;
regex_directores =r&quot;Directores:\s*([\s\S]*?)(?:\n|\r\n?)&quot; here
</code></pre>
<p>The way I extract is as follows:</p>
<pre><code>            if not b_resumen:
                match = re.search(regex_resumen, text)
                if match:
                    b_resumen = True
                    resumen = match.group(1).strip()
                else:
                    resumen = &quot;no encontrado&quot;

            if not b_claves:
                b_claves = True
                match = re.search(regex_claves, text)
                #print(text)
                if match:
                    claves = match.group(1).strip()
                else:
                    claves = &quot;no encontrado&quot;

            if not b_directores:
                match = re.search(regex_directores, text)
                if match:
                    b_directores = True
                    directores = match.group(1).split(',')
                else:
                    directores = [&quot;no encontrado&quot;]

            if not b_objetivos:
                match = re.search(regex_objetivos, text)
                if match:
                    b_objetivos = True
                    objetivos = match.group(1)
                    break
                else:
                    match = re.search(r&quot;Objetivo([\s\S]*)(?=$)&quot;, text)
                    if match:
                        b_objetivos = True
                        objetivos = match.group(1)
                        break
                    else:
                        objetivos = &quot;no encontrado&quot;
</code></pre>
<p>My question is, if the documents always have the same structure, is there a way to optimize my match in such a way that the searches are sequential without searching from 0?</p>
<p>I try to optimize my code so that the automation time is less</p>
",Preprocessing of the text & Tokenization,trace various pattern strategically python regex extract information pdf document document structure use following regular expression way extract follows question document always structure way optimize match way search sequential without searching try optimize code automation time le
Break down text into units of sense - text segmentation NLP Python,"<p>I have a dataframe text column (in french) and I want to split each text into sentences by their meaning ( break down text into units of sense ), any idea how to do it with Python libraries and NLP techniques ?!</p>
<p>P.S I tried NLTK sent_tokenize and word tokenize but it’s not well split respecting the meaning</p>
<p><strong>For example</strong>:
“ text discussing sports and then economic and then school systems”
=&gt; I want to break down the text into sentences like this:</p>
<ul>
<li>sport related text</li>
<li>economic related text</li>
<li>school system related text</li>
</ul>
<p>Or at least extract tags out of the whole text, so for this example: I’ll have the following tags:
sports/economic/school.</p>
<p>If I can achieve one of these two cases would be great</p>
",Preprocessing of the text & Tokenization,break text unit sense text segmentation nlp python dataframe text column french want split text sentence meaning break text unit sense idea python library nlp technique p tried nltk sent tokenize word tokenize well split respecting meaning example text discussing sport economic school system want break text sentence like sport related text economic related text school system related text least extract tag whole text example following tag sport economic school achieve one two case would great
Contains function with string splitting,"<p>I am trying to use a contains() function to do matching on two columns when joining tables.
I have two problems</p>
<p><strong>Problem 1</strong></p>
<p>The data looks as such:</p>
<blockquote>
<p>col1: '[&quot;Red&quot;,&quot;Blue&quot;,&quot;Green&quot;,&quot;yes&quot;,&quot;purple&quot;,&quot;car&quot;,&quot;yellow&quot;]'</p>
<p>col2: 'This Is Not Yellow'</p>
</blockquote>
<p>SO using <code>contains(LOWER(&quot;col1&quot;), LOWER(&quot;col2&quot;))</code> works for some examples, however the one above will not work properly, I need to split col2 and look for each value individually in col1 which I am having trouble doing.</p>
<p><strong>Problem 2</strong>
I also have cases that look like this:</p>
<blockquote>
<p>col1: '[&quot;House&quot;,&quot;brick&quot;,&quot;purple&quot;,&quot;blue&quot;]'</p>
<p>col2: 'Very big houses'</p>
</blockquote>
<p>So, again the above examples col2 would need to be split and looked for individually in col1, however <strong>houses</strong> would need to be trimmed by 1 character from the LEFT (to make house), but some other examples might need 2 characters taken off.</p>
<p>For this I was inclined to put together a dictionary to swap out for the appropriate names, or use some sort of NLP stemmers technique to remove the plurals from the word.</p>
<p>Any help on either of those very welcome</p>
<p>Thanks!</p>
",Preprocessing of the text & Tokenization,contains function string splitting trying use contains function matching two column joining table two problem problem data look col red blue green yes purple car yellow col yellow using work example however one work properly need split col look value individually col trouble problem also case look like col house brick purple blue col big house example col would need split looked individually col however house would need trimmed character left make house example might need character taken wa inclined put together dictionary swap appropriate name use sort nlp stemmer technique remove plural word help either welcome thanks
Search DataFrame column for words in list,"<p>I am trying to create a new DataFrame column that contains words that match between a list of keywords and strings in a df column...</p>
<pre><code>data = {
'Sandwich Opinions':['Roast beef is overrated','Toasted bread is always best','Hot sandwiches are better than cold']
}
df = pd.DataFrame(data)

keywords = ['bread', 'bologna', 'toast', 'sandwich']

df['Matches'] = [df.apply(lambda x: ' '.join([i for i in df['Sandwich iOpinions'].str.split() if i in keywords]), axis=1)
</code></pre>
<p>This seems like it should do the job but it's getting stuck in endless processing.</p>
",Preprocessing of the text & Tokenization,search dataframe column word list trying create new dataframe column contains word match list keywords string df column seems like job getting stuck endless processing
Why can&#39;t I import functions in bert after pip install bert,"<p>I am a beginner for bert, and I am trying to use files of bert given on the GitHub:<a href=""https://github.com/google-research/bert"" rel=""noreferrer"">https://github.com/google-research/bert</a></p>

<p>However I cannot import files(such as run_classifier, optimisation and so on) from bert after using <code>pip install bert</code> to install bert in terminal. I tried to run following codes in jupiter notebook:</p>

<pre><code>import bert
from bert import run_classifier
</code></pre>

<p>And the error is:</p>

<pre><code>ImportError: cannot import name 'run_classifier'
</code></pre>

<p>Then I found the file named 'bert' in <code>\anaconda3\lib\python3.6\site-packages</code>, and there were no python files named 'run_classifier', 'optimization' etc inside it. So I downloaded those files from GitHub and put them into file 'bert' by myself. After doing this I could import run_classifier.</p>

<p>However, another problem occurred. I couldn't use the functions inside the files although I could import them.
For example, there's a function <code>convert_to_unicode</code> in tokenization.py:</p>

<pre><code>Help on module bert.tokenization in bert:

NAME

    bert.tokenization - Tokenization classes.    
FUNCTIONS

    convert_to_unicode(text)
    Converts `text` to Unicode (if it's not already), assuming utf-8 input.
</code></pre>

<p>Then I tried this:</p>

<pre><code>import tokenization from bert
convert_to_unicode('input.txt')
</code></pre>

<p>And the error is:</p>

<pre><code>NameError: name 'convert_to_unicode' is not defined
</code></pre>

<p>Then I tried:</p>

<pre><code>from tokenization import convert_to_unicode
</code></pre>

<p>And the error is:</p>

<pre><code>ModuleNotFoundError: No module named 'tokenization'
</code></pre>

<p>I am really confused about this. </p>
",Preprocessing of the text & Tokenization,import function bert pip install bert beginner bert trying use file bert given github however import file run classifier optimisation bert using install bert terminal tried run following code jupiter notebook error found file named bert python file named run classifier optimization etc inside downloaded file github put file bert could import run classifier however another problem occurred use function inside file although could import example function tokenization py tried error tried error really confused
How can i group words to reduce vocabulary in python td idf vectorizer,"<p>I want to reduce the size of the sparse matrix of the tf-idf vectorizer outputs since i am using it with cosine similarity and it takes a long time to go through each vector. I have about 44,000 sentences so the vocabulary size is also very large.</p>
<p>I was wondering if there was a way to combine a group of words to mean one word for example teal, navy and turquiose will all mean blue and that will have same tf-idf value.</p>
<p>I am dealing with a dataset of clothing items so things like colour, and similar clothing articles like shirt, t-shirt and sweatshirts are things i want to group.</p>
<p>I know i can use stop words to give certain words a value of 1 but is it possible to group words to have the same value?</p>
<p>Here is my code</p>
<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

dataset_2 = &quot;/dataset_files/styles_2.csv&quot;
df = pd.read_csv(dataset_2)
df = df.drop(['gender', 'masterCategory', 'subCategory', 'articleType', 'baseColour', 'season', 'year', 'usage'], axis = 1)

tfidf = TfidfVectorizer(stop_words='english') 
tfidf_matrix = tfidf.fit_transform(new_df['ProductDisplayName'])
cos_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

</code></pre>
",Preprocessing of the text & Tokenization,group word reduce vocabulary python td idf vectorizer want reduce size sparse matrix tf idf vectorizer output since using cosine similarity take long time go vector sentence vocabulary size also large wa wondering wa way combine group word mean one word example teal navy turquiose mean blue tf idf value dealing dataset clothing item thing like colour similar clothing article like shirt shirt sweatshirt thing want group know use stop word give certain word value possible group word value code
How to remove spaces in a &quot;single&quot; word? (&quot;bo ok&quot; to &quot;book&quot;),"<p>I am reading a badly formatted text, and often there are unwanted spaces inside a single word. For example, &quot;int ernational trade is not good for economies&quot; and so forth. Is there any efficient tool that can cope with this? (There are a couple of other answers like <a href=""https://stackoverflow.com/questions/42011576/remove-spaces-between-word-in-a-sentence-not-a-single-input-word"">here</a>, which do not work in a sentence.)</p>
<p>Edit: About the impossibility mentioned, I agree. One option is to preserve all possible options. In my case this edited text will be matched with another database that has the original (clean) text. This way, any wrong removal of spaces just gets tossed away.\</p>
",Preprocessing of the text & Tokenization,remove space single word bo ok book reading badly formatted text often unwanted space inside single word example int ernational trade good economy forth efficient tool cope couple answer like href work sentence p edit impossibility mentioned agree one option preserve possible option case edited text matched another database ha original clean text way wrong removal space get tossed away
How to automatically detect words in a string without spaces,"<p>I am trying to automatically detect words from a string in python</p>
<p>Before:</p>
<pre><code>[&quot;Absoluteadvantage&quot;, &quot;Absorptioncosting&quot;, &quot;Accreditedinvestor&quot;]
</code></pre>
<p>After:</p>
<pre><code>[&quot;Absolute Advantage&quot;, &quot;Absorption Costing&quot;, &quot;Accredited Investor&quot;]
</code></pre>
<p>I understand that the accuracy will never be perfect but am looking for a method to seperate these strings of text into seperate words. I've tried using nltk's word_tokenize method to try and seperate these to no avail.</p>
",Preprocessing of the text & Tokenization,automatically detect word string without space trying automatically detect word string python understand accuracy never perfect looking method seperate string text seperate word tried using nltk word tokenize method try seperate avail
Removing stop words from tokenized text using NLTK: TypeError,"<pre><code>import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.tokenize import PunktSentenceTokenizer
from nltk.stem import WordNetLemmatizer
import re
import time

txt = input()

snt_tkn = sent_tokenize(txt)

wrd_tkn = [word_tokenize(s) for s in snt_tkn]

stp_wrd = set(stopwords.words(&quot;english&quot;))

flt_snt = [w for w in wrd_tkn if not w in stp_wrd]

print(flt_snt)
</code></pre>
<p>returns the following:</p>
<pre><code>Traceback (most recent call last):
  File &quot;compiler.py&quot;, line 19, in 
    flt_snt = [w for w in wrd_tkn if not w in stp_wrd]
  File &quot;compiler.py&quot;, line 19, in 
    flt_snt = [w for w in wrd_tkn if not w in stp_wrd]
TypeError: unhashable type: 'list'
</code></pre>
<p>I'd like to know, if possible, how to return the tokenized text with stop words removed without editing <code>wrd_tkn</code>.</p>
",Preprocessing of the text & Tokenization,removing stop word tokenized text using nltk typeerror return following like know possible return tokenized text stop word removed without editing
Converting Ragged Tensor to List,"<p>So I was trying to convert ragged tensor to list. The ragged tensor is returned as a result of tokenization of texts using <a href=""https://www.tensorflow.org/text/guide/subwords_tokenizer"" rel=""nofollow noreferrer"">sub_word Tokenizer</a>.
I am trying to convert this ragged tensor to list using the following code:</p>
<pre><code>token = tokenizer.pt.tokenize(text) # returns ragged tensor
token_as_list = token.to_list()

</code></pre>
<p>But this results in error :</p>
<blockquote>
<p>ValueError: to_list can only be used in eager mode.</p>
</blockquote>
<p>I checked if i was running in eager environment using <code>tf.executing_eagerly()</code> which return <code>True</code></p>
<p>Are  there any other methods to convert ragged tensor to list.
I want to do it because i want to increase and pad the last dimension for eg:</p>
<pre><code>tensor = (64,120) # (batch_size,seq_len)
# convert this to (64,128) by padding it
</code></pre>
",Preprocessing of the text & Tokenization,converting ragged tensor list wa trying convert ragged tensor list ragged tensor returned result tokenization text using sub word tokenizer trying convert ragged tensor list using following code result error valueerror list used eager mode checked wa running eager environment using return method convert ragged tensor list want want increase pad last dimension eg
Python too slow to find text in string in for loop,"<p>I want to <em><strong>improve the loop performance</strong></em> where it counts word occurrences in text, but it runs <em><strong>around 5 minutes for 5 records now</strong></em></p>
<p>DataFrame</p>
<pre><code>No                  Text   
1     I love you forever...*500 other words
2     No , i know that you know xxx *100 words
</code></pre>
<p>My word list</p>
<pre><code>wordlist =['i','love','David','Mary',......]
</code></pre>
<p>My code to count word</p>
<pre><code>for i in wordlist :
    df[i] = df['Text'].str.count(i)
</code></pre>
<p>Result :</p>
<pre><code>No   Text                  I    love  other_words
 1    I love you ...       1      1      4
 2    No, i know ...       1      0      5  
</code></pre>
",Preprocessing of the text & Tokenization,python slow find text string loop want improve loop performance count word occurrence text run around minute record dataframe word list code count word result
How to train a NLP text model where text files are stored in category named folders?,"<p>I've mainly worked on Image classification problems so far and the flow_from_directory of the ImageDataGenerator has always made it simple working with data stored in folder categories.
I'm trying to train a model that uses both image and text but need to first figure out how to read then preprocess text data stored in the same way.
I haven't found much answers on this after searching, any ideas?</p>
<p>I know it creating my own generator for this case could help but I couldn't make one that satisfies my needs.</p>
",Preprocessing of the text & Tokenization,train nlp text model text file stored category named folder mainly worked image classification problem far flow directory imagedatagenerator ha always made simple working data stored folder category trying train model us image text need first figure read preprocess text data stored way found much answer searching idea know creating generator case could help make one satisfies need
"Python: NLTK and Spacy, don&#39;t get same result when tokenize sentence in French","<p>I want to split french text into sentences.</p>
<p>With <a href=""https://www.nltk.org/_modules/nltk/tokenize.html#sent_tokenize"" rel=""nofollow noreferrer"">NLTK</a>, I use the sentence tokenizer directly as follows:</p>
<pre><code>import nltk.data
tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')
tokens = tokenizer.tokenize(&quot;Film culte, classique parmi les classiques.Enfin un conte de Noël bien adapté aux tout-petits sans les prendre pour des attardés.&quot;)
for sentence in tokens:
    print(sentence)
</code></pre>
<p>But I got just one sentence:</p>
<blockquote>
<p>Film culte, classique parmi les classiques.Enfin un conte de Noël bien
adapté aux tout-petits sans les prendre pour des attardés.</p>
</blockquote>
<p>With <a href=""https://spacy.io/models/fr"" rel=""nofollow noreferrer"">Spacy</a>, I do this:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;fr_core_news_sm&quot;)
doc = nlp(&quot;Film culte, classique parmi les classiques.Enfin un conte de Noël bien adapté aux tout-petits sans les prendre pour des attardés.&quot;)
for sentence in doc.sents:
    print(sentence.text)
</code></pre>
<p>I have both sentences right. Which is correct.</p>
<blockquote>
<p>Film culte, classique parmi les classiques.</p>
</blockquote>
<blockquote>
<p>Enfin un conte de Noël bien adapté aux tout-petits sans les prendre pour des attardés.</p>
</blockquote>
<p>Why it's not good with NLTK?</p>
",Preprocessing of the text & Tokenization,python nltk spacy get result tokenize sentence french want split french text sentence nltk use sentence tokenizer directly follows got one sentence film culte classique parmi le classiques enfin un conte de l bien adapt aux tout petits sans le prendre pour de attard spacy sentence right correct film culte classique parmi le classiques enfin un conte de l bien adapt aux tout petits sans le prendre pour de attard good nltk
Spacy incorrectly identifying pronouns,"<p>When I try this code using Spacy, I get the desired result:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_sm&quot;)

# example 1
test = &quot;All my stuff is at to MyBOQ&quot;
doc = nlp(test)
for word in doc:
    if word.pos_ == 'PRON':
        print(word.text)  
</code></pre>
<p>The output shows <code>All</code> and <code>my</code>. However, if I add a question mark:</p>
<pre><code>test = &quot;All my stuff is at to MyBOQ?&quot;
doc = nlp(test)
for word in doc:
    if word.pos_ == 'PRON':
        print(word.text)
</code></pre>
<p>now it also identifies <code>MyBOQ</code> as a pronoun. It should be classified as an organization name (<code>word.pos_ == 'ORG'</code>) instead.</p>
<p>How do I tell Spacy not to classify MyBOQ as a pronoun? Should I just remove all punctuation before checking for pronouns?</p>
",Preprocessing of the text & Tokenization,spacy incorrectly identifying pronoun try code using spacy get desired result output show however add question mark also identifies pronoun classified organization name instead tell spacy classify myboq pronoun remove punctuation checking pronoun
Ordinal numbers replacement,"<p>I am currently looking for the way to replace words like first, second, third,...with appropriate ordinal number representation (1st, 2nd, 3rd).
I have been googling for the last week and I didn't find any useful standard tool or any function from NLTK.</p>

<p>So is there any or should I write some regular expressions manually?</p>

<p>Thanks for any advice</p>
",Preprocessing of the text & Tokenization,ordinal number replacement currently looking way replace word like first second third appropriate ordinal number representation st nd rd googling last week find useful standard tool function nltk write regular expression manually thanks advice
"I get results only for the last class, not all of them","<p>I am doing a NLP work (just starting to learn) with policy descriptions (corpus) and policy areas (classes). I have calculated the word frequency, and made a list of words with the 100 most frequent terms. I have 27 classes. Some words are relevant for some classes but irrelevant for others, so I cant clean them from all the data set. I want to remove the words in the list from the classes where they are present in LESS than 50% od the rows, but when i print dfen_all it gives me only the results for the last class.
It seems that somehow it is not mergin the datasets for each class... Iàve tried a few modifications but nothing worked.</p>
<pre><code>for term in most_common_words_list:
    for class_num in arr2:
        df_class = dfen[dfen['Policycat1'] == class_num] # get dataframe of class
        num_with_term = len(df_class[df_class['corpus3'].str.contains(term, case=False)])# count number of records with term
        num_total= len(df_class['corpus3'])
        percent = num_with_term/num_total
        if percent &lt; 0.5:
            df_class['corpus3_clean'] = df_class['corpus3'].str.replace(term, '') # delete term from records

#join all dataframes into a single dataframe
dfen_all = pd.DataFrame()
for class_num in class_values:
    dfen_all = pd.concat([dfen_all, df_class[df_class['Policycat1'] == class_num]])


#save the dataframe to a csv
#df_all.to_csv('all_classes.csv', index=False)
dfen_all['totalwords_c3cl'] = dfen_all['corpus3_clean'].str.split().str.len()
dfen_all
</code></pre>
<p>I am expecting to get all the results together in a daframe, not only the last label.</p>
<p>This is my first question, I apologize if I missed something. Thanks a lot!</p>
",Preprocessing of the text & Tokenization,get result last class nlp work starting learn policy description corpus policy area class calculated word frequency made list word frequent term class word relevant class irrelevant others cant clean data set want remove word list class present le od row print dfen give result last class seems somehow mergin datasets class tried modification nothing worked expecting get result together daframe last label first question apologize missed something thanks lot
Text Extraction from real world messy files,"<pre><code>WET READ: ___ ___ ___ 7:31 PM
 Persistent right lower lung opacity and right pleural effusion. Effusion
   perhaps slightly decreased since radiograph dated ___.
 ______________________________________________________________________________
                                 FINAL REPORT

PA AND LATERAL CHEST RADIOGRAPH.

TECHNIQUE:  AP upright portable radiograph of chest was reviewed in comparison
to prior radiograph from ___.

 
 As compared to ___ there is interval improvement of pulmonary edema. 
 Right lower lobe consolidation with internal cavitation surrounded by pleural
 effusion appears to be grossly unchanged in the short interim.  There is no
 evidence of progression of left consolidation.  Small amount of left pleural
 effusion is noted.  

</code></pre>
<p>I have text files like this, I want to extract data from the files after &quot;FINAL REPORT&quot;. Means I want to delete everything from my text files up to and including &quot;FINAL REPORT&quot;.</p>
<p>I have tried regular expression, But couldn't find method to do that.</p>
",Preprocessing of the text & Tokenization,text extraction real world messy file text file like want extract data file final report mean want delete everything text file including final report tried regular expression find method
Is there a way to tokenize my text data in a df column into phrases instead of words?,"<p>Good day! I'm kinda new to Text Analytics and trying out something simple by myself. I have a df as following:</p>
<p><code>data =  ['bank a earned 3 million usd through investing in certain funds and earned 500k eur from other investments in 2020', 'bank b earned 2 million usd from borrowing and 500k gbp from investments in 2020', 'bank c earned 600k chf and 300k aud from investing and borrowing respectively in 2020‘]</code></p>
<p><code>df = pd.DataFrame(data, columns=['text'])</code>
`</p>
<p>My goal is to try and find out what the different banks did to earn their money through text analytics. I have done some pre-processing such as removing punctuations and turning the data to all lowercase. I'm aware that I should also stem the words for better accuracy and remove stop words. However, one question popped up in my mind about tokenization. If I try to tokenize them, wouldn't the text lose its meaning from critically important phrases such as '2 million usd' or '300k aud'? If so, how do I tokenize it into phrases that retain the meaning of my text data?</p>
<p>Thank you</p>
",Preprocessing of the text & Tokenization,way tokenize text data df column phrase instead word good day kinda new text analytics trying something simple df following goal try find different bank earn money text analytics done pre processing removing punctuation turning data lowercase aware also stem word better accuracy remove stop word however one question popped mind tokenization try tokenize text lose meaning critically important phrase million usd k aud tokenize phrase retain meaning text data thank
Add/remove custom stop words with spacy,"<p>What is the best way to add/remove stop words with spacy? I am using <a href=""https://spacy.io/docs/api/token"" rel=""noreferrer""><code>token.is_stop</code></a> function and would like to make some custom changes to the set. I was looking at the documentation but could not find anything regarding of stop words. Thanks!</p>
",Preprocessing of the text & Tokenization,add remove custom stop word spacy best way add remove stop word spacy using function would like make custom change set wa looking documentation could find anything regarding stop word thanks
I need to make a Pre Trained Tokenizer (Hugging Face) safer for privacy,"<p>I am new to NLP and Transformers library. Perhaps my doubt is naive but I am not finding a good solution for it.</p>
<p>I have documents whose content in sensitive and it is a requirements of mine not to publish it clearly on cloud. However my model is running on a Cloud Virtual Machine.</p>
<p>My idea would be to perform OCR and Tokenization on premise and then uploading the results.</p>
<p>However, tokenization with PreTrainedTokenizer by Transformers library returns the ids of the token from its vocabulary, and everyone can decode it having the same pretrained model.</p>
<p>So here is the question: it is possible to fine-tune or just change the vocabulary index so that the tokenization can't be easily decoded?</p>
",Preprocessing of the text & Tokenization,need make pre trained tokenizer hugging face safer privacy new nlp transformer library perhaps doubt naive finding good solution document whose content sensitive requirement mine publish clearly cloud however model running cloud virtual machine idea would perform ocr tokenization premise uploading result however tokenization pretrainedtokenizer transformer library return id token vocabulary everyone decode pretrained model question possible fine tune change vocabulary index tokenization easily decoded
Ensemble LDA: How to find more than 1 stable topic?,"<p>I used LDA (Latent Dirichlet Allocation) algorithm to analyse corpus from <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwi10uCnvJL8AhV-RmwGHec6DmQQFnoECBEQAQ&amp;url=https%3A%2F%2Fconsole.cloud.google.com%2Fmarketplace%2Fproduct%2Fstack-exchange%2Fstack-overflow&amp;usg=AOvVaw3b3AF70UCH8KOYJAH6adZ9"" rel=""nofollow noreferrer"">StackExchange database</a>. LDA is not ideal because it has problem with reproducibility because it gives different topics suggestion with every execution, which make it unreliable from my point of view. After reading an <a href=""https://radimrehurek.com/gensim/models/ensemblelda.html"" rel=""nofollow noreferrer"">introduction for Ensemble LDA (eLDA) from Gensim</a>, eLDA addresses this problem by executing LDA models multiple times and only outputs stable topics, which are topics that are occur multiple times from multiple LDAs (to explain it in a very simplified way).</p>
<p>How I executed eLDAs are:</p>
<ol start=""0"">
<li><p>Filter (so that only questions with topic &quot;security&quot; are left) and preprocess the data from StackExchange. I use last 10 years data of Q&amp;A from StackOverflow.</p>
</li>
<li><p>Do trial and error of normal LDA to get ideal hyper parameter (passes and iteration) so that most if not all documents have converged. I refer this step to <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html"" rel=""nofollow noreferrer"">this documentation</a>.</p>
</li>
<li><p>With the optimal LDA parameters, I ran eLDA:</p>
<pre><code> corpus = self.doc_matrix
 dictionary = vectorizer.id2word.id2token
 topic_model_class = LdaModel
 ensemble_workers = 4
 num_models = 12
 distance_workers = 4
 num_topics = 50
 passes=20
 iterations=5000
 epsilon = 1
 eval_every=10

 self.estimator = EnsembleLda(
     corpus=corpus,
     id2word=dictionary,
     num_topics=num_topics,
     passes=passes,
     iterations=iterations,
     num_models=num_models,
     topic_model_class=topic_model_class,
     ensemble_workers=ensemble_workers,
     distance_workers=distance_workers,
     epsilon=epsilon,
     eval_every=eval_every
 )
</code></pre>
</li>
<li><p>The result is always only one stable topic. This is just too few stable topic for my purpose. After couple days, I doesn't manage to increase stable topic findings/ improve my eLDA.
<a href=""https://i.sstatic.net/rpvqb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rpvqb.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
<p>My goal: find trend or topics in StackOverflow questions that are related to topic &quot;security&quot;.</p>
<p>My opinion: the finding should be more than just one stable topic, because in StackOverflow there are tags available for every questions. Tag by itself is a topic. Therefore it should find more than one stable topic.</p>
<p>Questions:</p>
<p>a. Can somebody point me a direction how to optimise my eLDA model so that it can find more than one stable topic?</p>
<p>b. Is my logic wrong to assume that there should be more than one stable topic? I could accept it if the data are very diverse. But since every StackOverflow question consists tags, I assume the finding should have &gt; 1 stable topic.</p>
<p>Thank you for any advice.</p>
<p>#################################################################</p>
<p>Why did I choose epsilon=1?
I execute the following:</p>
<pre><code>def optimize_ensembleLda(self):
    print('*** Optimize ensemble LDA model.')
    import numpy as np
    shape = self.estimator.asymmetric_distance_matrix.shape
    without_diagonal = self.estimator.asymmetric_distance_matrix[~np.eye(shape[0], dtype=bool)].reshape(shape[0], -1)
    print(&quot;Min, mean &amp; max value of asymetric distance matrix:&quot;)
    print(without_diagonal.min(), without_diagonal.mean(), without_diagonal.max())
    new_epsilon = without_diagonal.max()
    self.estimator.recluster(eps=new_epsilon, min_samples=2, min_cores=2)
    return 
</code></pre>
<p>I chose to use the maximum distance, because I am trying to get more stable topics.</p>
<p>#################################################################</p>
<p>After executing Erwan's suggestion from commentary (increasing num_models to e.g. 40), I got the following error:</p>
<pre><code>RecursionError: maximum recursion depth exceeded in comparison
</code></pre>
<p>The error points to the eLDA model from under step 2 above.
I doubled the ensemble_worker but the problem is not fixed. I will keep increasing it and give update here.</p>
<p>#################################################################</p>
<p>I solved the <code>RecursionError</code> by increasing the Python recursion limit with:</p>
<pre><code>sys.setrecursionlimit(int(len(self.ttda)*1.2))
</code></pre>
<p>inside my Gensim eLDA code...
The reason for this problem is my large corpus size and hence the large topic numbers.
Still my original problem still exists, after increasing <code>num_models</code> to 40, I still get one stable topic.</p>
<p><em>I would appreciate very much, if someone can help me finding more than one stable topic with Ensemble LDA from Gensim.</em></p>
",Preprocessing of the text & Tokenization,ensemble lda find stable topic used lda latent dirichlet allocation algorithm analyse corpus stackexchange database lda ideal ha problem reproducibility give different topic suggestion every execution make unreliable point view reading introduction ensemble lda elda gensim elda address problem executing lda model multiple time output stable topic topic occur multiple time multiple ldas explain simplified way executed eldas filter question topic security left preprocess data stackexchange use last year data q stackoverflow trial error normal lda get ideal hyper parameter pass iteration document converged refer step documentation optimal lda parameter ran elda result always one stable topic stable topic purpose couple day manage increase stable topic finding improve elda goal find trend topic stackoverflow question related topic security opinion finding one stable topic stackoverflow tag available every question tag topic therefore find one stable topic question somebody point direction optimise elda model find one stable topic b logic wrong assume one stable topic could accept data diverse since every stackoverflow question consists tag assume finding stable topic thank advice choose epsilon execute following chose use maximum distance trying get stable topic executing erwan suggestion commentary increasing num model e g got following error error point elda model step doubled ensemble worker problem fixed keep increasing give update solved increasing python recursion limit inside gensim elda code reason problem large corpus size hence large topic number still original problem still exists increasing still get one stable topic would appreciate much someone help finding one stable topic ensemble lda gensim
Unable to update &#39; and &quot;&quot; in the stop_word list,"<blockquote>
<p>I tried to update ' and &quot; in my stop_word list.</p>
</blockquote>
<pre><code>&gt; stop_words.update([&quot;'&quot;,&quot;&quot;&quot;])
&gt; stop_words
</code></pre>
<p>I got the following error.</p>
<pre><code>
    &gt; File &quot;&lt;ipython-input-85-54a2b8b08201&gt;&quot;, line 2
    &gt; stop_words
    &gt; 
    &gt; 
    &gt; SyntaxError: EOF while scanning triple-quoted string literal--

</code></pre>
<p>How to update those characters in the stop_word ?</p>
",Preprocessing of the text & Tokenization,unable update stop word list tried update stop word list got following error update character stop word
Tokenize Review wise for sentiment analysis,"<p>In this Amazon dataset I've Product_Description , Product_Type &amp; Sentiment column where I want to build classification model. keeping Product_Description &amp; Product_Type as X and Sentiment as Y. but i receive few error still not able to find the solution. I want the sentence itself to be tokenize for tfidf not different words.</p>
<pre><code>&gt; amazon.head()
</code></pre>
<p><a href=""https://i.sstatic.net/Czg9l.png"" rel=""nofollow noreferrer"">Link to data example</a></p>
<pre><code>&gt; `Z = amazon[&quot;Product_Description&quot;]
&gt; Y = amazon[&quot;Sentiment&quot;]
&gt; tfidf = TfidfVectorizer()
&gt; tf = pd.DataFrame(tfidf.fit_transform(Z),columns = [&quot;Product_Description&quot;])
&gt; X = pd.concat((tf,amazon[&quot;Product_Type&quot;]),axis = 1)
&gt; X.drop(X[X[&quot;Product_Description&quot;].isnull()].index, inplace = True)
&gt; test_size = 0.2
&gt; seed = 45
&gt; X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = test_size, random_state = seed)
&gt; X_train.shape,X_test.shape # [((5081, 2), (1271, 2))] is the output
&gt; model = LogisticRegression(max_iter = 500)
&gt; 
&gt; rfe = RFE(model, n_features_to_select = 2)
&gt; fit = rfe.fit(X, Y)
&gt; 
&gt; fit.n_features_
&gt; fit.support_
&gt; fit.ranking_`
</code></pre>
<pre><code>TypeError                                 Traceback (most recent call last)
TypeError: float() argument must be a string or a number, not 'csr_matrix'

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
Input In [57], in &lt;cell line: 7&gt;()
      4 model = LogisticRegression(max_iter = 500)
      6 rfe = RFE(model, n_features_to_select = 2)
----&gt; 7 fit = rfe.fit(X, Y)
      9 fit.n_features_
     10 fit.support_
```ValueError: setting an array element with a sequence.
</code></pre>
<p>​</p>
",Preprocessing of the text & Tokenization,tokenize review wise sentiment analysis amazon dataset product description product type sentiment column want build classification model keeping product description product type x sentiment receive error still able find solution want sentence tokenize tfidf different word link data example
difference between Tokenization and Segmentation,"<p>What is the difference between Tokenization and Segmentation in NLP. I searched about them but I didn't really find any differences
.</p>
",Preprocessing of the text & Tokenization,difference tokenization segmentation difference tokenization segmentation nlp searched really find difference
If dataframe column has specific words alter value,"<p>I have a dataframe, example:</p>
<pre><code>df = [{'id': 1, 'text': 'text contains ok words'}, , {'id':2, 'text':'text contains word apple'}, {'id':3, 'text':'text contains words ok'}]
</code></pre>
<p>Example:</p>
<pre><code>keywords = ['apple', 'orange', 'lime']
</code></pre>
<p>And I want to check all columns 'text' to check if contains any word from my keywords, if so I want to alter that text column to: 'disconsider this case'</p>
<p>I've tried to tokenize the column but then I'm not able to use the function I created to check, here is the example:</p>
<pre><code>df = pd.DataFrame(df)

def remove_keywords(inpt):
    keywords = ['apple', 'orange', 'lime']    
    if any(x in word for x in keyword):
        return 'disconsider this case'
    else:
        return inpt

        
df['text'] = df['text'].apply(remove_keywords)
df
</code></pre>
<pre><code>df['text'] = df.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)
for word in df['text']:
    if 'apple' in df['text']:
        return 'disconsider this case'
</code></pre>
<p>Any help appreciated. Thanks!!</p>
",Preprocessing of the text & Tokenization,dataframe column ha specific word alter value dataframe example example want check column text check contains word keywords want alter text column disconsider case tried tokenize column able use function created check example help appreciated thanks
Output Bash pipes to Python-compatible format,"<p>I'm working on text tokenization and lemmatization using UDPipe models. I can complete the task itself by using <code>!echo</code> commands or printing into a file, but I would like to generate a Python data structure to further process the output.</p>
<h1>What works</h1>
<p>Here is my working command:</p>
<pre><code>!echo 'the text I'm processing' | ./udpipe --tokenize --tag './path/to/my/model'
</code></pre>
<p>Out:</p>
<pre><code>Loading UDPipe model: done.
newdoc
newpar
sent_id = 1
text = прывітанне, сусвет
1   прывітанне  прывітанне  NOUN    NN  Animacy=Inan|Case=Nom|Gender=Neut|Number=Sing   _   _   _   SpaceAfter=No
2   ,   ,   PUNCT   PUNCT   _   _   _   _   _
3   сусвет  сусвет  NOUN    NN  Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing   _   _   _   SpacesAfter=\n
</code></pre>
<p>This works for printing the output into a file:</p>
<pre><code>!echo 'the text I'm processing' | ./udpipe --tokenize --tag './path/to/my/model' &gt;&gt; filename.txt
</code></pre>
<p><code>./udpipe</code> is the cloned repository of the package</p>
<h1>What I tried (without success)</h1>
<p><code>os.system()</code></p>
<pre><code>import os
text = 'the text I'm processing'
cmd = &quot;echo '{}' | ./udpipe --tokenize --tag './path/to/my/model'&quot;.format(text)
os.system(cmd)

Out: 0
</code></pre>
<p><code>subprocess.getoutput()</code></p>
<pre><code>import subprocess
cmd = &quot;'the text I'm processing' | ./udpipe --tokenize --tag './path/to/my/model'&quot;
output = subprocess.getoutput(cmd, stdout=subprocess.PIPE, shell=True)
print(output)

TypeError: getoutput() got an unexpected keyword argument 'stdout'
</code></pre>
",Preprocessing of the text & Tokenization,output bash pipe python compatible format working text tokenization lemmatization using udpipe model complete task using command printing file would like generate python data structure process output work working command work printing output file cloned repository package tried without success
Multinomial Naive Bayes can&#39;t use validation dataset because of ValueError but can use sklearn train_test_split,"<p>I am trying to make a MNB classifier for sentiment analysis. I had a dataset that consists of text and label in the following structure where labels are from 1-5. Using huggingface emotions dataset.</p>
<pre><code>feature                                   label
&quot;I feel good&quot;                             1
</code></pre>
<p>I was able to do it using only my train dataset and using train_test_split function of sklearn. But there is a problem when I try to do it with my dataset which gives</p>
<pre><code>ValueError: X has 3427 features, but MultinomialNB is expecting 10052 features as input.
</code></pre>
<p>on last line of the following code (predict)</p>
<pre><code>cv = CountVectorizer(stop_words='english')
val_ppd_df = cv.fit_transform(val_df[&quot;lemmatized&quot;])
val_labels = np.array(val_df['label'])
train_labels = np.array(train_df['label'])
mnb = MultinomialNB()
mnb.fit(train_ppd_df,train_labels)
predictions_NB = mnb.predict(val_ppd_df)
</code></pre>
<p>What I do is I do every operation (tokenization, stemming, lemmatization) to my validation dataset, but instead of doing test_train split I just split the labels of train and validation datasets. I checked what would come out of train_test_split and what val_ppd_df has and I noticed that they are different.</p>
<pre><code>&lt;16000x10052 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
    with 128627 stored elements in Compressed Sparse Row format&gt;
&lt;2000x3427 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
    with 15853 stored elements in Compressed Sparse Row format&gt;
</code></pre>
<p>How can I handle this difference? Every example on internet uses train_test_split and mine works okay on it but I want to do this first on validation then on a different test dataset, not only on train dataset.</p>
",Preprocessing of the text & Tokenization,multinomial naive bayes use validation dataset valueerror use sklearn train test split trying make mnb classifier sentiment analysis dataset consists text label following structure label using huggingface emotion dataset wa able using train dataset using train test split function sklearn problem try dataset give last line following code predict every operation tokenization stemming lemmatization validation dataset instead test train split split label train validation datasets checked would come train test split val ppd df ha noticed different handle difference every example internet us train test split mine work okay want first validation different test dataset train dataset
Python: Can I create a dummy based on search conditions in one column with text series?,"<p>I was wondering how I could create a dummy variable for the following condition: column 'lemmatised' contains at least two words from 'innovation_words'. Innovation_words is a list I defined myself:</p>
<pre><code>innovation_words = ['community', 'local', 'charity', 'event', 'partner',
                'volunteering', 'plastic', 'surplusfood']
</code></pre>
<p>The lemmatised column looks like this (I'm fine changing the type or formatting if needed):</p>
<p><a href=""https://i.sstatic.net/tWdud.png"" rel=""nofollow noreferrer"">data to use for condition</a></p>
<p>So, if any observation includes for example <em>local</em> and <em>plastic</em>, I would like to have a dummy variable: 'innovation' = 1. Hope someone can help me with this. Some code I already tried:</p>
<pre><code>conditions = [df_posts['lemmatised'].isin(innovation_words), 
          df_posts['lemmatised'].isin(innovation_words)]

dummy = [1,0]

df_posts['innovation'] = np.select(conditions, dummy)
</code></pre>
",Preprocessing of the text & Tokenization,python create dummy based search condition one column text series wa wondering could create dummy variable following condition column lemmatised contains least two word innovation word innovation word list defined lemmatised column look like fine changing type formatting needed data use condition observation includes example local plastic would like dummy variable innovation hope someone help code already tried
Why is the number of stem from NLTK Stemmer outputs different from expected output?,"<p>I have to perform Stemming on a text. The questions are as follows :</p>
<ol>
<li>Tokenize all the words given in <code>tc</code>. The word should contain alphabets or numbers or underscore. Store the tokenized list of words in <code>tw</code></li>
<li>Convert all the words into lowercase. Store the result into the variable <code>tw</code></li>
<li>Remove all the stop words from the unique set of <code>tw</code>. Store the result into the variable <code>fw</code></li>
<li>Stem each word present in <code>fw</code> with PorterStemmer, and store the result in the list <code>psw</code></li>
</ol>
<p>Below is my code :</p>
<pre><code>import re
import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.stem  import PorterStemmer,LancasterStemmer

pattern = r'\w+';
tw= nltk.regexp_tokenize(tc,pattern);
tw= [word.lower() for word in tw];
stop_word = set(stopwords.words('english'));
fw= [w for w in tw if not w in stop_word];
#print(sorted(filteredwords));
porter = PorterStemmer();
psw = [porter.stem(word) for word in fw];
print(sorted(psw));
</code></pre>
<p>My code works perfectly with all the provided testcases in hand-on but it fails only for the below test case where</p>
<blockquote>
<p>tc = &quot;I inadvertently went to See's Candy last week (I was in the mall looking for phone repair), and as it turns out, See's Candy now charges a dollar -- a full dollar -- for even the simplest of their wee confection offerings. I bought two chocolate lollipops and two chocolate-caramel-almond things. The total cost was four-something. I mean, the candies were tasty and all, but let's be real: A Snickers bar is fifty cents. After this dollar-per-candy revelation, I may not find myself wandering dreamily back into a See's Candy any time soon.&quot;</p>
</blockquote>
<p>My Output is :</p>
<blockquote>
<p>['almond', 'back', 'bar', 'bought', <strong>'candi', 'candi'</strong>, 'caramel', 'cent', 'charg', 'chocol', 'confect', 'cost', 'dollar', 'dreamili', 'even', 'fifti', 'find', 'four', 'full', 'inadvert', 'last', 'let', 'lollipop', 'look', 'mall', 'may', 'mean', 'offer', 'per', 'phone', 'real', 'repair', 'revel', 'see', 'simplest', 'snicker', 'someth', 'soon', 'tasti', 'thing', 'time', 'total', 'turn', 'two', 'wander', 'wee', 'week', 'went']</p>
</blockquote>
<p>Expected Output is :</p>
<blockquote>
<p>['almond', 'back', 'bar', 'bought', <strong>'candi', 'candi', 'candi'</strong>, 'caramel', 'cent', 'charg', 'chocol', 'confect', 'cost', 'dollar', 'dreamili', 'even', 'fifti', 'find', 'four', 'full', 'inadvert', 'last', 'let', 'lollipop', 'look', 'mall', 'may', 'mean', 'offer', 'per', 'phone', 'real', 'repair', 'revel', 'see', 'simplest', 'snicker', 'someth', 'soon', 'tasti', 'thing', 'time', 'total', 'turn', 'two', 'wander', 'wee', 'week', 'went']</p>
</blockquote>
<blockquote>
<p>The difference is the occurrence of 'Candi'</p>
</blockquote>
<p>Looking help to troubleshoot the issue.</p>
",Preprocessing of the text & Tokenization,number stem nltk stemmer output different expected output perform stemming text question follows tokenize word given word contain alphabet number underscore store tokenized list word convert word lowercase store result variable remove stop word unique set store result variable stem word present porterstemmer store result list code code work perfectly provided testcases hand fails test case tc inadvertently went see candy last week wa mall looking phone repair turn see candy charge dollar full dollar even simplest wee confection offering bought two chocolate lollipop two chocolate caramel almond thing total cost wa four something mean candy tasty let real snicker bar fifty cent dollar per candy revelation may find wandering dreamily back see candy time soon output almond back bar bought candi candi caramel cent charg chocol confect cost dollar dreamili even fifti find four full inadvert last let lollipop look mall may mean offer per phone real repair revel see simplest snicker someth soon tasti thing time total turn two wander wee week went expected output almond back bar bought candi candi candi caramel cent charg chocol confect cost dollar dreamili even fifti find four full inadvert last let lollipop look mall may mean offer per phone real repair revel see simplest snicker someth soon tasti thing time total turn two wander wee week went difference occurrence candi looking help troubleshoot issue
How to set custom stop words for sklearn CountVectorizer?,"<p>I'm trying to run LDA (Latent Dirichlet Allocation) on a non-English text dataset.</p>

<p>From sklearn's tutorial, there's this part where you count term frequency of the words to feed into the LDA:</p>

<pre><code>tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,
                            max_features=n_features,
                            stop_words='english')
</code></pre>

<p>Which has built-in stop words feature which is only available for English I think. How could I use my own stop words list for this?</p>
",Preprocessing of the text & Tokenization,set custom stop word sklearn countvectorizer trying run lda latent dirichlet allocation non english text dataset sklearn tutorial part count term frequency word feed lda ha built stop word feature available english think could use stop word list
Split torch tensor : max size and end of the sentence,"<p>I would like to split a tensor into several tensors with torch on Python.
The tensor is the tokenization of a long text.</p>
<p>First here is what I had done:</p>
<pre><code>tensor = tensor([[ 3746,  3120,  1024,  ...,  2655, 24051,  2015]]) #size 14714

result = tensor.split(510)
</code></pre>
<p>It works but now I would like to refine this, and make it so that it can't split in the middle of a sentence but at the <strong>end of a sentence</strong>, so recognizing the dot '.' (token 1012). Of course all the tensor will not be the same size but will have to respect a <strong>maximum size</strong> (510 for example).</p>
<p>Thanks for your help</p>
",Preprocessing of the text & Tokenization,split torch tensor max size end sentence would like split tensor several tensor torch python tensor tokenization long text first done work would like refine make split middle sentence end sentence recognizing dot token course tensor size respect maximum size example thanks help
How to Tokenize block of text as one token in python?,"<p>Recently I am working on a genome data set which consists of many blocks of genomes. On previous works on natural language processing, I have used <code>sent_tokenize</code> and <code>word_tokenize</code> from nltk to tokenize the sentences and words. But when I use these functions on genome data set, it is not able to tokenize the genomes correctly. The text below shows some part of the genome data set.</p>
<pre><code>&gt;NR_004049 1
tattattatacacaatcccggggcgttctatatagttatgtataatgtat
atttatattatttatgcctctaactggaacgtaccttgagcatatatgct
gtgacccgaaagatggtgaactatacttgatcaggttgaagtcaggggaa
accctgatggaagaccgaaacagttctgacgtgcaaatcgattgtcagaa
ttgagtataggggcgaaagaccaatcgaaccatctagtagctggttcctt
ccgaagtttccctcaggatagctggtgcattttaatattatataaaataa
tcttatctggtaaagcgaatgattagaggccttagggtcgaaacgatctt
aacctattctcaaactttaaatgggtaagaaccttaactttcttgatatg
aagttcaaggttatgatataatgtgcccagtgggccacttttggtaagca
gaactggcgctgtgggatgaaccaaacgtaatgttacggtgcccaaataa
caact
&gt;NR_004048 1
aatgttttatataaattgcagtatgtgtcacccaaaatagcaaaccccat
aaccaaccagattattatgatacataatgcttatatgaaactaagacatt
tcgcaacatttattttaggtatataaatacatttattgaaggaattgata
tatgccagtaaaatggtgtatttttaatttctttcaataaaaacataatt
gacattatataaaaatgaattataaaactctaagcggtggatcactcggc
tcatgggtcgatgaagaacgcagcaaactgtgcgtcatcgtgtgaactgc
aggacacatgaacatcgacattttgaacgcatatcgcagtccatgctgtt
atgtactttaattaattttatagtgctgcttggactacatatggttgagg
gttgtaagactatgctaattaagttgcttataaatttttataagcatatg
gtatattattggataaatataataatttttattcataatattaaaaaata
aatgaaaaacattatctcacatttgaatgt
&gt;NR_004047 1
atattcaggttcatcgggcttaacctctaagcagtttcacgtactgttta
actctctattcagagttcttttcaactttccctcacggtacttgtttact
atcggtctcatggttatatttagtgtttagatggagtttaccacccactt
agtgctgcactatcaagcaacactgactctttggaaacatcatctagtaa
tcattaacgttatacgggcctggcaccctctatgggtaaatggcctcatt
taagaaggacttaaatcgctaatttctcatactagaatattgacgctcca
tacactgcatctcacatttgccatatagacaaagtgacttagtgctgaac
tgtcttctttacggtcgccgctactaagaaaatccttggtagttactttt
cctcccctaattaatatgcttaaattcagggggtagtcccatatgagttg
&gt;NR_004052 1
</code></pre>
<p>When the tokenizer of ntlk is applied on this dataset, each line of text (for example <code>tattattatacacaatcccggggcgttctatatagttatgtataatgtat</code> ) becomes one token which is not correct. and a block of sequences should be considered as one token. For example in this case contents between <code>&gt;NR_004049 1</code>  and <code>&gt;NR_004048 1</code> should be consider as one token:</p>
<pre><code>&gt;NR_004049 1
tattattatacacaatcccggggcgttctatatagttatgtataatgtat
atttatattatttatgcctctaactggaacgtaccttgagcatatatgct
gtgacccgaaagatggtgaactatacttgatcaggttgaagtcaggggaa
accctgatggaagaccgaaacagttctgacgtgcaaatcgattgtcagaa
ttgagtataggggcgaaagaccaatcgaaccatctagtagctggttcctt
ccgaagtttccctcaggatagctggtgcattttaatattatataaaataa
tcttatctggtaaagcgaatgattagaggccttagggtcgaaacgatctt
aacctattctcaaactttaaatgggtaagaaccttaactttcttgatatg
aagttcaaggttatgatataatgtgcccagtgggccacttttggtaagca
gaactggcgctgtgggatgaaccaaacgtaatgttacggtgcccaaataa
caact
&gt;NR_004048 1 
</code></pre>
<p>So each block starting with special words such as <code>&gt;NR_004049 1</code> until the next special character should be considered as one token. The problem here is tokenizing this kind of data set and i dont have any idea how can i correctly tokenize them.
I really appreciate answers which helps me to solve this.</p>
<p><strong>Update:</strong>
One way to solve this problem is to append al lines within each block, and then using the nltk tokenizer. for example This means that to append all lines between <code>&gt;NR_004049 1</code> and <code>&gt;NR_004048 1</code> to make one string from several lines, so the nltk tokenizer will consider it as one token. Can any one help me how can i append lines within each block?</p>
",Preprocessing of the text & Tokenization,tokenize block text one token python recently working genome data set consists many block genome previous work natural language processing used nltk tokenize sentence word use function genome data set able tokenize genome correctly text show part genome data set tokenizer ntlk applied dataset line text example becomes one token correct block sequence considered one token example case content consider one token block starting special word next special character considered one token problem tokenizing kind data set dont idea correctly tokenize really appreciate answer help solve update one way solve problem append al line within block using nltk tokenizer example mean append line make one string several line nltk tokenizer consider one token one help append line within block
How can I exract a full sentence using Apache NLPCraft?,"<p>In my model file I am using a macro with a regex extract any space-separated alpha-numeric words to capture an user-input sentence i.e.</p>
<pre><code>macros:
  - name: &quot;&lt;GENERIC_INPUT&gt;&quot;
    macro: &quot;{//[a-zA-Z0-9 ]+//}&quot;
</code></pre>
<p>Then I am trying to capture it as following in the element:</p>
<pre><code>elements:
  - id: &quot;prop:title&quot;
    description: Set title
    synonyms:
      - &quot;{set|add} title &lt;GENERIC_INPUT&gt;&quot;
</code></pre>
<p>The intent term is as following:</p>
<pre><code>intents:
 - &quot;intent=myIntent term(createStory)~{tok_id() == 'prop:createStory'} term(title)~{tok_id() == 'prop:title'}?&quot;
</code></pre>
<p>In the Java Model I am correctly capturing the <code>title</code> property:</p>
<pre><code>public NCResult onMatch(
            NCIntentMatch ctx,
            @NCIntentTerm(&quot;createStory&quot;) NCToken createStory,
            @NCIntentTerm(&quot;title&quot;) Optional&lt;NCToken&gt; titleList,
{
...
</code></pre>
<p>When I run a query against the REST API service the probe is deployed in, I only get the first word of the last element &lt;GENERIC_INPUT&gt; (the regular expression) of the synonym defined as <code>{set|add} title &lt;GENERIC_INPUT&gt;</code> i.e.</p>
<pre><code>HTTP 200 [235ms]
{
  &quot;status&quot;: &quot;API_OK&quot;,
  &quot;state&quot;: {
    &quot;resType&quot;: &quot;json&quot;,
    &quot;mdlId&quot;: &quot;Create Story&quot;,
    &quot;txt&quot;: &quot;set title this is my story&quot;,
    &quot;resMeta&quot;: {},
    &quot;srvReqId&quot;: &quot;GKDY-QLBM-B6TQ-7KYO-KMR8&quot;,
    &quot;status&quot;: &quot;QRY_READY&quot;,
    &quot;resBody&quot;: {
      &quot;title&quot;: &quot;set title this&quot;,
      &quot;createStory&quot;: true,
    },
    &quot;usrId&quot;: 1,
    &quot;intentId&quot;: &quot;myIntent&quot;
  }
}

</code></pre>
<p>In the <code>resBody.title</code> I get <code>set title this</code> rather than the whole string as it should be allowed by the regex i.e. <code>set title this is my story</code></p>
<p>Any idea why? How can I get it to extract the whole title?</p>
<p>Many thanks</p>
",Preprocessing of the text & Tokenization,exract full sentence using apache nlpcraft model file using macro regex extract space separated alpha numeric word capture user input sentence e trying capture following element intent term following java model correctly capturing property run query rest api service probe deployed get first word last element generic input regular expression synonym defined e get rather whole string allowed regex e idea get extract whole title many thanks
Using Counter on a list of Spacy tokens returns a non unique dict of the tokens,"<p>I want to count a list of spacy tokens with the counter class. I.e.:</p>
<pre><code>[hello,how,are,you,hello]
</code></pre>
<p>where each element is of type <code>&lt;class 'spacy.tokens.token.Token'&gt;</code>.
However when i want to count the  occurences of each Token within the list via counter, as seen below:</p>
<pre><code>    return Counter(joined)
</code></pre>
<p>The result is a <strong>non unique dict of the tokens</strong> or in other words: the same list as before but its a dict now and each key has the value of 1. In the screenshot below it can be seen, that the dict seemingly has the same key twice in it.</p>
<p><a href=""https://i.sstatic.net/vO022.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vO022.png"" alt=""enter image description here"" /></a></p>
<p>What is the reason  for this?</p>
",Preprocessing of the text & Tokenization,using counter list spacy token return non unique dict token want count list spacy token counter class e element type however want count occurences token within list via counter seen result non unique dict token word list dict key ha value screenshot seen dict seemingly ha key twice reason
"How to split input text into equal size of tokens, not character length, and then concatenate the summarization results for Hugging Face transformers","<p>I am using the below methodology to summarize longer than 1024 token size long texts.</p>
<p>Current method splits the text by half. I took this from another user's post and modified it slightly.</p>
<p>So what I want to do is, instead of splitting into half, split whole text into 1024 equal sized tokens and get summarization each of them and then at the end, concatenate them with the correct order and write into file. How can I do this tokenization and getting the correct output?</p>
<p>text split with <code>Split(&quot; &quot;)</code> doesn't work same as tokenization. It produces different count.</p>
<pre><code>import logging
from transformers import pipeline

f = open(&quot;TextFile1.txt&quot;, &quot;r&quot;)

ARTICLE = f.read()

summarizer = pipeline(&quot;summarization&quot;, model=&quot;facebook/bart-large-cnn&quot; )

counter = 1

def summarize_text(text: str, max_len: int) -&gt; str:
    global counter
    try:
        #logging.warning(&quot;max_len &quot; + str(max_len))
        summary = summarizer(text, min_length=30, do_sample=False)
        with open('parsed_'+str(counter)+'.txt', 'w') as f:
            f.write(text)
        counter += 1
        return summary[0][&quot;summary_text&quot;]
    except IndexError as ex:
        logging.warning(&quot;Sequence length too large for model, cutting text in half and calling again&quot;)
        return summarize_text(text=text[:(len(text) // 2)], max_len=max_len) + &quot; &quot; + summarize_text(text=text[(len(text) // 2):], max_len=max_len)

gg = summarize_text(ARTICLE, 1024)

with open('summarized.txt', 'w') as f:
    f.write(gg)
</code></pre>
",Preprocessing of the text & Tokenization,split input text equal size token character length concatenate summarization result hugging face transformer using methodology summarize longer token size long text current method split text half took another user post modified slightly want instead splitting half split whole text equal sized token get summarization end concatenate correct order write file tokenization getting correct output text split work tokenization produce different count
Handling stop words that are part of hyphenated words while preprocessing text,"<p>While pre-processing text by removal of special characters followed by removal of stop words, words such as <code>add-on</code> and <code>non-committal</code> get converted to <code>add</code> and <code>committal</code> respectively. What is the best approach to handle these cases?</p>
",Preprocessing of the text & Tokenization,handling stop word part hyphenated word preprocessing text pre processing text removal special character followed removal stop word word get converted respectively best approach handle case
How can I separate a superscript from its root word using python?,"<p>I am working on a project that requires the separating of a superscript from its root word so that it can be tokenized as a separate token.</p>
<p>if I tokenize &quot;This is a sentence about testString™&quot; the results will be [&quot;this, &quot;is&quot;, &quot;a&quot;, &quot;sentence&quot;, &quot;about&quot;, &quot;testString™&quot;] and I need to get to a solution where the output can look like this [&quot;this, &quot;is&quot;, &quot;a&quot;, &quot;sentence&quot;, &quot;about&quot;, &quot;testString&quot;, &quot;™&quot;]</p>
<p>I am able to isolate the ™ by looking at characters that have a unicode &gt; 127 since that is the highest ascii value, and superscripts do not fall within that range</p>
<pre><code>    text = &quot;This is a sentence about testString™&quot;
    text.join([i for i in text if ord(i) &gt; 127])
    # result -&gt; &quot;™&quot;
</code></pre>
<p>How can I join this back into the text so that the results will be: &quot;This is a sentence about testString ™&quot;</p>
<p>Its really easy to just eliminate the superscripts using this logic, but I need to find a way to actually add it back to the string after the root word</p>
<p>If there are any more simple ideas as to how to tokenize these superscripts I'm certainly open to other suggestions. I could not find any way other that looking at the unicode values to separate them from the root, then applying tokenizer to the new string</p>
",Preprocessing of the text & Tokenization,separate superscript root word using python working project requires separating superscript root word tokenized separate token tokenize sentence teststring result sentence teststring need get solution output look like sentence teststring able isolate looking character unicode since highest ascii value superscript fall within range join back text result sentence teststring really easy eliminate superscript using logic need find way actually add back string root word simple idea tokenize superscript certainly open suggestion could find way looking unicode value separate root applying tokenizer new string
How to transform a list of lists of strings to a frequency DataFrame?,"<p>I have a list of lists of strings (Essentially it's a corpus) and I'd like to convert it to a matrix where a row is a document in the corpus and the columns are the corpus' vocabulary.</p>
<p>I can do this with <code>CountVectorizer</code> but it would require quite a lot of memory as I would need to convert each list into a string that in turn <code>CountVectorizer</code> would tokenize.</p>
<p>I think it's possible to do it with Pandas only but I'm not sure how.</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>corpus = [['a', 'b', 'c'],['a', 'a'],['b', 'c', 'c']]
</code></pre>
<p>expected result:</p>
<pre><code>| a | b | c |
|---|---|---|
| 1 | 1 | 1 |
| 2 | 0 | 0 |
| 0 | 1 | 2 |
</code></pre>
",Preprocessing of the text & Tokenization,transform list list string frequency dataframe list list string essentially corpus like convert matrix row document corpus column corpus vocabulary would require quite lot memory would need convert list string turn would tokenize think possible panda sure example expected result
Python spacy lemmatization issue,"<p>While performing lemmatization using spacy i want all proper nouns to be considered as nouns. How do I customise this?</p>
<p>Tried :[(term.lemma_ for term in nlp(&quot;awards mvp awards&quot;) ]
Expected results: &quot;award mvp award&quot;
Observed results: &quot;award mvp awards&quot;
I understand that this is observed because mvp awards is considered as a proper noun.</p>
",Preprocessing of the text & Tokenization,python spacy lemmatization issue performing lemmatization using spacy want proper noun considered noun customise tried term lemma term nlp award mvp award expected result award mvp award observed result award mvp award understand observed mvp award considered proper noun
nltk tokenization and contractions,"<p>I'm tokenizing text with nltk, just sentences fed to wordpunct_tokenizer. This splits contractions (e.g. 'don't' to 'don' +"" ' ""+'t') but I want to keep them as one word. I'm refining my methods for a more measured  and precise tokenization of text, so I need to delve deeper into the nltk tokenization module beyond simple tokenization. </p>

<p>I'm guessing this is common and I'd like feedback from others who've maybe had to deal with the particular issue before.</p>

<p>edit: </p>

<p>Yeah this a general, splattershot question I know</p>

<p>Also, as a novice to nlp, do I need to worry about contractions at all?</p>

<p>EDIT: </p>

<p>The SExprTokenizer or TreeBankWordTokenizer seems to do what I'm looking for for now.</p>
",Preprocessing of the text & Tokenization,nltk tokenization contraction tokenizing text nltk sentence fed wordpunct tokenizer split contraction e g want keep one word refining method measured precise tokenization text need delve deeper nltk tokenization module beyond simple tokenization guessing common like feedback others maybe deal particular issue edit yeah general splattershot question know also novice nlp need worry contraction edit sexprtokenizer treebankwordtokenizer seems looking
Is Spacy lemmatization not working properly or does it not lemmatize all words ending with &quot;-ing&quot;?,"<p>When I run the spacy lemmatizer, it does not lemmatize the word &quot;consulting&quot; and therefore I suspect it is failing.</p>
<p>Here is my code:</p>
<pre><code>nlp = spacy.load('en_core_web_trf', disable=['parser', 'ner'])
lemmatizer = nlp.get_pipe('lemmatizer')
doc = nlp('consulting')
print([token.lemma_ for token in doc])
</code></pre>
<p>And my output:</p>
<pre><code>['consulting']
</code></pre>
",Preprocessing of the text & Tokenization,spacy lemmatization working properly doe lemmatize word ending ing run spacy lemmatizer doe lemmatize word consulting therefore suspect failing code output
"Why are important, high-ranking (highly frequent) words that are not stop words missing in my word cloud?","<p><a href=""https://i.sstatic.net/DTJ03.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DTJ03.png"" alt=""enter image description here"" /></a>I uploaded a list of highly frequent words which includes stop words and I used the STOPWORDS list to remove stop words. When I print my word cloud, it's not displaying several non-stopwords that are high ranking.</p>
<p>For example, &quot;data&quot; and &quot;learning&quot; appear more frequently than &quot;distribution&quot;, &quot;algorithm&quot; and such. Other words like &quot;training&quot;, &quot;regression&quot; are also missing despite ranking high in frequency. (No, none of these words exist in the STOPWORDS list.) How can I get words like &quot;data&quot;, &quot;learning&quot;, &quot;training&quot; to appear according to their frequency?
(I've attached a screenshot of the words/frequencies.)</p>
<p>Here's the code:</p>
<pre><code>from wordcloud import WordCloud, STOPWORDS
text = open(&quot;test.txt&quot;, mode=&quot;r&quot;, encoding=&quot;utf-8&quot;).read()

wc = WordCloud(background_color=&quot;white&quot;, stopwords=STOPWORDS, height=400, width=600)
wc.generate(text)
wc.to_file(&quot;my_first_word_cloud.png&quot;)
</code></pre>
<p><a href=""https://i.sstatic.net/NezyZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NezyZ.png"" alt=""enter image description here"" /></a></p>
",Preprocessing of the text & Tokenization,important high ranking highly frequent word stop word missing word cloud uploaded list highly frequent word includes stop word used stopwords list remove stop word print word cloud displaying several non stopwords high ranking example data learning appear frequently distribution algorithm word like training regression also missing despite ranking high frequency none word exist stopwords list get word like data learning training appear according frequency attached screenshot word frequency code
NLP: Pre-processing in doc2vec / word2vec,"<p>A few papers on the topics of word and document embeddings (word2vec, doc2vec) mention that they used the Stanford CoreNLP framework to tokenize/lemmatize/POS-tag the input words/sentences:</p>

<blockquote>
  <p>The  corpora  were  lemmatized and POS-tagged with the Stanford CoreNLP (Manning  et  al.,  2014)  and  each  token  was  replaced with its lemma and POS tag</p>
</blockquote>

<p>(<a href=""http://www.ep.liu.se/ecp/131/039/ecp17131039.pdf"" rel=""noreferrer"">http://www.ep.liu.se/ecp/131/039/ecp17131039.pdf</a>)</p>

<blockquote>
  <p>For pre-processing, we tokenise and lowercase the words using Stanford CoreNLP</p>
</blockquote>

<p>(<a href=""https://arxiv.org/pdf/1607.05368.pdf"" rel=""noreferrer"">https://arxiv.org/pdf/1607.05368.pdf</a>)</p>

<p>So my questions are:</p>

<ul>
<li><p>Why does the first paper apply POS-tagging? Would each token then be replaced with something like <code>{lemma}_{POS}</code> and the whole thing used to train the model? Or are the tags used to filter tokens? 
For example, gensims WikiCorpus applies lemmatization per default and then only keeps a few types of part of speech (verbs, nouns, etc.) and gets rid of the rest. So what is the recommended way?</p></li>
<li><p>The quote from the second paper seems to me like they only split up words and then lowercase them. This is also what I first tried before I used WikiCorpus. In my opinion, this should give better results for document embeddings as most of POS types contribute to the meaning of a sentence. Am I right?</p></li>
</ul>

<p>In the original doc2vec paper I did not find details about their pre-processing.</p>
",Preprocessing of the text & Tokenization,nlp pre processing doc vec word vec paper topic word document embeddings word vec doc vec mention used stanford corenlp framework tokenize lemmatize po tag input word sentence corpus lemmatized po tagged stanford corenlp manning et al token wa replaced lemma po tag pre processing tokenise lowercase word using stanford corenlp question doe first paper apply po tagging would token replaced something like whole thing used train model tag used filter token example gensims wikicorpus applies lemmatization per default keep type part speech verb noun etc get rid rest recommended way quote second paper seems like split word lowercase also first tried used wikicorpus opinion give better result document embeddings po type contribute meaning sentence right original doc vec paper find detail pre processing
"How could I implement this for four elements, instead of 2?","<pre><code>for l in f:
    w1, w2 = l.strip().split('\t')
</code></pre>
<p>I'm doing computerized text analysis in Python. This is the original code for splitting bigram elements (words) in a list of bigrams deemed significant (and the overall code is of course longer and incorporates appending the words later, but this is the problematic part of it). For bigrams it works perfectly, but I would like to implement this also for ngrams of more word elements like trigrams and fourgrams. But of course</p>
<pre><code>for l in f:
    w1, w2, w3, w4 = l.strip().split('\t')
</code></pre>
<p>doesn't work.</p>
<p>What would be a good alternative in this scenario?</p>
<p>Thank you for the answers in advance!</p>
",Preprocessing of the text & Tokenization,could implement four element instead computerized text analysis python original code splitting bigram element word list bigram deemed significant overall code course longer incorporates appending word later problematic part bigram work perfectly would like implement also ngrams word element like trigram fourgrams course work would good alternative scenario thank answer advance
"Parsing user query with NLP to identify entities, typos and attributes and running a search on elasticsearch","<p>Hello StackOverflowers,</p>
<p>I have hundreds of thousands of documents in the following structure. I can modify the document before loading it into Elasticsearch, like adding vectors, synonyms or other annotations. Please assume that all the documents are well attributed. Attributes vary based on the category of the product.</p>
<ul>
<li><p>If there is a query, I want to show the precise product for the query.
For example, if someone is searching for &quot;Lee jeans&quot; I want to show all the products which are &quot;Jeans&quot; from the brand &quot;Lee&quot;.</p>
</li>
<li><p>If the user searches for &quot;Lee black jeans&quot; I want to filter out by the variant &quot;Black&quot;</p>
</li>
<li><p>If the user searches for &quot;Lee spring summer jeans&quot; then I just want to show only the following product.</p>
</li>
<li><p>It should be capable of understanding typo's</p>
</li>
<li><p>It should lemmatize. for example, &quot;Chocolate milk&quot; is &quot;Milk&quot;, &quot;Milk chocolate&quot; is &quot;Chocolate&quot;</p>
</li>
</ul>
<p>I've seen some of the approaches on the internet (some of the projects are outdated or not maintained anymore), but I want recommendations the developer community here on what opensource solutions which I can use and what are the changes I should make to the document before loading into Elasticsearch to achieve this.</p>
<pre><code>{
  &quot;product_id&quot;: 489929,
  &quot;name_en&quot;: &quot;Spring Summer Jeans&quot;,
  &quot;attributes&quot;: {
    &quot;category&quot;: &quot;Pants&quot;,
    &quot;type&quot;: [
      &quot;Jeans&quot;,
      &quot;Denim&quot;
    ],
    &quot;brand&quot;: &quot;Lee&quot;,
    &quot;material&quot;: [
      &quot;Cotton&quot;
    ]
  },
  &quot;variants&quot;: {
    &quot;size&quot;: [
      28,
      30,
      32,
      34,
      36
    ],
    &quot;colors&quot;: [
      &quot;Blue&quot;,
      &quot;Black&quot;
    ],
    &quot;fit&quot;: [
      &quot;Regular&quot;,
      &quot;Narrow&quot;
    ],
    &quot;gender&quot;: [
      &quot;Men&quot;,
      &quot;Women&quot;
    ]
  },
  &quot;description_en&quot;: &quot;Quick brown fox jumps over the lazy dog.&quot;,
  &quot;variant_ids&quot;: {
    &quot;1467547&quot;: {
      &quot;size&quot;: 30,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;7487751&quot;: {
      &quot;size&quot;: 32,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;11089927&quot;: {
      &quot;size&quot;: 32,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;11258137&quot;: {
      &quot;size&quot;: 34,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;13266321&quot;: {
      &quot;size&quot;: 30,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;13549929&quot;: {
      &quot;size&quot;: 30,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;17846649&quot;: {
      &quot;size&quot;: 28,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;22602397&quot;: {
      &quot;size&quot;: 36,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;22709931&quot;: {
      &quot;size&quot;: 28,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;23937102&quot;: {
      &quot;size&quot;: 28,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;28519361&quot;: {
      &quot;size&quot;: 30,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;31165878&quot;: {
      &quot;size&quot;: 36,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;31631591&quot;: {
      &quot;size&quot;: 30,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;36914467&quot;: {
      &quot;size&quot;: 36,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: false
    },
    &quot;39141069&quot;: {
      &quot;size&quot;: 28,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;41416888&quot;: {
      &quot;size&quot;: 36,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;43504246&quot;: {
      &quot;size&quot;: 34,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;45374599&quot;: {
      &quot;size&quot;: 34,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;46361047&quot;: {
      &quot;size&quot;: 28,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;46909634&quot;: {
      &quot;size&quot;: 32,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;49407526&quot;: {
      &quot;size&quot;: 32,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;54529078&quot;: {
      &quot;size&quot;: 34,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;55659499&quot;: {
      &quot;size&quot;: 28,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: false
    },
    &quot;55762371&quot;: {
      &quot;size&quot;: 34,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;57049076&quot;: {
      &quot;size&quot;: 36,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;57973674&quot;: {
      &quot;size&quot;: 36,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;58218538&quot;: {
      &quot;size&quot;: 28,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;58227462&quot;: {
      &quot;size&quot;: 30,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;58232621&quot;: {
      &quot;size&quot;: 30,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;59320783&quot;: {
      &quot;size&quot;: 30,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;63244508&quot;: {
      &quot;size&quot;: 32,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;66194331&quot;: {
      &quot;size&quot;: 36,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;71212553&quot;: {
      &quot;size&quot;: 32,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;84143801&quot;: {
      &quot;size&quot;: 34,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;86881320&quot;: {
      &quot;size&quot;: 34,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;89177537&quot;: {
      &quot;size&quot;: 32,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;90449959&quot;: {
      &quot;size&quot;: 36,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Narrow&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;92989653&quot;: {
      &quot;size&quot;: 34,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;93319121&quot;: {
      &quot;size&quot;: 32,
      &quot;color&quot;: &quot;Blue&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Women&quot;,
      &quot;in_stock&quot;: true
    },
    &quot;95212291&quot;: {
      &quot;size&quot;: 28,
      &quot;color&quot;: &quot;Black&quot;,
      &quot;fit&quot;: &quot;Regular&quot;,
      &quot;gender&quot;: &quot;Men&quot;,
      &quot;in_stock&quot;: true
    }
  }
}
</code></pre>
",Preprocessing of the text & Tokenization,parsing user query nlp identify entity typo attribute running search elasticsearch hello stackoverflowers hundred thousand document following structure modify document loading elasticsearch like adding vector synonym annotation please assume document well attributed attribute vary based category product query want show precise product query example someone searching lee jean want show product jean brand lee user search lee black jean want filter variant black user search lee spring summer jean want show following product capable understanding typo lemmatize example chocolate milk milk milk chocolate chocolate seen approach internet project outdated maintained anymore want recommendation developer community opensource solution use change make document loading elasticsearch achieve
Removing stop words from string using spacy in diffrent languages,"<p>I have an array of strings in different languages and I would like to remove stop words from these strings.
example of string :</p>
<pre><code>[&quot;mai fostul președinte egiptean mohamed morsi &quot;, &quot;em bon jovi lançou o álbum have a nice day a &quot;, &quot; otok škulj är en ö i kroatien den ligger i län&quot;...]
</code></pre>
<p>this is the list of languages I'm willing to use :</p>
<pre><code>['French',
 'Spanish',
 'Thai',
 'Russian',
 'Persian',
 'Indonesian',
 'Arabic',
 'Pushto',
 'Kannada',
 'Danish',
 'Japanese',
 'Malayalam',
 'Latin',
 'Romanian',
 'Swedish',
 'Portugese',
 'English',
 'Turkish',
 'Tamil',
 'Urdu',
 'Korean',
 'German',
 'Greek',
 'Italian',
 'Chinese',
 'Dutch',
 'Estonian',
 'Hindi']
</code></pre>
<p>I am using <code>Spacy</code> library, but I'm looking for something that support multiple languages.</p>
<p>what I have tried already:</p>
<pre><code>import pandas as pd
import nltk
nltk.download('punkt')
import spacy
nlp = spacy.load(&quot;xx_ent_wiki_sm&quot;)
from spacy.tokenizer import Tokenizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

doc = nlp(&quot;This is a sentence about Facebook.&quot;)
print([(ent.text, ent.label) for ent in doc.ents])
all_stopwords = nlp.Defaults.stop_words
all_stopwords = nlp.Defaults.stop_words

data_text=df1['Text'] #here where i store my strings 

for x in data_text:
    text_tokens = word_tokenize(x)
    tokens_without_sw=[word for word in text_tokens if not word inall_stopwords]

print(tokens_without_sw)
</code></pre>
",Preprocessing of the text & Tokenization,removing stop word string using spacy diffrent language array string different language would like remove stop word string example string list language willing use using library looking something support multiple language tried already
"Regex for strings from a, b such that each a is immediately preceded and followed by b","<p>I'm trying to learn regular expressions through some practice problems I found online. The question apparently comes from Jurafsky's book on NLP and the solution cited everywhere is:
(b+(ab+)+)?</p>

<p>I don't think this is correct because it would leave out the possibility of strings consisting in a single b.</p>

<p>My proposed solution is (b+(ab+)*)? or (b+(ab+)*)*</p>

<p>Is my conclusion incorrect?</p>

<p>Question original text: The set of all strings from the alphabet a, b such that each a is immediately preceded by and immediately followed by a b.</p>
",Preprocessing of the text & Tokenization,regex string b immediately preceded followed b trying learn regular expression practice problem found online question apparently come jurafsky book nlp solution cited everywhere b ab think correct would leave possibility string consisting single b proposed solution b ab b ab conclusion incorrect question original text set string alphabet b immediately preceded immediately followed b
Output text with specifically chosen tokens in parenthesis with Spacy,"<p>I want to print a sentence in my terminal with some specific words in curly parenthesis.
For instance if I want the word in 5th and 7th position of this sentence to be parenthesised:</p>
<pre><code>My important word is here and there.
</code></pre>
<p>The output should be:</p>
<pre><code>My important word is {here} and {there}.
</code></pre>
<p>I want the solution to be in python and in particular with spacy.
So far I managed to do a program like this:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
nlp = spacy.load('en_core_web_sm')
doc = nlp('My important word is here and there.')
my_important_words = [4,6]
for token in doc:
    if token.i in my_important_words:
        print(&quot;{&quot;+token.text+&quot;}&quot;)
    else:
        print(token.text)
</code></pre>
<p>But not only my for loop displays words line by lines but also it sounds pretty verbose program to me. I cannot believe a library like spacy has not a straightforward one/twoliner way to do that.</p>
<p>Any solution?</p>
<p>PS:
I know there is displacy fancy solutions for stressing words with some labeled property like this:
<a href=""https://stackoverflow.com/questions/51739273/spacy-verb-highlight"">Spacy Verb highlight?</a></p>
<p>but it is not really the same because 1) my set of words is a list of words/tokens arbitrary chosen by me 2) I do not want some displacy render html things. I just want plain print on my terminal.</p>
",Preprocessing of the text & Tokenization,output text specifically chosen token parenthesis spacy want print sentence terminal specific word curly parenthesis instance want word th th position sentence parenthesised output want solution python particular spacy far managed program like loop display word line line also sound pretty verbose program believe library like spacy ha straightforward one twoliner way solution p know displacy fancy solution stressing word labeled property like href verb highlight really set word list word token arbitrary chosen want displacy render html thing want plain print terminal
error while removing the stop-words from the text,"<p>I am trying to remove stopwords from my data and I have used this statement to download the stopwords.</p>
<p>stop = set(stopwords.words('english'))</p>
<p>This has character 'd' as one of the stopwords. So, when I apply this to my function it is removing 'd' from the word. Please see the attached picture for the reference and guide me how to fix this.</p>
<p><a href=""https://i.sstatic.net/x0JjI.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",Preprocessing of the text & Tokenization,error removing stop word text trying remove stopwords data used statement download stopwords stop set stopwords word english ha character one stopwords apply function removing word please see attached picture reference guide fix enter image description
Speed up NLP cleaning preprocessing by multiprocessing/parallelisation,"<p>All in the title, I have to clean up a list of sentences varying from 10 000 to 100 000 with a whole cleaning pipeline before tokenization for an NLP clustering subject, I am looking for a way to optimize performance on list operations.</p>
<p>On the cleaning pipeline there is a part where I need to search and replace expression (I use re.sub with a dictionary)
Until now it was working well, but now the dictionary has increased to 5000 expressions (10x more). Time complexity is like n * w * rw (n number of sentences, m number of words, rw number of regex expression)). The execution cluster has 32 CPUs, that's why I think it could be a solution.</p>
<p>I tried to use python multiprocessing but I've encountered well known issues such as Pickle problems.. The code is implemented in classes and I don't want to transform all the code. Besides, I'm looking for a &quot;best practice&quot; solution. I've found that multiprocessing with python is awkward.</p>
<p>I'm currently exploring solutions such as ray, pyspark ..</p>
<p>What would be the best practice to clean up a text with best performances?</p>
<p>I have never saw a ressource or course that talk about this specific task for cleaning very long corpus.</p>
",Preprocessing of the text & Tokenization,speed nlp cleaning preprocessing multiprocessing parallelisation title clean list sentence varying whole cleaning pipeline tokenization nlp clustering subject looking way optimize performance list operation cleaning pipeline part need search replace expression use sub dictionary wa working well dictionary ha increased expression x time complexity like n w rw n number sentence number word rw number regex expression execution cluster ha cpu think could solution tried use python multiprocessing encountered well known issue pickle problem code implemented class want transform code besides looking best practice solution found multiprocessing python awkward currently exploring solution ray pyspark would best practice clean text best performance never saw ressource course talk specific task cleaning long corpus
DocumentTermMatrix misses some words,"<p>I am using DocumentTermMatrix to find a list of keywords in a long text. Most of the words in my list are correctly found, but there are a couple that are missing. Now, I would love to post here a minimal working example, but the problem is: there is one of the words (&quot;insolvency&quot;, so not a short word as in the problem <a href=""https://stackoverflow.com/questions/46639795/documenttermmatrix-in-tm-package-does-not-return-all-words"">here</a>) in a document of 32 pages which is missed. Now, this word is actually in page 7 of the text. But if I reduce my text with <code>text &lt;- text[7]</code>, then DocumentTermMatrix actually finds it! So I am not able to reproduce this with a minimal working example...</p>
<p>Do you have any ideas?</p>
<p>Below a sketch of my script:</p>
<pre><code>library(fastpipe)
library(openxlsx)
library(tm)

`%&gt;&gt;%` &lt;- fastpipe::`%&gt;&gt;%`

source(&quot;cleanText.R&quot;) # Custom function to clean up the text from reports

keywords_xlsx &lt;- read.xlsx(paste0(getwd(),&quot;/Keywords.xlsx&quot;),
                           sheet = &quot;all&quot;,
                           startRow = 1,
                           colNames = FALSE,
                           skipEmptyRows = TRUE,
                           skipEmptyCols = TRUE)

keywords &lt;- keywords_xlsx[1] %&gt;&gt;%
  tolower(as.character(.[,1]))

# Custom function to read pdfs
read &lt;- readPDF(control = list(text = &quot;-layout&quot;))

# Extract text from pdf
report &lt;- &quot;my_report.pdf&quot;
document &lt;- Corpus(URISource(paste0(&quot;./Annual reports/&quot;, report)), readerControl = list(reader = read))
text &lt;- content(document[[1]]) 

text &lt;- cleanText(report, text) # This is a custom function to clean up the texts

# text &lt;- text[7] # If I do this, my word is found! Otherwise it is missed

# Create a corpus  
text_corpus &lt;- Corpus(VectorSource(text))


matrix &lt;- t(as.matrix(inspect(DocumentTermMatrix(text_corpus,
                                                 list(dictionary = keywords,
                                                      list(wordLengths=c(1, Inf))
                                                 )
))))
  
  
words &lt;- sort(rowSums(matrix),decreasing=TRUE) 
df &lt;- data.frame(word = names(words),freq=words)

</code></pre>
",Preprocessing of the text & Tokenization,documenttermmatrix miss word using documenttermmatrix find list keywords long text word list correctly found couple missing would love post minimal working example problem one word insolvency short word problem href document page missed word actually page text reduce text documenttermmatrix actually find able reproduce minimal working example p idea sketch script
Special Case Lemmatization ValueError while Using spacy for NLP,"<h1>Special Case Lemmatization ValueError while Using spacy for NLP</h1>
<h2>Problem (What I think is happening)</h2>
<p>While exploring special case lemmatization, I ran into a ValueError (provided below). I actually want to modify the text, changing &quot;Friso&quot; to &quot;San Francisco.&quot; Does anyone see what I am doing wrong here?</p>
<h2>Code</h2>
<pre><code># (using spacy v3.4)
import spacy
from spacy.symbols import *  
nlp = spacy.load(&quot;en_core_web_sm&quot;)  
doc = nlp(&quot;I am flying to Frisco.&quot;)  
special_case = [ {NORM:&quot;Frisco&quot;, ORTH:&quot;San Francisco&quot;} ]  
print([token.text for token in doc])
nlp.tokenizer.add_special_case(&quot;Frisco&quot;, special_case)  
print([token.lemma_ for token in nlp(&quot;I am flying to Frisco.&quot;)])
</code></pre>
<h2>Error</h2>
<p><code>ValueError: [E997] Tokenizer special cases are not allowed to modify the text. This would map 'Frisco' to 'San Francisco' given token attributes '[{67: 'Frisco', 65: 'San Francisco'}]'</code></p>
",Preprocessing of the text & Tokenization,special case lemmatization valueerror using spacy nlp special case lemmatization valueerror using spacy nlp problem think happening exploring special case lemmatization ran valueerror provided actually want modify text changing friso san francisco doe anyone see wrong code error
Why does my nltk &#39;for&#39; loop repeat results instead of moving to the next sentence?,"<p>Let's imagine I have these 5 sentences in the df2['CleanDescr'] after removing stop words and lemmatization:</p>
<pre><code>garcia cash drawer reconciliation report distribution hill specialty
jiang report not delivered oic surgical minute
rosario requesting case log - chadwycke r. smith
villalta clarity report - &quot;solid organ transplant&quot;
wallace need assistance with monthly clarity report
</code></pre>
<p>I tried to run the nltk.tag.pos_tag for each sentence in 2 different ways, but they kept repeating after the first sentence. These are the 2 ways I did it:</p>
<p>include_tags = {'NN', 'VB', 'PRP', 'VBZ', 'VBP', 'VPB', 'VBD', 'NNS', 'NNPS'}</p>
<p>1.</p>
<pre><code>def remove_tag(tagset):
    for word in df2['CleanDescr']:
        tagged_sent = nltk.tag.pos_tag(word.split())
        #print(tagged_sent)
        edited_sent = ' '.join([words for words,tag in tagged_sent if tag in include_tags])
        #print(edited_sent)
        return edited_sent

df2['CleanDescr'] = df2['CleanDescr'].apply(remove_tag)
df2['CleanDescr']
</code></pre>
<ol start=""2"">
<li></li>
</ol>
<pre><code>def remove_tag(tagset):
    for word in df2['CleanDescr']:
        tagged_sent = nltk.tag.pos_tag(word.split())
        #print(tagged_sent)
        for tag in tagged_sent:
            if tag in include_tags:
                edited_sent = ' '.join()
                return edited_sent

df2['CleanDescr'] = df2['CleanDescr'].apply(remove_tag)
df2['CleanDescr']
</code></pre>
<p>The result is supposed to run through all 5 sentences. Instead, they repeat after the first sentence. Here's my result:</p>
<pre><code>0        garcia cash drawer distribution hill specialty...
1        garcia cash drawer distribution hill specialty...
2        garcia cash drawer distribution hill specialty...
3        garcia cash drawer distribution hill specialty...
4        garcia cash drawer distribution hill specialty...
</code></pre>
",Preprocessing of the text & Tokenization,doe nltk loop repeat result instead moving next sentence let imagine sentence df cleandescr removing stop word lemmatization tried run nltk tag po tag sentence different way kept repeating first sentence way include tag nn vb prp vbz vbp vpb vbd nns nnps result supposed run sentence instead repeat first sentence result
Complex Regex not working in Spacy entity ruler,"<p>I'm trying to identify the entities by passing the Regular expression (Regex) to the Spacy model using Entity Ruler but, Spacy is unable to identify based on the below regex.</p>
<p>But, I tested the regex <a href=""https://regex101.com/r/A5iiWc/4"" rel=""nofollow noreferrer"">here</a> and it's working.</p>
<pre><code>import model_training
import spacy

nlp = spacy.load('en_core_web_trf')
nlp.add_pipe(&quot;spacytextblob&quot;)

nlp = model_training.train_model_with_regex(nlp)
</code></pre>
<p>model_training.py</p>
<pre><code>def train_model_with_regex(nlp):
ruler = nlp.add_pipe(&quot;entity_ruler&quot;, before=&quot;ner&quot;)
patterns = [
    {
        &quot;label&quot;: &quot;VOLUME&quot;,
        &quot;pattern&quot;: [{&quot;LOWER&quot;: {'REGEX': &quot;(?:\d+\s(?:million|hundred|thousand|billion)*\s*)+&quot;}}]
    }
]

ruler.add_patterns(patterns)
return nlp
</code></pre>
<p>I wanted to achieve this, for the below example</p>
<pre><code>text = &quot;I have spent 5 million to buy house and 70 thousand for the furniture&quot;
</code></pre>
<p>expected output:</p>
<pre><code>{'result': [
    {'label': 'VOLUME', 'text': '5 million'},
    {'label': 'VOLUME', 'text': '70 thousand'}
]}
</code></pre>
",Preprocessing of the text & Tokenization,complex regex working spacy entity ruler trying identify entity passing regular expression regex spacy model using entity ruler spacy unable identify based regex tested regex working model training py wanted achieve example expected output
How to find the plural version of a stem word in Swift,"<p>I have this code to lemmatize a word to find it's stem version.</p>
<pre><code>let text = &quot;people&quot;
let tagger = NLTagger(tagSchemes: [.lemma])
tagger.string = text
tagger.setLanguage(.english, range: text.startIndex..&lt;text.endIndex)
let (tag, range) = tagger.tag(at: text.startIndex, unit: .word, scheme: .lemma)
let stemForm = tag?.rawValue ?? String(text[range]) // person
</code></pre>
<p>Is it possible to do the reverse in Swift and find the plural version of a word?</p>
",Preprocessing of the text & Tokenization,find plural version stem word swift code lemmatize word find stem version possible reverse swift find plural version word
How to lemmatize a single word in Swift,"<p>How do you get the stem form of a single word token? Here is my code. It works for some words, but not others.</p>
<pre><code>let text = &quot;people&quot; // works
// let text = &quot;geese&quot; // doesn't work
let tagger = NLTagger(tagSchemes: [.lemma])
tagger.string = text
let (tag, range) = tagger.tag(at: text.startIndex, unit: .word, scheme: .lemma)
let stemForm = tag?.rawValue ?? String(text[range])
</code></pre>
<p>However, if I lemmatize the entire text it's able to find all the stem forms of words.</p>
<pre><code>let text = &quot;This is text with plurals such as geese, people, and millennia.&quot;
let tagger = NLTagger(tagSchemes: [.lemma])
tagger.string = text

var words: [String] = []
tagger.enumerateTags(in: text.startIndex..&lt;text.endIndex, unit: .word, scheme: .lemma, options: [.omitWhitespace, .omitPunctuation]) { tag, range in
    let stemForm = tag?.rawValue ?? String(text[range])
    words += [stemForm]
    return true
}

// this be text with plural such as goose person and millennium
words.joined(separator: &quot; &quot;)
</code></pre>
<p>Also, is it possible to reverse the process and find the plural version of a stem word?</p>
",Preprocessing of the text & Tokenization,lemmatize single word swift get stem form single word token code work word others however lemmatize entire text able find stem form word also possible reverse process find plural version stem word
splitting string made out of dataframe row wise,"<p>I'm trying to tokenize the words within dataframe which looks like</p>
<pre><code>  A            B       C          D            E           F
0 Orange     robot   x eyes   discomfort   striped tee    nan
1 orange     robot  blue beams   grin      vietnam jacket nan
2 aquamarine robot   3d          bored        cigarette   nan   
     
</code></pre>
<p>After removing all the special characters the dataframe became a string like this</p>
<pre><code>df_str = df.to_string(header=False)
    
import re

normalised_text = bayc_features_str.lower()
text = re.sub(r&quot;[^\a-zA-Z0-9 ]&quot;,&quot;&quot;, normalised_text)

print(text)

    1    orange   robot   x eyes   discomfort   striped tee   nan
    2    orange   robot   blue beams   grin   vietnam jacket  nan
    3    aquamarine  robot   3d       bored       cigarette    nan   
</code></pre>
<p>so when I tokenize this string, with below code</p>
<pre><code>def tokenize(obj):
    if obj is None:
        return None
    elif isinstance(obj, str): 
        return word_tokenize(obj)
    elif isinstance(obj, list):
        return [tokenize(i) for i in obj
    else:
        return obj

tokenized_text = (tokenize(text))
</code></pre>
<p>I get the output</p>
<pre><code>['orange', 'robot', 'x', 'eyes', 'discomfort', 'striped', 'tee', nan,'orange', 'robot', 'blue', 'beams', 'grin', 'vietnam', 'jacket', nan,'aquamarine', 'robot', '3d', 'bored', 'cigarette', nan, 'sea', 'captains', 'hat']
</code></pre>
<p>which is quite different from the output I expected</p>
<pre><code>[['orange'], ['robot'], ['x', 'eyes'], ['discomfort'], ['striped', 'tee'], nan]
[['orange'], ['robot'], ['blue', 'beams'], ['grin'], ['vietnam', 'jacket'], nan]
[['aquamarine'], ['robot'], ['3d'], ['bored', 'cigarette'], nan, ['sea', 'captains', 'hat']]
</code></pre>
<p>Any ideas on how can I get the output I expected?
Any help would be greatly appreciated!</p>
",Preprocessing of the text & Tokenization,splitting string made dataframe row wise trying tokenize word within dataframe look like removing special character dataframe became string like tokenize string code get output quite different output expected idea get output expected help would greatly appreciated
Keras&#39; Tokenizer vs sklearn&#39;s CountVectorizer,"<p>I have a big pile of raw-text documents.</p>

<p>I am not sure whether to use <strong>keras.text_preprocessing.Tokenizer</strong> or <strong>sklearn.feature_extraction.text.CountVectorizer</strong>. Any idea, suggestions? Which is faster? Do they have any major difference that I might be missing? </p>

<p>Thanks </p>
",Preprocessing of the text & Tokenization,kera tokenizer v sklearn countvectorizer big pile raw text document sure whether use kera text preprocessing tokenizer sklearn feature extraction text countvectorizer idea suggestion faster major difference might missing thanks
R text analysis: Counting occurences of any combinations of words from two different keyword lists with a given distance of each other,"<p>Thanks for reading. For a reserach project, I'm doing some text analysis. We are analyzing large texts (company reports) and I'm looking to count keyword frequencies within that text.</p>
<p>However, I have two lists of keywords, and I dont want to count the number of occurances of these words, but the number of times any two words from these lists appear within a certain distance from each other in the main text.</p>
<pre><code>text &lt;- c(&quot;The house is blue. The car is very big and red.&quot;)
words1 &lt;- c(&quot;car&quot;, &quot;house&quot;) 
words2 &lt;- c(&quot;blue&quot;, &quot;red&quot;) 
</code></pre>
<p>The desired functionality should, for example, return 1 for distance 3. (Number of any combinations in given distance.)</p>
<p>I know about the <code>text_count</code> function from the <code>stringb</code> package and <code>kwic</code> from <code>quantea</code>. However, thats not really what Im looking for.</p>
<p>Thanks, any help is appreciated.</p>
",Preprocessing of the text & Tokenization,r text analysis counting occurences combination word two different keyword list given distance thanks reading reserach project text analysis analyzing large text company report looking count keyword frequency within text however two list keywords dont want count number occurances word number time two word list appear within certain distance main text desired functionality example return distance number combination given distance know function package however thats really im looking thanks help appreciated
Remove a certain word from a list of sentences,"<p>Is there a way to remove a certain word from a list of sentences if that word appears after a list of words?</p>
<p>For example, I want to remove the word &quot;and&quot; if &quot;and&quot; appears after a list of words (<code>[ &quot;red&quot;, &quot;blue&quot;, &quot;green&quot;]</code>). I know how to remove a word if it appears after one particular word but is there a way to do the same for a list of words? A regular expression?</p>
",Preprocessing of the text & Tokenization,remove certain word list sentence way remove certain word list sentence word appears list word example want remove word appears list word know remove word appears one particular word way list word regular expression
Lowercase Lemmatization with spacy in german,"<p>There seems to be a problem with noun singularization with spacy in german.
Spacy seems to rely on words to be capitalized to recognize them as nouns. An example:</p>

<pre><code>import spacy
nlp = spacy.load(""C:\\Users\\somepath\\spacy\\de_core_md\\de_core_news_md\\de_core_news_md-2.2.5"")

def lemmatize_text(text):
    """"""returns the text with each word in its basic form""""""
    doc = nlp(text)
    return [word.lemma_ for word in doc]

lemmatize_text('Das Wort Tests wird erkannt. Allerdings werden tests nicht erkannt')
--&gt; ['der', 'Wort', 'Test', 'werden', 'erkennen', '.', 'Allerdings', 'werden', 'tests', 'nicht', 'erkennen']

# should say 'Test' for both sentences
</code></pre>

<p>That would not be a problem if I was lemmatizing the original text right away. However, my preprocessing looks like this:</p>

<ol>
<li>turn to lowercase</li>
<li>remove punctuation</li>
<li>remove stopwords</li>
<li>lemmatize</li>
</ol>

<p>Is there a recommended order in which to execute the above steps?</p>

<p>I am not lemmatizing first because words at the beginning of a sentence are then not recognized correctly:</p>

<pre><code>lemmatize_text('Größer wird es nicht mehr. größer wird es nicht mehr.')
--&gt; ['Größer', 'werden', 'ich', 'nicht', 'mehr', '.', 'groß', 'werden', 'ich', 'nicht', 'mehr', '.']

# should say 'groß' for both sentences
</code></pre>
",Preprocessing of the text & Tokenization,lowercase lemmatization spacy german seems problem noun singularization spacy german spacy seems rely word capitalized recognize noun example would problem wa lemmatizing original text right away however preprocessing look like turn lowercase remove punctuation remove stopwords lemmatize recommended order execute step lemmatizing first word beginning sentence recognized correctly
BPE multiple ways to encode a word,"<p>With BPE or WordPiece there might be multiple ways to encode a word. For instance, assume (for simplicity) the token vocabulary contains all letters as well as the merged symbols (&quot;to&quot;, &quot;ke&quot;, &quot;en&quot;). Then the word &quot;token&quot; could be encoded as (&quot;to&quot;, &quot;ke&quot;, &quot;n&quot;) or (&quot;to&quot;, &quot;k&quot;, &quot;en&quot;). Such ambiguous encodings are also mentioned in this tutorial <a href=""https://blog.floydhub.com/tokenization-nlp/"" rel=""nofollow noreferrer"">https://blog.floydhub.com/tokenization-nlp/</a></p>
<p>However, in the hugginface tutorial it is mentioned that &quot;BPE and WordPiece [...] work out rules in a certain order that you can then apply in the same order when tokenizing new text&quot;, see <a href=""https://huggingface.co/transformers/master/tokenizer_summary.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/master/tokenizer_summary.html</a>.</p>
<p>How exactly are these rules stored and applied when using BPE/WordPiece, e.g., in my example above, how is it determined which tokenization to use?</p>
",Preprocessing of the text & Tokenization,bpe multiple way encode word bpe wordpiece might multiple way encode word instance assume simplicity token vocabulary contains letter well merged symbol ke en word token could encoded ke n k en ambiguous encoding also mentioned tutorial however hugginface tutorial mentioned bpe wordpiece work rule certain order apply order tokenizing new text see exactly rule stored applied using bpe wordpiece e g example determined tokenization use
Dictionary Based Text CLassification in Python,"<p>It's quite a while now that I'm looking for a descent <strong>dictionary based text classification</strong> library in Python.</p>
<p>My use case is as follow: I will be receiving a long <strong><code>text</code></strong> which would likely talk about several things and hopefully mention some of a pre-defined <strong><code>entities</code></strong>.</p>
<pre><code>text = &quot;Yesterday, I ate a Yelow-fruit. It was the longest fruit I ever ate.&quot;
entities = {&quot;apple&quot;: [&quot;pink&quot;, &quot;sphere&quot;], &quot;banana&quot;: [&quot;yellow&quot;, &quot;tasty&quot;, &quot;long&quot;]}
</code></pre>
<p><strong>Please note that spelling errors are intentional !</strong></p>
<p>My goal is to have a program such that given this text and the entities dict (which will change over time), the program should output <strong><code>banana</code></strong>. Hence, the problem can be seen as a <strong>dictionay based text classification</strong> problem where one will classify the text based on the <strong>entities</strong> dictionary.</p>
<p>The latest problem seems so standard to me but I fail finding a descent implementation in Python.</p>
<p>Of course, I can go through the text and count word occurences by entity and finally output the most frequent entity. But this approach is very simple and won't survive in real word scenario where the occurences would <strong>NOT</strong> be exact. I would expect a good approach to include some text similarity metrics and allow user to choose which pre-processings are accpetable (lowercase, stemming, stopwords removal, ...). How tokenization should be done ? Is there a <strong>semantic</strong> similarity or not ? If there is a semantic similarity, does a dictionay expansion algorithm is offered ?</p>
<p>So far, I've read this <a href=""https://sicss.io/2019/materials/day3-text-analysis/dictionary-methods/rmarkdown/Dictionary-Based_Text_Analysis.html"" rel=""nofollow noreferrer"">R blog post</a> which gives a starting point for a dictionary based text classification in R. This <a href=""https://datascience.stackexchange.com/questions/111912/performing-a-text-classification-based-on-a-dictionary"">datascience stack-exchange</a> question seems to be related to mine as well. But none of these give a satisfactory anwser.</p>
<p>So, Is there any straightforward library in Python for this kind of task ?</p>
<p>Thanks in adance for your replies.</p>
",Preprocessing of the text & Tokenization,dictionary based text classification python quite looking descent dictionary based text classification library python use case follow receiving long would likely talk several thing hopefully mention pre defined please note spelling error intentional goal program given text entity dict change time program output hence problem seen dictionay based text classification problem one classify text based entity dictionary latest problem seems standard fail finding descent implementation python course go text count word occurences entity finally output frequent entity approach simple survive real word scenario occurences would exact would expect good approach include text similarity metric allow user choose pre processing accpetable lowercase stemming stopwords removal tokenization done semantic similarity semantic similarity doe dictionay expansion algorithm offered far read r blog post give starting point dictionary based text classification r href stack exchange question seems related mine well none give satisfactory anwser straightforward library python kind task thanks adance reply
cleantext module can not be imported on google colab pro,"<p>In google colab, I could install and import clean-text module well, but after changing the account to google colab pro, this module can not be imported. I've tried these lines of code for installing:</p>
<pre><code>!pip install -q clean-text[gpl]
</code></pre>
<p>and</p>
<pre><code>!conda install clean-text[gpl]
</code></pre>
<p>and</p>
<pre><code>!pip install cleantext
</code></pre>
<p>and I imported it like this:</p>
<pre><code>import cleantext
</code></pre>
<p>or</p>
<pre><code>from cleantext import clean
</code></pre>
<p>but I got this error:</p>
<blockquote>
<p>ModuleNotFoundError: No module named 'cleantext'</p>
</blockquote>
<p>How can I fix this on google colab pro?</p>
",Preprocessing of the text & Tokenization,cleantext module imported google colab pro google colab could install import clean text module well changing account google colab pro module imported tried line code installing imported like got error modulenotfounderror module named cleantext fix google colab pro
NLP Stemming and Lemmatization using Regular expression tokenization,"<p>Define a function called <code>performStemAndLemma</code>, which takes a parameter. The first parameter, <code>textcontent</code>, is a string. The function definition code stub is given in the editor. Perform the following specified tasks:</p>
<p>1.Tokenize all the words given in <code>textcontent</code>. The word should contain alphabets or numbers or underscore. Store the tokenized list of words in <code>tokenizedwords</code>. (Hint: Use regexp_tokenize)</p>
<ol start=""2"">
<li><p>Convert all the words into lowercase. Store the result into the variable <code>tokenizedwords</code>.</p>
</li>
<li><p>Remove all the stop words from the unique set of <code>tokenizedwords</code>. Store the result into the variable <code>filteredwords</code>. (Hint: Use stopwords corpora)</p>
</li>
<li><p>Stem each word present in <code>filteredwords</code> with PorterStemmer, and store the result in the list <code>porterstemmedwords</code>.</p>
</li>
<li><p>Stem each word present in <code>filteredwords</code> with LancasterStemmer, and store the result in the list <code>lancasterstemmedwords</code>.</p>
</li>
<li><p>Lemmatize each word present in <code>filteredwords</code> with WordNetLemmatizer, and store the result in the list <code>lemmatizedwords</code>.</p>
</li>
</ol>
<p>Return <code>porterstemmedwords</code>, <code>lancasterstemmedwords</code>, <code>lemmatizedwords</code> variables from the function.</p>
<p>My code:</p>
<pre><code>from nltk.corpus import stopwords
def performStemAndLemma(textcontent):
    # Write your code here
    #Step 1
    tokenizedword = nltk.tokenize.regexp_tokenize(textcontent, pattern = '\w*', gaps = False)
    #Step 2
    tokenizedwords = [x.lower() for x in tokenizedword if x != '']
    #Step 3
    unique_tokenizedwords = set(tokenizedwords)
    stop_words = set(stopwords.words('english')) 
    filteredwords = []
    for x in unique_tokenizedwords:
        if x not in stop_words:
            filteredwords.append(x)
    #Steps 4, 5 , 6
    ps = nltk.stem.PorterStemmer()
    ls = nltk.stem.LancasterStemmer()
    wnl = nltk.stem.WordNetLemmatizer()
    porterstemmedwords =[]
    lancasterstemmedwords = []
    lemmatizedwords = []
    for x in filteredwords:
        porterstemmedwords.append(ps.stem(x))
        lancasterstemmedwords.append(ls.stem(x))
        lemmatizedwords.append(wnl.lemmatize(x))
    return porterstemmedwords, lancasterstemmedwords, lemmatizedwords
</code></pre>
<p>Still the program is not working fine. Not passing the 2 test cases. Highlight the mistake in above code and provide alternate solution for the same.</p>
",Preprocessing of the text & Tokenization,nlp stemming lemmatization using regular expression tokenization define function called take parameter first parameter string function definition code stub given editor perform following specified task tokenize word given word contain alphabet number underscore store tokenized list word hint use regexp tokenize convert word lowercase store result variable remove stop word unique set store result variable hint use stopwords corpus stem word present porterstemmer store result list stem word present lancasterstemmer store result list lemmatize word present wordnetlemmatizer store result list return variable function code still program working fine passing test case highlight mistake code provide alternate solution
processing natural language that descripe time frequency with R,"<p>I'm dealing with data that descripe onset frequency of a symptom. The text in each cell was not in the same format. For example:</p>
<pre><code>&quot;Pattern 1: 1-3/months, Pattern 2: 6 times per month&quot;                                                                                                                   
&quot;4-5 times/day&quot;                                                                                                                                                          
&quot;10 times/day to 7-8 times/week&quot;                                                                                                                                         
&quot;varies (average 0-10 fits/month)&quot;                                                                                                                                       
&quot;1-2 times/month, but 2-3/week in recent 3 months\n&quot;                                                                                                                     
&quot;2-3/month&quot;
</code></pre>
<p>What I'm trying to do is to generate the minimum and maximum frequency of each description for further statistical analysis. I can try to complete this job by regular expression, although it might not be very efficient. I'm very new to natural lauguage processing, is there a more efficient way to do this other than the regular expression way? Thanks!</p>
<p>I can read and type the frequency by hand. But from what I've learned, this is always a bad practice to data analysis.</p>
<p>(Sorry about not being able to ask the question in a more specific way.)</p>
",Preprocessing of the text & Tokenization,processing natural language descripe time frequency r dealing data descripe onset frequency symptom text cell wa format example trying generate minimum maximum frequency description statistical analysis try complete job regular expression although might efficient new natural lauguage processing efficient way regular expression way thanks read type frequency hand learned always bad practice data analysis sorry able ask question specific way
spaCy phrase matcher:TypeError when trying to remove matched phrases,"<p>I am trying to effectively clean text that was derived from automatic speech recognition software Using spaCy phrase matcher (<a href=""https://spacy.io/usage/rule-based-matching#phrasematcher"" rel=""nofollow noreferrer"">https://spacy.io/usage/rule-based-matching#phrasematcher</a>).  The data is very dirty and does not separate speakers, so I am trying to remove repetitive phrases across all data samples.  Using the rule-based phrase matcher, I am able to find the target text in my sample strings, but in trying to replace them with spaces, I receive a type error below:
<code>TypeError: replace() argument 1 must be str, not spacy.tokens.token.Token</code></p>
<p>My code is below:</p>
<pre class=""lang-py prettyprint-override""><code># Import the required libraries:
import spacy
from spacy.matcher import PhraseMatcher

# Declare string from text extracted from a dataframe.  Please note that there are many errors in the ASR, including words recognized incorrectly such as &quot;mercado&quot; which a mis-translated utterance from the IVR.  

conv_str = &quot;Welcome to companyx, to continue in English, please press one but I contin into mercado. Hello, I am V companyx, virtual assistant to best serve you during our conversation. Please provide your responses after I finished speaking in a few words please tell me what you're calling about. You can say something like I want to change my account information&quot;

# call the matcher
matcher = PhraseMatcher(nlp.vocab, attr=&quot;LOWER&quot;)

# Declare a list of strings to search for in another string

terms = [&quot;Welcome to CompanyX&quot;, &quot;to continue in English, please press one&quot;, &quot;virtual assistant&quot;, &quot;In a few words please tell me what you're calling about&quot;, &quot;CompanyX&quot;]
# the stack overflow interface is incorrectly coloring some of the term strings, but it works in python

# create patterns variable
patterns = [nlp.make_doc(text) for text in terms]
matcher.add(&quot;TerminologyList&quot;, patterns)

doc = nlp(conv_str)
matches = matcher(doc)
for match_id, start, end in matches:
    span = doc[start:end] # span is a list
    terms_not_needed = list(span)
    for item in terms_not_needed:
        conv_str.replace(item, ' ')
</code></pre>
<p>As I mentioned, I get the TypeError as printed out above.  I understand that the str.replace argument requires a string, but I was thinking that by declaring the span a list that I could iterate through that <code>terms_not_needed</code> list for individual string matches.  Any guidance would be very helpful.</p>
",Preprocessing of the text & Tokenization,spacy phrase matcher typeerror trying remove matched phrase trying effectively clean text wa derived automatic speech recognition software using spacy phrase matcher data dirty doe separate speaker trying remove repetitive phrase across data sample using rule based phrase matcher able find target text sample string trying replace space receive type error code mentioned get typeerror printed understand str replace argument requires string wa thinking declaring span list could iterate list individual string match guidance would helpful
Re:sub expected string or byte-like object error,"<pre><code>class REReplacer(object):
   def __init__(self, pattern = R_patterns):
      self.pattern = [(re.compile(regex), repl) for (regex, repl) in patterns]

   def replace(self, text):
      s = text
      for (pattern, repl) in self.pattern:
         s = re.sub(pattern, repl, s)
      return s
</code></pre>
<p>I have got this code which replaces certain words with their replacements. When I call the method replace of class REplacer,</p>
<pre><code>rep=REplacer()
rep.replace(&quot;I like oranges&quot;) 
</code></pre>
<p>it works perfectly fine with strings but gives an error with list or nested lists.</p>
<p>Error- (re.sub) expected string or bytes-like object.</p>
<p>Is there a way (except for converting the list to string) in order to make the function work of list of sentences? Thanks in advance.
Seems like predefined re.sub takes string as argument. Should I split the words of the list?</p>
",Preprocessing of the text & Tokenization,sub expected string byte like object error got code replaces certain word replacement call method replace class replacer work perfectly fine string give error list nested list error sub expected string byte like object way except converting list string order make function work list sentence thanks advance seems like predefined sub take string argument split word list
stopword removing when using the word2vec,"<p>I have been trying word2vec for a while now using the gensim's word2vec library. My question is do I have to remove stopwords from my input text?  Because, based on my initial experimental results, I could see words like 'of', 'when'.. (stopwords) popping up when I do a <code>model.most_similar('someword')</code>..?</p>

<p>But I didn't see anywhere referring that stop word removal is necessary with word2vec? Does the word2vec is supposed to handle stop words even if you don't remove them?</p>

<p>What are the must do pre processing things (like for topic modeling, it's almost a must that you should do stopword removal)?</p>
",Preprocessing of the text & Tokenization,stopword removing using word vec trying word vec using gensim word vec library question remove stopwords input text based initial experimental result could see word like stopwords popping see anywhere referring stop word removal necessary word vec doe word vec supposed handle stop word even remove must pre processing thing like topic modeling almost must stopword removal
Text classification and most informative features,"<p>I'm trying to do some text classification and understand which are the most informative feature the model uses.</p>
<p>I get the Accuracy of the model but when it comes to understand the feature_names it's shows and empty row ''
(accuracy: 0.38405797101449274
Top 10 features used to predict:
Class 1 best:
(0.006863555955628847, '')
Class 2 best:
(0.006863555955628847, '')</p>
<p>If i call the feature_names by itself it shows an empty result but i dont understand where in the function i've made the mistake.</p>
<p>Those are the Def created for cleaning</p>
<pre><code>STOPLIST = stopwords.words('italian')
SYMBOLS = &quot; &quot;.join(string.punctuation).split(&quot; &quot;) + [&quot;-&quot;, &quot;...&quot;, &quot;”&quot;, &quot;”&quot;]
class CleanTextTransformer(TransformerMixin):
    def transform(self, X, **transform_params):
        return [cleanText(text) for text in X]
    def fit(self, X, y=None, **fit_params):
        return self
    def get_params(self, deep=True):
        return {}
    
def cleanText(text):
    text = text.strip().replace(&quot;\n&quot;, &quot; &quot;).replace(&quot;\r&quot;, &quot; &quot;)
    text = text.lower()
    return text
</code></pre>
<p>Those are the Def created for tokenizing</p>
<pre><code>
def tokenizeText(sample):
    tokens = parser(sample)
    lemmas = []
    for tok in tokens:
        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != &quot;-PRON-&quot; else tok.lower_)
    tokens = lemmas
    tokens = [tok for tok in tokens if tok not in STOPLIST]
    tokens = [tok for tok in tokens if tok not in SYMBOLS]
    return tokens
</code></pre>
<p>and the one for the Most Informative features</p>
<pre><code>
def printNMostInformative(vectorizer, clf, N):
    feature_names = vectorizer.get_feature_names()
    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))
    topClass1 = coefs_with_fns[:N]
    topClass2 = coefs_with_fns[:-(N + 1):-1]
    print(&quot;Class 1 best: &quot;)
    for feat in topClass1:
        print(feat)
    print(&quot;Class 2 best: &quot;)
    for feat in topClass2:
        print(feat)
</code></pre>
<p>Then the classic Pipeline with the LinearSVC model (i wanted to try more but i'm trying to understand which one perform better).</p>
<pre><code># data
train1 = train['Column Interested'].tolist()
labelsTrain1 = train['Column Interested'].tolist()
test1 = test['Column Interested'].tolist()
labelsTest1 = test['Column Interested'].tolist()

vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))
clf = LinearSVC()

pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])



# train
pipe.fit(train1, labelsTrain1)

# test
preds = pipe.predict(test1)
print(&quot;accuracy:&quot;, accuracy_score(labelsTest1, preds))
print(&quot;Top 10 features used to predict: &quot;)

printNMostInformative(vectorizer, clf, 10)


</code></pre>
<p>For more usability there is a very similar use of this &quot;NMostinformative on this tutorial which I adapted to my data
<a href=""https://towardsdatascience.com/machine-learning-for-text-classification-using-spacy-in-python-b276b4051a49"" rel=""nofollow noreferrer"">https://towardsdatascience.com/machine-learning-for-text-classification-using-spacy-in-python-b276b4051a49</a></p>
",Preprocessing of the text & Tokenization,text classification informative feature trying text classification understand informative feature model us get accuracy model come understand feature name show empty row accuracy top feature used predict class best class best call feature name show empty result dont understand function made mistake def created cleaning def created tokenizing one informative feature classic pipeline linearsvc model wanted try trying understand one perform better usability similar use nmostinformative tutorial adapted data
BART Tokenizer tokenises same word differently?,"<p>I have noticed that if I tokenize a full text with many sentences, I sometimes get a different number of tokens than if I tokenise each sentence individually and add up the tokens. I have done some debugging and have this small reproducible example to show the issue</p>
<pre><code>from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-cnn')

print(tokenizer.tokenize(&quot;Thames is a river&quot;))
print(tokenizer.tokenize(&quot;We are in London. Thames is a river&quot;))
</code></pre>
<p>I get the following output</p>
<pre><code>['Th', 'ames', 'Ġis', 'Ġa', 'Ġriver']
['We', 'Ġare', 'Ġin', 'ĠLondon', '.', 'ĠThames', 'Ġis', 'Ġa', 'Ġriver']
</code></pre>
<p>I would like to understand why the word Thames has been split into two tokens when it’s at the start of sequence, whereas it’s a single word if it’s not at the start of sequence. I have noticed this behaviour is very frequent and, assuming it’s not a bug, I would like to understand why the BART tokeniser behaves like this.</p>
",Preprocessing of the text & Tokenization,bart tokenizer tokenises word differently noticed tokenize full text many sentence sometimes get different number token tokenise sentence individually add token done debugging small reproducible example show issue get following output would like understand word thames ha split two token start sequence whereas single word start sequence noticed behaviour frequent assuming bug would like understand bart tokeniser behaves like
Customization of Wav2Vec2CTCTokenizer with rules,"<p>my goal is to fine-tune an ASR model, WavLM, that relies on the pretrained tokenizer <code>Wav2Vec2CTCTokenizer</code>.</p>
<p>I want to fine-tune this ASR model with another language and to perform the tokenization according to phonological rules, such as syllable segmentation.</p>
<p>Providing a vocabulary with all the possible syllables (aka my tokens), is it possible to customize the <code>Wav2Vec2CTCTokenizer</code> segmentation so that it will respect syllable segmentation rules?</p>
<p>Example:</p>
<pre><code>Original sentence:
Il tentativo era cosi bello

Segmentation made by Wav2Vec2CTCTokenizer (not respecting syllabification rules):
['il', 'ten', 'tat', 'iv', 'o', 'Er', 'a', 'kos', 'i', 'bEl', 'lo']

Expected segmentation according to syllabification rules:
['il', 'ten', 'ta', 'ti', 'vo', 'E', 'ra', 'ko', 'si', 'bEl', 'lo']
</code></pre>
<p>Basically, I need to state and include some rules in the tokenizer, for example to give priority to tokens with a consonant in the onset position instead of in the coda of the syllable.</p>
<p>Is it possible to insert this kind of rules in the tokenizer?
If so, where can I modify these parameters?</p>
<p>If not, if I train a new tokenizer, will it be ok to implement it in the pre-trained WavLm model that I need to fine-tune?</p>
<p>Thanks in advance!</p>
",Preprocessing of the text & Tokenization,customization wav vec ctctokenizer rule goal fine tune asr model wavlm relies pretrained tokenizer want fine tune asr model another language perform tokenization according phonological rule syllable segmentation providing vocabulary possible syllable aka token possible customize segmentation respect syllable segmentation rule example basically need state include rule tokenizer example give priority token consonant onset position instead coda syllable possible insert kind rule tokenizer modify parameter train new tokenizer ok implement pre trained wavlm model need fine tune thanks advance
Spacy Memory Usage Performance Improvements,"<p>I have tens of thousands of documents, where each doc is about ~150k characters, ~25k white-space bounded tokens, and ~2k unique tokens. I'm using Spacy to pre-process (stopword removal and lemmatization). The preprocessing depends on <code>token.pos_</code> and <code>token.lemma_</code> as shown below.</p>
<p>I learned that I incorrectly implemented Spacy by disabling the <code>tok2vec</code> pipeline component (needed for POS tagging); after fixing that, my memory usage is crazy high. The app hangs then the OOM killer kills my python.</p>
<p>My approach is to feed the docs into <code>nlp.pipe</code> in chunks of 100 and <code>n_process=4</code>. This worked fine until fixing the above bug. The only way the app runs without hanging/OOM killer is to reduce the number of docs I feed into the pipe ~25-50. Reducing <code>n_process</code> to 1 doesn't seem to have an impact. Here's my rough approach:</p>
<pre><code>import spacy
from bs4 import BeautifulSoup
import unidecode
import re

nlp = spacy.load('en_core_web_lg')
nlp.max_length = 5000000
nlp.disable_pipe(&quot;parser&quot;)
nlp.disable_pipe(&quot;ner&quot;)
nlp.enable_pipe(&quot;senter&quot;)

def pre_pre_process(record, synswap=True):
    (doc_id, text) = record

    # partial pre-preprocessing = just strip HTML
    text1 = BeautifulSoup(text, &quot;html.parser&quot;).get_text(separator=&quot; &quot;)

    # full pre-preprocessing = do all the pre-preprocessing
    text2 = &quot; &quot;.join(text1.strip().split())
    text2 = unidecode.unidecode(text2)
    text2 = text2.lower()
    
    return (text2, {'doc_id': doc_id, 'strip_html': text1, 'ppp': 'full-ppp'})


def pre_process_text(doc, convert_num=True, lemmatization=True,
                     punctuations=True, remove_num=True, special_chars=True,
                     stop_words=True, short_char=True, remove_edgar_junk=True):
    fully_processed = []
    edgar_jnk_patt = re.compile('(?is)ex-\d+\.?\d*')  # noqa: W605
    edgar_jnk = []

    for token in doc:
        # (token, token.pos_, token.is_stop, token.is_punct, token.lemma_)
        flag = True  # assume every token should be added to the vocab
        edit = token.text
        # remove stop words
        if stop_words is True and token.is_stop and token.pos_ != 'NUM':
            flag = False
        # remove punctuations
        if punctuations is True and (token.pos_ == 'PUNCT' or token.is_punct) and flag is True:
            flag = False
        # remove special characters
        if special_chars is True and token.pos_ == 'SYM' and flag is True:
            flag = False
        # remove numbers
        if remove_num is True and (token.pos_ == 'NUM' or token.text.isnumeric()) and flag is True:
            flag = False
        # remove short tokens
        if short_char is True and len(token) &lt; 3 and flag is True:
            flag = False
        # convert tokens to base form
        elif lemmatization is True and token.lemma_ != &quot;-PRON-&quot; and flag is True:
            edit = token.lemma_
        # remove edgar junk
        if remove_edgar_junk is True:
            if token.i &lt; 10:
                if token.text.endswith(('.htm', '.html')):
                    flag = False
                    edgar_jnk.append(token.lemma)
                elif edgar_jnk_patt.search(token.lemma_):
                    flag = False
                    edgar_jnk.append(token.lemma)
            if token.lemma in edgar_jnk and flag is True:
                flag = False

        # append tokens edited and not removed to list
        if edit != &quot;&quot; and flag is True:
            fully_processed.append(edit)
    return fully_processed

# In the complete script, `data` is queried from a DB limited by a param, `query_limit = 50`. It continues in a while true loop grabbing `query_limit` records until there aren't any more records to query. 

# For reproducibility, `data` sample here: https://gist.github.com/roablep/09731a9a0996fc82aecedb6fcb7c026a

completed_jobs = []
pipeline_texts = [pre_pre_process(d) for d in data]
for doc, context in nlp.pipe(pipeline_texts, as_tuples=True, n_process=4):
    tokens = pre_process_text(doc)
    completed_jobs.append((context, tokens))
</code></pre>
<p>My questions are:</p>
<ol>
<li>Why is <code>tok2vec</code> eating so much memory?</li>
<li>How can I profile what's happening in <code>nlp.pipe</code>?</li>
<li>Is there a better way to implement this pipeline overall?</li>
<li>Is there a better way to implement the pre-processing? (Is there a built-in Spacy approach or is what I have pretty standard)</li>
</ol>
<p>Related to question 2: Interesting spikiness in memory:
<a href=""https://i.sstatic.net/ZvHQK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZvHQK.png"" alt=""enter image description here"" /></a></p>
",Preprocessing of the text & Tokenization,spacy memory usage performance improvement ten thousand document doc k character k white space bounded token k unique token using spacy pre process stopword removal lemmatization preprocessing depends shown learned incorrectly implemented spacy disabling pipeline component needed po tagging fixing memory usage crazy high app hang oom killer kill python approach feed doc chunk worked fine fixing bug way app run without hanging oom killer reduce number doc feed pipe reducing seem impact rough approach question eating much memory profile happening better way implement pipeline overall better way implement pre processing built spacy approach pretty standard related question interesting spikiness memory
spaCy custom tokenizer to separate word with underscore and also to include the whole word,"<p>After referring to the link: <a href=""https://stackoverflow.com/questions/62500973/how-to-tokenize-word-with-hyphen-in-spacy"">How to tokenize word with hyphen in Spacy</a><br/>
I got to know how to tokenize by separating words containing hyphen/underscore but my requirement is to tokenize by separating it and also to include that whole word.<br/>
For example:<br/>
Input: bs_it<br/>
Output: [&quot;bs&quot;, &quot;it&quot;, &quot;bs_it&quot;]</p>
",Preprocessing of the text & Tokenization,spacy custom tokenizer separate word underscore also include whole word referring link href tokenize word hyphen spacy got know tokenize separating word containing hyphen underscore requirement tokenize separating also include whole word example input b output b b
How to replace a column with text in DataFrame with preprocessed text after NLP,"<p>I'm trying to replace a column in a DataFrame with preprocessed text data.</p>
<p>I have imported an Excel file as pandas dataframe.</p>
<pre><code>df = pd.read_excel (*file path*)
</code></pre>
<p>This file consists of x rows of documents and 12 columns.</p>
<p>I extracted the column 'Text' for NLP.</p>
<pre><code>text_article = (df['Text'])
</code></pre>
<p>I have preprocessed this column (removal of digits, stopwords, tokenization, lemmatization etc.) Resulting in the following variable: text_article['final']</p>
<p>I now want to replace the column (df['Text']) with text_article['final'], but don't know how.</p>
<p>When I export the dataframe, I get the original column 'Text'</p>
<pre><code>df.to_excel('*name*.xlsx', index=False)
</code></pre>
<p>I've tried the following code to replace the column or add the column, but it doesn't seem to work.</p>
<pre><code>df.insert(text_article['final'])
</code></pre>
<p>and</p>
<pre><code>text_article['final'] = df['Text']
</code></pre>
<p>I'm relatively new to Python, so I hope I've clearly formulated my question. Thanks in advance.</p>
",Preprocessing of the text & Tokenization,replace column text dataframe preprocessed text nlp trying replace column dataframe preprocessed text data imported excel file panda dataframe file consists x row document column extracted column text nlp preprocessed column removal digit stopwords tokenization lemmatization etc resulting following variable text article final want replace column df text text article final know export dataframe get original column text tried following code replace column add column seem work relatively new python hope clearly formulated question thanks advance
Bi-gram model to predict text,"<p>I am planning to implement bi-gram model to predict a search text. If a user has frequently searched ""Test search word"" and then if user types ""Test"" I am looking to automatically suggest ""Test search word""</p>

<p>I have the list of data of searched text. I am trying with bi-gram as even if user types ""Tast"" it should still provide ""Test search word"". I am implementing it in Java. I am looking for a library to supply the data that I have and when I pass the user keyed in text, it should provide the prediction. </p>

<p>After research I found below links</p>

<ul>
<li><p><a href=""https://www.javatips.net/api/Solbase-Lucene-master/contrib/analyzers/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java"" rel=""nofollow noreferrer"">https://www.javatips.net/api/Solbase-Lucene-master/contrib/analyzers/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java</a></p></li>
<li><p><a href=""https://opennlp.apache.org/docs/1.8.1/apidocs/opennlp-tools/opennlp/tools/ngram/NGramUtils.html"" rel=""nofollow noreferrer"">https://opennlp.apache.org/docs/1.8.1/apidocs/opennlp-tools/opennlp/tools/ngram/NGramUtils.html</a></p></li>
</ul>

<p>but they are not helping in my case. Are there any Java libraries that suits my purpose?</p>
",Preprocessing of the text & Tokenization,bi gram model predict text planning implement bi gram model predict search text user ha frequently searched test search word user type test looking automatically suggest test search word list data searched text trying bi gram even user type tast still provide test search word implementing java looking library supply data pas user keyed text provide prediction research found link helping case java library suit purpose
deleting a specific line from a dataframe python NLP,"<p>I am trying to preprocess my data for NLP model. I wrote this code to remove numbers, symbols and hyper links. But now I want to delete every line that has a specific instance of the word 'system'. I don't seem to figure how to do that. <code>df</code> is my dataframe and <code>df['Content']</code> is where I have the text I want to delete the line from.</p>
<p>for example the text can be :</p>
<p>&quot;system: hi im the line that is meant to be deleted
Leena: this line must not be deleted
system: hi again im the line that is meant to be deleted &quot;</p>
<p>the output should be :
Leena: this line must not be deleted</p>
<pre><code>def CleaningTXT(df):
    Allchat=list()
    lines=df['Content'].values.tolist()
    for text in lines:
        text=text.lower()
        #remove links
        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        text = pattern.sub('', text)
        #remove session join/leave
        pattern = re.compile('new party join session')
        text = pattern.sub('', text)
        pattern = re.compile('new party leave session')
        text = pattern.sub('', text)
        #remove sympols
        text = re.sub(r&quot;[,.\&quot;!@#$%^&amp;*(){}?/;`~:&lt;&gt;+=-]&quot;, &quot;&quot;, text)
        #seperating words
        tokens = word_tokenize(text)
        table = str.maketrans('', '', string.punctuation)
        stripped = [w.translate(table) for w in tokens]
        #removing numbers
        words = [word for word in stripped if word.isalpha()]
        words = ' '.join(words)
        Allchat.append(words)
    return Allchat
</code></pre>
",Preprocessing of the text & Tokenization,deleting specific line dataframe python nlp trying preprocess data nlp model wrote code remove number symbol hyper link want delete every line ha specific instance word system seem figure dataframe text want delete line example text system hi im line meant deleted leena line must deleted system hi im line meant deleted output leena line must deleted
Trouble with applying a UDF on a column in Pyspark Dataframe,"<p>My goal is to clean the Data in a column in a Pyspark DF. I have written a function for cleaning .</p>
<pre><code>def preprocess(text):
    text = text.lower() 
    text=text.strip()  
    text=re.compile('&lt;.*?&gt;').sub('', text) 
    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  
    text = re.sub('\s+', ' ', text)  
    text = re.sub(r'\[[0-9]*\]',' ',text) 
    text=re.sub(r'[^\w\s]', '', text.lower().strip())
    text = re.sub(r'\d',' ',text) 
    text = re.sub(r'\s+',' ',text) 
    return text

 

#LEMMATIZATION
# Initialize the lemmatizer
wl = WordNetLemmatizer()

stop_words = set(stopwords.words('english'))
def remove_stopwords(text):
    text = [i for i in text.split() if not i in stop_words]
    return text
 
# This is a helper function to map NTLK position tags
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN
# Tokenize the sentence
def lemmatizer(string):
    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags
    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token
    return &quot; &quot;.join(a)

#Final Function
def finalpreprocess(string):
    return lemmatizer(' '.join(remove_stopwords(preprocess(string))))
</code></pre>
<p>The functions seems to work fine when I test it . When I do</p>
<pre><code>text = 'Ram and Bheem are buddies. They (both) like &lt;b&gt;running&lt;/b&gt;. They got better at it over the weekend'

print(finalpreprocess(text))
</code></pre>
<p>I see the exact result I want.</p>
<pre><code>ram bheem buddy like run get well weekend
</code></pre>
<p>How ever when I try to apply this function finalpreprocess() to a column in pyspark dataframe . I am getting errors.
Here is what I did.</p>
<p>udf_txt_clean = udf(lambda x: finalpreprocess(x),StringType())
df.withColumn(&quot;cleaned_text&quot;,lem(col(&quot;reason&quot;))).select(&quot;reason&quot;,&quot;cleaned_text&quot;).show(10,False)</p>
<p>Then I am getting the error :</p>
<pre><code>Traceback (most recent call last):
  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 473, in dumps
    return cloudpickle.dumps(obj, pickle_protocol)
  File &quot;/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py&quot;, line 73, in dumps
    cp.dump(obj)
  File &quot;/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py&quot;, line 563, in dump
    return Pickler.dump(self, obj)
TypeError: cannot pickle '_thread.RLock' object
PicklingError: Could not serialize object: TypeError: cannot pickle '_thread.RLock' object
</code></pre>
<p>So far here is what I did. In my finalpreprocess(), I am using three different functions preprocess(),remove_stopwords(), lemmatizer() . I changed my udf_txt_clean accordingly . Like</p>
<pre><code>udf_txt_clean = udf(lambda x: preprocess(x),StringType())
udf_txt_clean = udf(lambda x: remove_stopwords(x),StringType())
</code></pre>
<p>These two run fine But -</p>
<p>udf_txt_clean = udf(lambda x: lemmatizer (x),StringType())</p>
<p>is the one that is giving me the error. I am not able to understand why this function is giving the error but not the other two. From my limited understating I see that its having trouble trying to pickle this function but I am not able to understand why its trying to pickle this in the first place or if there is a work around for it.</p>
",Preprocessing of the text & Tokenization,trouble applying udf column pyspark dataframe goal clean data column pyspark df written function cleaning function seems work fine test see exact result want ever try apply function finalpreprocess column pyspark dataframe getting error udf txt clean udf lambda x finalpreprocess x stringtype df withcolumn cleaned text lem col reason select reason cleaned text show false getting error far finalpreprocess using three different function preprocess remove stopwords lemmatizer changed udf txt clean accordingly like two run fine udf txt clean udf lambda x lemmatizer x stringtype one giving error able understand function giving error two limited understating see trouble trying pickle function able understand trying pickle first place work around
Add rules to Spacy lemmatization,"<p>I am using Spacy lemmatization for preprocessing texts.</p>
<pre><code>doc = 'ups'
for i in nlp(doc):
print(i.lemma_)
&gt;&gt; up
</code></pre>
<p>I understand why spacy remove the 's', but it is important for me that in that case, it won't do it. Is there a way to add specific rules to spacy or do I have to use If statements outside the process (which is something I don't want to do )</p>
",Preprocessing of the text & Tokenization,add rule spacy lemmatization using spacy lemmatization preprocessing text understand spacy remove important case way add specific rule spacy use statement outside process something want
Writing a custom Rasa component to preprocess text in pipeline,"<p>We are trying to write a custom component for our rasa pipeline (using <a href=""https://rasa.com"" rel=""nofollow noreferrer"">rasa OS</a>), but can't seem to get it to work. This is what we currently have:</p>
<pre><code>from rasa.nlu.components import Component

class PreProcessing(Component):
    &quot;&quot;&quot;A custom sentiment analysis component&quot;&quot;&quot;
    name = &quot;preprocessing&quot;

    provides = []
    requires = []
    defaults = {}
    language_list = []

    def __init__(self, component_config=None):
        super(PreProcessing, self).__init__(component_config)

    def process(self, message, **kwargs):
        &quot;&quot;&quot;Retrieve the tokens of the new message, pass it to the classifier
            and append prediction results to the message class.&quot;&quot;&quot;
        
        return message.lower() # here, process text somehow. For example lowercase on input
</code></pre>
<p>and the <code>config.yml</code> file looks like:</p>
<pre><code>language: &quot;../../language_models/nb_core_news_lg&quot;
recipe: default.v1

# Read the following docs on how to fine-tune a model using config.
# https://rasa.com/docs/rasa/tuning-your-model#component-lifecycle

pipeline:

  - name: &quot;preprocessing.PreProcessing&quot; 

  ## Required pipeline components ##
  - name: SpacyNLP
    # note: this path is relative to ./live-bots/bot_name, where it's imported.
    model: &quot;../../language_models/nb_core_news_lg&quot;
    case_sensitive: False
    ...
</code></pre>
<p>This however does not seem to be working. The preprocessing of the text is not being picked up by the component SpacyNLP. Our custom component does not need to be trained, but the input text just needs to be processed. Any tips?</p>
",Preprocessing of the text & Tokenization,writing custom rasa component preprocess text pipeline trying write custom component rasa pipeline using rasa seem get work currently file look like however doe seem working preprocessing text picked component spacynlp custom component doe need trained input text need processed tip
The idea behind the regular expression r&#39;[^\d]+&#39;,"<p>Suppose we have</p>
<pre><code>phrase = &quot;there are 3 numbers 34 inside 5 this sentence.&quot;
</code></pre>
<p>I don't understand why <code>re.findall(r'[^\d]+',phrase)</code> returns the following:</p>
<pre><code>['there are ', ' numbers ', ' inside ', ' this sentence.']
</code></pre>
<p>I know that <code>re.findall([^\d],phrase)</code> returns the list of all characters except for digits and that <code>+</code>, in general, is used to find patterns with one or more occurrences, but I still don't understand how that combination returns that particular list. What is the thing whose one or more occurrences are being asked to return under <code>re.findall(r'[^\d]+',phrase)</code>?</p>
",Preprocessing of the text & Tokenization,idea behind regular expression r suppose understand return following know return list character except digit general used find pattern one occurrence still understand combination return particular list thing whose one occurrence asked return
Mismatch in the count of stop-words in `Defaults.stop_words` and the ones derived from `nlp.vocab`?,"<p>Suppose we have <code>nlp = spacy.load('en_core_web_sm')</code>. When typing in <code>len(nlp.Defaults.stop_words)</code>, it returns <code>326</code>, but when I run the following code (essentially counting the stopwords of the vocabulary), I get <code>111</code>:</p>
<pre><code>i=0
for word in nlp.vocab:
    if word.is_stop:
        print(word.text)
        i+=1
print(i)
</code></pre>
<p>Given that (presumably) both <code>Defaults.stop_words</code> and <code>nlp.vocab</code> work with the same underlying vocabulary loaded through <code>nlp = spacy.load('en_core_web_sm')</code>, I don't understand why the number mismatch. Any thoughts?</p>
",Preprocessing of the text & Tokenization,mismatch count stop word one derived suppose typing return run following code essentially counting stopwords vocabulary get given presumably work underlying vocabulary loaded understand number mismatch thought
Slow and Fast tokenizer gives different outputs(sentencepiece tokenization),"<p>When i use T5TokenizerFast(Tokenizer of T5 architecture), the output is expected as follows:</p>
<pre><code>['▁', '&lt;/s&gt;', '▁Hello', '▁', '&lt;sep&gt;', '&lt;/s&gt;']
</code></pre>
<p>But when i use the normal tokenizer, it starts to split special token &quot;/s&gt;&quot; as follows:</p>
<pre><code>['▁&lt;/', 's', '&gt;', '▁Hello', '&lt;sep&gt;', '&lt;/s&gt;']
</code></pre>
<p><strong>And this is print of not fast tokenizer</strong>:</p>
<pre><code>PreTrainedTokenizer(name_or_path='', vocab_size=60000, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'pad_token': '&lt;pad&gt;'})
</code></pre>
<p><strong>For fast</strong> :</p>
<pre><code>PreTrainedTokenizerFast(name_or_path='', vocab_size=60000, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'pad_token': '&lt;pad&gt;'})
</code></pre>
<p>Code that i am using to produce these outputs:</p>
<pre><code>tokenizer = T5TokenizerFast('new_sp.model', extra_ids=0)
tokenizer.add_tokens(['&lt;sep&gt;'])
print(tokenizer.convert_ids_to_tokens(tokenizer.encode(&quot;&lt;/s&gt; Hello &lt;sep&gt;&quot;)))
</code></pre>
<p>I would appreciate any help. Thanks.</p>
",Preprocessing of the text & Tokenization,slow fast tokenizer give different output sentencepiece tokenization use tokenizerfast tokenizer architecture output expected follows use normal tokenizer start split special token follows print fast tokenizer fast code using produce output would appreciate help thanks
turning &#39;dict&#39; output result into one string in python,"<p>I'm using <a href=""https://farasa.qcri.org/lemmatization/"" rel=""nofollow noreferrer"">Farasa</a> library and I'm using its lemmatization module that I want to add to my nlp text cleaning code
I've used this code</p>
<pre><code>import json
import requests

url = 'https://farasa.qcri.org/webapi/lemmatization/'
text = 'ينظم معهد الشارقة للفنون معرضاً فنياً تحت عنوان باقة الفن، وذلك عند الساعة السابعة من مساء اليوم في مقر المعهد في منطقة الفنون في حي الشويهين في الشارقة، وتتلاقى في المعرض إبداعات 62 طالباً من المنتسبين للدراسة في المعهد في كافة التخصصات الفنية .' 
api_key = &quot;####&quot;
payload = {'text': text, 'api_key': api_key}
data = requests.post(url, data=payload)
result = json.loads(data.text)

print(result) 
</code></pre>
<p>and I got this result :</p>
<pre><code>{'text': ['نظم', 'معهد', 'شارقة', 'فن', 'معرض', 'فني', 'تحت', 'عنوان', 'باقة', 'فن', '،', 'ذلك', 'عند', 'ساعة', 'سابع', 'من', 'مساء', 'يوم', 'في', 'مقر', 'معهد', 'في', 'منطقة', 'فن', 'في', 'حي', 'شويه', 'في', 'شارقة', '،', 'تلاقى', 'في', 'معرض', 'إبداع', '62', 'طالب', 'من', 'منتسب', 'دراسة', 'في', 'معهد', 'في', 'كافة', 'تخصص', 'فني', '.']}
</code></pre>
<p>I don't know the output type so I added <code>print(type(result))</code>  and I got <code>&lt;class 'dict'&gt;</code>
I tried using str() to convert it or .join() but I couldn't get any results</p>
<p>I want to end up with a clean string without those commas and {text} like this</p>
<pre><code>نظم معهد شارقة فن معرض فني تحت عنوان باقة فن ، ذلك عند ساعة سابع من مساء يوم في مقر معهد في منطقة فن في حي شويه في شارقة ، تلاقى في معرض إبداع 62 طالب من منتسب دراسة في معهد في كافة تخصص فني
</code></pre>
<p>and I don't know how.</p>
",Preprocessing of the text & Tokenization,turning dict output result one string python using farasa library using lemmatization module want add nlp text cleaning code used code got result know output type added got tried using str convert join get result want end clean string without comma text like know
How to create a list of tokenized words from dataframe column using spaCy?,"<p>I'm trying to apply <code>spaCy</code>s tokenizer on dataframe column to get a new column containing list of tokens.
Assume we have the following dataframe:</p>
<pre><code>import pandas as pd
details = {
    'Text_id' : [23, 21, 22, 21],
    'Text' : ['All roads lead to Rome', 
              'All work and no play makes Jack a dull buy', 
              'Any port in a storm', 
              'Avoid a questioner, for he is also a tattler'],
}
  
# creating a Dataframe object 
example_df = pd.DataFrame(details)
</code></pre>
<p>The code below aims to tokenize <code>Text</code> column:</p>
<pre><code>import spacy

nlp = spacy.load(&quot;en_core_web_sm&quot;)

example_df[&quot;tokens&quot;] = example_df[&quot;Text&quot;].apply(lambda x: nlp.tokenizer(x))

example_df
</code></pre>
<p>The results looks like:</p>
<p><a href=""https://i.sstatic.net/0pFqs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0pFqs.png"" alt=""enter image description here"" /></a></p>
<p>Now, we have a new column <code>tokens</code>, which returns <code>doc</code> object for each sentence.</p>
<p>How could we change the code to get a <strong>python list of tokenized words</strong>?</p>
<p>I've tried the following line:</p>
<pre><code>example_df[&quot;tokens&quot;] = example_df[&quot;Text&quot;].apply(token.text for token in (lambda x: nlp.tokenizer(x)))

</code></pre>
<p>but I have the following error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
/tmp/ipykernel_33/3712416053.py in &lt;module&gt;
     14 nlp = spacy.load(&quot;en_core_web_sm&quot;)
     15 
---&gt; 16 example_df[&quot;tokens&quot;] = example_df[&quot;Text&quot;].apply(token.text for token in (lambda x: nlp.tokenizer(x)))
     17 
     18 example_df

TypeError: 'function' object is not iterable
</code></pre>
<p>Thank you in advance!</p>
<p><strong>UPDATE</strong>: I have a solution, but I still have another problem. I want to count words using built-in class <code>Counter</code>, which takes a list as input and can be incrementally updated with a list of tokens of other document using <code>update</code> function. The below code should returns the number of occurences for each word in dataframe:</p>
<pre><code>from collections import Counter
# instantiate counter object
counter_df = Counter()

# call update function of the counter object in update the counts
example_df[&quot;tokens&quot;].map(counter_df.update)
</code></pre>
<p>However, the output is:</p>
<pre><code>0    None
1    None
2    None
3    None
Name: tokens, dtype: object
</code></pre>
<p>The expected output must be like:</p>
<pre><code>Counter({'All': 2, 'roads': 1, 'lead': 1, 'to': 1, 'Rome': 1, 'work': 1, 'and': 1, 'no': 1, 'play': 1, 'makes': 1, 'a': 4, 'dull':1, 'buy':1, 'Any':1, 'port':1, 'in': 1, 'storm':1, 'Avoid':1, 'questioner':1, ',':1, 'for':1, 'he':1})
</code></pre>
<p>Thank you again :)</p>
",Preprocessing of the text & Tokenization,create list tokenized word dataframe column using spacy trying apply tokenizer dataframe column get new column containing list token assume following dataframe code aim tokenize column result look like new column return object sentence could change code get python list tokenized word tried following line following error thank advance update solution still another problem want count word using built class take list input incrementally updated list token document using function code return number occurences word dataframe however output expected output must like thank
how to remove custom words from word cloud in jupyter notebook,"<pre><code>wordcloud = WordCloud(background_color=&quot;white&quot;,width=1600,height=800).generate(' '.join(df1['text'].tolist()))
plt.figure(figsize=(20,10), facecolor='k')
plt.imshow(wordcloud)
</code></pre>
<p>The word cloud that gets shown has data that I don't think is relevant, such as user, need, anyone, issue, trying, and some other words. I have already removed stop words, but how can I remove a custom list of words from the word cloud? This is a word cloud from data I scraped from slack, and I want it to representative of questions people are asking in the channel that are more related to things like email, server, outlook, duo etc.
<a href=""https://i.sstatic.net/pExEp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pExEp.png"" alt=""enter image description here"" /></a></p>
",Preprocessing of the text & Tokenization,remove custom word word cloud jupyter notebook word cloud get shown ha data think relevant user need anyone issue trying word already removed stop word remove custom list word word cloud word cloud data scraped slack want representative question people asking channel related thing like email server outlook duo etc
sent_tokenize returns a list with only one item,"<p>I'm doing project for a university course on Python for linguists.
For my final project I'm doing all sorts of treatment with Hamlet in a txt file using NLTK.</p>
<p>The problem is, it comes to a point where sent_tokenize returns all the text as a single item in a list.</p>
<p>The text I'm working with looks like this (but is the entire play):</p>
<p>FRANCISCO<br />
For this relief much thanks   tis bitter cold<br />
And I am sick at heart</p>
<p>BERNARDO<br />
Have you had quiet guard</p>
<p>FRANCISCO<br />
Not a mouse stirring</p>
<p>BERNARDO<br />
Well  good night<br />
If you do meet Horatio and Marcellus<br />
The rivals of my watch  bid them make haste</p>
<p>FRANCISCO<br />
I think I hear them  Stand  ho  Who s there</p>
<p>What I tried to do is:</p>
<pre><code>hamlet_sentence_tokens = nltk.sent_tokenize(hamlet)
number_of_sentences = len(hamlet_sentence_tokens)
strnumber_of_sentences = str(number_of_sentences)
print(number_of_sentences)
</code></pre>
<p>But no matter what I do, the output is always 1.</p>
<p>Could anybody help me?</p>
",Preprocessing of the text & Tokenization,sent tokenize return list one item project university course python linguist final project sort treatment hamlet txt file using nltk problem come point sent tokenize return text single item list text working look like entire play francisco relief much thanks ti bitter cold sick heart bernardo quiet guard francisco mouse stirring bernardo well good night meet horatio marcellus rival watch make haste francisco think hear stand ho tried matter output always could anybody help
How to replace compound words in a string using a dictionary?,"<p>I have a dictionary whose key:value pairs correspond to compound words and the expression i want to replace them for in a text. For example let's say:</p>
<pre><code>terms_dict = {'digi conso': 'digi conso', 'digi': 'digi conso', 'digiconso': 'digi conso', '3xcb': '3xcb', '3x cb': '3xcb', 'legal entity identifier': 'legal entity identifier'}
</code></pre>
<p><strong>My goal is to create a function replace_terms(text, dict) that takes a text and a dictionary like this one as parameters, and returns the text after replacing the compound words.</strong></p>
<p>For instance, this script:</p>
<pre><code>test_text = &quot;i want a digi conso loan for digiconso&quot; 

print(replace_terms(test_text, terms_dict))
</code></pre>
<p>Should return:</p>
<pre><code>&quot;i want a digi conso loan for digi conso&quot;
</code></pre>
<hr />
<p>I have tried using .replace() but for some reasons it doesn't work properly, probably because the terms to replace are composed of multiple words.</p>
<p>I also tried this:</p>
<pre><code>def replace_terms(text, terms_dict):
    if len(terms_dict) &gt; 0:
        words_in = [k for k in terms_dict.keys() if k in text]  # ex: words_in = [digi conso, digi, digiconso]
        if len(words_in) &gt; 0:
            for w in words_in:
                pattern = r&quot;\b&quot; + w + r&quot;\b&quot;
                text = re.sub(pattern, terms_dict[w], text)

    return text
</code></pre>
<p>But when applied to my text, this function returns: <em>&quot;i want a digi conso conso loan for digi conso&quot;</em>, the word <em>conso</em> get's doubled and I can see why (because the words_in list is created by going through the dictionary keys, and the text is not altered when one key is appended to the list).</p>
<p>Is there an efficient way to do this?</p>
<p>Thanks a lot!</p>
",Preprocessing of the text & Tokenization,replace compound word string using dictionary dictionary whose key value pair correspond compound word expression want replace text example let say goal create function replace term text dict take text dictionary like one parameter return text replacing compound word instance script return tried using replace reason work properly probably term replace composed multiple word also tried applied text function return want digi conso conso loan digi conso word conso get doubled see word list created going dictionary key text altered one key appended list efficient way thanks lot
How do I use tf serving with transforming methods?,"<p>I have a problem. I have a free text, for example: &quot;Today the weather is good&quot; and this free text I would like to predict which class it is. For this I would like to use <code>tf serving</code>. I don't put free text directly into the model, but I first clean the sentence and then connect it to a vector.
However, how can I incorporate these steps within the <code>tf serving</code> process. My <code>x_ready</code> that goes into <code>model.predict(x_ready)</code> looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>[[
  0 0 0 0 
  0 0 0 0 
  1 5 987 54
]]
</code></pre>
<p>How can I call my methods and then pass <code>tf serving</code> my <code>x</code>?
In addition, I would like to get the probability. How can I get the class and probability with <code>tf serving</code>?</p>
<p>That is how I predict a single class</p>
<pre class=""lang-py prettyprint-override""><code>def predict_single(x):    
    # lower the text
    x = x.lower()
    x = remove_URL(x)
    x = remove_punct(x)
    x = remove_stopwords(x)
    
    x = tokenizer.texts_to_sequences([x])
    # pad
    x_ready = pad_sequences(x, maxlen=maxlen)
    print(x)
    pred = model.predict(x_ready)
    print(&quot;Probability&quot;, pred.max(axis=1))
    pred = pred.argmax(axis=1)
    pred = le.classes_[pred]
    
    return pred[0]
    
    # Clean the text
    return x
</code></pre>
<pre class=""lang-py prettyprint-override""><code>[OUT]
Probability [0.7528353]
Class 3
</code></pre>
<hr />
<p>How I installed <code>tf-serving</code></p>
<pre class=""lang-bash prettyprint-override""><code>docker pull tensorflow/serving

docker run -it -v C:\Code\tensorflow\demo:/demo-p 8601:8601 --entrypoint /bin/bash tensorflow/serving

tensorflow_model_server --rest_api_port=8601 --model_name=model --model_base_path=/demo/models/
</code></pre>
<p>How I saved the model</p>
<pre><code>model.fit(...)
model.save(&quot;models/1&quot;)
model.save(&quot;models/2&quot;)
</code></pre>
",Preprocessing of the text & Tokenization,use tf serving transforming method problem free text example today weather good free text would like predict class would like use put free text directly model first clean sentence connect vector however incorporate step within process go look like call method pas addition would like get probability get class probability predict single class installed saved model
Error in fit_transform while finding tf-idf in Python,"<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
mylist = [
    'a a b c',
    'a c c c d e f',
    'a c d d d',
    'a d f',
]
df = pd.DataFrame({&quot;texts&quot;: mylist})
tfidf_vectorizer = TfidfVectorizer(ngram_range=[1, 1])
tfidf_separate = tfidf_vectorizer.fit_transform(df[&quot;texts&quot;])
</code></pre>
<p>I am trying to find tf-idf value for “d” in line 3. But, it is showing me empty vocabulary error &quot;ValueError: empty vocabulary; perhaps the documents only contain stop words&quot;.</p>
<p>Any advice on how to resolve the error would be appreciated!</p>
",Preprocessing of the text & Tokenization,error fit transform finding tf idf python trying find tf idf value line showing empty vocabulary error valueerror empty vocabulary perhaps document contain stop word advice resolve error would appreciated
Using &#39;isin&#39; in python for three filters,"<p>I have the following dataframe</p>
<pre><code># Import pandas library
import pandas as pd
import numpy as np

# initialize list elements
data = ['george',
        'instagram',
        'nick',
        'basketball',
        'tennis']
  
# Create the pandas DataFrame with column name is provided explicitly
df = pd.DataFrame(data, columns=['Unique Words'])
  
# print dataframe.
df
</code></pre>
<p>and I want to create a new column based on the following two lists that looks like this</p>
<pre><code>key_words = [&quot;football&quot;, &quot;basketball&quot;, &quot;tennis&quot;]
usernames = [&quot;instagram&quot;, &quot;facebook&quot;, &quot;snapchat&quot;]

Label
-----
0
2
0
1
1
</code></pre>
<p>So the words that are in the list key_words take the label 1, in the list usernames take the label 2 and all the other the label 0.</p>
<p>Thank you so much for your time and help!</p>
",Preprocessing of the text & Tokenization,using isin python three filter following dataframe want create new column based following two list look like word list key word take label list usernames take label label thank much time help
How to perform stemming and put back the words in the orginal review format?,"<p>I have a dataset with one column being <code>full_text</code> that contains review text from an online website. I wanted to clean these reviews, by removing stop words and stemming and putting them back to their original format (having all stemmed words forming a sentence, i.e.: one row per review instead of having 1 stemmed word per row.)</p>
<p>I am attempting the following:</p>
<pre><code>sw &lt;- stop_words %&gt;% filter(lexicon == &quot;SMART&quot;)

for (j in 1:nrow(reviews_df)) {

  nostopwords &lt;- reviews_df[j,] %&gt;% unnest_tokens(word, full_text) %&gt;%
                  anti_join(sw, by = &quot;word&quot;)
  stemmed &lt;- wordStem(nostopwords[ , &quot;word&quot;], language = &quot;porter&quot;)
  
reviews_df[j, &quot;stemmed_Description&quot;] &lt;- paste(stemmed, collapse = &quot; &quot;)

}
</code></pre>
<p>However, this new column <code>stemmed_Description</code> does not look how I wanted. It didn't perform stemming and also it is not in &quot;sentence&quot; style but rather as a vector of strings <code>c(&quot;word1&quot;, &quot;word2&quot;, &quot;word3&quot;)</code>.</p>
<p>How can I achieve a result of the style: &quot;stemmedword1 stemmedword2 stemmedword3&quot; ?</p>
<p>Current output:</p>
<pre><code>full_text
1 pseudoindependence no one looking over your shoulder and youre free to use your own judgement to problem solve. they sometimes expect more than what a person can give. dont overwork yourself. the packages aint going no where!
stemmed_Description
1 c(&quot;pseudoindependence&quot;, &quot;shoulder&quot;, &quot;youre&quot;, &quot;free&quot;, &quot;judgement&quot;, &quot;problem&quot;, &quot;solve&quot;, &quot;expect&quot;, &quot;person&quot;, &quot;give&quot;, &quot;dont&quot;, &quot;overwork&quot;, &quot;packages&quot;, &quot;ain't&quot;)
</code></pre>
",Preprocessing of the text & Tokenization,perform stemming put back word orginal review format dataset one column contains review text online website wanted clean review removing stop word stemming putting back original format stemmed word forming sentence e one row per review instead stemmed word per row attempting following however new column doe look wanted perform stemming also sentence style rather vector string achieve result style stemmedword stemmedword stemmedword current output
Is it always necessary to either stem/lemmatize words when working with TF-IDF?,"<p>I'm using TF-IDF along with cosine similarity in order to compute document similarity. I was wondering if it's always necessary to stem/lemmatize the words in the document. Are there times where based on the task, it's better not to stem/lemmatize?</p>
",Preprocessing of the text & Tokenization,always necessary either stem lemmatize word working tf idf using tf idf along cosine similarity order compute document similarity wa wondering always necessary stem lemmatize word document time based task better stem lemmatize
Is it against privacy of clients if I have a global tokenizer in Federated Learning (TFF)?,"<p>I am currently stuck in a dead end. I am trying to make an image caption generator from a federated approach. My initial idea was to have a different tokenizer for each client. That poses these issues however:</p>
<ol>
<li><p>Every client will have a different sized vocabulary, and thus a
different shape of y, which will cause issues with the global model
configuration.</p>
</li>
<li><p>To counter the above issue, I could make size of y in each client
equivalent to the largest size across all clients, and fill the
extra columns in each client with 0. <strong>Example:</strong> [0,1,1,1] mapped to a size
of 6 would become [0,1,1,1,0,0]</p>
</li>
<li><p>This brings me to the last possible flaw, which is that the same
words in different clients will be having different indices. A word
&quot;rock&quot; in client 1 might have an index of 6, while the same can have
an index of 9 in another client. While training the global model, it
will cause issues since the model is trying to learn different label
indices for the same word, which will impact the accuracy?</p>
</li>
</ol>
<p><strong>This brings me to the final question</strong>: Is it against the idea of Federated Learning to tokenize all the words of all the training clients in a single tokenizer?</p>
",Preprocessing of the text & Tokenization,privacy client global tokenizer federated learning tff currently stuck dead end trying make image caption generator federated approach initial idea wa different tokenizer client pose issue however every client different sized vocabulary thus different shape cause issue global model configuration counter issue could make size client equivalent largest size across client fill extra column client example mapped size would become brings last possible flaw word different client different index word rock client might index index another client training global model cause issue since model trying learn different label index word impact accuracy brings final question idea federated learning tokenize word training client single tokenizer
Struggling with removing stop words using nltk,"<p>I'm trying to remove the stop words from &quot;I don't like ice cream.&quot; I have defined:</p>
<p><code>stop_words = set(nltk.corpus.stopwords.words('english'))</code></p>
<p>and the function</p>
<pre><code>def stop_word_remover(text):
    return [word for word in text if word.lower() not in stop_words]
</code></pre>
<p>But when I apply the function to the string in question, I get this list:</p>
<pre><code>[' ', 'n', '’', ' ', 'l', 'k', 'e', ' ', 'c', 'e', ' ', 'c', 'r', 'e', '.']
</code></pre>
<p>which, when joining the strings together as in <code>''.join(stop_word_remover(&quot;I don’t like ice cream.&quot;))</code>, I get</p>
<p><code>' n’ lke ce cre.'</code></p>
<p>which is not what I was expecting.</p>
<p>Any tips on where have I gone wrong?</p>
",Preprocessing of the text & Tokenization,struggling removing stop word using nltk trying remove stop word like ice cream defined function apply function string question get list joining string together get wa expecting tip gone wrong
Regular expression to recognize digits written as words in Python?,"<p>It is easy to recognize numbers as digits or integers from the text but not when numbers are written as words in natural language text.</p>
<p>For recognizing the digits using ReGeX one can just the following regular expression.</p>
<pre><code>digits_recognize = r'[0-9]+'
</code></pre>
<p>How can one develop a pattern to recognize digits written as numbers?</p>
",Preprocessing of the text & Tokenization,regular expression recognize digit written word python easy recognize number digit integer text number written word natural language text recognizing digit using regex one following regular expression one develop pattern recognize digit written number
nltk PunktSentenceTokenizer: tokenize sentences without whitespace in between,"<p>Is it possible to make the <a href=""https://www.nltk.org/"" rel=""nofollow noreferrer"">NLTK</a> <a href=""https://www.nltk.org/api/nltk.tokenize.punkt.html#nltk.tokenize.punkt.PunktSentenceTokenizer"" rel=""nofollow noreferrer"">PunktSentenceTokenizer</a> to split sentences that do not have whitespace between each other?</p>
<pre class=""lang-py prettyprint-override""><code>from nltk.tokenize.punkt import PunktSentenceTokenizer

sent_tokenizer = PunktSentenceTokenizer()
print(sent_tokenizer.tokenize('Sky is blue.Metal is black.'))

'''
Output:

['Sky is blue.Metal is black.']
'''
</code></pre>
",Preprocessing of the text & Tokenization,nltk punktsentencetokenizer tokenize sentence without whitespace possible make nltk punktsentencetokenizer split sentence whitespace
Python Polyglot: How to prevent hyphens separating words which belong together,"<p>I'm trying to clean up sentences in order to create better word clouds, and I'm having an issue with hyphens splitting up words which belong together.</p>

<p>An extreme case is the following where I am dropping all numbers. <code>2-Mics</code> should be found in the image instead of just <code>Mics</code>:</p>

<pre><code>  ""text"": ""ReSpeaker 2-Mics Pi HAT - Seeed Wiki"",
  ""lang"": ""English"",
  ""confidence"": 97.0,
  ""tags"": [
    [
      ""Mics"",
      ""NUM""
    ],
    [
      ""Pi"",
      ""NOUN""
    ],
    [
      ""HAT"",
      ""PROPN""
    ],
    [
      ""Seeed"",
      ""NUM""
    ],
    [
      ""Wiki"",
      ""NOUN""
    ]
  ]
},
</code></pre>

<p>or <code>K2-18b</code> would also be more meaningful than <code>K2</code> and someplace else in the word cloud <code>18b</code>.</p>

<pre><code>{
  ""text"": ""Supererde: Forscher finden erstmals Wasser auf K2-18b - SPIEGEL ONLINE"",
  ""lang"": ""German"",
  ""confidence"": 98.0,
  ""tags"": [
    [
      ""Supererde"",
      ""PROPN""
    ],
    [
      ""Forscher"",
      ""NOUN""
    ],
    [
      ""finden"",
      ""VERB""
    ],
    [
      ""Wasser"",
      ""NOUN""
    ],
    [
      ""K2"",
      ""PROPN""
    ],
    [
      ""18b"",
      ""PROPN""
    ],
    [
      ""SPIEGEL"",
      ""PROPN""
    ],
    [
      ""ONLINE"",
      ""PROPN""
    ]
  ]
},
</code></pre>

<p>The dashes can be removed, that is fully ok. For example the one between <code>K2-18b</code> and <code>SPIEGEL</code> in the segment <code>K2-18b - SPIEGEL</code>.</p>

<p>Here's another case, where respecting the hyphens would be meaningful:</p>

<pre><code>{
  ""text"": ""docker-spacy-alpine/Dockerfile at master \u00b7 cluttered-code/docker-spacy-alpine"",
  ""lang"": ""English"",
  ""confidence"": 98.0,
  ""tags"": [
    [
      ""docker"",
      ""NUM""
    ],
    [
      ""spacy"",
      ""NUM""
    ],
    [
      ""Dockerfile"",
      ""NUM""
    ],
    [
      ""master"",
      ""NOUN""
    ],
    [
      ""cluttered"",
      ""VERB""
    ],
    [
      ""code"",
      ""NOUN""
    ],
    [
      ""docker"",
      ""NUM""
    ],
    [
      ""spacy"",
      ""NUM""
    ],
    [
      ""alpine"",
      ""ADJ""
    ]
  ]
},
</code></pre>

<p>since this would then end up as <code>docker-spacy-alpine</code> <code>Dockerfile</code> <code>cluttered-code</code> in the image, with <code>docker-spacy-alpine</code> being more promintent.</p>

<p>This is the code I'm using</p>

<pre><code>from polyglot.text import Text

#...

for item in result:
  if 'title' in item:
    text = Text(item['title'])
    if text.language.code in ['en', 'de']:
      tags = []
      try:
        unfiltered_tags = text.pos_tags
        for tag in unfiltered_tags:
          try:
            x = float(tag[0])
          except:
            if tag[1] in ['NUM', 'ADJ', 'VERB', 'PROPN', 'INTJ', 'NOUN']:
              tags.append(tag)
      except:
        traceback.print_exc()
      titles.append({
        'text': item['title'],
        'lang': text.language.code,
        'confidence': text.language.confidence,
        'tags': tags,
      })
</code></pre>

<p>Is there a way to tune <code>polyglot</code> so it does not do this splitting, or do I need to do some manual post-processing on the sentences?</p>
",Preprocessing of the text & Tokenization,python polyglot prevent hyphen separating word belong together trying clean sentence order create better word cloud issue hyphen splitting word belong together extreme case following dropping number found image instead would also meaningful someplace else word cloud dash removed fully ok example one segment another case respecting hyphen would meaningful since would end image promintent code using way tune doe splitting need manual post processing sentence
ALBERT: first word associations,"<p>I have a list of words (e.g., &quot;apple,&quot; &quot;banana,&quot; &quot;mango&quot;) and would like to use ALBERT (<a href=""https://huggingface.co/albert-base-v2"" rel=""nofollow noreferrer"">https://huggingface.co/albert-base-v2</a>) to identify the 10 words that are most strongly associated with each word in my list. In simple terms: &quot;Hey ALBERT, what's the first word that comes to your mind when hearing apple/banana/mango?&quot;</p>
<p>My first idea was using a prompt like &quot;apple is related to [MASK].&quot; but some top predictions are quite weird or not proper words like 'evalle'.</p>
<p>My second idea was to use a k-nearest neighbors approach. However, I don't know how to implement that into the Hugginface transformers. Is it possible to do that without fine-tuning? Do you have another idea?</p>
",Preprocessing of the text & Tokenization,albert first word association list word e g apple banana mango would like use albert identify word strongly associated word list simple term hey albert first word come mind hearing apple banana mango first idea wa using prompt like apple related mask top prediction quite weird proper word like evalle second idea wa use k nearest neighbor approach however know implement hugginface transformer possible without fine tuning another idea
Is `sklearn.Pipeline` with regex really more performant than `spacy` for preprocessing huge volumes of text?,"<h1>TL;DR</h1>
<p>I need help selecting between <code>spacy</code> and <code>sklearn</code> for processing a huge text corpus. I ran a test to measure the performance of each, but the results were unexpected. Moreover, because I'm new-ish to the frameworks involved, I lack confidence that my test is completely valid. I'd really appreciate some guidance.</p>
<ul>
<li><p><a href=""https://pastebin.com/dBAW1rrD"" rel=""nofollow noreferrer"">Code</a></p>
</li>
<li><p><a href=""https://i.sstatic.net/KiSbg.jpg"" rel=""nofollow noreferrer"">Results</a></p>
</li>
</ul>
<h1>Background</h1>
<p>I'm doing a project that involves preprocessing 35 million Reddit comments. This is a pretty massive amount of text. So I'm searching for the most efficient framework to accomplish this with.</p>
<p>Currently, I am considering using either <code>spacy</code>’s <code>nlp.pipe</code> with several custom components, or a <code>sklearn.Pipeline</code> with a ton of regex-based data transformers. Since (1) <code>spacy</code> is optimized for text and (2) regex in Python is slow, I figured the <code>spacy</code> option is the way to go. But I wanted to test my assumptions before proceeding.</p>
<h1>The test</h1>
<p><a href=""https://pastebin.com/dBAW1rrD"" rel=""nofollow noreferrer"">So I wrote a quick and dirty script to do just that.</a> It seems like a lot of code at first skim, but it's actually not. It's very modular, mostly consisting of simple classes. Skip to <code>if __name__ ...</code> at the end to see the overall logic.</p>
<p>Anyway, this script defines what I <em>think</em> are broadly equivalent pipelines, one <code>spacy</code>-based and one <code>sklearn</code>-based, that simply remove (1) punctuation and (2) inline code <code>like this</code>. These pipelines subclass an additional class which actually carries out the test. So the script loads a ~7.5k-comment sample from <a href=""https://www.reddit.com/r/LanguageTechnology/"" rel=""nofollow noreferrer"">r/LanguageTechnology</a> as a <code>dask.dataframe</code> (for parallelization), applies the same preprocessing 100 times using each pipeline, then averages out the results.</p>
<p>To be clear, my actual pipeline will do several more things than just remove punctuation and inline code. I only chose those particular transformations for testing purposes, to keep my tests simple and to the point.</p>
<h1>Results</h1>
<p>My findings (in seconds) are as follows, illustrated graphically <a href=""https://i.sstatic.net/KiSbg.jpg"" rel=""nofollow noreferrer"">here</a>:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">pipeline</th>
<th style=""text-align: right;"">mean</th>
<th style=""text-align: right;"">standard dev</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;""><code>spacy</code></td>
<td style=""text-align: right;"">13.49772</td>
<td style=""text-align: right;"">1.182763</td>
</tr>
<tr>
<td style=""text-align: center;""><code>sklearn</code></td>
<td style=""text-align: right;"">6.853291</td>
<td style=""text-align: right;"">0.127701</td>
</tr>
</tbody>
</table>
</div>
<p>Clearly, <code>spacy</code> was massively slower. This contradicted my expectations, and leaves me unable to draw firm conclusions.</p>
<p>Is <code>sklearn.Pipeline</code> with regex truly the more efficient framework for this? Or was there an issue with my test, or how I structured my pipelines? The latter seems plausible because almost everything the script uses is new-ish to me - <code>dask.dataframe</code>, <code>spacy</code> with custom components, and <code>sklearn.Pipeline</code> with custom transformers. So it may very well be that e.g., I'm just using <code>spacy</code> wrong, or there's something about my script that renders the comparison apples to oranges instead of apples to apples.</p>
<h1>Cry for help</h1>
<p>In light of this uncertainty, I'd sincerely appreciate some input from anyone familiar with these frameworks. I'd also appreciate some eyes on my code, if possible, just to check that I've actually used everything properly.</p>
<p>Any and all input is welcome. Thank you!</p>
",Preprocessing of the text & Tokenization,regex really performant preprocessing huge volume text tl dr need help selecting processing huge text corpus ran test measure performance result unexpected moreover new ish framework involved lack confidence test completely valid really appreciate guidance code result background project involves preprocessing million reddit comment pretty massive amount text searching efficient framework accomplish currently considering using either several custom component ton regex based data transformer since optimized text regex python slow figured option way go wanted test assumption proceeding test wrote quick dirty script seems like lot code first skim actually modular mostly consisting simple class skip end see overall logic anyway script defines think broadly equivalent pipeline one based one based simply remove punctuation inline code pipeline subclass additional class actually carry test script load k comment sample r languagetechnology parallelization applies preprocessing time using pipeline average result clear actual pipeline several thing remove punctuation inline code chose particular transformation testing purpose keep test simple point result finding second follows illustrated graphically pipeline mean standard dev clearly wa massively slower contradicted expectation leaf unable draw firm conclusion regex truly efficient framework wa issue test structured pipeline latter seems plausible almost everything script us new ish custom component custom transformer may well e g using wrong something script render comparison apple orange instead apple apple cry help light uncertainty sincerely appreciate input anyone familiar framework also appreciate eye code possible check actually used everything properly input welcome thank
Should I remove stopwords when feed sentence to RNN,"<p>In bag-of-words model, I know we should remove stopwords and punctuation before training. But in RNN model, if I want to do text classification, should I remove stopwords too ? </p>
",Preprocessing of the text & Tokenization,remove stopwords feed sentence rnn bag word model know remove stopwords punctuation training rnn model want text classification remove stopwords
Generate or find a shortest text given list of words,"<p>Let's say I have a list of 1000+ words and I would like to generate a text that includes these words from the list. I would like to use as few extra words outside of the list as possible. How would one tackle such a problem? Or alternatively, is there a way to efficiently search for a smaller portion of text containing these words the most, given some larger text (millions of words)? Basically, the resulting text from the search should be optimized to be shortest but to contain all the words from the list.</p>
",Preprocessing of the text & Tokenization,generate find shortest text given list word let say list word would like generate text includes word list would like use extra word outside list possible would one tackle problem alternatively way efficiently search smaller portion text containing word given larger text million word basically resulting text search optimized shortest contain word list
How to remove stop words and get lemmas in a pandas data frame using spacy?,"<p>I have a column of tokens in a pandas data frame in python. Something that looks like:</p>
<pre><code> word_tokens
 (the,cheeseburger,was,great)
 (i,never,did,like,the,pizza,too,much)
 (yellow,submarine,was,only,an,ok,song)
</code></pre>
<p>I want get two more new columns in this dataframe using the spacy library. One column that contains each row's tokens with the stopwords removed, and the other one containing the lemmas from the second column. How could I do that?</p>
",Preprocessing of the text & Tokenization,remove stop word get lemma panda data frame using spacy column token panda data frame python something look like want get two new column dataframe using spacy library one column contains row token stopwords removed one containing lemma second column could
replace consecutive values of a list if they are equal to one single value of another list,"<p>For instance, given a list <em>article_list</em> and <em>disease_list</em>, which are defined as follows:</p>
<pre><code>article_list = ['Tricuspid', 'valve', 'regurgitation', 'and', 'lithium', 'carbonate', 
               'toxicity', 'in', 'a', 'newborn', 'infant', '.']

disease_list = ['Tricuspid valve regurgitation', 'lithium carbonate', 'toxicity']
</code></pre>
<p>As you can see article_list is a list of tokens of an article. However, I would like to tokenize disease names (which often consist of several words) as one token instead of several. That means to replace the tokens of article_list with the tokens in disease list on the right places.</p>
<p>The function should be something like this</p>
<pre class=""lang-py prettyprint-override""><code>def insert_disease_tokens(article_list, disease_list):
    # do something
</code></pre>
<pre><code>print(insert_disease_tokens(article_list, disease_list))
&gt;&gt;&gt; ['Tricuspid valve regurgitation', 'and', 'lithium carbonate','toxicity', 'in', 'a', 'newborn', 'infant', '.']
</code></pre>
<p>Any idea with an efficient way to do that in Python? I only came up with unnecessarily complicated solutions.</p>
",Preprocessing of the text & Tokenization,replace consecutive value list equal one single value another list instance given list article list disease list defined follows see article list list token article however would like tokenize disease name often consist several word one token instead several mean replace token article list token disease list right place function something like idea efficient way python came unnecessarily complicated solution
Text Cleaning before using TextBlob for Sentiment Analysis,"<p>Do you clean the text (data) before using TextBlob or do you use TextBlob first and then clean the text (from punctuation, stopwords etc)?</p>
",Preprocessing of the text & Tokenization,text cleaning using textblob sentiment analysis clean text data using textblob use textblob first clean text punctuation stopwords etc
Faster Python Lemmatization,"<p>I have been testing different lemmatization methods since it will be used on a very large corpus. Below are my methods and results. Does anyone have any tips to speed any of these methods up? Spacy was the fastest with part of speech tags included (preferred), followed by lemminflect. Am I going about this the wrong way? These functions are being applied with pandas .apply() on a dataframe containing the text.</p>
<pre><code>def prepareString_nltk_current(x):
    lemmatizer = WordNetLemmatizer()
    x = re.sub(r&quot;[^0-9a-z]&quot;, &quot; &quot;, x)
    if len(x)==0:
        return ''
    tokens = word_tokenize(x)
    tokens = [lemmatizer.lemmatize(word).strip() for word in tokens if word not in stop_words]
    if len(tokens)==0:
        return ''
    return ' '.join(map(str,tokens))

def prepareString_pattern(x):
    error = 'Error'
    x = re.sub(r&quot;[^0-9a-z.,;]&quot;, &quot; &quot;, x)
    if len(x)==0:
        return ''
    try:
        return &quot; &quot;.join([lemma(wd) if wd not in ['this', 'his'] else wd for wd in x.split()])
    except StopIteration:
        return error

def prepareString_pattern(x):
    error = 'Error'
    x = re.sub(r&quot;[^0-9a-z.,;]&quot;, &quot; &quot;, x)
    if len(x)==0:
        return ''
    try:
        return &quot; &quot;.join([lemma(wd) if wd not in ['this', 'his'] else wd for wd in x.split()])
    except StopIteration:
        return error


def prepareString_spacy_pretrained(x):
    if len(x)==0:
        return ''
    doc = nlp(x)
    return re.sub(r&quot;[^0-9a-zA-Z]&quot;, &quot; &quot;, &quot; &quot;.join(str(token.lemma) for token in doc)).lower()

def get_wordnet_pos(word):
    lemmatizer = WordNetLemmatizer()
    &quot;&quot;&quot;Map POS tag to first character lemmatize() accepts&quot;&quot;&quot;
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {&quot;J&quot;: 'a',
                    &quot;N&quot;: 'n',
                    &quot;V&quot;: 'v',
                    &quot;R&quot;: 'r'}

    return lemmatizer.lemmatize(word, tag_dict.get(tag, 'n'))

def prepareString_nltk_pos(x):
    
    tokens = word_tokenize(x)
    if len(x)==0:
        return ''
    return &quot; &quot;.join(get_wordnet_pos(w) for w in tokens)

def prepareString_textblob(x):
    sent = TextBlob(x)
    tag_dict = {&quot;J&quot;: 'a', 
                &quot;N&quot;: 'n', 
                &quot;V&quot;: 'v', 
                &quot;R&quot;: 'r'}
    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    
    return &quot; &quot;.join([wd.lemmatize(tag) for wd, tag in words_and_tags])

def prepareString_genism(x):
    return &quot; &quot;.join([wd.decode('utf-8').split('/')[0] for wd in lemmatize(x)])

def prepareString_leminflect(x):
    doc = nlp(x)
    return &quot; &quot;.join([str(x._.lemma) for x in doc])


def prepareString_pattern_pos(x):
    s = parsetree(x, tags=True, lemmata=True)
    for sentence in s:
        return re.sub(r&quot;[^0-9a-zA-Z]&quot;, &quot; &quot;, &quot; &quot;.join([str(x._.lemma()) for x in doc])).lower()
</code></pre>
<p><a href=""https://i.sstatic.net/vdKd8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vdKd8.png"" alt=""enter image description here"" /></a></p>
",Preprocessing of the text & Tokenization,faster python lemmatization testing different lemmatization method since used large corpus method result doe anyone tip speed method spacy wa fastest part speech tag included preferred followed lemminflect going wrong way function applied panda apply dataframe containing text
Lemmatize a tm corpus,"<p>I am trying to lemmatize a <code>VCorpus</code> using a custom dictionary, in particular <a href=""https://raw.githubusercontent.com/brema76/lemmatization-ita/master/lemmatization_ita.csv"" rel=""nofollow noreferrer"">this</a> one I found on github. I have not been able to find a function that correctly performs this. I have tried the following:</p>
<pre><code>lemma &lt;- read.csv(&quot;~/lemma.txt&quot;)
corpus &lt;- tm_map(corpus,lemmatize_strings, dictionary = lemma)
</code></pre>
<p>But with no luck.</p>
<p>The other solution I can come up with is to loop through every word and gsub it, but it seems quite inefficient.</p>
<p>Thanks a lot in advance.</p>
",Preprocessing of the text & Tokenization,lemmatize tm corpus trying lemmatize using custom dictionary particular one found github able find function correctly performs tried following luck solution come loop every word gsub seems quite inefficient thanks lot advance
Tokenize paragraphs by special characters; then rejoin so tokenized segments to reach certain length,"<p>I have this long paragraph:</p>
<pre><code>paragraph = &quot;The weakening of the papacy by the Avignon exile and the Papal Schism; the breakdown of monastic discipline and clerical celibacy; the luxury of prelates, the corruption of the Curia, the worldly activities of the popes; the morals of Alexander VI, the wars of Julius II, the careless gaiety of Leo X; the relicmongering and peddling of indulgences; the triumph of Islam over Christendom in the Crusades and the Turkish wars; the spreading acquaintance with non-Christian faiths; the influx of Arabic science and philosophy; the collapse of Scholasticism in the irrationalism of Scotus and the skepticism of Ockham; the failure of the conciliar movement to effect reform; the discovery of pagan antiquity and of America; the invention of printing; the extension of literacy and education; the translation and reading of the Bible; the newly realized contrast between the poverty and simplicity of the Apostles and the ceremonious opulence of the Church; the rising wealth and economic independence of Germany and England; the growth of a middle class resentful of ecclesiastical restrictions and claims; the protests against the flow of money to Rome; the secularization of law and government; the intensification of nationalism and the strengthening of monarchies; the nationalistic influence of vernacular languages and literatures; the fermenting legacies of the Waldenses, Wyclif, and Huss; the mystic demand for a less ritualistic, more personal and inward and direct religion: all these were now uniting in a torrent of forces that would crack the crust of medieval custom, loosen all standards and bonds, shatter Europe into nations and sects, sweep away more and more of the supports and comforts of traditional beliefs, and perhaps mark the beginning of the end for the dominance of Christianity in the mental life of European man.&quot;
</code></pre>
<p><strong>My goal</strong> is to split this long paragraph into multiple sentences <strong>keeping the sentences around 18 - 30 words each.</strong></p>
<p>There is only one full-stop at the end; so nltk tokenizer is of no use. I can use regex to tokenize; I have this pattern that works in splitting:</p>
<pre><code>regex_special_chars = '([″;*&quot;(§=!‡…†\\?\\]‘)¿♥[]+)'
new_text = re.split(regex_special_chars, paragraph)

</code></pre>
<p>The question is how to join this paragraph into a list of multiple sentences that would be around <strong>18 to 30</strong>; where possible; because sometimes it's not possible with this regex.</p>
<p>The end result will look like the following list below:</p>
<pre><code>tokenized_paragraph = ['The weakening of the papacy by the Avignon exile and the Papal Schism; the breakdown of monastic discipline and clerical celibacy;',
 'the luxury of prelates, the corruption of the Curia, the worldly activities of the popes; the morals of Alexander VI, the wars of Julius II, the careless gaiety of Leo X;',
 'the relicmongering and peddling of indulgences; the triumph of Islam over Christendom in the Crusades and the Turkish wars; the spreading acquaintance with non-Christian faiths; ',
 'the influx of Arabic science and philosophy; the collapse of Scholasticism in the irrationalism of Scotus and the skepticism of Ockham; the failure of the conciliar movement to effect reform; ',
 'the discovery of pagan antiquity and of America; the invention of printing; the extension of literacy and education; the translation and reading of the Bible; ',
 'the newly realized contrast between the poverty and simplicity of the Apostles and the ceremonious opulence of the Church; the rising wealth and economic independence of Germany and England;',
 'the growth of a middle class resentful of ecclesiastical restrictions and claims; the protests against the flow of money to Rome; the secularization of law and government; ',
 'the intensification of nationalism and the strengthening of monarchies; the nationalistic influence of vernacular languages and literatures; the fermenting legacies of the Waldenses, Wyclif, and Huss;',
 'the mystic demand for a less ritualistic, more personal and inward and direct religion: all these were now uniting in a torrent of forces that would crack the crust of medieval custom, loosen all standards and bonds, shatter Europe into nations and sects, sweep away more and more of the supports and comforts of traditional beliefs, and perhaps mark the beginning of the end for the dominance of Christianity in the mental life of European man.']
</code></pre>
<p>if we check the lengths of the end result; we get this many words into each tokenized segment:</p>
<pre><code>[len(sent.split()) for sent in tokenized_paragraph]
[21, 31, 25, 30, 25, 29, 27, 26, 76]

</code></pre>
<p>Only the last segment exceeded 30 words (76 words), and that's okay!</p>
<p><strong>Edit</strong>
<strong>The regex could include a colon <code>:</code> So the last segment would be less than 76</strong></p>
",Preprocessing of the text & Tokenization,tokenize paragraph special character rejoin tokenized segment reach certain length long paragraph goal split long paragraph multiple sentence keeping sentence around word one full stop end nltk tokenizer use use regex tokenize pattern work splitting question join paragraph list multiple sentence would around possible sometimes possible regex end result look like following list check length end result get many word tokenized segment last segment exceeded word word okay edit regex could include colon last segment would le
Removing Stop Words From Text in R,"<p>I have a problem with removing stop_words from text data. The data set is web scraped and contains customer reviews and looks like:</p>
<pre><code>data$Review &lt;- c(&quot;Won't let me use my camera&quot;, &quot;Does not load&quot;,&quot;I'ts truly mind blowing!&quot;)
</code></pre>
<p>I did the bellow data manipulation, and created a new variable in the data frame and now reviews look like this:</p>
<pre><code>Manipulation Part: 
data$Proc_Review &lt;- gsub(&quot;'&quot;, &quot;&quot;, data$Review) # Removes Apostrophes white spaces
data$Proc_Review &lt;-  gsub('[[:punct:] ]+',' ',data$Proc_Review) # Remove Punctuation 
data$Proc_Review &lt;- gsub('[[:digit:]]+', '', data$Proc_Review) # Remove numbers
data$Proc_Review &lt;- as.character(data$Proc_Review)
&quot;wont let me use my camera&quot;, &quot;does not load&quot;, &quot;its truly mind blowing&quot;
</code></pre>
<p>The next step is to remove stop words, for which I use the bellow code:</p>
<pre><code>    data(&quot;stop_words&quot;)

j&lt;-1
for (j in 1:nrow(data)) {
  description&lt;-  anti_join((data[j,] %&gt;% unnest_tokens(word,Proc_Review, drop=FALSE,to_lower=FALSE) ),stop_words)
  data[j,&quot;Proc_Review&quot;]&lt;-paste((description),collapse = &quot; &quot;)
}
</code></pre>
<p>After that the output is</p>
<pre><code>c(1, 1) c(17304, 17304) c(\&quot;Won't let me use my camera\&quot;, \&quot;Won't let me use my camera\&quot;) c(1, 1) c(1, 1) c(32, 32) c(4, 4) c(\&quot;wont let me use my camera\&quot;, \&quot;wont let me use my camera\&quot;) c(\&quot;wont\&quot;, \&quot;camera\&quot;)&quot;
</code></pre>
<p>I have tried some other ways, however, the result was not the wanted one, as it removed some stop_words from some reviews but not for all of them. For example, it removed &quot;it's&quot; in some reviews, but in some &quot;it's&quot; remained.</p>
<p>What I want to do is reviews to appear in a new column in the data set without the stop words!
Thank you so much in advance!!</p>
",Preprocessing of the text & Tokenization,removing stop word text r problem removing stop word text data data set web scraped contains customer review look like bellow data manipulation created new variable data frame review look like next step remove stop word use bellow code output tried way however result wa wanted one removed stop word review example removed review remained want review appear new column data set without stop word thank much advance
Getting the unique word count from each row in a pandas column in Python,"<p>I am pretty new to Python and trying to preprocess some text data for my NLP project on hiphop lyrics.
I have a column in my dataframe with (already cleaned) lyrics and want to make another column containing the length of the unique words in the lyrics column for each artist.</p>
<p>This is my dataframe.tail()</p>
<p><a href=""https://i.sstatic.net/MBOXS.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MBOXS.jpg"" alt=""enter image description here"" /></a></p>
<p>I only managed to make a set of unique words with this code.</p>
<p>unique_words = set()</p>
<p>unique_wordsDF['clean_lyrics1'].str.lower().str.split().apply(unique_words.update)</p>
<p>print(unique_words)</p>
<p>I know I somehow have to put the set method into a for loop to iterate over all the songs but cannot seem to figure it out how to do it. My desired output would be to have a 'unique_count' column based on the number of unique words inside the 'clean_lyrics1' column</p>
",Preprocessing of the text & Tokenization,getting unique word count row panda column python pretty new python trying preprocess text data nlp project hiphop lyric column dataframe already cleaned lyric want make another column containing length unique word lyric column artist dataframe tail managed make set unique word code unique word set unique wordsdf clean lyric str lower str split apply unique word update print unique word know somehow put set method loop iterate song seem figure desired output would unique count column based number unique word inside clean lyric column
Issues with extracting URLs from text,"<p>I am trying to find a regular expression to extract any valid URLs (not only http[s]) using a regular expression. Unfortunately, each one outputs weird things. The best results I achieved using this regex:</p>
<pre><code>\b((?:[a-z][\w\-]+:(?:\/{1,3}|[a-z0-9%])|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}\/)(?:[^\s()&lt;&gt;]|\((?:[^\s()&lt;&gt;]|(?:\([^\s()&lt;&gt;]+\)))*\))+(?:\((?:[^\s()&lt;&gt;]|(?:\([^\s()&lt;&gt;]+\)))*\)|[^\s`!()\[\]{};:'&quot;.,&lt;&gt;?«»“”‘’]))
</code></pre>
<p>But I can mark at least the following issues:</p>
<ul>
<li><a href=""http://208.206.41.61/email/email_log.cfm?useremail=3Dtana.jones@enron.com&amp;=refdoc=3D(01-128)"" rel=""nofollow noreferrer"">http://208.206.41.61/email/email_log.cfm?useremail=3Dtana.jones@enron.com&amp;=refdoc=3D(01-128)</a> is extracted as <a href=""http://208.206.41.61/email/email_log.cfm?useremail=3Dtana.jones@enron.com&amp;="" rel=""nofollow noreferrer"">http://208.206.41.61/email/email_log.cfm?useremail=3Dtana.jones@enron.com&amp;=</a></li>
<li>http://www.onlinefilefolder.com',AJAXTHRESHOLD should be extracted without AJAXTHRESHOLD</li>
<li>CSS / HTML styling is extracted, for example <code>xmlns:x=&quot;urn:schemas-microsoft-com:xslt, ze:12px;color:#666, font-size:12px;color</code> etc</li>
</ul>
<p>How can I improve this regex to make sure only valid URLs are extracted? I am not only extracting it from the HTML, but also from a plain text. Therefore, using only <code>beautifulsoup</code> is impossible for my use case.</p>
",Preprocessing of the text & Tokenization,issue extracting url text trying find regular expression extract valid url using regular expression unfortunately one output weird thing best result achieved using regex mark least following issue extracted extracted without ajaxthreshold cs html styling extracted example etc improve regex make sure valid url extracted extracting html also plain text therefore using impossible use case
Incorrect sentence sequencing after tokenization,"<p>I'm using this data from
<a href=""https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews"" rel=""nofollow noreferrer"">here</a> called Women's E-Commerce Clothing Review by applying the idea of combining features into text from <a href=""https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/#33-tokenize--encode"" rel=""nofollow noreferrer"">here</a> for NLP learning.</p>
<pre><code>tokenizer = Tokenizer(num_words=60000)
tokenizer.fit_on_texts(text_train)
sequences_train = tokenizer.texts_to_sequences(text_train) #text_train - training set containing text feature only
sequences_test = tokenizer.texts_to_sequences(text_test)
word_index = tokenizer.word_index
print(&quot;\nWord Index = &quot;, word_index)

Word Index =  {'i': 1, 'the': 2, 'and': 3, 'this': 4, 'is': 5, 'item': 6, 'a': 7, 'it': 8, 'of': 9, '5': 10, 'am': 11, 'out': 12, 'to': 13, 'from': 14, 'under': 15, 'years': 16, 'comes': 17, 'old': 18, 'stars': 19, 'rate': 20, 'department': 21, 'division': 22, 'classified': 23, 'in': 24, 'general': 25, 'but': 26, 'on': 27, 'for': 28, 'with': 29, 'so': 30, 'dresses': 31, 'was': 32, 'dress': 33, 'my': 34, 'not': 35, 'love': 36, 'tops': 37, 'petite': 38, 'that': 39, 'size': 40, 'very': 41, 'top': 42, 'have': 43, 'fit': 44, 'great': 45, 'like': 46, 'are': 47, 'be': 48, 'me': 49, 'as': 50, 'too': 51, 'wear': 52, &quot;it's&quot;: 53, '4': 54, 'or': 55, 'just': 56, &quot;i'm&quot;: 57, 'you': 58, 'fabric': 59, 'small': 60, 'would': 61, 'up': 62, 'they': 63, 'color': 64, 'at': 65, 'cute': 66, 'perfect': 67, 'knits': 68, 'beautiful': 69, 'look': 70, 'really': 71, 'if': 72, 'flattering': 73, 'more': 74, 'little': 75, 'these': 76, 'ordered': 77, 'bottoms': 78, 'soft': 79, 'jeans': 80, 'one': 81, 'comfortable': 82, 'will': 83, 'nice': 84, 'pants': 85, 'well': 86, 'an': 87, 'back': 88, '\r': 89, '3': 90, 'because': 91, 'can': 92, 'had': 93, 'shirt': 94, 'than': 95, 'large': 96, 'all': 97, 'blouses': 98, 'bought': 99, 'looks': 100, 'bit': 101, 'fits': 102, 'sweater': 103, 'down': 104, 'when': 105, 'pretty': 106, '2': 107, 'much': 108, 'material': 109, 'which': 110, 'them': 111, 'length': 112, 'long': 113, 'also': 114, 'has': 115, 'quality': 116, 'colors': 117, 'waist': 118, 'got': 119, 'skirt': 120, 'xs': 121, 'work': 122, 'medium': 123, 'even': 124, 'think': 125, 'good': 126, 'retailer': 127, 'tried': 128, 'runs': 129, 'short': 130, 'big': 131, 'summer': 132, 'store': 133, 'super': 134, 'made': 135, 'other': 136, 'about': 137, 'usually': 138, 'way': 139, 'cut': 140, 'get': 141, 'could': 142, 'only': 143, 'black': 144, 'style': 145, 'see': 146, &quot;don't&quot;: 147, &quot;didn't&quot;: 148, 'right': 149, 'there': 150, 'were': 151, 'jackets': 152, 'still': 153, 'design': 154, 'no': 155, 'true': 156, 'fine': 157, 'did': 158, 'sleeves': 159, 'online': 160, 'white': 161, 'do': 162, 'intimate': 163, 'go': 164, 'lovely': 165, 'sweaters': 166, 'wearing': 167, 'off': 168, 'gorgeous': 169, 'purchased': 170, 'tight': 171, 'perfectly': 172, 'however': 173, 'does': 174, 'enough': 175, 'feel': 176, 'some': 177, 'fall': 178, 'better': 179, 'front': 180, 'person': 181, 'over': 182, 'initmates': 183, 'model': 184, 'definitely': 185, 'what': 186, '39': 187, 'looked': 188, 'been': 189, 'blue': 190, 'jacket': 191, 'though': 192, 'comfy': 193, 'price': 194, 'sale': 195, 'loved': 196, 'lbs': 197, 'how': 198, 'body': 199, 'casual': 200, 'your': 201, 'loose': 202, 'piece': 203, 'wanted': 204, 'high': 205, 'bottom': 206, 'light': 207, 'first': 208, 'try': 209, 'going': 210, 'blouse': 211, 'around': 212, 'skirts': 213, 'shape': 214, 'looking': 215, 'time': 216, 'regular': 217, 'worn': 218, 'many': 219, 'thin': 220, '6': 221, 'chest': 222, 'both': 223, 'make': 224, 'thought': 225, 'through': 226, 'arms': 227, '1': 228, 'pattern': 229, 'fun': 230, 's': 231, 'shoulders': 232, 'saw': 233, 'print': 234, 'makes': 235, 'recommend': 236, 'after': 237, 'gauge': 238, 'bust': 239, 'unique': 240, 'compliments': 241, &quot;doesn't&quot;: 242, 'need': 243, 'without': 244, 'want': 245, 'being': 246, 'weight': 247, 'pair': 248, 'wish': 249, 'buy': 250, 'sure': 251, 'wore': 252, 'hips': 253, 'find': 254, &quot;i've&quot;: 255, 'side': 256, 'run': 257, 'found': 258, 'went': 259, 'reviews': 260, 'bra': 261, &quot;can't&quot;: 262, 'different': 263, '34': 264, 'easy': 265, 'order': 266, 'usual': 267, 'shorts': 268, 'picture': 269, '35': 270, 'since': 271, 'lot': 272, 'by': 273, '36': 274, '8': 275, 'tee': 276, '38': 277, 'longer': 278, 'received': 279, 'detail': 280, 'quite': 281, 'versatile': 282, 'day': 283, 'while': 284, 'its': 285, 'leggings': 286, 'most': 287, 'any': 288, 'absolutely': 289, 'boxy': 290, 'nicely': 291, 'normally': 292, 'm': 293, 'two': 294, 'adorable': 295, 'overall': 296, 'another': 297, 'slightly': 298, 'then': 299, 'tank': 300, 'keep': 301, 'green': 302, 'may': 303, '26': 304, 'wide': 305, 'fitted': 306, 'lace': 307, 'return': 308, 'sizing': 309, 'red': 310, 'felt': 311, 'stretch': 312, '33': 313, 'underneath': 314, 'neck': 315, 'might': 316, '37': 317, 'warm': 318, 'say': 319, 't': 320, 'now': 321, 'happy': 322, 'almost': 323, 'flowy': 324, &quot;wasn't&quot;: 325, 'sheer': 326, 'lounge': 327, 'favorite': 328, '10': 329, 'low': 330, 'something': 331, '46': 332, 'photo': 333, 'sizes': 334, 'actually': 335, 'probably': 336, 'extra': 337, '40': 338, '41': 339, 'amazing': 340, 'huge': 341, 'pockets': 342, '32': 343, 'worth': 344, 'every': 345, 'spring': 346, 'purchase': 347, 'area': 348, 'yet': 349, 'such': 350, '29': 351, 'disappointed': 352, '28': 353, 'feminine': 354, 'dressed': 355, 'full': 356, 'tall': 357, &quot;5'4&quot;: 358, 'best': 359, 'feels': 360, 'smaller': 361, 'into': 362, 'boots': 363, 'put': 364, 'shorter': 365, '42': 366, 'details': 367, 'said': 368, 'same': 369, '48': 370, 'unfortunately': 371, 'denim': 372, 'everything': 373, 'buttons': 374, 'between': 375, 'winter': 376, '30': 377, '27': 378, 'tunic': 379, 'tts': 380, 'glad': 381, '44': 382, 'navy': 383, 'sized': 384, 'reference': 385, 'neckline': 386, 'skinny': 387, 'always': 388, 'decided': 389, 'pictured': 390, 'give': 391, 'line': 392, '31': 393, 'liked': 394, 'larger': 395, 'maybe': 396, '43': 397, 'should': 398, 'coat': 399, 'thick': 400, 'above': 401, '53': 402, 'lightweight': 403, 'easily': 404, 'wait': 405, 'snug': 406, 'wash': 407, 'part': 408, 'figure': 409, 'fitting': 410, 'cozy': 411, &quot;i'd&quot;: 412, 'new': 413, 'take': 414, 'pink': 415, 'problem': 416, 'where': 417, 'especially': 418, 'button': 419, 'returned': 420, 'thing': 421, 'came': 422, 'expected': 423, 'grey': 424, &quot;isn't&quot;: 425, '47': 426, 'room': 427, 'seems': 428, 'reviewers': 429, 'classic': 430, 'heavy': 431, '45': 432, 'cardigan': 433, 'cotton': 434, '25': 435, 'slip': 436, 'hem': 437, &quot;5'3&quot;: 438, 'l': 439, 'kind': 440, 'normal': 441, 'few': 442, 'know': 443, 'beautifully': 444, '49': 445, &quot;couldn't&quot;: 446, 'knit': 447, 'stylish': 448, 'cool': 449, 'curvy': 450, 'lining': 451, 'knee': 452, 'dark': 453, 'embroidery': 454, 'xl': 455, '0': 456, &quot;that's&quot;: 457, '56': 458, 'belt': 459, 'never': 460, 'took': 461, 'cami': 462, '12': 463, 'frame': 464, 'before': 465, 'although': 466, 'bad': 467, 'hit': 468, 'lined': 469, &quot;i'll&quot;: 470, 'torso': 471, 'typically': 472, 'goes': 473, 'extremely': 474, 'washed': 475, 'able': 476, 'arm': 477, &quot;5'5&quot;: 478, 'excited': 479, 'ever': 480, &quot;5'2&quot;: 481, 'works': 482, 'who': 483, 'vest': 484, 'legs': 485, 'reviewer': 486, 'hits': 487, 'stunning': 488, '50': 489, 'returning': 490, 'someone': 491, 'once': 492, 'highly': 493, 'show': 494, 'anything': 495, 'xxs': 496, 'those': 497, 'weather': 498, 'simple': 499, 'either': 500, 'type': 501, 'wardrobe': 502, '52': 503, 'suit': 504, 'unflattering': 505, 'shoulder': 506, 'zipper': 507, 'photos': 508, 'issue': 509, 'ended': 510, &quot;5'7&quot;: 511, 'year': 512, 'today': 513, 'things': 514, 'needed': 515, 'stretchy': 516, 'exactly': 517, 'gray': 518, 'staple': 519, 'sleeve': 520, 'sold': 521, 'why': 522, '51': 523, 'tie': 524, 'swing': 525, 'layer': 526, &quot;you're&quot;: 527, 'hot': 528, 'shown': 529, 'trying': 530, 'already': 531, 'fell': 532, 'below': 533, 'days': 534, 'arrived': 535, 'wonderful': 536, 'swim': 537, 'last': 538, 'layering': 539, 'brand': 540, 'must': 541, 'orange': 542, '60': 543, 'agree': 544, 'itchy': 545, &quot;5'6&quot;: 546, 'trend': 547, 'add': 548, 'slim': 549, 'nothing': 550, 'basic': 551, 'skin': 552, 'otherwise': 553, 'away': 554, 'tad': 555, &quot;wouldn't&quot;: 556, 'straps': 557, 'cheap': 558, 'come': 559, 'holes': 560, 'getting': 561, 'again': 562, 'sadly': 563, 'baggy': 564, 'due': 565, '54': 566, 'odd': 567, '57': 568, 'local': 569, 'hoping': 570, 'dry': 571, 'delicate': 572, &quot;they're&quot;: 573, 'sides': 574, 'hard': 575, 'across': 576, 'vibrant': 577, 'ran': 578, &quot;5'8&quot;: 579, '62': 580, 'inches': 581, 'outerwear': 582, 'weird': 583, 'others': 584, 'pilcro': 585, 'cropped': 586, 'plus': 587, 'several': 588, 'rather': 589, 'waisted': 590, 'elegant': 591, 'bright': 592, 'awesome': 593, 'surprised': 594, 'busty': 595, 'ordering': 596, 'special': 597, 'heels': 598, 'less': 599, 'seem': 600, 'keeping': 601, 'drape': 602, 'straight': 603, &quot;5'&quot;: 604, 'roomy': 605, 'open': 606, 'version': 607, 'hangs': 608, 'least': 609, 'use': 610, 'yellow': 611, 'kept': 612, 'touch': 613, '55': 614, 'night': 615, '66': 616, 'pictures': 617, 'pull': 618, 'plan': 619, 'based': 620, 'pounds': 621, 'ok': 622, 'own': 623, 'oversized': 624, 'drapes': 625, 'seemed': 626, 'v': 627, 'sometimes': 628, 'season': 629, 'ankle': 630, 'paired': 631, 'review': 632, 'tights': 633, 'shirts': 634, 'far': 635, '24': 636, 'elastic': 637, 'jumpsuit': 638, '59': 639, 'lots': 640, 'their': 641, 'bigger': 642, 'worked': 643, 'myself': 644, 'totally': 645, 'flare': 646, 'available': 647, 'having': 648, 'flat': 649, 'wedding': 650, 'jean': 651, 'here': 652, 'gives': 653, 'she': 654, 'booties': 655, '58': 656, 'sexy': 657, 'making': 658, 'immediately': 659, 'poor': 660, 'mentioned': 661, &quot;5'1&quot;: 662, 'mine': 663, 'cover': 664, 'completely': 665, 'falls': 666, '115': 667, 'collar': 668, 'times': 669, 'interesting': 670, 'wool': 671, 'end': 672, 'sleep': 673, 'instead': 674, 'stiff': 675, 'tiny': 676, 'higher': 677, 'leg': 678, 'ivory': 679, 'cold': 680, &quot;won't&quot;: 681, 'finally': 682, 'classy': 683, 'neutral': 684, 'pant': 685, 'yes': 686, 'mid': 687, 'previous': 688, &quot;there's&quot;: 689, 'dressy': 690, 'throw': 691, 'shows': 692, 'athletic': 693, 'slimming': 694, 'wrong': 695, 'lower': 696, 'people': 697, 'read': 698, 'buying': 699, 'thinking': 700, 'substantial': 701, 'adds': 702, 'sandals': 703, 'texture': 704, '23': 705, 'cream': 706, 'used': 707, 'form': 708, 'knees': 709, 'incredibly': 710, 'chic': 711, &quot;5'9&quot;: 712, 'eye': 713, 'fact': 714, 'clothes': 715, 'months': 716, 'similar': 717, '64': 718, 'fabulous': 719, 'curves': 720, 'addition': 721, 'half': 722, 'close': 723, 'knew': 724, 'armholes': 725, 'closet': 726, 'product': 727, 'bulky': 728, 'else': 729, 'idea': 730, 'outfit': 731, 'maternity': 732, 'stripes': 733, 'inside': 734, 'hourglass': 735, 'broad': 736, 'linen': 737, 'prefer': 738, 'worried': 739, 'gotten': 740, 'detailing': 741, 'rich': 742, 'often': 743, 'hip': 744, 'home': 745, 'clothing': 746, 'brown': 747, '65': 748, 'reason': 749, 'flats': 750, 'sweatshirt': 751, '120': 752, 'subtle': 753, 'second': 754, 'silk': 755, 'peplum': 756, 'baby': 757, 'rise': 758, 'amount': 759, '20': 760, 'depending': 761, 'expect': 762, 'deep': 763, 'next': 764, 'during': 765, 'appropriate': 766, '61': 767, 'comfort': 768, 'tent': 769, 'gave': 770, 'c': 771, 'fantastic': 772, 'except': 773, 'until': 774, 'floral': 775, '63': 776, 'mind': 777, 'mail': 778, 'strange': 779, 'b': 780, 'appears': 781, 'flow': 782, 'couple': 783, 'sad': 784, 'blazer': 785, 'romper': 786, 'hope': 787, 'w': 788, 'purple': 789, 'soon': 790, 'thicker': 791, 'left': 792, 'tell': 793, 'upper': 794, 'washing': 795, 'hang': 796, 'awkward': 797, 'fan': 798, 'point': 799, '34c': 800, 'complaint': 801, 'together': 802, 'issues': 803, 'pleased': 804, 'colored': 805, 'maeve': 806, 'disappointing': 807, 'along': 808, 'thighs': 809, 'anyway': 810, 'hold': 811, '34b': 812, 'perhaps': 813, 'nude': 814, 'taller': 815, 'hand': 816, 'her': 817, '125': 818, 'looser': 819, 'build': 820, 'live': 821, 'deal': 822, 'gone': 823, 'added': 824, '135': 825, 'forward': 826, 'justice': 827, '140': 828, 'exchange': 829, 'slight': 830, 'somewhat': 831, 'maxi': 832, 'everywhere': 833, 'real': 834, 'tag': 835, 'needs': 836, 'cannot': 837, 'fairly': 838, 'please': 839, 'inch': 840, 'case': 841, 'wow': 842, 'hung': 843, 'sweet': 844, 'truly': 845, 'beach': 846, 'feeling': 847, 'place': 848, 'girls': 849, 'sort': 850, 'middle': 851, &quot;haven't&quot;: 852, 'bodice': 853, 'seam': 854, 'motif': 855, 'spot': 856, 'flows': 857, 'past': 858, 'pulled': 859, 'seams': 860, 'whole': 861, 'coverage': 862, 'butt': 863, 'leather': 864, 'meant': 865, 'expecting': 866, 'excellent': 867, 'simply': 868, 'women': 869, 'party': 870, 'office': 871, 'pieces': 872, 'transition': 873, 'narrow': 874, 'scratchy': 875, 'opinion': 876, 'we': 877, 'pairs': 878, 'airy': 879, 'items': 880, 'heavier': 881, 'tummy': 882, 'waistband': 883, 'says': 884, 'plaid': 885, 'itself': 886, 'zip': 887, 'weekend': 888, 'band': 889, 'typical': 890, 'done': 891, 'girl': 892, 'generally': 893, 'styling': 894, 'stitching': 895, 'slender': 896, 'barely': 897, 'sewn': 898, 'relaxed': 899, 'imagine': 900, '130': 901, 'three': 902, 'reading': 903, '100': 904, 'legwear': 905, 'seen': 906, 'stay': 907, 'unless': 908, 'noticed': 909, 'darker': 910, 'dinner': 911, 'difficult': 912, 'bag': 913, 'chested': 914, 'each': 915, 'belly': 916, '14': 917, 'lay': 918, 'everyday': 919, 'ago': 920, 'gold': 921, 'oh': 922, 'showing': 923, 'clean': 924, 'ladies': 925, 'husband': 926, 'swingy': 927, 'everyone': 928, 'pregnant': 929, 'lines': 930, 'portion': 931, 'sack': 932, 'clingy': 933, 'wider': 934, 'anyone': 935, 'shapeless': 936, 'ag': 937, '67': 938, 'believe': 939, 'prettier': 940, 'surprise': 941, '34d': 942, 'frumpy': 943, 'places': 944, 'seasons': 945, 'slits': 946, 'necklace': 947, 'running': 948, &quot;5'10&quot;: 949, 'care': 950, 'tailored': 951, 'okay': 952, 'chance': 953, 'cuter': 954, 'lighter': 955, 'perfection': 956, 'keeper': 957, 'shade': 958, 'shaped': 959, 'petites': 960, 'rest': 961, 'coral': 962, 'beauty': 963, 'finding': 964, 'caught': 965, 'money': 966, 'intimates': 967, 'tucked': 968, 'help': 969, 'thigh': 970, 'romantic': 971, 'hanging': 972, 'cup': 973, 'pleats': 974, 'stomach': 975, 'height': 976, 'silhouette': 977, &quot;aren't&quot;: 978, 'solid': 979, 'polyester': 980, 'expensive': 981, 'dressing': 982, 'guess': 983, 'p': 984, 'silky': 985, 'plenty': 986, 'd': 987, 'flowing': 988, 'description': 989, 'patterns': 990, 'purchasing': 991, 'layers': 992, 'structured': 993, 'life': 994, 'poncho': 995, 'pear': 996, 'beige': 997, 'uncomfortable': 998, 'vintage': 999, 'camisole': 1000, ...,'bored': 13489} # 1001-13488 has been removed due to word limit

print(&quot;The encoding for document\n&quot;,text_train[1356],&quot;\n is : &quot;,sequences_train[1356])

The encoding for document
 This item comes from the Bottoms department and General division, and is classified under Pants. I am 36 years old. I rate this item 5 out of 5 stars. Lovely. These trousers are wonderful. the fabric is comfortable and does have a little give to it. 
 is :  [4, 6, 17, 14, 2, 78, 21, 3, 25, 22, 3, 5, 23, 15, 85, 1, 11, 445, 16, 18, 1, 20, 4, 6, 54, 12, 9, 10, 19, 36, 76, 1, 93, 189, 1410, 76, 28, 7, 284, 26, 50, 63, 148, 600, 41, 3478, 1, 1314, 412, 405, 28, 7, 195, 1, 682, 99, 111, 65, 2, 338, 168, 9, 195, 194, 2135, 3, 1, 36, 111, 573, 474, 82, 3, 41, 73, 412, 319, 63, 257, 156, 13, 40, 26, 396, 27, 2, 361, 672, 9, 2, 40, 28, 2, 118, 2, 190, 85, 47, 74, 2528, 24, 64, 95, 2, 269, 692, 26, 153, 69, 57, 215, 826, 13, 167, 76, 321, 3, 362, 2, 346]
</code></pre>
<p>But the encoding is not right when I checked this particular sentence for example. What have I not done right? Thanks a lot.</p>
",Preprocessing of the text & Tokenization,incorrect sentence sequencing tokenization using data called woman e commerce clothing review applying idea combining feature text nlp learning encoding right checked particular sentence example done right thanks lot
Count the number of times a group of words appear in a text,"<p>I have 4 lists of words that categorise something and a tokenised text by word.</p>
<pre><code>animals = [&quot;cat&quot;, &quot;dog&quot;, &quot;fish&quot;]
colours = [&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;]
food = [&quot;pasta&quot;, &quot;chips&quot;, &quot;beef&quot;]
sport = [&quot;football&quot;, &quot;basketball&quot;, &quot;tennis&quot;]

text = [&quot;Once&quot;,&quot;upon&quot;,&quot;a&quot;,&quot;time&quot;,.......]
</code></pre>
<p>I would like to count the number of occurrences of the words in these lists in a certain text but as a sum of the words for each list. Therefore the results would show an occurrence of 10 animal words, 20 colour words, 6 food words and 13 sport words across the whole text.</p>
<p>The data I'm actually working on is quite large, so anything that works quickly is required.</p>
<p>Thanks for any help!</p>
",Preprocessing of the text & Tokenization,count number time group word appear text list word categorise something tokenised text word would like count number occurrence word list certain text sum word list therefore result would show occurrence animal word colour word food word sport word across whole text data actually working quite large anything work quickly required thanks help
Python syntax error in list comprehension on string for Lemmatization,"<p>I'm trying to only perform Lemmatization on words in a string that have more than 4 letters. The desired output from the following code should be 'us american', but I received an invalid syntax error.</p>
<pre><code>import nltk
from nltk.tokenize import TweetTokenizer
lemmatizer = nltk.stem.WordNetLemmatizer()
w_tokenizer = TweetTokenizer()    

wd = w_tokenizer.tokenize(('us americans'))
    [lemmatizer.lemmatize(w) for w in wd if len(w)&gt;4 else wd for wd in w]
</code></pre>
",Preprocessing of the text & Tokenization,python syntax error list comprehension string lemmatization trying perform lemmatization word string letter desired output following code u american received invalid syntax error
NLP Lemmatizer removes &#39;s&#39; from &#39;us&#39;,"<p>I'm trying to use the following lemmatize function, but when inputting text with &quot;us&quot;, the function removes the &quot;s&quot;. How to prevent that?</p>
<pre><code>lemmatizer = nltk.stem.WordNetLemmatizer()
w_tokenizer = TweetTokenizer()
def lemmatize_text(text):
    return [(lemmatizer.lemmatize(w)) for w in \
                                     w_tokenizer.tokenize((text))]
lemmatize_text('us')   
</code></pre>
",Preprocessing of the text & Tokenization,nlp lemmatizer remove u trying use following lemmatize function inputting text u function remove prevent
Tokenization of Compound Words not Working in Quanteda,"<p>I'm trying to create a dataframe containing specific keywords-in-context using the kwic() function, but unfortunately, I'm running into some error when attempting to tokenize the underlying dataset.</p>
<p>This is the subset of the dataset I'm using as a reproducible example:</p>
<pre><code>test_cluster &lt;- speeches_subset %&gt;%
  filter(grepl('Schwester Agnes',
                speechContent,
                ignore.case = TRUE))

test_corpus &lt;- corpus(test_cluster,
                      docid_field = &quot;id&quot;,
                      text_field = &quot;speechContent&quot;)
</code></pre>
<p>Here, <code>test_cluster</code> contains six observations of 12 variables, that is, six rows in which the column <code>speechContent</code> contains the compound word &quot;Schwester Agnes&quot;. <code>test_corpus</code> transforms the underlying data into a <code>quanteda</code> corpus object.</p>
<p>When I then run the following code, I would expect, first, the content of the <code>speechContent</code> variables to be tokenized, and due to <code>tokens_compound</code>, the compound word &quot;Schwester Agnes&quot; to be tokenized as such. In a second step, I would expect the kwic() function to return a dataframe consisting of six rows, with the <code>keyword</code> variable including the compound word &quot;Schwester Agnes&quot;. Instead, however, kwic() returns an empty dataframe containing 0 observations of 7 variables. I think this is because of some mistake I'm making with <code>tokens_compound()</code>, but I'm not sure... Any help would be greatly appreciated!</p>
<pre><code>test_tokens &lt;- tokens(test_corpus, 
                      remove_punct = TRUE,
                      remove_numbers = TRUE) %&gt;%
  tokens_compound(pattern = phrase(&quot;Schwester Agnes&quot;))

test_kwic &lt;- kwic(test_tokens,
                  pattern = &quot;Schwester Agnes&quot;,
                  window = 5)
</code></pre>
<p>EDIT: I realize that the examples above are not easily reproducible, so please refer to the reprex below:</p>
<pre><code>speech = c(&quot;This is the first speech. Many words are in this speech, but only few are relevant for my research question. One relevant word, for example, is the word stack overflow. However there are so many more words that I am not interested in assessing the sentiment of&quot;, &quot;This is a second speech, much shorter than the first one. It still includes the word of interest, but at the very end. stack overflow.&quot;, &quot;this is the third speech, and this speech does not include the word of interest so I'm not interested in assessing this speech.&quot;)

data &lt;- data.frame(id=1:3, 
                   speechContent = speech)

test_corpus &lt;- corpus(data,
                      docid_field = &quot;id&quot;,
                      text_field = &quot;speechContent&quot;)

test_tokens &lt;- tokens(test_corpus, 
                      remove_punct = TRUE,
                      remove_numbers = TRUE) %&gt;%
  tokens_compound(pattern = c(&quot;stack&quot;, &quot;overflow&quot;))

test_kwic &lt;- kwic(test_tokens,
                  pattern = &quot;stack overflow&quot;,
                  window = 5)
</code></pre>
",Preprocessing of the text & Tokenization,tokenization compound word working quanteda trying create dataframe containing specific keywords context using kwic function unfortunately running error attempting tokenize underlying dataset subset dataset using reproducible example contains six observation variable six row column contains compound word schwester agnes transforms underlying data corpus object run following code would expect first content variable tokenized due compound word schwester agnes tokenized second step would expect kwic function return dataframe consisting six row variable including compound word schwester agnes instead however kwic return empty dataframe containing observation variable think mistake making sure help would greatly appreciated edit realize example easily reproducible please refer reprex
Tell `kwic()` to ignore stopwords when situating keywords in context?,"<p>I once again have a question about the <code>kwic()</code> function from the <code>quanteda</code> package. I want to extract the five words around a specific keyword (in the example below, these are &quot;stack overflow&quot; and &quot;radio star&quot;). However, after removing stopwords in the tokenization process, <code>kwic()</code> does not return the actual window of 5 words pre and post the keyword, but less words than that. Is there a way to tell <code>kwic()</code> to ignore stopwords when counting keywords in context?</p>
<p>Reprex below:</p>
<pre><code>library(quanteda)

speech = c(&quot;This is the first speech. Many words are in this speech, but only few are relevant for my research question. One relevant word, for example, is the word stack overflow. However there are so many more words that I am not interested in assessing the sentiment of. Now I am also adding a few words that would not be removed as stopwords, as follows: Maintenance, Television, Superstar, Textual Analysis. Video killed the radio star is another sentence I would like to include.&quot;, 
           &quot;This is a second speech, much shorter than the first one. It still includes the word of interest, but at the very end. stack overflow. Once again adding some non-stopwords: Maintenance, television, superstar, textual analysis. Video killed the radio star is another sentence I would like to include.&quot;, 
           &quot;Finally, this is the third speech, and this speech does not include the word of interest so I'm not interested in assessing this speech. Here are some more non-stopwords: Maintenance, television, superstar, textual analysis&quot;)

data &lt;- data.frame(id=1:3, 
                   speechContent = speech)

test_corpus &lt;- corpus(data,
                      docid_field = &quot;id&quot;,
                      text_field = &quot;speechContent&quot;)

test_tokens &lt;- tokens(test_corpus, 
                      remove_punct = TRUE,
                      remove_numbers = TRUE) %&gt;%
  tokens_remove(stopwords(&quot;en&quot;), padding = TRUE) %&gt;%
  tokens_compound(pattern = phrase(c(&quot;stack overflow*&quot;, &quot;radio star*&quot;)),
                  concatenator = &quot; &quot;)

test_kwic &lt;- kwic(test_tokens,
                  pattern = c(&quot;stack overflow&quot;, &quot;radio star&quot;),
                  window = 5)
</code></pre>
",Preprocessing of the text & Tokenization,tell ignore stopwords situating keywords context question function package want extract five word around specific keyword example stack overflow radio star however removing stopwords tokenization process doe return actual window word pre post keyword le word way tell ignore stopwords counting keywords context reprex
Removing Custom-Defined Words from List (Part II)- Python,"<p>This is a continuation of my previous thread: <a href=""https://stackoverflow.com/questions/72015653/removing-custom-defined-words-from-list-python"">Removing Custom-Defined Words from List - Python</a></p>
<p>I have a df as such:</p>
<pre><code>df = pd.DataFrame({'PageNumber': [175, 162, 576], 'new_tags': [['flower architecture people'], ['hair red bobbles'], ['sweets chocolate shop']})

&lt;OUT&gt;
PageNumber   new_tags
   175       flower architecture people...
   162       hair red bobbles...
   576       sweets chocolate shop...
</code></pre>
<p>And another df (which will act as the reference df (see more below)):</p>
<pre><code>top_words= pd.DataFrame({'ID': [1,2,3], 'tag':['flower, people, chocolate']})

&lt;OUT&gt;
   ID      tag
   1       flower
   2       people
   3       chocolate
</code></pre>
<p>I'm trying to remove values in a list in a df based on the values of another df. The output I wish to gain is:</p>
<pre><code>&lt;OUT&gt; df
PageNumber   new_tags
   175       flower people
   576       chocolate
</code></pre>
<p>I've tried the inner join method: <a href=""https://stackoverflow.com/questions/50655370/filtering-the-dataframe-based-on-the-column-value-of-another-dataframe"">Filtering the dataframe based on the column value of another dataframe</a>, however no luck unfortunately.</p>
<p>So I have resorted to tokenizing all tags in both of the df columns and trying to loop through each and retaining only the values in the reference df. Currently, it returns empty lists...</p>
<pre><code>df['tokenised_new_tags'] = filtered_new[&quot;new_tags&quot;].astype(str).apply(nltk.word_tokenize)
topic_words['tokenised_top_words']= topic_words['tag'].astype(str).apply(nltk.word_tokenize)
df['top_word_tokens'] = [[t for t in tok_sent if t in topic_words['tokenised_top_words']] for tok_sent in df['tokenised_new_tags']]

</code></pre>
<p>Any help is much appreciated - thanks!</p>
",Preprocessing of the text & Tokenization,removing custom defined word list part ii python continuation previous thread however luck unfortunately resorted tokenizing tag df column trying loop retaining value reference df currently return empty list help much appreciated thanks
Topic model for each row in dataframe,"<p>I have a <em>subset</em> of a dataframe that looks like (note, the new_tags are <em>not</em> exhaustively illustrated here):</p>
<pre><code>df = pd.DataFrame({'PageNumber': [175, 162, 576], 'new_tags': [['flower architecture people'], ['hair red bobbles'], ['sweets chocolate shop']})

&lt;OUT&gt;
PageNumber   new_tags
   175       flower architecture people...
   162       hair red bobbles...
   576       sweets chocolate shop...
</code></pre>
<p><strong>I am hoping to iterate through each row (also termed a document) and conduct a topic model then extract the top 20 words from each topic into a csv. I am using Gensim.</strong></p>
<p>I have the code that works for conducting the topic model, but I am unsure how to do this by row. The issue I think I am having is that when converting the df into a dictionary it doesn't allow me to subset it for the loop.</p>
<p><em><strong>Here is my progress at the moment:</strong></em></p>
<p><em><strong>First, I want to tokenize and lemmatize the tags.</strong></em></p>
<pre><code>nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])

def lemmatization(texts,allowed_postags=['NOUN', 'ADJ']): 
       output = []
       for sent in texts:
             doc = nlp(sent) 
             output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags ])
       return output

#convert column to list
text_list=df['new_tags'].tolist()

#lemmatisation and tokenisation
tokenized_tags = lemmatization(text_list)
</code></pre>
<p><em><strong>Next, I define a function to conduct a topic model and then write that to the csv.</strong></em></p>
<pre><code>i = 1
def topic_model(tokenized_tags):
    ''' this function is used to conduct topic modelling for each grid/document '''
    for row in tokenized_tags:
    #convert tokenized lists into dictionary
        dictionary = corpora.Dictionary(row)
        #create document term matrix
        doc_term_matrix = [dictionary.doc2bow(tag) for tag in row]
        #initialise topic model from gensim
        LDA = gensim.models.ldamodel.LdaModel
        #build and train topic model
        lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=40, random_state=100, chunksize=400, passes=50,iterations=100)
        #write top 20 words from each document as csv
        top_words_per_topic = []
        for t in range(lda_model.num_topics):
            top_words_per_topic.extend([(t, ) + x for x in lda_model.show_topic(t, topn = 20)])
        #return csv - write first row then append subsequent rows
    return pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(&quot;top_words.csv&quot;, mode='a', index = False, header=False)
    i+=1
topic_model(tokenized_tags)
</code></pre>
<p>As a side note, is there a way to work out the optimal parameters e.g. coherence value for each document after running the topic model and somehow adjust the model to take in the best value?</p>
<p>Any help is very much appreciated! Thanks!</p>
<hr />
<p><strong>UPDATED CODE:</strong>
I've updated the function so I'm passing the tokenized version of the df and wanting to apply a topic model to each row and append that onto the df as a new column. How will I be able to do this?</p>
<pre><code>tokens = central_edi_posts_grouped['new_tags'].astype(str).apply(nltk.word_tokenize)

def topic_model(central_edi_posts_grouped):
    ''' this function is used to conduct topic modelling for each grid/document '''
    #convert tokenized lists into dictionary
    dictionary = corpora.Dictionary(tokens)
    #create document term matrix
    doc_term_matrix = [dictionary.doc2bow(tag) for tag in tokens]
    #initialise topic model from gensim
    LDA = gensim.models.ldamodel.LdaModel
    #build and train topic model
    lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=8, random_state=100,
                chunksize=400, passes=50,iterations=100)
    #let's check out the coheence number 
    from gensim.models.coherencemodel import CoherenceModel
    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokens, dictionary=dictionary , coherence='c_v')
    coherence_lda = coherence_model_lda.get_coherence()

    #write top 20 words from each document as csv
    top_words_per_topic = []
    for t in range(lda_model.num_topics):
        top_words_per_topic.extend([(t, ) + x for x in lda_model.show_topic(t, topn = 20)])
    #return csv - write first row then append subsequent rows
    pd.DataFrame(top_words_per_topic, coherence_lda, columns=['Topic', 'Word', 'P', 'Coherence_value']).to_csv(&quot;top_words_loop_test.csv&quot;, mode='a', index = False, header=False)
    return coherence_lda


df['new_col'] = df['new_tags'].apply(lambda tokens: topic_model((tokens)))

</code></pre>
",Preprocessing of the text & Tokenization,topic model row dataframe subset dataframe look like note new tag exhaustively illustrated hoping iterate row also termed document conduct topic model extract top word topic csv using gensim code work conducting topic model unsure row issue think converting df dictionary allow subset loop progress moment first want tokenize lemmatize tag next define function conduct topic model write csv side note way work optimal parameter e g coherence value document running topic model somehow adjust model take best value help much appreciated thanks updated code updated function passing tokenized version df wanting apply topic model row append onto df new column able
NLP preprocessing remove all words in string not found in my list,"<p>I've made a list of <code>important_words</code> and a have a dataframe that has a column <code>df['reviews']</code>, that has one <code>string</code> of <code>review text</code> per row (thousands of rows). I want to update the 'reviews' by removing everything that is not in the <code>important_words</code> <code>list</code> from the string, like the opposite of having <code>stop words</code>, so that I am only left with the <code>important_words</code> per every <code>review</code> (row) in the df.</p>
<p>Also, later in my starter code I tokenize and normalize the column of <code>df[reviews]</code>, it seems like applying to this column should make everything easier, since punctuation removal and lowercasing has also been applied. I'll try which ever method someone can share, thanks.</p>
<pre><code>important_words = [actor, action, awesome]

   df['reviews'][1] = 'The actor, in the action movie was awesome'
   df['reviews'][2] = 'The action movie was not good'
   ....
   df['tokenized_normalized_reviews'][1] = [the,actor,in,the,action,movie,was,awesome]
   df['tokenized_normalized_reviews'][2] = [the, action, movie, was, not, good]

I want: 
df['review_important_words'][1] = 'actor, action, awesome' 
df['review_important_words'][2] = 'action' 
&lt; either str or applied to the tokenized column&gt;
 
</code></pre>
",Preprocessing of the text & Tokenization,nlp preprocessing remove word string found list made list dataframe ha column ha one per row thousand row want update review removing everything string like opposite left per every row df also later starter code tokenize normalize column seems like applying column make everything easier since punctuation removal lowercasing ha also applied try ever method someone share thanks
can some one explain to me the horrible model performance I&#39;m seeing in a text classifier?,"<p>I'm fitting data to a simple model for a text classifier NLP project I'm working on.
My results are, in a word, bad:
<em><strong>loss: -268148.5000</strong></em> - accuracy: 0.0762 - <em><strong>val_loss: -933167.1250</strong></em> - val_accuracy: 0.0784</p>
<p>What could be the cause of the loss &amp; val_loss values being so high?!</p>
<p>My model, just in case it helps, is this:</p>
<pre><code>#basic model
model = Sequential()

model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))
model.add(Flatten())
model.add(Dense(2, activation='relu'))
model.add(Dense(1,activation=&quot;sigmoid&quot;))
model.summary()

#basic model compile &amp; run
model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])
</code></pre>
<p>Update: the tokenizing:</p>
<pre><code>vocab_size = 10000
embedding_dim = 50
max_length = 100
padding_type='post'
trunc_type='post'
oov_tok = &quot;&lt;OOV&gt;&quot;


tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(lyrics_train)

word_index = tokenizer.word_index
print(len(word_index))
print(word_index)


lyrics_train = tokenizer.texts_to_sequences(lyrics_train)
lyrics_train_final = pad_sequences(lyrics_train,maxlen=max_length, padding=padding_type, 
                       truncating=trunc_type)

lyrics_test = tokenizer.texts_to_sequences(lyrics_test)
lyrics_test_final = pad_sequences(lyrics_test,maxlen=max_length, 
                               padding=padding_type, truncating=trunc_type)
</code></pre>
",Preprocessing of the text & Tokenization,one explain horrible model performance seeing text classifier fitting data simple model text classifier nlp project working result word bad loss accuracy val loss val accuracy could cause loss val loss value high model case help update tokenizing
Remove Stop words from multi-lingual Text,"<p>I am running textual and sentiment analysis on multi-lingual text files from the healthcare sector, and I want to remove stopwords from all the languages at once. I don't want to write the name of every language in the code to remove the stopwords. Is there any way I can do it fast?</p>
<p>Here is my code: The total number of files is 596</p>
<pre><code>files = list.files(path = getwd(), pattern = &quot;txt&quot;, all.files = FALSE,
                   full.names = TRUE, recursive = TRUE)
txt = {}
for (i in 1:596) 
  try( 
    {
      txt[[i]] &lt;- readLines(files[i], warn = FALSE) 
  
  filename &lt;- txt[[i]]
  filename &lt;- trimws(filename)
  corpus &lt;- iconv(filename, to = &quot;utf-8&quot;)
  corpus &lt;- Corpus(VectorSource(corpus))
  
  # Clean Text
  corpus &lt;- tm_map(corpus, removePunctuation)
  corpus &lt;- tm_map(corpus, removeNumbers)
  cleanset &lt;- tm_map(corpus, removeWords, stopwords(&quot;english&quot;))
  cleanset &lt;- tm_map(cleanset, removeWords, stopwords(&quot;spanish&quot;))
  cleanset &lt;- tm_map(cleanset, content_transformer(tolower))
  cleanset &lt;- tm_map(cleanset, stripWhitespace)
  
  # Remove spaces and newlines
  cleanset &lt;- tm_map(&quot;\n&quot;, &quot; &quot;, cleanset)
  cleanset &lt;- tm_map(&quot;^\\s+&quot;, &quot;&quot;, cleanset)
  cleanset &lt;- tm_map(&quot;\\s+$&quot;, &quot;&quot;, cleanset)
  cleanset &lt;- tm_map(&quot;[ |\t]+&quot;, &quot; &quot;, cleanset)

  }, silent = TRUE) 
</code></pre>
",Preprocessing of the text & Tokenization,remove stop word multi lingual text running textual sentiment analysis multi lingual text file healthcare sector want remove stopwords language want write name every language code remove stopwords way fast code total number file
Remove Words with less than Certain Character Lengths plus Noise Reduction before Tokenization,"<p>I have the following data frame</p>
<pre><code>report &lt;- data.frame(Text = c(&quot;unit 1 crosses the street&quot;, 
       &quot;driver 2 was speeding and saw driver# 1&quot;, 
        &quot;year 2019 was the year before the pandemic&quot;,
        &quot;hey saw       hei hei in        the    wood&quot;,
        &quot;hello: my kityy! you are the best&quot;), id = 1:5)
report 
                                         Text id
1                   unit 1 crosses the street  1
2     driver 2 was speeding and saw driver# 1  2
3  year 2019 was the year before the pandemic  3
4 hey saw       hei hei in        the    wood  4
5           hello: my kityy! you are the best  5
</code></pre>
<p>From a previous coding help, we can remove stop words using the following code.</p>
<pre><code>report$Text &lt;- gsub(paste0('\\b',tm::stopwords(&quot;english&quot;), '\\b', 
                          collapse = '|'), '', report$Text)
report
                                    Text id
1                 unit 1 crosses  street  1
2      driver 2  speeding  saw driver# 1  2
3            year 2019   year   pandemic  3
4 hey saw       hei hei             wood  4
5                 hello:  kityy!    best  5
</code></pre>
<p>I want to remove words less than certain character length (for example, want to remove words less than 4 characters such as <code>hei</code> and <code>hey</code>). Plus need to remove manual stop words (for example, <code>saw</code> and <code>kitty</code>) and common noises (whitespaces, numbers, and punctuations) before tokenization. The final outcome would be:</p>
<pre><code>                                    Text id
1                   unit crosses  street  1
2                driver speeding  driver  2
3                     year year pandemic  3
4                                   wood  4
5                             hello best  5
</code></pre>
<p>A similar question regarding noise and manual stop words is posted <a href=""https://stackoverflow.com/questions/71971099/remove-numbers-punctuations-white-spaces-before-tokenization/71971183"">here</a>.</p>
",Preprocessing of the text & Tokenization,remove word le certain character length plus noise reduction tokenization following data frame previous coding help remove stop word using following code want remove word le certain character length example want remove word le character plus need remove manual stop word example common noise whitespaces number punctuation tokenization final outcome would similar question regarding noise manual stop word posted href
How do I remove nonsensical or incomplete words from a corpus?,"<p>I am using some text for some NLP analyses. I have cleaned the text taking steps to remove non-alphanumeric characters, blanks, duplicate words and stopwords, and also performed stemming and lemmatization:</p>

<pre><code>from nltk.tokenize import word_tokenize
import nltk.corpus
import re
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
import pandas as pd

data_df = pd.read_csv('path/to/file/data.csv')

stopwords = nltk.corpus.stopwords.words('english') 

stemmer = SnowballStemmer('english')
lemmatizer = WordNetLemmatizer()

# Function to remove duplicates from sentence
def unique_list(l):
    ulist = []
    [ulist.append(x) for x in l if x not in ulist]
    return ulist

for i in range(len(data_df)):

    # Convert to lower case, split into individual words using word_tokenize
    sentence = word_tokenize(data_df['O_Q1A'][i].lower()) #data['O_Q1A'][i].split(' ')

    # Remove stopwords
    filtered_sentence = [w for w in sentence if not w in stopwords]

    # Remove duplicate words from sentence
    filtered_sentence = unique_list(filtered_sentence)

    # Remove non-letters
    junk_free_sentence = []
    for word in filtered_sentence:
        junk_free_sentence.append(re.sub(""[^\w\s]"", "" "", word)) # Remove non-letters, but don't remove whitespaces just yet
        #junk_free_sentence.append(re.sub(""/^[a-z]+$/"", "" "", word)) # Take only alphabests

    # Stem the junk free sentence
    stemmed_sentence = []
    for w in junk_free_sentence:
        stemmed_sentence.append(stemmer.stem(w))

    # Lemmatize the stemmed sentence
    lemmatized_sentence = []
    for w in stemmed_sentence:
        lemmatized_sentence.append(lemmatizer.lemmatize(w))

    data_df['O_Q1A'][i] = ' '.join(lemmatized_sentence)
</code></pre>

<p>But when I display the top 10 words (according to some criteria), I still get some junk like:</p>

<pre><code>ask
much
thank
work
le
know
via
sdh
n
sy
t
n t
recommend
never
</code></pre>

<p>Out of these top 10 words, only 5 are sensible (<code>ask</code>, <code>know</code>, <code>recommend</code>, <code>thank</code> and <code>work</code>). What more do I need to do to retain only meaningful words?</p>
",Preprocessing of the text & Tokenization,remove nonsensical incomplete word corpus using text nlp analysis cleaned text taking step remove non alphanumeric character blank duplicate word stopwords also performed stemming lemmatization display top word according criterion still get junk like top word sensible need retain meaningful word
Tokenizing an HTML document,"<p>I have an HTML document and I'd like to tokenize it using spaCy while keeping HTML tags as a single token.
Here's my code:</p>

<pre><code>import spacy
from spacy.symbols import ORTH
nlp = spacy.load('en', vectors=False, parser=False, entity=False)

nlp.tokenizer.add_special_case(u'&lt;i&gt;', [{ORTH: u'&lt;i&gt;'}])
nlp.tokenizer.add_special_case(u'&lt;/i&gt;', [{ORTH: u'&lt;/i&gt;'}])

doc = nlp('Hello, &lt;i&gt;world&lt;/i&gt; !')

print([e.text for e in doc])
</code></pre>

<p>The output is:</p>

<pre><code>['Hello', ',', '&lt;', 'i', '&gt;', 'world&lt;/i', '&gt;', '!']
</code></pre>

<p>If I put spaces around the tags, like this:</p>

<pre><code>doc = nlp('Hello, &lt;i&gt; world &lt;/i&gt; !')
</code></pre>

<p>The output is as I want it:</p>

<pre><code>['Hello', ',', '&lt;i&gt;', 'world', '&lt;/i&gt;', '!']
</code></pre>

<p>but I'd like avoiding complicated pre-processing to the HTML.</p>

<p>Any idea how can I approach this?</p>
",Preprocessing of the text & Tokenization,tokenizing html document html document like tokenize using spacy keeping html tag single token code output put space around tag like output want like avoiding complicated pre processing html idea approach
I&#39;m trying to scrape most frequent words in a web page and filter out stop words,"<p>My code works with scraping the most frequent words but once I introduce the code to try and remove or filter out any stop words my output is all funky.
Here is my full code</p>
<pre><code>nltk.download('stopwords')

stopwords = stopwords.words('english')
print(stopwords)

def start(url):
    worldlist = []
    source_code = requests.get(url).text
    soup = BeautifulSoup(source_code, 'html.parser')
    for each_text in soup.findAll('div', {'class': 'centerPar'}):
        content = each_text.text
        words = content.lower().split()
        for each_word in words:
            worldlist.append(each_word)
        clean_wordlist(worldlist)

def clean_wordlist(wordlist):
    clean_list = []
    for word in wordlist:
        symbols = &quot;!@#$%^&amp;*()_-+={[}]|\;:\&quot;&lt;&gt;?/., &quot;
        for i in range(len(symbols)):
            word = word.replace(symbols[i], '')
        if len(word) &gt; 0:
            clean_list.append(word)
    filter_list(clean_list)

def filter_list(clean_list):
    filtered_list = []
    for word in clean_list:
        for i in range(len(stopwords)):
            word = word.replace(stopwords[i], '')
        if len(word) &gt; 0:
            filtered_list.append(word)
    create_dict(filtered_list)

def create_dict(filtered_list):
    word_count = {}
    for word in filtered_list:
        if word in word_count:
            word_count[word] += 1
        else:
            word_count[word] = 1
    c = Counter(word_count)
    top = c.most_common(20)
    print(top)

if __name__ != '__main__':
    pass
else:
    url = &quot;https://www.pwc.com/us/en/about-us/purpose-and-values.html&quot;
    start(url)
</code></pre>
<p>I'm having issues with the filter_list function</p>
<p>my out put is as follows:</p>
<pre><code>[('n', 82), ('e', 23), ('f', 17), ('r', 14), ('ce', 14), ('wk', 14), ('pwc', 13), ('gl', 13), ('c', 12), ('wh', 12), ('u', 12), ('l', 11), ('cn', 10), ('peple', 10), ('purpe', 9), ('’', 9), ('ke', 7), ('en', 7), ('h', 7), ('p', 7)]
</code></pre>
",Preprocessing of the text & Tokenization,trying scrape frequent word web page filter stop word code work scraping frequent word introduce code try remove filter stop word output funky full code issue filter list function put follows
Multilingual NLTK for POS Tagging and Lemmatizer,"<p>Recently I approached to the NLP and I tried to use <a href=""http://www.nltk.org"" rel=""noreferrer"">NLTK</a> and <a href=""http://textblob.readthedocs.org"" rel=""noreferrer"">TextBlob</a> for analyzing texts. I would like to develop an app that analyzes reviews made by travelers and so I have to manage a lot of texts written in different languages. I need to do two main operations: POS Tagging and lemmatization. I have seen that in NLTK there is a possibility to choice the the right language for sentences tokenization like this:</p>

<pre><code>tokenizer = nltk.data.load('tokenizers/punkt/PY3/italian.pickle')
</code></pre>

<p>I haven't found the the right way to set the language for POS Tagging and Lemmatizer in different languages yet. How can I set the correct corpora/dictionary for non-english texts such as Italian, French, Spanish or German? I also see that there is a possibility to import the ""TreeBank"" or ""WordNet"" modules, but I don't understand how I can use them.  Otherwise, where can I find the respective corporas?</p>

<p>Can you give me some suggestion or reference? Please take care that I'm not an expert of NLTK.</p>

<p>Many Thanks. </p>
",Preprocessing of the text & Tokenization,multilingual nltk po tagging lemmatizer recently approached nlp tried use nltk textblob analyzing text would like develop app analyzes review made traveler manage lot text written different language need two main operation po tagging lemmatization seen nltk possibility choice right language sentence tokenization like found right way set language po tagging lemmatizer different language yet set correct corpus dictionary non english text italian french spanish german also see possibility import treebank wordnet module understand use otherwise find respective corporas give suggestion reference please take care expert nltk many thanks
Identify strings having words from two different lists,"<p>I have a <strong>dataframe</strong> with three columns like this:</p>
<pre><code>index   string                                         Result
1       The quick brown fox jumps over the lazy dog 
2       fast and furious was a good movie   
</code></pre>
<p>and i have <strong>two lists</strong> of words like this:</p>
<pre><code>list1   [&quot;over&quot;, &quot;dog&quot;, &quot;movie&quot;]
list2   [&quot;quick&quot;, &quot;brown&quot;, &quot;sun&quot;, &quot;book&quot;]
</code></pre>
<p>I want to identify strings that have at least one word from list1 <strong>AND</strong> at least one word from list2, such that the result will be as follows:</p>
<pre><code>index   string                                      Result
1   The quick brown fox jumps over the lazy dog     TRUE
2   fast and furious was a good movie               FALSE
</code></pre>
<p><strong>Explanation</strong>: The first sentence has words from both lists and so the result is TRUE. The second sentence has only one word from list1 and so it has a result of False.</p>
<p>Can we do that with python? I used search techniques from NLTK but i don't know how to combine results from the two lists. Thanks</p>
",Preprocessing of the text & Tokenization,identify string word two different list dataframe three column like two list word like want identify string least one word list least one word list result follows explanation first sentence ha word list result true second sentence ha one word list ha result false python used search technique nltk know combine result two list thanks
keras pad_sequence and Tokenizer,"<br>
i learn on kaggle dataset <a href=""https://www.kaggle.com/datasets/kazanova/sentiment140"" rel=""nofollow noreferrer"">Here</a> to practice on nlp i have an error when i tokenize the tweets and go to padding them i got an error i search for an solution but i don't get answer
<br>
<pre><code># Get tha max Number Of Word In Tweets
texts = df['text']
LENGTH = texts.apply(lambda p:len(p.split()))

x = df ['text']
y = df['target']
x_train,x_test , y_train,y_test =train_test_split(x,y,test_size=.30,random_state=41)


tokenize = Tokenizer()
tokenize.fit_on_texts(x)
x = tokenize.texts_to_sequences(x)

print('start padding ...')

# Padding Tweets To Be The Same Length
x = pad_sequences(x ,maxlen=LENGTH)
</code></pre>
<p>i got this error</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/tmp/ipykernel_34/2607522322.py in &lt;module&gt;
      8 
      9 # Padding Tweets To Be The Same Length
---&gt; 10 x = pad_sequences(x ,maxlen=LENGTH)

/opt/conda/lib/python3.7/site-packages/keras/preprocessing/sequence.py in pad_sequences(sequences, maxlen, dtype, padding, truncating, value)
    152   return sequence.pad_sequences(
    153       sequences, maxlen=maxlen, dtype=dtype,
--&gt; 154       padding=padding, truncating=truncating, value=value)
    155 
    156 keras_export(

/opt/conda/lib/python3.7/site-packages/keras_preprocessing/sequence.py in pad_sequences(sequences, maxlen, dtype, padding, truncating, value)
     83                          .format(dtype, type(value)))
     84 
---&gt; 85     x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)
     86     for idx, s in enumerate(sequences):
     87         if not len(s):

/opt/conda/lib/python3.7/site-packages/numpy/core/numeric.py in full(shape, fill_value, dtype, order, like)
    340         fill_value = asarray(fill_value)
    341         dtype = fill_value.dtype
--&gt; 342     a = empty(shape, dtype, order)
    343     multiarray.copyto(a, fill_value, casting='unsafe')
    344     return a

TypeError: 'Series' object cannot be interpreted as an integer

</code></pre>
",Preprocessing of the text & Tokenization,kera pad sequence tokenizer learn kaggle dataset practice nlp error tokenize tweet go padding got error search solution get answer got error
Analysis of text using Gunning Fox index,"<p>While doing Analysis of readability using Gunning Fox index-. I have to calculate following values</p>
<ol>
<li>Average Sentence Length = the number of words / the number of sentences</li>
<li>Percentage of Complex words = the number of complex words / the number of words</li>
<li>Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)</li>
</ol>
<p>I want to know whether the number of words  will be calculated after removing duplicates and stop words i.e. after cleaning  or just the total no of words in the text without removing any words or cleaning?</p>
<p>Thanks for help!</p>
",Preprocessing of the text & Tokenization,analysis text using gunning fox index analysis readability using gunning fox index calculate following value average sentence length number word number sentence percentage complex word number complex word number word fog index average sentence length percentage complex word want know whether number word calculated removing duplicate stop word e cleaning total word text without removing word cleaning thanks help
special characters problem after cleaning stop-words,"<p>I am new to text analysis with Python and struggling to clean my data from special characters.</p>
<p>I have survey data where one of the columns has comments. I want to analyse these comments and find the most frequent words. I try to exclude the stop words by using pandas.Series.str.replace.</p>
<p>Here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>stop_words = set(stopwords.words('english'))

# get the relevant column from the dataset:
df_comments = df.iloc[:,[-3]].dropna()

#clean it from stop words
pat = r'\b(?:{})\b'.format('|'.join(stop_words))
df['comment_without_stopwords'] = df[&quot;comment&quot;].str.replace(pat, '')
df['comment_without_stopwords'] = df['comment_without_stopwords'].str.replace(r'\s+', ' ')

# get the most frequent 20 words:
result = df['comment_without_stopwords'].str.split(expand=True).stack().value_counts(normalize=True, ascending = False).head(20)
</code></pre>
<p>But as a result I get the following characters: <code>.</code>, and <code>-</code> in my top list as can be seen below. How can I get rid of them?</p>
<pre class=""lang-py prettyprint-override""><code>staff        0.015001
need         0.009265
work         0.007942
-            0.007059
action       0.006618
project      0.005074
contract     0.005074
.            0.004853
field        0.004412
support      0.004412
employees    0.004191
projects     0.004191
HR           0.003971
time         0.003971
HQ           0.003971
needs        0.003750
field        0.003530
training     0.003530
capacity     0.003530
good         0.003530
dtype: float64
</code></pre>
",Preprocessing of the text & Tokenization,special character problem cleaning stop word new text analysis python struggling clean data special character survey data one column ha comment want analyse comment find frequent word try exclude stop word using panda series str replace code result get following character top list seen get rid
Count instances where pairs of words occur within a given distance of each other,"<p>I have two lists of words, like so:</p>
<pre><code>LIST1 = ['whisky', 'spirits', 'liqueur']
LIST2 = ['bottle', 'barrel', 'can', 'cup']
</code></pre>
<p>I also have a string of text (call the string object TEXT) that I would like to search. The end result of the search should be a count of the number of times each word in LIST1 appears in TEXT within a given distance (e.g., within 10 words) of any of the words in LIST2. I can imagine complicated methods of accomplishing this by iterating regular expression searches over both lists. But my actual LIST1 and LIST2 are quite long, and the text that I am searching is large, so iterating isn't a good option. I was hopeful that there might be a purpose built tool when I found NLTK, but unless I am missing something there is no functionality of the type I need. Is there an easy way to accomplish my task?</p>
<p>Note: I can't tell for sure, but I think my problem may be similar to the one discussed in <a href=""https://stackoverflow.com/questions/70689324/do-two-defined-groups-of-words-occur-within-25-words-of-each-other"">this unanswered post</a>.</p>
",Preprocessing of the text & Tokenization,count instance pair word occur within given distance two list word like also string text call string object text would like search end result search count number time word list appears text within given distance e g within word word list imagine complicated method accomplishing iterating regular expression search list actual list list quite long text searching large iterating good option wa hopeful might purpose built tool found nltk unless missing something functionality type need easy way accomplish task note tell sure think problem may similar one discussed href unanswered post
Tokenizing Strings without Punctuation in Python and putting punctuation back subsequently,"<p>After reading here for a while already, I have decided to make a post because I am not getting anywhere with my problem. Unfortunately, I am just a &quot;finance guy&quot; and need some help in coding with python. I have posts from social media platforms and would like to tokenize the sentences for NLP purposes (without punctuation), lemmatize the tokens and then reinstate the punctuation. For tokenizing and bringing back punctuation, I have used the following code so far:</p>
<pre><code>from nltk.tokenize import word_tokenize
</code></pre>
<p>With the <code>join_punctuation</code> function, I wanted to put the punctuation back together correctly.</p>
<pre><code>def join_punctuation(seq, characters='''!&quot;#%&amp;'()*+-.,/:;&lt;=&gt;?@[\]^_`{|}~'''):
    characters = set(characters)
    seq = iter(seq)
    current = next(seq)

    for nxt in seq:
        if nxt in characters:
            current += nxt
        else:
            yield current
            current = nxt
    yield current
</code></pre>
<p>So far, tokenizing works the way I want it to, as now I can lemmatize the words without being disturbed by punctuation. Here is an example:</p>
<pre><code>text = &quot;Today, (I) bought $GME and sold $TSLA!!&quot;
tokens = word_tokenize(text)
print(tokens): &quot;['Today', ',', '(', 'I', ')', 'bought', '$', 'GME', 'and', 'sold', '$', 'TSLA', '!', '!']&quot;
</code></pre>
<p>But when I then want to put the punctuation back together, it works perfectly fine with punctuation behind words, but not if the punctuation occurs in front of the words (like <code>&quot;( I)&quot;</code> and <code>&quot;$ GME&quot;</code> instead of <code>&quot;(I)&quot;</code> and <code>&quot;$GME&quot;</code>):</p>
<pre><code>newtext = &quot; &quot;.join(join_punctuation([w for w in tokens]))
print(newtext): &quot;Today,( I) bought $ GME and sold $ TSLA!!&quot;
</code></pre>
<p>Does anyone have an idea how to solve this? Thank you in advance!</p>
",Preprocessing of the text & Tokenization,tokenizing string without punctuation python putting punctuation back subsequently reading already decided make post getting anywhere problem unfortunately finance guy need help coding python post social medium platform would like tokenize sentence nlp purpose without punctuation lemmatize token reinstate punctuation tokenizing bringing back punctuation used following code far function wanted put punctuation back together correctly far tokenizing work way want lemmatize word without disturbed punctuation example want put punctuation back together work perfectly fine punctuation behind word punctuation occurs front word like instead doe anyone idea solve thank advance
How to use Stemming algorithm for a list of words in python,"<p>I have a word list:</p>
<pre><code>'AWS', 
'jQuery', 
'jQuery', 
'Sliding', 
'jQuery', 
'jQuery', 
'Manipulating', 
'Us!'
</code></pre>
<p>I removed common words and need to apply stemming to make the word list more clear.</p>
<p>My Code to remove common words:</p>
<pre><code>raw2 = second_headers CORPUS = Common_word_corpus  #my personal word corpus added here

corpus = [w.lower() for w in CORPUS]  
processed_H2_tag = [w for w in raw2.split(' ') if w.lower() not in corpus] 

print(processed_H2_tag)
</code></pre>
",Preprocessing of the text & Tokenization,use stemming algorithm list word python word list removed common word need apply stemming make word list clear code remove common word
Need help to remove punctuation and replace numbers for an nlp task,"<p>For example, I have a string:</p>
<pre><code>sentence = ['cracked $300 million','she\'s resolutely, smitten ', 'that\'s creative [r]', 'the market ( knowledge check : prices up!']
</code></pre>
<p>I want to remove the punctuation and replace numbers with the '£' symbol.
I have tried this but can only replace one or the other when I try to run them both.
my code is below</p>
<pre><code>import re
s =([re.sub(r'[!&quot;:$()[]\',]',' ', word) for word in sentence]) 

s= [([re.sub(r'\d+','£', word) for word in s])]
s)
</code></pre>
<p>I think the problem could be in the square brackets??
thank you!</p>
",Preprocessing of the text & Tokenization,need help remove punctuation replace number nlp task example string want remove punctuation replace number symbol tried replace one try run code think problem could square bracket thank
When to use Word2vec and bag of words?,"<p>I'm still unsure about when to use word2vec and when to rely on the bag of words. For example, if I want to develop a text clustering model that takes text as an input and outputs a cluster for each input, should I care about the word representation and use word2vec or should I rely on the bag of words and treat the input text as a document?
Please share any more reading and understanding resources with me; I'm very interested in text preprocessing and clustering and want to learn everything I can about it.</p>
<p>Furthermore, if I want to use k-Means for the clustering, should I split the data or it's okay to just work with the whole data in one?</p>
",Preprocessing of the text & Tokenization,use word vec bag word still unsure use word vec rely bag word example want develop text clustering model take text input output cluster input care word representation use word vec rely bag word treat input text document please share reading understanding resource interested text preprocessing clustering want learn everything furthermore want use k mean clustering split data okay work whole data one
Removing a custom list of stopwords for an nlp task,"<p>I have written a function to clean my text corpus, which is of the following form:</p>
<pre><code>[&quot;wild things is a suspenseful .. twists .  &quot;,
 &quot;i know it already.. film goers .  &quot;,
.....,
&quot;touchstone pictures..about it .  okay ?  &quot;]
</code></pre>
<p>which is a list with the sentences separated by commas.</p>
<p>my function is:</p>
<pre><code>def clean_sentences(sentences):  
   
    sentences = (re.sub(r'\d+','£', s) for s in sentences
 
    stopwords = ['a', 'and', 'any', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'is' , 'it']
       
    sentences = ' '.join(w for w in sentences if w not in stopwords)

    return sentences 
</code></pre>
<p>It replaces the numbers with '£' but it does not remove the stopwords.</p>
<p>Output:</p>
<pre><code>'wild things is a suspenseful thriller...

and a £ . £ rating , it\'s still watchable , just don\'t think about it .  okay ?  '
</code></pre>
<p>I dont understand why.
thank you.</p>
",Preprocessing of the text & Tokenization,removing custom list stopwords nlp task written function clean text corpus following form list sentence separated comma function replaces number doe remove stopwords output dont understand thank
Tokenize text but keep compund hyphenated words together,"<p>I am trying to clean up text using a pre-processing function. I want to remove all non-alpha characters such as punctuation and digits, but I would like to retain compound words that use a dash without splitting them (e.g. pre-tender, pre-construction).</p>
<pre><code>def preprocess(text):
  #remove punctuation
  text = re.sub('\b[A-Za-z]+(?:-+[A-Za-z]+)+\b', '-', text)
  text = re.sub('[^a-zA-Z]', ' ', text)
  text = text.split()
  text = &quot; &quot;.join(text)
  return text
</code></pre>
<p>For instance, the original text:</p>
<pre><code>&quot;Attended pre-tender meetings&quot; 
</code></pre>
<p>should be split into</p>
<pre><code>['attended', 'pre-tender', 'meeting'] 
</code></pre>
<p>rather than</p>
<pre><code>['attended', 'pre', 'tender', 'meeting']
</code></pre>
<p>Any help would be appreciated!</p>
",Preprocessing of the text & Tokenization,tokenize text keep compund hyphenated word together trying clean text using pre processing function want remove non alpha character punctuation digit would like retain compound word use dash without splitting e g pre tender pre construction instance original text split rather help would appreciated
"How can I say that if I want to return an operation on a list, but it stays the same when it comes out null?","<p>I have a list-of-list of word groups in Turkish. I want to apply stemming and I found turkishnlp package. Although it has some shortcomings, it often returns the right word. However, when I apply this to the list, I don't want the structure of my list to change and I want the words that he doesn't know to stay the same.</p>
<p>For example, I have this list:
<strong>mylist = [['yolda','gelirken','kopek', 'gördüm'],['cok', 'tatlıydı']]</strong></p>
<p>And I wrote this function:</p>
<pre><code>from trnlp import TrnlpWord
def tr_stemming(x):
    obj = TrnlpWord()
    obj.setword(x) if isinstance(x, str) else type(x)(map(tr_stemming, x))
    return obj.get_stem if isinstance(x, str) else type(x)(map(tr_stemming, x))
</code></pre>
<p>This function returns this list:</p>
<pre><code>tr_stemming(mylist)
</code></pre>
<p><strong>[['yol', 'gelir', '', 'gör'], ['', 'tatlı']]</strong></p>
<p>However, I want to get this as the output:
<strong>[['yol', 'gelir', 'kopek', 'gör'], ['cok', 'tatlı']]</strong></p>
<p>How can I update my function?
Thank you for your helps!</p>
",Preprocessing of the text & Tokenization,say want return operation list stay come null list list word group turkish want apply stemming found turkishnlp package although ha shortcoming often return right word however apply list want structure list change want word know stay example list mylist yolda gelirken kopek g rd cok tatl yd wrote function function return list yol gelir g r tatl however want get output yol gelir kopek g r cok tatl update function thank help
What is gensim&#39;s simple_preprocess alternative in Scikit learn,"<p>Using Gensim to perform LDA, I was able to do initial text preprocessing and cleanup using:</p>
<pre><code>gensim.utils.simple_preprocess(str(sentence),deacc=True)
</code></pre>
<p>It was very efficient and almost does all required forms of text cleanup in one command. Now, I am trying to learn LDA using Scikit learn and I was wondering if there is a similar way to achieve the same preprocessing using Sci-kit learn, instead of having to load both libraries.</p>
",Preprocessing of the text & Tokenization,gensim simple preprocess alternative scikit learn using gensim perform lda wa able initial text preprocessing cleanup using wa efficient almost doe required form text cleanup one command trying learn lda using scikit learn wa wondering similar way achieve preprocessing using sci kit learn instead load library
Stemming and lemming words,"<p>I have a text document i need to use stemming and Lemmatization on. I have already cleaned the data and tokenised it as well as removing stop words</p>
<p>what i need to do is take the list as an input and return a dict and the dict should have the keys 'original stem and lemmma. and the values being the nth word transformed in that way</p>
<pre><code>  snowball stemmer is defined as Stemmer()
  and WordNetLemmatizer is defined as lemmatizer()
</code></pre>
<p>heres the code ive written but it does give our an error</p>
<pre><code>def find_roots(token_list, n):
n = 2
original = tokens
stem = [ele for sub in original for idx, ele in 
enumerate(sub.split()) if idx == (n - 1)]
stem = stemmer(stem)
lemma = [ele for sub in original for idx, ele in 
enumerate(sub.split()) if idx == (n - 1)]
lemma = lemmatizer()
return 
</code></pre>
<p>Any help would be appreciated</p>
",Preprocessing of the text & Tokenization,stemming lemming word text document need use stemming lemmatization already cleaned data tokenised well removing stop word need take list input return dict dict key original stem lemmma value nth word transformed way code ive written doe give error help would appreciated
"Best stemming algorithm in NLTK, Python","<p>I am trying to stem the word tokens I get after tokenizing the data using PorterStemmer but am getting incorrect results. Which stemming algorithm would be the best one to go with?</p>
<p>Code-</p>
<pre><code>from nltk.stem import PorterStemmer

porter = PorterStemmer()
porter.stem(&quot;mobile&quot;)
</code></pre>
<p>Code Output-</p>
<pre><code>mobil
</code></pre>
<p>Expected Output-</p>
<pre><code>mobile
</code></pre>
",Preprocessing of the text & Tokenization,best stemming algorithm nltk python trying stem word token get tokenizing data using porterstemmer getting incorrect result stemming algorithm would best one go code code output expected output
"NLP stopword removal, stemming and lemmatization","<p>def clean_text(text):
# get English stopwords
english_stopwords = set(stopwords.words('english'))</p>
<pre><code># change to lower case and remove punctuation
#text = text.lower().translate(str.maketrans('', '', string.punctuation))
text = text.map(lambda x: x.lower().translate(str.maketrans('', '', string.punctuation)))

# divide string into individual words
def custom_tokenize(text):
    if not text:
        #print('The text to be tokenized is a None type. Defaulting to blank string.')
        text = ''
    return word_tokenize(text)

token = df['transcription'].apply(custom_tokenize)

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

clean_tokens = []
for tok in tokens:
    tok = tok.strip(&quot;#&quot;) 
    #tok = tok.strip() # remove space
    if tok not in english_stopwords:
        clean_tok = lemmatizer.lemmatize(tok) # lemmatizition
        clean_tok = stemmer.stem(clean_tok) # Stemming
        clean_tokens.append(clean_tok)
return &quot; &quot;.join(clean_tokens)

 22     #tok = [[tok for tok in sent if tok not in stop] for sent in text]
 23     for tok in tokens:
</code></pre>
<p>---&gt; 24         tok = tok.strip(&quot;#&quot;)
25         #tok = tok.strip() # remove space
26         if tok not in english_stopwords:</p>
<p>AttributeError: 'list' object has no attribute 'strip'</p>
<p>I have been getting this; AttributeError: 'list' object has no attribute 'strip'</p>
",Preprocessing of the text & Tokenization,nlp stopword removal stemming lemmatization def clean text text get english stopwords english stopwords set stopwords word english tok tok strip tok tok strip remove space tok english stopwords attributeerror list object ha attribute strip getting attributeerror list object ha attribute strip
How to stop Stemming from removing the letters that change the meaning of the word?,"<p>I am trying to select stemming or lemmatizing or both for NLP processing. When I do stemming, it takes away important suffixes. When I do lemmatizing, it does not reduce some words.</p>
<p>For example,</p>
<pre><code>from nltk.stem import LancasterStemmer, WordNetLemmatizer

tokens = ['carry', 'xy', 'known', 'size', 'may', 'use', 'column', 'value', 'contracting']

stemmer = LancasterStemmer()
stems = []

for wrd in tokens:
   stems.append(stemmer.stem(wrd))
print(stems)
</code></pre>
<p>This is producing the following stemming output:</p>
<pre><code>['carry', 'xy', 'known', 'siz', 'may', 'us', 'column', 'valu', 'contract']
</code></pre>
<p>where it rightly reduces <code>contracting</code> to <code>contract</code> but does not do the same for <code>use</code> or <code>size</code> or <code>value</code>.</p>
<pre><code>lem = WordNetLemmatizer()
lems []
for wrd in tokens:
   lems.append(lem.lemmatize(wrd))
print(lems)
</code></pre>
<p>This is producing the following lemmatizing output:</p>
<pre><code>['carry', 'xy', 'known', 'size', 'may', 'use', 'column', 'value', 'contracting']
</code></pre>
<p>where <code>contracting</code> is not reduced to <code>contract</code> but others are rightly captured.</p>
<p>Which would be the best option to go with if the data is either large or small?</p>
",Preprocessing of the text & Tokenization,stop stemming removing letter change meaning word trying select stemming lemmatizing nlp processing stemming take away important suffix lemmatizing doe reduce word example producing following stemming output rightly reduces doe producing following lemmatizing output reduced others rightly captured would best option go data either large small
"How to remove punctuation from tokens, when quanteda tokenizes at sentence level?","<p>It is my ultimate goal to select some sentences from a corpus which match a certain pattern &amp; perform a sentiment analysis upon these selected cutouts from the corpus. I am trying to do all of that with a current version of quanteda in R.</p>
<p>I noticed that <code>remove_punctuation</code> does not remove punctuation when <code>tokens</code> is applied at the sentence-level (<code>what = &quot;sentence&quot;</code>). When decomposing the selected sentence-tokens to word-tokens for the sentiment analysis, the word-tokens will contain punctuation such as &quot;,&quot; or &quot;.&quot;. Dictionaries are then no longer able to match on these tokens. Reproducible example:</p>
<pre><code>mypattern &lt;- c(&quot;country&quot;, &quot;honor&quot;)
#
txt &lt;- c(wash1 &lt;- &quot;Fellow citizens, I am again called upon by the voice of my country to execute the functions of its Chief Magistrate.&quot;,
         wash2 &lt;- &quot;When the occasion proper for it shall arrive, I shall endeavor to express the high sense I entertain of this distinguished honor.&quot;, 
         blind &lt;- &quot;Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat.&quot;)
#
toks &lt;- tokens_select(tokens(txt, what = &quot;sentence&quot;, remove_punct = TRUE), 
                             pattern = paste0(mypattern, collapse = &quot;|&quot;), 
                             valuetype = &quot;regex&quot;, 
                             selection = &quot;keep&quot;)
#
toks
</code></pre>
<p>For instance, the tokens in <code>toks</code> contain &quot;citizens,&quot; or &quot;arrive,&quot;. I thought about splitting the tokens back to word-tokens by <code>tokens_split(toks, separator = &quot; &quot;)</code> but <code>separator</code> does allow one input parameter only.</p>
<p>Is there a way to remove the punctuation from the sentences when tokenizing at the sentence-level?</p>
",Preprocessing of the text & Tokenization,remove punctuation token quanteda tokenizes sentence level ultimate goal select sentence corpus match certain pattern perform sentiment analysis upon selected cutout corpus trying current version quanteda r noticed doe remove punctuation applied sentence level decomposing selected sentence token word token sentiment analysis word token contain punctuation dictionary longer able match token reproducible example instance token contain citizen arrive thought splitting token back word token doe allow one input parameter way remove punctuation sentence tokenizing sentence level
Any way to remove symbols from a lemmatize word set using python,"<p>I got a lemmatize output from the below code with a output words consisting of &quot; : , ? , !, ( )&quot; symbols</p>
<p><code>output_H3 = [lemmatizer.lemmatize(w.lower(), pos=wordnet.VERB) for w in processed_H3_tag]</code></p>
<p>output :-</p>
<ul>
<li>['<strong>hide()</strong>', '<strong>show()</strong>', '<strong>methods:</strong>', 'jquery', 'slide', '<strong>elements:</strong>',
'launchedw3schools', '<strong>today!</strong>']</li>
</ul>
<p>Expected output :-</p>
<ul>
<li>['<strong>hide</strong>', '<strong>show</strong>', '<strong>methods</strong>', 'jquery', 'slide', '<strong>elements</strong>',
'launchedw3schools', '<strong>today</strong>']</li>
</ul>
",Preprocessing of the text & Tokenization,way remove symbol lemmatize word set using python got lemmatize output code output word consisting symbol output hide show method jquery slide element launchedw school today expected output hide show method jquery slide element launchedw school today
Python: How to solve merged words when extracting text from pdf?,"<p>I'm struggling with the words extraction from a set of pdf files. This files are academic papers that I downloaded from the web.</p>
<p>The data is stored in my local device, sorted by name,  following this relative path inside the project folder: './papers/data'. You can find my data <a href=""https://drive.google.com/file/d/1EP85DTjvHQHqPqeUEP9NAgixCt2kucka/view?usp=sharing"" rel=""nofollow noreferrer"">here</a>.</p>
<p>My code is executing inside a code folder in the project repo ('./code')</p>
<p>The pdf word extraction section of the code look like this:</p>
<pre><code>import PyPDF2 as pdf
from os import listdir 

#Open the files:
#I) List of files:
files_in_dir = listdir('../papers/data')
#II) Open and saving files to python objects:
papers_text_list = []
for idx in range(len(files_in_dir)):
    with open(f&quot;../papers/data/{files_in_dir[idx]}&quot;, mode=&quot;rb&quot;) as paper:
    my_pdf = pdf.PdfFileReader(paper)
    vars()[&quot;text_%s&quot; % idx] = ''
    for i in range(my_pdf.numPages):
        page_to_print = my_pdf.getPage(i)
        vars()[&quot;text_%s&quot; % idx] += page_to_print.extractText()
    papers_text_list.append(vars()[&quot;text_%s&quot; %idx])
</code></pre>
<p>The problem is that for some texts I'm geting merged words inside the python list.</p>
<pre><code>text_1.split()
</code></pre>
<blockquote>
<p>[ ... ,'examinedthee', 'ectsofdi',
'erentoutdoorenvironmentsinkindergartenchildren',
'™sPAlevel,',
'ages3',
'Œ5.The',
'ndingsrevealedthatchildren',
'‚sPAlevelhigherin',
'naturalgreenenvironmentsthaninthekindergarten',
'™soutdoorenvir-',
'onment,whichindicatesgreenenvironmentso',
'erbetteropportunities',
'forchildrentodoPA.', ...]</p>
</blockquote>
<p>While other list are imported in a correct way.</p>
<pre><code>text_0.split()
</code></pre>
<blockquote>
<p>['Urban','Forestry', '&amp;', 'Urban', 'Greening', '16',
'(2016)','76–83Contents', 'lists', 'available', 'at',
'ScienceDirect',  'Urban', 'Forestry', '&amp;', 'Urban',
'Greening', ...]</p>
</blockquote>
<p>At this point,  I thought that tokenize could solve my problem. So I give it a chance to the <code>nltk</code> module.</p>
<pre><code>from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')
doc = tokenizer,tokenize(text_1)
paper_words = [token for token in doc]
paper_words_lower = []
for token  in paper_words:
    try:
        word = token.lower()
    except TypeError:
        word = token 
    finally:
        paper_words_lower.append(word)
</code></pre>
<blockquote>
<p>['contentslistsavailableat',
'sciencedirecturbanforestry',
'urbangreening',
'journalhomepage',
'www',
'elsevier',
'com',
'locate',
'ufug',
'urbangreenspacesforchildren',
'across',
'sectionalstudyofassociationswith',
'distance',
'physicalactivity',
'screentime',
'generalhealth',
'andoverweight',
'abdullahakpinar',
'adnanmenderesüniversitesi',
'ziraatfakültesi',
'peyzajmimarl',
'bölümü',
'09100ayd',
'õn',
'turkey',
...
'sgeneralhealth',
'onlychildren',
'sagewas',
'signicantlyassociatedwiththeiroverweight',
...]</p>
</blockquote>
<p>I even tried with the <code>spacy</code> module... but the problem was still there.</p>
<p>My conclusion here is that if the problem can be solved It has to be in the pdf extracting words section. I found this <a href=""https://stackoverflow.com/questions/52009695/no-space-between-words-while-reading-and-extracting-the-text-from-a-pdf-file-in"">StackOverflow</a> related question but the solution couldn't solve my problem.</p>
<p>Why is this happening? and How can I solve it?</p>
<p><strong>PD:</strong> A paper on the list that serve as an example of trouble is <code>&quot;AKPINAR_2017_Urban green spaces for children.pdf&quot;</code>.</p>
<p>You can use the following code to import.</p>
<pre><code>import PyPDF2 as pdf
with open(&quot;AKPINAR_2017_Urban green spaces for children.pdf&quot;, mode=&quot;rb&quot;) as paper:
    my_pdf = pdf.PdfFileReader(paper)
    text = ''
    for i in range(my_pdf.numPages):
         page_to_print = my_pdf.getPage(i)
         text += page_to_print.extractText()
</code></pre>
",Preprocessing of the text & Tokenization,python solve merged word extracting text pdf struggling word extraction set pdf file file academic paper downloaded web data stored local device sorted name following relative path inside project folder paper data find data code executing inside code folder project repo code pdf word extraction section code look like problem text geting merged word inside python list examinedthee ectsofdi erentoutdoorenvironmentsinkindergartenchildren spalevel age ndingsrevealedthatchildren spalevelhigherin naturalgreenenvironmentsthaninthekindergarten soutdoorenvir onment whichindicatesgreenenvironmentso erbetteropportunities forchildrentodopa list imported correct way urban forestry urban greening content list available sciencedirect urban forestry urban greening point thought tokenize could solve problem give chance module contentslistsavailableat sciencedirecturbanforestry urbangreening journalhomepage elsevier com locate ufug urbangreenspacesforchildren across sectionalstudyofassociationswith distance physicalactivity screentime generalhealth andoverweight abdullahakpinar adnanmenderes niversitesi ziraatfak ltesi peyzajmimarl b l ayd n turkey sgeneralhealth onlychildren sagewas signicantlyassociatedwiththeiroverweight even tried module problem wa still conclusion problem solved ha pdf extracting word section found href related question solution solve problem p happening solve pd paper list serve example trouble use following code import
Search for a list of values in a nested dictionary,"<p>so i'm trying to do a search into a nested dictionary with a list of values.
My nested dictionary contains per every key the information about different products (for every single key i have one unique product).</p>
<p>Im trying to get just those keys from my dictionary that contains this three words that I have on my list.</p>
<p>The idea is that this search can be flexible in the way that I'm able to change the list adding new words (so the length of my list could change).</p>
<p>For example:</p>
<p>My list of values is:</p>
<pre><code>details = ['PARACETAMOL','100','MG']
</code></pre>
<p>And I have the next dictionary:</p>
<pre><code>dict = {{
    1:  'description': 'KITADOL  500  TABL  1 . 00 G x  18 ',
        'key_words': ['KITADOL', '500', 'TABL', '1', '.', '00', 'G', 'x', '18'],
        'molecule': ['PARACETAMOL']},
    {
    2:  'description': 'ACAMOL GOTAS  100 MG  15 ML x  1  /ML',
        'key_words': ['ACAMOL', 'GOTAS', '100', 'MG', '15', 'ML', 'x', '1', '/ML'],
        'molecule': ['PARACETAMOL']},
    {   
    3:  'description': 'PANAGESIC GOTAS  100 MG  15 ML x  1 ',
        'key_words': ['PANAGESIC', 'GOTAS', '100', 'MG', '15', 'ML', 'x', '1'],
        'molecule': ['PARACETAMOL']},
    {
    4:  'description': 'GRIPASAN DIA Y NOC CA.D 12 +CA.N 6  x  18 ',
        'key_words': ['GRIPASAN','DIA','Y','NOC','CA.D','12','+CA.N','6','x','18'],
        'molecule': ['CAFEINA','CLORFENAMINA','PARACETAMOL','PROPIFENAZONA','PSEUDOEFEDRINA']}
    }
</code></pre>
<p>So i want to get the next keys <code>3</code> and <code>4</code>.</p>
<p>I wonder if someone could help me or give some ideas.
Thanks!</p>
",Preprocessing of the text & Tokenization,search list value nested dictionary trying search nested dictionary list value nested dictionary contains per every key information different product every single key one unique product im trying get key dictionary contains three word list idea search flexible way able change list adding new word length list could change example list value next dictionary want get next key wonder someone could help give idea thanks
Getting word count in a sentence without punctuation marks NLTK python,"<p>I am trying to get the word count in a sentence with nltk in python</p>
<p>This is the code I wrote</p>
<pre><code>import nltk

data = &quot;Sample sentence, for checking. Here is an exclamation mark! Here is a question? This isn't an easy-task.&quot;

for i in nltk.sent_tokenize(data):
    print(nltk.word_tokenize(i))
</code></pre>
<p>This was the output</p>
<pre><code>['Sample', 'sentence', ',', 'for', 'checking', '.']
['Here', 'is', 'an', 'exclamation', 'mark', '!']
['Here', 'is', 'a', 'question', '?']
['This', 'is', &quot;n't&quot;, 'an', 'easy-task', '.']
</code></pre>
<p>Is there any way to remove the punctuation marks, prevent <code>isn't</code> from splitting into two words and split <code>easy-task</code> into two?</p>
<p>The answer I need is something like ths:</p>
<pre><code>['Sample', 'sentence', 'for', 'checking']
['Here', 'is', 'an', 'exclamation', 'mark']
['Here', 'is', 'a', 'question']
['This', &quot;isn't&quot;, 'an', 'easy', 'task']
</code></pre>
<p>I can kind of manage punctuation marks by using stopwords like:</p>
<pre><code>import nltk

data = &quot;Sample sentence, for checking. Here is an exclamation mark! Here is a question? This isn't an easy-task.&quot;

stopwords = [',', '.', '?', '!']

for i in nltk.sent_tokenize(data):
    for j in nltk.word_tokenize(i):
        if j not in stopwords:
            print(j, ', ', end=&quot;&quot;)
    print('\n')
</code></pre>
<p>output:</p>
<pre><code>Sample , sentence , for , checking , 

Here , is , an , exclamation , mark , 

Here , is , a , question , 

This , is , n't , an , easy-task , 
</code></pre>
<p>but this does not fix <code>isn't</code> and <code>easy-task</code>. Is there a way to do this?
Thank you</p>
",Preprocessing of the text & Tokenization,getting word count sentence without punctuation mark nltk python trying get word count sentence nltk python code wrote wa output way remove punctuation mark prevent splitting two word split two answer need something like th kind manage punctuation mark using stopwords like output doe fix way thank
Why is the length of the word_index greater than num_words?,"<p>I have a code, about text preprocessing for deep learning:</p>
<pre><code>from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words = 10000)
tokenizer.fit_on_texts(X)
tokenizer.word_index
</code></pre>
<p>but when I check the length of tokenizer.word_index, safe in the knowledge to get 10000, I get 13233.The length of X is equal to 11541(a dataframe column containing 11541, if it matters to know, however). So my question arises: which is vocabulary size? num_words or the length of word_index? It seems I have confused! Any helps appreciated.</p>
",Preprocessing of the text & Tokenization,length word index greater num word code text preprocessing deep learning check length tokenizer word index safe knowledge get get length x equal dataframe column containing matter know however question arises vocabulary size num word length word index seems confused help appreciated
How to loop and delete stop words from a folder,"<p>I am currently working on the task of deleting stop words. This code can be run, but I would like to ask how to change it into a loop statement, that is, loop to extract stop words in a folder instead of a single file. It might be the &quot;file1.... this statement&quot;, but I don't know how to change it. The code is attached as follows, Thanks!</p>
<pre><code>from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

stop_words = set(stopwords.words('english'))
file1 = open(
    r&quot;D:\1.1 SEC EDGAR年报源文件 (10Q_10KA_10QA)\2001\QTR1\20010102_10-K-A_edgar_data_1024302_0001092388-00-500453.txt&quot;)
line = file1.read()
words = word_tokenize(line)
words_witout_stop_words = [&quot;&quot; if word in stop_words else word for word in words]
new_words = &quot; &quot;.join(words_witout_stop_words).strip()
appendFile = open(
    r&quot;D:\1.1 SEC EDGAR年报源文件 (10Q_10KA_10QA)\2001\QTR1\20010102_10-K-A_edgar_data_1024302_0001092388-00-500453.txt&quot;, 'w')
appendFile.write(new_words)
appendFile.close()
</code></pre>
",Preprocessing of the text & Tokenization,loop delete stop word folder currently working task deleting stop word code run would like ask change loop statement loop extract stop word folder instead single file might file statement know change code attached follows thanks
Should I perform both lemmatization and stemming?,"<p>I'm writing a text classification system in Python. This is what I'm doing to canonicalize each token:</p>

<pre><code>lem, stem = WordNetLemmatizer(), PorterStemmer()
for doc in corpus:
    for word in doc:
        lemma = stem.stem(lem.lemmatize(word))
</code></pre>

<p>The reason I don't want to just lemmatize is because I noticed that <code>WordNetLemmatizer</code> wasn't handling some common inflections. In the case of adverbs, for example, <code>lem.lemmatize('walking')</code> returns <code>walking</code>.</p>

<p>Is it wise to perform both stemming and lemmatization? Or is it redundant? Do researchers typically do one or the other, and not both?</p>
",Preprocessing of the text & Tokenization,perform lemmatization stemming writing text classification system python canonicalize token reason want lemmatize noticed handling common inflection case adverb example return wise perform stemming lemmatization redundant researcher typically one
Extract sentence embeddings features with Pandas and spaCy,"<p>I'm currently learning spaCy, and I have an exercise on word and sentence embeddings. Sentences are stored in a pandas DataFrame columns, and, we're requested to train a classifier based on the vector of these sentences.</p>
<p>I have a dataframe that looks like this:</p>
<pre><code>+---+---------------------------------------------------+
|   |                                          sentence |
+---+---------------------------------------------------+
| 0 | &quot;Whitey on the Moon&quot; is a 1970 spoken word poe... |
+---+---------------------------------------------------+
| 1 | St Anselm's Church is a Roman Catholic church ... |
+---+---------------------------------------------------+
| 2 | Nymphargus grandisonae (common name: giant gla... |
+---+---------------------------------------------------+
</code></pre>
<p>Next, I apply an NLP function to these sentences:</p>
<pre class=""lang-py prettyprint-override""><code>import en_core_web_md
nlp = en_core_web_md.load()
df['tokenized'] = df['sentence'].apply(nlp)
</code></pre>
<p>Now, if I understand correctly, each item in df['tokenized'] has an attribute that returns the vector of the sentence in a <em>2D</em> array.</p>
<pre class=""lang-py prettyprint-override""><code>print(type(df['tokenized'][0].vector))
print(df['tokenized'][0].vector.shape)
</code></pre>
<p>yields</p>
<pre><code>&lt;class 'numpy.ndarray'&gt;
(300,)
</code></pre>
<p>How do I add the content of this array (300 rows) as columns to the <code>df</code> dataframe for the corresponding sentence, <em>ignoring stop words</em>?</p>
<p>Thanks!</p>
",Preprocessing of the text & Tokenization,extract sentence embeddings feature panda spacy currently learning spacy exercise word sentence embeddings sentence stored panda dataframe column requested train classifier based vector sentence dataframe look like next apply nlp function sentence understand correctly item df tokenized ha attribute return vector sentence array yield add content array row column dataframe corresponding sentence ignoring stop word thanks
Python clean text - remove unknown characters and special characters,"<p>I would like to remove unknown words and characters from the sentence. The text is the output of the transformers model program. So, Sometimes it produces unknown repeated words. I have to remove those words in order to make the sentence readable.</p>
<p>Input</p>
<pre><code>text = &quot;This is an example sentence 098-1832-1133 and this is another sentence.WAA-FAHHaAA. This is the third sentence WA WA WA aZZ aAD&quot;
</code></pre>
<p>Expected Output</p>
<pre><code>text = &quot;This is an example sentence and this is another sentence. This is the third sentence&quot;
</code></pre>
",Preprocessing of the text & Tokenization,python clean text remove unknown character special character would like remove unknown word character sentence text output transformer model program sometimes produce unknown repeated word remove word order make sentence readable input expected output
how to tokenize indic languages using inltk,"<p>i did this using this NLP documentation check it out:
<a href=""https://inltk.readthedocs.io/en/latest/index.html"" rel=""nofollow noreferrer"">https://inltk.readthedocs.io/en/latest/index.html</a></p>
<pre><code>from inltk.inltk import tokenize 
text=&quot;जो मुझको सताती है तुझे वो बातें आती है जब सामने तू होता नहीं बेचैनी बढ़ जाती है मैं रूठ &quot;
tokenize(text ,'hi')
</code></pre>
<p>the error is:</p>
<pre><code>RuntimeError: Internal: src/sentencepiece_processor.cc(890)
[model_proto-&gt;ParseFromArray(serialized.data(), serialized.size())] 
</code></pre>
",Preprocessing of the text & Tokenization,tokenize indic language using inltk using nlp documentation check error
How does the ISRI Stemmer give better stem words than Lancaster or Snowball Stemmer,"<p>I have this sample text which i want to tokenize and subsequently find the stem words</p>
<pre><code>sample_text = &quot;'I am a student from the University of Alabama. \
I was born in Ontario, Canada and I am a huge fan of the United States. \
I am going to get a degree in Philosophy to improve\
my chances of becoming a Philosophy professor. \
I have been working towards this goal for 4 years. \
I am currently enrolled in a PhD program. \
It is very difficult, but I am confident that it will be a good decision'&quot;
</code></pre>
<p>Using Lancaster Stemmer I am getting the following result -</p>
<pre><code>sentences = sent_tokenize(sample_text)

from nltk.stem import LancasterStemmer

lancaster = LancasterStemmer()

for i in range(len(sentences)):
    
    sentences[i] = re.sub('[^A-Za-z0-9]', ' ', sentences[i])
    
    sentences[i] = word_tokenize(sentences[i])
    
    stopwds = [word.lower() for word in stopwords.words('english')]
    
    sentences[i] = [word.lower() for word in sentences[i] if word.lower() not in stopwds]
    
    sentences[i] = [lancaster.stem(word) for word in sentences[i]]
    
    print(sentences[i])
</code></pre>
<p>Output of Lancaster Stemmer:</p>
<pre><code>['stud', 'univers', 'alabam']
['born', 'ontario', 'canad', 'hug', 'fan', 'unit', 'stat']
['going', 'get', 'degr', 'philosoph', 'improvemy', 'chant', 'becom', 'philosoph', 'profess']
['work', 'toward', 'goal', '4', 'year']
['cur', 'enrol', 'phd', 'program']
['difficult', 'confid', 'good', 'decid']
</code></pre>
<p>Output with Snowball stemmer -</p>
<pre><code>['student', 'univers', 'alabama']
['born', 'ontario', 'canada', 'huge', 'fan', 'unit', 'state']
['go', 'get', 'degre', 'philosophi', 'improvemi', 'chanc', 'becom', 'philosophi', 'professor']
['work', 'toward', 'goal', '4', 'year']
['current', 'enrol', 'phd', 'program']
['difficult', 'confid', 'good', 'decis']
</code></pre>
<p>Output of Porter Stemmer</p>
<pre><code>['student', 'univers', 'alabama']
['born', 'ontario', 'canada', 'huge', 'fan', 'unit', 'state']
['go', 'get', 'degre', 'philosophi', 'improvemi', 'chanc', 'becom', 'philosophi', 'professor']
['work', 'toward', 'goal', '4', 'year']
['current', 'enrol', 'phd', 'program']
['difficult', 'confid', 'good', 'decis']
</code></pre>
<p>Whereas ISRI Stemmer almost gives me same results as if the words had been lemmatized</p>
<pre><code>sentences = sent_tokenize(sample_text)
from nltk.stem import ISRIStemmer
isri = ISRIStemmer()
for i in range(len(sentences)):
    
    sentences[i] = re.sub('[^A-Za-z0-9]', ' ', sentences[i])
    
    sentences[i] = word_tokenize(sentences[i])
    
    stopwds = [word.lower() for word in stopwords.words()]
    
    sentences[i] = [word.lower() for word in sentences[i] if word.lower() not in stopwds]
    
    sentences[i] = [ isri.stem(word) for word in sentences[i]]
    
    print(sentences[i])
</code></pre>
<p>Output :</p>
<pre><code>['student', 'university', 'alabama']
['born', 'ontario', 'canada', 'huge', 'fan', 'united', 'states']
['going', 'get', 'degree', 'philosophy', 'improvemy', 'chances', 'becoming', 'philosophy', 'professor']
['working', 'towards', 'goal', '4', 'years']
['currently', 'enrolled', 'phd', 'program']
['difficult', 'confident', 'good', 'decision']

</code></pre>
<p>Can someone explain how ISRI Stemmer gives almost Lemmatized words</p>
",Preprocessing of the text & Tokenization,doe isri stemmer give better stem word lancaster snowball stemmer sample text want tokenize subsequently find stem word using lancaster stemmer getting following result output lancaster stemmer output snowball stemmer output porter stemmer whereas isri stemmer almost give result word lemmatized output someone explain isri stemmer give almost lemmatized word
Can I use cosine distance as inputs for model in text classification task?,"<p>I mostly deal with text data. I'm not sure if I can do something like this.</p>
<pre><code>for plaint,law in zip(train_seqs1,train_seqs2):
   plaint_emb = get_tokens_embeddings(plaint).numpy().flatten()
   law_emb = get_tokens_embeddings(law).numpy().flatten()
   cosine_dis = cosine(plaint_emb,law_emb)
</code></pre>
<p>I tried getting word embeddings from bert , and used them to calculate distance between two texts. I want to know that , can I use this cosine distances to train model? If I can , how to preprocess these number inputs. I want to know the right threshold of cosine distances to predict that these two texts are relevant or not relevant to each other. (Binary classification)</p>
",Preprocessing of the text & Tokenization,use cosine distance input model text classification task mostly deal text data sure something like tried getting word embeddings bert used calculate distance two text want know use cosine distance train model preprocess number input want know right threshold cosine distance predict two text relevant relevant binary classification
Stem Spanish words in isolation to validate that they are &quot;words&quot; in SpaCy&#39;s (or any) dictionary,"<p>I have a list of 20,000 words. I want to know which of the 20k words are &quot;weird&quot; in some way. This is part of a text cleaning task.</p>
<p><code>Albóndiga</code> is fine, <code>huticotai</code> is no Spanish word I know... neither is <code>56%$3estapa</code></p>
<p>This means I must compare declined/conjugated words in isolation to some source of truth. Everyone recommends SpaCy. Fine.</p>
<p>Somehow, though, using the code below and a test file with a few dozen words, spaCy thinks they are all &quot;ROOT&quot; words. Si hablas castellano, sabrás que así no es.
technically, I don't want to lemmatize anything! I want to stem the words. <strong>I just want to pair down the 20k-long wordlist to something I as a Spanish-speaking linguist can look at to determine what sorts of of crazy desmadre (B.S.) is going on.</strong></p>
<p>Here is an example of the output I get:</p>
<ul>
<li>trocito NOUN ROOT trocito</li>
<li>ayuntamiento NOUN ROOT ayuntamiento</li>
<li>eyre NOUN ROOT eyre</li>
<li>suscribíos NOUN ROOT suscribío</li>
<li>mezcal ADJ ROOT mezcal</li>
<li>marivent VERB ROOT mariventir</li>
<li>inversores NOUN ROOT inversor</li>
<li>stenger VERB ROOT stenger</li>
</ul>
<p>Clearly, &quot;stenger&quot; is not a Spanish word, though naïvely, spaCy thinks it is. Mezcal is a NOUN (and a very good time). You get the picture.</p>
<p>Here is my code:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;es_core_news_sm&quot;)

new_lst = []
with open(&quot;vocabu_suse.txt&quot;, 'r') as lst:
    for i in lst:
        # print(i)
        new_lst.append(i.strip())

for i in new_lst:
    j = nlp(i)
    for token in j:
        print(token.text, token.pos_, token.dep_, token.lemma_)
</code></pre>
",Preprocessing of the text & Tokenization,stem spanish word isolation validate word spacy dictionary list word want know k word weird way part text cleaning task fine spanish word know neither mean must compare conjugated word isolation source truth everyone recommends spacy fine somehow though using code test file dozen word spacy think root word si hablas castellano sabr que e technically want lemmatize anything want stem word want pair k long wordlist something spanish speaking linguist look determine sort crazy desmadre b going example output get trocito noun root trocito ayuntamiento noun root ayuntamiento eyre noun root eyre suscrib noun root suscrib mezcal adj root mezcal marivent verb root mariventir inversores noun root inversor stenger verb root stenger clearly stenger spanish word though na vely spacy think mezcal noun good time get picture code
How to tokenize/parse data in an excel sheet using spacy,"<p>I'm trying to convert an excel sheet into a doc object using spacy, I spent the last couple of days trying to go around it but it seems a bit challenging.  I have opened the sheet in both openpyxl and pandas, I can read the excel sheet and output the content but I couldn't integrate spacy to create doc/token objects.</p>
<p>Is it possible to process excel sheets in spacy's pipeline?</p>
<p>Thank you!</p>
",Preprocessing of the text & Tokenization,tokenize parse data excel sheet using spacy trying convert excel sheet doc object using spacy spent last couple day trying go around seems bit challenging opened sheet openpyxl panda read excel sheet output content integrate spacy create doc token object possible process excel sheet spacy pipeline thank
find named entities from tokenized sentences in SPACY v2.0,"<p>I am trying to do:</p>

<ul>
<li>Tokenize sentences from text</li>
<li>Compute Named Entities for each word present in sentence</li>
</ul>

<p>This is what I have done so far:</p>

<pre><code>nlp = spacy.load('en')
sentence = ""Germany and U.S.A are popular countries. I am going to gym tonight""
sentence = nlp(sentence)
tokenized_sentences = []
for sent in sentence.sents:
        tokenized_sentences.append(sent)
for s in tokenized_sentences:
        labels = [ent.label_ for ent in s.ents]
        entities = [ent.text for ent in s.ents]
</code></pre>

<p>Error: </p>

<pre><code>    labels = [ent.label_ for ent in s.ents]
    AttributeError: 'spacy.tokens.span.Span' object has no attribute 'ents'
</code></pre>

<p>Is there any alternative way to find named entities of tokenized sentence?</p>

<p>Thanks in advance</p>
",Preprocessing of the text & Tokenization,find named entity tokenized sentence spacy v trying tokenize sentence text compute named entity word present sentence done far error alternative way find named entity tokenized sentence thanks advance
"Python, NLP - finding the top document containing given list of words","<p>I am new to learning NLP. I am trying to do an exercise of finding the best matching resume.</p>

<p>For example, I have a list of skills that I am looking for like ['java', 'python', 'SQL', 'API', ...], and a set of documents. I want to create a model to find the document that is the best match with these skills. Similar to resume matching.</p>

<p>I started with this tutorial - <a href=""https://medium.com/better-programming/how-to-convert-pdfs-into-searchable-key-words-with-python-85aab86c544f"" rel=""nofollow noreferrer"">Extracting words from pdf</a> as a reference</p>

<p>I was able to extract the text from pdf, remove stop words, perform lemmatization, compute the number of times these keywords appear in each document and I am not sure how to go head from here.</p>

<p>Could anyone provide me with what the next steps should be? Any tutorials or references would be helpful as well.</p>
",Preprocessing of the text & Tokenization,python nlp finding top document containing given list word new learning nlp trying exercise finding best matching resume example list skill looking like java python sql api set document want create model find document best match skill similar resume matching started tutorial extracting word pdf reference wa able extract text pdf remove stop word perform lemmatization compute number time keywords appear document sure go head could anyone provide next step tutorial reference would helpful well
NLP - obtain Beg Of Word &gt; taxonomy matching rate with new text,"<p>I'm struggling to understand the correct approach for the following problem:</p>
<ul>
<li>I have some texts belong to same class (i.e. twitter text describing a particular product)</li>
<li>I want to generate a &quot;taxonomy-beg-of-word&quot;</li>
<li>I want to use the generated taxonomy to compare with next text in order to evaluate how much match the topic</li>
</ul>
<p>I also know Bag of Words just creates a set of vectors containing the count of word occurrences in the document (i.e twitter texts), while the TF-IDF model contains information on the more important words and the less important ones as well.</p>
<p>But the point is, independently of BoX or TF-IDF, what is the correct approach to compare a new text against it?
If I have for example this generated supervised product BoW form 3 different texts:
<a href=""https://i.sstatic.net/HCoTM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HCoTM.png"" alt=""enter image description here"" /></a></p>
<p>I expect the dominant terms will be &quot;shoes&quot; and &quot;modelY&quot;</p>
<p>I don't get how to use this BoW to compare with new text and get a match rate (after stop-word removal and/or lemming stemming...)</p>
<p>For example:</p>
<ul>
<li>&quot;We sell new cars&quot; &gt; I'm expecting 0% match against BoW</li>
<li>&quot;new shoes available&quot; &gt; I'm expecting some proportional rate match (&quot;shoes&quot; is an important match weithed to total token lengh)</li>
<li>&quot;ModelY shoes on sale now!&quot; &gt; I'm expecting even higher rate as 2 important terms match the BoW</li>
</ul>
<p>Any tips suggestion will be really appreciated.
Many Thanks</p>
",Preprocessing of the text & Tokenization,nlp obtain beg word taxonomy matching rate new text struggling understand correct approach following problem text belong class e twitter text describing particular product want generate taxonomy beg word want use generated taxonomy compare next text order evaluate much match topic also know bag word creates set vector containing count word occurrence document e twitter text tf idf model contains information important word le important one well point independently box tf idf correct approach compare new text example generated supervised product bow form different text expect dominant term shoe modely get use bow compare new text get match rate stop word removal lemming stemming example sell new car expecting match bow new shoe available expecting proportional rate match shoe important match weithed total token lengh modely shoe sale expecting even higher rate important term match bow tip suggestion really appreciated many thanks
What does Keras Tokenizer num_words specify?,"<p>Given this piece of code:</p>
<pre><code>from tensorflow.keras.preprocessing.text import Tokenizer

sentences = [
    'i love my dog',
    'I, love my cat',
    'You love my dog!'
]

tokenizer = Tokenizer(num_words = 1)
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
print(word_index)
</code></pre>
<p>whether <code>num_words=1</code> or <code>num_words=100</code>, I get the same output when I run this cell on my jupyter notebook, and I can't seem to understand what difference it makes in tokenization.</p>
<blockquote>
<p>{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}</p>
</blockquote>
",Preprocessing of the text & Tokenization,doe kera tokenizer num word specify given piece code whether get output run cell jupyter notebook seem understand difference make tokenization love dog cat
Python NLTK - Tokenize sentences into words while removing numbers,"<p>hoping someone can assist with this! I have a list of sentences which is read from a text file. I am trying to tokenize the sentences into words, while also removing sentences while contain only numbers. There is no pattern for when the numbers will appear.</p>
<p>The sentences I have:</p>
<pre><code>[
  ['                    1'], 
  ['This is a text file,'], 
  ['to keep the words,'],
  ['                    2'],
  ['Another line of the text:'],
  ['                    3']
]
</code></pre>
<p>Desired output:</p>
<pre><code>[
  ['This', 'is', 'a', 'text', 'file,'], 
  ['to', 'keep', 'the', 'words,'],
  ['Another', 'line', 'of', 'the', 'text:'],
]
</code></pre>
",Preprocessing of the text & Tokenization,python nltk tokenize sentence word removing number hoping someone assist list sentence read text file trying tokenize sentence word also removing sentence contain number pattern number appear sentence desired output
How to remove the ending sign from Bangla Text?,"<pre><code>লোকাল ট্রেন পরিষেবা চালু করার জন্য দীর্ঘদিন ধরেই সরব হয়েছেন বহু নেতা নেত্রী।
</code></pre>
<p>How can I remove the <strong>&quot;|&quot;</strong> sign from the Bangla Sentence?
and then tokenize it?</p>
<p>in this format &quot;নেত্রী&quot; instead of &quot;নেত্রী।&quot;</p>
<p>I am using python language.</p>
",Preprocessing of the text & Tokenization,remove ending sign bangla text remove sign bangla sentence tokenize format instead using python language
Use word2vec to expand a glossary in order to classify texts,"<p>I have a database containing about 3 million texts (tweets). I put clean texts (removing stop words, tags...) in a list of lists of tokens called <code>sentences</code> (so it contains a list of tokens for each text).</p>
<p>After these steps, if I write</p>
<p><code>model = Word2Vec(sentences, min_count=1)</code></p>
<p>I obtain a vocabulary of about 400,000 words.</p>
<p>I have also a list of words (belonging to the same topic, in this case: economics) called <code>terms</code>. I found that 7% of the texts contain at least one of these words (so we can say that 7% of total tweets talk about economics).</p>
<p>My goal is to expand the list <code>terms</code> in order to retrieve more texts belonging to the economic topic.</p>
<p>Then I use</p>
<p><code>results = model.most_similar(terms, topn=5000)</code></p>
<p>to find, within the list of lists of tokens <code>sentences</code>, the words most similar to those contained in <code>terms</code>.</p>
<p>Finally if I create the data frame</p>
<p><code>df = pd.DataFrame(results, columns=['key', 'similarity'])</code></p>
<p>I get something like that:</p>
<pre><code>key       similarity
word1     0.795432
word2     0.787954
word3     0.778942
...       ...
</code></pre>
<p>Now I think I have two possibilities to define the expanded glossary:</p>
<ul>
<li>I take the first N words (what should be the value of N?);</li>
<li>I look at the suggested words one by one and decide which one to include in the expanded glossary based on my knowledge (does this word really belong to the economic glossary?)</li>
</ul>
<p>How should I proceed in a case like this?</p>
",Preprocessing of the text & Tokenization,use word vec expand glossary order classify text database containing million text tweet put clean text removing stop word tag list list token called contains list token text step write obtain vocabulary word also list word belonging topic case economics called found text contain least one word say total tweet talk economics goal expand list order retrieve text belonging economic topic use find within list list token word similar contained finally create data frame get something like think two possibility define expanded glossary take first n word value n look suggested word one one decide one include expanded glossary based knowledge doe word really belong economic glossary proceed case like
Stop Words not being removed from list,"<p>I am doing some analysis on a large corpus, but my function to remove custom stop words is just not working. I tried several different solutions from questions already asked here, but I can't find why words are not being removed from the Test list.</p>
<p>Any help pointing out my stupidness is welcome.</p>
<pre><code>test = [['acesso em',
  'agindo',
  'alegre',
  'ambiente escolar',
  'ambientes digitais',
  'anual',
  'aplicativos digitais',
  'apresentar conclusões',
  'argumentação cuidado',
  'articuladas projeto',
  'associadas eixos',
  'associação',
  'ativas',
  'atos linguagem',
  'avaliar oportunidades',
  'bairro',
  'base critérios',
  'base estudos',
  'bibliográfica exploratória',
  'blogs',
  'buscando apresentar',
  'campo artístico']]
removed = ['anual']
new_words = [word for word in test if word not in removed]
new_words
</code></pre>
",Preprocessing of the text & Tokenization,stop word removed list analysis large corpus function remove custom stop word working tried several different solution question already asked find word removed test list help pointing stupidness welcome
"From a bunch of n vectors, get all vectors which are mutually orthogonal","<p>Original problem - context: NLP - from a list of <em>n</em> strings, choose all the strings which don't have common words (without considering the words in a pre-defined list of stop words)</p>
<p>Approach that I tried: using sklearn's count vectoriser, get the vectors for each string and compute dot product for each vector with every other vector. Those vectors with zero dot product will be added to a set.</p>
<p>This is done using O(n<sup>2</sup>) dot product computations. Is there a better way to approach this problem?</p>
",Preprocessing of the text & Tokenization,bunch n vector get vector mutually original problem context nlp list n string choose string common word without considering word pre defined list stop word approach tried using sklearn count vectoriser get vector string compute dot product vector every vector vector zero dot product added set done using n dot product computation better way approach problem
calculate similarity between one given word and a RANDOM list of words,"<p>I want to calculate the similarity between a given one word and a RANDOM list of words, then would rank the result in a new list, for example:</p>
<pre><code>list = ['bark','black','cat','bite','human','book'] #it could be another list
</code></pre>
<p>is similar to the word:</p>
<pre><code>word = ['dog']
</code></pre>
<p>--</p>
<pre><code>import spacy
nlp = spacy.load('en_core_web_md')


bark = nlp(&quot;bark&quot;)
bite = nlp(&quot;bite&quot;)
human = nlp(&quot;human&quot;)
book = nlp(&quot;book&quot;)
cat = nlp(&quot;cat&quot;)
black = nlp(&quot;black&quot;)

print(&quot;dog - bark&quot;, dog.similarity(bark)) #0.4258176903285793
print(&quot;dog - bite&quot;, dog.similarity(bite)) #0.4781574605069981
print(&quot;dog - human&quot;, dog.similarity(human)) #0.35814872466230835
print(&quot;dog - book&quot;, dog.similarity(book)) #0.22838638167627964
print(&quot;dog - cat&quot;, dog.similarity(cat)) #0.8016854705531046
print(&quot;dog - black&quot;, dog.similarity(black)) #0.30601667459001575
</code></pre>
<p>So how I can calculate the similarity of every word in the list to the given word automatically?</p>
",Preprocessing of the text & Tokenization,calculate similarity one given word random list word want calculate similarity given one word random list word would rank result new list example similar word calculate similarity every word list given word automatically
Java library for keywords extraction from input text,"<p>I'm looking for a Java library to extract keywords from a block of text.</p>

<p>The process should be as follows:</p>

<p>stop word cleaning -> stemming -> searching for keywords based on English linguistics statistical information - meaning if a word appears more times in the text than in the English language in terms of probability than it's a keyword candidate.</p>

<p>Is there a library that performs this task?</p>
",Preprocessing of the text & Tokenization,java library keywords extraction input text looking java library extract keywords block text process follows stop word cleaning stemming searching keywords based english linguistics statistical information meaning word appears time text english language term probability keyword candidate library performs task
gensim/ Training a LDA Model: &#39;int&#39; object is not subscriptable,"<p>I create a new word list in which stop words from 'text8' have been removed, in order to train a LDA Model. However, I received <code>TypeError: 'int' object is not subscriptable</code>, guessing problems from corpus, and cannot find the solutions.</p>
<p>Here is my code:</p>
<pre><code>import gensim.downloader as api
corpus=api.load('text8')
dictionary=gensim.corpora.Dictionary(corpus) # generate a dictionary from the text corpus

# removing stop words
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
nltk.download('stopwords')
nltk.download('punkt')

stop_words = set(stopwords.words('english'))
word_tokens = dictionary

filtered_sentence = []
for w in word_tokens:
    if word_tokens[w] not in stop_words:
        filtered_sentence.append(word_tokens[w])

#print(filtered_sentence)

# generate a new dictionary from &quot;filtered_sentence&quot;

dct=gensim.corpora.Dictionary([filtered_sentence])
corpus2=dct.doc2bow(filtered_sentence)
</code></pre>
<p>The following line is not working-- TypeError: 'int' object is not subscriptable</p>
<pre><code>model=gensim.models.ldamodel.LdaModel(corpus2, num_topics=5, id2word=dct) #TypeError

model.print_topics(num_words=5)
</code></pre>
<p>Detailed error message:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-64-75e1fe1a727b&gt; in &lt;module&gt;()
----&gt; 1 model=gensim.models.ldamodel.LdaModel(corpus2, num_topics=5, id2word=dct) #TypeError: 'int' object is not subscriptable
      2 model.print_topics(num_words=5)

3 frames
/usr/local/lib/python3.7/dist-packages/gensim/models/ldamodel.py in inference(self, chunk, collect_sstats)
    651         # to Blei's original LDA-C code, cool!).
    652         for d, doc in enumerate(chunk):
--&gt; 653             if len(doc) &gt; 0 and not isinstance(doc[0][0], six.integer_types + (np.integer,)):
    654                 # make sure the term IDs are ints, otherwise np will get upset
    655                 ids = [int(idx) for idx, _ in doc]

TypeError: 'int' object is not subscriptable
</code></pre>
<p>Really appreciate your help. Thank you so much!</p>
",Preprocessing of the text & Tokenization,gensim training lda model int object subscriptable create new word list stop word text removed order train lda model however received guessing problem corpus find solution code following line working typeerror int object subscriptable detailed error message really appreciate help thank much
Python Alternative (Equivalent) to Wink Tokenizer JS,"<p>I have some JS functions that have helped me to tokenize my strings using Wink Tokenizer.</p>
<p>I'm moving some services to Python and now I would like to get an equivalent tokenizer function. I have researched a lot and it seems Wink tokenizer is just available for JS. I'm also not that aware of the subtle differences between Wink and other Python Tokenizers like spaCY for example.</p>
<p>Basically I would like to be able to get the same results as:</p>
<pre><code>var tokenizer = require( 'wink-tokenizer' );
// Create it's instance.
var myTokenizer = tokenizer();
 
// Tokenize a tweet.
var s = '@superman: hit me up on my email r2d2@gmail.com, 2 of us plan party🎉 tom at 3pm:) #fun';
myTokenizer.tokenize( s );
</code></pre>
<p>On Python</p>
<p>Can anyone help me out by pointing me in the right direction of how I could go on replicating the tokenization functions Wink offers on Python? What parameters, configs, regexes do I have to check to get an equivalent behaviout?</p>
",Preprocessing of the text & Tokenization,python alternative equivalent wink tokenizer j j function helped tokenize string using wink tokenizer moving service python would like get equivalent tokenizer function researched lot seems wink tokenizer available j also aware subtle difference wink python tokenizers like spacy example basically would like able get result python anyone help pointing right direction could go replicating tokenization function wink offer python parameter configs regexes check get equivalent behaviout
TypeError: lemmatize() missing 1 required positional argument: &#39;word (WordNetLemmatizer),"<p>I am facing a problem with WordNetLemmatizer.
What I am doing is filtering out useless words like int and int + str by using CountVectorizer.</p>
<p>Without further ado. My code is following:</p>
<blockquote>
<p>letters_only is a function to give me Fales if a word contains int, or int + str</p>
</blockquote>
<pre><code>groups = fetch_20newsgroups()
cleaned = []
all_names = set(names.words())

for post in groups:
  cleaned.append(''.join([lemmatizer.lemmatize(word.lower())
  for word in post.split()
  if letters_only(word)
  and word not in all_names]))
  
</code></pre>
<p>Just in case, I tried this as well:</p>
<pre><code>for post in groups.data:
  for word in post.split():
    if letters_only(word) and word not in all_names:
      cleaned.append(''.join(lemmatizer.lemmatize(word.lower()))
</code></pre>
<p>Two separate codes give me the same error which is &quot;TypeError: lemmatize() missing 1 required positional argument: 'word&quot;.</p>
",Preprocessing of the text & Tokenization,typeerror lemmatize missing required positional argument word wordnetlemmatizer facing problem wordnetlemmatizer filtering useless word like int int str using countvectorizer without ado code following letter function give fales word contains int int str case tried well two separate code give error typeerror lemmatize missing required positional argument word
"Print frequency of the words along with word like (W1 : N1 ) (W2 : N2) with N1&gt;N2 from a dictionary containing tuple in form of {(w1,n1),(w2,n2)}","<p>I am trying to create a function that prints the word frequency after taking out the unnecessary words from the given texts and prints out : firs the word, then number of times it occurred in the text. So far I have been able to clean the text, count the words, and create a dictionary of the words with number of times its occurring in the text. Now I want to print it in a sorted format where word with highest count gets printed first, along with number of times its occurring. Here is the code I have been able to come up with. What should I do next ?</p>
<p>Here is a sample output of the dictionary :</p>
<p>{'welcome': 3, 'probe': 5, 'pm': 2, 'security': 6, 'breach': 2, 'propaganda': 4, 'sc': 2}</p>
<p>I want the output as</p>
<p>security   : 6</p>
<p>probe      : 5</p>
<p>Propaganda : 4</p>
<p>welcome    : 3</p>
<pre><code>enter code here

file = &quot;/Users/learninguser/Documents/GitHub/Projects/News_analysis/news.txt&quot;
f = open(file)
text = f.read()



def RemovePunchuations(text):
     # removes all the special characters supplied in
     # the variable punchuations from the supplied
     # text string
       newtext = text

    punctuations = '''!()-[]{};:'&quot;\,&lt;&gt;./?@—#$%^&amp;*_~'''

    for punchuation in punctuations:
        newtext = newtext.replace(punchuation, &quot; &quot;)

    return newtext




def TokenfromText(text):
    # take a string as input
    # return a list of words in the text

    newtext = text
    newnewtext = newtext.split()
    return_list = []
    for word in newnewtext:
        newword = word.lower()
        return_list.append(newword)
    return return_list




def FrequencyWithUninterestingRemoved(listofwords):
    # takes the tokens from text
    # calculates frequency of words supplied in the list
    # removes uninteresting words based on corpus supplied
    # returns a dictionary of frequency

    frequency = {}
    supplied_list = listofwords

    for word in supplied_list:
        if word not in frequency:
            frequency[word] = 1
        frequency[word] += 1

    uninteresting_words = [&quot;the&quot;, &quot;a&quot;, &quot;to&quot;, &quot;if&quot;, &quot;is&quot;, &quot;it&quot;, &quot;of&quot;,
                       &quot;and&quot;, &quot;or&quot;, &quot;an&quot;, &quot;as&quot;, &quot;i&quot;, &quot;me&quot;, &quot;my&quot;,
                       &quot;we&quot;, &quot;our&quot;, &quot;ours&quot;, &quot;you&quot;, &quot;your&quot;, &quot;yours&quot;,
                       &quot;he&quot;, &quot;she&quot;, &quot;him&quot;, &quot;his&quot;, &quot;her&quot;, &quot;hers&quot;,
                       &quot;its&quot;, &quot;they&quot;, &quot;them&quot;,     &quot;their&quot;, &quot;what&quot;,
                       &quot;which&quot;, &quot;who&quot;, &quot;whom&quot;, &quot;this&quot;, &quot;that&quot;,
                       &quot;am&quot;, &quot;are&quot;, &quot;was&quot;, &quot;were&quot;, &quot;be&quot;, &quot;been&quot;,
                       &quot;being&quot;,     &quot;have&quot;, &quot;has&quot;, &quot;had&quot;, &quot;do&quot;,
                       &quot;does&quot;, &quot;did&quot;, &quot;but&quot;, &quot;at&quot;, &quot;by&quot;, &quot;with&quot;,
                       &quot;from&quot;, &quot;here&quot;, &quot;when&quot;, &quot;where&quot;, &quot;how&quot;,
                       &quot;all&quot;, &quot;any&quot;, &quot;both&quot;, &quot;each&quot;, &quot;few&quot;, &quot;more&quot;,
                       &quot;some&quot;, &quot;such&quot;, &quot;no&quot;, &quot;nor&quot;, &quot;too&quot;, &quot;very&quot;,
                       &quot;can&quot;, &quot;will&quot;, &quot;just&quot;, 'a', 'about', 'above',
                       'after', 'again', 'against', 'all', 'am',
                       'an', 'and', 'any', 'are', &quot;aren't&quot;, 'as',
                       'at', 'be', 'because', 'been', 'before',
                       'didst', 'us', 'one',
                       'being', 'below', 'between', 'both',
                       'but', 'by', &quot;can't&quot;, 'cannot', 'could',
                       &quot;couldn't&quot;, 'did', &quot;didn't&quot;, 'do', 'does',
                       &quot;doesn't&quot;, 'doing', &quot;don't&quot;, 'down', 'during',
                       'each', 'few', 'for', 'from', 'further', 'had',
                       &quot;hadn't&quot;, 'has', &quot;hasn't&quot;, 'have', &quot;haven't&quot;,
                       'having',
                       'he', &quot;he'd&quot;, &quot;he'll&quot;, &quot;he's&quot;, 'her', 'here', &quot;here's&quot;,
                       'hers', 'herself', 'him', 'himself', 'his', 'how', &quot;how's&quot;,
                       'i', &quot;i'd&quot;, &quot;i'll&quot;, &quot;i'm&quot;, &quot;i've&quot;, 'if', 'in', 'into', 'is',
                       &quot;isn't&quot;, 'it', &quot;it's&quot;, 'its', 'itself', &quot;let's&quot;, 'me', 'more',
                       'most', &quot;mustn't&quot;, 'my', 'myself', 'no', 'nor', 'not', 'of',
                       'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our',
                       'ours', 'ourselves', 'out', 'over', 'own', 'same', &quot;shan't&quot;,
                       'she', &quot;she'd&quot;, &quot;she'll&quot;, &quot;she's&quot;, 'should', &quot;shouldn't&quot;, 'so',
                       'some', 'such', 'than', 'that', &quot;that's&quot;, 'the', 'their', 'theirs',
                       'them', 'themselves', 'then', 'there', &quot;there's&quot;, 'these', 'they',
                       &quot;they'd&quot;, &quot;they'll&quot;, &quot;they're&quot;, &quot;they've&quot;, 'this', 'those', 'through',
                       'to', 'too', 'under', 'until', 'up', 'very', 'was', &quot;wasn't&quot;, 'we',
                       &quot;we'd&quot;, &quot;we'll&quot;, &quot;we're&quot;, &quot;we've&quot;, 'were', &quot;weren't&quot;, 'what', &quot;what's&quot;,
                       'when', &quot;when's&quot;, 'where', &quot;where's&quot;, 'which', 'while', 'who', &quot;who's&quot;,
                       'whom', 'why', &quot;why's&quot;, 'with', &quot;won't&quot;, 'would', &quot;wouldn't&quot;, 'you',
                       &quot;you'd&quot;, &quot;you'll&quot;, &quot;you're&quot;, &quot;you've&quot;, 'your', &quot;thou&quot;, &quot;thee&quot;, &quot;thy&quot;,
                       &quot;but&quot;, &quot;man&quot;, 'yours', 'yourself', 'yourselves']

    for uword in uninteresting_words:
        if uword in frequency:
            del frequency[uword]
    return frequency



def Frequency_dict(text):
    text1 = RemovePunchuations(text)

    listofwords = TokenfromText(text1)

    frequencyofwords = FrequencyWithUninterestingRemoved(listofwords)

    return frequencyofwords


f_dict = Frequency_dict(text)

print(f_dict)
</code></pre>
",Preprocessing of the text & Tokenization,print frequency word along word like w n w n n n dictionary containing tuple form w n w n trying create function print word frequency taking unnecessary word given text print fir word number time occurred text far able clean text count word create dictionary word number time occurring text want print sorted format word highest count get printed first along number time occurring code able come next sample output dictionary welcome probe pm security breach propaganda sc want output security probe propaganda welcome
Can a stemming dictionary be used as rejection criteria in R?,"<p>I am struggling through some text analysis, and I'm not sure I'm doing the stemming correctly. Right now, my command for single-term stemming is</p>
<pre><code>text_stem &lt;- text_clean %&gt;% mutate(stem = wordStem(word, language = &quot;english&quot;))
</code></pre>
<p>Is it possible to use this not only as a stemmer, but as a filter? For example, if &quot;text_clean&quot; contains the word aksdjhgla and that word is not in whatever SnowballC uses as a dictionary, the stemmed text would reject it? Maybe there's another command that does this kind of filtering?</p>
",Preprocessing of the text & Tokenization,stemming dictionary used rejection criterion r struggling text analysis sure stemming correctly right command single term stemming possible use stemmer filter example text clean contains word aksdjhgla word whatever snowballc us dictionary stemmed text would reject maybe another command doe kind filtering
Apostrophes and regular expressions; Cleaning text in R,"<p>I working on cleaning a large collection of text. My process thus far is:</p>
<ul>
<li>Remove any non-ASCII characters</li>
<li>Remove URLs</li>
<li>Remove email addresses</li>
<li>Correct kerning (i.e., &quot;B A D&quot; becomes &quot;BAD&quot;)</li>
<li>Correct elongated words (i.e., &quot;baaaaaad&quot; becomes &quot;bad&quot;)</li>
<li>Ensure there is a space after every comma</li>
<li>Replace all numerals and punctuation with a space - except apostrophes</li>
<li>Remove any term 22 characters or longer (anything this size is likely garbage)</li>
<li>Remove any single letters that are leftover</li>
<li>Remove any blank lines</li>
</ul>
<p>My issue is in the next-to-last step. Originally, my code was:</p>
<pre><code>gsub(pattern = &quot;\\b\\S\\b&quot;, replacement = &quot;&quot;, perl = TRUE)
</code></pre>
<p>but this wrecked any contractions that were left (that I left in on purpose). Then I tried</p>
<pre><code>gsub(pattern = &quot;\\b(\\S^'\\s)\\b&quot;, replacement = &quot;&quot;, perl = TRUE)
</code></pre>
<p>but this left a lot of single characters.</p>
<p>Then I realized that I needed to keep three single-letter words: &quot;A&quot;, &quot;I&quot;, and &quot;O&quot; (either case).</p>
<p>Any suggestions?</p>
",Preprocessing of the text & Tokenization,apostrophe regular expression cleaning text r working cleaning large collection text process thus far remove non ascii character remove url remove email address correct kerning e b becomes bad correct elongated word e baaaaaad becomes bad ensure space every comma replace numeral punctuation space except apostrophe remove term character longer anything size likely garbage remove single letter leftover remove blank line issue next last step originally code wa wrecked contraction left left purpose tried left lot single character realized needed keep three single letter word either case suggestion
Detect / replace utf characters,"<p>I want to detect and/or replace weird utf, non-emoji characters that break my tokenization pipeline, like <code>\uf0fc</code>, which renders like a cup/glass:
<a href=""https://i.sstatic.net/SV6hO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SV6hO.png"" alt=""Rendering"" /></a></p>
<p>That image / code is not contained in the <a href=""https://pypi.org/project/emoji/"" rel=""nofollow noreferrer"">emojis package</a>, which I tried for filtering.</p>
<p>Is there a class that describes all such characters?
Is there a way I can reliably detect them?</p>
",Preprocessing of the text & Tokenization,detect replace utf character want detect replace weird utf non emoji character break tokenization pipeline like render like cup glass image code contained emojis package tried filtering class describes character way reliably detect
Split string into sentence list by dot &amp; linebreaks,"<p>I have a string and I want to split it into sentences. To do so I'm using &quot;NLTK&quot; and &quot;sent_tokenize&quot;</p>
<p><code>h2text_list = nltk.tokenize.sent_tokenize(h2listtotal[xx][1])</code></p>
<p>However, the issue is that it doesn't recognize linebreaks (&quot;/n&quot;) as new sentences, instead, it only recognizes a new sentence if it ends in a dot (.)</p>
<p>I need it to recognize the linebreaks as well (while still keeping them). Basically split a text into sentences and add each sentence to a list based on dots and linebreaks (while still keeping them in the text inside the list).</p>
<p>If it also recognizes ! and : and ? as possible linebreaks too that's even better!</p>
<p>Examples of text that keeps triggering the errors:</p>
<pre><code>Here are sample some food sources of vitamin K: 10 sprigs of parsley contains 90 micrograms (mcg)

a 3-ounce serving of natto contains 850 mcg

a half-cup serving of frozen and boiled collard greens contains 530 mcg

one cup of raw spinach contains 145 mcg

1 tablespoon of soybean oil contains 25 mcg

a half-cup serving of grapes contains 11 mcg

a hard-boiled egg contains 4 mcg Most adults in the U.S. are believed to consume enough vitamin K. Recipe tips These healthy recipes have been developed by a registered dietitian.
</code></pre>
<p>This is all considered a SINGLE sentence by &quot;nltk.sent_tokenize&quot; because it only ends in &quot;.&quot; at the very end.</p>
",Preprocessing of the text & Tokenization,split string sentence list dot linebreaks string want split sentence using nltk sent tokenize however issue recognize linebreaks n new sentence instead recognizes new sentence end dot need recognize linebreaks well still keeping basically split text sentence add sentence list based dot linebreaks still keeping text inside list also recognizes possible linebreaks even better example text keep triggering error considered single sentence nltk sent tokenize end end
How Replace a dot (.) in sentence except when it appears in an abbreviation using regular Expression,"<p>I want to replace every dot with a space in a sentence except when it is used with an abbreviation. When it is used with an abbreviation, I want to replace it with <code>''</code> NULL.</p>
<p>Abbreviation means a dot surrounded at least two Capital letters.</p>
<p>My <code>regex</code> are working except they catch <code>U.S.</code></p>
<pre><code>r1 = r'\b((?:[A-Z]\.){2,})\s*'
r2 = r'(?:[A-Z]\.){2,}'

'U.S.A is abbr  x.y  is not. But I.I.T. is also valid ABBVR and so is M.Tech'

should become

'USA is abbr  x y  is not But IIT is also valid ABBVR and so is MTech'
</code></pre>
<p><strong>UPDATE</strong>: It should not be considering any numbers or special characters.</p>
<pre><code>X.2 -&gt; X 2
X. -&gt; X 
X.* -&gt; X - 
</code></pre>
",Preprocessing of the text & Tokenization,replace dot sentence except appears abbreviation using regular expression want replace every dot space sentence except used abbreviation used abbreviation want replace null abbreviation mean dot surrounded least two capital letter working except catch update considering number special character
Sentence tokenization w/o relying on punctuations and capitalizations,"<p>Is there a possible approach for extracting sentences from paragraphs / sentence tokenization for paragraphs that doesn't have any punctuations and/or all lowercased? We have a specific need for being able to split paragraphs into sentences while expecting the worst case that paragraph inputted are improper.</p>
<p>Example:</p>
<p><code>this is a sentence this is a sentence this is a sentence this is a sentence this is a sentence</code>
into
<code>[&quot;this is a sentence&quot;, &quot;this is a sentence&quot;, &quot;this is a sentence&quot;, &quot;this is a sentence&quot;, &quot;this is a sentence&quot;]</code></p>
<p>The sentence tokenizer that we have tried so far seems to rely on punctuations and true casing:
Using <strong>nltk.sent_tokenize</strong>
<code>&quot;This is a sentence. This is a sentence. This is a sentence&quot;</code>
into
<code>['This is a sentence.', 'This is a sentence.', 'This is a sentence']</code></p>
",Preprocessing of the text & Tokenization,sentence tokenization w relying punctuation capitalization possible approach extracting sentence paragraph sentence tokenization paragraph punctuation lowercased specific need able split paragraph sentence expecting worst case paragraph inputted improper example sentence tokenizer tried far seems rely punctuation true casing using nltk sent tokenize
Is it possible to access via VBA the grammatical information that MS Word must have to perform its grammar checks?,"<p>I am trying to leverage the AI used in MS Word to ascertain the part of speech of every word in a doc file. That is, for a sentence like: &quot;The cat sat on it.&quot;, I would get something along the lines of: &quot;The [wdDeterminer] cat[wdNoun] sat[wdVerb] on[wdPreposition] it[wdPronoun]&quot;.</p>
<p>I know that Word must do this internally for its grammar proofing tools to work. My question is: is it possible for a script to access this information.</p>
<p>Please note, I am NOT trying to run spellcheck through VBA (though this will be a trivial part of the solution). Also, I am not interested in the thesaurus module.</p>
<p>There are solutions outside of VBA, such as the python Lemmatization with NLTK, but I was wondering if anyone knew of a purely VBA solution. My investigations so far suggest no, but I'm hoping I've overlooked something.</p>
",Preprocessing of the text & Tokenization,possible access via vba grammatical information word must perform grammar check trying leverage ai used word ascertain part speech every word doc file sentence like cat sat would get something along line wddeterminer cat wdnoun sat wdverb wdpreposition wdpronoun know word must internally grammar proofing tool work question possible script access information please note trying run spellcheck vba though trivial part solution also interested thesaurus module solution outside vba python lemmatization nltk wa wondering anyone knew purely vba solution investigation far suggest hoping overlooked something
How to identify full sentences in a scraped web page with Python,"<p>I am currently working on a school project and try to analyze articles on different web pages. With the help of BeautifulSoup I was able to clean out all code parts out of the content.</p>
<p>Now, I want to clean out the other parts like menus, sitemap entries, buttons etc. to only have full sentences as text from the web page. Do you know how I am able to identify full sentences like</p>
<pre><code>The sequel trilogy is the third installment of films of the Star Wars saga to be produced.
</code></pre>
<p>but to clean out words that do not make sense as a group like a navigation</p>
<pre><code>Explore Trending Navigation About Us Community
</code></pre>
<p>I already used the tokenization of words, but this is usually used to clean words in the sense of singular / plural, endings, stop words and so on. I want to have the text like it was written but without the &quot;noise&quot;.</p>
<p>I hope I was able to describe my problem in an understandable way.</p>
",Preprocessing of the text & Tokenization,identify full sentence scraped web page python currently working school project try analyze article different web page help beautifulsoup wa able clean code part content want clean part like menu sitemap entry button etc full sentence text web page know able identify full sentence like clean word make sense group like navigation already used tokenization word usually used clean word sense singular plural ending stop word want text like wa written without noise hope wa able describe problem understandable way
Creating an index from a list of words in Python (to vectorise words),"<p>So I have a long list of words called word_list (that contains duplicates) and also have a set word_set that is just the word_list but without duplicates.</p>
<pre><code>word_list = ['this','start','wonderland','amaze','this',.....]
word_set = set(word_list)
</code></pre>
<p>I also have a function that takes the word_list as input (raw_text):</p>
<pre><code>def CBOW(raw_text, window_size=2):
   data = []
   for i in range(window_size, len(raw_text) - window_size):
       context = [raw_text[i - window_size], raw_text[i - (window_size - 1)], raw_text[i + (window_size - 1)], raw_text[i + window_size]]
       target = raw_text[i]
       data.append((context, target))

   return data

# The returned data has the form: [(['this', 'start', 'amaze', 'this'], 'wonderland'), .....]
</code></pre>
<p>It returns the corresponding words within the window size of a word (here it is wonderland)</p>
<p>I would like to instead have this return the <strong>index</strong> of those words from word_set:</p>
<p>For example, instead of</p>
<pre><code>[(['this', 'start', 'amaze', 'this'], 'wonderland'), .....]
</code></pre>
<p>I would want</p>
<pre><code>[(['0', '1', '2', '0'], 'wonderland'), .....] (as 'this' is a duplicate value so has index 0 )
</code></pre>
<p>Could anyone help me with this task?</p>
<p>For testing I have put a more comprehensive word_list here (along with my function) <a href=""https://pastebin.com/EuS20u60"" rel=""nofollow noreferrer"">https://pastebin.com/EuS20u60</a></p>
",Preprocessing of the text & Tokenization,creating index list word python vectorise word long list word called word list contains duplicate also set word set word list without duplicate also function take word list input raw text return corresponding word within window size word wonderland would like instead return index word word set example instead would want could anyone help task testing put comprehensive word list along function
"How can I preprocess NLP text (lowercase, remove special characters, remove numbers, remove emails, etc) in one pass?","<p>How can I preprocess NLP text (lowercase, remove special characters, remove numbers, remove emails, etc) in one pass using Python?</p>

<pre><code>Here are all the things I want to do to a Pandas dataframe in one pass in python:
1. Lowercase text
2. Remove whitespace
3. Remove numbers
4. Remove special characters
5. Remove emails
6. Remove stop words
7. Remove NAN
8. Remove weblinks
9. Expand contractions (if possible not necessary)
10. Tokenize
</code></pre>

<p>Here's how I am doing it all individually:</p>

<pre><code>    def preprocess(self, dataframe):


    self.log.info(""In preprocess function."")

    dataframe1 = self.remove_nan(dataframe)
    dataframe2 = self.lowercase(dataframe1)
    dataframe3 = self.remove_whitespace(dataframe2)

    # Remove emails and websites before removing special characters
    dataframe4 = self.remove_emails(self, dataframe3)
    dataframe5 = self.remove_website_links(self, dataframe4)

    dataframe6 = self.remove_special_characters(dataframe5)
    dataframe7 - self.remove_numbers(dataframe6)
    self.remove_stop_words(dataframe8) # Doesn't return anything for now
    dataframe7 = self.tokenize(dataframe6)

    self.log.info(f""Sample of preprocessed data: {dataframe4.head()}"")

    return dataframe7

def remove_nan(self, dataframe):
    """"""Pass in a dataframe to remove NAN from those columns.""""""
    return dataframe.dropna()

def lowercase(self, dataframe):
    logging.info(""Converting dataframe to lowercase"")
    lowercase_dataframe = dataframe.apply(lambda x: x.lower())
    return lowercase_dataframe


def remove_special_characters(self, dataframe):
    self.log.info(""Removing special characters from dataframe"")
    no_special_characters = dataframe.replace(r'[^A-Za-z0-9 ]+', '', regex=True)
    return no_special_characters

def remove_numbers(self, dataframe):
    self.log.info(""Removing numbers from dataframe"")
    removed_numbers = dataframe.str.replace(r'\d+','')
    return removed_numbers

def remove_whitespace(self, dataframe):
    self.log.info(""Removing whitespace from dataframe"")
    # replace more than 1 space with 1 space
    merged_spaces = dataframe.str.replace(r""\s\s+"",' ')
    # delete beginning and trailing spaces
    trimmed_spaces = merged_spaces.apply(lambda x: x.str.strip())
    return trimmed_spaces

def remove_stop_words(self, dataframe):
    # TODO: An option to pass in a custom list of stopwords would be cool.
    set(stopwords.words('english'))

def remove_website_links(self, dataframe):
    self.log.info(""Removing website links from dataframe"")
    no_website_links = dataframe.str.replace(r""http\S+"", """")
    return no_website_links

def tokenize(self, dataframe):
    tokenized_dataframe = dataframe.apply(lambda row: word_tokenize(row))
    return tokenized_dataframe

def remove_emails(self, dataframe):
    no_emails = dataframe.str.replace(r""\S*@\S*\s?"")
    return no_emails

def expand_contractions(self, dataframe):
    # TODO: Not a priority right now. Come back to it later.
    return dataframe
</code></pre>
",Preprocessing of the text & Tokenization,preprocess nlp text lowercase remove special character remove number remove email etc one pas preprocess nlp text lowercase remove special character remove number remove email etc one pas using python individually
Text Preprocessing for NLP but from List of Dictionaries,"<p>I'm attempting to do an NLP project with a goodreads data set. my data set is a list of dictionaries. Each dictionary looks like so (the list is called 'reviews'):</p>
<pre><code>&gt;&gt;&gt; reviews[0]
{'user_id': '8842281e1d1347389f2ab93d60773d4d',
'book_id': '23310161',
'review_id': 'f4b4b050f4be00e9283c92a814af2670',
'rating': 4,
'review_text': 'Fun sequel to the original.',
'date_added': 'Tue Nov 17 11:37:35 -0800 2015',
'date_updated': 'Tue Nov 17 11:38:05 -0800 2015',
'read_at': '',
'started_at': '',
'n_votes': 7,
'n_comments': 0}
</code></pre>
<p>There are 700k+ of these dictionaries in my dataset.</p>
<p><strong>First question</strong>: I am only interested in the elements <code>'rating'</code> and <code>'review_text'</code>. I know I can delete elements from each dictionary, but how do I do it for all of the dictionaries?</p>
<p><strong>Second question</strong>: I am able to do sentence and word tokenization of an individual dictionary in the list by specifying the dictionary in the list, then the element <code>'review_text'</code> within the dictionary like so:</p>
<pre><code>paragraph = reviews[0]['review_text']
</code></pre>
<p>And then applying <code>sent_tokenize</code> and <code>word_tokenize</code> like so:</p>
<pre><code>print(sent_tokenize(paragraph))
print(word_tokenize(paragraph))
</code></pre>
<p>But how do I apply these methods to the entire data set? I am stuck here, and cannot even attempt to do any of the text preprocessing (lower casing, removing punctuation, lemmatizing, etc).</p>
<p>TIA</p>
",Preprocessing of the text & Tokenization,text preprocessing nlp list dictionary attempting nlp project goodreads data set data set list dictionary dictionary look like list called review k dictionary dataset first question interested element know delete element dictionary dictionary second question able sentence word tokenization individual dictionary list specifying dictionary list element within dictionary like applying like apply method entire data set stuck even attempt text preprocessing lower casing removing punctuation lemmatizing etc tia
Sentiment analysis Python tokenization,"<p>my problem is the follow: I want to do a sentiment analysis on Italian tweet and I would to tokenise and lemmatise my Italian text in order to find new analysis dimension for my thesis. The problem is that I would like to tokenise my hashtag, splitting also the composed one. For example if I have #nogreenpass, I would have  also without the # symbol, because the sentiment of the phrase would be better understand with all word of the text. How could I do this? I tried with sapCy, but I have no results. I created a function to clean my text, but I can't have the hashtag in the way I would. I'm using this code:</p>
<pre><code>import re
import spacy
from spacy.tokenizer import Tokenizer

nlp = spacy.load('it_core_news_lg')

# Clean_text function
def clean_text(text):
    text = str(text).lower()
    doc = nlp(text)
    text = re.sub(r'#[a-z0-9]+', str(' '.join(t in nlp(doc))), str(text))
    text = re.sub(r'\n', ' ', str(text)) # Remove /n
    text = re.sub(r'@[A-Za-z0-9]+', '&lt;user&gt;', str(text)) # Remove and replace @mention
    text = re.sub(r'RT[\s]+', '', str(text)) # Remove RT
    text = re.sub(r'https?:\/\/\S+', '&lt;url&gt;', str(text)) # Remove and replace links
    return text
</code></pre>
<p>For example here I don't know how add the first &lt; and last &gt; replacing the # symbol and the tokenisation process doesn't work as I would. Thank you for the time spent for me and for the patience. I hope to became stronger in the Jupiter analysis and python coding so I could give an help also to your problem. Thank you guys!</p>
",Preprocessing of the text & Tokenization,sentiment analysis python tokenization problem follow want sentiment analysis italian tweet would tokenise lemmatise italian text order find new analysis dimension thesis problem would like tokenise hashtag splitting also composed one example nogreenpass would also without symbol sentiment phrase would better understand word text could tried sapcy result created function clean text hashtag way would using code example know add first last replacing symbol tokenisation process work would thank time spent patience hope became stronger jupiter analysis python coding could give help also problem thank guy
BPE vs WordPiece Tokenization - when to use / which?,"<p>What's the general tradeoff between choosing BPE vs WordPiece Tokenization? When is one preferable to the other? Are there any differences in model performance between the two? I'm looking for a general overall answer, backed up with specific examples. Thanks!</p>
",Preprocessing of the text & Tokenization,bpe v wordpiece tokenization use general tradeoff choosing bpe v wordpiece tokenization one preferable difference model performance two looking general overall answer backed specific example thanks
Remove stopwords using spaCy from list dataframe,"<p>I want to remove stopwords using spaCy after tokenize. But, given me an error and the error is <code>AttributeError: 'str' object has no attribute 'is_stop'</code> The data I want to do is the data after the tokenizing process which is in column named 'tokenizing'
How to fix it?</p>
<pre><code>import pandas as pd
from spacy.lang.id import Indonesian

nlp = Indonesian()

data = [
    'Aku suka sekali beradai di wilayah yang dingin',
    'Kembali jika terjadi sesuatu di sana',
    'Sampai berapa lama kamu akan pergi dari sini',
]
df = pd.DataFrame({'text': data})
df['text'] = df['text'].str.lower()
df.head()

#Tokenizing
def tokenize(word):
  return [token.text for token in nlp(word)]

df['tokenizing'] = df['text'].apply(tokenize)
df.head()

#Remove stopwords
def stopwords_remover(words):
  return [stopwords for stopwords in words if not stopwords.is_stop]

df['stopwords'] = df['tokenizing'].apply(stopwords_remover)
df
</code></pre>
",Preprocessing of the text & Tokenization,remove stopwords using spacy list dataframe want remove stopwords using spacy tokenize given error error data want data tokenizing process column named tokenizing fix
What to do if the padding is of different length on test data in NLP?,"<p>I am having a confusion right now regarding the <code>maxlen</code> in <code>pad_sequences()</code>. Currently I am doing Fake News Classification. My question is while processing the test data what is the <code>maxlen</code> is longer than the training data. Let me give you my situation.</p>
<p>My code snippet:</p>
<pre><code>tokenizer = Tokenizer()
tokenizer.fit_on_texts(text)

text_tokenizer = tokenizer.texts_to_sequences(text)

</code></pre>
<pre><code>len(text_tokenizer) # Getting 18285
</code></pre>
<pre><code>max([len(x) for x in text_tokenizer])
</code></pre>
<p>I am getting <code>maxlen</code> = 51.</p>
<pre><code>text_tokenizer_pad = pad_sequences(text_tokenizer, maxlen=max([len(x) for x in text_tokenizer]))
</code></pre>
<p>I am doing padding based on this maxlen size which is 51.
If I see the shape of <code>text_tokenizer_pad</code>, I get <code>(18285, 51)</code></p>
<pre><code>text_tokenizer_pad.shape
</code></pre>
<p>This is what I did in training part.</p>
<p>Let me show you what I did for testing part.</p>
<pre><code>text_test_tokenizer = tokenizer.texts_to_sequences(text_test)
</code></pre>
<p>As u can see I didn't <code>fit_on_texts()</code> on test data.</p>
<pre><code>len(text_test_tokenizer) # Gives 4575
</code></pre>
<pre><code>max([len(x) for x in text_test_tokenizer])
</code></pre>
<p>This gives <code>maxlen</code> as 43, which is less than 51. Fine.</p>
<p>Now I am padding the test data with the same length as training data i.e. 51 because it is longer than test data length and I used in <code>train_test_split</code> and in training purpose. So the columns will be 51 there while training.</p>
<pre><code>text_test_tokenizer_pad = pad_sequences(text_test_tokenizer, maxlen=max([len(x) for x in text_tokenizer]))
</code></pre>
<p>After that I am predicting. I am 2 questions regarding all of this-</p>
<ol>
<li>Am I doing this correctly, I mean the tokenize and padding part on train data and test data ?</li>
<li>What if the <code>maxlen</code> for test data is longer than training data, what value should I choose then, because <code>text_tokenize_pad</code> is the one I used in <code>train_test_split</code></li>
</ol>
<pre><code>x_train, x_test, y_train, y_test = train_test_split(text_tokenizer_pad, labels, test_size = 0.2, shuffle=True, random_state=42)
</code></pre>
<p>I tried with normal constant value say choose <code>maxlen</code> as 100 for both train and test. But this is not a perfect solution. There can always be a test data longer than 100.</p>
",Preprocessing of the text & Tokenization,padding different length test data nlp confusion right regarding currently fake news classification question processing test data longer training data let give situation code snippet getting padding based maxlen size see shape get training part let show testing part u see test data give le fine padding test data length training data e longer test data length used training purpose column training predicting question regarding correctly mean tokenize padding part train data test data test data longer training data value choose one used tried normal constant value say choose train test perfect solution always test data longer
NLP: Tokenize : TypeError: expected string or bytes-like object,"<blockquote>
<p><strong>I also tried .apply(str) and .astype(str) before tokenization, yet I get TypeError: expected string or bytes-like object.</strong></p>
</blockquote>
<hr />
<pre><code>data.info()

&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 8 entries, 0 to 7
Data columns (total 3 columns):
 #   Column           Non-Null Count  Dtype 
---  ------           --------------  ----- 
 0   tag              8 non-null      object
 1   clean_patterns   8 non-null      object
 2   clean_responses  8 non-null      object
dtypes: object(3)
memory usage: 320.0+ bytes
</code></pre>
<hr />
<blockquote>
<p><strong>I am trying to word_tokenize the data for the NLP chatbot.</strong></p>
</blockquote>
<hr />
<pre><code>print(word_tokenize(data))
</code></pre>
<blockquote>
<p>TypeError                                 Traceback (most recent call
last)  in 
----&gt; 1 print(word_tokenize(data))</p>
<p>D:\anaconda\lib\site-packages\nltk\tokenize_<em>init</em>_.py in
word_tokenize(text, language, preserve_line)
128     :type preserve_line: bool
129     &quot;&quot;&quot;
--&gt; 130     sentences = [text] if preserve_line else sent_tokenize(text, language)
131     return [
132         token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)</p>
<p>D:\anaconda\lib\site-packages\nltk\tokenize_<em>init</em>_.py in
sent_tokenize(text, language)
106     &quot;&quot;&quot;
107     tokenizer = load(&quot;tokenizers/punkt/{0}.pickle&quot;.format(language))
--&gt; 108     return tokenizer.tokenize(text)
109
110</p>
<p>D:\anaconda\lib\site-packages\nltk\tokenize\punkt.py in tokenize(self,
text, realign_boundaries)    1272         Given a text, returns a list
of the sentences in that text.    1273         &quot;&quot;&quot;
-&gt; 1274         return list(self.sentences_from_text(text, realign_boundaries))    1275     1276     def debug_decisions(self,
text):</p>
<p>D:\anaconda\lib\site-packages\nltk\tokenize\punkt.py in
sentences_from_text(self, text, realign_boundaries)    1326<br />
follows the period.    1327         &quot;&quot;&quot;
-&gt; 1328         return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]    1329     1330     def _slices_from_text(self,
text):</p>
<p>D:\anaconda\lib\site-packages\nltk\tokenize\punkt.py in (.0)
1326         follows the period.    1327         &quot;&quot;&quot;
-&gt; 1328         return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]    1329     1330     def _slices_from_text(self,
text):</p>
<p>D:\anaconda\lib\site-packages\nltk\tokenize\punkt.py in
span_tokenize(self, text, realign_boundaries)    1316         if
realign_boundaries:    1317             slices =
self._realign_boundaries(text, slices)
-&gt; 1318         for sl in slices:    1319             yield (sl.start, sl.stop)    1320</p>
<p>D:\anaconda\lib\site-packages\nltk\tokenize\punkt.py in
_realign_boundaries(self, text, slices)    1357         &quot;&quot;&quot;    1358         realign = 0
-&gt; 1359         for sl1, sl2 in _pair_iter(slices):    1360             sl1 = slice(sl1.start + realign, sl1.stop)    1361             if not
sl2:</p>
<p>D:\anaconda\lib\site-packages\nltk\tokenize\punkt.py in _pair_iter(it)
314     it = iter(it)
315     try:
--&gt; 316         prev = next(it)
317     except StopIteration:
318         return</p>
<p>D:\anaconda\lib\site-packages\nltk\tokenize\punkt.py in
_slices_from_text(self, text)    1330     def _slices_from_text(self, text):    1331         last_break = 0
-&gt; 1332         for match in self._lang_vars.period_context_re().finditer(text):    1333<br />
context = match.group() + match.group(&quot;after_tok&quot;)    1334<br />
if self.text_contains_sentbreak(context):</p>
<p>TypeError: expected string or bytes-like object</p>
</blockquote>
",Preprocessing of the text & Tokenization,nlp tokenize typeerror expected string byte like object also tried apply str astype str tokenization yet get typeerror expected string byte like object trying word tokenize data nlp chatbot typeerror traceback recent call last print word tokenize data anaconda lib site package nltk tokenize init py word tokenize text language preserve line type preserve line bool sentence text preserve line else sent tokenize text language return token sent sentence token treebank word tokenizer tokenize sent anaconda lib site package nltk tokenize init py sent tokenize text language tokenizer load tokenizers punkt pickle format language return tokenizer tokenize text anaconda lib site package nltk tokenize punkt py tokenize self text realign boundary given text return list sentence text return list self sentence text text realign boundary def debug decision self text anaconda lib site package nltk tokenize punkt py sentence text self text realign boundary follows period return text e e self span tokenize text realign boundary def slice text self text anaconda lib site package nltk tokenize punkt py follows period return text e e self span tokenize text realign boundary def slice text self text anaconda lib site package nltk tokenize punkt py span tokenize self text realign boundary realign boundary slice self realign boundary text slice sl slice yield sl start sl stop anaconda lib site package nltk tokenize punkt py realign boundary self text slice realign sl sl pair iter slice sl slice sl start realign sl stop sl anaconda lib site package nltk tokenize punkt py pair iter iter try prev next except stopiteration return anaconda lib site package nltk tokenize punkt py slice text self text def slice text self text last break match self lang var period context finditer text context match group match group tok self text contains sentbreak context typeerror expected string byte like object
Is there a way to remove 2 words from a sentence only if they occur consecutively?,"<p>I am trying to remove words from a dataframe column only if the two words occur consecutively.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">text</th>
<th style=""text-align: left;"">user</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">boy live on island</td>
<td style=""text-align: left;"">user1</td>
</tr>
<tr>
<td style=""text-align: left;"">mall on the street boy live go</td>
<td style=""text-align: left;"">user2</td>
</tr>
</tbody>
</table>
</div>
<p>in the above example, I'd like to remove the words <strong>boy</strong> and <strong>live</strong> if they occur consecutively to obtain output as follows:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">text</th>
<th style=""text-align: left;"">user</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">on island</td>
<td style=""text-align: left;"">user1</td>
</tr>
<tr>
<td style=""text-align: left;"">mall on the street go</td>
<td style=""text-align: left;"">user2</td>
</tr>
</tbody>
</table>
</div>
<p>I am using nltk stopwords to remove stopwords. But I think it can only contain single words.</p>
",Preprocessing of the text & Tokenization,way remove word sentence occur consecutively trying remove word dataframe column two word occur consecutively text user boy live island user mall street boy live go user example like remove word boy live occur consecutively obtain output follows text user island user mall street go user using nltk stopwords remove stopwords think contain single word
Pattern extract using Regex in Python,"<p>I am trying to use regex for word extraction in python, since I am beginner and not experienced in regex I want you to help me, I have this String :</p>
<pre><code>Deadline for NSF-BSF programs in Elementary Particle Physics – Theory; Particle Astrophysics and Cosmology – Theory; Quantum Information Science (NSF deadline is Dec. 14)
</code></pre>
<p>And I want the output to be a list of area or research in this word, so the output should be:</p>
<pre><code>[Elementary Particle Physics, Particle Astrophysics and Cosmology, Quantum Information Science]
</code></pre>
<p>could any one give regular expression to identify this pattern using <code>re.findall()</code>.
Thanks in advance!</p>
",Preprocessing of the text & Tokenization,pattern extract using regex python trying use regex word extraction python since beginner experienced regex want help string want output list area research word output could one give regular expression identify pattern using thanks advance
Normalization words for sentiment analysis,"<p>I'm currently doing sentiment analysis and having a problem.</p>
<p>I have a big normalization for word and I want to normalization text before tokenize like this example:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>data</th>
<th>normal</th>
</tr>
</thead>
<tbody>
<tr>
<td>kamu knp sayang</td>
<td>kamu kenapa sayang</td>
</tr>
<tr>
<td>drpd sedih mending belajar</td>
<td>dari pada sedih mending belajar</td>
</tr>
<tr>
<td>dmna sekarang</td>
<td>di mana sekarang</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>knp: kenapa</li>
<li>drpd: dari pada</li>
<li>dmna: di mana</li>
</ul>
<p>This is my code:</p>
<pre><code>import pandas as pd

slang = pd.DataFrame({'before': ['knp', 'dmna', 'drpd'], 'after': ['kenapa', 'di mana', 'dari pada']})
df = pd.DataFrame({'data': ['kamu knp sayang', 'drpd sedih mending bermain']})
                  
normalisasi = {}

for index, row in slang.iterrows():
  if row[0] not in normalisasi:
    normalisasi[row[0]] = row[1]


def normalized_term(document):
    return [normalisasi[term] if term in normalisasi else term for term in document]

df['normal'] = df['data'].apply(normalized_term)
df
</code></pre>
<p>But, the result like this:
<a href=""https://i.sstatic.net/GCpCB.png"" rel=""nofollow noreferrer"">result</a></p>
<p>I want the result like the example table.</p>
",Preprocessing of the text & Tokenization,normalization word sentiment analysis currently sentiment analysis problem big normalization word want normalization text tokenize like example data normal kamu knp sayang kamu kenapa sayang drpd sedih mending belajar dari pada sedih mending belajar dmna sekarang di mana sekarang knp kenapa drpd dari pada dmna di mana code result like result want result like example table
Labelling for analysis sentiment with file,"<p>I have a data called:</p>
<ul>
<li>after_tokenize.xlsx</li>
<li>positive.xlsx</li>
<li>negative.xlsx
<a href=""https://i.sstatic.net/7JADW.png"" rel=""nofollow noreferrer"">after tokenize</a>
<a href=""https://i.sstatic.net/THRI6.png"" rel=""nofollow noreferrer"">positive</a>
<a href=""https://i.sstatic.net/BZuyJ.png"" rel=""nofollow noreferrer"">negative</a></li>
</ul>
<p>What I want to is labelling sentiment positive and negative for data from after_tokenize.xlsx. If data on after tokenize have a lot of positive word from data positive.xlsx it will be positive and If data have a lot negative word from negative it will be negative. the result will be entered into a label named label.
sample:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>data</th>
<th>label</th>
</tr>
</thead>
<tbody>
<tr>
<td>[i, like, love, hate, you]</td>
<td>positive</td>
</tr>
<tr>
<td>[i, worst, hate, like, you]</td>
<td>negative</td>
</tr>
</tbody>
</table>
</div>
<pre><code>import pandas as pd
import nltk

df = pd.DataFrame({'data': ['i like love hate you', 'i dont hate like you']})
pos = pd.DataFrame(data=['like', 'love'], columns=['positive'])
neg = pd.DataFrame(data=['dont', 'hate'], columns=['negative'])
df['data'] = df.apply(lambda row: nltk.word_tokenize(row['data']), axis=1)
</code></pre>
",Preprocessing of the text & Tokenization,labelling analysis sentiment file data called tokenize xlsx positive xlsx negative xlsx tokenize positive negative want labelling sentiment positive negative data tokenize xlsx data tokenize lot positive word data positive xlsx positive data lot negative word negative negative result entered label named label sample data label like love hate positive worst hate like negative
text data cleaning for NLP,"<p>I am having a project in NLP, where I have to clean text data, even though I have done most of it, I am finding it a challenge to clean the following text format.</p>
<p>[&quot;data-science&quot;]</p>
<p>After cleaning it should become</p>
<p>[&quot;data&quot;, &quot;science&quot;]</p>
",Preprocessing of the text & Tokenization,text data cleaning nlp project nlp clean text data even though done finding challenge clean following text format data science cleaning become data science
Transcript transformation for sentiment analysis,"<p>I'm doing sentiment analysis on users' transcripts for UX website testing. I get the transcript from the testing session and then I analyze the transcript for sentiment analysis - what's the user's opinion about the website, what problems did the user encounter, whether he had any problems, got stuck, lost. Since this is quite domain-specific, I'm testing both TextBlob and Vader and see which gives better results. My issue is at the beginning of the process - the speech-to-text API's transcript isn't perfect. <strong>Sentences (periods) are not captured or are minimal</strong>. <strong>I'm not sure on what level the analysis should be since I was hoping I could do it on sentence-level</strong>. I tried making n-grams and analyzing those short chunks of text, but it isn't ideal and the results are slightly difficult to read - because there will be some parts that are repeated. Apart from this, I do classical text cleaning, tokenization, pos tagging, lemmatization and feed it to TextBlob and Vader.</p>
<p>Transcript example: <code>okay so if I go just back over here it has all the information I need it seems like which is great so I'm pretty impressed with it similar to how a lot of government websites are set up over here it looks like I have found all the information I need it's a great website it has everything  overall though it had more than enough information... </code></p>
<p>I did:</p>
<pre><code>ngram_object = TextBlob(lines)
ngrams = ngram_object.ngrams(n=4) 
</code></pre>
<p>which gives me something like (actually a WordList): <code>[okay so if I, so if I go, if I go just...]</code></p>
<p>Then the results look like:</p>
<pre><code>62  little bit small    -0.21875    Negative  
61  like little bit     -0.18750    Negative

0 information hard find not see -0.291666667    Negative
1 hard find not see information -0.291666667    Negative
</code></pre>
<p>Is there a better way to analyze unstructured text in chunks rather than a full transcript?</p>
<p>This makes it difficult to capture what was the issue with the website. Changing the API isn't really an option since I'm working with something that was given to me to use as data collection for this particular sentiment analysis problem.</p>
<p>Any tips or suggestions would be highly appreciated, couldn't find anyone doing something similar to this.</p>
",Preprocessing of the text & Tokenization,transcript transformation sentiment analysis sentiment analysis user transcript website testing get transcript testing session analyze transcript sentiment analysis user opinion website problem user encounter whether problem got stuck lost since quite domain specific testing textblob vader see give better result issue beginning process speech text api transcript perfect sentence period captured minimal sure level analysis since wa hoping could sentence level tried making n gram analyzing short chunk text ideal result slightly difficult read part repeated apart classical text cleaning tokenization po tagging lemmatization feed textblob vader transcript example give something like actually wordlist result look like better way analyze unstructured text chunk rather full transcript make difficult capture wa issue website changing api really option since working something wa given use data collection particular sentiment analysis problem tip suggestion would highly appreciated find anyone something similar
Text preprocessing for fasttext pretrained models,"<p>I want to use pretreained fastext model for language detection: <a href=""https://fasttext.cc/docs/en/language-identification.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/language-identification.html</a> . Where can I find <strong>the exact Python code</strong> for text preprocessing used for training this specific model? I am not interested in general answers about how should we prepare text for using models - I ma looking for identical transformations as those used for training.</p>
",Preprocessing of the text & Tokenization,text preprocessing fasttext pretrained model want use pretreained fastext model language detection find exact python code text preprocessing used training specific model interested general answer prepare text using model looking identical transformation used training
Removing punctuation marks in tokenization nltk with dataframe (python),"<p>I have some text that I was able to process from stop words, links, emoticons, etc. After tokenizing my dataframe, I get a not-so-good picture. There are a lot of extra punctuation marks that are identified as separate words and appear in the processed text. Add an image
<a href=""https://i.sstatic.net/57SQI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/57SQI.png"" alt=""enter image description here"" /></a></p>
<p>For this I use the following command:</p>
<pre><code>Data_preprocessing['clean_custom_content_tokenize'] = Data_preprocessing['tweet_without_stopwords'].apply(nltk.word_tokenize)

</code></pre>
<p>As you can see, there are many characters like dashes, colons, etc.The question immediately pops up, why not apply the removal of punctuation before tokenization. The point is that there are decimal values in the text that I need. Removing punctuation marks before tokenization splits them into two words, which is not correct.</p>
<p>An example of what happens when you remove punctuation marks before tokenization:</p>
<pre><code>custom_pipeline2 = [preprocessing.remove_punctuation]
Data_preprocessing['clean_custom_content_tokenize'] = Data_preprocessing['tweet_without_stopwords'].pipe(hero.clean, custom_pipeline2)
Data_preprocessing['clean_custom_content_tokenize'] = Data_preprocessing['clean_custom_content_tokenize'].apply(nltk.word_tokenize)
</code></pre>
<p><a href=""https://i.sstatic.net/n9Owo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/n9Owo.png"" alt=""enter image description here"" /></a></p>
<p>I have found a couple of examples how to solve my sign problem but when the data is not a data frame but a string. Can you somehow customize nltk tokenization? Or use some kind of regular expression to process the resulting list later?</p>
",Preprocessing of the text & Tokenization,removing punctuation mark tokenization nltk dataframe python text wa able process stop word link emoticon etc tokenizing dataframe get good picture lot extra punctuation mark identified separate word appear processed text add image use following command see many character like dash colon etc question immediately pop apply removal punctuation tokenization point decimal value text need removing punctuation mark tokenization split two word correct example happens remove punctuation mark tokenization found couple example solve sign problem data data frame string somehow customize nltk tokenization use kind regular expression process resulting list later
tokenizing text with features in specif format,"<p>Hello there I am trying to create tokens with some features and arrange them in some kind of JSON format, using the following text example:</p>
<pre><code>words = ['The study of aviation safety report in the aviation industry usually relies', 
         'The experimental results show that compared with traditional',
         'Heterogeneous Aviation Safety Cases: Integrating the Formal and the Non-formal']
</code></pre>
<pre><code>{&quot;sentence&quot;: [
           {
             indexSentence:0,
             tokens: [{
                       &quot;indexWord&quot;: 1,
                        &quot;word&quot;: &quot;The&quot;,
                         &quot;len&quot;: 3
                      },
                      { &quot;indexWord&quot;: 2,
                        &quot;word&quot;: &quot;study&quot;,
                         &quot;len&quot;: 5},
                      {&quot;indexWord&quot;: 3,
                        &quot;word&quot;: &quot;of&quot;,
                         &quot;len&quot;: 2
                       },
                       {&quot;indexWord&quot;: 4,
                        &quot;word&quot;: &quot;aviation&quot;,
                         &quot;len&quot;: 8},
                        ...
                        ]
           },
           {
            &quot;indexSentence&quot; : 1,
            &quot;tokens&quot; : [{
                        ...
                        }]
           },
           ....
         ]}
</code></pre>
<p>I trying to use the following code with no success...</p>
<pre><code>t_d = {len(i):i for i in words}

[{'Lon' : len(t_d[i]),
  'tex' : t_d[i], 
  'Sub' : [{'index' : j,
            'token': [{
                      'word':['word: ' + j for i,j in enumerate(str(t_d[i]).split(' '))] 
                      }],
            'lenTo' : len(str(t_d[i]).split(' '))
           }
          ],
  'Sub1':[{'index' : j}]
 } for j,i in enumerate(t_d)]
</code></pre>
",Preprocessing of the text & Tokenization,tokenizing text feature specif format hello trying create token feature arrange kind json format using following text example trying use following code success
Call function for column in datafame,"<p>I've successfully imported these libraries</p>
<pre><code>import pandas as pd
from underthesea import word_tokenize
</code></pre>
<p>and load data</p>
<pre><code>df=pd.read_csv(&quot;/content/gdrive/MyDrive/BigData/BookingReview.csv&quot;)
print (df)
</code></pre>
<p><a href=""https://i.sstatic.net/vU79d.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vU79d.png"" alt=""here is image of data"" /></a></p>
<p>I tried to use function word_tokenize for the column named 'Review'</p>
<pre><code>df['Review']=df['Review'].apply(word_tokenize)
</code></pre>
<p>but it show this error</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-80-386271f3d8ee&gt; in &lt;module&gt;()
----&gt; 1 df['Review']=df['Review'].apply(word_tokenize)

3 frames
/usr/local/lib/python3.7/dist-packages/pandas/core/series.py in apply(self, func, convert_dtype, args, **kwds)
   4211             else:
   4212                 values = self.astype(object)._values
-&gt; 4213                 mapped = lib.map_infer(values, f, convert=convert_dtype)
   4214 
   4215         if len(mapped) and isinstance(mapped[0], Series):

pandas/_libs/lib.pyx in pandas._libs.lib.map_infer()

/usr/local/lib/python3.7/dist-packages/underthesea/pipeline/word_tokenize/__init__.py in word_tokenize(sentence, format)
     32     'Bác_sĩ bây_giờ có_thể thản_nhiên báo_tin bệnh_nhân bị ung_thư'
     33     &quot;&quot;&quot;
---&gt; 34     tokens = tokenize(sentence)
     35     crf_model = CRFModel.instance()
     36     output = crf_model.predict(tokens, format)

/usr/local/lib/python3.7/dist-packages/underthesea/pipeline/word_tokenize/regex_tokenize.py in tokenize(text, format, tag)
    233     :return: tokenize text
    234     &quot;&quot;&quot;
--&gt; 235     text = Text(text)
    236     text = text.replace(&quot;\t&quot;, &quot; &quot;)
    237     matches = [m for m in re.finditer(patterns, text)]

/usr/local/lib/python3.7/dist-packages/underthesea/feature_engineering/text.py in Text(text)
      9     &quot;&quot;&quot;
     10     if not is_unicode(text):
---&gt; 11         text = text.decode(&quot;utf-8&quot;)
     12     text = unicodedata.normalize(&quot;NFC&quot;, text)
     13     return text

AttributeError: 'float' object has no attribute 'decode'
</code></pre>
<p>I used the function above to seperate word for sentiment analysing.
I've tried to found the answer for my problem and still don't know why this error occurs. Any help is greatly appreciated. Thanks all!</p>
",Preprocessing of the text & Tokenization,call function column datafame successfully imported library load data tried use function word tokenize column named review show error used function seperate word sentiment analysing tried found answer problem still know error occurs help greatly appreciated thanks
How to improve NLTK sentence segmentation?,"<p>Given the paragraph from Wikipedia:</p>
<blockquote>
<p>An ambitious campus expansion plan was proposed by Fr. Vernon F.
Gallagher in 1952. Assumption Hall, the first student dormitory, was
opened in 1954, and Rockwell Hall was dedicated in November 1958,
housing the schools of business and law. It was during the tenure of
F. Henry J. McAnulty that Fr. Gallagher's ambitious plans were put to
action.</p>
</blockquote>
<p>I run NLTK <code>nltk.sent_tokenize</code> to get the sentences. This returns:</p>
<pre><code>['An ambitious campus expansion plan was proposed by Fr.', 
'Vernon F. Gallagher in 1952.', 
'Assumption Hall, the first student dormitory, was opened in 1954, and Rockwell Hall was dedicated in November 1958, housing the schools of business and law.', 
'It was during the tenure of Fr.', 
'Henry J. McAnulty that Fr. Gallagher's ambitious plans were put to action.'
 ] 
</code></pre>
<p>While NTLK could handle <strong>F. Henry J. McAnulty</strong> as one entity,
It failed for <strong>Fr. Vernon F. Gallagher</strong>, and this broke the sentence into two.</p>
<p>The correct tokenization should be:</p>
<pre><code>[
'An ambitious campus expansion plan was proposed by Fr. Vernon F. Gallagher in 1952.', 
'Assumption Hall, the first student dormitory, was opened in 1954, and Rockwell Hall was dedicated in November 1958, housing the schools of business and law.', 
'It was during the tenure of Fr. Henry J. McAnulty that Fr. Gallagher's ambitious plans were put to action.'
 ] 
</code></pre>
<p>How can I improve the tokenizer performance?</p>
",Preprocessing of the text & Tokenization,improve nltk sentence segmentation given paragraph wikipedia ambitious campus expansion plan wa proposed fr vernon f gallagher assumption hall first student dormitory wa opened rockwell hall wa dedicated november housing school business law wa tenure f henry j mcanulty fr gallagher ambitious plan put action run nltk get sentence return ntlk could handle f henry j mcanulty one entity failed fr vernon f gallagher broke sentence two correct tokenization improve tokenizer performance
How to replace BERT tokenizer special tokens,"<p>I am using an AutoTokenizer --&gt; <code>tokenizer1 = AutoTokenizer.from_pretrained(&quot;vinai/bertweet-base&quot;, normalization=True)</code> which is more complete than the tokenizer of bert-base-uncased. The problem is that when I tokenize some text, this tokenizer has different special tokens:</p>
<pre><code>special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'sep_token': '&lt;/s&gt;', 'pad_token': '&lt;pad&gt;', 'cls_token': '&lt;s&gt;', 'mask_token': '&lt;mask&gt;'})&gt; 
</code></pre>
<p>whereas the bert-base-uncased tokenizer, has these ones:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, normalization=True)
</code></pre>
<pre><code>special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})&gt;
</code></pre>
<p>I would like to replace these special tokens in order to make them fit for bert-base-uncased, so it doesn´t raise an error.</p>
",Preprocessing of the text & Tokenization,replace bert tokenizer special token using autotokenizer complete tokenizer bert base uncased problem tokenize text tokenizer ha different special token whereas bert base uncased tokenizer ha one would like replace special token order make fit bert base uncased raise error
How to sum up the word frequencies after stemming in Racket?,"<p>As background I'm trying to make a NLP application in Racket and I arrived at the part where I have to stem the words (I also obtained their frequency).</p>
<p>I am using the <code>(planet dyoo/porter-stemmer)</code> package in order to stem, and as an example we can write:</p>
<pre><code>(map (λ(x) (list (stem (first x)) (second x)))
     '((&quot;cryed&quot; 1)
       (&quot;racketeer&quot; 2)
       (&quot;crying&quot; 3)
       (&quot;playing&quot; 4)
       (&quot;racketing&quot; 5)
       (&quot;plays&quot; 6)
       (&quot;Racket&quot; 7)))
</code></pre>
<p>Which produces: <code>'((&quot;cry&quot; 1) (&quot;racket&quot; 2) (&quot;cry&quot; 3) (&quot;plai&quot; 4) (&quot;racket&quot; 5) (&quot;plai&quot; 6) (&quot;racket&quot; 7))</code></p>
<p>Now my goal is to sum up the frequency for each term, aka to arrive at: <code>'((&quot;cry&quot; 4) (&quot;racket&quot; 14) (&quot;plai&quot; 10))</code></p>
<p>I came up with a way to do it, but I don't like my solution:</p>
<pre><code>(define (frequency string)
  (map (λ(x) (list (first x) (length x)))
       (group-by (λ(x) x) (string-split string))))

(define (recalculate lst)
  (frequency
   (string-join
    (flatten
     (map (λ(x) (make-list (second x) (first x))) lst)))))
</code></pre>
<p>Basically I retype each word as many times as it's frequency, then make a single string containing all words and finally compute the frequency again. Is there a simpler(faster) way to achieve this?</p>
<p>I should perhaps add that the order doesn't matter (&quot;plai&quot; can come up before &quot;cry&quot; and so on). Also I'm looking for a simpler solution because I'm gonna have to use larger datasets and I want to make this faster (I'd also be glad even if the <code>frequency</code> function can be made more faster).</p>
",Preprocessing of the text & Tokenization,sum word frequency stemming racket background trying make nlp application racket arrived part stem word also obtained frequency using package order stem example write produce goal sum frequency term aka arrive came way like solution basically retype word many time frequency make single string containing word finally compute frequency simpler faster way achieve perhaps add order matter plai come cry also looking simpler solution gon na use larger datasets want make faster also glad even function made faster
Tokenize entities in dataframe,"<p>I'm looking for a way to tokenize my entities directly in my dataframe with associated Tag.</p>
<p>my tokenization applies to elisions as for example :</p>
<pre><code>[&quot;d'Angers&quot;] =&gt; [&quot;d'&quot;, &quot;Angers&quot;] 
[&quot;l'impératrice&quot; ] =&gt; [&quot;l'&quot;, &quot;impératrice&quot;]
</code></pre>
<p><strong>Input dataframe</strong> :</p>
<pre><code>Sentence  Mention  Tag
3   Vincennes   B-LOCATION
3   .   O

4   Confirmation    O
4   des O
4   privilèges  O
4   de  O
4   la  O
4   ville   O
4   d'Aire  O
4   1   O
4   ,   O
4   au  O
4   bailliage   B-ORGANISATION
4   d'Amiens    I-ORGANISATION
4 .

5 Projet O
5 de O
5 &quot; O
5 tour O
5 de O
5 l'impératrice B-TITLE
5 Eugénie B-PERSON
5 .

6 session
6 à O
6 l'ONU B-ORGANISATION
6 du
6 17
6 mai
6 .

</code></pre>
<p><strong>Expected output</strong> :</p>
<pre><code>Sentence  Mention  Tag
3   Vincennes   B-LOCATION
3   .   O

4   Confirmation    O
4   des O
4   privilèges  O
4   de  O
4   la  O
4   ville   O
4   d'Aire  O
4   1   O
4   ,   O
4   au  O
4   bailliage   B-ORGANISATION
4   d' I-ORGANISATION
4   Amiens    I-ORGANISATION
4 . 

5 Projet O
5 de O
5 &quot; O
5 tour O
5 de O
5 l' O
5 impératrice B-TITLE
5 Eugénie B-PERSON
5 .

6 session
6 à O
6 l' O
6 ONU B-ORGANISATION
6 du
6 17
6 mai
6 .
</code></pre>
<p>the difficulty is to be able to keep the label associated with the tokenized mention.
If anyone has any leads, thank you in advance.</p>
",Preprocessing of the text & Tokenization,tokenize entity dataframe looking way tokenize entity directly dataframe associated tag tokenization applies elision example input dataframe expected output difficulty able keep label associated tokenized mention anyone ha lead thank advance
How to split regex resulting list by new line after stemming and removing punctuation?,"<p>The resulting files are two  very long one-element list where all the processed text put together.
I tried to move the list.append command under the if else statement and I got a very huge list where every few words are lumped together followed by the same previous words with some new words added to them until I get a full sentence that I am after, then it starts to do the same with the next match.
I am sure it can be solved with a better loop.
I also tried to work with the resulting files but it's quite inefficient as I no longer have any basis to split them.
is it possible that this is a result of the &quot;or&quot; operand in the written regular expression ?</p>
<pre><code>import csv
import re 
import string
import nltk
from nltk.tokenize import punkt, word_tokenize 
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
import langid

h=SnowballStemmer(&quot;hungarian&quot;) # hungarian stemmer
stop_words=set(stopwords.words(&quot;hungarian&quot;)) # - {&quot;Nem,nem&quot;} 
i=0.0
j=0.0
latin_counter=0.0
result=[]
result2=[]
tokenized_txt=[]
tokenized_txt_latin=[]
unstemmed_list=[]
auxlist=[]

stop_words_latin={'ab', 'ac', 'ad', 'adhic', 'aliqui', 'aliquis', 'an', 'ante', 'apud', 'at', 'atque',
 'aut', 'autem', 'cum', 'cur', 'de', 'deinde', 'dum', 'ego', 'enim', 'ergo', 'es', 'est', 'et', 'etiam', 'etsi', 'ex', 'fio', 'haud', 
 'hic', 'iam', 'idem', 'igitur', 'ille', 'in', 'infra', 'inter', 'interim', 'ipse', 'is', 'ita', 'magis', 'modo',
 'mox', 'nam', 'ne', 'nec', 'necque', 'neque', 'nisi', 'non', 'nos', 'o', 'ob', 'per', 'possum', 'post', 'pro', 'quae', 'quam', 'quare', 'qui',
 'quia', 'quicumque', 'quidem', 'quilibet', 'quis', 'quisnam', 'quisquam', 'quisque', 'quisquis', 'quo', 'quoniam', 'sed', 'si', 'sic',
 'sive', 'sub', 'sui', 'sum', 'super', 'suus', 'tam', 'tamen', 'trans', 'tu', 'tum', 'ubi', 'uel', 'uero'}

with open('data/onkology.csv', 'r') as csv_file:
    csv_reader= csv.reader(csv_file  ,delimiter=';') 

    exp=(r'[l L]u.r[o i]n\b|(\w)*peptyl\b|(\w)*lutamid\b')
    for line in csv_reader:
            i+= 1
            for lineElem in line: 
               
                if  (re.search(exp,lineElem) and len(lineElem)&gt;80) : 
                    result2.append(lineElem) # if we want to see what we matched 
                    tst_txt=lineElem
                    j+=1
                    
            #if(i &gt;= 10000): 
             #   break


                
    for listElem in result2:
                k,_ =langid.classify(listElem) 

                if(k=='la'):
                    #print (tst_txt)
                    latin_counter+=1
                    words=word_tokenize(listElem)
                    # removing stop words 
                    for w in words:
                        if w not in stop_words_latin:
                            #Stemming and add to a list 
                            tokenized_txt_latin.append(w)
                    # removing punctuation 
                    tokenized_txt_latin = [word for word in tokenized_txt_latin if word.isalpha()]
                    words=' '.join(tokenized_txt_latin) # rejoining tokens to form a string 

                    



                else :
                    words=word_tokenize(listElem)
                    # removing stop words 
                    for w in words:
                        if w not in stop_words:
                            #Stemming and add to a list 
                            auxlist.append(w)
                            tokenized_txt.append(h.stem(w))
                            #unstemmed_list.append(words)
                    # removing punctuation 

                    auxlist = [word for word in auxlist if word.isalpha()]
                    words2=' '.join(auxlist) # rejoining tokens to form a string

                    tokenized_txt = [word for word in tokenized_txt if word.isalpha()]
                    words=' '.join(tokenized_txt) # rejoining tokens to form a string
                     

    result.append(words)
    unstemmed_list.append(words2) 


print(&quot;Matching rate is :&quot;,  (j/i) )

print(unstemmed_list ,&quot;\n&quot;)
print(result,&quot;\n&quot;)

# write results to a file 
with open('listfile.txt', 'w') as filehandle:
    for listitem in result:
        filehandle.write('%s\n' % listitem)

with open('listfile_unstemmed.txt', 'w') as filehandle:
    for listitem in unstemmed_list:
        filehandle.write('%s\n' % listitem)

</code></pre>
",Preprocessing of the text & Tokenization,split regex resulting list new line stemming removing punctuation resulting file two long one element list processed text put together tried move list append command else statement got huge list every word lumped together followed previous word new word added get full sentence start next match sure solved better loop also tried work resulting file quite inefficient longer basis split possible result operand written regular expression
Keras multi-classifier for chatbot answering,"<p>I have implemented in python a <strong>chatbot</strong> which is trained with a dataset of &quot;<em>intents</em>&quot; which is a json file in this form:</p>
<pre><code>{&quot;intents&quot;: [
    {&quot;tag&quot;: &quot;greeting&quot;,
     &quot;patterns&quot;: [&quot;Hi there&quot;, &quot;How are you&quot;, &quot;Is anyone there?&quot;,&quot;Hey&quot;,&quot;Hola&quot;, &quot;Hello&quot;, &quot;Good day&quot;],
     &quot;responses&quot;: [&quot;Hello, thanks for asking&quot;, &quot;Good to see you again&quot;, &quot;Hi there, how can I help?&quot;],

    },
    {&quot;tag&quot;: &quot;goodbye&quot;,
     &quot;patterns&quot;: [&quot;Bye&quot;, &quot;See you later&quot;, &quot;Goodbye&quot;, &quot;Nice chatting to you, bye&quot;, &quot;Till next time&quot;],
     &quot;responses&quot;: [&quot;See you!&quot;, &quot;Have a nice day&quot;, &quot;Bye! Come back again soon.&quot;],
     
    },
    {&quot;tag&quot;: &quot;thanks&quot;,
     &quot;patterns&quot;: [&quot;Thanks&quot;, &quot;Thank you&quot;, &quot;That's helpful&quot;, &quot;Awesome, thanks&quot;, &quot;Thanks for helping me&quot;],
     &quot;responses&quot;: [&quot;Happy to help!&quot;, &quot;Any time!&quot;, &quot;My pleasure&quot;],
     
    },
    {&quot;tag&quot;: &quot;noanswer&quot;,
     &quot;patterns&quot;: [],
     &quot;responses&quot;: [&quot;Sorry, can't understand you&quot;, &quot;Please give me more info&quot;, &quot;Not sure I understand&quot;],
     .
     .
     .
</code></pre>
<p>in which the tag are the category of the user's questions (<em>patterns</em>) with the associated possible <em>responses</em>.
Before the training phase the dataset has been transformed extracting each words of pattern with the <strong>tokenization</strong> and is then applied the <strong>lemmatization</strong>. So, the trainig set is composed by the pattern with the associated label (the tag), where the patterns are represented as Bag of Words and the labels are encoded with one-hot encoding.
Then the model has been defined as the following:</p>
<pre><code>model = Sequential()
model.add(Dense(128, input_shape=(x_train.shape[1],), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(classes), activation=&quot;softmax&quot;))
# set the optimizer
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
# compile the model
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
</code></pre>
<p>which is trained for 500 epochs with a batch size of 16.</p>
<p>The classification works well, the model is able to classify correctly unseen question given the correct &quot;tag&quot;. If the prediction probability is higher than 0.75, the model returns the correct tag, otherwise should it returns the tag &quot;<em>noanswer</em>&quot;.</p>
<p>The problem is when I ask to the chatbot an intentionally wrong question, writing a random string like &quot;fejfeajlflnk&quot; or similar to test in which situation the returned tag is &quot;<em>noanswer</em>&quot; ( low prediction probability, lower than 0.75 ) the classify predicts always the class associated to the tag &quot;greeting&quot; with a high probability (from 0.8 to 0.99), and I cannot understand this fact. Can anyone help me in understand why the classifier behaves likes that?</p>
",Preprocessing of the text & Tokenization,kera multi classifier chatbot answering implemented python chatbot trained dataset intent json file form tag category user question pattern associated possible response training phase dataset ha transformed extracting word pattern tokenization applied lemmatization trainig set composed pattern associated label tag pattern represented bag word label encoded one hot encoding model ha defined following trained epoch batch size classification work well model able classify correctly unseen question given correct tag prediction probability higher model return correct tag otherwise return tag noanswer problem ask chatbot intentionally wrong question writing random string like fejfeajlflnk similar test situation returned tag noanswer low prediction probability lower classify predicts always class associated tag greeting high probability understand fact anyone help understand classifier behaves like
Direction needed: finding terms in a corpus,"<p>My question isn't about a specific code issue, but rather about the best direction to take on a Natural Language Processing challenge.</p>
<p>I have a collection of several hundreds of Word and PDF files (from which I can then export the raw text) on the one hand, and a list of terms on the other. Terms can consist in one or more words. What I need to do is identify in which file(s) each term is used, applying stemming and lemmatization.</p>
<p>How would I best approach this? I know how to extract text, apply tokenization, lemmatization, etc., but I'm not sure how I could search for occurrences of terms in lemmatized form inside a corpus of documents.</p>
<p>Any hint would be most welcome.</p>
<p>Thanks!</p>
",Preprocessing of the text & Tokenization,direction needed finding term corpus question specific code issue rather best direction take natural language processing challenge collection several hundred word pdf file export raw text one hand list term term consist one word need identify file term used applying stemming lemmatization would best approach know extract text apply tokenization lemmatization etc sure could search occurrence term lemmatized form inside corpus document hint would welcome thanks
Counting the bigram frequencies of a specific pair of words in a list using python without NLTK,"<p>For example, I have a list of words['car','hamburger','airplane',.........,'hamburger']. How can I count the frequency of bigram (car,hamburger) without using NLTK?</p>
",Preprocessing of the text & Tokenization,counting bigram frequency specific pair word list using python without nltk example list word car hamburger airplane hamburger count frequency bigram car hamburger without using nltk
How to use porterstemmer,"<p>I have a data set includes restaurant reviews. I've processed my data and this is how my data set look like(0 and 1 shows is it positive or negative review):</p>
<pre><code>0   ['wow', 'loved', 'place']   1
1   ['crust', 'good']   0
2   ['not', 'tasty', 'texture', 'nasty']    0
3   ['stopped', 'late', 'may', 'bank', 'holiday', ...   1
4   ['the', 'selection', 'menu', 'great', 'prices'] 1
</code></pre>
<p>To be brief, i want to use PorterStemmer and this is how i studied to use it:</p>
<pre><code>for i in range(1000):
  for word in df['Review'][i]:
    word = stemmer.stem(word)
</code></pre>
<p>I studied to use porterstemmer to stemming but it did not work. Any word did not stem(for example, in first data i expected the 'loved' word should become a 'love'). My data is still same with the dataframe which i shared above and i could not fix this.</p>
",Preprocessing of the text & Tokenization,use porterstemmer data set includes restaurant review processed data data set look like show positive negative review brief want use porterstemmer studied use studied use porterstemmer stemming work word stem example first data expected loved word become love data still dataframe shared could fix
Splitting tokenize a corpus with R and Quanteda,"<p>I am working on a project for NLP. I need to take some blogs, news and tweets (you have probably heard of this capstone already) in .txt files and create n-grams frequencies.</p>

<p>I did experiments on the steps to take the <code>txt</code> files to a frequencies <code>data frame</code> for analysis:</p>

<pre><code>Read &gt; Conver to corpus &gt; Clean corpus &gt; Tokenize &gt; Convert to dfm &gt; Convert to df
</code></pre>

<p>The bottle necks in the process were the tokenize and convert to dfm steps (over 5x more time).</p>

<p>I had two choices:</p>

<pre><code>1. Split the cleaned corpus to tokenize by piece
2. Split-read the .txt files from the beginning
</code></pre>

<p>No. 1 seemed the best, but so far I have not found a function or package that
can do this in a way I want. So I will write a long code to split-read from the beginning in 20 chunks (due to my computing constraints).</p>

<p>Is there a way I can split a corpus (""corpus"" ""list"") created with the <code>quanteda</code> package in chunks (defined lines by me) so I can tokenize and turn to dfm in a ""streaming"" kinda way?</p>
",Preprocessing of the text & Tokenization,splitting tokenize corpus r quanteda working project nlp need take blog news tweet probably heard capstone already txt file create n gram frequency experiment step take file frequency analysis bottle neck process tokenize convert dfm step x time two choice seemed best far found function package way want write long code split read beginning chunk due computing constraint way split corpus corpus list created package chunk defined line tokenize turn dfm streaming kinda way
word tokenization takes too much time to run,"<p>I use Pythainlp package to tokenize my Thai language data for doing sentiment analysis.
first, I build a function to add new words set and tokenize it</p>
<pre><code>from pythainlp.corpus.common import thai_words
from pythainlp.util import dict_trie
from pythainlp import word_tokenize

def text_tokenize(Mention):
 new_words = {'คนละครึ่ง', 'ยืนยันตัวตน', 'เติมเงิน', 'เราชนะ', 'เป๋าตัง', 'แอปเป๋าตัง'}
 words = new_words.union(thai_words())
 custom_dictionary_trie = dict_trie(words)
 dataa = word_tokenize(Mention, custom_dict=custom_dictionary_trie, keep_whitespace=False)
 return dataa
</code></pre>
<p>after that I apply it within my text_process function which including remove punctuation and stop words.</p>
<pre><code>puncuations = '''.?!,;:-_[]()'/&lt;&gt;{}\@#$&amp;%~*ๆฯ'''
from pythainlp import word_tokenize
def text_process(Mention):
  final = &quot;&quot;.join(u for u in Mention if u not in puncuations and ('ๆ', 'ฯ'))
  final = text_tokenize(final)
  final = &quot; &quot;.join(word for word in final)
  final = &quot; &quot;.join(word for word in final.split() if word.lower not in thai_stopwords)
  return final

dff['text_tokens'] = dff['Mention'].apply(text_process) 
dff
</code></pre>
<p>the point is it takes too long to run this function. it took 17 minutes and still not finished. I tried to replace
<code>final = text_tokenize(final)</code> with  <code>final = word_tokenize(final)</code></p>
<p>and it took just 2 minutes but I can't no longer use it because I need to add new custom dictionary. I know there is something wrong but really don't know how to fix it</p>
<p>I am new to python and nlp so please help.
Ps. sorry for my broken English</p>
",Preprocessing of the text & Tokenization,word tokenization take much time run use pythainlp package tokenize thai language data sentiment analysis first build function add new word set tokenize apply within text process function including remove punctuation stop word point take long run function took minute still finished tried replace took minute longer use need add new custom dictionary know something wrong really know fix new python nlp please help p sorry broken english
How to preprocess a text to remove stopwords?,"<p>I would like to remove a list of stopwords, namely the ones in</p>
<pre><code>from gensim.parsing.preprocessing import STOPWORDS
print(STOPWORDS)
</code></pre>
<p>In gensim, this should be pretty straightforward with <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/parsing/preprocessing.py"" rel=""nofollow noreferrer""><code>remove_stopwords </code>function</a>.</p>
<p>My code to read the text and remove the stopwords is the following:</p>
<pre><code>def read_text(text_path):
  text = []
  with open(text_path) as file:
    lines = file.readlines()
    for index, line in enumerate(lines):
      text.append(simple_preprocess(remove_stopwords(line)))
  return text

text = read_text('/content/text.txt')
text =  [x for x in text if x]
text[:3]
</code></pre>
<p>This is the output I get that contains words such as &quot;we&quot; or &quot;however&quot; which should have been removed from the <a href=""https://drive.google.com/file/d/1JKIPRb9y6XK7dnVybfwm5Yoz_lBV7mLP/view?usp=sharing"" rel=""nofollow noreferrer"">original text</a> though for instance &quot;the&quot; has been correctly removed from the first setence. I am very confused... what am I missing here?</p>
<pre><code>[['clinical', 'guidelines', 'management', 'ibd'],
 ['polygenetic',
  'risk',
  'scores',
  'add',
  'predictive',
  'power',
  'clinical',
  'models',
  'response',
  'anti',
  'tnfα',
  'therapy',
  'inflammatory',
  'bowel',
  'disease'],
 ['anti',
  'tumour',
  'necrosis',
  'factor',
  'alpha',
  'tnfα',
  'therapy',
  'widely',
  'management',
  'crohn',
  'disease',
  'cd',
  'ulcerative',
  'colitis',
  'uc',
  'however',
  'patients',
  'respond',
  'induction',
  'therapy',
  'patients',
  'lose',
  'response',
  'time',
  'to',
  'aid',
  'patient',
  'stratification',
  'polygenetic',
  'risk',
  'scores',
  'identified',
  'predictors',
  'response',
  'anti',
  'tnfα',
  'therapy',
  'we',
  'aimed',
  'replicate',
  'association',
  'polygenetic',
  'risk',
  'scores',
  'response',
  'anti',
  'tnfα',
  'therapy',
  'independent',
  'cohort',
  'patients',
  'establish',
  'clinical',
  'validity']]
</code></pre>
<p><strong>Text</strong> (complete file available <a href=""https://drive.google.com/file/d/1JKIPRb9y6XK7dnVybfwm5Yoz_lBV7mLP/view?usp=sharing"" rel=""nofollow noreferrer"">here</a>)</p>
<p>Clinical Guidelines for the Management of IBD.</p>
<p>Polygenetic risk scores do not add predictive power to clinical models for response to anti-TNFα therapy in inflammatory bowel disease.
Anti-tumour necrosis factor alpha (TNFα) therapy is widely used in the management of Crohn's disease (CD) and ulcerative colitis (UC). However, up to a third of patients do not respond to induction therapy and another third of patients lose response over time. To aid patient stratification, polygenetic risk scores have been identified as predictors of response to anti-TNFα therapy. We aimed to replicate the association between polygenetic risk scores and response to anti-TNFα therapy in an independent cohort of patients, to establish its clinical validity.</p>
",Preprocessing of the text & Tokenization,preprocess text remove stopwords would like remove list stopwords namely one gensim pretty straightforward function code read text remove stopwords following output get contains word however removed original text though instance ha correctly removed first setence confused missing text complete file available clinical guideline management ibd polygenetic risk score add predictive power clinical model response anti tnf therapy inflammatory bowel disease anti tumour necrosis factor alpha tnf therapy widely used management crohn disease cd ulcerative colitis uc however third patient respond induction therapy another third patient lose response time aid patient stratification polygenetic risk score identified predictor response anti tnf therapy aimed replicate association polygenetic risk score response anti tnf therapy independent cohort patient establish clinical validity
Inference on Multiple Label Sentiment analysis using pytorch_lightning,"<p>I have trained an LSTM model for sentiment analysis using pytorch_lightning but I've been having difficulties incorporating the inference.
This is my model:</p>
<pre><code>class LSTM(pl.LightningModule):
 def __init__(self,n_vocab,n_embed, 
 n_hidden,n_output,n_layers,learning_rate,embedding_matrix=None):
  super().__init__()
  self.n_vocab = n_vocab
  self.n_layer = n_layers
  self.n_hidden = n_hidden
  self.embedding = nn.Embedding(n_vocab, n_embed, padding_idx = 0)
  if embedding_matrix is not None:
   self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
   self.embedding.weight.requires_grad = False
  self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, bidirectional = True)
  self.fc = nn.Linear(2 * n_hidden, n_output)
  self.dropout = nn.Dropout(0.2)
  self.sigmoid = nn.Sigmoid()
  self.batch_size = batch_size
  self.learning_rate = learning_rate
 
 def forward(self,input_words):
  embedded_words = self.embedding(input_words)
  lstm_out, _ = self.lstm(embedded_words)
  lstm_out_f=lstm_out[:,-1  , :300 ]
  lstm_out_b=lstm_out[:, 0 , 300: ]
  lstm_out_final = torch.cat([lstm_out_f,lstm_out_b], dim=-1)
  lstm_out_final = self.dropout(lstm_out_final)
  fc_out = self.fc(lstm_out_final)
  return fc_out

 def configure_optimizers(self):
  optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)
  return optimizer

 def training_step(self, batch, batch_nb):
    x , y= batch
    y_hat = self(x)
    loss = F.cross_entropy(y_hat, y)
    self.log(&quot;train_loss&quot;, loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
    return loss

 def validation_step(self, batch, batch_nb):
    x, y = batch
    y_hat = self(x)
    loss = F.cross_entropy(y_hat, y).to(device = 'cuda')
    f1 = torchmetrics.F1(num_classes=5).to(device = 'cuda')
    f1_score = f1(y_hat, y)
    accuracy = Accuracy().to(device = 'cuda')
    accur = accuracy(y_hat, y)
    self.log(&quot;val_loss&quot;, loss)
    self.log(&quot;f1_score&quot;, f1_score)
    self.log(&quot;accuracy&quot;, accur)

  def test_step(self, batch, batch_idx):
    x, y = batch
    logits = self(x)
    print(logits)
    logitsies = softmax(logits)
    choice = argmax(logitsies)
    loss = F.nll_loss(logits, y)
    self.log(&quot;test_loss&quot;, loss)
    return choice

  def predict_step(self, batch, batch_idx, dataloader_idx):
    x, y = batch
    x = x.view(x.size(0), -1)
    y_hat = self(x)
    logitsies = softmax(logits)
    choice = argmax(logitsies)
    loss = F.nll_loss(logits, y)
    self.log(&quot;predict_loss&quot;, loss)
    return choice
</code></pre>
<p>I called the model as such:</p>
<pre><code>model = LSTM(
n_vocab=size_of_emb_matrix,
n_embed=embed_vector_len,
n_hidden=150,
n_output=5,
n_layers=1,
learning_rate=1e-4,
embedding_matrix=embedding_matrix    
)
</code></pre>
<p>Now I am trying to write a function that would allow me to do inference. I have managed to succesfuly tokenize the input sentence and encode it through an already pre-defined functions, yet I have been getting several errors no matter what I try. I have found myself stuck and don't know how to continue. This is my function so far.</p>
<pre><code>def get_sentiment(text):
 x = encode_sentence(text,vocab2index)
 x_bar = x[0]
 y_hat = torch.tensor(x_bar)
 trainer.predict(model,y_hat)
</code></pre>
<p>The encode_sentence function is as follows:</p>
<pre><code>def encode_sentence(text, vocab2index, N=70):
 tokenized = tokenize(text)
 encoded = np.zeros(N, dtype=int)
 enc1 = np.array([vocab2index.get(word, vocab2index[&quot;UNK&quot;]) for word in tokenized])
 length = min(N, len(enc1))
 encoded[:length] = enc1[:length]
 return encoded, length
</code></pre>
<p>As I call the get_sentiment function, I am using the trainer.predict function which allows me to predict, hence doing inference.
But I have been getting the following Issue:</p>
<pre><code>AttributeError                            Traceback (most recent call 
last)
&lt;ipython-input-195-825c536cbbb2&gt; in &lt;module&gt;()
----&gt; 1 get_sentiment(&quot;love that dress&quot;)

13 frames
/usr/local/lib/python3.7/dist- 
packages/pytorch_lightning/loops/epoch/prediction_epoch_loop.py in 
_store_batch_indices(self, dataloader_idx)
162     def _store_batch_indices(self, dataloader_idx: int) -&gt; None:
163         &quot;&quot;&quot;Stores the batch indices if the predictions should be 
stored&quot;&quot;&quot;
--&gt; 164         batch_sampler = 
self.trainer.predict_dataloaders[dataloader_idx].batch_sampler
165         if isinstance(batch_sampler, IndexBatchSamplerWrapper):
166             self.current_batch_indices = batch_sampler.batch_indices

AttributeError: 'Tensor' object has no attribute 'batch_sampler'
</code></pre>
",Preprocessing of the text & Tokenization,inference multiple label sentiment analysis using pytorch lightning trained lstm model sentiment analysis using pytorch lightning difficulty incorporating inference model called model trying write function would allow inference managed succesfuly tokenize input sentence encode already pre defined function yet getting several error matter try found stuck know continue function far encode sentence function follows call get sentiment function using trainer predict function allows predict hence inference getting following issue
How to use custom Tokenizer in Hugging Face pretrained model for text summarization?,"<p>I need to make a for loop for running text summarization models as they have a maximum input limit for text summarization using <code>huggingface transformers</code>.</p>
<p>To execute the for loop and get its range, I need to pass tokenized input to the model and prevent it from tokenizing again inside the pipeline.</p>
<p>here is the code snippet:</p>
<pre class=""lang-py prettyprint-override""><code>summarizer = transformers.pipeline(&quot;summarization&quot;, model = 't5-small', tokenizer = 't5-small')

tokenized_text = summarizer.tokenizer(text)

</code></pre>
<p>I need to iterate over this tokenized_text.</p>
<p>If I pass slices of <code>tokenizer_text</code> into <code>summarizer</code> , it will get tokenizer again. My aim is to prevent that from happening the second time.</p>
",Preprocessing of the text & Tokenization,use custom tokenizer hugging face pretrained model text summarization need make loop running text summarization model maximum input limit text summarization using execute loop get range need pas tokenized input model prevent tokenizing inside pipeline code snippet need iterate tokenized text pas slice get tokenizer aim prevent happening second time
How to get the word of a embedding vector from the pretrained model of hugging face?,"<p>I use hugging face's pretrained model, bert, to help me get the meaning of sentence pooling(which means tokenize the sentence and get the average vector of all embedding words). My codes are as follows. I want to get the word which pooling vector refers to.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import BertModel, BertTokenizer
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
# Load the model
model = BertModel.from_pretrained(model_name)
# input sentence
input_text = &quot;Here is some text to encode&quot;
# from tokenizer to token_id
input_ids = tokenizer.encode(input_text, add_special_tokens=True)
# input_ids: [101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102]
input_ids = torch.tensor([input_ids])
# get the tensors
with torch.no_grad():
    last_hidden_states = model(input_ids)[0] # Models outputs are now tuples
# sentence pooling
last_hidden_states = last_hidden_states.mean(1)
print(last_hidden_states)
# last_hidden_states.shape = [1,768]
</code></pre>
<p>After this, I want to get the word of this encode vector([1,768]).<br />
Theoretically, I should use this <code>embedding vecter @ embedding_matrix(size is[ dictionary_dimention ,embedding_dimention])</code><br />
And then use the result of above matrix to be the index of the dictionary.<br />
How could I get the embedding_matrix in embedding layers of hugging face, Please.</p>
",Preprocessing of the text & Tokenization,get word embedding vector pretrained model hugging face use hugging face pretrained model bert help get meaning sentence pooling mean tokenize sentence get average vector embedding word code follows want get word pooling vector refers want get word encode vector theoretically use use result matrix index dictionary could get embedding matrix embedding layer hugging face please
Remove stopwords from tokenizaton,"<p>I having problem to remove stopwords from tokenization. I have already tokenized sentences and insert the result with pandas to the column named &quot;tweets_tokenize&quot;. The problem is I have double bracket ( [ ] ), the result only one and repeated (details see image) and the stopwords didn't work if using first function. But, if using second function is good. Can explain why?</p>
<pre><code>from nltk.corpus import stopwords
stopwords_indonesia = stopwords.words('indonesian')

# First function
def stopwords_remover(words):
    words = df['tweets_tokenize']
    tweets_stopwords = []
    for word in words:
        if word not in stopwords_indonesia:
            tweets_stopwords.append(word)
    return tweets_stopwords

# Second function
def stopwords_remover(words):
    tweets_stopwords = []
    for word in words:
        if word not in stopwords_indonesia:
            tweets_stopwords.append(word)
    return tweets_stopwords

df['tweets_tokenize'].apply(stopwords_remover)
df.head()
</code></pre>
<p>Result using first function.</p>
<p><a href=""https://i.sstatic.net/U1w2Y.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/U1w2Y.png"" alt=""First function"" /></a></p>
<p>Result using second function.</p>
<p><a href=""https://i.sstatic.net/kq4o9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kq4o9.png"" alt=""Second function"" /></a></p>
",Preprocessing of the text & Tokenization,remove stopwords tokenizaton problem remove stopwords tokenization already tokenized sentence insert result panda column named tweet tokenize problem double bracket result one repeated detail see image stopwords work using first function using second function good explain result using first function result using second function
how can i get the number of words that have been influenced by the lemmatization approach in a text?,"<p>For example, in the below sentence where the lemmatizer has affected 5 words, the number 5 should be displayed in the output.</p>
<pre><code>lemmatizer = WordNetLemmatizer()
sentence = &quot;The striped bats are hanging on their feet for best&quot;
print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])
#&gt; ['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']
</code></pre>
",Preprocessing of the text & Tokenization,get number word influenced lemmatization approach text example sentence lemmatizer ha affected word number displayed output
Remove punctuation marks from tokenized text using for loop,"<p>I'm trying to remove punctuations from a tokenized text in python like so:</p>
<pre><code>word_tokens = ntlk.tokenize(text)
w = word_tokens
for e in word_tokens:
    if e in punctuation_marks:
        w.remove(e)
</code></pre>
<p>This works somewhat, I manage to remove a lot of the punctuation marks but for some reason a lot of the punctuation marks in word_tokens are still left.
If I run the code another time, it again removes some more of the punctuations. After running the same code 3 times all the marks are removed. Why does this happen?</p>
<p>It doesn't seem to matter whether punctuation_marks is a list, a string or a dictionary. I've also tried to iterate over word_tokens.copy() which does a bit better, it almost removes all marks the first time, and all the second time.
Is there a simple way to fix this problem so that it is sufficient to run the code only once?</p>
",Preprocessing of the text & Tokenization,remove punctuation mark tokenized text using loop trying remove punctuation tokenized text python like work somewhat manage remove lot punctuation mark reason lot punctuation mark word token still left run code another time remove punctuation running code time mark removed doe happen seem matter whether punctuation mark list string dictionary also tried iterate word token copy doe bit better almost remove mark first time second time simple way fix problem sufficient run code
Negative lookahead to match words not in list,"<p>Need some help coming up with the right regex! I have a list of words, and essentially want to match any words (including alphabet, number, special characters, etc. since I'm planning to clean the text I match) that are NOT in the list.</p>
<p>Example list of words: <code>[&quot;chip_n_dale&quot;, &quot;86&quot;, &quot;fast_&amp;_furious&quot;, &quot;apple&quot;, &quot;b&amp;w&quot;, &quot;abc_123&quot;]</code></p>
<p>Example input text:</p>
<pre><code>heres some example text to convert to &amp;cleaned te3'xt &amp; 8 86 is a number and fast_&amp;_furious is a show what about chip_n_dale those are carto0n_ cha_racters not b&amp;w dont know about abc_123 5nonalpha apple
</code></pre>
<p>Example of what parts of the text should be matched:</p>
<pre><code>heres some example text to convert to &amp;cleaned te3'xt &amp; 8 is a number and is a show what about those are carto0n_ cha_racters not dont know about 5nonalpha
</code></pre>
<p>To solve this, I'm trying to use negative lookahead, but each of my attempts seem to encounter a different problem.</p>
<ol>
<li><code>phrases_re = re.compile(r&quot;\b(?!(&quot; + '|'.join(re.escape(w) for w in phrase_list) + r&quot;)\b)\w+&quot;)</code></li>
</ol>
<p>This gives me the result closest to what I need. But underscored words containing special characters aren't matched properly. For instance, <code>'fast_&amp;_furious'</code> yields a match of <code>'_furious'</code>. I'm guessing this is probably because <code>'fast_&amp;_furious'</code> is not consumed in negative lookahead, and the underscore is not captured by <code>'\w+'</code>. Unfortunately I'm stuck on how to capture the entire word instead of assuming <code>'_furious'</code> is part of a separate word.</p>
<p>Same problem for <code>'b&amp;w'</code>: <code>'w'</code> still gets matched.</p>
<ol start=""2"">
<li>phrases_re = re.compile(rf'\b(?!:{&quot;|&quot;.join(map(re.escape, phrase_list))})\b')</li>
</ol>
<p>This is strangely matching the spots between characters. For convenience, using <code>'^'</code> to mark what parts get matched:</p>
<blockquote>
<p>fast_^&amp;^_furious^ chip_n_dale^ b^&amp;^w^</p>
</blockquote>
<ol start=""3"">
<li><code>phrases_re = re.compile(r'(?&lt;!\S)(?!:{})(?!\S)'.format(&quot;|&quot;.join(map(re.escape, phrase_list))))</code></li>
</ol>
<p>In this case, only the spots between spaces or new lines get matched. For instance, using <code>'^'</code> to mark what parts get matched:</p>
<blockquote>
<p>chip_n_dale
twenty
^
ten tea ^ b&amp;w ^ ^ ^</p>
</blockquote>
<p>EDIT:</p>
<ul>
<li>expect that the text the regex is being applied to has already been partially processed (changed to lowercase, hyphens replaced by whitespace, all punctuation except ampersands removed)</li>
<li>Also I want to match anything (other than whitespaces, which I handle removing later by calling <code>.split</code> on the text) in the text that is not exactly something in my list of words.</li>
<li>So this text:</li>
</ul>
<blockquote>
<p>An Apple, apple, apples, apple-tree with an apple and a dapple
&quot;apple&quot;'</p>
</blockquote>
<p>Would actually come as:</p>
<blockquote>
<p>an apple apple apples apple tree with an apple and a dapple apple</p>
</blockquote>
<p>And regex would match:</p>
<blockquote>
<p>an apples tree with an and a dapple</p>
</blockquote>
<ul>
<li>Note that I'm doing this because my end goal is to only clean words not in my list. In other words, take the matched (non-list) words, call a processing function on them, and then replace the matched words in the original text with the cleaned matched words. I call re.sub using the <code>phrases_re</code> regex pattern like so (assume <code>post_phrase_tokenize_processing</code> just removes any remaining numbers/punctuation):</li>
</ul>
<pre><code>content = phrases_re.sub(lambda m: post_phrase_tokenize_processing(m.group()), content)
</code></pre>
",Preprocessing of the text & Tokenization,negative lookahead match word list need help coming right regex list word essentially want match word including alphabet number special character etc since planning clean text match list example list word example input text example part text matched solve trying use negative lookahead attempt seem encounter different problem give result closest need underscored word containing special character matched properly instance yield match guessing probably consumed negative lookahead underscore captured unfortunately stuck capture entire word instead assuming part separate word problem still get matched phrase compile rf b join map escape phrase list b strangely matching spot character convenience using mark part get matched fast furious chip n dale b w case spot space new line get matched instance using mark part get matched chip n dale twenty ten tea b w edit expect text regex applied ha already partially processed changed lowercase hyphen replaced whitespace punctuation except ampersand removed also want match anything whitespaces handle removing later calling text text exactly something list word text apple apple apple apple tree apple dapple apple would actually come apple apple apple apple tree apple dapple apple regex would match apple tree dapple note end goal clean word list word take matched non list word call processing function replace matched word original text cleaned matched word call sub using regex pattern like assume remove remaining number punctuation
NLP - Python - Conditional Frequency Distribution,"<p>I am trying to solve a question in hackerrank, which determine conditional frequency distribution of all the words(lowercase and removing stop words) for the given category 'cfdconditions', and events 'cfdevents'. Also compute conditional frequency distribution of category 'cfdconditions'  and events ending with 'ing' or 'ed'. And then display frequency modal for both distributions.</p>
<p>My code is -</p>
<pre><code>def calculateCFD(cfdconditions, cfdevents):
    # Write your code here
    from nltk.corpus import brown
    from nltk import ConditionalFreqDist
    from nltk.corpus import stopwords
    stopword = set(stopwords.words('english'))
    cdev_cfd = [ (genre, word.lower()) for genre in cfdconditions for word in brown.words(categories=genre) if word.lower() not in stopword]
    cdev_cfd = [list(x) for x in cdev_cfd]
    cdev_cfd = nltk.ConditionalFreqDist(cdev_cfd)
    a = cdev_cfd.tabulate(condition = cfdconditions, samples = cfdevents)
    inged_cfd = [ (genre, word.lower()) for genre in cfdconditions for word in brown.words(categories=genre) if (word.lower().endswith('ing') or word.lower().endswith('ed')) ]
    inged_cfd = [list(x) for x in inged_cfd]
    for wd in inged_cfd:
        if wd[1].endswith('ing') and wd[1] not in stopword:
            wd[1] = 'ing'
        elif wd[1].endswith('ed') and wd[1] not in stopword:
            wd[1] = 'ed'

    inged_cfd = nltk.ConditionalFreqDist(inged_cfd)    
    b = inged_cfd.tabulate(cfdconditions, samples = ['ed','ing'])
    return(a,b)
</code></pre>
<p>But result is still failing for 2 test cases, for which my output is -</p>
<pre><code>                 many years 
      adventure    24    32 
        fiction    29    44 
science_fiction    11    16 
                  ed  ing 
      adventure 3281 1844 
        fiction 2943 1767 
science_fiction  574  293 
</code></pre>
<p>and</p>
<pre><code>                  good    bad better 
      adventure     39      9     30 
        fiction     60     17     27 
        mystery     45     13     29 
science_fiction     14      1      4 
                  ed  ing 
      adventure 3281 1844 
        fiction 2943 1767 
        mystery 2382 1374 
science_fiction  574  293 
</code></pre>
<p>If anyone can help me for the solution, it will be of great help.</p>
",Preprocessing of the text & Tokenization,nlp python conditional frequency distribution trying solve question hackerrank determine conditional frequency distribution word lowercase removing stop word given category cfdconditions event cfdevents also compute conditional frequency distribution category cfdconditions event ending ing ed display frequency modal distribution code result still failing test case output anyone help solution great help
In Google Sheets remove serie of ngrams from cells containing lists of comma separated ngrams in primary sheet,"<p>Have been working in Google Sheets on a general table containing approximately a thousand texts. In one column derived form the column containing the texts in their original &quot;written&quot; form, are ngrams (words and the like) extracted from them, and listed in alphabetic order, one list of ngrams corresponding to each text. I’ve been trying without success to derive a second column, from these lists of such ngrams, from which I want to remove instances of certain ngrams of which I have a list (a long list, hundreds of ngrams, and a list to which I could make additions later). In other words, from the text mining vocabulary, I want to remove stop words from lists of tokens.</p>
<p><a href=""https://i.sstatic.net/iAtUJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iAtUJ.png"" alt=""enter image description here"" /></a></p>
<p>I tried with SPLIT and REGEXREPLACE functions, or a combination of both, but with no success.</p>
<pre><code>=JOIN(&quot;,&quot;;SORT(TRANSPOSE(SPLIT(REGEXREPLACE(AL3;&quot;\bau\b;\baux\b;\bavec\b;\bce\b;\bces\b;\bdans\b;\bde\b;\bdes\b;\bdu\b;\belle\b;\ben\b;\bet\b;\beux\b;\bil\b;\bje\b;\bla\b;\ble\b;\bleur\b;\blui\b;\bma\b;\bmais\b;\bme\b;\bmême\b;\bmes\b;\bmoi\b;\bmon\b;\bne\b;\bnos\b;\bnotre\b;\bnous\b;\bon\b;\bou\b;\bpar\b;\bpas\b;\bpour\b;\bqu\b;\bque\b;\bqui\b;\bsa\b;\bse\b;\bses\b;\bson\b;\bsur\b;\bta\b;\bte\b;\btes\b;\btoi\b;\bton\b;\btu\b;\bun\b;\bune\b;\bvos\b;\bvotre\b;\bvous\b;\bc\b;\bd\b;\bj\b;\bl\b;\bà\b;\bm\b;\bn\b;\bs\b;\bt\b;\by\b;\bété\b;\bétée\b;\bétées\b;\bétés\b;\bétant\b;\bsuis\b;\bes\b;\best\b;\bsommes\b;\bêtes\b;\bsont\b;\bserai\b;\bseras\b;\bsera\b;\bserons\b;\bserez\b;\bseront\b;\bserais\b;\bserait\b;\bserions\b;\bseriez\b;\bseraient\b;\bétais\b;\bétait\b;\bétions\b;\bétiez\b;\bétaient\b;\bfus\b;\bfut\b;\bfûmes\b;\bfûtes\b;\bfurent\b;\bsois\b;\bsoit\b;\bsoyons\b;\bsoyez\b;\bsoient\b;\bfusse\b;\bfusses\b;\bfût\b;\bfussions\b;\bfussiez\b;\bfussent\b;\bayant\b;\beu\b;\beue\b;\beues\b;\beus\b;\bai\b;\bas\b;\bavons\b;\bavez\b;\bont\b;\baurai\b;\bauras\b;\baura\b;\baurons\b;\baurez\b;\bauront\b;\baurais\b;\baurait\b;\baurions\b;\bauriez\b;\bauraient\b;\bavais\b;\bavait\b;\bavions\b;\baviez\b;\bavaient\b;\beut\b;\beûmes\b;\beûtes\b;\beurent\b;\baie\b;\baies\b;\bait\b;\bayons\b;\bayez\b;\baient\b;\beusse\b;\beusses\b;\beût\b;\beussions\b;\beussiez\b;\beussent\b;\bceci\b;\bcela\b;\bcelà\b;\bcet\b;\bcette\b;\bici\b;\bils\b;\bles\b;\bleurs\b;\bquel\b;\bquels\b;\bquelle\b;\bquelles\b;\bsans\b;\bsoi\b&quot;;&quot;&quot;);&quot; &quot;)));&quot;&quot;)
</code></pre>
<p>Dumky <a href=""https://www.dumky.net/posts/tf-idf-in-google-sheets-implementing-text-analysis-keyword-counting-tokenisation-and-stemming-in-a-spreadsheet/"" rel=""nofollow noreferrer"">here</a> has done something neat, writing a script with custom functions that does a couple of things, among them calling a list from a secondary sheet, to clean such lists of ngrams-words-tokens, but what he has done exceed both what I want to do, and what I can do in Google Sheets on my own.</p>
<p>An exemple of a list of stop words (ngrams to remove):
à,ai,aie,aient,aies,ait,as,au,aura,aurai,auraient,aurais,aurait,auras,aurez,auriez,aurions,aurons,auront,aux,avaient,avais,avait,avec,avez,aviez,avions,avons,ayant,ayez,ayons,c,ce,ceci,cela,celà,ces,cet,cette,d,dans,de,des,du,elle,en,es,est,et,étaient,étais,était,étant,été,étée,étées,êtes,étés,étiez,étions,eu,eue,eues,eûmes,eurent,eus,eusse,eussent,eusses,eussiez,eussions,eut,eût,eûtes,eux,fûmes,furent,fus,fusse,fussent,fusses,fussiez,fussions,fut,fût,fûtes,ici,il,ils,j,je,l,la,le,les,leur,leurs,lui,m,ma,mais,me,même,mes,moi,mon,n,ne,nos,notre,nous,on,ont,ou,par,pas,pour,qu,que,quel,quelle,quelles,quels,qui,s,sa,sans,se,sera,serai,seraient,serais,serait,seras,serez,seriez,serions,serons,seront,ses,soi,soient,sois,soit,sommes,son,sont,soyez,soyons,suis,sur,t,ta,te,tes,toi,ton,tu,un,une,vos,votre,vous,y</p>
<p>An exemple of the output I am looking for —</p>
<p>List of ngrams derived from a short text:
10,11,21,à,à,ami,ami,amour,aux,baiser,brusko,brusko,car,ce,comme,comme,compagnon,de,de,déclare,déclare,déteste,électricité,elle,en,est,est,est,et,étudiant,fluide,gare,germes,heures,idéalistes,il,infectueux,je,l,l,la,la,le,lumière,médecine,meilleur,moi,moi,mon,montréal,pée,pensez,poète,que,que,qui,rancune,répondez,sans,science,ses,sic,un,un,vébicule,vôtre,vous</p>
<p>Output:
ami,ami,amour,baiser,brusko,brusko,car,comme,comme,compagnon,déclare,déclare,déteste,électricité,étudiant,fluide,gare,germes,heures,idéalistes,infectueux,lumière,médecine,meilleur,montréal,pée,pensez,poète,rancune,répondez,science,sic,vébicule,vôtre</p>
<p><a href=""https://i.sstatic.net/UVbJ5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UVbJ5.png"" alt=""enter image description here"" /></a></p>
",Preprocessing of the text & Tokenization,google sheet remove serie ngrams cell containing list comma separated ngrams primary sheet working google sheet general table containing approximately thousand text one column derived form column containing text original written form ngrams word like extracted listed alphabetic order one list ngrams corresponding text trying without success derive second column list ngrams want remove instance certain ngrams list long list hundred ngrams list could make addition later word text mining vocabulary want remove stop word list token tried split regexreplace function combination success dumky ha done something neat writing script custom function doe couple thing among calling list secondary sheet clean list ngrams word token ha done exceed want google sheet exemple list stop word ngrams remove ai aie aient aies ait au aura aurai auraient aurais aurait aura aurez auriez aurions aurons auront aux avaient avais avait avec avez aviez avions avon ayant ayez ayons c ce ceci cela cel ce cet cette dans de de du elle en e est et taient tai tait tant e e te tiez tions eu eue eues e eurent eu eusse eussent eu eussiez eussions eut e e te eux f furent fus fusse fussent fuss fussiez fussions fut f f te ici il il j je l la le le leur leurs lui mais moi mon n ne notre nous ont ou par pa pour qu que quel quelle quelles quels qui sa sans se serum serai seraient serais serait seras serez seriez serions serons seront soi soient sois soit somme son sont soyez soyons suis sur ta te te toi ton tu un vos votre vous exemple output looking list ngrams derived short text ami ami amour aux baiser brusko brusko car ce comme comme compagnon de de clare clare teste lectricit elle en est est est et tudiant fluide gare germes heures id alistes il infectueux je l l la la le lumi decine meilleur moi moi mon montr al p e pensez po te que que qui rancune r pondez sans science sic un un v bicule v tre vous output ami ami amour baiser brusko brusko car comme comme compagnon clare clare teste lectricit tudiant fluide gare germes heures id alistes infectueux lumi decine meilleur montr al p e pensez po te rancune r pondez science sic v bicule v tre
Keep only sentences in corpus that contain specific key words (in R),"<p>I have a corpus with <code>.txt</code> documents. From these <code>.txt</code> documents, I do not need all sentences, but I only want to keep certain sentences that contain specific key words. From there on, I will perform similarity measures etc.</p>
<p>So, here is an example.
From the data_corpus_inaugural data set of the quanteda package, I only want to keep the sentences in my corpus that contain the words &quot;future&quot; and/or &quot;children&quot;.</p>
<p>I load my packages and create the corpus:</p>
<pre><code>library(quanteda)
library(stringr)


## corpus with data_corpus_inaugural of the quanteda package
corpus &lt;- corpus(data_corpus_inaugural)
summary(corpus)
</code></pre>
<p>Then I want to keep only those sentences that contain my key words</p>
<pre><code>## keep only those sentences of a document that contain words future or/and 
children
</code></pre>
<p>First, let's see which documents contain these key words</p>
<pre><code>## extract all matches of future or children
str_extract_all(corpus, pattern = &quot;future|children&quot;)
</code></pre>
<p>So far, I only found out how to exclude the sentences that contain my key words, which is the opposite of what I want to do.</p>
<pre><code>## excluded sentences that contains future or children or both (?)
corpustrim &lt;- corpus_trimsentences(corpus, exclude_pattern = 
&quot;future|children&quot;)
summary(corpustrim)
</code></pre>
<p>The above command excludes sentences containing my key words.
My idea here with the corpus_trimsentences function is to exclude all sentences BUT those containing &quot;future&quot; and/or &quot;children&quot;.</p>
<p>I tried with regular expression. However, I did not manage to do it. It does not return what I want.</p>
<p>I looked into the <code>corpus_reshape</code> and <code>corpus_subset</code> functions of the quanteda package but I can't figure out how to use them for my purpose.</p>
",Preprocessing of the text & Tokenization,keep sentence corpus contain specific key word r corpus document document need sentence want keep certain sentence contain specific key word perform similarity measure etc example data corpus inaugural data set quanteda package want keep sentence corpus contain word future child load package create corpus want keep sentence contain key word first let see document contain key word far found exclude sentence contain key word opposite want command excludes sentence containing key word idea corpus trimsentences function exclude sentence containing future child tried regular expression however manage doe return want looked function quanteda package figure use purpose
how to remove custom words in list using the last character in python,"<p>I have data like this:</p>
<pre><code>lst = [['name','address','addressgmailcom'],['name','address','namegmailcom']]
</code></pre>
<p>Here need to remove 'addressgmailcom' and 'namegmailcom' using 'gmailcom' or last characters.</p>
<p>My list only contain name, address</p>
",Preprocessing of the text & Tokenization,remove custom word list using last character python data like need remove addressgmailcom namegmailcom using gmailcom last character list contain name address
How to calculate a column using the most common words calculated from another dataframe in Python?,"<p>Example of the dataframe:</p>
<pre><code>cup = {'Description': ['strawberry cupcake', 'blueberry cupcake', 'strawberry cookie', 'grape organic cookie', 'blueberry organic cookie', 'lemon organic cupcake'], 
'Days_Sold': [3, 4, 1, 2, 2, 1]}

cake = pd.DataFrame(data=cup)

cake
</code></pre>
<ol>
<li><p>I calculated the most common words of the dataframe (with stop words removed)</p>
<pre><code>from collections import Counter

Counter(&quot; &quot;.join(cake['Description']).split()).most_common()
</code></pre>
</li>
<li><p>I put this into a new dataframe and reset the index</p>
<pre><code>count = pd.DataFrame(Counter(&quot; &quot;.join(cake['Description']).split()).most_common())

count.columns = ['Words', 'Values']

count.index= np.arange(1, len(count)+1)

count.head()
</code></pre>
</li>
<li><p>The Values is in the 'count' dataframe. The Days_Sold is in the 'cake' dataframe. What I would like to do now is if the common word in the 'count' dataframe shows up, like cupcake, how long would this take for me to sell the product using the 'cake' dataframe, and that would go through every common word in the 'count' dataframe until it's done? The answer should come out to be (3+4+1) 8 for cupcake.</p>
</li>
</ol>
<p>My actual dataframe is over 3000 lines (and not exactly about cakes). The description is longer. I need over 40 common words, adjustable to my need.</p>
<p>This is why I can't be typing in each word. I believe this requires a 'nested for loop'. But I am stuck on it.</p>
<pre><code>for day in cake:

    for top in count:

       top= count.Words

    day= cake.loc[cake['CleanDescr'] == count, ['Days_Sold']]
</code></pre>
<p>The error says: 'int' object is not iterable</p>
<p>Thank you!</p>
<p><strong>Update:</strong></p>
<p>Thank you so much to everyone helping me on this large project. I am posting my solution to #3, adjusted from the answer by <em>Mark Moretto</em>.</p>
<pre><code># Split and explode Description
df = cake.iloc[:, 0].str.lower().str.split(r&quot;\W+&quot;).explode().reset_index()
df

# Merge counts to main DataFrame
df_freq = pd.merge(df, count, on=&quot;Description&quot;)
df_freq

# Left join cake DataFrame onto df_freq by index values.
df_freq = (pd.merge(df_freq, cake, left_on = &quot;index&quot;, right_index = True)
            .loc[:, [&quot;Description_x&quot;, &quot;Values&quot;, &quot;Days_Sold&quot;]]
            .rename(columns={&quot;Description_x&quot;: &quot;Description&quot;})
            )
df_freq

# Group by Description and return max result for value fields
df_metrics = df_freq.groupby(&quot;Description&quot;).mean().round(4)
df_metrics

df_metrics.head(5).sort_values(by='Values', ascending=False)
#print(df_metrics)
</code></pre>
",Preprocessing of the text & Tokenization,calculate column using common word calculated another dataframe python example dataframe calculated common word dataframe stop word removed put new dataframe reset index value count dataframe day sold cake dataframe would like common word count dataframe show like cupcake long would take sell product using cake dataframe would go every common word count dataframe done answer come cupcake actual dataframe line exactly cake description longer need common word adjustable need typing word believe requires nested loop stuck error say int object iterable thank update thank much everyone helping large project posting solution adjusted answer mark moretto
Regex cutting the word in python,"<pre><code>import PyPDF2

fileReader = PyPDF2.PdfFileReader(file)
s=&quot;&quot;
for i in range(2, fileReader.numPages):
    s+=fileReader.getPage(i).extractText()

from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')



sentences = []
while s.find('.') != -1:
    index = s.find('.')
    sentences.append(s[:index])
    s = s[index+1:]

#splits the text into array of sentences based on where we see a '.' - need to account for how to avoid breaking at e.g. Mr.



corpus=[]
for sentence in sentences:
    corpus.append(tokenizer.tokenize(sentence))
print(corpus[20])
</code></pre>
<p>The above is code for reading files and tokenizing the string. The output I get is as follows:</p>
<p><a href=""https://i.sstatic.net/Rv6Q9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Rv6Q9.png"" alt=""Output"" /></a></p>
<p>But the desired output is:</p>
<pre><code>['With', 'the', 'graph', 'of', 'COVID', '19', 'at', 'this', 'moment', 'have', 'started', 'a', 'slide', 'downward', 'trend', 'we', 'are', 'confident', 'of', 'all', 'our', 'brands', 'performing', 'strongly', 'in', 'the', 'coming', 'quarters']
</code></pre>
<p>i.e. the words should not get broken down. Is there any way to avoid this?</p>
<p>The string 's' is taken from a pdf and looks something like this:
<a href=""https://i.sstatic.net/TWdQ5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TWdQ5.png"" alt=""pdf text"" /></a></p>
",Preprocessing of the text & Tokenization,regex cutting word python code reading file tokenizing string output get follows desired output e word get broken way avoid string taken pdf look something like
"Porter Stemmer, Step 1b","<p>Similar question to this [1]<a href=""https://stackoverflow.com/questions/4522459/porter-stemming-algorithm-implementation-question"">porter stemming algorithm implementation question?</a>, but expanded.</p>

<p>Basically, step1b is defined as:</p>

<blockquote>
  <p>Step1b</p>

<pre><code>`(m&gt;0) EED -&gt; EE                    feed      -&gt;  feed
                               agreed    -&gt;  agree
(*v*) ED  -&gt;                       plastered -&gt;  plaster
                               bled      -&gt;  bled
(*v*) ING -&gt;                       motoring  -&gt;  motor
                               sing      -&gt;  sing `
</code></pre>
</blockquote>

<p>My question is why does     <code>feed</code> stem to     <code>feed</code> and not     <code>fe</code>? All the online Porter Stemmer's I've tried online stems to     <code>feed</code>, but from what I see, it should stem to     <code>fe</code>.</p>

<p>My train of thought is:</p>

<pre><code>`feed` does not pass through     `(m&gt;0) EED -&gt; EE` as measure of     `feed` minus suffix     `eed` is `m(f)`, hence     `=0`

`feed` will pass through     `(*v*) ED  -&gt;`, as there is a vowel in the stem     `fe` once the suffix     `ed` is removed. So will stem at this point to     `fe`
</code></pre>

<p>Can someone explain to me how online Porter Stemmers manage to stem to     <code>feed</code>?</p>

<p>Thanks.</p>
",Preprocessing of the text & Tokenization,porter stemmer step b similar question href stemming algorithm implementation question expanded basically step b defined step b question doe stem online porter stemmer tried online stem see stem train thought someone explain online porter stemmer manage stem thanks
Spacy: how make a clean segmentation?,"<p>I have a string like this:</p>
<pre><code>THIS IS UPPERCASE TEXT :PART 1 - PARAGRAPHLorem ipsum:1.1First phrase «test».1.2Second phrase «test» end of phrase.
</code></pre>
<p>I would like to have this output (this segmentation with Spacy):</p>
<blockquote>
<p>&quot;THIS IS UPPERCASE TEXT :PART 1 - PARAGRAPH&quot;
&quot;Lorem ipsum:1.1First phrase «test».&quot;
&quot;1.2Second phrase «test» end of phrase.&quot;</p>
</blockquote>
<p>I tried this with Spacy:</p>
<pre><code>import spacy
from spacy.language import Language
import re

nlp = spacy.load('fr_core_news_lg')
boundary = re.compile('^[0-9]$')

@Language.component('custom_seg')

def custom_seg(doc):
    prev = doc[0].text
    length = len(doc)
    for index, token in enumerate(doc):
        if (token.text == '.' and boundary.match(prev) and index!=(length - 1)):
            doc[index+1].sent_start = False
        prev = token.text
    return doc

nlp.add_pipe('custom_seg', before='parser')

test = &quot;THIS IS UPPERCASE TEXT :PART 1 - PARAGRAPHLorem ipsum:1.1First phrase «test».1.2Second phrase «test» end of phrase.&quot;
doc = nlp(test)

for sentence in doc.sents:
    print(&quot;Length &quot; + str(len(sentence.text))), print(sentence.text), print('____________')
</code></pre>
<p>But the output is:</p>
<pre><code>    Length 4
    THIS
    ____________
    Length 12
    IS UPPERCASE
    ____________
    Length 12
    TEXT :PART 1
    ____________
    Length 1
    -
    ____________
    Length 29
    PARAGRAPHLorem ipsum:1.1First
    ____________
    Length 8
    phrase «
    ____________
    Length 24
    test».1.2Second phrase «
    ____________
    Length 20
    test» end of phrase.
    ____________
</code></pre>
<p>I don't know where I am wrong. I don't understand why I have these segmentations and how to improve it.</p>
",Preprocessing of the text & Tokenization,spacy make clean segmentation string like would like output segmentation spacy uppercase text part paragraph lorem ipsum first phrase test second phrase test end phrase tried spacy output know wrong understand segmentation improve
how to skip newline /n or sentences that don&#39;t make sense?,"<p>I have used spacy to tokenize and to try and get some word stats from a subset of text but it also prints new lines \n and short sentences or ones that don't make sense. How do I get rid of these?</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_md&quot;)
def describe_sentence_stats(text):
    doc = nlp(text)
    sent_df = pd.DataFrame()
    for i, sent in enumerate(doc.sents):
        sent_df.loc[i, 'sentence'] = sent.text
        sent_df.loc[i, 'num_words'] = len([token for token in sent])
                                          
    return sent_df

sentences = describe_sentence_stats(example_text)
</code></pre>
<p>I would like to get rid of these types of outputs:</p>
<p><a href=""https://i.sstatic.net/eAptU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eAptU.png"" alt=""Some of the output"" /></a></p>
",Preprocessing of the text & Tokenization,skip newline n sentence make sense used spacy tokenize try get word stats subset text also print new line n short sentence one make sense get rid would like get rid type output
Remove stopwords using Gensim,"<p>I have got this example code.</p>
<pre><code>from gensim.parsing.preprocessing import remove_stopwords

text = &quot;Nick likes to play football, however he is not too fond of tennis.&quot;
filtered_sentence = remove_stopwords(text)

print(filtered_sentence)
</code></pre>
<p>However, I'm not familiar with pandas and loop. Say, I have</p>
<pre><code>data = {'Text':['I like it.', 'He is happy.', 'This is apple.', 'It is great.']}
df = pandas.DataFrame(data)
</code></pre>
<p>How can I use loop (I guess it is loop) to remove stop words in <code>Text</code> with Gensim <code>remove_stopwords</code> and put the results in <code>data['text_without_stopwords']</code>? Thanks!</p>
",Preprocessing of the text & Tokenization,remove stopwords using gensim got example code however familiar panda loop say use loop guess loop remove stop word gensim put result thanks
&quot;Which one to choose? Lemmatization or Stemming?&quot;,"<p>To perform NLP tasks, like &quot;<strong>Predict which Tweets are about real disasters and which ones are not</strong>&quot; from KAGGLE, link: <a href=""https://www.kaggle.com/c/nlp-getting-started"" rel=""nofollow noreferrer"">https://www.kaggle.com/c/nlp-getting-started</a></p>
<p>Which task should I perform to normalize my texts, Lemmatization or stemming?</p>
<p>Thank you!</p>
",Preprocessing of the text & Tokenization,one choose lemmatization stemming perform nlp task like predict tweet real disaster one kaggle link task perform normalize text lemmatization stemming thank
How to remove stop words and lemmatize at the same time when using spaCy?,"<p>When I use spaCy for cleaning data, I run the following line:</p>
<p><code>df['text'] = df.sentence.progress_apply(lambda text: &quot; &quot;.join(token.lemma_ for token in nlp(text) if not token.is_stop and token.is_alpha))</code></p>
<p>Which lemmatizes each word in the text row if the word in not a stop-word. The problem is that text.lemma_ is applied to the token after the token is checked for being a stop-word or not. Therefore, if the stop-word is not in the lemmatized form, it will not be considered stop word. For example, if I add &quot;friend&quot; to the list of stop words, the output will still contain &quot;friend&quot; if the original token was &quot;friends&quot;. The easy solution is to run this line twice. But that sounds silly. Anyone can suggest a solution to remove the stop words that are not in the lemmatized form in the first run?</p>
<p>Thanks!</p>
",Preprocessing of the text & Tokenization,remove stop word lemmatize time using spacy use spacy cleaning data run following line lemmatizes word text row word stop word problem text lemma applied token token checked stop word therefore stop word lemmatized form considered stop word example add friend list stop word output still contain friend original token wa friend easy solution run line twice sound silly anyone suggest solution remove stop word lemmatized form first run thanks
Find if some word from list in text python3,"<p>I looking for method to find all text that appropriate to value in list:
for example:</p>
<pre><code>words =['a', 'ball','hello']
# No. of text:   1                2             3                   4
texts= [&quot;i'm need a answer&quot;, &quot;Hi daddy&quot;, &quot;hello world&quot;, &quot;I love to dance ballet&quot;]
</code></pre>
<p>The desired result:</p>
<ol>
<li>Shown: because the word &quot;a&quot;</li>
<li>Not shown: although having the letter &quot;a&quot;, It does not appear as an independent word</li>
<li>Shown: because the word &quot;hello&quot;</li>
<li>Not Shown: although having the word &quot;ballet&quot;, the word &quot;ball&quot; not appear as an independent word</li>
</ol>
<p>I tried use loop by loop (search if word from words in texts, but without success</p>
<p>Really thank you for your help</p>
",Preprocessing of the text & Tokenization,find word list text python looking method find text appropriate value list example desired result shown word shown although letter doe appear independent word shown word hello shown although word ballet word ball appear independent word tried use loop loop search word word text without success really thank help
NLTK doesn&#39;t lemmatize uppercase words,"<p>I'm trying to change plural words to singular in a string with a mix of upper case and lowercase words. e.g. CARDBOARD BOXES, DIMENSIONS: 19cm H x 10cm W x 30cm D</p>
<p>I used NLTK package to do so but it only accept lowercase strings and I don't want to lowercase all words in my string. any suggestion?</p>
",Preprocessing of the text & Tokenization,nltk lemmatize uppercase word trying change plural word singular string mix upper case lowercase word e g cardboard box dimension cm h x cm w x cm used nltk package accept lowercase string want lowercase word string suggestion
How to remove unnecessary words from string for better search,"<p>I have different strings for searching the related data but due to unnecessary words, retrieved results are not good. For example, &quot;Working of genetic algorithm&quot;, so the words &quot;working of&quot; are not important in here. I can remove &quot;of&quot; by considering it as a stop word. But how about &quot;working&quot;? I can do stemming but it will just remove &quot;ing&quot;, which doesn't solve the problem. Similarly another string &quot;Determination of.....&quot;, I consider that other words in the string are important and &quot;Determination of&quot; are not important, so I want to remove them before proceeding further. Any ideas or hints how I can remove these words, since there are a lot of these types of words and I cannot hardcode them.</p>
",Preprocessing of the text & Tokenization,remove unnecessary word string better search different string searching related data due unnecessary word retrieved result good example working genetic algorithm word working important remove considering stop word working stemming remove ing solve problem similarly another string determination consider word string important determination important want remove proceeding idea hint remove word since lot type word hardcode
Replace apostrophe/short words in python,"<p>I am using python to clean a given sentence. Suppose that my sentence is:</p>

<pre><code>What's the best way to ensure this?
</code></pre>

<p>I want to convert:</p>

<pre><code>What's -&gt; What is
</code></pre>

<p>Similarly,</p>

<pre><code> must've -&gt; must have
</code></pre>

<p>Also, verbs to original form,</p>

<pre><code>told -&gt; tell
</code></pre>

<p>Singular to plural, and so on.</p>

<p>I am currently exploring textblob. But not all of the above is possible using it. </p>
",Preprocessing of the text & Tokenization,replace apostrophe short word python using python clean given sentence suppose sentence want convert similarly also verb original form singular plural currently exploring textblob possible using
How many words are lmmatized?,"<p>In a data frame with 1000 texts, after doing preprocessing lemmatization, how can I find out how many words have been lemmatized in each text?</p>
",Preprocessing of the text & Tokenization,many word lmmatized data frame text preprocessing lemmatization find many word lemmatized text
Converting journal titles to their abbreviated form,"<p>Good morning my hero!</p>
<p>I have a list of journal titles in English, Spanish and Portuguese that I want to convert to their abbreviated form. The official abbreviation dictionary for journal titles is the <a href=""https://www.issn.org/services/online-services/access-to-the-ltwa/"" rel=""nofollow noreferrer"">List of Title Word Abbreviations</a> found on the ISSN website.</p>
<pre><code># example of my data
journal names &lt;- c(journals = c(&quot;peste revista psicanalise sociedade&quot;, &quot;abanico veterinario&quot;, &quot;abcd arquivos brasileiros cirurgia digestiva sao paulo&quot;, &quot;academo asuncion&quot;, &quot;accion psicologica&quot;, &quot;acimed&quot;, &quot;acta academica&quot;, &quot;acta amazonica&quot;, &quot;acta bioethica&quot;, &quot;acta bioquimica clinica latinoamericana&quot;)
</code></pre>
<p>I have split each title into a list of single words. So currently I have a list of lists, where each title is a list of its individual words.</p>
<pre><code>[[1]]
[1] &quot;peste&quot;       &quot;revista&quot;     &quot;psicanalise&quot; &quot;sociedade&quot;  

[[2]]
[1] &quot;abanico&quot;     &quot;veterinario&quot;
</code></pre>
<p>Once I remove the stop words (as seen above), I need to match any relevant words to the suffixes or prefixes in the LTWA and then convert them to the abbreviation. I have converted the LTWA words so that they have regular expressions and can be used to search for a match easily with a package like <strong>stringi</strong>.</p>
<pre><code># this is an excerpt from the dataframe I created with the LTWA
the ABBREVIATIONS_NA replaces the n.a. with the original word and the REXP has the prefix/suffix with the regular expressions

WORDS,ABBREVIATIONS,LANGUAGES,REXP,ABBREVIATIONS_NA
proofreader,proofread.,eng,proofreader,proofread.
prophylact-,prophyl.,eng,^prophylact.*,prophyl.
propietario,prop.,spa,propietario,prop.
propriedade,propr.,por,propriedade,propr.
prostético,prostét.,spa,prostético,prostét.
protecção,prot.,por,protecção,prot.
proteccion-,prot.,spa,^proteccion.*,prot.
prototyping,prototyp.,eng,prototyping,prototyp.
provisional,n.a.,eng,provisional,provisional
provisóri-,n.a.,por,^provisóri.*,provisóri-
proyección,proyecc.,spa,proyección,proyecc.
psicanalise,psicanal.,por,psicanalise,psicanal.
psicoeduca-,psicoeduc.,spa,^psicoeduca.*,psicoeduc.
psicosomat-,psicosom.,spa,^psicosomat.*,psicosom.
psicotecni-,psicotec.,spa,^psicotecni.*,psicotec.
psicoterap-,psicoter.,spa,^psicoterap.*,psicoter.
psychedelic,n.a.,eng,psychedelic,psychedelic
psychoanal-,psychoanal.,eng,^psychoanal.*,psychoanal.
psychodrama,n.a.,eng,psychodrama,psychodrama
psychopatha,n.a.,por,psychopatha,psychopatha
pteridolog-,pteridol.,eng,^pteridolog.*,pteridol.
publicitar-,public.,spa,^publicitar.*,public.
puericultor,pueric.,spa,puericultor,pueric.
Puerto Rico,P. R.,spa,Puerto Rico,P. R.
</code></pre>
<p>The search and conversion needs to be done from largest prefix/suffix to smallest prefix/suffix, and words that have already been processed cannot be processed again.</p>
<p><strong>The issue: I would like to convert each title word to its proper abbreviation.</strong> However, if there is a prefix like 'latinoamericano', it should only respond to the prefix 'latinoameri-' and be converted to latinoam. The problem is that it will also respond to 'latin-' and then get converted to 'latin.' How can I make it so that each word is only processed once?</p>
<p>Also note that my LTWA database only has about 12,000 words in total, so there will be words that don't have a match at all.</p>
<p>I have gotten up to this point, but not sure where to go from here to accomplish this. So far, I have only come up with very clunky solutions that do not work perfectly.</p>
<p>Thank you!</p>
",Preprocessing of the text & Tokenization,converting journal title abbreviated form good morning hero list journal title english spanish portuguese want convert abbreviated form official abbreviation dictionary journal title list title word abbreviation found issn website split title list single word currently list list title list individual word remove stop word seen need match relevant word suffix prefix ltwa convert abbreviation converted ltwa word regular expression used search match easily package like stringi search conversion need done largest prefix suffix smallest prefix suffix word already processed processed issue would like convert title word proper abbreviation however prefix like latinoamericano respond prefix latinoameri converted latinoam problem also respond latin get converted latin make word processed also note ltwa database ha word total word match gotten point sure go accomplish far come clunky solution work perfectly thank
How do I configure Spacy pipeline to lemmatize a spellchecker component&#39;s results?,"<p>I'm currently trying to add a spellchecker step into one of Spacy's built-in pipelines, specifically <code>'en_core_web_sm'</code></p>
<p>I found a pretty neat component called <a href=""https://spacy.io/universe/project/contextualSpellCheck"" rel=""nofollow noreferrer"">Contextual Spell Check</a> that I've inserted into the pipeline. The problem is that the lemmatize step isn't lemmatizing the spell-checked word, but rather the original text, EVEN after I've reordered the pipeline to <code>['tok2vec', 'parser', 'contextual spellchecker', 'tagger', 'attribute_ruler', 'lemmatizer', 'ner']</code>.</p>
<p>For example:</p>
<pre><code>doc_a = nlp(&quot;Income wes $9.4 milion compared to the prior year of $2.7 milion.&quot;)
doc_b = nlp(&quot;Income was $9.4 milion compared to the prior year of $2.7 milion.&quot;)
</code></pre>
<p>Will return the correctly spell-checked results of:</p>
<pre><code>print(doc_a._.outcome_spellCheck)
# Income was $9.4 million compared to the prior year of $2.7 million.

print(doc_b._.outcome_spellCheck)
# Income was $9.4 million compared to the prior year of $2.7 million.
</code></pre>
<p>However, examining the underlying results:</p>
<pre><code># doc_a with misspelled 'was'. Note lemma is still the original typo 'wes'
print(doc_a.to_json()['tokens'])
# {'id': 1, 'start': 7, 'end': 10, 'tag': 'MD', 'pos': 'AUX', 'morph': 'VerbType=Mod', 'lemma': 'wes', 'dep': 'ROOT', 'head': 1}

# doc_b with correctly spelled 'was'. Correctly lemmatized to 'be'
print(doc_b.to_json()['tokens'])
# {'id': 1, 'start': 7, 'end': 10, 'tag': 'VBD', 'pos': 'AUX', 'morph': 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin', 'lemma': 'be', 'dep': 'ROOT', 'head': 1}
</code></pre>
<p>How do I ensure lemmatize happens on the spell-checked term?</p>
",Preprocessing of the text & Tokenization,configure spacy pipeline lemmatize spellchecker component result currently trying add spellchecker step one spacy built pipeline specifically found pretty neat component called contextual spell check inserted pipeline problem lemmatize step lemmatizing spell checked word rather original text even reordered pipeline example return correctly spell checked result however examining underlying result ensure lemmatize happens spell checked term
Preprocess words that do not match list of words,"<p>I have a very specific case I'm trying to match: I have some text and a list of words (which may contain numbers, underscores, or ampersand), and I want to clean the text of numeric characters (for instance) unless it is a word in my list. This list is also long enough that I can't just make a regex that matches every one of the words.</p>
<p>I've tried to use regex to do this (i.e. doing something along the lines of <code>re.sub(r'\d+', '', text)</code>, but trying to come up with a more complex regex to match my case. This obviously isn't quite working, as I don't think regex is meant to handle that kind of case.</p>
<p>I'm trying to experiment with other options like pyparsing, and tried something like the below, but this also gives me an error (probably because I'm not understanding pyparsing correctly):</p>
<pre><code>from pyparsing import *
import re

phrases = [&quot;76&quot;, &quot;tw3nty&quot;, &quot;potato_man&quot;, &quot;d&amp;&quot;]
text = &quot;there was once a potato_man with tw3nty cars and d&amp; 76 different homes&quot;
parser = OneOrMore(oneOf(phrases) ^ Word(alphanums).setParseAction(lambda word: re.sub(r'\d+', '', word)))
parser.parseString(text)
</code></pre>
<p>What's the best way to approach this sort of matching, or are there other better suited libraries that would be worth a try?</p>
",Preprocessing of the text & Tokenization,preprocess word match list word specific case trying match text list word may contain number underscore ampersand want clean text numeric character instance unless word list list also long enough make regex match every one word tried use regex e something along line trying come complex regex match case obviously quite working think regex meant handle kind case trying experiment option like pyparsing tried something like also give error probably understanding pyparsing correctly best way approach sort matching better suited library would worth try
"Anyone have a way to tokenize a paragraph, put each sentence into a pandas data frame, and perform sentiment analysis on each?","<p>Beginner NLP/python programmer. Title says it all. I basically need a code that will tokenize a paragraph, perform sentiment analysis on each sentence put each sentence along with it's rating on a pandas data frame. I already have code that can tokenize a paragraph and even perform sentiment analysis, but I'm struggling with putting both into a data frame. Thus far, I have:</p>
<p>I used newspaper3k to extract the url and text.</p>
<pre><code>from newspaper import fulltext
import requests
url = &quot;https://www.click2houston.com/news/local/2021/06/18/houston-water-wastewater-proposed-increase-this-is-what-mayor-sylvester-turner-wants-you-to-know/&quot;
text = fulltext(requests.get(url).text)
</code></pre>
<p>Then I used the BERT extractive summarizer to summarize the article text.</p>
<pre><code>models = Summarizer()
result = models(text, min_length=30)
full = &quot;&quot;.join(result)
type(full)
</code></pre>
<p>Then I tokenized the summary into sentences using nltk.</p>
<pre><code>tokens=sent_tokenize(full)
print(type(np.array(tokens)[0]))
</code></pre>
<p>Lastly, I put it into a basic dataframe.</p>
<pre><code>df = pd.DataFrame(np.array(tokens), columns=['sentences'])
</code></pre>
<p>The only thing I'm missing is the sentiment analysis. I simply need a sentiment analysis (preferably from BERT) rating on each sentence implemented into the data frame.</p>
",Preprocessing of the text & Tokenization,anyone way tokenize paragraph put sentence panda data frame perform sentiment analysis beginner nlp python programmer title say basically need code tokenize paragraph perform sentiment analysis sentence put sentence along rating panda data frame already code tokenize paragraph even perform sentiment analysis struggling putting data frame thus far used newspaper k extract url text used bert extractive summarizer summarize article text tokenized summary sentence using nltk lastly put basic dataframe thing missing sentiment analysis simply need sentiment analysis preferably bert rating sentence implemented data frame
Tokenize characters except when encapsulated by brackets and keep brackets,"<p>I am parsing my keystroke data. It looks something like this:</p>
<pre><code>&gt; key_data = 'stuff[up][left][return]end'
</code></pre>
<p>I want to tokenize the characters, but treat the modifiers surrounded by [] as a single token.</p>
<pre><code>&gt; print(key_tokens)
['s','t','u','f','f','[up]','[left]','[return]','e','n','d']
</code></pre>
<p>I know I can do something like this to find the encapsulated sections:</p>
<pre><code>&gt; key_tokens = re.split(r'([\[\]])', key_data)
&gt; print(key_tokens)
['stuff','[','up',']','[','left',']','[','return',']','end']
</code></pre>
<p>I can also of course do something like this to separate each character:</p>
<pre><code>&gt; key_tokens = [c for c in key_data]
&gt; print(key_tokens)
['s','t','u','f','f','[','u','p',']','[','l','e','f','t',']','[','r','e','t','u','r','n',']','e','n','d']
</code></pre>
<p>I am just having trouble putting it all together.</p>
<p>Edit: Now I am seeing a corner case where the opening square bracket is used as text. Unfortunately, it is not escaped or anything.</p>
<pre><code>&gt; key_data = 'stuff[but[up][left][return]end'
&gt; key_tokens = re.findall('\[.*?\]|.', key_data)
&gt; print(key_tokens)
['s','t','u','f','f','[but[up]','[left]','[return]','e','n','d']
</code></pre>
<p>What I want to see is:</p>
<pre><code>&gt; print(key_tokens)
['s','t','u','f','f','[','b','u','t','[up]','[left]','[return]','e','n','d']
</code></pre>
",Preprocessing of the text & Tokenization,tokenize character except encapsulated bracket keep bracket parsing keystroke data look something like want tokenize character treat modifier surrounded single token know something like find encapsulated section also course something like separate character trouble putting together edit seeing corner case opening square bracket used text unfortunately escaped anything want see
How to stop BERT from breaking apart specific words into word-piece,"<p>I am using a pre-trained BERT model to tokenize a text into meaningful tokens. However, the text has many specific words and I don't want BERT model to break them into word-pieces. Is there any solution to it?
For example:</p>

<pre><code>tokenizer = BertTokenizer('bert-base-uncased-vocab.txt')
tokens = tokenizer.tokenize(""metastasis"")
</code></pre>

<p>Create tokens like this:</p>

<pre><code>['meta', '##sta', '##sis']
</code></pre>

<p>However, I want to keep the whole words as one token, like this:</p>

<pre><code>['metastasis']
</code></pre>
",Preprocessing of the text & Tokenization,stop bert breaking apart specific word word piece using pre trained bert model tokenize text meaningful token however text ha many specific word want bert model break word piece solution example create token like however want keep whole word one token like
Calculate average vector of a text on SpaCy,"<p>I'm using SpaCy calculate de distance between several documents, my approach is the as follows:</p>

<p>1) Convert the text into a spacy object
2) Remove the stop words
3) For each of the remaining words, get the vector representation and calculate the average. 
4) Use several methods to measure the distance between the documents.</p>

<p>My problem with this approach is that it takes a long time for big documents.</p>

<p>I have found the spacy has a method, called similarity, to do this that works faster:</p>

<pre><code>nlp_latin = spacy.load(""/tmp/la_vectors_wiki_lg"")
doc1 = nlp_latin(u""Caecilius est in horto"")
doc2 = nlp_latin(u""servus est in atrio"")
doc1.similarity(doc2)
</code></pre>

<p>But it uses cosine distance only, is there a method that just return the average vector of the document? </p>
",Preprocessing of the text & Tokenization,calculate average vector text spacy using spacy calculate de distance several document approach follows convert text spacy object remove stop word remaining word get vector representation calculate average use several method measure distance document problem approach take long time big document found spacy ha method called similarity work faster us cosine distance method return average vector document
Text Classification and VIF,"<p>Does VIF matter when it comes to dealing with Text Classification. I have a large dataset, after tokenizing the text, I have 400 columns of stemmed words. After running tests for VIF, I noticed that most variable's VIF were less than 5. The remaining VIF's fluctuated between 6-37, should I be removing these variables?</p>
",Preprocessing of the text & Tokenization,text classification vif doe vif matter come dealing text classification large dataset tokenizing text column stemmed word running test vif noticed variable vif le remaining vif fluctuated removing variable
Problems using Spacy tokenizer with special characters,"<p>I'm new to Spacy and I'm trying to find some patterns in a text, but I'm having trouble because of the form that tokenization works. For example, I have created the following pattern, trying to find percentage elements like &quot;0,42%&quot; using the Matcher (it's not exactly what I want, but I'm just practicing for now):</p>
<pre><code>nlp = spacy.load(&quot;pt_core_news_sm&quot;)

matcher = Matcher(nlp.vocab)

text = 'total: 1,80%:(comex 1,30% + deriv 0,50%/ativo: 1,17% '

pattern_test =  [{&quot;TEXT&quot;: {&quot;REGEX&quot;: &quot;[0-9]+[,.]+[0-9]+[%]&quot;}}]  

text_ = nlp(text)

matcher.add(&quot;pattern test&quot;, [pattern_test] )
result = matcher(text_)

for id_, beg, end in result:
    print(id_)
    print(text_[beg:end])
</code></pre>
<p>The thing is that it is returning results like the one below, cause tokenization considers this as only one token:</p>
<pre><code>9844711491635719110
1,80%:(comex
9844711491635719110
0,50%/ativo
</code></pre>
<p>I tried using Python's .replace() method on the string to replace special characters for blank spaces before tokenizing it, but now when I print the tokenization result it's separating everything like this:</p>
<pre><code>text_adjustment = text.replace(&quot;:&quot;, &quot; &quot;).replace(&quot;(&quot;, &quot; &quot;).replace(&quot;)&quot;, &quot; &quot;).replace(&quot;/&quot;, &quot; &quot;).replace(&quot;;&quot;, &quot; &quot;).replace(&quot;-&quot;, &quot; &quot;).replace(&quot;+&quot;, &quot; &quot;)

print([token for token in text_adjustment])

['t', 'o', 't', 'a', 'l', ' ', ' ', '1', ',', '8', '0', '%', ' ', ' ', 'c', 'o', 'm', 'e', 'x', ' ', '1', ',', '3', '0', '%', ' ', ' ', ' ', 'd', 'e', 'r', 'i', 'v', ' ', '0', ',', '5', '0', '%', ' ', 'a', 't', 'i', 'v', 'o', ' ', ' ', '1', ',', '1', '7', '%', ' ']
</code></pre>
<p>I would like the tokenization result to be like that:</p>
<pre><code>['total', '1,80%', 'comex', '1,30%', 'deriv', '0,50%', 'ativo', '1,17%']
</code></pre>
<p>Is there a better way to do that? I'm using the 'pt_core_news_sm' model, but I can change the language if I want to.</p>
<p>Thanks in advance :)</p>
",Preprocessing of the text & Tokenization,problem using spacy tokenizer special character new spacy trying find pattern text trouble form tokenization work example created following pattern trying find percentage element like using matcher exactly want practicing thing returning result like one cause tokenization considers one token tried using python replace method string replace special character blank space tokenizing print tokenization result separating everything like would like tokenization result like better way using pt core news sm model change language want thanks advance
Create clean text (without \n ) and add the language : HTTPError: HTTP Error 403: Forbidden Python,"<p>I am currently working on som Natural Language Processing in Python. The goal here is to get some pdf into text then do some text cleaning.
As the title is trying to tell I'm trying to do some text cleaning, I use TextBlob and here below is my def:</p>
<p>Additional info previsous to this cell I Used PDFMiner: PDF to Text.</p>
<pre><code>
def get_language(row):
    b = TextBlob(row['clean_text'])
    return b.detect_language()

df_cvs['clean_text'] = df_cvs['text'].str.replace('\n', ' ')
df_cvs.loc[:, 'language'] = df_cvs.apply(lambda row: get_language(row), axis=1)

df_cvs.head(2)```



Now the error is the following: 


Error details below: **HTTPError: HTTP Error 403: Forbidden**
---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
&lt;ipython-input-71-f02d06c663d3&gt; in &lt;module&gt;
      6 
      7 df_cvs['clean_text'] = df_cvs['text'].str.replace('\n', ' ')
----&gt; 8 df_cvs.loc[:, 'language'] = df_cvs.apply(lambda row: get_language(row), axis=1)
      9 
     10 df_cvs.head(2)

~\Anaconda3\lib\site-packages\pandas\core\frame.py in apply(self, func, axis, raw, result_type, args, **kwds)
   7546             kwds=kwds,
   7547         )
-&gt; 7548         return op.get_result()
   7549 
   7550     def applymap(self, func) -&gt; &quot;DataFrame&quot;:

~\Anaconda3\lib\site-packages\pandas\core\apply.py in get_result(self)
    178             return self.apply_raw()
    179 
--&gt; 180         return self.apply_standard()
    181 
    182     def apply_empty_result(self):

~\Anaconda3\lib\site-packages\pandas\core\apply.py in apply_standard(self)
    269 
    270     def apply_standard(self):
--&gt; 271         results, res_index = self.apply_series_generator()
    272 
    273         # wrap results

~\Anaconda3\lib\site-packages\pandas\core\apply.py in apply_series_generator(self)
    298                 for i, v in enumerate(series_gen):
    299                     # ignore SettingWithCopy here in case the user mutates
--&gt; 300                     results[i] = self.f(v)
    301                     if isinstance(results[i], ABCSeries):
    302                         # If we have a view on v, we need to make a copy because

&lt;ipython-input-71-f02d06c663d3&gt; in &lt;lambda&gt;(row)
      6 
      7 df_cvs['clean_text'] = df_cvs['text'].str.replace('\n', ' ')
----&gt; 8 df_cvs.loc[:, 'language'] = df_cvs.apply(lambda row: get_language(row), axis=1)
      9 
     10 df_cvs.head(2)

&lt;ipython-input-71-f02d06c663d3&gt; in get_language(row)
      3 def get_language(row):
      4     b = TextBlob(row['clean_text'])
----&gt; 5     return b.detect_language()
      6 
      7 df_cvs['clean_text'] = df_cvs['text'].str.replace('\n', ' ')

~\Anaconda3\lib\site-packages\textblob\blob.py in detect_language(self)
    566         :rtype: str
    567         &quot;&quot;&quot;
--&gt; 568         return self.translator.detect(self.raw)
    569 
    570     def correct(self):

~\Anaconda3\lib\site-packages\textblob\translate.py in detect(self, source, host, type_)
     70         data = {&quot;q&quot;: source}
     71         url = u'{url}&amp;sl=auto&amp;tk={tk}'.format(url=self.url, tk=_calculate_tk(source))
---&gt; 72         response = self._request(url, host=host, type_=type_, data=data)
     73         result, language = json.loads(response)
     74         return language

~\Anaconda3\lib\site-packages\textblob\translate.py in _request(self, url, host, type_, data)
     90         if host or type_:
     91             req.set_proxy(host=host, type=type_)
---&gt; 92         resp = request.urlopen(req)
     93         content = resp.read()
     94         return content.decode('utf-8')

~\Anaconda3\lib\urllib\request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context)
    220     else:
    221         opener = _opener
--&gt; 222     return opener.open(url, data, timeout)
    223 
    224 def install_opener(opener):

~\Anaconda3\lib\urllib\request.py in open(self, fullurl, data, timeout)
    529         for processor in self.process_response.get(protocol, []):
    530             meth = getattr(processor, meth_name)
--&gt; 531             response = meth(req, response)
    532 
    533         return response

~\Anaconda3\lib\urllib\request.py in http_response(self, request, response)
    638         # request was successfully received, understood, and accepted.
    639         if not (200 &lt;= code &lt; 300):
--&gt; 640             response = self.parent.error(
    641                 'http', request, response, code, msg, hdrs)
    642 

~\Anaconda3\lib\urllib\request.py in error(self, proto, *args)
    567         if http_err:
    568             args = (dict, 'default', 'http_error_default') + orig_args
--&gt; 569             return self._call_chain(*args)
    570 
    571 # XXX probably also want an abstract factory that knows when it makes

~\Anaconda3\lib\urllib\request.py in _call_chain(self, chain, kind, meth_name, *args)
    500         for handler in handlers:
    501             func = getattr(handler, meth_name)
--&gt; 502             result = func(*args)
    503             if result is not None:
    504                 return result

~\Anaconda3\lib\urllib\request.py in http_error_default(self, req, fp, code, msg, hdrs)
    647 class HTTPDefaultErrorHandler(BaseHandler):
    648     def http_error_default(self, req, fp, code, msg, hdrs):
--&gt; 649         raise HTTPError(req.full_url, code, msg, hdrs, fp)
    650 
    651 class HTTPRedirectHandler(BaseHandler):

HTTPError: HTTP Error 403: Forbidden
</code></pre>
",Preprocessing of the text & Tokenization,create clean text without n add language httperror http error forbidden python currently working som natural language processing python goal get pdf text text cleaning title trying tell trying text cleaning use textblob def additional info previsous cell used pdfminer pdf text
How to tag unknown words (Tokens with tag UNK) in combined taggers,"<p>I am using the combined tagger described in the <a href=""https://www.nltk.org/book/ch05.html"" rel=""nofollow noreferrer"">nltk book - chapter 5</a></p>
<p>Here is the code</p>
<pre><code>t0 = nltk.DefaultTagger('NN')

t1 = nltk.UnigramTagger(train_sents, backoff=t0)

t2 = nltk.BigramTagger(train_sents, backoff=t1)
 
</code></pre>
<p>Since the default tagger tags every token to <code>NN</code> every token that is goes to t0 will be tagged <code>NN</code> they say this can be resolved by following below method</p>
<blockquote>
<p>Our approach to tagging unknown words still uses backoff to a regular-expression tagger or a default tagger. These are unable to make use of context. Thus, if our tagger encountered the word blog, not seen during training, it would assign it the same tag, regardless of whether this word appeared in the context the blog or to blog. How can we do better with these unknown words, or out-of-vocabulary items?</p>
</blockquote>
<blockquote>
<p>A useful method to tag unknown words based on context is to limit the vocabulary of a tagger to the most frequent n words, and to replace every other word with a special word UNK using the method shown in <strong>3</strong>. During training, a unigram tagger will probably learn that UNK is usually a noun. However, the n-gram taggers will detect contexts in which it has some other tag. For example, if the preceding word is to (tagged TO), then UNK will probably be tagged as a verb.</p>
</blockquote>
<p>I have written the method shown in <strong>3</strong> that maps every word to <code>UNK</code></p>
<pre><code>&gt;&gt;&gt; alice = nltk.corpus.gutenberg.words('carroll-alice.txt')
&gt;&gt;&gt; vocab = nltk.FreqDist(alice)
&gt;&gt;&gt; v1000 = [word for (word, _) in vocab.most_common(1000)]
&gt;&gt;&gt; mapping = defaultdict(lambda: 'UNK')
&gt;&gt;&gt; for v in v1000:
...     mapping[v] = v
...
&gt;&gt;&gt; alice2 = [mapping[v] for v in alice]
&gt;&gt;&gt; alice2[:100]
['UNK', 'Alice', &quot;'&quot;, 's', 'UNK', 'in', 'UNK', 'by', 'UNK', 'UNK', 'UNK',
'UNK', 'CHAPTER', 'I', '.', 'UNK', 'the', 'Rabbit', '-', 'UNK', 'Alice',
'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by',
'her', 'sister', 'on', 'the', 'UNK', ',', 'and', 'of', 'having', 'nothing',
'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'UNK', 'into', 'the',
'book', 'her', 'sister', 'was', 'UNK', ',', 'but', 'it', 'had', 'no',
'pictures', 'or', 'UNK', 'in', 'it', ',', &quot;'&quot;, 'and', 'what', 'is', 'the',
'use', 'of', 'a', 'book', &quot;,'&quot;, 'thought', 'Alice', &quot;'&quot;, 'without',
'pictures', 'or', 'conversation', &quot;?'&quot; ...]
&gt;&gt;&gt; len(set(alice2))
1001
</code></pre>
<p>My question is how do we implement this method in combined taggers? Where do I put the new mapped dictionary (in this example <code>mapping</code>) in combined taggers?</p>
",Preprocessing of the text & Tokenization,tag unknown word token tag unk combined tagger using combined tagger described nltk book chapter code since default tagger tag every token every token go tagged say resolved following method approach tagging unknown word still us backoff regular expression tagger default tagger unable make use context thus tagger encountered word blog seen training would assign tag regardless whether word appeared context blog blog better unknown word vocabulary item useful method tag unknown word based context limit vocabulary tagger frequent n word replace every word special word unk using method shown training unigram tagger probably learn unk usually noun however n gram tagger detect context ha tag example preceding word tagged unk probably tagged verb written method shown map every word question implement method combined tagger put new mapped dictionary example combined tagger
How to delete spaces beween letters that make up the same word?,"<p>I basically have this a whole document that at some point includes the text = &quot;R E S U L T&quot;, and after I tokenize it, the function tokenizes it letter by letter, so I would like to let the computer know &quot;RESULT&quot; must be tokenized as a whole word. Any ideas as to how to eliminate the spaces of the words separated by spaces in the document?</p>
<p>I was thinking somhow letting the computer know that if it finds a pattern like Letter+space+Letter+space maybe it could identify it?</p>
<p>this is the output I am getting:</p>
<p>R           NOUN      conj</p>
<p>E           NOUN      obj</p>
<p>S           PROPN     obj</p>
<p>U           PROPN     flat</p>
<p>L           PROPN     flat</p>
<p>T           PROPN     flat</p>
<p>and this is my code:</p>
<pre><code>for value in dict.values():
    sentence= nlp2(value)
    for token in sentence:
        token_text = token.text
        token_pos = token.pos_
        token_dep = token.dep_
        print(&quot;{:&lt;12}{:&lt;10}{:&lt;10}&quot;.format(token_text,token_pos,token_dep))
</code></pre>
<p>thanks so much!:)</p>
",Preprocessing of the text & Tokenization,delete space beween letter make word basically whole document point includes text r e u l tokenize function tokenizes letter letter would like let computer know result must tokenized whole word idea eliminate space word separated space document wa thinking somhow letting computer know find pattern like letter space letter space maybe could identify output getting r noun conj e noun obj propn obj u propn flat l propn flat propn flat code thanks much
Find a word in a list of words that has minimum distance with a given word,"<p>let say given a word list <code>['windows','hello','python','world','software','desk']</code> and an input word <code>'widow'</code>, how to (quickly) find the word from the word list that has the minimum edit distance with the input word <code>'widow'</code> (The answer in this example is <code>'windows'</code>)? Are there available libraries/functions to achieve it? Thanks!</p>
",Preprocessing of the text & Tokenization,find word list word ha minimum distance given word let say given word list input word quickly find word word list ha minimum edit distance input word answer example available library function achieve thanks
Remove words before specific word in a list of list,"<p>I have the following code to extract text from pdf. However, I need to remove all the words at the beginning of each file, specifically all the words before the word &quot;Information&quot; (just the first one).
I need to remove them and keep the rest of the file. All the text extracted must be stored at the end in a list of list as you can see in the code.
Here two sample PDFs that can be used a test.</p>
<pre><code>pdf=[] #creating an empty list
for k in range(1,90):
    # open the pdf file
    output_string = StringIO()
    with open(&quot;G:/File (%s).pdf&quot;%(k), 'rb') as in_file:
        parser = PDFParser(in_file)
        doc = PDFDocument(parser)
        rsrcmgr = PDFResourceManager()
        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())
        interpreter = PDFPageInterpreter(rsrcmgr, device)
        for page in PDFPage.create_pages(doc):
            interpreter.process_page(page)
    text = output_string.getvalue()
    text = text.split('\n')
    pdf.append(text)
    print(pdf[:])

</code></pre>
<p>PDFs are here <a href=""https://drive.google.com/drive/folders/1wmH98caKNe-dg4YrjxBSHj5bAd7bc93Z?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/1wmH98caKNe-dg4YrjxBSHj5bAd7bc93Z?usp=sharing</a></p>
",Preprocessing of the text & Tokenization,remove word specific word list list following code extract text pdf however need remove word beginning file specifically word word information first one need remove keep rest file text extracted must stored end list list see code two sample pdfs used test pdfs
Getting similar words no longer working in spacy,"<p>I have a Google Colab notebook from a while ago which uses spacy 2.2.4 and successfully gets the most similar words for a list of words:</p>
<pre><code>import spacy
import spacy.cli
spacy.cli.download(&quot;en_core_web_lg&quot;)
import en_core_web_lg
nlp = en_core_web_lg.load()
import numpy as np
import pandas as pd

print(spacy.__version__)

all_search_terms = [&quot;technology&quot;, &quot;internet&quot;, &quot;smartphone&quot;]

# define a function to get the x most similar words to a word
def most_similar(word, topn=2):
    print(word)
    word = nlp.vocab[str(word)]
    print(word.prob)
    queries = [
        w for w in word.vocab 
        if w.is_lower == word.is_lower and w.prob &gt;= -15 and np.count_nonzero(w.vector)
    ]

    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)
    return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]


# create function to receive a list of words and return the 
# top 2 similar words for each word in the list

def get_similar_words(list_of_words):
    
    all_similar_words = []
    
    for word in list_of_words:
        spacy_word = nlp.vocab[str(word)]
        if spacy_word.has_vector:
        
            # find similar words to the word, and store them in a dataframe along with their scores
            similar_words = pd.DataFrame(most_similar(word, topn=2), columns=[&quot;word&quot;, &quot;similarity_score&quot;])

            # save the list of similar words
            similar_words_list = list(similar_words[&quot;word&quot;])

            # append the list of similar words to the list to be returned
            all_similar_words.append(similar_words_list)
        
    # flatten the list of lists to one list
    all_similar_words = [item for sublist in all_similar_words for item in sublist]
    
    # remove duplicates from the list
    all_similar_words = list(dict.fromkeys(all_similar_words))
    
    # sort list in alphabetical order
    all_similar_words.sort()

    return all_similar_words


# run the function on the search terms entered by the user
new_search_terms = get_similar_words(all_search_terms)
new_search_terms

</code></pre>
<p>The output is:</p>
<pre><code>technology
-10.063644409179688
internet
-8.897857666015625
smartphone
-12.11159896850586
['handset', 'online', 'smartphones', 'technological', 'technologies', 'web']
</code></pre>
<p><strong>THE PROBLEM:</strong> I've just tried running the same code in a different environment within RStudio (i.e. NOT using Google Colab) where the version of spacy is 3.0.6 and the list of similar words (new_search_terms) is <strong>empty</strong>. I've also noticed that the word <strong>probabilities are all the same (-20)</strong>.</p>
<p>The output with spacy 3.0.6:</p>
<pre><code>technology
-20.0
internet
-20.0
smartphone
-20.0
[]
</code></pre>
<p>What do I need to do differently in this new version of spacy to get the same output as before?</p>
",Preprocessing of the text & Tokenization,getting similar word longer working spacy google colab notebook ago us spacy successfully get similar word list word output problem tried running code different environment within rstudio e using google colab version spacy list similar word new search term empty also noticed word probability output spacy need differently new version spacy get output
how to speed up NE recognition with stanford NER with python nltk,"<p>First I tokenize the file content into sentences and then call Stanford NER on each of the sentences. But this process is really slow. I know if I call it on the whole file content if would be faster, but I'm calling it on each sentence as I want to index each sentence before and after NE recognition. </p>

<pre><code>st = NERTagger('stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')
for filename in filelist:
    sentences = sent_tokenize(filecontent) #break file content into sentences
    for j,sent in enumerate(sentences): 
        words = word_tokenize(sent) #tokenize sentences into words
        ne_tags = st.tag(words) #get tagged NEs from Stanford NER
</code></pre>

<p>This is probably due to calling <code>st.tag()</code> for each sentence, but is there any way to make it run faster?</p>

<p><strong>EDIT</strong></p>

<p>The reason that I want to tag sentences separate is that I want to write sentences to a file (like sentence indexing) so that given the ne tagged sentence at a later stage, i can get the unprocessed sentence (i'm also doing lemmatizing here)</p>

<p>file format:</p>

<blockquote>
  <p>(sent_number, orig_sentence, NE_and_lemmatized_sentence)</p>
</blockquote>
",Preprocessing of the text & Tokenization,speed ne recognition stanford ner python nltk first tokenize file content sentence call stanford ner sentence process really slow know call whole file content would faster calling sentence want index sentence ne recognition probably due calling sentence way make run faster edit reason want tag sentence separate want write sentence file like sentence indexing given ne tagged sentence later stage get unprocessed sentence also lemmatizing file format sent number orig sentence ne lemmatized sentence
Set the parameters of Word2Vec for a practical example,"<p>I have a database containing about 2.8 million texts (more precisely tweets, so they are short texts). I put clean tweets (removing hashtags, tags, stop words...) in a list of lists of tokens called <code>sentences</code> (so it contains a list of tokens for each tweet).</p>
<p>After these steps, if I write</p>
<p><code>model = Word2Vec(sentences, min_count=1)</code></p>
<p>I obtain a vocabulary of about 400,000 words.</p>
<p>This was just an attempt, I would need some help to set the parameters (<code>size</code>, <code>window</code>, <code>min_count</code>, <code>workers</code>, <code>sg</code>) of <code>Word2Vec</code> in the most appropriate and consistent way.</p>
<p>Consider that my goal is to use</p>
<p><code>model.most_similar(terms)</code> (where <code>terms</code> is a list of words)</p>
<p>to find, within the list of lists of tokens <code>sentences</code>, the words most similar to those contained in <code>terms</code>.</p>
<p>The words in <code>terms</code> belong to the same topic and I would like to see if there are other words within the texts that could have to do with the topic.</p>
",Preprocessing of the text & Tokenization,set parameter word vec practical example database containing million text precisely tweet short text put clean tweet removing hashtags tag stop word list list token called contains list token tweet step write obtain vocabulary word wa attempt would need help set parameter appropriate consistent way consider goal use list word find within list list token word similar contained word belong topic would like see word within text could topic
Gensim word2vec and large amount of texts,"<p>I need to put the texts contained in a column of a MySQL database (about 3 million rows) into a list of lists of tokens. These texts (which are tweets, therefore they are generally short) must be preprocessed before being included in the list (stop words, hashtags, tags etc. must be removed). This list should be passed later as a <code>Word2Vec</code> parameter. This is the part of the code involved</p>
<pre><code>import mysql.connector
import re
from gensim.models import Word2Vec
import preprocessor as p
p.set_options(
    p.OPT.URL,
    p.OPT.MENTION,
    p.OPT.HASHTAG,
    p.OPT.NUMBER
)

conn = mysql.connector.connect(...)
cursor = conn.cursor()
query = &quot;SELECT text FROM tweet&quot;
cursor.execute(query)
table = cursor.fetchall()

stopwords = open('stopwords.txt', encoding='utf-8').read().split('\n')
sentences = []
for row in table:
    sentences = sentences + [[w for w in re.sub(r'[^\w\s-]', ' ', p.clean(row[0])).lower().split() if w not in stopwords and len(w) &gt; 2]]

cursor.close()
conn.close()

model = Word2Vec(sentences)
...
</code></pre>
<p>Obviously it takes a lot of time and I know that my method is probably inefficient. Can anyone recommend a better one? I know it is not a question directly related to <code>gensim</code> and <code>Word2Vec</code> but perhaps those who use them have already faced the problem of working with a large amount of texts.</p>
",Preprocessing of the text & Tokenization,gensim word vec large amount text need put text contained column mysql database million row list list token text tweet therefore generally short must preprocessed included list stop word hashtags tag etc must removed list passed later parameter part code involved obviously take lot time know method probably inefficient anyone recommend better one know question directly related perhaps use already faced problem working large amount text
Why do you need a threshold when tokenizing a text corpus?,"<p>So I'm a self-learning NLP and came across <a href=""https://www.kaggle.com/singhabhiiitkgp/text-summarization-using-lstm"" rel=""nofollow noreferrer"">this kaggle notebook</a> that does text summarization using an LSTM. When it makes an <code>orderedDict</code> of words to integers, there's some code that apparently calculates the percentage of rare words in the vocabulary:</p>
<pre><code>thresh=4

cnt, tot_cnt, freq, tot_freq = 0, 0, 0, 0

for key,value in x_tokenizer.word_counts.items():
    tot_cnt += 1
    tot_freq += value
    if(value &lt; thresh):
        cnt += 1
        freq += value
    
print(&quot;% of rare words in vocabulary:&quot;,(cnt/tot_cnt)*100)
print(&quot;Total Coverage of rare words:&quot;,(freq/tot_freq)*100)
</code></pre>
<p>Why is there a threshold value of 4 there? As far as I can see, the word to integer mappings are arbitrary (unless each integer = number of times the word was repeated), so the threshold value of 4 seems a bit arbitrary to me.</p>
<p>Thanks in advance for helping :)</p>
",Preprocessing of the text & Tokenization,need threshold tokenizing text corpus self learning nlp came across kaggle notebook doe text summarization using lstm make word integer code apparently calculates percentage rare word vocabulary threshold value far see word integer mapping arbitrary unless integer number time word wa repeated threshold value seems bit arbitrary thanks advance helping
removing words from strings without affecting words using python spacy,"<p>I am using spacy, i have a list of sentences i want to remove stop words and punctuation from it.</p>
<pre><code>for i in sentences_list: 
for token in docfile:
    if token.is_stop or token.is_punct and token.text in i[1]:
       i[1] = i[1].replace(token.text, '') 
print(sentences_list)
</code></pre>
<p>but it affect words too for example the word <code>I</code> is a stop word so the word <code>big</code> becomes <code>bg</code>.</p>
",Preprocessing of the text & Tokenization,removing word string without affecting word using python spacy using spacy list sentence want remove stop word punctuation affect word example word stop word word becomes
tokenizing: how to not tokenize punctuation like `^* in python for NLP,"<p><strong>I want to tokenize string punctuation except `*^</strong></p>
<p>I've tried but the result, all types of punctuation are separated, while for some punctuation I don't want to separate</p>
<p>when i use:</p>
<pre><code>text = &quot;hai*ini^ema`il saya lunar!?&quot;
tokenizer = TweetTokenizer()
nltk_tokens = tokenizer.tokenize(text)
nltk_tokens
</code></pre>
<p>i get:</p>
<pre><code>['hai', '*', 'ini', '^', 'ema', '`', 'il', 'saya', 'lunar', '!', '?']
</code></pre>
<p>what i want is:</p>
<pre><code>['hai*ini^ema`il', 'saya', 'lunar', '!', '?']
</code></pre>
<p>I want to tokenize but not tokenize *^`</p>
",Preprocessing of the text & Tokenization,tokenizing tokenize punctuation like python nlp want tokenize string punctuation except
how to append tokenized sentences as row to a csv,"<p>I am trying to do sentence tokenization several .txt files from a path, and then append each tokenized sentence to a new row with the *.txt document ID as csv.</p>
<p>There are several *txt files in the path (work_dir)
In the below example, the first column needs to be the file name (WLTW_5_2016_02_29), and the next column tokenized sentence. such that, if there are 40 sentences in a document, I would expect 40 rows with the same file name in the first column and the second column sentences.  I also attached a picture to show how the csv output is expected.</p>
<pre><code>import nltk
work_dir='/content/drive/My Drive/deneme'
filename = 'WLTW_5_2016_02_29.txt'
file = open(filename, 'rt')
text = file.read()
#file.close()
# split into sentences
from nltk import sent_tokenize
sentences = sent_tokenize(text)
print(sentences)
import csv

with open('writeData.csv', mode='w') as file:
    writer = csv.writer(file, delimiter=',', quotechar='&quot;', quoting=csv.QUOTE_MINIMAL)
    writer.writerow((&quot;filename&quot;, &quot;sentence&quot;))
    writer.writerow((filename, sentences))
</code></pre>
<p>I tried this approach but I couldnot manage it. <a href=""https://stackoverflow.com/questions/58151887/how-to-save-words-in-a-csv-file-tokenized-from-articles-with-sentence-id-number""> here</a></p>
<p><a href=""https://i.sstatic.net/ihOc5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ihOc5.png"" alt=""enter image description here"" /></a></p>
<p>with the above code, it writes everything to the same row. However as seen in the above example, I want to write them to the same column by appending as row.</p>
",Preprocessing of the text & Tokenization,append tokenized sentence row csv trying sentence tokenization several txt file path append tokenized sentence new row txt document id csv several txt file path work dir example first column need file name wltw next column tokenized sentence sentence document would expect row file name first column second column sentence also attached picture show csv output expected tried approach couldnot manage code writes everything row however seen example want write column appending row
Lemmatization corpus in R,"<p>I found there is one package which can be used to implement lemmatization ""textstem"".
But I am unable to install it.
I am sharing script used to install and error which I am getting.</p>

<pre><code>&gt; install.packages(""textstem"")
Installing package into ‘C:/Users/nilesh.ulhas.hulyal/Documents/R/win-library/3.2’
(as ‘lib’ is unspecified)
Warning in install.packages :
  package ‘textstem’ is not available (for R version 3.2.5)
&gt; if (!require(""pacman"")) install.packages(""pacman"")
&gt; pacman::p_load_gh(""trinker/textstem"")
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Peer certificate cannot be authenticated with given CA certificates
Warning message:
In pacman::p_load_gh(""trinker/textstem"") : 
Failed to install/load:
trinker/textstem
&gt; library(textstem)
Error in library(textstem) : there is no package called ‘textstem’
&gt; docs1 &lt;- tm_map(docs1, lemmatize_strings())
Error in match.fun(FUN) : could not find function ""lemmatize_strings""
</code></pre>

<p>I am sharing below code and error when tried to use Lemmatizationfunction from tm package </p>

<pre><code>&gt; docs1 &lt;- tm_map(docs1, lemmatize_strings)
Error in match.fun(FUN) : object 'lemmatize_strings' not found
</code></pre>

<p>Is there any other approach which we can use to implement lemmatization corpus in R.</p>
",Preprocessing of the text & Tokenization,lemmatization corpus r found one package used implement lemmatization textstem unable install sharing script used install error getting sharing code error tried use lemmatizationfunction tm package approach use implement lemmatization corpus r
Fuzzy matching with long sentence(s),"<p>suppose I have the following dataframe:</p>
<pre><code>ID       CompanyName         JobDescription
1        Green Grass LLC     &quot;In the centre of Green Grass area...&quot;
2        Johnny Inc.          &quot;Johnny is currently looking for data analist that...&quot;
3        Liamloy             &quot;LiamLoy Corp. is established in New York...&quot;
4        KaasKan             &quot;In the forest we are walking...&quot;
</code></pre>
<p>My main goal is to exclude the <code>CompanyName</code> in each <code>JobDescription</code>. The desired output would be:</p>
<pre><code>ID       CompanyName         JobDescription
1        Green Grass LLC     &quot;In the centre of area...&quot;
2        Johnny Inc.          &quot;is currently looking for data analist that...&quot;
3        Liamloy             &quot;is established in New York...&quot;
4        KaasKan             &quot;In the forest we are walking&quot;
</code></pre>
<p>I have tried to <code>word tokenize</code> the <code>JobDescription</code> (convert the sentence in to words) and apply <code>fuzzymatching</code> to detect and remove the matches. However, this was not very successful. For example, when tokenizing the third <code>JobDescription</code>. &quot;<code>Liamloy</code>&quot; is compared to &quot;<code>LiamLoy</code>&quot; and &quot;<code>Corp</code>.&quot;. Maybe this approach is not ideal. I have no idea at this point. I wonder if any of you would like to share their opinion and enlighten me how I can succesfully remove the <code>CompanyName</code> in each <code>JobDescription</code>.</p>
",Preprocessing of the text & Tokenization,fuzzy matching long sentence suppose following dataframe main goal exclude desired output would tried convert sentence word apply detect remove match however wa successful example tokenizing third compared maybe approach ideal idea point wonder would like share opinion enlighten succesfully remove
How to speed up for loop execution using multiprocessing in python,"<p>I have two lists. List A contains 500 words. List B contains 10000 words. I am trying to find similar words for List A with respect to B.I am using Spacy's similarity function. </p>

<p>The problem I am facing is that it takes ages to compute. I am new to multiprocessing usage, hence request help.</p>

<p>How do I speed up the execution of the for loop part through multiprocessing in python?</p>

<p>The following is my code.</p>

<pre><code>ListA =['Dell', 'GPU',......] #500 words lists
ListB = ['Docker','Ec2'.......] #10000 words lists
s_words = []
for token1 in ListB:
    list_to_sort = [] 
    for token2 in ListA:    
        list_to_sort.append((token1, token2,nlp(str(token1)).similarity(nlp(str(token2)))))
        sorted_list = sorted(list_to_sort, key = itemgetter(2), reverse=True)[0][:2]
        s_words.append(sorted_list)
</code></pre>
",Preprocessing of the text & Tokenization,speed loop execution using multiprocessing python two list list contains word list b contains word trying find similar word list respect b using spacy similarity function problem facing take age compute new multiprocessing usage hence request help speed execution loop part multiprocessing python following code
Creating bigrams list not in list of tuples but in list of strings of both words combined,"<p>I am using this code to create bigrams for tokenization of a list of titles (headlines).</p>
<pre><code>from nltk.util import ngrams
def bigram_creator(headlines):
    bigrams = []
    for line in headlines:
        bigrm = nltk.bigrams(line.split())
        bigrams.extend(bigrm)
    return bigrams
</code></pre>
<p>However the code is giving me a list of tuples:</p>
<p>ex: [('opinion', 'one'), ('one', 'good')]</p>
<p>and I would like it to output a list of strings of both words together:</p>
<p>ex[('opinion one'), ('one good')]</p>
<p>Anybody know what I have to do to my code to change it?
Thank you in advance</p>
",Preprocessing of the text & Tokenization,creating bigram list list tuples list string word combined using code create bigram tokenization list title headline however code giving list tuples ex opinion one one good would like output list string word together ex opinion one one good anybody know code change thank advance
Python stemming (with pandas dataframe),"<p>I created a dataframe with sentences to be stemmed.
I would like to use a Snowballstemmer to obtain higher accuracy with my classification algorithm. How can I achieve this?</p>
<pre><code>import pandas as pd
from nltk.stem.snowball import SnowballStemmer

# Use English stemmer.
stemmer = SnowballStemmer(&quot;english&quot;)

# Sentences to be stemmed.
data = [&quot;programmers program with programming languages&quot;, &quot;my code is working so there must be a bug in the interpreter&quot;] 
    
# Create the Pandas dataFrame.
df = pd.DataFrame(data, columns = ['unstemmed']) 

# Split the sentences to lists of words.
df['unstemmed'] = df['unstemmed'].str.split()

# Make sure we see the full column.
pd.set_option('display.max_colwidth', -1)

# Print dataframe.
df 

+----+---------------------------------------------------------------+
|    | unstemmed                                                     |
|----+---------------------------------------------------------------|
|  0 | ['programmers', 'program', 'with', 'programming', 'languages']|
|  1 | ['my', 'code', 'is', 'working', 'so', 'there', 'must',        |  
|    |  'be', 'a', 'bug', 'in', 'the', 'interpreter']                |
+----+---------------------------------------------------------------+
</code></pre>
",Preprocessing of the text & Tokenization,python stemming panda dataframe created dataframe sentence stemmed would like use snowballstemmer obtain higher accuracy classification algorithm achieve
"RegEx to insert spaces between (Number-Number), (Number-char), (Char-Special char) and (special char- char) except &#39;\&#39;","<p>I have written a code to clean the <code>LaTex</code> (Just a string) where I want to insert spaces among the nnumbers to tokenize the string. My code is as below:</p>
<pre><code>def insert_spaces(sentence):
  '''
  Add a space around special characters, number and digits. So &quot;2x+y -1/3x&quot; becomes: &quot;2 x + y - 1 / 3 x&quot;
  '''
  dummy_list = []
  splitted_sent = list(sentence)
    
  for i in range(len(splitted_sent)-1):
    dummy_list.append(splitted_sent[i])
    
    if splitted_sent[i].isalpha(): # if it is an alphabet
      if splitted_sent[i+1].isdigit() or (not splitted_sent[i+1].isalnum()):
        dummy_list.append(' ')
    
    elif splitted_sent[i].isdigit(): # if it is a number
      if splitted_sent[i+1].isalpha() or (not splitted_sent[i+1].isalnum()):
        dummy_list.append(' ')
        
    elif (not splitted_sent[i].isalnum()) and (splitted_sent[i] not in [' ','\\']): # if it is a special char but not ' ' already
      if splitted_sent[i+1].isalnum():
        dummy_list.append(' ')
        
  dummy_list.append(splitted_sent[-1])
  
  return ''.join(dummy_list)
</code></pre>
<p>For Example, if my original query is:</p>
<pre><code>'ds^{2} = (1 - {qcos\\theta\\over r})^{2\\over 1 + \\alpha^{2}}\\lbrace dr^2+r^2d\\theta^2+r^2sin^2\\theta d\\varphi^2\\rbrace -{dt^2\\over  (1 - {qcos\\theta\\over r})^{2\\over 1 + \\alpha^{2}}}\\, .\\label{eq:sps1} \\widetilde\\gamma_{\\rm hopf}\\simeq\\sum_{n&gt;0}\\widetilde{G}_n{(-a)^n\\over2^{2n-1}}\\label{H4}3455'
</code></pre>
<p>then I want it cleaned as:</p>
<pre><code>'d s ^ { 2 } = ( 1  - { q c o s \\theta \\over  r } ) ^ { 2 \\over  1  + \\alpha ^ { 2 } } \\lbrace  d r ^ 2 + r ^ 2 d \\theta ^ 2 + r ^ 2 sin ^ 2 \\theta  d \\varphi ^ 2 \\rbrace  -{ d t ^ 2 \\over   ( 1  - { q c o s \\theta \\over  r } ) ^ { 2 \\over  1  + \\alpha ^ { 2 } } } \\ , . \\label { eq : sps 1 } \\widetilde \\gamma _ { \\rm  h o p f } \\simeq \\sum _ { n &gt; 0 } \\widetilde { G } _ n { ( - a ) ^ n \\over 2 ^ { 2 n - 1 } } \\label { H 4 } 3 4 5 5'
</code></pre>
<p><a href=""https://github.com/ritheshkumar95/im2latex-tensorflow/blob/master/im2markup/scripts/preprocessing/preprocess_formulas.py"" rel=""nofollow noreferrer"">The above result is a product of this this script</a> which basically calls <a href=""https://github.com/ritheshkumar95/im2latex-tensorflow/blob/master/im2markup/scripts/preprocessing/preprocess_latex.js"" rel=""nofollow noreferrer"">this <code>KaTex</code> script</a></p>
<p>But right now, I am getting my results as:</p>
<pre><code>'ds ^{ 2 } = ( 1  - { qcos \\theta \\over  r })^{ 2 \\over  1  + \\alpha ^{ 2 }}\\lbrace  dr ^ 2 + r ^ 2 d \\theta ^ 2 + r ^ 2 sin ^ 2 \\theta  d \\varphi ^ 2 \\rbrace  -{ dt ^ 2 \\over   ( 1  - { qcos \\theta \\over  r })^{ 2 \\over  1  + \\alpha ^{ 2 }}}\\, .\\label { eq : sps 1 } \\widetilde \\gamma _{\\rm  hopf }\\simeq \\sum _{ n &gt; 0 }\\widetilde { G }_ n {(- a )^ n \\over 2 ^{ 2 n - 1 }}\\label { H 4 } 3455'
</code></pre>
<p>Is there any way of achieving the same with the help of RegEx?</p>
",Preprocessing of the text & Tokenization,regex insert space number number number char char special char special char char except written code clean string want insert space among nnumbers tokenize string code example original query want cleaned result product script basically call script right getting result way achieving help regex
nltk.corpus - &#39;getset_descriptor&#39; object has no attribute &#39;setdefault&#39;,"<p>I am using below code and importing stop words from nltk</p>
<pre><code>   #from nltk.corpus import words as word_corp
    from nltk.corpus import stopwords
    nlp = spacy.load('en_core_web_sm')
    phrase_matcher = PhraseMatcher(nlp.vocab)
    en_words = nltk.corpus.words.words('en')
    stop_words = stopwords.words('english')
</code></pre>
<p>But error is <code>AttributeError: 'getset_descriptor' object has no attribute 'setdefault' for ----&gt; 3 nlp = spacy.load('en_core_web_sm')</code> this line.</p>
",Preprocessing of the text & Tokenization,nltk corpus getset descriptor object ha attribute setdefault using code importing stop word nltk error line
Less Frequent Words appearing bigger - WordCloud in Python,"<p>I have been plotting the <code>wordcloud</code> using the <code>wordcloud</code> package from Python. Here's a sample of the code:</p>

<pre><code>from wordcloud import WordCloud, STOPWORDS
import matplotlib
import matplotlib.pyplot as plt
stopwords = set(STOPWORDS)

def show_wordcloud(data, title = None):
    wordcloud = WordCloud(
        background_color='black',
        stopwords=stopwords,
        max_words=200,
        max_font_size=40, 
        scale=3,
        random_state=1 # chosen at random by flipping a coin; it was heads
).generate(str(data))

    fig = plt.figure(1, figsize=(15, 15))
    plt.axis('off')
    if title: 
        fig.suptitle(title, fontsize=20)
        fig.subplots_adjust(top=2.3)
    matplotlib.rcParams.update({'font.size': 22})
    plt.title('Most Used Words for Emotion Tag 2 (What is the highlight?)')    
    plt.imshow(wordcloud)
    plt.savefig('2.jpg')
    plt.show()

show_wordcloud(df2['words'])
</code></pre>

<p><a href=""https://i.sstatic.net/3rbRX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3rbRX.jpg"" alt=""enter image description here""></a></p>

<p>Now, what I understood from  the official documentation of Wordcloud is that, most frequent non-stop words appear to be bigger, but here chirping is appearing than Bengal. But then when I check out the frequency of chirping:</p>

<pre><code>In [20]: df2[df2['words'].str.contains(""Chirping"")]
Out[20]:    words             tagid
           Chirping of birds    2
           Chirping of birds    2
</code></pre>

<p>And now, when I check the frequency of Bengal:</p>

<pre><code>In [21]: df2[df2['words'].str.contains(""Bengal"")]
Out[21]:     words                 tagid
        The mighty Bay Of Bengal    2
        Royal Bengal Tigers🐯       2
        #NammaBengaluru             2
        Traditional Bengali Meal    2
        Royal Bengal Tiger          2
        Enterning Taj Bengal.       2
</code></pre>

<p>""Bengal"" is appearing small in ""yellow"" color just below the word ""Part"" left of ""Trekking"".
Now I'm not able to understand why is that happening, or how I can fix that. Also I want to know is there a way to remove prepositions from <code>wordcloud</code>, like at, beside, inside, etc.</p>

<p>Is there a way I can assign weightage or frequency and then plot the <code>wordcloud</code>?</p>
",Preprocessing of the text & Tokenization,le frequent word appearing bigger wordcloud python plotting using package python sample code understood official documentation wordcloud frequent non stop word appear bigger chirping appearing bengal check frequency chirping check frequency bengal bengal appearing small yellow color word part left trekking able understand happening fix also want know way remove preposition like beside inside etc way assign weightage frequency plot
How to detect that two sentences are similar?,"<p>I want to compute how similar two arbitrary sentences are to each other.  For example:</p>

<blockquote>
  <ol>
  <li>A mathematician found a solution to the problem.</li>
  <li>The problem was solved by a young mathematician.</li>
  </ol>
</blockquote>

<p>I can use a tagger, a stemmer, and a parser, but I don’t know how detect that these sentences are similar.</p>
",Preprocessing of the text & Tokenization,detect two sentence similar want compute similar two arbitrary sentence example mathematician found solution problem problem wa solved young mathematician use tagger stemmer parser know detect sentence similar
remove words using regular expression,"<p>I am trying to create a function that takes a tweet and process it for sentiment analysis.</p>
<pre><code>tweet &lt;- &quot;Here’s a list of all of the exchanges #safemoon is affiliated with going into the AMAs today!\nIf you look at the roadmap- \nthey are planning to add more!\nWe’re still early and JUST getting started!\nCredit to @_tokendad \n#safemooncommunity #safemoonarmy #crypto #cryptotwitter&quot;
</code></pre>
<p>I want to remove all hyperlinks, words that are hashtags (starting with #) and mentions (starting with @)</p>
<p>I solved the removing hyperlink function. How can I use regular expression to look for hashtags and mentions and remove them from tweet</p>
<pre><code>process_tweet(tweet){

   tweet &lt;- gsub('http\\S+', '', tweet) # get rid of hyperlinks
   tweet &lt;- gsub(, '', tweet) # how do I look for words that start with @ or # and remove them

   return(tweet)
}
</code></pre>
",Preprocessing of the text & Tokenization,remove word using regular expression trying create function take tweet process sentiment analysis want remove hyperlink word hashtags starting mention starting solved removing hyperlink function use regular expression look hashtags mention remove tweet
STANZA/ RuntimeError: Integer division of tensors using div or / is no longer supported,"<p>I want to use stanza for tokenizing, pos tagging and parsing some text I have, but it keeps giving me this error. I've tried changing the way a I call it but nothing happens. Any ideas?</p>
<p>My code(Here a iterate through a list of list of text and appli stanza to each one)</p>
<pre><code>t = time()

data_stanza = []
for text in data:
    stz = apply_stanza(text[0])
    data_stanza.append(stz)

print('Time to run: {} mins'.format(round((time() - t) / 60, 2)))
</code></pre>
<p>This is the function I use to <code>apply_stanza</code> to each text:</p>
<pre><code>nlp = stanza.Pipeline('pt')

def apply_stanza(text):
    doc = nlp(text)
    All = []
    for sent in doc.sentences:
        for word in sent.words:
            All.append((word.id,word.text,word.lemma,word.upos,word.feats,word.head,word.deprel))
    return All
</code></pre>
<p>The error:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-17-7ac303eec8e8&gt; in &lt;module&gt;
      3 data_staza = []
      4 for text in data:
----&gt; 5     stz = apply_stanza(text[0])
      6     data_stanza.append(stz)
      7 

&lt;ipython-input-16-364c3ac30f32&gt; in apply_stanza(text)
      2 
      3 def apply_stanza(text):
----&gt; 4     doc = nlp(text)
      5     All = []
      6     for sent in doc.sentences:

~\anaconda3\lib\site-packages\stanza\pipeline\core.py in __call__(self, doc)
    174         assert any([isinstance(doc, str), isinstance(doc, list),
    175                     isinstance(doc, Document)]), 'input should be either str, list or Document'
--&gt; 176         doc = self.process(doc)
    177         return doc
    178 

~\anaconda3\lib\site-packages\stanza\pipeline\core.py in process(self, doc)
    168         for processor_name in PIPELINE_NAMES:
    169             if self.processors.get(processor_name):
--&gt; 170                 doc = self.processors[processor_name].process(doc)
    171         return doc
    172 

~\anaconda3\lib\site-packages\stanza\pipeline\mwt_processor.py in process(self, document)
     31                 preds = []
     32                 for i, b in enumerate(batch):
---&gt; 33                     preds += self.trainer.predict(b)
     34 
     35                 if self.config.get('ensemble_dict', False):

~\anaconda3\lib\site-packages\stanza\models\mwt\trainer.py in predict(self, batch, unsort)
     77         self.model.eval()
     78         batch_size = src.size(0)
---&gt; 79         preds, _ = self.model.predict(src, src_mask, self.args['beam_size'])
     80         pred_seqs = [self.vocab.unmap(ids) for ids in preds] # unmap to tokens
     81         pred_seqs = utils.prune_decoded_seqs(pred_seqs)

~\anaconda3\lib\site-packages\stanza\models\common\seq2seq_model.py in predict(self, src, src_mask, pos, beam_size)
    259             done = []
    260             for b in range(batch_size):
--&gt; 261                 is_done = beam[b].advance(log_probs.data[b])
    262                 if is_done:
    263                     done += [b]

~\anaconda3\lib\site-packages\stanza\models\common\beam.py in advance(self, wordLk, copy_indices)
     82         # bestScoresId is flattened beam x word array, so calculate which
     83         # word and beam each score came from
---&gt; 84         prevK = bestScoresId / numWords
     85         self.prevKs.append(prevK)
     86         self.nextYs.append(bestScoresId - prevK * numWords)

RuntimeError: Integer division of tensors using div or / is no longer supported, and in a future release div will perform 
true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.
</code></pre>
<p>ATT: It turns after all that it was and error with the mwt module of stanza pipeline, so I just specified not to use it.</p>
",Preprocessing of the text & Tokenization,stanza runtimeerror integer division tensor using div longer supported want use stanza tokenizing po tagging parsing text keep giving error tried changing way call nothing happens idea code iterate list list text appli stanza one function use text error att turn wa error mwt module stanza pipeline specified use
How do i retain numbers while preprocessing data using gensim in python?,"<p>I have used gensim.utils.simple_preprocess(str(sentence) to create a dictionary of words that I want to use for topic modelling. However, this is also filtering important numbers (house resolutions, bill no, etc) that I really need. How did I overcome this? Possibly by replacing digits with their word form. How do i go about it, though?</p>
",Preprocessing of the text & Tokenization,retain number preprocessing data using gensim python used gensim utils simple preprocess str sentence create dictionary word want use topic modelling however also filtering important number house resolution bill etc really need overcome possibly replacing digit word form go though
How to extract values from string? (swift),"<p>I'am trying to analyze string and decompose it into clear values: <code>taskValue</code> and <code>timeValue</code>.</p>
<pre><code>var str1 = &quot;20 minutes (to do)/(for) some kind of task&quot;
// other possibilities
var str2 = &quot;1 hour 30 minutes for some kind of task&quot;
var str3 = &quot;do some kind of task for 1 hour&quot;
</code></pre>
<p>How can I apply multiple regexes in one function? Maybe, something like array of regexes</p>
<pre><code>[&quot;[0-9]{1,} minutes&quot;, 
 &quot;[0-9] hour&quot;, 
 &quot;[0-9] hour, [0-9]{1,} minutes&quot;,
  ...]
</code></pre>
<p>The values returned from function aren't clean, it remains with <code>&quot;of ..&quot;, &quot;for...&quot;, &quot;to...&quot;</code> etc.</p>
<p>Can you give me advice how to improve it? Maybe it's possible to do some machine learning with MLKit? How to add a couple of regex patterns? Or to check if string contains certain things manually?</p>
<pre><code>// check it out
var str = &quot;20 minutes to do some kind of task&quot;
func decompose(_ inputText: String) -&gt; (time: String, taskName: String) {
    
    let pattern = &quot;[0-9]{1,} minutes&quot;
    let regexOptions: NSRegularExpression.Options = [.caseInsensitive]
    let matchingOptions: NSRegularExpression.MatchingOptions = [.reportCompletion]
    let range = NSRange(location: 0, length: inputText.utf8.count)
    
    var time = &quot;&quot;
    var taskName = inputText
    
    let regex = try! NSRegularExpression(pattern: pattern, options: regexOptions)
    if let matchIndex = regex.firstMatch(in: inputText, options: matchingOptions, range: range) {
        
        let startIndex = inputText.index(inputText.startIndex, offsetBy: matchIndex.range.lowerBound)
        let endIndex = inputText.index(inputText.startIndex, offsetBy: matchIndex.range.upperBound)
        
        time = String(inputText[startIndex..&lt;endIndex])

        taskName.removeSubrange(startIndex..&lt;endIndex)
           
    } else {
        print(&quot;No match.&quot;)
    }


    return (time, taskName)
}

print(decompose(str))
</code></pre>
<p>Overall, I look to learn how to do text analysis on premise that we know the thematics beforehand.</p>
",Preprocessing of the text & Tokenization,extract value string swift trying analyze string decompose clear value apply multiple regexes one function maybe something like array regexes value returned function clean remains etc give advice improve maybe possible machine learning mlkit add couple regex pattern check string contains certain thing manually overall look learn text analysis premise know thematics beforehand
Is there a module or regex in Python to convert all fonts to a uniform font? (Text is coming from Twitter),"<p>I'm working with some text from twitter, using Tweepy. All that is fine, and at the moment I'm just looking to start with some basic frequency counts for words. However, I'm running into an issue where the ability of users to use different fonts for their tweets is making it look like some words are their own unique word, when in reality they're words that have already been encountered but in a different font/font size, like in the picture below (those words are words that were counted previously and appear in the spreadsheet earlier up).</p>
<p><a href=""https://i.sstatic.net/NJ9Vj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NJ9Vj.png"" alt=""enter image description here"" /></a></p>
<p>This messes up the accuracy of the counts. I'm wondering if there's a package or general solution to make all the words a uniform font/size - either while I'm tokenizing it (just by hand, not using a module) or while writing it to the csv (using the csv module). Or any other solutions for this that I may not be considering. Thanks!</p>
",Preprocessing of the text & Tokenization,module regex python convert font uniform font text coming twitter working text twitter using tweepy fine moment looking start basic frequency count word however running issue ability user use different font tweet making look like word unique word reality word already encountered different font font size like picture word word counted previously appear spreadsheet earlier mess accuracy count wondering package general solution make word uniform font size either tokenizing hand using module writing csv using csv module solution may considering thanks
How can I find words that occur frequently across several different texts?,"<p>So, I'm trying to find words that crop up in a collection several texts. They don't necessarily have to be very frequent in any given text, or even across all the texts -- their frequency for any given text in the sample has to be roughly the same, that's all.</p>
<p>This seems fairly simple, but I haven't been able to find a clean and elegant way to do it -- the only idea that comes to mind is getting the frequency for all words in each given text (using something like <a href=""https://towardsdatascience.com/very-simple-python-script-for-extracting-most-common-words-from-a-story-1e3570d0b9d0"" rel=""nofollow noreferrer"">this</a>, say), turning those lists into dictionaries, and then getting every key where the range of values across all the dictionaries for that key is fairly low (like if the lowest value is within 25% of the highest, or whatever). That seems like it'd work, but it feels like such a kludge and this problem seems fairly common and banal, so I thought I'd ask if there's a better solution out there.</p>
",Preprocessing of the text & Tokenization,find word occur frequently across several different text trying find word crop collection several text necessarily frequent given text even across text frequency given text sample ha roughly seems fairly simple able find clean elegant way idea come mind getting frequency word given text using something like say turning list dictionary getting every key range value across dictionary key fairly low like lowest value within highest whatever seems like work feel like kludge problem seems fairly common banal thought ask better solution
Lemmatize multiple MB of raw text with Spacy and Inline::Python in Perl. Why is this slow?,"<p>I work on an NLP and I need to lemmatize tons of tokens from raw input text file from 10MB to 300MB and I decided to use <code>Inline::Python</code> with <code>spacy</code> to do this task. The problem is that it's very slow. After this, I create bags of words to put in a cosine similarity module to classify texts from the past years. Is there a way to process faster, multi-processing, multi-threading, or is it the pipe to Python that is slow? And i have i9, 64GB RAM, RTX 2080TI and SSD connected by nvme.</p>
<p>Here is the piece of code to lemmatize in French some text content and filter stop words:</p>
<pre class=""lang-perl prettyprint-override""><code>use Inline Python =&gt; &lt;&lt;'END_OF_PYTHON';

import spacy
from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop
nlp = spacy.load('fr_core_news_md')
nlp.max_length = 40000000

def lemmatizer(words):
    doc = nlp(words)
    return list(filter(lambda x: x not in list(fr_stop), list(map(lambda token: token.lemma_ , doc))))

END_OF_PYTHON
</code></pre>
<p>Unfortunately, there is no good French lemmatizer in Perl and the lemmatization increases my accuracy to classify text files in good categories by 5%. It's important when you have already 90% good results without it. In this piece of code, I only use the function <code>lemmatizer</code> in Perl after this. I don't reload each time the nlp <code>spacy</code> module in French (I think ?)</p>
<p>I thought about creating one thread per file. I have 15 big text files to lemmatize. One file per category from the recent years. But imo, the I/O is the problem. Do you have some ideas? I can't show more code because there are 1500 lines. I need 1000 seconds to process automatic classification with the smallest category (50/60 files from the current year). The biggest is 10x bigger than the smallest.</p>
",Preprocessing of the text & Tokenization,lemmatize multiple mb raw text spacy inline python perl slow work nlp need lemmatize ton token raw input text file mb mb decided use task problem slow create bag word put cosine similarity module classify text past year way process faster multi processing multi threading pipe python slow gb ram rtx ti ssd connected nvme piece code lemmatize french text content filter stop word unfortunately good french lemmatizer perl lemmatization increase accuracy classify text file good category important already good result without piece code use function perl reload time nlp module french think thought creating one thread per file big text file lemmatize one file per category recent year imo problem idea show code line need second process automatic classification smallest category file current year biggest x bigger smallest
NLP for Reference Classification,"<p>I need to parse several technical text documents to find every instance of references.</p>
<p>Using regular expressions, I easily managed to extract every mention of references, because all our documents follow naming standards.</p>
<p>For example : ABC0001, AB-000-001, AB-00001, etc.</p>
<p>The problem I am facing is that often, a text won't just reference another document, but will instead  say that it is being referenced by another document.</p>
<p>For example, here are 2 possible sentences:</p>
<blockquote>
<p>For further detail, go see AB-000-001.</p>
</blockquote>
<p>or</p>
<blockquote>
<p>This document is used as a reference by AB-00001.</p>
</blockquote>
<p>Since these sentences don't necessarily follow a pattern, I'd like to use a simple text classification model to detect whether a sentence is referencing  to another document, or is the subject of a reference.</p>
<p>I was inspired by this <a href=""https://towardsdatascience.com/sarcasm-text-classification-using-spacy-in-python-7cd39074f32e"" rel=""nofollow noreferrer"">article</a>, but I can't find a pre-exisiting data set for my problem. Do I have to train a model from scratch, or are there simpler ways of getting the job done ?</p>
<p>Thanks,</p>
<p>Liam</p>
<p>PS: I'm working in Python</p>
",Preprocessing of the text & Tokenization,nlp reference classification need parse several technical text document find every instance reference using regular expression easily managed extract every mention reference document follow naming standard example abc ab ab etc problem facing often text reference another document instead say referenced another document example possible sentence detail go see ab document used reference ab since sentence necessarily follow pattern like use simple text classification model detect whether sentence referencing another document subject reference wa inspired article find pre exisiting data set problem train model scratch simpler way getting job done thanks liam p working python
What pre-processing does VADER sentiment analysis use?,"<p>I read that VADER performs tokenization / lemmatization automatically when you use <code>SentimentIntensityAnalyzer()</code> but I cannot get a clear confirmation on their <a href=""https://github.com/cjhutto/vaderSentiment"" rel=""nofollow noreferrer"">github</a> page. I tried tokenize and lemmatize my input text anyways and the outputs (compound scores) are identical than if I didn't do any of my own pre-processing.</p>
<p>Can anyone confirm what pre-processing techniques they use?</p>
<p>Thank you.</p>
",Preprocessing of the text & Tokenization,pre processing doe vader sentiment analysis use read vader performs tokenization lemmatization automatically use get clear confirmation github page tried tokenize lemmatize input text anyways output compound score identical pre processing anyone confirm pre processing technique use thank
Is it possible to detokenize DataFrame in PySpark?,"<p>I'm using app.zelp.com to perform NLP. After tokenization and removing the stopwords, I would like to detokenize the remaining words and export to csv. Is that possible?</p>
<pre><code>%python
# Start Spark session
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName(&quot;StopWords&quot;).getOrCreate()
from pyspark.ml.feature import Tokenizer, StopWordsRemover 
from pyspark import SparkFiles
url =&quot;myamazon s3 url&quot;
spark.sparkContext.addFile(url)
df = spark.read.csv(SparkFiles.get(&quot;myfile.csv&quot;), sep=&quot;,&quot;, header=True)

# Tokenize DataFrame
review_data = Tokenizer(inputCol=&quot;Text&quot;, outputCol=&quot;Words&quot;)
# Transform DataFrame
reviewed = review_data.transform(df)
# Remove stop words
remover = StopWordsRemover(inputCol=&quot;Words&quot;, outputCol=&quot;filtered&quot;)
newFrame = remover.transform(reviewed)

final = newFrame.select(&quot;filtered&quot;)
</code></pre>
<p>I would like to combine the remaining words and export to csv. Is it possible?</p>
",Preprocessing of the text & Tokenization,possible detokenize dataframe pyspark using app zelp com perform nlp tokenization removing stopwords would like detokenize remaining word export csv possible would like combine remaining word export csv possible
R Tm package dictionary matching leads to higher frequency than actual words of text,"<p>I have been using the code below to load text as a corpus and using the tm package to clean the text. As a next step I am loading a dictionary and cleaning it as well. Then I am matching the words from the text with the dictionary to calculate a score. However, the matching results in a higher number of matches than actual words in the text (e.g., the competence score is 1500 but the actual number of words in the text is only 1000).</p>
<p>I think it is related to the stemming of the text and the dictionary as the matches are lower when there is no stemming performed.</p>
<p>Do you have any ideas why this is happening?</p>
<p>Thank you very much.</p>
<p><strong>R Code</strong></p>
<p><strong>Step 1 Storing data as corpus</strong></p>
<p><code>file.path &lt;- file.path(here(&quot;Generated Files&quot;, &quot;Data Preparation&quot;)) corpus &lt;- Corpus(DirSource(file.path))</code></p>
<p><strong>Step 2 Cleaning data</strong></p>
<pre><code>#Removing special characters
toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x))
corpus &lt;- tm_map(corpus, toSpace, &quot;/&quot;)
corpus &lt;- tm_map(corpus, toSpace, &quot;@&quot;)
corpus &lt;- tm_map(corpus, toSpace, &quot;\\|&quot;) 

#Convert the text to lower case
corpus &lt;- tm_map(corpus, content_transformer(tolower))
#Remove numbers
corpus &lt;- tm_map(corpus, removeNumbers)
#Remove english common stopwords
corpus &lt;- tm_map(corpus, removeWords, stopwords(&quot;english&quot;))
#Remove your own stop word
specify your stopwords as a character vector
corpus &lt;- tm_map(corpus, removeWords, c(&quot;view&quot;, &quot;pdf&quot;)) 
#Remove punctuations
corpus &lt;- tm_map(corpus, removePunctuation)
#Eliminate extra white spaces
corpus &lt;- tm_map(corpus, stripWhitespace)
#Text stemming
corpus &lt;- tm_map(corpus, stemDocument)
#Unique words
corpus &lt;- tm_map(corpus, unique)
</code></pre>
<p><strong>Step 3 DTM</strong></p>
<p><code>dtm &lt;- DocumentTermMatrix(corpus)</code></p>
<p><strong>Step 4 Load Dictionaries</strong></p>
<pre><code>dic.competence &lt;- read_excel(here(&quot;Raw Data&quot;, &quot;6. Dictionaries&quot;, &quot;Brand.xlsx&quot;))
dic.competence &lt;- tolower(dic.competence$COMPETENCE)
dic.competence &lt;- stemDocument(dic.competence)
dic.competence &lt;- unique(dic.competence)
</code></pre>
<p><strong>Step 5 Count frequencies</strong></p>
<pre><code>corpus.terms = colnames(dtm)
competence = match(corpus.terms, dic.competence, nomatch=0)
</code></pre>
<p><strong>Step 6 Calculate scores</strong></p>
<pre><code>competence.score = sum(competence) / rowSums(as.matrix(dtm))
competence.score.df = data.frame(scores = competence.score)
</code></pre>
",Preprocessing of the text & Tokenization,r tm package dictionary matching lead higher frequency actual word text using code load text corpus using tm package clean text next step loading dictionary cleaning well matching word text dictionary calculate score however matching result higher number match actual word text e g competence score actual number word text think related stemming text dictionary match lower stemming performed idea happening thank much r code step storing data corpus step cleaning data step dtm step load dictionary step count frequency step calculate score
Custom tokenization rule spacy,"<p>How do I add a custom tokenization rule to spacy for the case of wanting a number and a symbol or word to be tokenized together. E.g. the following sentence:</p>
<p>&quot;I 100% like apples. I like 500g of apples&quot;</p>
<p>is tokenized as follows:</p>
<p>['I', '100', '%', 'like', 'apples', '.', 'I', 'like', '500', 'g', 'of', 'apples']</p>
<p>It would be preferable if it was tokenized like this:</p>
<p>['I', '100%', 'like', 'apples', '.', 'I', 'like', '500g', 'of', 'apples']</p>
<p>The following code was used to generate this:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_sm&quot;)
text = &quot;I 100% like apples. I like 500g of apples&quot;
print([token.text for token in nlp(text)])
</code></pre>
",Preprocessing of the text & Tokenization,custom tokenization rule spacy add custom tokenization rule spacy case wanting number symbol word tokenized together e g following sentence like apple like g apple tokenized follows like apple like g apple would preferable wa tokenized like like apple like g apple following code wa used generate
text mining preprocessing must be applied to test or to train set?,"<p>I'm doing some text-mining tasks and I have such a simple question and I still can't reach a conclusion.</p>
<p>I am applying pre-processing, such as tokenization and stemming to my training set so i can train my model.</p>
<p>Should I also apply this pre-processing to my test set?</p>
",Preprocessing of the text & Tokenization,text mining preprocessing must applied test train set text mining task simple question still reach conclusion applying pre processing tokenization stemming training set train model also apply pre processing test set
How to classify all words in a sentence with a context?,"<p>I have the names of the companies (in Russian). The name can contain abbreviations, words with capital letters, words with lowercase letters, and mixed words. The model is trained according to the principle: At the input, the name is given in upper case, at the output - in the &quot;correct&quot; version. For example:</p>
<pre><code>ОТДЕЛЕНИЕ СТД РФ (ВТО) - СТД РЕСПУБЛИКИ АДЫГЕЯ -&gt; Отделение СТД РФ (ВТО) - СТД Республики Адыгея

ОБЩЕСТВО С ОГРАНИЧЕННОЙ ОТВЕТСТВЕННОСТЬЮ &quot;СИГМА&quot; -&gt; Общество с ограниченной ответственностью &quot;СИГМА&quot;

&quot;ЭЛЕКТРОПРОФСОЮЗ&quot; РБ ОО - &quot;ВЭП&quot; -&gt; &quot;Электропрофсоюз&quot; РБ ОО - &quot;ВЭП&quot; 
</code></pre>
<p>And then I have to predict sentences that I don't know answers for.</p>
<p>I tried to make a character-based seq2seq model with attention and bidirectional GRU, but with all the hyperparameters I tried, it seems to be underfitted. It generates the beginning of phrases quite well, but at the end it breaks down.</p>
<p>Now I think I need to work with word tokens. But I do not know if there are methods for classifying words from a single text in a context.</p>
<p>I want to tokenize a sentence, and assign a property to each word, what it is: uppercase, lowercase, or with a capital first letter. Also I need to do something with mixed words like &quot;МосГосПаравоз&quot; (or like &quot;McDonald's&quot; which not only starts with a capital letter, but also contains it inside.).</p>
<p>Perhaps you need a completely different approach. I will be happy to accept your help</p>
<p><strong>UPD.</strong> English interpretation of examples:</p>
<pre><code>&quot;SIGMA&quot; LIMITED LIABILITY COMPANY -&gt; &quot;SIGMA&quot; Limited liability company
&quot;SIGMA&quot; LLT -&gt; &quot;SIGMA&quot; LLT
PJSC &quot;GAZPROM&quot;-&gt; PJSC &quot;Gazprom&quot;
STATE BUDGETARY EDUCATIONAL INSTITUTION OF THE CITY OF MOSCOW &quot;LYCEUM NO. 1568&quot; -&gt; State budgetary Educational institution of the city of Moscow &quot;Lyceum No. 1568&quot;
</code></pre>
",Preprocessing of the text & Tokenization,classify word sentence context name company russian name contain abbreviation word capital letter word lowercase letter mixed word model trained according principle input name given upper case output correct version example predict sentence know answer tried make character based seq seq model attention bidirectional gru hyperparameters tried seems underfitted generates beginning phrase quite well end break think need work word token know method classifying word single text context want tokenize sentence assign property word uppercase lowercase capital first letter also need something mixed word like like mcdonald start capital letter also contains inside perhaps need completely different approach happy accept help upd english interpretation example
MultinomialNB fails with &quot;ValueError: shapes not aligned&quot; during prediction phase,"<p>I am trying to do a MultinomialNB(). I have a csv, that I read into a dataframe (data) and did some tokenizing and lemmatization on the data in order to have the most used words. The code for the model is this:</p>
<pre><code>max_features = 5000
count_vectorizer = CountVectorizer(max_features=max_features , stop_words= &quot;english&quot;) 
sparce_matrix = count_vectorizer.fit_transform(Tweet_list).toarray()
y = data.iloc[:,0].values
x = sparce_matrix

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.1)

from sklearn.naive_bayes import MultinomialNB

Mn = MultinomialNB()
Mn.fit(x_train, y_train)
y_pred = Mn.predict(x_test)
print(&quot;Accuracy: &quot;, Mn.score(y_pred.reshape(-1,1),y_test))
</code></pre>
<p>When i print the sizes of the variables:</p>
<pre><code>print(y.size)
print(x.size)
print(x_train.size)
print(y_train.size)
print(x_test.size)
print(&quot;y test&quot;, y_test.size)
print(&quot;y pred&quot;, y_pred.size)
</code></pre>
<p>I get:</p>
<pre><code>86460
432300000
389070000
77814
43230000
y test 8646
y pred 8646
</code></pre>
<p>However the model fails with ValueError: shapes (8646,1) and (5000,2) not aligned: 1 (dim 1) != 5000 (dim 0).</p>
<p>As far as I understand the problem is somewhere in the computation it does behind the methods where some np.dot(a, b) fails. It somehow computes the y_pred or y_test (8646) with a vector of the size of max features vector (5000). That is the only place where the value 5000 appears.</p>
",Preprocessing of the text & Tokenization,multinomialnb fails valueerror shape aligned prediction phase trying multinomialnb csv read dataframe data tokenizing lemmatization data order used word code model print size variable get however model fails valueerror shape aligned dim dim far understand problem somewhere computation doe behind method np dot b fails somehow computes pred test vector size max feature vector place value appears
Letter Missing in PDF Text Extraction,"<p>I am beginner python user (Python 3.8.8 mac), and facing the problem of letter missing in the process of pdf to text conversion.</p>
<ul>
<li>My Issue:</li>
</ul>
<p>I tried to extract texts from pdfs and tokenise words in texts. However, some words are missing ending letters such as 'currenc', 'innov', 'sourc' (in the original documents, they are &quot;currency&quot; &quot;innovation&quot; and &quot;source&quot;</p>
<ul>
<li><p>My Attempts:
I tried pdftotext, pdfminer, pdfminer3, and textract (with language specification method='tesseract'), however none provide the accurate words list (the code is below).The issue is not specific to certain document, but for all documents text extraction.</p>
</li>
<li><p>Results Sample</p>
</li>
</ul>
<p>'achain',
'blockchain',
'whitepap',
'build',
'boundless',
'tabl',
'content',
'abstract',
'background',
'commit',
'histori',
'network',
'develop',
'need',
'blockchain',
'topic',
'unsolv',
'problem',
'need',
'achain',
'principl',
'stabil',
'safeti',
'scalabl',
'easi',
'use',
'implement',
'smart',
'contract',
'lvm',
'consensu',
'agreement',
'account',
'fork',
'network',
'valu',
'exchang',
'protocol',
'event',
'driven',
'applic',
'suppli',
'chain',
'financ',
'authent',
'develop',
'plan',
'plan',
'program',
'govern',
'achain',
'foundat',
'foundat',
'govern',
'contact',
'us',
'open',
'sourc',
'technic',
'team',
'abstract',</p>
<ul>
<li>Code</li>
</ul>
<pre><code>#template of extract text from pdf
from pdfminer3.layout import LAParams, LTTextBox
from pdfminer3.pdfpage import PDFPage
from pdfminer3.pdfinterp import PDFResourceManager
from pdfminer3.pdfinterp import PDFPageInterpreter
from pdfminer3.converter import PDFPageAggregator
from pdfminer3.converter import TextConverter
import io

resource_manager = PDFResourceManager()
fake_file_handle = io.StringIO()
converter = TextConverter(resource_manager, fake_file_handle)
page_interpreter = PDFPageInterpreter(resource_manager, converter)


def pdf_to_text(filepath):
    print('Getting text content for {}...'.format(filepath))
    process = subprocess.Popen(['pdf2txt.py', filepath], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    stdout, stderr = process.communicate()

    if process.returncode != 0 or stderr:
        raise OSError('Executing the command for {} caused an error:\nCode: {}\nOutput: {}\nError: {}'.format(filepath, process.returncode, stdout, stderr))

    return stdout.decode('utf-8')

#transform all pdf to csvs
if __name__ == &quot;__main__&quot;:
    csv_files = list() # a list of dataframes
    for file in pdf:
        csv_file = pdf_to_text(file)
        csv_files.append(csv_file)

#combine all documents(csvs) into one list
documents = []
for i in range(len(csv_files)):
    documents.append(csv_files[i])

# prepare the stopping words from nltk
nltk.download('stopwords')
stop = stopwords.words('english')

def mypreprocess(text,n,opt=1):
    # text: document  n: word with length &lt;= n will be ignored  opt: which stemer to use
    text = text.lower()
    # remove all the irrelevant numbers and punctuation
    text = re.sub(r'[^a-z]+',' ',text)
    # tokenize the words
    token = word_tokenize(text)
    # remove the meaningless stopping words
    token = [t for t in token if t not in stop]
    # stemming transformation
   
    if opt == 1:
        token = [porter.stem(t) for t in token]
    else:
        token = [lancaster.stem(t) for t in token]
    token = [x for x in token if len(x) &gt; n]
    return token

tokens = []
for i in range(len(documents)):
    token = mypreprocess(documents[i],1)   # remove irelavant words and stop words, and stem
    tokens.append(token)
    documents[i] = ' '.join(token)
tokens[:50]

</code></pre>
",Preprocessing of the text & Tokenization,letter missing pdf text extraction beginner python user python mac facing problem letter missing process pdf text conversion issue tried extract text pdfs tokenise word text however word missing ending letter currenc innov sourc original document currency innovation source attempt tried pdftotext pdfminer pdfminer textract language specification method tesseract however none provide accurate word list code issue specific certain document document text extraction result sample achain blockchain whitepap build boundless tabl content abstract background commit histori network develop need blockchain topic unsolv problem need achain principl stabil safeti scalabl easi use implement smart contract lvm consensu agreement account fork network valu exchang protocol event driven applic suppli chain financ authent develop plan plan program govern achain foundat foundat govern contact u open sourc technic team abstract code
Need to build entires sentences from an odd xml file splitted in tokens,"<p>I work on an NLP project with XSLT and i need to create sentences with the following XML doc :</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;cordial2xml&gt;
&lt;w&gt;&lt;forme&gt;la&lt;/forme&gt;&lt;lemme&gt;le&lt;/lemme&gt;&lt;categorie&gt;DETDFS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;grande&lt;/forme&gt;&lt;lemme&gt;grand&lt;/lemme&gt;&lt;categorie&gt;ADJFS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;douleur&lt;/forme&gt;&lt;lemme&gt;douleur&lt;/lemme&gt;&lt;categorie&gt;NCFS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;du&lt;/forme&gt;&lt;lemme&gt;du&lt;/lemme&gt;&lt;categorie&gt;DETDMS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;père&lt;/forme&gt;&lt;lemme&gt;père&lt;/lemme&gt;&lt;categorie&gt;NCMS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;duchesne&lt;/forme&gt;&lt;lemme&gt;duchesne&lt;/lemme&gt;&lt;categorie&gt;NCMS&lt;/categorie&gt;&lt;/w&gt;
&lt;w&gt;&lt;forme&gt;au sujet de&lt;/forme&gt;&lt;lemme&gt;au sujet de&lt;/lemme&gt;&lt;categorie&gt;PREP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;la&lt;/forme&gt;&lt;lemme&gt;le&lt;/lemme&gt;&lt;categorie&gt;DETDFS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;mort&lt;/forme&gt;&lt;lemme&gt;mort&lt;/lemme&gt;&lt;categorie&gt;NCFS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;de&lt;/forme&gt;&lt;lemme&gt;de&lt;/lemme&gt;&lt;categorie&gt;PREP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;marat&lt;/forme&gt;&lt;lemme&gt;marat&lt;/lemme&gt;&lt;categorie&gt;NPMS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;assassiné&lt;/forme&gt;&lt;lemme&gt;assassiner&lt;/lemme&gt;&lt;categorie&gt;VPARPMS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;à&lt;/forme&gt;&lt;lemme&gt;à&lt;/lemme&gt;&lt;categorie&gt;PREP&lt;/categorie&gt;&lt;/w&gt;
&lt;w&gt;&lt;forme&gt;coups&lt;/forme&gt;&lt;lemme&gt;coup&lt;/lemme&gt;&lt;categorie&gt;NCMP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;de&lt;/forme&gt;&lt;lemme&gt;de&lt;/lemme&gt;&lt;categorie&gt;PREP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;couteau&lt;/forme&gt;&lt;lemme&gt;couteau&lt;/lemme&gt;&lt;categorie&gt;NCMS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;par&lt;/forme&gt;&lt;lemme&gt;par&lt;/lemme&gt;&lt;categorie&gt;PREP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;une&lt;/forme&gt;&lt;lemme&gt;un&lt;/lemme&gt;&lt;categorie&gt;DETIFS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;garce&lt;/forme&gt;&lt;lemme&gt;gars&lt;/lemme&gt;&lt;categorie&gt;NCFS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;du&lt;/forme&gt;&lt;lemme&gt;du&lt;/lemme&gt;&lt;categorie&gt;DETDMS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;calvados&lt;/forme&gt;&lt;lemme&gt;calvados&lt;/lemme&gt;&lt;categorie&gt;NCMIN&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;,&lt;/forme&gt;&lt;lemme&gt;,&lt;/lemme&gt;&lt;categorie&gt;PCTFAIB&lt;/categorie&gt;&lt;/w&gt;
&lt;w&gt;&lt;forme&gt;dont&lt;/forme&gt;&lt;lemme&gt;dont&lt;/lemme&gt;&lt;categorie&gt;PRI&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;l'&lt;/forme&gt;&lt;lemme&gt;le&lt;/lemme&gt;&lt;categorie&gt;DETDMS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;évêque&lt;/forme&gt;&lt;lemme&gt;évêque&lt;/lemme&gt;&lt;categorie&gt;NCMS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;fauchet&lt;/forme&gt;&lt;lemme&gt;fauchet&lt;/lemme&gt;&lt;categorie&gt;NCMS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;était&lt;/forme&gt;&lt;lemme&gt;être&lt;/lemme&gt;&lt;categorie&gt;VINDI3S&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;le&lt;/forme&gt;&lt;lemme&gt;le&lt;/lemme&gt;&lt;categorie&gt;DETDMS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;directeur.ses&lt;/forme&gt;&lt;lemme&gt;directeur.ses&lt;/lemme&gt;&lt;categorie&gt;NCI&lt;/categorie&gt;&lt;/w&gt;
&lt;w&gt;&lt;forme&gt;bons&lt;/forme&gt;&lt;lemme&gt;bon&lt;/lemme&gt;&lt;categorie&gt;ADJMP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;avis&lt;/forme&gt;&lt;lemme&gt;avis&lt;/lemme&gt;&lt;categorie&gt;NCMIN&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;aux&lt;/forme&gt;&lt;lemme&gt;au&lt;/lemme&gt;&lt;categorie&gt;DETDPIG&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;braves&lt;/forme&gt;&lt;lemme&gt;brave&lt;/lemme&gt;&lt;categorie&gt;NCPIG&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;sans&lt;/forme&gt;&lt;lemme&gt;sans&lt;/lemme&gt;&lt;categorie&gt;PREP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;-culottes&lt;/forme&gt;&lt;lemme&gt;culotte&lt;/lemme&gt;&lt;categorie&gt;NCFP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;pour&lt;/forme&gt;&lt;lemme&gt;pour&lt;/lemme&gt;&lt;categorie&gt;PREP&lt;/categorie&gt;&lt;/w&gt;
&lt;w&gt;&lt;forme&gt;qu'&lt;/forme&gt;&lt;lemme&gt;que&lt;/lemme&gt;&lt;categorie&gt;SUB&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;ils&lt;/forme&gt;&lt;lemme&gt;il&lt;/lemme&gt;&lt;categorie&gt;PPER3P&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;se&lt;/forme&gt;&lt;lemme&gt;se&lt;/lemme&gt;&lt;categorie&gt;PPER3S&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;tiennent&lt;/forme&gt;&lt;lemme&gt;tenir&lt;/lemme&gt;&lt;categorie&gt;VINDP3P&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;sur&lt;/forme&gt;&lt;lemme&gt;sur&lt;/lemme&gt;&lt;categorie&gt;PREP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;leurs&lt;/forme&gt;&lt;lemme&gt;leur&lt;/lemme&gt;&lt;categorie&gt;DETPOSS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;gardes&lt;/forme&gt;&lt;lemme&gt;garde&lt;/lemme&gt;&lt;categorie&gt;NCFP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;,&lt;/forme&gt;&lt;lemme&gt;,&lt;/lemme&gt;&lt;categorie&gt;PCTFAIB&lt;/categorie&gt;&lt;/w&gt;
&lt;w&gt;&lt;forme&gt;attendu qu'&lt;/forme&gt;&lt;lemme&gt;attendu que&lt;/lemme&gt;&lt;categorie&gt;SUB&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;il&lt;/forme&gt;&lt;lemme&gt;il&lt;/lemme&gt;&lt;categorie&gt;PPER3S&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;y&lt;/forme&gt;&lt;lemme&gt;y&lt;/lemme&gt;&lt;categorie&gt;PPER3S&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;a&lt;/forme&gt;&lt;lemme&gt;avoir&lt;/lemme&gt;&lt;categorie&gt;VINDP3S&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;dans&lt;/forme&gt;&lt;lemme&gt;dans&lt;/lemme&gt;&lt;categorie&gt;PREP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;paris&lt;/forme&gt;&lt;lemme&gt;pari&lt;/lemme&gt;&lt;categorie&gt;NCMP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;plusieurs&lt;/forme&gt;&lt;lemme&gt;plusieurs&lt;/lemme&gt;&lt;categorie&gt;ADJIND&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;milliers&lt;/forme&gt;&lt;lemme&gt;millier&lt;/lemme&gt;&lt;categorie&gt;NCMP&lt;/categorie&gt;&lt;/w&gt;
&lt;w&gt;&lt;forme&gt;de&lt;/forme&gt;&lt;lemme&gt;de&lt;/lemme&gt;&lt;categorie&gt;PREP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;tondus&lt;/forme&gt;&lt;lemme&gt;tondu&lt;/lemme&gt;&lt;categorie&gt;ADJMP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;de&lt;/forme&gt;&lt;lemme&gt;de&lt;/lemme&gt;&lt;categorie&gt;PREP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;la&lt;/forme&gt;&lt;lemme&gt;le&lt;/lemme&gt;&lt;categorie&gt;DETDFS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;vendée&lt;/forme&gt;&lt;lemme&gt;vendée&lt;/lemme&gt;&lt;categorie&gt;NPFS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;qui&lt;/forme&gt;&lt;lemme&gt;qui&lt;/lemme&gt;&lt;categorie&gt;PRI&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;ont&lt;/forme&gt;&lt;lemme&gt;avoir&lt;/lemme&gt;&lt;categorie&gt;VINDP3P&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;la&lt;/forme&gt;&lt;lemme&gt;le&lt;/lemme&gt;&lt;categorie&gt;DETDFS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;patte&lt;/forme&gt;&lt;lemme&gt;patte&lt;/lemme&gt;&lt;categorie&gt;NCFS&lt;/categorie&gt;&lt;/w&gt;
&lt;w&gt;&lt;forme&gt;graissée&lt;/forme&gt;&lt;lemme&gt;graisser&lt;/lemme&gt;&lt;categorie&gt;VPARPFS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;pour&lt;/forme&gt;&lt;lemme&gt;pour&lt;/lemme&gt;&lt;categorie&gt;PREP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;égorger&lt;/forme&gt;&lt;lemme&gt;égorger&lt;/lemme&gt;&lt;categorie&gt;VINF&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;tous&lt;/forme&gt;&lt;lemme&gt;tout&lt;/lemme&gt;&lt;categorie&gt;ADJMP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;les&lt;/forme&gt;&lt;lemme&gt;le&lt;/lemme&gt;&lt;categorie&gt;DETDPIG&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;bons&lt;/forme&gt;&lt;lemme&gt;bon&lt;/lemme&gt;&lt;categorie&gt;ADJMP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;citoyens&lt;/forme&gt;&lt;lemme&gt;citoyen&lt;/lemme&gt;&lt;categorie&gt;NCMP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;.&lt;/forme&gt;&lt;lemme&gt;.&lt;/lemme&gt;&lt;categorie&gt;PCTFORTE&lt;/categorie&gt;&lt;/w&gt;
&lt;w&gt;&lt;forme&gt;marat&lt;/forme&gt;&lt;lemme&gt;marat&lt;/lemme&gt;&lt;categorie&gt;NPMS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;n'&lt;/forme&gt;&lt;lemme&gt;ne&lt;/lemme&gt;&lt;categorie&gt;ADV&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;est&lt;/forme&gt;&lt;lemme&gt;être&lt;/lemme&gt;&lt;categorie&gt;VINDP3S&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;plus&lt;/forme&gt;&lt;lemme&gt;plus&lt;/lemme&gt;&lt;categorie&gt;ADV&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;,&lt;/forme&gt;&lt;lemme&gt;,&lt;/lemme&gt;&lt;categorie&gt;PCTFAIB&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;foutre.peuple&lt;/forme&gt;&lt;lemme&gt;foutre.peuple&lt;/lemme&gt;&lt;categorie&gt;NCI&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;,&lt;/forme&gt;&lt;lemme&gt;,&lt;/lemme&gt;&lt;categorie&gt;PCTFAIB&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;gémis&lt;/forme&gt;&lt;lemme&gt;gémir&lt;/lemme&gt;&lt;categorie&gt;VPARPMP&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;,&lt;/forme&gt;&lt;lemme&gt;,&lt;/lemme&gt;&lt;categorie&gt;PCTFAIB&lt;/categorie&gt;&lt;/w&gt;
&lt;w&gt;&lt;forme&gt;pleure&lt;/forme&gt;&lt;lemme&gt;pleurer&lt;/lemme&gt;&lt;categorie&gt;VIMPP2S&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;ton&lt;/forme&gt;&lt;lemme&gt;ton&lt;/lemme&gt;&lt;categorie&gt;DETPOSS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;meilleur&lt;/forme&gt;&lt;lemme&gt;meilleur&lt;/lemme&gt;&lt;categorie&gt;ADJMS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;ami&lt;/forme&gt;&lt;lemme&gt;ami&lt;/lemme&gt;&lt;categorie&gt;NCMS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;;&lt;/forme&gt;&lt;lemme&gt;;&lt;/lemme&gt;&lt;categorie&gt;PCTFORTE&lt;/categorie&gt;&lt;/w&gt;
&lt;w&gt;&lt;forme&gt;il&lt;/forme&gt;&lt;lemme&gt;il&lt;/lemme&gt;&lt;categorie&gt;PPER3S&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;meurt&lt;/forme&gt;&lt;lemme&gt;mourir&lt;/lemme&gt;&lt;categorie&gt;VINDP3S&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;martyr&lt;/forme&gt;&lt;lemme&gt;martyr&lt;/lemme&gt;&lt;categorie&gt;ADJMS&lt;/categorie&gt;&lt;/w&gt;&lt;w&gt;&lt;forme&gt;de&lt;/forme&gt;&lt;lemme&gt;de&lt;/lemme&gt;&lt;categorie&gt;PREP&lt;/categorie&gt;&lt;/w&gt;
&lt;/cordial2xml&gt;
</code></pre>
<p>It's just a piece of this XML doc. From it, i need to construct sentences so i need to concatenate the forme nodes with space between and stop until a word where there is a point between (end of sentence) or only '.' For the last word cutted in 2 by the point, i need to keep the first one sustring before point on current sentence and the sustring after will be the begenning of a new sentence.
The problem is that, i don't know how to do that. I don't know if i have the create a template matching //w but i can't figure out with only word at the time so i need all the nodes forme until a node which frome contains '.' keeping the first substring forme which contains '.' in the current sentence and the sustring after '.' is the first word of a new sentence
Actually, for the first 2 sentences, i need the desired output :</p>
<pre><code>&lt;p&gt;la grande douleur du pere duchesne au sujet de la mort de marat à coups de couteau par un gard du calvados, l'eveque fauchet etait le directeur.&lt;/p&gt;

&lt;p&gt;bons avis au brave sans culottes pour que se tiennent sur leurs gardes, attendu qu'il a dans paris plusieurs milliers de tondus de la vendée qui ont la patte grasser pour egorger tous les bons citoyens.&lt;p&gt;
etc...
</code></pre>
<p>At the begenning, i would like to apply a recursive template to create each sentence but how to keep the following word cutted but the point in the next sentence.
I could also use a template matching //w but i don't know how to deal with the p in xslt code tags
I could captured all nodes between the current one and the node where the forme contains a '.' like this w[1]/following-sibling::*[not(contains(./forme, '.'))][1] (grouping node between the first w and the last w with forme contains '.' and recursively. The absolute first forme is the begenning of the first sentence.
Do you have any idea to achieve my task ? I'm lost. I don't want necessary a code but a method.
If you have pieces of code, it's better for sure.
Thx a lot.</p>
",Preprocessing of the text & Tokenization,need build entire sentence odd xml file splitted token work nlp project xslt need create sentence following xml doc piece xml doc need construct sentence need concatenate forme node space stop word point end sentence last word cutted point need keep first one sustring point current sentence sustring begenning new sentence problem know know create template matching w figure word time need node forme node frome contains keeping first substring forme contains current sentence sustring first word new sentence actually first sentence need desired output begenning would like apply recursive template create sentence keep following word cutted point next sentence could also use template matching w know deal p xslt code tag could captured node current one node forme contains like w following sibling contains forme grouping node first w last w forme contains recursively absolute first forme begenning first sentence idea achieve task lost want necessary code method piece code better sure thx lot
Define category of a word,"<p>I have a set of 20'000 words and simple phrases. I need to pick each word and define it's general concept, or category.</p>
<p>So if I take &quot;hockey&quot; it should fall into a large &quot;Sports&quot; category. If it's &quot;Barack Obama&quot; then it's &quot;Politics&quot;. Here is a sample from my word list:</p>
<pre><code>israel
illness
face
experts
throat
tory
moments
numerous
</code></pre>
<p>All the weird stuff can fall into &quot;General&quot; category.</p>
<p>That's my problem. Following are my thoughts that you could probably ignore, cause I have no good clue how to deal with the problem.</p>
<p>Probably I am looking for some kind of opened dictionary or API that can define a general concept of a word. I was thinking to take a simple dictionary and run every word through it parsing it's <em>Economics</em> categories. But not all words have it.</p>
",Preprocessing of the text & Tokenization,define category word set word simple phrase need pick word define general concept category take hockey fall large sport category barack obama politics sample word list weird stuff fall general category problem following thought could probably ignore cause good clue deal problem probably looking kind opened dictionary api define general concept word wa thinking take simple dictionary run every word parsing economics category word
How to remove stopwords from dataframe with tokenized data?,"<p>I am trying to remove stopwords from dataframe.
Each row has only one column named text where I stored all the paragraphs of article.</p>
<p>This is the very first method I tried</p>
<pre><code>stopwords  = ['cat', 'dog', 'lion', 'fox']
df['text'] = df['text'].apply(lambda x: str.split(x))
df['text'] = df['text'].apply(lambda x: [item for item in x if item.lower() not in stop_words])

x=0

for i in df['text']:
    df['text'][x] = ' '.join(i)
    x += 1
    
df
</code></pre>
<p>Strangely, this did not remove all the words in stopwords from <code>df['text']</code>.
I couldn't figure out why so I moved on to tokenization. After tokenization, each paragraphs were divided and formed columns.</p>
<p>From this dataframe where some rows have more than 50,000 columns, how can I remove words in stopwords?</p>
<p>Thank you</p>
",Preprocessing of the text & Tokenization,remove stopwords dataframe tokenized data trying remove stopwords dataframe row ha one column named text stored paragraph article first method tried strangely remove word stopwords figure moved tokenization tokenization paragraph divided formed column dataframe row column remove word stopwords thank
Unable to import a column from csv into NLTK using python,"<p>I am unable to tokenize data from csvfile into nltk</p>
<p>this is my code</p>
<pre><code>import nltk
import csv
import numpy

from nltk import sent_tokenize
from nltk import word_tokenize
from nltk import pos_tag

reader = csv.reader(open('/Users/yoshithKotla/Desktop/dingdang/tweets.csv', 'rU'), delimiter= &quot;,&quot;,quotechar='|')
tokenData = nltk.word_tokenize(reader)
</code></pre>
",Preprocessing of the text & Tokenization,unable import column csv nltk using python unable tokenize data csvfile nltk code
Python Text Cleaning,"<p>I am working on cleansing some text data and have one function that cleans out any non-english/jibberish words. It does a good job, however, there are a few words that are product names that are not recognized as real words so they get eliminated. I am trying to come up with a way to keep certain words in the text</p>
<p>Here is the code I have so far:</p>
<pre><code>    def clean_non_eng(text):
       words = set(nltk.corpus.words.words())
       text = &quot; &quot;.join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not 
       w.isalpha())
       return text
</code></pre>
<p>What I am thinking is having some kind of list containing words to keep, and incorporating this into my function to avoid eliminating them</p>
<pre><code>    words_to_keep = ('wordtokeep1', 'wordtokeep2', 'wordtokeep3')
</code></pre>
<p>Is there a way I can incorporate another 'or' statement like &quot;or not in words_to_keep&quot; ? I have tried a few different ways but have not been successful so far</p>
<p>as of now, if I call the function it will look something like this</p>
<p>clean_non_eng('hello, this is a test of wordtokeep')</p>
<p>it will return: 'hello, this is a test of'</p>
",Preprocessing of the text & Tokenization,python text cleaning working cleansing text data one function clean non english jibberish word doe good job however word product name recognized real word get eliminated trying come way keep certain word text code far thinking kind list containing word keep incorporating function avoid eliminating way incorporate another statement like word keep tried different way successful far call function look something like clean non eng hello test wordtokeep return hello test
Removing stopwords from a string with ordered set and join retains a single stopword,"<p>I don't understand why I don't remove the stopword &quot;a&quot; in this loop. It seems so obvious that this should work...</p>
<p>Given a list of stop words, write a function that takes a string and returns a string stripped of the stop words. Output: stripped_paragraph = 'want figure out how can better data scientist'</p>
<ol>
<li><p>Below I define 'stopwords'</p>
</li>
<li><p>I split all the words by a space, make a set of words while retaining the order</p>
</li>
<li><p>loop through the ordered and split substring set ('osss' var) and conditionally remove each word if it's a word in the list 'stopwords'</p>
<pre><code>paragraph = 'I want to figure out how I can be a better data scientist'
def rm_stopwards(par):
    stopwords = ['I', 'as', 'to', 'you', 'your','but','be', 'a']
    osss = list(list(dict.fromkeys(par.split(' ')))) # ordered_split_shortened_set
    for word in osss:
        if word.strip() in stopwords:
            osss.remove(word)
        else:
            next
    return ' '.join(osss)
print(&quot;stripped_paragraph = &quot;+&quot;'&quot;+(rm_stopwards(paragraph))+&quot;'&quot;)
</code></pre>
</li>
</ol>
<p>My <strong>incorrect</strong> output is: 'want figure out how can a better data scientist'</p>
<p><strong>Correct output</strong>: 'want figure out how can better data scientist'</p>
<p>edit: note that .strip() in the condition check with word.strip() is unnecessary and I still get the same output, that was me checking to make sure there wasn't an extra space somehow</p>
<p>edit2: this is an interview question, so I can't use any imports</p>
",Preprocessing of the text & Tokenization,removing stopwords string ordered set join retains single stopword understand remove stopword loop seems obvious work given list stop word write function take string return string stripped stop word output stripped paragraph want figure better data scientist define stopwords split word space make set word retaining order loop ordered split substring set os var conditionally remove word word list stopwords incorrect output want figure better data scientist correct output want figure better data scientist edit note strip condition check word strip unnecessary still get output wa checking make sure extra space somehow edit interview question use import
Build vocab using spacy,"<p>I'm using spacy tokenizer to tokenize my data, and then build vocab.</p>
<p>This is my code:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_sm&quot;)

def build_vocab(docs, max_vocab=10000, min_freq=3):
 stoi = {'&lt;PAD&gt;':0, '&lt;UNK&gt;':1}
 itos = {0:'&lt;PAD&gt;', 1:'&lt;UNK&gt;'}
 word_freq = {}
 idx = 2
 for sentence in docs:
  for word in [i.text.lower() for i in nlp(sentence)]:
   
   if word not in word_freq:
    word_freq[word] = 1
   else:
    word_freq[word] += 1

   if word_freq[word] == min_freq:
    if len(stoi) &lt; max_vocab:
     stoi[word] = idx
     itos[idx] = word
     idx += 1
 return stoi, itos
</code></pre>
<p>But it takes hours to complete since I have more than 800000 sentences.</p>
<p>Is there a faster and better way to achieve this? Thanks.</p>
<p>update: tried to remove min_freq:</p>
<pre><code>def build_vocab(docs, max_vocab=10000):
  stoi = {'&lt;PAD&gt;':0, '&lt;UNK&gt;':1}
  itos = {0:'&lt;PAD&gt;', 1:'&lt;UNK&gt;'}
  idx = 2
  for sentence in docs:
    for word in [i.text.lower() for i in nlp(sentence)]:
      if word not in stoi:
        if len(stoi) &lt; max_vocab:
          stoi[word] = idx
          itos[idx] = word
          idx += 1
  return stoi, itos
</code></pre>
<p>still takes a long time, does spacy have a function to build vocab like in torchtext (.build_vocab).</p>
",Preprocessing of the text & Tokenization,build vocab using spacy using spacy tokenizer tokenize data build vocab code take hour complete since sentence faster better way achieve thanks update tried remove min freq still take long time doe spacy function build vocab like torchtext build vocab
spacy lemmatization of nouns and noun chunks,"<p>I am trying to create a corpus of documents which consists of lemmatized nouns and noun-chunks. I am using this code:</p>
<pre><code>import spacy
nlp = spacy.load('en_core_web_sm')

def lemmatizer(doc, allowed_postags=['NOUN']):                                                     
    doc = [token.lemma_ for token in doc if token.pos_ in allowed_postags]
    doc = u' '.join(doc)
    return nlp.make_doc(doc)


nlp.add_pipe(nlp.create_pipe('merge_noun_chunks'), after='ner')
nlp.add_pipe(lemmatizer, name='lemm', after='merge_noun_chunks')

doc_list = []                                                                                      
for doc in data:                                                                                    
    pr = nlp(doc)
    doc_list.append(pr) 

   
</code></pre>
<p>The sentence <code>'the euro area has advanced a long way as a monetary union'</code> after identifiying noun-chunks <code>['the euro area', 'advanced', 'long', 'way', 'a monetary union']</code> and lemmatization gets to: <code>['euro', 'area', 'way', 'monetary', 'union']</code>.
Is there a way to combine the words of the identified noun-chunks to get an output like this: <code>['the euro area','way', 'a monetary union']</code> or <code>['the_euro_area','way', 'a_monetary_union']</code>?</p>
<p>Thanks for your help!</p>
",Preprocessing of the text & Tokenization,spacy lemmatization noun noun chunk trying create corpus document consists lemmatized noun noun chunk using code sentence identifiying noun chunk lemmatization get way combine word identified noun chunk get output like thanks help
most frequent words in list which contains lists,"<p>I have a dataset of a list which contains other lists and I want to find the top 1000 words</p>
<p>I tried this, but it doesn't work:</p>
<p>from collections import Counter
counts_top1000 = [word for word, word_count in Counter(mainlist).most_common(1000)]</p>
<p>Note that my dataset is 'mainlist'.</p>
<p>I would be grateful if you have any more ideas.</p>
",Preprocessing of the text & Tokenization,frequent word list contains list dataset list contains list want find top word tried work collection import counter count top word word word count counter mainlist common note dataset mainlist would grateful idea
How to get similarity (PMI) score between a keyword and paragraphs using python?,"<p>I'm working on a project which extracts keywords from customer reviews. I somehow managed to extract the keywords using a topic modelling technique.</p>
<p>Now I'm looking for a technique or algorithm in python to rank the reviews based on similarity between the keyword.</p>
<p>for example:
for the keyword <strong>'delicious food'</strong> I would like to get the similarity score for reviews as below.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>review</th>
<th>score</th>
</tr>
</thead>
<tbody>
<tr>
<td>this is place is costly but their food is delicious</td>
<td>0.7</td>
</tr>
<tr>
<td>I would not recommend this place for hangout.</td>
<td>0.0</td>
</tr>
<tr>
<td>This is place is very clean and friendly, perhaps, food is not so great!</td>
<td>0.2</td>
</tr>
</tbody>
</table>
</div>
<p>How can I get the semantic similarity score between a keyword and sentence?</p>
",Preprocessing of the text & Tokenization,get similarity pmi score keyword paragraph using python working project extract keywords customer review somehow managed extract keywords using topic modelling technique looking technique algorithm python rank review based similarity keyword example keyword delicious food would like get similarity score review review score place costly food delicious would recommend place hangout place clean friendly perhaps food great get semantic similarity score keyword sentence
Tokenizing &amp; encoding dataset uses too much RAM,"<p>Trying to tokenize and encode data to feed to a neural network.</p>
<p>I only have 25GB RAM and everytime I try to run the code below my google colab crashes. Any idea how to prevent his from happening? “Your session crashed after using all available RAM”</p>
<p>I thought tokenize/encoding chunks of 50000 sentences would work but unfortunately not.
The code works on a dataset with length 1.3 million. The current dataset has a length of  5 million.</p>
<pre><code>max_q_len = 128
max_a_len = 64    
trainq_list = train_q.tolist()    
batch_size = 50000
    
def batch_encode(text, max_seq_len):
      for i in range(0, len(trainq_list), batch_size):
        encoded_sent = tokenizer.batch_encode_plus(
            text,
            max_length = max_seq_len,
            pad_to_max_length=True,
            truncation=True,
            return_token_type_ids=False
        )
      return encoded_sent

    # tokenize and encode sequences in the training set
    tokensq_train = batch_encode(trainq_list, max_q_len)
</code></pre>
<p>The tokenizer comes from HuggingFace:</p>
<pre><code>tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')
</code></pre>
",Preprocessing of the text & Tokenization,tokenizing encoding dataset us much ram trying tokenize encode data feed neural network gb ram everytime try run code google colab crash idea prevent happening session crashed using available ram thought tokenize encoding chunk sentence would work unfortunately code work dataset length million current dataset ha length million tokenizer come huggingface
Updating spacy.tokenizer._get_regex_pattern(nlp.tokenizer.token_match) so that hashtags are tokenized as a single token,"<p>This is my first time using spacy and I am trying to learn how to edit the tokenizer on one of the pretrained models (en_core_web_md) so that when tweets are tokenized, the entire hashtag becomes a single token (e.g. I want one token '#hashtagText', the default would be two tokens, '#' and 'hashtagText').</p>
<p>I know I am not the first person that has faced this issue. I have tried implementing the advice other places online but after using their methods the output remains the same (#hashtagText is two tokens). These articles show the methods I have tried.</p>
<p><a href=""https://the-fintech-guy.medium.com/spacy-handling-of-hashtags-and-dollartags-ed1e661f203c"" rel=""nofollow noreferrer"">https://the-fintech-guy.medium.com/spacy-handling-of-hashtags-and-dollartags-ed1e661f203c</a></p>
<p><a href=""https://towardsdatascience.com/pre-processing-should-extract-context-specific-features-4d01f6669a7e"" rel=""nofollow noreferrer"">https://towardsdatascience.com/pre-processing-should-extract-context-specific-features-4d01f6669a7e</a></p>
<hr />
<p>Shown in the code below, my troubleshooting steps have been:</p>
<ol>
<li>save the default pattern matching regex (default_token_matching_regex)</li>
<li>save the regex that nlp (the pretrained model) is using before any updates (nlp_token_matching_regex_pre_update)</li>
</ol>
<p>Note: I originally suspected these would be the same, but they are not. See below for outputs.</p>
<ol start=""3"">
<li><p>Append the regex I need (#\w+) to the list that nlp is current using, save this combination as updated_token_matching_regex</p>
</li>
<li><p>Update the regex nlp is using with the variable created above (updated_token_matching_regex)</p>
</li>
<li><p>Save the new regex used by nlp to verify things were updated correctly (nlp_token_matching_regex_post_update).</p>
</li>
</ol>
<p>See code below:</p>
<pre><code>import spacy
import en_core_web_md
import re

nlp = en_core_web_md.load()

# Spacys default token matching regex.
default_token_matching_regex = spacy.tokenizer._get_regex_pattern(nlp.Defaults.token_match)

# Verify what regex nlp is using before changing anything.
nlp_token_matching_regex_pre_update = spacy.tokenizer._get_regex_pattern(nlp.tokenizer.token_match)

# Create a new regex that combines the default regex and a term to treat hashtags as a single token. 
updated_token_matching_regex = f&quot;({nlp_token_matching_regex_pre_update}|#\w+)&quot;

# Update the token matching regex used by nlp with the regex created in the line above.
nlp.tokenizer.token_match = re.compile(updated_token_matching_regex).match

# Verify that nlp is now using the updated regex.
nlp_token_matching_regex_post_update = spacy.tokenizer._get_regex_pattern(nlp.tokenizer.token_match)

# Now let's try again
s = &quot;2020 can't get any worse #ihate2020 @bestfriend &lt;https://t.co&gt;&quot;
doc = nlp(s)

# Let's look at the lemmas and is stopword of each token
print(f&quot;Token\t\tLemma\t\tStopword&quot;)
print(&quot;=&quot;*40)
for token in doc:
    print(f&quot;{token}\t\t{token.lemma_}\t\t{token.is_stop}&quot;)
</code></pre>
<p><a href=""https://i.sstatic.net/ul01t.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ul01t.png"" alt=""Output of print statements:"" /></a></p>
<p>As you can see above, the tokenization behavior is not as it should be with the addition of '#\w+'. See below for printouts of all the troubleshooting variables.</p>
<p><a href=""https://i.sstatic.net/bN981.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bN981.png"" alt=""default_token_matching_regex - Default regex, not used by my nlp"" /></a></p>
<p><a href=""https://i.sstatic.net/Gqmw6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Gqmw6.png"" alt=""nlp_token_matching_regex_pre_update - Regex used by my nlp before I change anything."" /></a></p>
<p><a href=""https://i.sstatic.net/MqYID.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MqYID.png"" alt=""updated_token_matching_regex - Shows the regex I will use to overwrite nlps regex with"" /></a></p>
<p><a href=""https://i.sstatic.net/3SpqO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3SpqO.png"" alt=""nlp_token_matching_regex_post_update - shows the regex nlp uses was successfully updated."" /></a></p>
<p>Since I feel like I have proven to myself above that I did correctly update the regex nlp is using, the only possible issue I could think of is that the regex itself was wrong. I tested the regex by itself and it seems to behave as intended, see below:</p>
<p><a href=""https://i.sstatic.net/PUxcq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PUxcq.png"" alt=""testing regex to pull out #hashTagText as a single token."" /></a></p>
<p>Is anyone able to see the error that is causing nlp to tokenize #hashTagText as two tokens after its nlp.tokenizer.token_match regex was updated to do it as a single token?</p>
<p>Thank you!!</p>
",Preprocessing of the text & Tokenization,updating spacy tokenizer get regex pattern nlp tokenizer token match hashtags tokenized single token first time using spacy trying learn edit tokenizer one pretrained model en core web md tweet tokenized entire hashtag becomes single token e g want one token hashtagtext default would two token hashtagtext know first person ha faced issue tried implementing advice place online using method output remains hashtagtext two token article show method tried shown code troubleshooting step save default pattern matching regex default token matching regex save regex nlp pretrained model using update nlp token matching regex pre update note originally would see output append regex need w list nlp current using save combination updated token matching regex update regex nlp using variable created updated token matching regex save new regex used nlp verify thing updated correctly nlp token matching regex post update see code see tokenization behavior addition w see printout troubleshooting variable since feel like proven correctly update regex nlp using possible issue could think regex wa wrong tested regex seems behave intended see anyone able see error causing nlp tokenize hashtagtext two token nlp tokenizer token match regex wa updated single token thank
Get Indian postal codes from address string with other numbers,"<p>I am working on an Address parsing project where, I need to detect various components of the address, such as city, state, postal_code, street_no etc.</p>

<p>I wrote a regular expression to filter out the postal codes handling all user inputs.</p>

<pre><code>sample_add = ""16th main road btm layout 560029 5-6-00-76 56 00 78 560-029 25 -000-1""
regexp = re.compile(r""([\d])[ -]*?([\d])[ -]*?([\d])[ -]*?([\d])[ -]*?([\d])[ -]*?([\d])"")
print(re.findall(regexp, sample_add))

Output :- [560029, 560076, 560078, 560029, 250001]
</code></pre>

<p>This is able to identify postal_codes for such addresses, However, when an address like the following comes, it combines the Street nos and interprets it as the postal code, </p>

<pre><code>Ex. `sample_add_2 = ""House no 323/46 16th main road, btm layout, bengaluru 560029""
</code></pre>

<p><strong>In this case, the postal code is identified as 323461, while the correct one should have been 560029</strong>.</p>
",Preprocessing of the text & Tokenization,get indian postal code address string number working address parsing project need detect various component address city state postal code street etc wrote regular expression filter postal code handling user input able identify postal code address however address like following come combine street interprets postal code case postal code identified correct one
Prevent Spacy tokenizer from splitting on specific character,"<p>When using spacy to tokenize a sentence, I want it to not split into tokens on <code>/</code></p>
<p>Example:</p>
<pre><code>import en_core_web_lg
nlp = en_core_web_lg.load()
for i in nlp(&quot;Get 10ct/liter off when using our App&quot;):
    print(i)
</code></pre>
<p>Output:</p>
<pre><code>Get
10ct
/
liter
off
when
using
our
App
</code></pre>
<p>I want it to be like <code>Get , 10ct/liter, off, when ....</code></p>
<p>I was able to find how to add more ways to split into tokens for spacy, but not how to avoid specific splitting techniques.</p>
",Preprocessing of the text & Tokenization,prevent spacy tokenizer splitting specific character using spacy tokenize sentence want split token example output want like wa able find add way split token spacy avoid specific splitting technique
How to identify multi-word expression in a sentence without a predefined multi-word list,"<p>I am trying to identify every multi-word expression in a sentence and tokenize that sentence. For instance, the example input sentence is &quot;In short, this merchandise is in short supply.&quot; and I wish the output could be shown as below:</p>
<pre><code>['In short', ',', 'this', 'merchandise', 'is', 'in short supply', '.']
</code></pre>
<p>I have already achieved the aforesaid result by using a predefined list and the following python code.</p>
<pre><code>from nltk import sent_tokenize, word_tokenize
from nltk.tokenize import MWETokenizer

multiwordExpressionList = [(&quot;In&quot;, &quot;short&quot;), (&quot;in&quot;, &quot;short&quot;, &quot;supply&quot; )]  ## this is a predefined list 
sentence = &quot;In short, this merchandise is in short supply.&quot;

mwe = MWETokenizer(multiwordExpressionList, separator = ' ')
resultList = mwe.tokenize(word_tokenize(sentence))
print(resultList)
</code></pre>
<p>However, the drawback is quite clear. This program need a predefined multi-word expression list to identify whether any multi-word expression exist in a sentence. Is there any suggested python package, module or method can identify any multi-word expression exist in a sentence ?</p>
",Preprocessing of the text & Tokenization,identify multi word expression sentence without predefined multi word list trying identify every multi word expression sentence tokenize sentence instance example input sentence short merchandise short supply wish output could shown already achieved aforesaid result using predefined list following python code however drawback quite clear program need predefined multi word expression list identify whether multi word expression exist sentence suggested python package module method identify multi word expression exist sentence
Glove Model Taking lots of time to give n similar words,"<p>I have a list of token and i am trying to find the top 10 similar word for each token but Glove model is taking a lot of time to return similar word.My code is:</p>
<pre><code>class GloveModel:
    def __init__(self):
        self.embedding_dict={}
        with open(&quot;glove.6B.50d&quot;, 'r', encoding=&quot;utf-8&quot;) as f:
            for line in f:
                values = line.split()
                word = values[0]
                vector = np.asarray(values[1:], &quot;float32&quot;)
                self.embedding_dict[word] = vector
                
    def find_closest_embeddings(self,token):
        
        return sorted(self.embedding_dict.keys(), key=lambda word: spatial.distance.euclidean(self.embedding_dict[word],token))


    def analyze(self,final_token):
        summary_txt_token=[]
        result=[]
        for i in final_token:
            if i in self.embedding_dict:
                temp=self.find_closest_embeddings(self.embedding_dict[i])[0:10] ##TOp ten similar word from the token
                summary_txt_token.extend(temp)
       
        summary_txt_token=list(set(summary_txt_token))
        return summary_txt_token
summary_obj=GloveModel()
final_token=summary_obj.analyze(final_token)
</code></pre>
",Preprocessing of the text & Tokenization,glove model taking lot time give n similar word list token trying find top similar word token glove model taking lot time return similar word code
using spacy how do I make a pattern for splitting words having dash within itself,"<p>I'm trying to split words like 'olive-oil','high-fat','all-purpose' which are tokenized into one chunk.</p>
<p>The desired tokenization has to be</p>
<p>['olive','-','oil','high','-','fat','all','-','purpose']</p>
<p>I looked into retokenizer and the usage was like below.</p>
<pre><code>doc = nlp(&quot;I live in NewYork&quot;)
with doc.retokenize() as retokenizer:
    heads = [(doc[3], 1), doc[2]]
    attrs = {&quot;POS&quot;: [&quot;PROPN&quot;, &quot;PROPN&quot;],
             &quot;DEP&quot;: [&quot;pobj&quot;, &quot;compound&quot;]}
    retokenizer.split(doc[3], [&quot;New&quot;, &quot;York&quot;], heads=heads, attrs=attrs)
</code></pre>
<p>As you could see in the last line,
to retokenize a chunk into pieces, I have provide what the result would be.
I don't think this is an efficient way of processing words because if I have to provide all the result, it means I just manually type all the possibilities, which I don't think feasible plan.</p>
<p>Given that I know all the cases and provide the ending result one by one
it might be way more efficient that just find the words to be replaced and replace them into what I want manually.</p>
<p>I believe there must be a way to generalize them.</p>
<p>If anyone knows to the way to tokenize the words I put at the top, can you help me?</p>
<p>Thank you</p>
",Preprocessing of the text & Tokenization,using spacy make pattern splitting word dash within trying split word like olive oil high fat purpose tokenized one chunk desired tokenization ha olive oil high fat purpose looked retokenizer usage wa like could see last line retokenize chunk piece provide result would think efficient way processing word provide result mean manually type possibility think feasible plan given know case provide ending result one one might way efficient find word replaced replace want manually believe must way generalize anyone know way tokenize word put top help thank
How can I extract numbers based on context of the sentence in python?,"<p>I tried using regular expressions but it doesn't do it with any context</p>
<p>Examples::
&quot;250 kg Oranges for Sale&quot;
&quot;I want to sell 100kg of Onions at 100 per kg&quot;</p>
",Preprocessing of the text & Tokenization,extract number based context sentence python tried using regular expression context example kg orange sale want sell kg onion per kg
Spacy - custom stop words are not working,"<p>I am trying to add custom STOP_WORDS to spacy.
The following code shall add the custom STOP_WORD ""Bestellung"" to the standard set of STOP_WORDS.
The problem I have is, that the adding works,i.e. the set contains ""Bestellung"" after adding it but when testing the custom stopword ""Bestellung"" with .is_stop, python returns FALSE. </p>

<p>Another test with an default STOP_WORD (i.e. it is standard in STOP_WORDS) ""darunter"" returns TRUE. I dont get it, beacause both words ""Bestellung"" and ""darunter"" are in the same set of STOP_WORDS. </p>

<p>Does anyone have an idea why it behaves like that?</p>

<p>Thank you </p>

<pre><code>import spacy
from spacy.lang.de.stop_words import STOP_WORDS

STOP_WORDS.add(""Bestellung"")
print(STOP_WORDS) #Printing STOP_WORDS proofs, that ""Bestellung"" is part of the Set ""STOP_WORDS"". Both tested words ""darunter"" and ""Bestellung"" are part of it.
nlp=spacy.load(""de_core_news_sm"")
print(nlp.vocab[""Bestellung""].is_stop) # return: FALSE
print(nlp.vocab[""darunter""].is_stop) # return: TRUE
</code></pre>

<p>Thank you</p>
",Preprocessing of the text & Tokenization,spacy custom stop word working trying add custom stop word spacy following code shall add custom stop word bestellung standard set stop word problem adding work e set contains bestellung adding testing custom stopword bestellung stop python return false another test default stop word e standard stop word darunter return true dont get beacause word bestellung darunter set stop word doe anyone idea behaves like thank thank
Customize spacy stop words and save the model,"<p>I am using this to add stopwords to the spacy's list of stopwords</p>
<p><code>nlp.Defaults.stop_words |= {&quot;my_new_stopword1&quot;,&quot;my_new_stopword2&quot;,}</code></p>
<p>However, when I save the nlp object using <code>nlp.to_disk()</code> and load it back again with <code>nlp.from_disk()</code>,
I am loosing the list of custom stop words.
Is there a way to save the custom stopwords with the nlp model?</p>
<p>Thanks in advance</p>
",Preprocessing of the text & Tokenization,customize spacy stop word save model using add stopwords spacy list stopwords however save nlp object using load back loosing list custom stop word way save custom stopwords nlp model thanks advance
How to get unique words from a list quickly?,"<p>I have a file with 3 million sentences (approx). Each sentence has around 60 words. I want to combine all the words and find unique words from them.</p>

<p>I tried the following code:</p>

<pre><code> final_list = list()
 for sentence in sentence_list:
     words_list = nltk.word_tokenize(sentence)
     words = [word for word in words_list if word not in stopwords.words('english') ]
     final_list = final_list + set(words)
</code></pre>

<p>This code gives unique words but, it's taking too long to process. Around 50k sentences per hour. It might take 3 days to process.</p>

<p>I tried with lambda function too:</p>

<pre><code>    final_list = list(map(lambda x: list(set([word for word in sentence])) ,sentence_list))
</code></pre>

<p>But, there is no significant improvement in execution. Please suggest a better solution with an effective time of execution. Parallel processing suggestions are welcome. </p>
",Preprocessing of the text & Tokenization,get unique word list quickly file million sentence approx sentence ha around word want combine word find unique word tried following code code give unique word taking long process around k sentence per hour might take day process tried lambda function significant improvement execution please suggest better solution effective time execution parallel processing suggestion welcome
Understanding TfidfVectorizer output,"<p>I'm testing <code>TfidfVectorizer</code> with simple example, and I can't figure out the results.</p>
<pre><code>corpus = [&quot;I'd like an apple&quot;,
          &quot;An apple a day keeps the doctor away&quot;,
          &quot;Never compare an apple to an orange&quot;,
          &quot;I prefer scikit-learn to Orange&quot;,
          &quot;The scikit-learn docs are Orange and Blue&quot;]
vect = TfidfVectorizer(min_df=1, stop_words=&quot;english&quot;)
tfidf = vect.fit_transform(corpus)

print(vect.get_feature_names())    
print(tfidf.shape)
print(tfidf)
</code></pre>
<p>output:</p>
<pre><code>['apple', 'away', 'blue', 'compare', 'day', 'docs', 'doctor', 'keeps', 'learn', 'like', 'orange', 'prefer', 'scikit']
(5, 13)
  (0, 0)    0.5564505207186616
  (0, 9)    0.830880748357988
  ...
</code></pre>
<p>I'm calculating the <code>tfidf</code> of the first sentence and I'm getting different results:</p>
<ul>
<li>The first document (&quot;<code>I'd like an apple</code>&quot;) contains just 2 words (after removeing stop words (according to the print of <code>vect.get_feature_names()</code> (we stay with: &quot;<code>like</code>&quot;, &quot;<code>apple</code>&quot;)</li>
<li>TF(&quot;apple&quot;, Doucment_1) = 1/2 = 0.5</li>
<li>TF(&quot;like&quot;, Doucment_1) = 1/2 = 0.5</li>
<li>The word <code>apple</code> appears 3 times in the corpus.</li>
<li>The word <code>like</code> appears 1 time in the corpus.</li>
<li>IDF (&quot;apple&quot;) = ln(5/3) = 0.51082</li>
<li>IDF (&quot;like&quot;) = ln(5/1) = 1.60943</li>
</ul>
<p>so:</p>
<ul>
<li><code>tfidf(&quot;apple&quot;)</code> in document1 = 0.5 * 0.51082 = 0.255 != 0.5564</li>
<li><code>tfidf(&quot;like&quot;)</code> in document1 = 0.5 * 1.60943 = 0.804 != 0.8308</li>
</ul>
<p>What am I missing ?</p>
",Preprocessing of the text & Tokenization,understanding tfidfvectorizer output testing simple example figure result output calculating first sentence getting different result first document contains word removeing stop word according print stay tf apple doucment tf like doucment word appears time corpus word appears time corpus idf apple ln idf like ln document document missing
NLTK sent_tokenize,"<p>I'm using NLTK's sent_tokenize to split sentences.</p>
<p>The module doesn't split sentences with abbreviations such as U.S., U.S.A, U.K.
This is sensible when the abbreviation is in the middle of the sentence. For example: &quot;The U.S. market is doing great.&quot;</p>
<p>However, this is an issues when the abbreviation appears at the end of the sentence. For example, NLTK do not split the following two sentences into two: &quot;During 2003, we improved our focus on core customers in regions outside the U.S. As a result, several agreements were completed...&quot;.</p>
<p>I thought of using 'U.S.\s[A-Z]', but this would not work since there are instances where a capitalized letter appears after the abbreviation in the middle of a sentence. For example, &quot;The U.S. Dollar...&quot;. Is there a way to overcome this?</p>
",Preprocessing of the text & Tokenization,nltk sent tokenize using nltk sent tokenize split sentence module split sentence abbreviation u u u k sensible abbreviation middle sentence example u market great however issue abbreviation appears end sentence example nltk split following two sentence two improved focus core customer region outside u result several agreement completed thought using u z would work since instance capitalized letter appears abbreviation middle sentence example u dollar way overcome
Should data feed into Universal Sentence Encoder be normalized?,"<p>I am currently working with Tensor Flow's Universal Sentence Encoder (<a href=""https://arxiv.org/pdf/1803.11175.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1803.11175.pdf</a>) for my B.Sc. thesis where I study extractive summarisation techniques.
In the vast majority of techniques for this task (like <a href=""https://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/view/11225/10855"" rel=""nofollow noreferrer"">https://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/view/11225/10855</a>), the sentences are first normalized (lowercasing, stop word removal, lemmantisation), but I couldn't find a hint whether sentences feed into the USE should first be normalized. Is that the case? Does is matter?</p>
",Preprocessing of the text & Tokenization,data feed universal sentence encoder normalized currently working tensor flow universal sentence encoder b sc thesis study extractive summarisation technique vast majority technique task like sentence first normalized lowercasing stop word removal lemmantisation find hint whether sentence feed use first normalized case doe matter
Split a sentence by words just as BERT Tokenizer would do?,"<p>I'm trying to localize all the [UNK] tokens of BERT tokenizer on my text. Once I have the position of the UNK token, I need to identify what word it belongs to. For that, I tried to get the position of the word using words_ids() or token_to_words() methods (the result is the same, I think) which give me the id word of this token.</p>
<p>The problem is, for a large text, there are many ways to split the text by words, and the ways I tried don't match with the position I get from token_to_words method. How I can split my text in the same way Bert tokenizer do?</p>
<p>I saw BERT use WordPiece for tokenize in sub-words, but nothing for complete words.</p>
<p>I'm at this point:</p>
<pre><code>  tokenized_text = tokenizer.tokenize(texto) # Tokens
  encoding_text = tokenizer(texto) # Esto es de tipo batchEncoding, como una instancia del tokenizer
  tpos = [i for i, element in enumerate(tokenized_text) if element == &quot;[UNK]&quot;]  # Posicion en la lista de tokens

  word_list = texto.split(&quot; &quot;)
  for x in tpos:
    wpos = encoding_text.token_to_word(x) # Posicion en la lista de palabras
    print(&quot;La palabra:  &quot;, word_list[wpos], &quot;    contiene un token desconocido: &quot;, tokenizer.tokenize(word_list[wpos]))
</code></pre>
<p>but it fails because the index &quot;wpos&quot; doesn't fit properly with my word_list.</p>
",Preprocessing of the text & Tokenization,split sentence word bert tokenizer would trying localize unk token bert tokenizer text position unk token need identify word belongs tried get position word using word id token word method result think give id word token problem large text many way split text word way tried match position get token word method split text way bert tokenizer saw bert use wordpiece tokenize sub word nothing complete word point fails index wpos fit properly word list
calculate positional proximity of two multiword exact phrases inside a large text in Python,"<p>How to calculate minimum positional distance between two multi-word, exact phrases inside a large text (e.g. an article), by using Python?
Assumption is that there might be multiple occurrences of both phrases.
To avoid miss-understanding, this is not a question about fuzzy string matching, edit distance, word lists etc.This is question about calculating positional proximity/distance between two exact phrases inside a text.</p>
<p>EDIT (modified solution by <a href=""https://stackoverflow.com/users/2359945/razzle-shazl"">https://stackoverflow.com/users/2359945/razzle-shazl</a>):</p>
<pre><code>def str_to_raw(s):
    raw_map = {8:r'\b', 7:r'\a', 12:r'\f', 10:r'\n', 13:r'\r', 9:r'\t', 11:r'\v'}
    return r''.join(i if ord(i) &gt; 32 else raw_map.get(ord(i), i) for i in s)

def find_smallest_distance1(sentence, word1, word2):
    distances = []
    dist = float('inf')
    p1 = str_to_raw(word1)
    p2 = str_to_raw(word2)
    s = sentence
    &quot;&quot;&quot;
    f1 = re.finditer(r'\bred fox\b', s, re.I)
    f2 = re.finditer(r'\bblue hen\b', s, re.I)
    &quot;&quot;&quot;
    f1 = re.finditer(p1, s, re.I)
    f2 = re.finditer(p2, s, re.I)    
    _f1 = _f2 = None
    while True:
        try:
            _f1 = next(f1)
        except StopIteration:
            break
    
        if _f2 == None:
            try:
                _f2 = next(f2)
            except StopIteration:
                break
            
        if _f1.span()[0] &gt; _f2.span()[0]:
            # we want f1 to always be closer to start / lower start index
            f1, f2 = f2, f1
            _f1, _f2 = _f2, _f1
        
        dist = min(dist, _f2.span()[0] - _f1.span()[1])
    return dist
</code></pre>
<p>I was wondering, how can it be modified, so that distance of phrase2 (word2) is calculated only to left or only to right direction from position of phrase1 (word1)?</p>
",Preprocessing of the text & Tokenization,calculate positional proximity two multiword exact phrase inside large text python calculate minimum positional distance two multi word exact phrase inside large text e g article using python assumption might multiple occurrence phrase avoid miss understanding question fuzzy string matching edit distance word list etc question calculating positional proximity distance two exact phrase inside text edit modified solution href pre wa wondering modified distance phrase word calculated left right direction position phrase word
What is the easiest way to strip HTML from scraped web data so that I am only left with strings of words?,"<p>I am interested in collecting a large corpus of text from various websites. The result will have lots of html. Is there an easy way of getting rid of the HTML so that I am left with only strings of words which I can then analyse?</p>
<p>I don't mind paying, but I prefer free and fast tools.</p>
<p>I have had a look and it looks like you can do this manually using packages like beautiful soup in python or using paid services like import.io to automatically clean data as the scraping occurs.</p>
<p>But are there better tools avaliable for stripping html from raw text?</p>
",Preprocessing of the text & Tokenization,easiest way strip html scraped web data left string word interested collecting large corpus text various website result lot html easy way getting rid html left string word analyse mind paying prefer free fast tool look look like manually using package like beautiful soup python using paid service like import io automatically clean data scraping occurs better tool avaliable stripping html raw text
How can we feed our custom synonym word list to NLP model (Word2Vec is preferred),"<p>I am using Word2Vec  model for making a vectorizer from my data.
My data has custom/business defined synonym word list which i want my NLP model should consider.
For e.g if &quot;A&quot; is a synonym of &quot;B&quot; then if i try to find synonym word for &quot;A&quot; using Word2Vec then it should give &quot;B&quot; with 100% match.</p>
<p>I can try different NLP models as well provided i am able to achieve the above requirement.</p>
",Preprocessing of the text & Tokenization,feed custom synonym word list nlp model word vec preferred using word vec model making vectorizer data data ha custom business defined synonym word list want nlp model consider e g synonym b try find synonym word using word vec give b match try different nlp model well provided able achieve requirement
Python: Improving performance of code performing spelling correction on text data,"<p>I have a text-data in form of comments that I want to preprocess. Apart from cutting away noise like URLs, numbers, ... and performing lemmatization, I also want to perform spelling correction. Specifically, I want to perform spelling correction only on words that do not occur more often than a given number of times to avoid false positives. For that purpose, I use <a href=""https://pyspellchecker.readthedocs.io/en/latest/code.html#"" rel=""nofollow noreferrer"">pyspellchecker</a> for the correction and <a href=""https://www.kite.com/python/docs/nltk.FreqDist"" rel=""nofollow noreferrer"">nltks FreqDist</a> to get word frequencies, however, doing that increases the time needed for preprocessing significantly.</p>
<p>I tried making things as performant as I could, but I am stuck and was wondering if there are still improvements I could make.</p>
<p>Here is my code:
Imports:</p>
<pre><code>from spacy.lang.en import English
from spellchecker import SpellChecker
from nltk.probability import FreqDist
nlp = spacy.load(&quot;en_core_web_sm&quot;)
spell = SpellChecker()
fdist = FreqDist()
</code></pre>
<p>Code:</p>
<pre><code>dict_misspell = {}

pipe = nlp.pipe(list_of_comments, batch_size = 512 ,disable = [&quot;tagger&quot;, &quot;parser&quot;])
    for j, doc in enumerate(pipe):
        tokens = [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_digit\
                                  and not token.like_url and not token.like_email and not token.like_num]
        processed_comments.append(&quot; &quot;.join(tokens))
        fdist += FreqDist(tokens)
        
        #remember which comments contain missspellings to avoid having to look at every comment later
        misspelled = spell.unknown(tokens)
        if (len(misspelled) &gt; 0):
            for misspelled_word in misspelled:
                if misspelled_word in dict_misspell.keys():
                    dict_misspell[misspelled_word].append(j)
                else:
                    dict_misspell[misspelled_word] = [j]
    
    #spell correction is done after the rest because only then is the frequency dict fully build.
    for k, mis in enumerate(dict_misspell.keys()):
        if(fdist[mis] &lt;= 5):  #only fix below certain word frequency to avoid false positives
            missspelling_idxs = dict_misspell[mis]
            correct_spelling = spell.correction(mis)
            for idx in missspelling_idxs:
                processed_comments[idx] = processed_comments[idx].replace(mis, correct_spelling)
</code></pre>
<p>As you can see above, I preprocess each individual comment, add all words of that comment to the frequency dictionary and for each word that the spellchecker considers misspelled I save those words and the index of the comment in which they occur in a misspell dictionary. After doing that the frequency dictionary is fully built and I start correcting possibly misspelled words who's frequency meet a condition in the individual comments.</p>
<p>Does anyone see a way to improve performance here?</p>
",Preprocessing of the text & Tokenization,python improving performance code performing spelling correction text data text data form comment want preprocess apart cutting away noise like url number performing lemmatization also want perform spelling correction specifically want perform spelling correction word occur often given number time avoid false positive purpose use pyspellchecker correction nltks freqdist get word frequency however increase time needed preprocessing significantly tried making thing performant could stuck wa wondering still improvement could make code import code see preprocess individual comment add word comment frequency dictionary word spellchecker considers misspelled save word index comment occur misspell dictionary frequency dictionary fully built start correcting possibly misspelled word frequency meet condition individual comment doe anyone see way improve performance
Findind index of words in a list of words,"<p>For BIO tagging problem, I'm looking for a way to find the index of specific words in a list of strings.</p>
<p>For example:</p>
<pre><code>text = &quot;Britain has reduced its carbon emissions more than any rich country&quot;
word = 'rich'
print(text.split())
['Britain', 'has', 'reduced', 'its', 'carbon', 'emissions', 'more', 'than', 'any', 'rich', 'country']

text.split(' ').index(word) # returns 9

text.split(' ').index('rich country') # occurring an error as expected 
</code></pre>
<p>My desired answer would be:</p>
<pre><code>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]
</code></pre>
<p>I think I can just use a loop to find the first word's index and the last word's index and then replace them into either in 0 or 1.</p>
<p>However my question is what if the <code>text</code> list would be like this:</p>
<pre><code>['Britain', 'has', 'reduced', 'its', 'carbon', 'emissions', 'more', 'than', 'any', 'rich', 'count', '_ry']
</code></pre>
<p>or maybe</p>
<pre><code>['Britain', 'has', 'reduced', 'its', 'carbon', 'emissions', 'more', 'than', 'any', 'richcountry']
</code></pre>
<p>I believe I can solve this problem with using dirty for loops, but I believe there would be another clean and simple way to solve this task.</p>
<p>I would appreciate if you guys could give me any advice on this problem.</p>
<p>Thanks in advance!</p>
",Preprocessing of the text & Tokenization,findind index word list word bio tagging problem looking way find index specific word list string example desired answer would think use loop find first word index last word index replace either however question list would like maybe believe solve problem using dirty loop believe would another clean simple way solve task would appreciate guy could give advice problem thanks advance
anti_join is not recognizing tidytext stop words in my dataset,"<p>I am working on removing stop words from a body of text with the tidytext approach in R.
<a href=""https://www.tidytextmining.com/tidytext.html"" rel=""nofollow noreferrer"">https://www.tidytextmining.com/tidytext.html</a></p>
<p>The following example works:</p>
<pre><code>library(tidytext)
library(dplyr)

data(stop_words)
str_v &lt;- paste(c(&quot;i've been dancing after midnight, i'd know because it's 
daylight&quot;))

str_v %&gt;% 
as_tibble %&gt;% 
unnest_tokens(word, value) %&gt;%
anti_join(stop_words)
</code></pre>
<p>When I apply this method to the data I'm working with it does not error, but the stop words are not removed. Does something invisible need to happen to the structure of the text for the stop words to match? The output rows appear identical to the stop words (lowered, squished, etc), and yet they remain... I'm working with protected data and am unable to share out source material. Any suggestions or advice on this problem would be super helpful, thank you!</p>
",Preprocessing of the text & Tokenization,anti join recognizing tidytext stop word dataset working removing stop word body text tidytext approach r following example work apply method data working doe error stop word removed doe something invisible need happen structure text stop word match output row appear identical stop word lowered squished etc yet remain working protected data unable share source material suggestion advice problem would super helpful thank
Unable to locate words in phrases in text files using python,"<p>I am trying to locate words and phrases contained in multiple text files.  My code will locate the words but it is completely missing the phrases.  Can anyone suggest a better way of doing this?  Regular expressions may taker too long given that I have several thousand txt files.</p>
<p>My code is as follows.</p>
<pre><code>import glob


work_list = ['at least', 'from time to time', 'in effect', 'greater than', 'lower than', 
'more than', 'up to', 'approximate', 'approximately', 'at the same time']

path = 'D:/Testing10'


context_d = {} # this will be used later 
for filename in glob.glob(os.path.join(path, '*.txt')):
    # if filename.endswith('.txt'):
        f = open(filename)
        file = f.read()
        txt = file.lower()
        txt = txt.split()
        txt = [item.replace('May', '') for item in txt] # locate and replace all months of May before lowering
        txt = [item.replace('\n', '') for item in txt]
        txt = [item.replace('\\n', '') for item in txt]
        txt = [item.replace('\\', '') for item in txt]
        txt = [item.replace('\\s', '') for item in txt]
        y = range(len(work_list))
        for i in y:
            if work_list[i] in txt:
              print(work_list[i], &quot;found in filename&quot;, filename)
            else:
              print(&quot;not found&quot;)```

</code></pre>
",Preprocessing of the text & Tokenization,unable locate word phrase text file using python trying locate word phrase contained multiple text file code locate word completely missing phrase anyone suggest better way regular expression may taker long given several thousand txt file code follows
turn list into string for tokenization,"<p>I am trying to loc a sentence from a dataframe based on it's report_id. But when I try to tokenize the sentence I get an error. I want to check for each word from the sentence if it exists in a list stored in row.keywords.</p>
<p>Here is my code:</p>
<pre><code>def sentence_color(start = 50, end = 51):
    for index, row in df_lda_result[start:end].iterrows():
        report = [rep.REPORTS for index, rep in data.iterrows() if rep.REPORT_ID == row.report_id]
        print(report)
        doc = nlp(report)
        tokens = [token.text for token in doc]
        
        for word in tokens:
            if not word in row.keywords:
                print('False:', word)
            else:
                print('True:', word)

sentence_color()
</code></pre>
<p>And this is the error: <em>TypeError: Argument 'string' has incorrect type (expected str, got list)</em></p>
",Preprocessing of the text & Tokenization,turn list string tokenization trying loc sentence dataframe based report id try tokenize sentence get error want check word sentence exists list stored row keywords code error typeerror argument string ha incorrect type expected str got list
Match stock tickers in the text with a list of stock tickers without matching the stop words,"<p>I have a python list with around 28,000 stock tickers.</p>
<p>I'm parsing through text which I scraped to match with the stock tickers and increase the count whenever I get a match.</p>
<p>The issue I am having is all the stop words are matching with some of the tickers which I don't want; eg V is a legitimate ticker and is matching with the individual tokenized words as it is free flowing social media text. eg V want TSLA.</p>
<p>Could you suggest me some logic where I could apply some logical smart matching with using these stop words?</p>
<pre><code>counts = dict()
Symbol_list =['TSLA','V','T','AAPL',...]

example sentence = { 'V want TSLA but not. T + 5 times' } 
</code></pre>
<p>This is what I've tried so far:</p>
<pre><code>sen = example_sentence.translate(str.maketrans('','',string.punctuation))

sentence_words = sen.split()
for words in sentence_words:
    if(word in symbol_list):
        counts[word] = counts.get(word,0) + 1
</code></pre>
<p>I would like to have <code>{'TSLA':1}</code> but not <code>{'TSLA':1, 'V':1, 'T': 1}</code>.
There might be some instances where I might need T and V to be added to the dictionary but contextually.</p>
",Preprocessing of the text & Tokenization,match stock ticker text list stock ticker without matching stop word python list around stock ticker parsing text scraped match stock ticker increase count whenever get match issue stop word matching ticker want eg v legitimate ticker matching individual tokenized word free flowing social medium text eg v want tsla could suggest logic could apply logical smart matching using stop word tried far would like might instance might need v added dictionary contextually
Tokenizing without breaking up key phrases,"<p>I have a string of text like <code>s = 'hi, welcome to grade 3'</code></p>
<p>currently when I tokenize the string I get</p>
<p><code>tokens = ['hi', 'welcome', 'to', 'grade', '3']</code></p>
<p>How can I tokenize the string without generating separate tokens for common phrases like 'grade 3'</p>
<p>I would want the output to be something like</p>
<p><code>tokens = ['hi', 'welcome', 'to', 'grade 3']</code></p>
<p>I have a list of common phrases I want to keep in one token if that makes it simpler</p>
<p>Ultimately I don't want to make all of my tokens bigrams as I still need the single word tokens for other parts of the program</p>
",Preprocessing of the text & Tokenization,tokenizing without breaking key phrase string text like currently tokenize string get tokenize string without generating separate token common phrase like grade would want output something like list common phrase want keep one token make simpler ultimately want make token bigram still need single word token part program
Python: selecting sentence based on given words and export them as excel,"<p>let say, I have the following paragraph as string:</p>
<pre><code>str=You can also use negative index numbers to slice a string. As we went through before, negative index numbers of a string start at -1, and count down from there until we reach the beginning of the string. When using negative index numbers, we’ll start with the lower number first as it occurs earlier in the string.
</code></pre>
<p>I want to find a sentence that contains the word <code>input words= 'negative' and 'string'</code>. And list them in separate <code>str</code> of each sentence. For example, I am expected to get output</p>
<pre><code>negative= a) You can also use negative index numbers to slice a string.
          b ) As we went through before, negative index numbers of a string start at -1, and count down from there until we reach the beginning of the string. 
          c) When using negative index numbers, we’ll start with the lower number first as it occurs earlier in the string.
string= a) You can also use negative index numbers to slice a string
        b) As we went through before, negative index numbers of a string start at -1, and count down from there until we reach the beginning of the string.
        c) When using negative index numbers, we’ll start with the lower number first as it occurs earlier in the string.
</code></pre>
<p>As you can see all sentences contains both 'negative' and 'string' words, hence includes all sentences.</p>
<p>If there is any way to do this without using <code>for loop</code> or in a simple way?</p>
",Preprocessing of the text & Tokenization,python selecting sentence based given word export excel let say following paragraph string want find sentence contains word list separate sentence example expected get output see sentence contains negative string word hence includes sentence way without using simple way
Spacy lemmatization: Incorrect results for the word &quot;number&quot;,"<p>I found that spacy incorrectly lemmatizes the word &quot;number&quot; to &quot;numb&quot;, and this results in inaccurate topics when I do the topic modeling afterwards. Below is a sample code with the output &quot;numb&quot;. I tried to replace &quot;word.lemma_&quot; with &quot;word.lemma_.strip()&quot; but got the same results.</p>
<p>How do I customize the lemma_ function so that &quot;number&quot; won't be transformed to &quot;numb&quot; after lemmatization?</p>
<pre><code>test = nlp('number')

for word in test:
        print(word.lemma_)

##This output &quot;numb&quot; 
</code></pre>
<p>Update:
I tried to force the POS of 'number' to NOUN, but still get 'numb' after the .lemma_ function.</p>
<pre><code>from spacy.symbols import NOUN, PROPN

test = nlp('my phone number is 3')

for word in test:
    if word.text == &quot;number&quot;:
        word.pos = NOUN
    
    print(word.lemma_)
</code></pre>
",Preprocessing of the text & Tokenization,spacy lemmatization incorrect result word number found spacy incorrectly lemmatizes word number numb result inaccurate topic topic modeling afterwards sample code output numb tried replace word lemma word lemma strip got result customize lemma function number transformed numb lemmatization update tried force po number noun still get numb lemma function
How to design a special array list in Python?,"<p>I am working on text in NLP. I want to count the co-occurrence of words with windows size 4. it means that for a selected node we consider its left and right locations (-2 -1 +1 +2). For example, we have a list that contains keywords: <code> K=[go,to,with]</code> and we want to check the keywords in left and right of words in a sentence and count number of co-occurrences of them.
for example, suppose we have this sentence: <code>S=[I go to university to learn every day.].</code> the window with size 4 for word <code>university</code> will be like this: <code>go to university to learn</code>. as instance the vector for word <code>university</code> is as follows: <code>university=[1 0 0 0 1 0 0 1 0 0 0 0]</code>. for example if we consider location -2 , we see that only there is word <code>go</code>, so we put 1 for it. then in location -1 of word <code>university</code> we have word <code>to</code>, so <code>go</code> and <code>with</code> will be zero and only <code>to</code> will be equal to 1. as another instance, when we are in +2, none of the words in list<code>k</code> is appeared there, so value for <code>go,to,with</code> will be zero.</p>
<p>The problem is that, which data structure should i consider for this to be able to fond keywords in vector and update their value for a given word? i don't have any idea how can i implement it.</p>
",Preprocessing of the text & Tokenization,design special array list python working text nlp want count co occurrence word window size mean selected node consider left right location example list contains keywords want check keywords left right word sentence count number co occurrence example suppose sentence window size word like instance vector word follows example consider location see word put location word word zero equal another instance none word list appeared value zero problem data structure consider able fond keywords vector update value given word idea implement
Why does spaCy not preserve intra-word-hyphens during tokenization like Stanford CoreNLP does?,"<p>SpaCy Version: 2.0.11</p>

<p>Python Version: 3.6.5</p>

<p>OS: Ubuntu 16.04</p>

<p>My Sentence Samples:</p>

<p><code>Marketing-Representative- won't die in car accident.</code></p>

<p>or</p>

<p><code>Out-of-box implementation</code></p>

<p>Expected Tokens:</p>

<p><code>[""Marketing-Representative"", ""-"", ""wo"", ""n't"", ""die"", ""in"", ""car"", ""accident"", "".""]</code></p>

<p><code>[""Out-of-box"", ""implementation""]</code></p>

<p>SpaCy Tokens(Default Tokenizer):</p>

<p><code>[""Marketing"", ""-"", ""Representative-"", ""wo"", ""n't"", ""die"", ""in"", ""car"", ""accident"", "".""]</code></p>

<p><code>[""Out"", ""-"", ""of"", ""-"", ""box"", ""implementation""]</code></p>

<p>I tried creating custom tokenizer but it won't handle all edge cases as handled by spaCy using tokenizer_exceptions(Code below):</p>

<pre><code>import spacy
from spacy.tokenizer import Tokenizer
from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex
import re
nlp = spacy.load('en')
prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)
suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)
infix_re = re.compile(r'''[.\,\?\:\;\...\‘\’\`\“\”\""\'~]''')

def custom_tokenizer(nlp):
    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,
                                suffix_search=suffix_re.search,
                                infix_finditer=infix_re.finditer,
                                token_match=None)
nlp.tokenizer = custom_tokenizer(nlp)
doc = nlp(""Marketing-Representative- won't die in car accident."")
for token in doc:
    print(token.text)
</code></pre>

<p>Output:</p>

<pre><code>Marketing-Representative-
won
'
t
die
in
car
accident
.
</code></pre>

<p>I need someone to guide me towards the appropriate way of doing this.</p>

<p>Either making changes in the regex above could do it or any other method or I even tried spaCy's Rule-Based Matcher but wasn't able to create rule to handle hyphens between more than 2 words e.g. ""out-of-box"" so that a Matcher can be created to be used with span.merge().</p>

<p>Either way I need to have words containing intra-word-hyphens to become single token as handled by Stanford CoreNLP.</p>
",Preprocessing of the text & Tokenization,doe spacy preserve intra word hyphen tokenization like stanford corenlp doe spacy version python version ubuntu sentence sample expected token spacy token default tokenizer tried creating custom tokenizer handle edge case handled spacy using tokenizer exception code output need someone guide towards appropriate way either making change regex could method even tried spacy rule based matcher able create rule handle hyphen word e g box matcher created used span merge either way need word containing intra word hyphen become single token handled stanford corenlp
Remove Stop Words in Python List Using List Comprehension,"<p>Python noob so sorry for simple question but I can't find the exact solution for my situation.</p>
<p>I've got a python list, I want to remove stop words from a list. My code isn't removing the stopword if it's paired with another token.</p>
<pre><code>    from nltk.corpus import stopwords
    rawData = ['for', 'the', 'game', 'the movie']
    text = [each_string.lower() for each_string in rawData]
    newText = [word for word in text if word not in stopwords.words('english')]
    print(newText)
</code></pre>
<p>current output:
['game', 'the movie']</p>
<p>desired output
['game', 'movie']</p>
<p>I'd prefer to use list comprehension for this.</p>
",Preprocessing of the text & Tokenization,remove stop word python list using list comprehension python noob sorry simple question find exact solution situation got python list want remove stop word list code removing stopword paired another token current output game movie desired output game movie prefer use list comprehension
Expand word contraction,"<p>I am writing a function to expand the word contraction. It takes a dataframe as input parameter and outputs the dataframe with &quot;clean_text&quot; column with the expanded pattern in the text. I can do this by using qdap mgsub function to replace the patterns in the texts. However, I am wondering if there is a better solution for this.</p>
<pre><code>contrap_pattern &lt;- c(&quot;i'm&quot;,&quot;you're&quot;,&quot;he's&quot;,&quot;she's&quot;,&quot;it's&quot;, &quot;we're&quot;, &quot;they're&quot;,&quot;i've&quot;,&quot;you've&quot;,&quot;we've&quot;,&quot;they've&quot;,&quot;i'd&quot;,&quot;you'd&quot;,&quot;he'd&quot;,&quot;she'd&quot;,&quot;we'd&quot;,&quot;they'd&quot;,&quot;i'll&quot;,&quot;you'll&quot;,&quot;he'll&quot;,&quot;she'll&quot;,&quot;we'll&quot;,&quot;they'll&quot;,&quot;isn't&quot;,&quot;aren't&quot;,&quot;wasn't&quot;,&quot;weren't&quot;,&quot;hasn't&quot;,&quot;haven't&quot;,&quot;hadn't&quot;,&quot;doesn't&quot;,&quot;don't&quot;,&quot;didn't&quot;,&quot;won't&quot;,&quot;wouldn't&quot;,&quot;shan't&quot;,&quot;shouldn't&quot;,&quot;can't&quot;,&quot;couldn't&quot;,&quot;mustn't&quot;,&quot;let's&quot;,&quot;that's&quot;,&quot;who's&quot;,&quot;what's&quot;,&quot;here's&quot;,&quot;there's&quot;,&quot;when's&quot;,&quot;where's&quot;,&quot;why's&quot;,&quot;how's&quot;)


replacement_pattern &lt;- c(&quot;I am&quot;,&quot;you are&quot;,&quot;he is&quot; ,&quot;she is&quot; ,&quot;it is&quot;,&quot;we are&quot; , &quot;they are&quot;, &quot;I have&quot;,&quot;you have&quot;,&quot;we have&quot;, &quot;they have&quot;,&quot;I would&quot;,&quot;you would&quot;,&quot;he would&quot;,  &quot;she would&quot;,&quot;we would&quot;,&quot;they would&quot;, &quot;I will&quot;,&quot;you will&quot;,&quot;he will&quot;, &quot;she will&quot; ,&quot;we will&quot;,&quot;they will&quot;,&quot;is not&quot;,&quot;are not&quot;,&quot;was not&quot;,&quot;were not&quot;,&quot;has not&quot; , &quot;have not&quot;,&quot;had not&quot;,&quot;does not&quot;,&quot;do not&quot;, &quot;did not&quot; ,&quot;will not&quot;,&quot;would not&quot;, &quot;shall not&quot;,&quot;should not&quot;,&quot;can not&quot;,&quot;could not&quot;,&quot;must not&quot;,&quot;let us&quot;,&quot;that is&quot;, &quot;who is&quot;,&quot;what is&quot;,&quot;here is&quot;, &quot;there is&quot;,&quot;when is&quot;,&quot;where is&quot;,&quot;why is&quot;,&quot;how is&quot;)


clean$text_clean &lt;- qdap::mgsub(pattern = contrap_pattern, replacement = replacement_pattern, clean$text_clean)
</code></pre>
<p>Update: without explicitly writing the patterns in the code, the function replace_contraction() serves the need. Thanks @phiver for the suggestion.</p>
",Preprocessing of the text & Tokenization,expand word contraction writing function expand word contraction take dataframe input parameter output dataframe clean text column expanded pattern text using qdap mgsub function replace pattern text however wondering better solution update without explicitly writing pattern code function replace contraction serf need thanks phiver suggestion
Python: difflib.get_close_matches comparing modified text but returning original,"<p>I extracted a list of words from a text, but during text preprocessing I have lowercased everything for easier comparison.</p>
<p>My question is how to make the extracted words in the list appear as they exactly appeared in the original text?</p>
<p>I have tried to first tokenize the original text, and then find the closest matches in thise tokenized list to the word list I have extracted from the text. I used the each of the following for finding the closest matches:</p>
<ol>
<li>nltk.edit_distance</li>
<li>difflib.get_close_matches</li>
</ol>
<p>But neither of them worked as I wanted. They extract somehow similar words but not exactly as they appear in the original text. I think the problem is that these methods treat lowercased, and uppercased words differently.</p>
<blockquote>
<p>Words extracted can be unigram, bigram up to 5-gram.</p>
</blockquote>
<p>Example:</p>
<p>I have extracted the following bigram from text <strong>[rfid alert]</strong>, but in original text it appeared like this <strong>[RFID alert]</strong>.</p>
<p>After using</p>
<p><code>difflib.get_close_matches('rfid alert', original_text_unigram_tokens_list)</code></p>
<p>it's output was <strong>[profile Caller]</strong>  and not <strong>[RFID alert]</strong>. That is because python is case-sensitive. I think it found that the bigram in <code>original_text_unigram_tokens_list</code> with the least number of different characters from <strong>[rfid alert]</strong> is <strong>[profile Caller]</strong> so it returned <strong>[profile Caller]</strong>.</p>
<p>Therefore my question is: Is there any ready method or any workaround I could do to return the original form of the ngram as it appeared in text exactly? For instance, I want to get <strong>[RFID alert]</strong> instead of <strong>[profile Caller]</strong> in the above example, and so on.</p>
<p>I appreciate any help. Thank you in advance.</p>
",Preprocessing of the text & Tokenization,python difflib get close match comparing modified text returning original extracted list word text text preprocessing lowercased everything easier comparison question make extracted word list appear exactly appeared original text tried first tokenize original text find closest match thise tokenized list word list extracted text used following finding closest match nltk edit distance difflib get close match neither worked wanted extract somehow similar word exactly appear original text think problem method treat lowercased uppercased word differently word extracted unigram bigram gram example extracted following bigram text rfid alert original text appeared like rfid alert using output wa profile caller rfid alert python case sensitive think found bigram least number different character rfid alert profile caller returned profile caller therefore question ready method workaround could return original form ngram appeared text exactly instance want get rfid alert instead profile caller example appreciate help thank advance
How to remove punctuation from a text?,"<p>I have a very big data set . I am wondering How I can remove all punctuation from a big dataset in pyspark? For example <code>, . &amp; \ | - _</code></p>
",Preprocessing of the text & Tokenization,remove punctuation text big data set wondering remove punctuation big dataset pyspark example
Get all leaf words for a stemmed keyword,"<p>I am looking for something like un-stemming. Is there a way to get all possible list of words which have share a common stem. Something like</p>
<pre><code>&gt;&gt;&gt; get_leaf_words('play')
&gt;&gt;&gt; ['player', 'play', 'playing' ... ]
</code></pre>
",Preprocessing of the text & Tokenization,get leaf word stemmed keyword looking something like un stemming way get possible list word share common stem something like
Tokenizing emojis contiguous to words,"<p>I am trying to tokenize strings that have the two following patterns:</p>

<ul>
<li>contiguous emojis, for instance ""Hey, 😍🔥""</li>
<li>emojis contiguous to words, for instance ""surprise💥 !!""</li>
</ul>

<p>To do this, I have tried the <code>word_tokenize()</code> function from <code>nltk</code> (<a href=""https://kite.com/python/docs/nltk.tokenize.word_tokenize"" rel=""nofollow noreferrer"">doc</a>). However, it does not split the contiguous entities when emojis are involved.</p>

<p>For instance,</p>

<pre><code>from nltk.tokenize import word_tokenize
word_tokenize(""Hey, 😍🔥"")
</code></pre>

<p>output: <code>['Hey', ',', '😍🔥']</code></p>

<p>I'd like to get: <code>['Hey', ',', '😍', '🔥']</code></p>

<p>and</p>

<pre><code>word_tokenize(""surprise💥 !!"")
</code></pre>

<p>output: <code>['surprise💥', '!', '!']</code></p>

<p>I'd like to get <code>['surprise', '💥', '!', '!']</code></p>

<p>Therefore, I was thinking maybe using specific regex pattern could solve the issue but I don't know what pattern to use.</p>
",Preprocessing of the text & Tokenization,tokenizing emojis contiguous word trying tokenize string two following pattern contiguous emojis instance hey emojis contiguous word instance surprise tried function doc however doe split contiguous entity emojis involved instance output like get output like get therefore wa thinking maybe using specific regex pattern could solve issue know pattern use
Tokenization of unbalanced dataset,"<p>I'm working with a dataset of emails' content which I want to transform with doc2vec. This is a labeled dataset (spam/not-spam) and it is unbalanced (90-10 ratio).
My question is: when tokenizing the emails' content, should I first oversample (using SMOTE), or is it ok to use the dataset as is?</p>
",Preprocessing of the text & Tokenization,tokenization unbalanced dataset working dataset email content want transform doc vec labeled dataset spam spam unbalanced ratio question tokenizing email content first oversample using smote ok use dataset
Stemming on tokenized words,"<p>Having this dataset:</p>
<pre><code>&gt;cleaned['text']
0         [we, have, a, month, open, #postdoc, position,...
1         [the, hardworking, biofuel, producers, in, iow...
2         [the, hardworking, biofuel, producers, in, iow...
3         [in, today, s, time, it, is, imperative, to, r...
4         [special, thanks, to, gaetanos, beach, club, o...
                                ...                        
130736    [demand, gw, sources, fossil, fuels, renewable...
130737         [there, s, just, not, enough, to, go, round]
130738    [the, answer, to, deforestation, lies, in, space]
130739    [d, filament, from, plastic, waste, regrind, o...
130740          [gb, grid, is, generating, gw, out, of, gw]
Name: text, Length: 130741, dtype: object
</code></pre>
<p>Is there a simple way to stem all the words?</p>
",Preprocessing of the text & Tokenization,stemming tokenized word dataset simple way stem word
Lemmatize a doc with spacy?,"<p>I have a spaCy <code>doc</code> that I would like to lemmatize.</p>

<p>For example:</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_lg')

my_str = 'Python is the greatest language in the world'
doc = nlp(my_str)
</code></pre>

<p>How can I convert every token in the <code>doc</code> to its lemma?</p>
",Preprocessing of the text & Tokenization,lemmatize doc spacy spacy would like lemmatize example convert every token lemma
How do you correctly cluster document names &amp; find similarities between documents based on Word2Vec model?,"<p>I have a set of documents (3000) which each contain a short description. I want to use Word2Vec model to see if I can cluster these documents based on the description. </p>

<p>I'm doing it the in the following way, but I am not sure if this is a ""good"" way to do it. Would love to get feedback.</p>

<p>I'm using Google's trained w2v model.</p>

<pre><code>wv = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True,encoding=""ISO-8859-1"", limit = 100000)
</code></pre>

<p>Each document is split into words where stop words are removed, and I have used stemming as well.</p>

<p>My initial idea was to fetch the word vector for each word in each documents description, average it, and then cluster based on this. </p>

<pre><code>doc2vecs = []
for i in range(0, len(documents_df['Name'])):
    vec = [0 for k in range(300)] 
    for j in range(0, len(documents_df['Description'][i])):
        if documents_df['Description'][i][j] in wv:
            vec += wv[documents_df['Description'][i][j]]
    doc2vecs.append(vec/300)
</code></pre>

<p>I'm then finding similarities using</p>

<pre><code>similarities = squareform(pdist(doc2vecs, 'cosine'))
</code></pre>

<p>Which returns a matrix of the cosine between each vector in <code>doc2vec</code>.</p>

<p>I then try to cluster the documents. </p>

<pre><code>num_clusters = 2
km = cluster.KMeans(n_clusters=num_clusters)
km.fit(doc2vecs)
</code></pre>

<p>So basically what I am wondering is:</p>

<p>Is this method of clustering the average word vector for each word in the document a reasonable way to cluster the documents?</p>
",Preprocessing of the text & Tokenization,correctly cluster document name find similarity document based word vec model set document contain short description want use word vec model see cluster document based description following way sure good way would love get feedback using google trained w v model document split word stop word removed used stemming well initial idea wa fetch word vector word document description average cluster based finding similarity using return matrix cosine vector try cluster document basically wondering method clustering average word vector word document reasonable way cluster document
When exactly should we perform spell correction in the text pre-processing pipeline?,"<p>Does the step for correcting spellings of words have to be done before Lexicon normalization(i.e. stemming, lemmatization) or after? If we do it after lexicon normalization, wouldn't the words already be reduced to their root form if we perform lemmatization?(By passing the POS i.e. Parts Of Speech tag of the word as an argument) so there wouldn't be any use for spell checking after lexicon normalization right?</p>
",Preprocessing of the text & Tokenization,exactly perform spell correction text pre processing pipeline doe step correcting spelling word done lexicon normalization e stemming lemmatization lexicon normalization word already reduced root form perform lemmatization passing po e part speech tag word argument use spell checking lexicon normalization right
Should we preprocess text data before or after train/test split?,"<p>I have read many text supervised classification tutorials and I implemented tidytext, qunateda, tm, text2vec, RTextTools for my data. <em><strong>Until now, I have one unsolved puzzle</strong></em>. It seems there is no general consensus on when to tokenize the text data. <strong>Before or after the train-test split?</strong>. In one stack overflow post, some argued that it is even illegal to tokenize before you split. With dfm_match functions, quanteda package looks like it is designed to do the tokenization after splitting the data. Others recommend doing the split after preprocessing. I have seen nice tutorials by Julia Silge and Emil Hvitfeldt.<br />
To me, it would save me many lines of code if I do the preprocessing step before the split. But, what are the risks? Data leakage or what? Is there any evidence comparing the two in terms of classification performance, validity, etc?</p>
",Preprocessing of the text & Tokenization,preprocess text data train test split read many text supervised classification tutorial implemented tidytext qunateda tm text vec rtexttools data one unsolved puzzle seems general consensus tokenize text data train test split one stack overflow post argued even illegal tokenize split dfm match function quanteda package look like designed tokenization splitting data others recommend split preprocessing seen nice tutorial julia silge emil hvitfeldt would save many line code preprocessing step split risk data leakage evidence comparing two term classification performance validity etc
How to find matching word in two list then insert the matches word into a column in dataframe?,"<p>I have two list, one contains list of positive word and the other contains list of tokenize word. I want to compare both list and if the positive word and tokenize word matching then I want to insert into a positive column in dataframe but if it's not match then I want to insert into negative column.</p>
<p>I tried to loop through the tokenize word and use if statement:</p>
<pre class=""lang-py prettyprint-override""><code>word_classify = pd.DataFrame()
words = [word for word in a]
for word in words:
    if word in pos_dic:
        word_classify['pos'] = word
    elif word in neg_dic:
        word_classify['neg'] = word
</code></pre>
<p>But then it return blank dataframe. Here is my list of tokenize words:</p>
<p><a href=""https://i.sstatic.net/Oxwqc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Oxwqc.png"" alt=""tokenize_word"" /></a></p>
<p>And here is my list of positive words:</p>
<p><a href=""https://i.sstatic.net/a38Fo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/a38Fo.png"" alt=""positive_words"" /></a></p>
<p>Any suggestion how to fix it? Am I doing something wrong?</p>
",Preprocessing of the text & Tokenization,find matching word two list insert match word column dataframe two list one contains list positive word contains list tokenize word want compare list positive word tokenize word matching want insert positive column dataframe match want insert negative column tried loop tokenize word use statement return blank dataframe list tokenize word list positive word suggestion fix something wrong
How to count specific terms in tokenized sentences wthin a pandas df,"<p>I'm new to Python and nltk, so I would really appreciate your input on the following problem.</p>
<p><strong>Goal:</strong></p>
<p>I want to search and count the occurrence of specific terminology in tokenized sentences which are stored in a pandas DataFrame. The terms I'm searching for are stored in a list of strings. The output should be saved in a new column.</p>
<p>Since the words I'm searching for are grammatically inflected (e.g. cats instead of cat) I need a solution which not only displays exact matches. I guess stemming the data and searching for specific stems would be a proper approach but let's assume this is not an option here, as we would still have semantic overlaps.</p>
<p><strong>What I tried so far:</strong></p>
<p>In order to further handle the data I preprocessed the data while following these steps:</p>
<ol>
<li>Put everything in lower case</li>
<li>Remove punctuation</li>
<li>Tokenization</li>
<li>Remove stop words</li>
</ol>
<p>I tried searching for single terms with <code>str.count('cat')</code> but this doesn't do the trick and the data is marked as missing with <code>NaN</code>. Additionally, I don't know how to iterate over the search word list in an efficient way while using pandas.</p>
<p><strong>My code so far:</strong></p>
<pre><code>import numpy as np
import pandas as pd
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Function to remove punctuation
def remove_punctuation(text):
    return re.sub(r'[^\w\s]','',text)


# Target data where strings should be searched and counted
data = {'txt_body': ['Ab likes dogs.', 'Bc likes cats.',
                     'De likes cats and dogs.', 'Fg likes cats, dogs and cows.',
                     'Hi has two grey cats, a brown cat and two dogs.']}


df = pd.DataFrame(data=data)

# Search words stored in a list of strings
search_words = ['dog', 'cat', 'cow']

# Store stopwords from nltk.corpus
stop_words = set(stopwords.words('english'))

# Data preprocessing
df['txt_body'] = df['txt_body'].apply(lambda x: x.lower())
df['txt_body'] = df['txt_body'].apply(remove_punctuation)
df['txt_body'] = df['txt_body'].fillna(&quot;&quot;).map(word_tokenize)
df['txt_body'] = df['txt_body'].apply(lambda x: [word for word in x if word not in stop_words])

# Here is the problem space
df['search_count'] = df['txt_body'].str.count('cat')



print(df.head())

</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>                                       txt_body  search_count
0                             [ab, likes, dogs]             1
1                             [bc, likes, cats]             1
2                       [de, likes, cats, dogs]             2
3                 [fg, likes, cats, dogs, cows]             3
4  [hi, two, grey, cats, brown, cat, two, dogs]             3

</code></pre>
",Preprocessing of the text & Tokenization,count specific term tokenized sentence wthin panda df new python nltk would really appreciate input following problem goal want search count occurrence specific terminology tokenized sentence stored panda dataframe term searching stored list string output saved new column since word searching grammatically inflected e g cat instead cat need solution display exact match guess stemming data searching specific stem would proper approach let assume option would still semantic overlap tried far order handle data preprocessed data following step put everything lower case remove punctuation tokenization remove stop word tried searching single term trick data marked missing additionally know iterate search word list efficient way using panda code far expected output
How do I count all occurrences of a phrase in a text file using regular expressions?,"<p>I am reading in multiple files from a directory and attempting to find how many times a specific phrase (in this instance &quot;at least&quot;) occurs in each file (not just that it occurs, but how many times in each text file it occurs)  My code is as follows</p>
<pre><code>import glob
import os

path = 'D:/Test'

k = 0

for filename in glob.glob(os.path.join(path, '*.txt')):
    if filename.endswith('.txt'):
        f = open(filename)
        data = f.read()
        data.split()
        data.lower()
        S = re.findall(r' at least ', data, re.MULTILINE)
        count = []
        if S == True:
         for S in data:
          count.append(data.count(S))
          k= k + 1
          print(&quot;'{}' match&quot;.format(filename), count)
        else:
         print(&quot;'{}' no match&quot;.format(filename))
print(&quot;Total number of matches&quot;, k)
</code></pre>
<p>At this moment I get no matches at all.  I can count whether or not there is an occurrence of the phrase but am not sure why I can't get a count of all occurrences in each text file.</p>
<p>Any help would be appreciated.</p>
<p>regards</p>
",Preprocessing of the text & Tokenization,count occurrence phrase text file using regular expression reading multiple file directory attempting find many time specific phrase instance least occurs file occurs many time text file occurs code follows moment get match count whether occurrence phrase sure get count occurrence text file help would appreciated regard
Ho to do lemmatization on German text?,"<p>I have a German text that I want to apply lemmatization to. If lemmatization is not possible, then I can live with stemming too.</p>
<p><strong>Data:</strong> This is my German text:</p>
<pre><code>mails=['Hallo. Ich spielte am frühen Morgen und ging dann zu einem Freund. Auf Wiedersehen', 'Guten Tag Ich mochte Bälle und will etwas kaufen. Tschüss']
</code></pre>
<p><strong>Goal:</strong> After applying lemmatization it should look similar to this:</p>
<pre><code>mails_lemma=['Hallo. Ich spielen am früh Morgen und gehen dann zu einer Freund. Auf Wiedersehen', 'Guten Tag Ich mögen Ball und wollen etwas kaufen Tschüss']
</code></pre>
<p>I tried using spacy</p>
<blockquote>
<p>conda install -c conda-forge spacy</p>
<p>python -m spacy download de_core_news_md</p>
</blockquote>
<pre><code>import spacy
from spacy.lemmatizer import Lemmatizer
lemmatizer = Lemmatizer()
[lemmatizer.lookup(word) for word in mails]
</code></pre>
<p>I see following problems.</p>
<ol>
<li><p>My data is structured in sentences and not single words</p>
</li>
<li><p>In my case spacy lemmatization doesn't seem to work even for single words.</p>
</li>
</ol>
<p>Can you please tell me how this works?</p>
",Preprocessing of the text & Tokenization,ho lemmatization german text german text want apply lemmatization lemmatization possible live stemming data german text goal applying lemmatization look similar tried using spacy conda install c conda forge spacy python spacy download de core news md see following problem data structured sentence single word case spacy lemmatization seem work even single word please tell work
How do I extract a sequence of contiguous prespecified words from a list?,"<p>I have two lists given:</p>
<blockquote>
<p><code>list_1</code> is a list of words which I'm interested in<br />
<code>list_2</code> is a tokenized sequence of words extracted from text</p>
</blockquote>
<p>What I want to do is to extract sequences of words out of <code>list_2</code> if they are contained in <code>list_1</code> and concatenate them as long as the following word in <code>list_2</code> is also contained in <code>list_1</code>.</p>
<p>Unfortunately I don't quite know how to start. Any tipp would be much appreciated.</p>
<p>Best regards!</p>
",Preprocessing of the text & Tokenization,extract sequence contiguous prespecified word list two list given list word interested tokenized sequence word extracted text want extract sequence word contained concatenate long following word also contained unfortunately quite know start tipp would much appreciated best regard
R remove stopwords from a character vector using %in%,"<p>I have a data frame with strings that I'd like to remove stop words from.  I'm trying to avoid using the <code>tm</code> package as it's a large data set and <code>tm</code> seems to run a bit slowly.  I am using the <code>tm</code> <code>stopword</code> dictionary.</p>

<pre><code>library(plyr)
library(tm)

stopWords &lt;- stopwords(""en"")
class(stopWords)

df1 &lt;- data.frame(id = seq(1,5,1), string1 = NA)
head(df1)
df1$string1[1] &lt;- ""This string is a string.""
df1$string1[2] &lt;- ""This string is a slightly longer string.""
df1$string1[3] &lt;- ""This string is an even longer string.""
df1$string1[4] &lt;- ""This string is a slightly shorter string.""
df1$string1[5] &lt;- ""This string is the longest string of all the other strings.""

head(df1)
df1$string1 &lt;- tolower(df1$string1)
str1 &lt;-  strsplit(df1$string1[5], "" "")

&gt; !(str1 %in% stopWords)
[1] TRUE
</code></pre>

<p>This is not the answer I'm looking for.  I'm trying to get a vector or string of the words NOT in the <code>stopWords</code> vector. </p>

<p>What am I doing wrong?</p>
",Preprocessing of the text & Tokenization,r remove stopwords character vector using data frame string like remove stop word trying avoid using package large data set seems run bit slowly using dictionary answer looking trying get vector string word vector wrong
How to perform Lemmatization in R?,"<p>This question is a possible duplicate of <strong><a href=""https://stackoverflow.com/questions/22993796/lemmatizer-in-r-or-python-am-are-is-be"">Lemmatizer in R or python (am, are, is -> be?)</a></strong>, but I'm adding it again since the previous one was closed saying it was too broad and the only answer it has is not efficient (as it accesses an external website for this, which is too slow as I have very large corpus to find the lemmas for). So a part of this question will be similar to the above mentioned question.</p>

<p>According to Wikipedia, lemmatization is defined as:</p>

<blockquote>
  <p>Lemmatisation (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.</p>
</blockquote>

<p>A simple Google search for lemmatization in R will <em>only</em> point to the package <code>wordnet</code> of R. When I tried this package expecting that a character vector <code>c(""run"", ""ran"", ""running"")</code> input to the lemmatization function would result in <code>c(""run"", ""run"", ""run"")</code>, I saw that this package only provides functionality similar to <code>grepl</code> function through various filter names and a dictionary.</p>

<p>An example code from <code>wordnet</code> package, which gives maximum of 5 words starting with ""car"", as the filter name explains itself:</p>

<pre><code>filter &lt;- getTermFilter(""StartsWithFilter"", ""car"", TRUE)
terms &lt;- getIndexTerms(""NOUN"", 5, filter)
sapply(terms, getLemma)
</code></pre>

<p>The above is <strong>NOT</strong> the lemmatization that I'm looking for. What I'm looking for is, using <code>R</code> I want to find true roots of the words: (For e.g. from <code>c(""run"", ""ran"", ""running"")</code> to <code>c(""run"", ""run"", ""run"")</code>).</p>
",Preprocessing of the text & Tokenization,perform lemmatization r question possible duplicate lemmatization looking looking using want find true root word e g
How to add sentence numbering for tokenised words in a dataframe,"<p>I currently have a dataframe with a column of already tokenized words and other columns with tags:</p>
<pre><code>  token      tag
1    I        PRN
2    like     VBD
3    apples   NNP
4    .         .
5    John     PRN
6    likes    VBD
7    pears    NNP
8     .        .
</code></pre>
<p>I would like to add sentence numbering within the df, by adding an extra column:</p>
<pre><code>  token      tag   sentence #
1    I        PRN  sentence 1
2    like     VBD  sentence 1
3    apples   NNP  sentence 1
4    .         .   sentence 1
5    John     PRN  sentence 2
6    likes    VBD  sentence 2
7    pears    NNP  sentence 2
8     .        .   sentence 2
</code></pre>
<p>I am working with a human annotated dataset that has been pre-tokenized. I already tried de-tokenizing it, adding the sentence count and then re-tokenizing it; which gave me an entirely different token count, unfortunately. This method would results in the tag columns not aligning with the token column.</p>
<p>Thank you very much!</p>
",Preprocessing of the text & Tokenization,add sentence numbering tokenised word dataframe currently dataframe column already tokenized word column tag would like add sentence numbering within df adding extra column working human annotated dataset ha pre tokenized already tried de tokenizing adding sentence count tokenizing gave entirely different token count unfortunately method would result tag column aligning token column thank much
How to modify word in a for loop in python,"<p>Im trying to stem some text in python with SnowballStemmer, but it wont work. Here is the code:</p>
<pre><code>import nltk
from nltk import SnowballStemmer

stem = SnowballStemmer(&quot;spanish&quot;)

def limpiar (texto):
  texto = texto.split()
  stemm = SnowballStemmer('spanish')
  for palabra in texto:
    palabra = stem.stem(palabra.lower())
    
  return texto
</code></pre>
<p>It returns the text in lower capitals, but without stemming</p>
",Preprocessing of the text & Tokenization,modify word loop python im trying stem text python snowballstemmer wont work code return text lower capital without stemming
Python remove words containing &quot;unusal&quot; chracters,"<p>I have an array of words and and I would like to remove all words that contain any unusual characters like umlauts, accents, etc. (I know there are ways to normalize them to regular characters instead, but I specifically want to remove them).</p>
<p>My idea so far is to create an array of accepted characters (letters a-z), because this is easier than making a blacklist accounting for all possible combinations of letters and accents, and go through my array of words checking if the word has any characters other than the accepted ones.</p>
<p>I've found <a href=""https://www.geeksforgeeks.org/python-remove-words-containing-list-characters/"" rel=""nofollow noreferrer"">this</a> article that describes the opposite, removing all words that <em>do contain</em> a certain characters:</p>
<p><code>filtered_tokens = [w for w in tokens if all(ch not in w for ch in accepted_characters)]</code></p>
<p>unfortunately a simple negation doesn't make this work for my problem.</p>
<p>I'm open to suggestions and new approaches altogether, but ideally I would like to get this to work with no additional packages besides maybe nltk, which I am using to extract my words from a text.</p>
",Preprocessing of the text & Tokenization,python remove word containing unusal chracters array word would like remove word contain unusual character like umlaut accent etc know way normalize regular character instead specifically want remove idea far create array accepted character letter z easier making blacklist accounting possible combination letter accent go array word checking word ha character accepted one found article describes opposite removing word contain certain character unfortunately simple negation make work problem open suggestion new approach altogether ideally would like get work additional package besides maybe nltk using extract word text
remove stopwords using jieba in Python,"<p>I have encountered an error when I run the following code.<br>
I want to remove stopwords, however it doesn't work! </p>

<pre><code>def cut_txt(old_file):
from string import punctuation
import jieba
jieba.load_userdict(""user_dictionary.csv"") 

stopwords = [line.strip().decode('utf-8') for line in open('stop_words.txt').readlines() ]

global cut_file     # 分词之后保存的文件名
cut_file = old_file + '_cut.txt'

try:
    fi = open(old_file, 'r', encoding='utf-8')
except BaseException as e:  # 因BaseException是所有错误的基类，用它可以获得所有错误类型
    print(Exception, "":"", e)    # 追踪错误详细信息

text = fi.read()  # 获取文本内容
new_text = jieba.cut(text, cut_all=False)  # 精确模式
str_out = ' '.join(new_text).replace('，', '').replace('。', '').replace('？', '').replace('！', '') \
    .replace('“', '').replace('”', '').replace('：', '').replace('…', '').replace('（', '').replace('）', '') \
    .replace('—', '').replace('《', '').replace('》', '').replace('、', '').replace('‘', '') \
    .replace('’', '').replace(',', '').replace('【', '').replace('】', '').replace('""', '')\
    .replace('#','').replace('...','').replace('?','').replace('『','')                                     # 去掉标点符号

#去除停用词
final = ''
for seg in str_out:
    seg = seg.encode('gbk')
    if seg not in stopwords:
           final += seg      
fo = open(cut_file, 'w', encoding='utf-8')
fo.write(final)

cut_txt('你的天空')
</code></pre>
",Preprocessing of the text & Tokenization,remove stopwords using jieba python encountered error run following code want remove stopwords however work
Adding tokens to GPT-2 BPE tokenizer,"<p>I want to add new words to my BPE tokenizer. I know the symbol Ġ means the end of a new token and the majority of tokens in vocabs of pre-trained tokenizers start with Ġ. Assume I want to add the word <strong>Salah</strong> to my tokenizer. I tried to add both <strong>Salah</strong> token and <strong>ĠSalah</strong>:
tokenizer.add_tokens(['Salah', 'ĠSalah']) # they get 50265 and 50266 values respectively.
However, when I tokenize a sentence where <strong>Salah</strong> appears, the tokenizer will never return me the second number (neither when using <code>.tokenize</code>nor<code>.encode</code>), for instance:
<code>tokenizer.tokenize('I love Salah and salad')</code> returns <code>['I', 'Ġlove', 'Salah', 'Ġand', 'Ġsalad']</code>.
The question is: should I use the symbol <code>Ġ</code> when adding new tokens or the tokenizer does it itself? Or, probably, it must be specified manually?
Thanks in advance!</p>
",Preprocessing of the text & Tokenization,adding token gpt bpe tokenizer want add new word bpe tokenizer know symbol mean end new token majority token vocabs pre trained tokenizers start assume want add word salah tokenizer tried add salah token salah tokenizer add token salah salah get value respectively however tokenize sentence salah appears tokenizer never return second number neither using instance return question use symbol adding new token tokenizer doe probably must specified manually thanks advance
spacy aggressive lemmatization and removing unexpected words,"<p>I am trying to clean some text data.
fisrt i removed the stop words, then i tried to Lemmatize the text. But words such as nouns are removed</p>
<p><strong>Sample Data</strong></p>
<p><a href=""https://drive.google.com/file/d/1p9SKWLSVYeNScOCU_pEu7A08jbP-50oZ/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1p9SKWLSVYeNScOCU_pEu7A08jbP-50oZ/view?usp=sharing</a>
<strong>udpated Code</strong></p>
<pre><code># Libraries  
import spacy
import pandas as pd
import gensim
from gensim.utils import simple_preprocess
import nltk; nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['covid', 'COVID-19', 'coronavirus'])

article= pd.read_csv(&quot;testdata.csv&quot;)
data = article.title.values.tolist()
nlp = spacy.load('en_core_web_sm')

def sent_to_words(sentences):
    for sentence in sentences:
      yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

data_words = list(sent_to_words(data))

def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]
data_words_nostops = remove_stopwords(data_words)
print (&quot;*** Text  After removing Stop words:   &quot;)
print(data_words_nostops)
def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV','PRON']):
    &quot;&quot;&quot;https://spacy.io/api/annotation&quot;&quot;&quot;
    texts_out = []
    for sent in texts:
        doc = nlp(&quot; &quot;.join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out
data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV','PRON'])
print (&quot;*** Text  After Lemmatization:   &quot;)

print(data_lemmatized)
</code></pre>
<p>The output after removing Stopwords is :</p>
<blockquote>
<p>[['qaia', 'flags', 'amman', 'melbourne', 'jetstar', 'flights', 'recovery', 'plan'],<br />
['western', 'amman', 'suburb', 'new','nsw', 'ground', 'zero', children],<br />
['flight', 'returned', 'amman','qaia', 'staff', 'contract','driving'], ]]</p>
</blockquote>
<p>The output after Lematization :</p>
<blockquote>
<p>[['flight', 'recovery', 'plan']</p>
</blockquote>
<blockquote>
<p>['suburb', 'ground']</p>
</blockquote>
<blockquote>
<p>['return', 'contract','driving']</p>
</blockquote>
<p>on each reacord I do not understand the following :</p>
<p><strong>-1st reord: why these words are removed: &quot;'qaia', 'flags', 'amman', 'melbourne', 'jetstar'</strong></p>
<p><strong>-2ed recored: essential words are reomved same as the first reord, Also, I was expecting children to convert to child</strong></p>
<p><strong>-3ed, &quot;driving&quot; is not converted to &quot;drive&quot;</strong></p>
<p>I was expecting that words will such as &quot;Amman&quot; will not removed, Also i am expecting the words will be converted from plural to singular. And the verbs will be converted to the infinitive ...</p>
<p>What i am missing here???
Thanx in advance</p>
",Preprocessing of the text & Tokenization,spacy aggressive lemmatization removing unexpected word trying clean text data fisrt removed stop word tried lemmatize text word noun removed sample data udpated code output removing stopwords qaia flag amman melbourne jetstar flight recovery plan western amman suburb new nsw ground zero child flight returned amman qaia staff contract driving output lematization flight recovery plan suburb ground return contract driving reacord understand following st reord word removed qaia flag amman melbourne jetstar ed recored essential word reomved first reord also wa expecting child convert child ed driving converted drive wa expecting word amman removed also expecting word converted plural singular verb converted infinitive missing thanx advance
How to vectorize dictionary of word tokens (bag of words implementation),"<p>I'm creating my own bag of words algorithm but I'm stuck. So far I've tokenized the words(A list of strings and a user inputted string) and put them in a dictionary. Now I would like to create word vectors where 0 indicates the word is not in the document and 1 means it's present. My idea is to create a zero vector the size of which corresponds to the amount of unique words. <strong>Then make copies of that base vector, update the values of the vector for each document, and store them in an array</strong>. This is the part where I'm stuck.</p>
<pre><code>import more_itertools as mit
import re
from collections import OrderedDict

def get_vector(lexicon, text):
   
    # Creates a dictionary with inital value 0 for all unique words in the vocabulary
    zero_vector = OrderedDict((token, 0) for token in lexicon)
    corpus_tokens = list(mit.collapse(text.split()))

def BoW(corpus: list, search_doc: str):
    
    word_count = {}
    
    # Regex to grab words here because its just a string
    search_doc_tokens = re.split(r'[-\s.,;!?]+', search_doc)
    
    # I have to do all this business here because it's a list of strings
    grab_words = [word.split() for word in corpus]
    corpus_tokens = list(mit.collapse(grab_words))
    
    # Concatenating the two lists
    vocabulary = corpus_tokens + search_doc_tokens
    
    # Filling dictionary
    for token in vocabulary:
        if token not in word_count:
            word_count[token] = 1
        else:
            word_count[token] += 1
                    
    
    # Unique words in vocab. Used determine size of zero vector
    lexicon = sorted(set(vocabulary))
    zero_vector = OrderedDict((token, 0) for token in lexicon)
    
    print(zero_vector)

documents = [&quot;This is a text document&quot;, &quot;This is another text document&quot;, &quot;Get the picture?&quot;]
BoW(documents, &quot;hello there&quot;) 
</code></pre>
",Preprocessing of the text & Tokenization,vectorize dictionary word token bag word implementation creating bag word algorithm stuck far tokenized word list string user inputted string put dictionary would like create word vector indicates word document mean present idea create zero vector size corresponds amount unique word make copy base vector update value vector document store array part stuck
Python regex selection of verbs with present perfect,"<p>In a given string, I am trying to catch verbs that are in present pefect tense. I do that by using the following regular expression in python:</p>
<pre><code>import re
sentence = &quot;The Batman has never shown his true identity but has done so much good for Gotham City&quot;

verb = re.findall(r'has\s[^\,\.\&quot;]{0,50}done', sentence)

</code></pre>
<p>And the outcome is:</p>
<pre><code>&gt;&gt;&gt; print(verb)

['has never shown his true identity but has done']
</code></pre>
<p>Here, the correct answer would have been 'has done', but the 'has' from 'has never shown' is the wrong 'has' catched. The part <code>[^\,\.\&quot;]{0,50}</code> permits some freedom with respect to what is between 'has' and 'done', which does not appear here but is useful on my real data. However, it catches the first 'has' it finds, which is not always the good one. Is it possible to take the last 'has' instead ?</p>
",Preprocessing of the text & Tokenization,python regex selection verb present perfect given string trying catch verb present pefect tense using following regular expression python outcome correct answer would ha done ha ha never shown wrong ha catched part permit freedom respect ha done doe appear useful real data however catch first ha find always good one possible take last ha instead
Best Algorithm to make correction typos in text,"<p>I have a list of word library and a text in which there are a spell error (typos), and I want to correct the word spell error to be correct according to list of library</p>

<p>for example </p>

<p>in list of word :</p>

<p><code>listOfWord = [...,""halo"",""saya"",""sedangkan"",""semangat"",""cemooh""..];</code></p>

<p>this is my string :</p>

<p><code>string = ""haaallllllooo ssya sdngkan ceemoooh , smngat semoga menyenangkan""</code></p>

<p>I want change the spellerror to be correct like :</p>

<p><code>string = ""halo saya sedangkan cemooh, semangat semoga menyenangkan""</code></p>

<p>what is the best algorithm to check each word in list, because I have millions of words in the list and have many possibilities</p>
",Preprocessing of the text & Tokenization,best algorithm make correction typo text list word library text spell error typo want correct word spell error correct according list library example list word string want change spellerror correct like best algorithm check word list million word list many possibility
Is it bad to not remove stopwords when I&#39;ve already set a ceiling on document frequency?,"<p>I'm using <code>sklearn.feature_extraction.text.TfidfVectorizer</code>. I'm processing text. It seems standard to remove stop words. However, it seems to me that if I already have a ceiling on document frequency, meaning I will not include tokens that are in a large percent of the document (eg <code>max_df=0.8</code>), dropping stop words doesn't seem necessary. Theoretically, stop words are words that appear often and should be excluded. This way, we don't have to debate on what to include in our list of stop words, right? It's my understanding that there is disagreement over what words are used often enough that they should be considered stop words, right? For example, scikit-learn includes &quot;whereby&quot; in its built-in list of English stop words.</p>
",Preprocessing of the text & Tokenization,bad remove stopwords already set ceiling document frequency using processing text seems standard remove stop word however seems already ceiling document frequency meaning include token large percent document eg dropping stop word seem necessary theoretically stop word word appear often excluded way debate include list stop word right understanding disagreement word used often enough considered stop word right example scikit learn includes whereby built list english stop word
How to remove stopwords from a multilingual excel text data,"<p>I'm working on a multilanguage text data where some text cleaning steps are to be done. I'm using spacy library for the removal of stopwords, so for multilanguage spacy has a package called <code>spacy.load(&quot;xx_ent_wiki_sm&quot;)</code> which is giving me error as follows.</p>
<p><img src=""https://i.sstatic.net/89NGP.png"" alt=""enter image description here"" /></p>
",Preprocessing of the text & Tokenization,remove stopwords multilingual excel text data working multilanguage text data text cleaning step done using spacy library removal stopwords multilanguage spacy ha package called giving error follows
Sligthly Different Word Frequency after Text Tokenization,"<p>I'm doing some work on NLP and I was doing some tasks of tokenization and text preprocessing while found this:</p>
<p>Funtion using to plot word frequency:</p>
<pre><code>def len_distribution(X):
    x = [len(n) for n in X]

    plt.hist(x, bins=len(x))
    plt.xlabel('Number of words')
    plt.ylabel('Number of texts')
    plt.title('Distribution of text length on dataset')
    plt.show()
</code></pre>
<p>Word frequency <strong>before</strong> tokenization (plain text):</p>
<pre><code>len_distribution([x.split() for x in X])
</code></pre>
<p><a href=""https://i.sstatic.net/QtUhT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QtUhT.png"" alt=""enter image description here"" /></a></p>
<p>Word frequency <strong>after</strong> tokenization (sequences):</p>
<pre><code>tokenizer = tf.keras.preprocessing.text.Tokenizer()

tokenizer.fit_on_texts(X)
X_encoded = tokenizer.texts_to_sequences(X)
len_distribution(X_encoded)
</code></pre>
<p><a href=""https://i.sstatic.net/GQ6fe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GQ6fe.png"" alt=""enter image description here"" /></a></p>
<p>Basically I thought both plots should be <strong>exaclty</strong> the same, shouldn't them? Because words are just being changed by numbers but the frequency of each number must be the same as before, why this is happening?</p>
",Preprocessing of the text & Tokenization,sligthly different word frequency text tokenization work nlp wa task tokenization text preprocessing found funtion using plot word frequency word frequency tokenization plain text word frequency tokenization sequence basically thought plot exaclty word changed number frequency number must happening
word in words.words() check too slow and inaccurate in Python,"<p>I am having a dataset, which is consisting of two columns, one is a Myers-Briggs personality type and the other one is containing the last 50 tweets of that person. I have tokenized, removed the URLs and the stop words from the list, and lemmatized the words.</p>
<p>I am then creating a <code>collections.Counter</code> of the most common words and I am checking whether they are valid English words with <code>nltk</code>.</p>
<p>The problem is that checking if the word exists in the corpora vocabulary takes too much time and I also think that a lot of words are missing from this vocabulary. This is my code:</p>
<pre><code>import nltk    
import collections
from nltk.corpus import words

# nltk.download(&quot;words&quot;)

# Creating a frequency Counter of all the words
frequency_counter = collections.Counter(df.posts.explode())
sorted_common_words = sorted(frequency_counter.items(), key = lambda pair: -pair[1])

words_lst = []
for i in range(len(sorted_common_words)):
    if sorted_common_words[i][1] &gt; 1000:
        words_lst.append(sorted_common_words[i][0])

valid_words = []
invalid_words = []

valid_words = [word for word in words_lst if word in words.words()]
invalid_words = [word for word in words_lst if word not in words.words()]
</code></pre>
<p>My problem is that the <code>invalid_words</code> list is containing some valid English words like:</p>
<ol>
<li>f*ck</li>
<li>changed</li>
<li>surprised</li>
<li>girlfriend</li>
<li>avatar</li>
<li>anymore</li>
</ol>
<p><a href=""https://i.sstatic.net/4z6fe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4z6fe.png"" alt=""word.words() check"" /></a></p>
<p>And some more of course. Even checking manually if those words exist in the <code>words.words()</code> it returns <code>False</code>. I tried initially to stem my text but this produced some root of the words, which didn't look right, and that's why decided to lemmatize them.</p>
<p>Is there a library in Python which have all the stemmed versions of the English words? I guess this will speed up significantly my script.</p>
<p>My original dataframe is around 9000 lines, and a bit more than 5M tokenized words and around 110.000 unique words after cleaning the dataset. 'words.words()<code>is containing 236736 words, so checking if those 110.000 words are within</code>words.words()` will take too much time. I have checked and checking of 1000 takes approximately a minute. This is mainly due to the limitation of Python to be run on only one core, so I cannot parallelize the operation on all available cores.</p>
",Preprocessing of the text & Tokenization,word word word check slow inaccurate python dataset consisting two column one myers briggs personality type one containing last tweet person tokenized removed url stop word list lemmatized word creating common word checking whether valid english word problem checking word exists corpus vocabulary take much time also think lot word missing vocabulary code problem list containing valid english word like f ck changed surprised girlfriend avatar anymore course even checking manually word exist return tried initially stem text produced root word look right decided lemmatize library python stemmed version english word guess speed significantly script original dataframe around line bit tokenized word around unique word cleaning dataset word word word word take much time checked checking take approximately minute mainly due limitation python run one core parallelize operation available core
Remove punctuation from a pandas Series,"<p>Here's my <code>series</code> which had been tokenized and removed stop words:</p>
<pre><code>0        [laptop, sits, 4, stars, similarly, priced, co...
1        [ordered, monitor, wanted, makeshift, area, po...
2        [monitor, great, deal, price, size, ., use, of...
3        [bought, height, adjustment, ., swivel, abilit...
4        [worked, month, died, ., 5, calls, hp, support...
                               ...                        
30618                                        [great, deal]
30619                                  [pour, le, travail]
30620                                      [business, use]
30621                                         [good, size]
30622    [pour, mon, ordinateur.plus, grande, image.vra...
Name: text_body, Length: 30623, dtype: object
</code></pre>
<p>I want to remove punctuation from the above series. I had tried something like this</p>
<pre><code>filtered_text = re.sub(r'[^\w\s]','',str(series))
</code></pre>
<p>the result comes out as a string.</p>
<p>2 questions I have.</p>
<ol>
<li>is there a way to convert the <code>filtered_text</code> string to back to list or series?</li>
<li>are there better ways to Remove punctuation from the original series?</li>
</ol>
",Preprocessing of the text & Tokenization,remove punctuation panda series tokenized removed stop word want remove punctuation series tried something like result come string question way convert string back list series better way remove punctuation original series
Using Regex to encompass a group of keys in a dictionary and match them inside of a list of strings,"<p>I'm new to text-cleaning in python but I currently created a dictionary with various slang words/acronyms/contractions that looks something like this:</p>
<blockquote>
<p>fulltext = {'BYOB': 'bring your own beer', 'couldn't': 'could not', 'finna': 'going to'}... etc.</p>
</blockquote>
<p>and I have another large corpus of text data:</p>
<blockquote>
<p>uncleaned_text = ['This is finna be crazy', 'I don't know why we couldn't be there', 'I should have known when the event was BYOB that it would be terrible']</p>
</blockquote>
<p>For which I am trying to 'clean' by replacing those words inside the list of strings that match the dictionary keys with their corresponding values. So, my ideal output would be:</p>
<blockquote>
<p>cleaned text = ['This is going to be crazy', 'I don't know why we could not be there', 'I should have known when the event was bring your own beer that it would be terrible']</p>
</blockquote>
<p>I know I should be using  REGEX in some way and I know I should be using loops, but I am definitely not even close to what I should be doing I think, because the error I get is builtin function not iterable...</p>
<p>Any suggestions?</p>
<p>for sentence in uncleaned_text:
for word in sentence:
if word in fulltext.keys:
word.replace(word, fulltext.key)</p>
",Preprocessing of the text & Tokenization,using regex encompass group key dictionary match inside list string new text cleaning python currently created dictionary various slang word acronym contraction look something like fulltext byob bring beer could finna going etc another large corpus text data uncleaned text finna crazy know known event wa byob would terrible trying clean replacing word inside list string match dictionary key corresponding value ideal output would cleaned text going crazy know could known event wa bring beer would terrible know using regex way know using loop definitely even close think error get builtin function iterable suggestion sentence uncleaned text word sentence word fulltext key word replace word fulltext key
Topic modeling with Spacy - not making very good predictions,"<p>I am working on a topic modelling task, whereby I am taking peoples feedback (text) and trying to extract the important topics from them.</p>
<p>The feedback is quite short, and I don't know if that is what is posing us the problem. Below is my code, is there anything really obvious I have missed?</p>
<p>I am removing stop words, lemmatizing, keeping only nouns and removing stop words. However I pass these into the model, it's not working quite as I hoped</p>
<p>One of the big issues is semantics, the customer can refer to the same concept in different ways : shop, boutique, store, supermarket, etc... They are all referring to the shop, but the LDA sees these as different concepts and dumps them into different topics, even though 'I love the store' and 'I love the shop' are the same statement.</p>
<pre><code>import spacy
import pandas as pd
from textblob import TextBlob

#set display options
pd.set_option('display.max_colwidth', 0)
pd.set_option('display.max_rows', 0)

#ingest data
df = pd.read_csv('surv.csv')

#import spacy language library and stopword dictionary
nlp = spacy.load('en_core_web_sm')
all_stopwords = nlp.Defaults.stop_words

#Limit DF to columns of interest and drop nulls
responses = df[['Comment', 'score']]
responses = responses.dropna()

#lemmatize the strings
def cleanup(row):
    comment = row['Comment']
    comment = nlp(comment)
    sent = []
    for word in comment:
        sent.append(word.lemma_)    
    return &quot; &quot;.join(sent)

#keep only nouns
def only_nouns(row):
    comment = row['nostops']
    blob = TextBlob(comment)
    x = blob.noun_phrases
    return &quot; &quot;.join(x)

def pos(row):
    comment = row['nostops']
    comment = nlp(comment)
    nouns = []
    i=0
    while i &lt; len(comment)-1:
        if comment[i].pos_ == 'NOUN':
            nouns.append(comment[i])
        i=i+1
    return nouns
        
#remove the stop words
def remove_stops(row):
    comment = row['Comment']
    comment = comment.split(' ')  
    rem = []
    for word in comment:
        if word not in all_stopwords:
            rem.append(word)
    return &quot; &quot;.join(rem)

#What entities are defined in the document
def split_entities(row):
    comment = row['Comment']
    comment = nlp(comment)
    entities = []
    for ent in comment.ents:
        entities.append(ent)
    return entities          

#Call functions
responses['lemmas'] = responses.apply(cleanup,axis=1)            
responses['nostops'] = responses.apply(remove_stops,axis=1)
responses['nouns'] = responses.apply(pos, axis=1)
responses['nouns2'] = responses.apply(only_nouns, axis=1)
responses['entities'] = responses.apply(split_entities,axis=1)


from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
cv = CountVectorizer(max_df=0.9, min_df=2, stop_words='english') 
document_term_matrix = cv.fit_transform(responses['nouns'])
lda = LatentDirichletAllocation(n_components=4, random_state=42)
lda.fit(document_term_matrix)
topic_results = lda.transform(document_term_matrix) 
</code></pre>
",Preprocessing of the text & Tokenization,topic modeling spacy making good prediction working topic modelling task whereby taking people feedback text trying extract important topic feedback quite short know posing u problem code anything really obvious missed removing stop word lemmatizing keeping noun removing stop word however pas model working quite hoped one big issue semantics customer refer concept different way shop boutique store supermarket etc referring shop lda see different concept dump different topic even though love store love shop statement
Scoring texts based on dictionaries other than positive / negative in Quanteda?,"<p>I would like to be able to give the texts in a very large corpus a score based on the number of words they contain from either 'Dictionary 1' (positive score), or 'Dictionary 2' (negative score). Can anybody help me to figure out how to do this?</p>
<p>I have read up on the use of Quanteda for sentiment analysis <a href=""https://tutorials.quanteda.io/advanced-operations/targeted-dictionary-analysis/"" rel=""nofollow noreferrer"">https://tutorials.quanteda.io/advanced-operations/targeted-dictionary-analysis/</a> and it seems like that is essentially just scoring texts based on whether they contain words from lists of 'positive' and 'negative' words.</p>
<p>I would like to use Quanteda for this, as I am familiar with the package.</p>
<p>How can I make modified dictionary which doesn't contain two lists of positive and negative words, but two list of other kinds of words e.g. 'right wing' and 'left wing' for example? I already know the words I wish to include in each list.</p>
",Preprocessing of the text & Tokenization,scoring text based dictionary positive negative quanteda would like able give text large corpus score based number word contain either dictionary positive score dictionary negative score anybody help figure read use quanteda sentiment analysis seems like essentially scoring text based whether contain word list positive negative word would like use quanteda familiar package make modified dictionary contain two list positive negative word two list kind word e g right wing left wing example already know word wish include list
How to get the TF-IDF scores as well for the most important words?,"<p>I am working on a project with tf-idf, I have a column (df['liststring']) in my dataframe that contains the preprocessed text (without punctuation, stop words, etc.) from my various documents.</p>
<p>I ran the following code, and I got the top 10 words with the highest tf-idf values but I would like to see their scores as well.</p>
<pre><code>    from sklearn.feature_extraction.text import TfidfVectorizer
    tfidf = TfidfVectorizer()
    X_tfidf = tfidf.fit_transform(df['liststring']).toarray()
    vocab = tfidf.vocabulary_
    reverse_vocab = {v:k for k,v in vocab.items()}
    feature_names = tfidf.get_feature_names()
    df_tfidf = pd.DataFrame(X_tfidf, columns = feature_names)
    idx = X_tfidf.argsort(axis=1)
    tfidf_max10 = idx[:,-10:]
    df_tfidf['top10'] = [[reverse_vocab.get(item) for item in row] for row in tfidf_max10 ]
    
df_tfidf['top10']

0      [kind, pose, world, preventive, sufficient, ke...
1      [mode, california, diseases, evidence, zoonoti...
2      [researcher, commentary, allegranzi, say, mora...
3      [carry, mild, man, whatever, suffering, downpl...
4      [region, service, almost, wednesday, detect, f...
                             ...                        
754    [americans, plan, year, black, online, shop, s...
755    [relate, manor, tuesday, death, portobello, ce...
756    [one, october, eight, exist, transmit, cluster...
757    [wolfe, shelter, county, resident, cupertino, ...
758    [firework, year, blasio, day, marching, reimag...
</code></pre>
<p>If we take the first row as an example, instead of [kind, pose, world, preventive, sufficient, ke...], I would like to get the output look like [kind:0.2, pose:0.3, world:0.4, preventive:0.5, sufficient:0.6, ke...]</p>
",Preprocessing of the text & Tokenization,get tf idf score well important word working project tf idf column df liststring dataframe contains preprocessed text without punctuation stop word etc various document ran following code got top word highest tf idf value would like see score well take first row example instead kind pose world preventive sufficient ke would like get output look like kind pose world preventive sufficient ke
How to find the similar sentence based on keyword that does not directly appear in sentences?,"<p>I need to return a text that contain the keyword. Let's consider the following example:</p>
<pre><code>keyword = &quot;configure&quot;
texts = [ 
   &quot;The system configuration document should be uploaded to the repository. Please contact the dev team.&quot;,
   &quot;To do the system setup, please follow the instructions.&quot; 
]
</code></pre>
<p>The keyword <code>configure</code> does not appear in any text. But the similar word <code>configuration</code> appears in the first sentence. Therefore the expected output is:</p>
<pre><code>The system configuration document should be uploaded to the repository. Please contact the dev team.
</code></pre>
<p>I know that it's possible to calculate the [semantic similarity between a word and texts][1]. However, it often returns inaccurate results for my case.</p>
<p>Another approach that I was evaluating is to apply stemming and lemmatization. However, <code>configure</code> and <code>configuration</code> have different stems.</p>
<p>Finally I also considered <code>Word2Vec</code> model... However, in this case I'm not sure how to efficiently use this approach.</p>
<pre><code>import gensim.downloader as api

word_vectors = api.load(&quot;glove-wiki-gigaword-100&quot;) 

word_vectors.similarity(&quot;configure&quot;,&quot;configuration&quot;)
</code></pre>
<p>Is there any state-of-the-art approach to deal with my task?
[1]: <a href=""https://medium.com/@adriensieg/text-similarities-da019229c894"" rel=""nofollow noreferrer"">https://medium.com/@adriensieg/text-similarities-da019229c894</a></p>
",Preprocessing of the text & Tokenization,find similar sentence based keyword doe directly appear sentence need return text contain keyword let consider following example keyword doe appear text similar word appears first sentence therefore expected output know possible calculate semantic similarity word text however often return inaccurate result case another approach wa evaluating apply stemming lemmatization however different stem finally also considered model however case sure efficiently use approach state art approach deal task
Creating Wordcloud from Pandas series,"<pre><code>import pandas as pd
import matplotlib.pyplot as plt
% matplotlib inline
import spacy
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
nlp= spacy.load('en_core_web_sm')
</code></pre>
<p>Here I have a series called <code>tokens_lemma</code> which had remove stopwords, had been <code>.lower()</code> and lemmalize</p>
<pre><code>tokens_lemma
0        [laptop, sit, 4, star, similarly, price, compa...
1        [order, monitor, want, makeshift, area, powerf...
2        [monitor, great, deal, price, size, ., use, of...
3        [buy, height, adjustment, ., swivel, ability, ...
4        [work, month, die, ., 5, call, hp, support, nu...
                               ...                        
30618                                        [great, deal]
30619                                  [pour, le, travail]
30620                                      [business, use]
30621                                         [good, size]
30622    [pour, mon, ordinateur.plus, grande, image.vra...
Name: text_body, Length: 30623, dtype: object
</code></pre>
<p>I want to create a wordcloud using the above series.</p>
<pre><code>for w in tokens_lemma:
w=str(w)
comment_words += &quot; &quot;.join(w)

wordcloud = WordCloud(width = 800, height = 800, 
background_color ='white',  
min_font_size = 10).generate(comment_words)

# plot the WordCloud image                        
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis(&quot;off&quot;) 
plt.tight_layout(pad = 0) 
  
plt.show() 

    ---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-33-9faee80d909b&gt; in &lt;module&gt;
----&gt; 1 wordcloud = WordCloud(width = 800, height = 800, 
      2 background_color ='white',
      3 min_font_size = 10).generate(comment_words)

~\anaconda3\envs\datasci\lib\site-packages\wordcloud\wordcloud.py in generate(self, text)
    629         self
    630         &quot;&quot;&quot;
--&gt; 631         return self.generate_from_text(text)
    632 
    633     def _check_generated(self):

    ~\anaconda3\envs\datasci\lib\site-packages\wordcloud\wordcloud.py in generate_from_text(self, text)
        611         &quot;&quot;&quot;
        612         words = self.process_text(text)
    --&gt; 613         self.generate_from_frequencies(words)
        614         return self
        615 
    
~\anaconda3\envs\datasci\lib\site-packages\wordcloud\wordcloud.py in generate_from_frequencies(self, frequencies, max_font_size)
    401         frequencies = sorted(frequencies.items(), key=itemgetter(1), reverse=True)
    402         if len(frequencies) &lt;= 0:
--&gt; 403             raise ValueError(&quot;We need at least 1 word to plot a word cloud, &quot;
    404                              &quot;got %d.&quot; % len(frequencies))
    405         frequencies = frequencies[:self.max_words]

ValueError: We need at least 1 word to plot a word cloud, got 0.
</code></pre>
<p>I personally think that the problem comes from the <code>comment_words</code> object due to wrong data format for the wordcloud. But again, I don't know how to change to match the wordcloud format.</p>
",Preprocessing of the text & Tokenization,creating wordcloud panda series series called remove stopwords lemmalize want create wordcloud using series personally think problem come object due wrong data format wordcloud know change match wordcloud format
What are the cases where NLTK&#39;s word_tokenize differs from str.split()?,"<p>Is there documentation where I can find all the possible cases where <code>word_tokenize</code> is different/better than simply splitting by whitespace? If not, could a semi-thorough list be given?</p>
",Preprocessing of the text & Tokenization,case nltk word tokenize differs str split documentation find possible case different better simply splitting whitespace could semi thorough list given
Using spacy to get rid of stopwords in pandas series,"<p>I had been trying to get rid of stopwords using spacy library.</p>
<p><strong>Code</strong></p>
<pre><code>import spacy
import pandas as pd
import numpy as np

nlp= spacy.load('en_core_web_sm')
</code></pre>
<p><strong>my_series:</strong></p>
<pre><code>my_series

0        this laptop sits at just over 4 stars while so...
1        i ordered this monitor because i wanted to mak...
2        this monitor is a great deal for the price and...
3        bought this for the height adjustment. the swi...
4        worked for a month and then it died. after 5 c...
                               ...                        
30618                                           great deal
30619                                      pour le travail
30620                                         business use
30621                                            good size
30622    pour mon ordinateur.plus grande image.vraiment...
Name: text_body, Length: 30623, dtype: object
</code></pre>
<p><strong>Tokenize</strong></p>
<pre><code>s_tokenized=my_series.apply(lambda x: nlp(x))
</code></pre>
<p><strong>Remove stopwords</strong></p>
<pre><code>all_stopwords = nlp.Defaults.stop_words
filtered_text=s_tokenized.apply(lambda x: [w for w in x if not w in all_stopwords])
filtered_text

0        [this, laptop, sits, at, just, over, 4, stars,...
1        [i, ordered, this, monitor, because, i, wanted...
2        [this, monitor, is, a, great, deal, for, the, ...
3        [bought, this, for, the, height, adjustment, ....
4        [worked, for, a, month, and, then, it, died, ....
                               ...                        
30618                                        [great, deal]
30619                                  [pour, le, travail]
30620                                      [business, use]
30621                                         [good, size]
30622    [pour, mon, ordinateur.plus, grande, image.vra...
Name: text_body, Length: 30623, dtype: object
</code></pre>
<p>tokenize seems to be working fine but removing stopwords does not seems to remove any word at all nor raising any errors. Is there something I miss or did wrong?</p>
",Preprocessing of the text & Tokenization,using spacy get rid stopwords panda series trying get rid stopwords using spacy library code series tokenize remove stopwords tokenize seems working fine removing stopwords doe seems remove word raising error something miss wrong
Extracts data using regular expression,"<pre><code>text='''

        Consumer Price Index:
        +0.3% in Aug 2020

        Unemployment Rate:
        +2.4% in Aug 2020
'''
</code></pre>
<p>extracts data into a list of tuples using regular expression, e.g.</p>
<pre><code>[('Consumer Price Index', '+0.2%', 'Aug 2020'), ...]
</code></pre>
<p>and returns the list of tuples</p>
<p>I try several times by</p>
<pre class=""lang-py prettyprint-override""><code>re.findall( , text)
</code></pre>
<p>Anyone have good ideas?</p>
",Preprocessing of the text & Tokenization,extract data using regular expression extract data list tuples using regular expression e g return list tuples try several time anyone good idea
Tokenizing words in pandas series,"<p>I am having a problem with tokenizing words in panda series.</p>
<p>my series named <code>df</code>:</p>
<pre><code>                        text
0     This monitor is a great deal for the price.
1     I would recommend it.
2     poor packaging.
dtype: object
</code></pre>
<p>I had tried <code>df_tokenized=nltk.word_tokenize(df)</code> but result in <code>TypeError: expected string or bytes-like object </code></p>
<p>I also tried 3 variations of <code>.apply(lambda row:)</code></p>
<pre><code>df_tokenized=df.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)
&gt; TypeError: &lt;lambda&gt;() got an unexpected keyword argument 'axis'

df_tokenized=df.apply(lambda row: nltk.word_tokenize(row['text']))
&gt; TypeError: string indices must be integers

df_tokenized=df.apply(lambda row: nltk.word_tokenize(row[1]))
&gt; TypeError: 'float' object is not subscriptable
</code></pre>
<p>Are there any other ways to tokenize words from series?</p>
",Preprocessing of the text & Tokenization,tokenizing word panda series problem tokenizing word panda series series named tried result also tried variation way tokenize word series
BERT training with character embeddings,"<p>Does it make sense to change the tokenization paradigm in the BERT model, to something else? Maybe just a simple word tokenization or character level tokenization?</p>
",Preprocessing of the text & Tokenization,bert training character embeddings doe make sense change tokenization paradigm bert model something else maybe simple word tokenization character level tokenization
faster spell correction in text preprocessing,"<p>While working on text data, I applied Python code for spell correction in different ways as below.</p>
<pre><code>from spellchecker import SpellChecker
import re

spell = SpellChecker()

def spell_correct(x):
   for w in spell.unknown(x.split()):
       x=re.sub(w,spell.correction(w),x)
   return x


df['twitts'] = df['twitts'].apply(lambda x :spell_correct(x))
</code></pre>
<p>and another way which i tried is:</p>
<pre><code>from textblob import TextBlob
df['twitts'] = df['twitts'].apply(lambda x :TextBlob(x).correct())
</code></pre>
<p>Both python code mentioned above are taking incredibly heavy time to execute the code for 30000 lines of data in df. ( actually execution has to be stopped manually).</p>
<p>If someone know the trick to get the spell check in more efficient way, please share or comment.</p>
",Preprocessing of the text & Tokenization,faster spell correction text preprocessing working text data applied python code spell correction different way another way tried python code mentioned taking incredibly heavy time execute code line data df actually execution ha stopped manually someone know trick get spell check efficient way please share comment
Need to reassign the value of a variable at the end of my function in Clojure,"<p>I'm new the the Clojure world and to functional programming in general. I'm trying to write a function that computes the probability of a particular list of words occurring given a vocabulary (just a list of words) and a set of probabilities (the probabilities of each of those words occurring). I'm using a simplified bag-of-words model and each outcome is assumed to be independent.</p>
<p>For example, given:</p>
<ul>
<li>Vocabulary (associated probability): sleep (0.3), dog (0.09), a (0.2), the (0.05), cow (0.17), boat (0.04), everything (0.15)</li>
<li>Sentence: <code>(list 'the 'dog 'boat)</code></li>
</ul>
<p>I want it to calculate (0.05) * (0.09) * (0.04) = 0.00018</p>
<p>I already have a function that fetches the probability of each individual word and it works as expected. I'll paste it here for reference:</p>
<pre><code>(defn lookup-probability [w outcomes probs]
  (if (not= w (first outcomes)) ;;if the current element is not equal to the word we're looking for...

    (lookup-probability w (rest outcomes) (rest probs)) ;;...keep cycling through the vocabulary

    (first probs) ;;once we find the right word, fetch the corresponding entry in the probability list
  )
)
</code></pre>
<p>Here's the part that I'm confused about:</p>
<pre><code>(def sentenceprobs '()) ;;STEP 1
(defn compute-BOW-prob [sentence vocabulary probabilities]
  (if (not(empty? sentence))
      (def sentenceprobs (conj sentenceprobs (lookup-probability (first sentence) vocabulary probabilities)) ;;STEP 2
      (compute-BOW-prob (rest sentence) vocabulary probabilities) ;;STEP 3
    )
    (product sentenceprobs) ;;STEP 4 (the product function just multiplies all the elements of a list together)
  )
)
</code></pre>
<p>Here's my general strategy:</p>
<ol>
<li>Start out by defining an empty list &quot;sentenceprobs&quot; where I will store the probabilities of each word in the sentence</li>
<li>If the sentence is nonempty, then add the probability of the first word to the list &quot;sentenceprobs&quot;</li>
<li>Recursively call the function on the rest of the sentence (minus the word we just found the probability for, ofc)</li>
<li>Once the sentence is empty, i.e. we have fetched the probability for every word, return the product of all the elements in &quot;sentenceprobs&quot;</li>
</ol>
<p>This works fine if I only want to use the function once. However, if I want to call it multiple times, sentenceprobs still contains all the probabilities from the previous call. The function will still run, but it just gives me the wrong probability (something much much tinier). <strong>So I tried to reset the value of the sentenceprobs at the very end of my function to make it &quot;reusable&quot;:</strong></p>
<pre><code>(def sentenceprobs '())
(defn compute-BOW-prob [sentence vocabulary probabilities]
  (if (not(empty? sentence))
      (def sentenceprobs (conj sentenceprobs (lookup-probability (first sentence) vocabulary probabilities))
      (compute-BOW-prob (rest sentence) vocabulary probabilities)
    )
    (product sentenceprobs)
  )
  (def sentenceprobs '()) ;; &lt;---THIS IS WHAT I ADDED
)
</code></pre>
<p>When I do this, the function doesn't return anything at all. In a sense, that is expected since the function has to return an operation on this list, so making it empty would probably mess that up. But I thought since I'm recursing and returning a value before we ever get out of the if-statement, this wouldn't be a problem. I guess I was mistaken haha.</p>
<p>I've done some poking around on the internet, and it seems like this isn't how <code>def</code> works in Clojure, but I have no idea how to fix it. Does anyone know how I could make this work? Thanks so much.</p>
",Preprocessing of the text & Tokenization,need reassign value variable end function clojure new clojure world functional programming general trying write function computes probability particular list word occurring given vocabulary list word set probability probability word occurring using simplified bag word model outcome assumed independent example given vocabulary associated probability sleep dog cow boat everything sentence want calculate already function fetch probability individual word work expected paste reference part confused general strategy start defining empty list sentenceprobs store probability word sentence sentence nonempty add probability first word list sentenceprobs recursively call function rest sentence minus word found probability ofc sentence empty e fetched probability every word return product element sentenceprobs work fine want use function however want call multiple time sentenceprobs still contains probability previous call function still run give wrong probability something much much tinier tried reset value sentenceprobs end function make reusable function return anything sense expected since function ha return operation list making empty would probably mess thought since recursing returning value ever get statement problem guess wa mistaken haha done poking around internet seems like work clojure idea fix doe anyone know could make work thanks much
Python NLP: How do I autocorrect and tokenize a body of text only to a set list of words?,"<p>Example:</p>
<pre><code>token_list = ['Allen Bradley', 'Haas', 'Fanuc']

input_string = 'I use Alln Brdly machins but dont no how to use Has ones.'

output_tokens = ['Allen Bradley', 'Haas']
</code></pre>
",Preprocessing of the text & Tokenization,python nlp autocorrect tokenize body text set list word example
"Tokenize, remove stop words using Lucene with Java","<p>I am trying to tokenize and remove stop words from a txt file with Lucene. I have this:</p>

<pre><code>public String removeStopWords(String string) throws IOException {

Set&lt;String&gt; stopWords = new HashSet&lt;String&gt;();
    stopWords.add(""a"");
    stopWords.add(""an"");
    stopWords.add(""I"");
    stopWords.add(""the"");

    TokenStream tokenStream = new StandardTokenizer(Version.LUCENE_43, new StringReader(string));
    tokenStream = new StopFilter(Version.LUCENE_43, tokenStream, stopWords);

    StringBuilder sb = new StringBuilder();

    CharTermAttribute token = tokenStream.getAttribute(CharTermAttribute.class);
    while (tokenStream.incrementToken()) {
        if (sb.length() &gt; 0) {
            sb.append("" "");
        }
        sb.append(token.toString());
    System.out.println(sb);    
    }
    return sb.toString();
}}
</code></pre>

<p>My main looks like this:</p>

<pre><code>    String file = ""..../datatest.txt"";

    TestFileReader fr = new TestFileReader();
    fr.imports(file);
    System.out.println(fr.content);

    String text = fr.content;

    Stopwords stopwords = new Stopwords();
    stopwords.removeStopWords(text);
    System.out.println(stopwords.removeStopWords(text));
</code></pre>

<p>This is giving me an error but I can't figure out why.</p>
",Preprocessing of the text & Tokenization,tokenize remove stop word using lucene java trying tokenize remove stop word txt file lucene main look like giving error figure
Find words which occur together (not necessarily next to each other) in a block of sentences. The words are the keywords of a dictionary | Python,"<p>I have a dictionary where the keywords can be any number of words and I am trying to see if I can find in a block of text whether those words are there. I have done the NLP processing with n-grams. I am trying to see if for example most or all of the words of a dictionary keyword would be in the block of text near each other.</p>
<p>e.g.</p>
<p><code>dictionary = {'countertop handle piece' : 'ID1234, 'fridge door button' : 'ID1235} </code></p>
<p><code>text = &quot;the situation in the kitchen where the handle of the countertop piece is broken in not looking very good. Blah blah blah blah. Moreover, we wanted to repair the button which is found on the yellow door of the refrigerator.&quot;</code></p>
<p>So I am looking to be able to identify the sentences, or maybe block of text that contains most or all of those individual words of the dictionary keywords. This is because as with the example above, someone might not call the fridge a refrigerator, and might also not use all the three worlds of the 'fridge door button' near each other.</p>
<p>I know stemming, and tokenization, as well as the removal of stop words and then applying n-gram matching, would help. However, I think the approach would need to be more complex?</p>
<p>I was told co-occurrence matrix would help as it will find the context of the block of text and assess if the block of text is describing a certain thing. With such a method I would be able to find the sentences where all or most of the words in the dictionary keyword occur together in a given sentence.</p>
<p>If co-occurrence or something similar is the way to go, then how do I go about solving it? Coz I tried to look into it, and I can't see how it can help me and most importantly how to implement it and actually solve the problem.</p>
<p>Any sample code/ example in your answers would be really appreciated as I am not very advanced in Python.</p>
",Preprocessing of the text & Tokenization,find word occur together necessarily next block sentence word keywords dictionary python dictionary keywords number word trying see find block text whether word done nlp processing n gram trying see example word dictionary keyword would block text near e g looking able identify sentence maybe block text contains individual word dictionary keywords example someone might call fridge refrigerator might also use three world fridge door button near know stemming tokenization well removal stop word applying n gram matching would help however think approach would need complex wa told co occurrence matrix would help find context block text ass block text describing certain thing method would able find sentence word dictionary keyword occur together given sentence co occurrence something similar way go go solving coz tried look see help importantly implement actually solve problem sample code example answer would really appreciated advanced python
"stop word removal code not working, returns same string","<p>I want to remove stop words from a string of sentences but my print function returns the exact strings with all the stopwords. Here is the code I am using where <code>chat_map['Phillips Allen']</code> is the string of sentences I parsed from a group chat.</p>
<pre class=""lang-py prettyprint-override""><code>from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
stop_words = set(stopwords.words(&quot;english&quot;))

filtered_sentences_phillip = []
for w in chat_map['Phillip Allen']:
  if w not in stop_words:
    filtered_sentences_phillip.append(w)
print(filtered_sentences_phillip)
</code></pre>
<p>This code returns this;</p>
<pre><code>['Hello?', 'Yeah, how are you?', &quot;Oh, sorry about that. I didn't know.&quot;, '(laughter) Oh because of the, the BBA thing?', &quot;you're not going to get paid any bro for this and we'll send you lunch around for the whole desk.&quot;, '100 yards...', &quot;hi guys i hope everybody's enjoying there trade this week&quot;, 'things seems to be going on well', 'later guys', &quot;don't touch it yet john it's still riding&quot;, 'but get ready any moment from now will be closing time', &quot;right now i'm having 64 pips&quot;, 'hopping to close higher', 'see you later', 'hi john i hope you closed your gbp/usd long with good pips all green like i did', 'i closed with 76pips', 'whats your position now', &quot;i've taking it short 1.6853&quot;, 'just follow and see how it will work out', &quot;so far i'm painting green&quot;, 'hi showtime 183', 'join me on skype', 'gbp/usd so far so good john', 'green 45 pips', &quot;i'm still holding john&quot;, &quot;but as you said may be it's getting near closing time&quot;, 'lets keep an eye out together', 'hi john', &quot;and how's your trading going on&quot;, &quot;hi mike and how's trading going on&quot;, 'hi steve', 'hi john', &quot;yes i'm trading today john&quot;, 'and very busy keeping an eye on it too', &quot;steve how's trading going on&quot;, &quot;hard work that's all it takes&quot;, &quot;i'd love it if you guys will be my friends on skype&quot;, 'i like having fellow traders as friends on skype', 'usd/cad positioned at 1.0939 short', 'eur/chf positioned at 1.2202 long', 'IMO', 'later guys', 'pip watching time', 'hi john', 'been a long time', 'eur/gbp was long', 'but now about to go long any time from now', 'long can still hold on for a while and lets see what the next candle will say at 4h time frame', 'sorry was short and about to go long', 'short can still hold on till the next candle at 4h time frame', 'long position expected', 'eur/gbp going long already', 'how do you see john', 'learn to control your emotions steve this is very important', 'control of emotions is part of success and failure', 'because trading to emotions can lead to and often does lead to wrong decisions', 'making entry and taking exit at the wrong time', 'Hi, John', 'Yes, I remember.', &quot;I'd prefer to keep the actual data&quot;, 'Hello mate? You all set?', &quot;Right listen we've had a couple of words with them, you want them lower right?&quot;, &quot;Alright okay, alright listen, we've had a couple words with them. You want them lower, right?&quot;, 'Glad to hear that you liked it', 'Did you hear last news?', 'Agree. Very promising', 'Happy birthday!']
</code></pre>
<p>Any idea what's wring please?</p>
",Preprocessing of the text & Tokenization,stop word removal code working return string want remove stop word string sentence print function return exact string stopwords code using string sentence parsed group chat code return idea wring please
I don&#39;t want to remove stop words by splitting words into letters,"<p>I am writing this piece of code to remove stop words from my text.</p>
<p><strong>Problem - This code works perfectly for removing stopwords but the problem arises when words like ant, ide is present in my text as it removes both words ant and ide because ant is present in important, want and ide is present in side.</strong> But I don't want to split words into a letter to remove stopwords.</p>
<pre><code>            String sCurrentLine;
            List&lt;String&gt; stopWordsofwordnet=new ArrayList&lt;&gt;();
            FileReader fr=new FileReader(&quot;G:\\stopwords.txt&quot;);
            BufferedReader br= new BufferedReader(fr);
                while ((sCurrentLine = br.readLine()) != null)
                {
                    stopWordsofwordnet.add(sCurrentLine);
                }
                //out.println(&quot;&lt;br&gt;&quot;+stopWordsofwordnet);
            List&lt;String&gt; wordsList = new ArrayList&lt;&gt;();
            
            String text = request.getParameter(&quot;textblock&quot;);
            text=text.trim().replaceAll(&quot;[\\s,;]+&quot;, &quot; &quot;);
            String[] words = text.split(&quot; &quot;);

//            wordsList.addAll(Arrays.asList(words));
                for (String word : words) {
                wordsList.add(word);
                }
            out.println(&quot;&lt;br&gt;&quot;);

            //remove stop words here from the temp list
            for (int i = 0; i &lt; wordsList.size(); i++) 
            {
            // get the item as string
            for (int j = 0; j &lt; stopWordsofwordnet.size(); j++) 
            {
            if (stopWordsofwordnet.get(j).contains(wordsList.get(i).toLowerCase())) 
            {
                out.println(wordsList.get(i)+&quot;&amp;nbsp;&quot;);
                wordsList.remove(i);
                i--;
                break;
            }
            }
            }
            out.println(&quot;&lt;br&gt;&quot;);
            for (String str : wordsList) {
            out.print(str+&quot; &quot;);
            }
</code></pre>
",Preprocessing of the text & Tokenization,want remove stop word splitting word letter writing piece code remove stop word text problem code work perfectly removing stopwords problem arises word like ant ide present text remove word ant ide ant present important want ide present side want split word letter remove stopwords
Python: How to Use ML for Extracting specific Data from various PDF,"<p>Need assistance with a ML project I am currently trying to create.</p>
<p>I receive a lot of invoices from a lot of different suppliers - all in their own unique layout. I need to extract 4 key elements from the invoices. These 4 elements are all located in a table/line/sections items for all the different invoices.</p>
<p>Elements are :</p>
<ol>
<li>Bill No / Invoice No</li>
<li>Date of Billing</li>
<li>SGST, CGST applied</li>
<li>Total Amount/ Total Amount Due/ Total Bill Due</li>
</ol>
<p>Input File:
<a href=""https://drive.google.com/file/d/1Kmg5QYQrnnhZ05wsGA0jBkCC1FJOUfbd/view"" rel=""nofollow noreferrer"">Please find the input file link</a></p>
<p>I started this project with a template approach, based on regular expressions. This, however, was not scalable at all and I ended up with tons of different rules.</p>
<p>What I have applied so far:</p>
<pre><code>import textract
text = textract.process('datafile.pdf')

processed_text=text.lower()
processed_text= processed_text.replace('\n',' ')
processed_text= processed_text.replace('*',' ')
processed_text= processed_text.replace('/',' ')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
text_tokens = word_tokenize(processed_text)
text_without_stopwords = [word for word in text_tokens if not word in stopwords.words{}]
processed_text = &quot; &quot;.join(text_without_stopwords)

regex_invoice_no = re.compile(r&quot;Invoice No\s*:\s*(\d+)&quot;)
regex_date = re.compile(r&quot;Date of Billing\s*:\s*(\d\d\.\d\d\.\d{4})&quot;)
regex_SGST = re.compile(r&quot;SGST:\s*:\s*(\d+)&quot;)
regex_CGST = re.compile(r&quot;CGST:\s*:\s*(\d+)&quot;)
regex_total_due = re.compile(r&quot;Total Amount Due:\s*:\s*(\d+)&quot;)

invoice_no= re.search(regex_invoice_no, output_string.getvalue()).group(1),
issue_date= re.search(regex_date, output_string.getvalue()).group(1),
sgst= re.search(regex_SGST, output_string.getvalue()).group(1),
cgst= re.search(regex_CGST, output_string.getvalue()).group(1),
amount= re.search(regex_total_due, output_string.getvalue()).group(1)}

print(invoice_no, issue_date, sgst,cgst, amount)
</code></pre>
<p><strong>Output:</strong></p>
<p>For each invoice like the one above, I need the output for each PDF. This could for example be something like this:</p>
<pre><code>{
&quot;Invoice No&quot;:&quot;INXXXXXXXX&quot;,
&quot;SGST&quot;:&quot;260&quot;,
&quot;CGST&quot;:&quot;290&quot;,
&quot;Date of Billing&quot;:&quot;3-&quot;,
&quot;Total Amount Due&quot;:&quot;258.93&quot;
}
</code></pre>
",Preprocessing of the text & Tokenization,python use ml extracting specific data various pdf need assistance ml project currently trying create receive lot invoice lot different supplier unique layout need extract key element invoice element located table line section item different invoice element bill invoice date billing sgst cgst applied total amount total amount due total bill due input file please find input file link started project template approach based regular expression however wa scalable ended ton different rule applied far output invoice like one need output pdf could example something like
CountVectorizer to build dictionary for removing extra words,"<p>I have a list of sentences within a pandas column:</p>
<pre><code>sentence
I am writing on Stackoverflow because I cannot find a solution to my problem.
I am writing on Stackoverflow. 
I need to show some code. 
Please see the code below
</code></pre>
<p>I would like to run some text mining and analysis through them, for example to get the word frequency.
To do it, I am using this approach:</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer
# list of text documents
text = [&quot;I am writing on Stackoverflow because I cannot find a solution to my problem.&quot;]
vectorizer = CountVectorizer()
# tokenize and build vocab
vectorizer.fit(text)
</code></pre>
<p>How can I apply it to my column, removing extra stopwords after building the vocabulary?</p>
",Preprocessing of the text & Tokenization,countvectorizer build dictionary removing extra word list sentence within panda column would like run text mining analysis example get word frequency using approach apply column removing extra stopwords building vocabulary
How do I tokenize a text data into words and sentences without getting a type error,"<p>My end goal is to use NER models to identify custom entities. Before doing this, I am tokenizing the text data into words and sentences. I have a folder of text files(.txt) that I opened and read into Jupyter using the os library. After reading the text file, whenever I try to tokenize the text files, I get a type error. Could please advise on what I am doing wrong? My code is below, Thanks.</p>
<pre><code>import os
outfile = open('result.txt', 'w')
path = &quot;C:/Users/okeke/Documents/Work flow/IT Text analytics Project/Extract/Dubuque_text-nlp&quot;
files = os.listdir(path)
for file in files:
    outfile.write(str(os.stat(path + &quot;/&quot; + file).st_size) + '\n')

outfile.close()
</code></pre>
<p>This code runs fine, whenever I run the outfile, I get this below</p>
<pre><code>outfile
&lt;_io.TextIOWrapper name='result.txt' mode='w' encoding='cp1252'&gt;
</code></pre>
<p>Next, tokenization.</p>
<pre><code>from nltk.tokenize import sent_tokenize, word_tokenize 
sent_tokens = sent_tokenize(outfile)
print(outfile)

word_tokens = word_tokenize(outfile)
print(outfile
</code></pre>
<p>But then I get an error after the running the code above. Check for error below</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-22-62f66183895a&gt; in &lt;module&gt;
      1 from nltk.tokenize import sent_tokenize, word_tokenize
----&gt; 2 sent_tokens = sent_tokenize(outfile)
      3 print(outfile)
      4 
      5 #word_tokens = word_tokenize(text)

~\AppData\Local\Continuum\anaconda3\envs\nlp_course\lib\site-packages\nltk\tokenize\__init__.py in sent_tokenize(text, language)
     93     &quot;&quot;&quot;
     94     tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))
---&gt; 95     return tokenizer.tokenize(text)
     96 
     97 # Standard word tokenizer.
TypeError: expected string or bytes-like object
</code></pre>
",Preprocessing of the text & Tokenization,tokenize text data word sentence without getting type error end goal use ner model identify custom entity tokenizing text data word sentence folder text file txt opened read jupyter using library reading text file whenever try tokenize text file get type error could please advise wrong code thanks code run fine whenever run outfile get next tokenization get error running code check error
does gensim Word2Vec include neighboring sentences in context?,"<p>I am working to calculate similarities of labels of materials. Where each label might have 1-10 words in it. I am using gensim word2vec to find cosine similarities.</p>
<p>My approach is simply to treat each label as a 'sentence' and tokenize each word.<br />
<strong>example:</strong><br />
<strong>labels</strong> = ['wooden desk cherry', 'long sleeve shirt cotton',..]</p>
<p><strong>sentences</strong> = [['wooden', 'desk', 'cherry'], ['long', 'sleeve', 'shirt', 'cotton'],..]</p>
<p>My question is does word2vec take neighboring sentences into its context. for example if i am using a window = 2, when looking at words around 'long', will 'cherry' be included or only 'sleeve'.</p>
<p>If neighboring sentences are considered is there a way to only consider words within the target words sentence.</p>
<p>Thanks for any help. I have read the Word2Vec documentation and couldn't find any information about this.</p>
",Preprocessing of the text & Tokenization,doe gensim word vec include neighboring sentence context working calculate similarity label material label might word using gensim word vec find cosine similarity approach simply treat label sentence tokenize word example label wooden desk cherry long sleeve shirt cotton sentence wooden desk cherry long sleeve shirt cotton question doe word vec take neighboring sentence context example using window looking word around long cherry included sleeve neighboring sentence considered way consider word within target word sentence thanks help read word vec documentation find information
Creating word pairs from phrases,"<p>Is there any way to get the possible word pairs from word phrases? I have a list of word phrases and I need to find all the possible word pair combinations from it. I can normally tokenize the phrases to get the words and then find all the possible pairs. But if I want to have some pair which consists of one phrase and one word of another phrase then is it possible to do it?
For example:
The list of phrases can be something as</p>
<pre><code>['influenza virus', 'elderberry liquid extract', 'previous study', 'elderberry fruit', 'berry good']
</code></pre>
<p>The possible pairs should also contain these</p>
<pre><code>[(influenza virus,elderberry),(influenza virus, liquid),(influenza virus, extract).....]
</code></pre>
<p>Is there any way to do this? I have done the word pairings for individual words. I am looking for something that can do the above.</p>
",Preprocessing of the text & Tokenization,creating word pair phrase way get possible word pair word phrase list word phrase need find possible word pair combination normally tokenize phrase get word find possible pair want pair consists one phrase one word another phrase possible example list phrase something possible pair also contain way done word pairing individual word looking something
Natural language interface to database using python,"<p>I am working on a project related to natural language process using python. In my project, I have my database and some data related to finance and I have to create my own page and if I give a question for example <code>what is the net income of apple?</code> then I have to transfer my question into a query like <code>select net_income from finance where name = apple</code> and get the answer to my question.</p>

<p>I've completed tokenizing the given question and removed stop words from it. I am unable to convert the tokens and add them into the query. I am very thankful if anyone could help me by giving your ideas?</p>
",Preprocessing of the text & Tokenization,natural language interface database using python working project related natural language process using python project database data related finance create page give question example transfer question query like get answer question completed tokenizing given question removed stop word unable convert token add query thankful anyone could help giving idea
Does keras-tokenizer perform the task of lemmatization and stemming?,"<p>Does keras tokenizer provide the functions such as stemming and lemmetization? If it does, then how is it done? Need an intuitive understanding. Also, what does <code>text_to_sequence</code> do in that?</p>
",Preprocessing of the text & Tokenization,doe kera tokenizer perform task lemmatization stemming doe kera tokenizer provide function stemming lemmetization doe done need intuitive understanding also doe
What is the input format of fastText and why does my model doesn&#39;t give me a meaningful similar output?,"<p>My goal is to find similarities between a word and a document. For example, I want to find the similarity between &quot;new&quot; and a document, for simplicity, say &quot;Hello World!&quot;.</p>
<p>I used word2vec from gensim, but the problem is it does not find the similarity for an unseen word. Thus, I tried to use fastText from gensim as it can find similarity for words that are out of vocabulary.</p>
<p>Here is a sample of my document data:</p>
<pre><code>[['This', 'is', 'the', 'only', 'rule', 'of', 'our', 'household'],
 ['If',
  'you',
  'feel',
  'a',
  'presence',
  'standing',
  'over',
  'you',
  'while',
  'you',
  'sleep',
  'do'],
 ['NOT', 'open', 'your', 'eyes'],
 ['Ignore', 'it', 'and', 'try', 'to', 'fall', 'asleep'],
 ['This',
  'may',
  'sound',
  'a',
  'bit',
  'like',
  'the',
  'show',
  'Bird',
  'Box',
  'from',
  'Netflix']]
</code></pre>
<p>I simply train data like this:</p>
<pre><code>from gensim.models.fasttext import FastText

model = FastText(sentences_cleaned)
</code></pre>
<p>Consequently, I want to find the similarity between say, &quot;rule&quot; and this document.</p>
<p><code>model.wv.most_similar(&quot;rule&quot;)</code></p>
<p>However, fastText gives me this:</p>
<pre><code>[('the', 0.1334390938282013),
 ('they', 0.12790171802043915),
 ('in', 0.12731242179870605),
 ('not', 0.12656228244304657),
 ('and', 0.11071767657995224),
 ('of', 0.08563747256994247),
 ('I', 0.06609072536230087),
 ('that', 0.05195673555135727),
 ('The', 0.002402491867542267),
 ('my', -0.009009800851345062)]
</code></pre>
<p>Obviously, it must have &quot;rule&quot; as the top similarity since the word &quot;rule&quot; appears in the first sentence of the document. I also tried stemming/lemmatization, but it doesn't work either.</p>
<p>Was my input format correct? I've seen lots of documents are using .cor or .bin format and I don't know what are those.</p>
<p>Thanks for any reply!</p>
",Preprocessing of the text & Tokenization,input format fasttext doe model give meaningful similar output goal find similarity word document example want find similarity new document simplicity say hello world used word vec gensim problem doe find similarity unseen word thus tried use fasttext gensim find similarity word vocabulary sample document data simply train data like consequently want find similarity say rule document however fasttext give obviously must rule top similarity since word rule appears first sentence document also tried stemming lemmatization work either wa input format correct seen lot document using cor bin format know thanks reply
Can not find file preprocessing.py in OpenNMT package,"<p>I used to use file preprocessing.py of OpenNMT package <a href=""https://github.com/OpenNMT/OpenNMT-py"" rel=""nofollow noreferrer"">https://github.com/OpenNMT/OpenNMT-py</a> to preprocess my raw data. Unfortunately recently I can not find that file in that package any more. Do you know where I can find that file or if you still have it, could you please send it to my email: tuankstn@gmail.com.
Thank you so much.</p>
",Preprocessing of the text & Tokenization,find file preprocessing py opennmt package used use file preprocessing py opennmt package preprocess raw data unfortunately recently find file package know find file still could please send email tuankstn gmail com thank much
How can I add a specific substring to tokenize on in spaCy?,"<p>I am using <code>spaCy</code> to tokenize a string, and the string is likely to contain a specific substring.  If the substring is present, I would like <code>spaCy</code> to treat the substring as a token, regardless of any other rules it has.  I would like to keep all other rules intact.  Is this possible?</p>
<p>To provide a concrete example, suppose the substring of interest is <code>'banana'</code>; I want <code>'I like bananabread.'</code> to be tokenized as <code>['I', 'like', 'banana', 'bread', '.']</code>.</p>
<p>Where do I go from here (keeping in mind that I would like to keep the rest of the tokenizer rules intact)?  I have tried adding <code>'banana'</code> to the prefixes, suffixes, and infixes, with no success.</p>
",Preprocessing of the text & Tokenization,add specific substring tokenize spacy using tokenize string string likely contain specific substring substring present would like treat substring token regardless rule ha would like keep rule intact possible provide concrete example suppose substring interest want tokenized go keeping mind would like keep rest tokenizer rule intact tried adding prefix suffix infix success
Tokenize Sentences or Tweets with Emoji Skin Tone Modifiers,"<p>I want to tokenize a tweet containing multiple emojis and they are not space-separated. I tried both <code>NLTK TweetTokenizer</code> and <code>Spacy</code> but they fail to tokenize Emoji Skin Tone Modifiers. This needs to be applied to a huge dataset so performance might be an issue. Any suggestions?</p>
<p><strong>You may need to use Firefox or Safari to see the exact color tone emoji because Chrome sometimes fails to render it!</strong></p>
<pre><code># NLTK
from nltk.tokenize.casual import TweetTokenizer
sentence = &quot;I'm the most famous emoji 😂😂😂 but what about 👍 and 🚗👍🏼😂👍🏿&quot;
t = TweetTokenizer()
print(t.tokenize(sentence))

# Output
[&quot;I'm&quot;, 'the', 'most', 'famous', 'emoji', '😂', '😂', '😂', 'but', 'what', 'about', '👍', 'and', '🚗', '👍', '🏼', '😂', '👍', '🏿']
</code></pre>
<p>And</p>
<pre><code># Spacy
import spacy
nlp = spacy.load(&quot;en_core_web_sm&quot;)
sentence = nlp(&quot;I'm the most famous emoji 😂😂😂 but what about 👍 and 🚗👍🏼😂👍🏿&quot;)
print([token.text for token in sentence])

Output
['I', &quot;'m&quot;, 'the', 'most', 'famous', 'emoji', '😂', '😂', '😂', 'but', 'what', 'about', '👍', 'and', '🚗', '👍', '🏼', '😂', '👍', '🏿']
</code></pre>
<p>Expected Output</p>
<pre><code>[&quot;I'm&quot;, 'the', 'most', 'famous', 'emoji', '😂', '😂', '😂', 'but', 'what', 'about', '👍', 'and', '🚗', '👍🏼', '😂', '👍🏿']
</code></pre>
",Preprocessing of the text & Tokenization,tokenize sentence tweet emoji skin tone modifier want tokenize tweet containing multiple emojis space separated tried fail tokenize emoji skin tone modifier need applied huge dataset performance might issue suggestion may need use firefox safari see exact color tone emoji chrome sometimes fails render expected output
Extracting sentence from a long string using start and end index of a word in python,"<p>Suppose I have a long text.</p>
<pre><code>doc = &quot;I was chasing a dog. I ran after it for a long time. 
... 
... 
... 
However, after running for about an hour, I caught the dog&quot;
</code></pre>
<p>After some processing and calculation, I know that the word 'time' has start index <code>i</code> i.e <code>doc[i:i+4]=&quot;time&quot;</code>. My question is, is there an efficient way by which I can extract the sentence from the <code>doc</code> variable that consists of the word doc?
In this case, the sentence I should get is</p>
<pre><code>I ran after it for a long time. 
</code></pre>
<p>So, using the start and index of a word in a long string, is it possible to extract the sentence that contains that word? I do not want to sentence tokenize the doc and iterate over each sentence and check if it contains the word. The main reason for it being I might have a lots of query words and hence do not want to have to iterate over each sentence every time I am querying with a word.</p>
",Preprocessing of the text & Tokenization,extracting sentence long string using start end index word python suppose long text processing calculation know word time ha start index e question efficient way extract sentence variable consists word doc case sentence get using start index word long string possible extract sentence contains word want sentence tokenize doc iterate sentence check contains word main reason might lot query word hence want iterate sentence every time querying word
How to find all word combinations from a string and a list of synonyms,"<p>I have a list of words where some of these words have a synonym</p>

<pre><code>words = ['cat','under','table']
</code></pre>

<p><code>feline;cat</code></p>

<p><code>below;down;under;beneath</code></p>

<p><code>table;bench;board</code></p>

<p>And I need to find different list combinations with these synonyms.</p>

<p>That example would return:</p>

<pre><code>['cat','under','table'],['feline','under','table'],['cat','below','table'],['feline','below','table'] ...
</code></pre>

<p>I'm wondering if there is an algorithm that solves this problem. Or how can I deal with it? I tried using a nested loop approach but i have problems because the length of the list of words and list of synonym are variable</p>
",Preprocessing of the text & Tokenization,find word combination string list synonym list word word synonym need find different list combination synonym example would return wondering algorithm solves problem deal tried using nested loop approach problem length list word list synonym variable
How to deal with large amount of sentences with gensim word2vec?,"<p>I have a very large amount of sentences, the problem is i cannot load them all at once in memory, specially when i tokenize the sentences and split them into list of words my RAM goes full really fast.</p>
<p>but i couldn't find any example of how can i train the gensim word2vec with batches, meaning in each epoch i guess i have to somehow load batches of data from disk, tokenize them and give it to the model then unload it and load the next batch.</p>
<p>how can i overcome this problem and train a word2vec model when i don't have enough ram to load all the sentences (not even 20% of them).</p>
<p>my sentences are basically in a text file, each line representing a sentence.</p>
",Preprocessing of the text & Tokenization,deal large amount sentence gensim word vec large amount sentence problem load memory specially tokenize sentence split list word ram go full really fast find example train gensim word vec batch meaning epoch guess somehow load batch data disk tokenize give model unload load next batch overcome problem train word vec model enough ram load sentence even sentence basically text file line representing sentence
Concatenate 2 consecutive words in a list,"<p>I have multiple list of word tokens as follows:</p>
<pre><code>sentence =  &quot;Pay ninety dollar in ninety days&quot;
a = ['Pay','ninety','dollar,'in','ninety','days']
</code></pre>
<p>I want to find and concatenate 2 consecutive words:</p>
<pre><code>x = ['ninety','days']
result = ['Pay','ninety','dollar','in','ninety_days'] or &quot;Pay ninety dollar in ninety_days&quot;
</code></pre>
",Preprocessing of the text & Tokenization,concatenate consecutive word list multiple list word token follows want find concatenate consecutive word
NLP text preprocessing,"<p>I am doing a small project on amazon product reviews. During text pre-processing self is unable to process the following things from the text.</p>
<p><strong>Problem 1:</strong> user has written a review like this</p>
<blockquote>
<p>The laptop is over all very good.: But few  points to note:</p>
<ol>
<li>The sound is not that much good</li>
<li>camera is ok</li>
</ol>
</blockquote>
<p>After processing this text using <code>re.sub</code> expression &amp; lemmatization still the point numbers are present in the output along with colon.</p>
<pre><code>text = re.sub('[^a-zA-Z]', ' ', X['long_review'][i]) 
text = re.sub(r'\[[0-9]*\]', ' ', X['long_review'][i])  
text = re.sub(r'\s+', ' ', X['long_review'][i]) 
text = re.sub(r'\d', ' ', X['long_review'][i]) 
## below code to remove the emoji in the sentences
text = re.sub('[(\U0001F600-\U0001F92F|\U0001F300-\U0001F5FF|\U0001F680-\U0001F6FF|\U0001F190-\U0001F1FF|\U00002702-\U000027B0|\U0001F926-\U0001FA9F|\u200d|\u2640-\u2642|\u2600-\u2B55|\u23cf|\u23e9|\u231a|\ufe0f)]+', '' , X['long_review'][i])
</code></pre>
<p>The output looks like this:</p>
<blockquote>
<p>laptop good.: point note: 1. sound much good.</p>
</blockquote>
<p><strong>Problem 2:</strong> input user review</p>
<blockquote>
<p>Core i7 10th gen in 60k! Are you kidding me!</p>
</blockquote>
<p>The output:</p>
<blockquote>
<p>core i7 10th gen 60k! kidding me!</p>
</blockquote>
<p>Required help to process these kinds of texts</p>
",Preprocessing of the text & Tokenization,nlp text preprocessing small project amazon product review text pre processing self unable process following thing text problem user ha written review like laptop good point note sound much good camera ok processing text using expression lemmatization still point number present output along colon output look like laptop good point note sound much good problem input user review core th gen k kidding output core th gen k kidding required help process kind text
how to get string with maximum of three words in a list of strings,"<p>I have a list of different strings with different number of words in each string, I want to be able to get string with maximum of three words from the list,</p>
<p>This is the list of strings below</p>
<pre><code>['organised crimes perpetrated globally using sltd .“',
 'major system supporting arm manned',
 'united states national central bureau',
 'interpol police global system',
 'national central bureau',
 'global community .“',
 'tracking system domiciled',
 'interpol global system',
 'nigerian sltd detection',
 'successfully yielded result',
 'statement made available',
 'mr muhammad babandede',
 'lost travel documents',
 'deliberate acts leading']
</code></pre>
<p>This is the output I'm expecting Expected output:</p>
<pre><code>['national central bureau',
 'global community .“',
 'tracking system domiciled',
 'interpol global system',
 'nigerian sltd detection',
 'successfully yielded result',
 'statement made available',
 'mr muhammad babandede',
 'lost travel documents',
 'deliberate acts leading']
</code></pre>
<p>Thanks in advance</p>
",Preprocessing of the text & Tokenization,get string maximum three word list string list different string different number word string want able get string maximum three word list list string output expecting expected output thanks advance
Using regular expression as a tokenizer?,"<p>I am trying tokenize my corpus into sentences. I tried using spacy and nltk and they did not work well since my text is a bit tricky. Below is an artificial sample I made which covers all the edge cases I know:</p>
<pre><code>It is relevant to point that Case No. 778 - Martin H. v. The Woods, it was mentioned that death
 to one cannot be generalised. However, the High Court while enhancing the same from life to 
death, in our view,has not assigned adequate and acceptable reasons. In our opinion, it is not a 
rarest of rare case where extreme penalty of death is called for instead sentence of 
imprisonment for life as ordered by the trial Court would be appropriate.15) In the light of the 
above discussion, while maintaining the conviction of the appellant-accused for the offence under Section 302. IPC, 
award of extreme penalty of death by the High Court is set aside and we restore the sentence of
 life imprisonment as directed by the trial Court.
</code></pre>
<p>How I would like the sentence to be tokenized:</p>
<pre><code>1) It is relevant to point that Case No. 778 - Martin H. v. The Woods, it was mentioned that death to one cannot be generalised.
2) However, the High Court while enhancing the same from life to death, in our view,has not assigned adequate and acceptable reasons.
3) In our opinion, it is not a rarest of rare case where extreme penalty of death is called for instead sentence of imprisonment for life as ordered by the trial Court would be appropriate.
4)15. In the light of the above discussion, while
 maintaining the conviction of the appellant-accused for the offence under Section 302. IPC, 
award of extreme penalty of death by the High Court is set aside and we restore the sentence of
 life imprisonment as directed by the trial Court.

</code></pre>
<p>Here is the regular expression I am using:</p>
<pre><code>sent = re.split('(?&lt;!\w\.\w.)(?&lt;![A-Z]\.)(?&lt;![1-9]\.)(?&lt;![1-9]\.)(?&lt;![v]\.)(?&lt;![vs]\.)(?&lt;=\.|\?) ',j)

</code></pre>
<p>I am not really versed with regular expressions but I am manually putting in conditions for example <code>v</code> and <code>vs</code>.  I am also ignoring if before te period there is a number for example <code>15.</code></p>
<p>Problems I am facing:</p>
<ol>
<li>If there is no gap between two sentences it does not split properly.</li>
<li>I also would like it to ingore the period if the word before it is capitalized. For example <code>No.</code> or <code>Mr.</code></li>
</ol>
",Preprocessing of the text & Tokenization,using regular expression tokenizer trying tokenize corpus sentence tried using spacy nltk work well since text bit tricky artificial sample made cover edge case know would like sentence tokenized regular expression using really versed regular expression manually putting condition example also ignoring te period number example problem facing gap two sentence doe split properly also would like ingore period word capitalized example
Converting misspelled words to actual words,"<p>I'm doing a project on NLP, I want to find words most prominent in a corpus, when I count the frequencies of all the words, words like <code>&quot;thi&quot;, &quot;realli&quot;, &quot;happi&quot;, &quot;babi&quot;</code> emerges, In my processing pipeline, No matter if I lemmatize or stemm, same result, can someone tell me how to turn these into actual words or something about lemmatizing or stemming.</p>
",Preprocessing of the text & Tokenization,converting misspelled word actual word project nlp want find word prominent corpus count frequency word word like emerges processing pipeline matter lemmatize stemm result someone tell turn actual word something lemmatizing stemming
Plotting the most frequent words in a text (removing stop words),"<p>I'm trying to plot the most frequent words in a DataFrame that has the following columns <code>tags</code> and <code>text</code>.</p>
<p>This DataFrame is a mix of <code>True</code> and <code>Fake</code> news. True news has a value of <code>0</code> in the DataFrame and Fake news has a value of <code>1</code>:</p>
<pre><code>                      tags              text

kt-rOnMBAC-oqacdW1Q-    1   On Monday night, Donald Trump traveled to West...
k9-rOnMBAC-oqacdW1Q-    1   Donald Trump is very busy right now trying to ...
lN-rOnMBAC-oqacdW1Q-    1   By now, we all know that upon having emergency...
ld-rOnMBAC-oqacdW1Q-    1   Donald Trump s horrible decisions and disgusti...
lt-rOnMBAC-oqacdW1Q-    1   It s tough sometimes to imagine that Donald Tr...
... ... ...
Y-CvOnMBAC-oqacdBwEJ    0   BRUSSELS (Reuters) - NATO allies on Tuesday we...
ZOCvOnMBAC-oqacdBwEJ    0   LONDON (Reuters) - LexisNexis, a provider of l...
ZeCvOnMBAC-oqacdBwEJ    0   MINSK (Reuters) - In the shadow of disused Sov...
ZuCvOnMBAC-oqacdBwEJ    0   MOSCOW (Reuters) - Vatican Secretary of State ...
Z-CvOnMBAC-oqacdBwEJ    0   JAKARTA (Reuters) - Indonesia will buy 11 Sukh...
</code></pre>
<p>I already plot the data using the following code:</p>
<pre><code>import nltk
from nltk import tokenize

tokenSpace = tokenize.WhitespaceTokenizer()
def counter(text, columnText, quantity):
    allWords = ' '.join([text for text in text[columnText].astype('str')])
    tokenPhrase = tokenSpace.tokenize(allWords)
    frequency = nltk.FreqDist(tokenPhrase) 
    dfFrequency = pd.DataFrame({&quot;Word&quot;: list(frequency.keys()), &quot;Frequency&quot;: list(frequency.values())}) 
    
    dfFrequency = dfFrequency.nlargest(columns = &quot;Frequency&quot;, n = quantity)
    plt.figure(figsize=(12,8))
    ax = sns.barplot(data = dfFrequency, x = &quot;Word&quot;, y = &quot;Frequency&quot;, palette=&quot;deep&quot;)
    ax.set(ylabel = &quot;Count&quot;)
    plt.xticks(rotation='horizontal')
    plt.show()
</code></pre>
<p>When i call the function:</p>
<pre><code>counter(df2[df2['tags'] == 1], 'text', 20)
</code></pre>
<p>I get the following Plot:</p>
<p><a href=""https://i.sstatic.net/tPTqd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tPTqd.png"" alt=""most frequent words for fake news"" /></a></p>
<p>So now my question is:</p>
<p>Is there a way to get as a result only the significant words? (Like skip low-meaning words. Such as (this, the, to...))</p>
",Preprocessing of the text & Tokenization,plotting frequent word text removing stop word trying plot frequent word dataframe ha following column dataframe mix news true news ha value dataframe fake news ha value already plot data using following code call function get following plot question way get result significant word like skip low meaning word
Text Normalizer on large text files TOO SLOW (Python),"<p>I am working on a text normalizer. It works just fine with small text files but takes a very long time with large text files such as 5 MB or more.</p>
<p>Is there anything to change in the code to make it run faster on large text files?
My guess would be something in the __preprocess(tmp) and __prenormalise(text)?</p>
<p>Thank you for your time</p>
<pre><code># -*- coding: utf-8 -*-
import re
import sys
import json
import os
import codecs
import copy
from num2words import num2words
from text_unidecode import unidecode
import argparse

class TextNormaliser:
    def __init__(self, debug=False):
        &quot;&quot;&quot;
        Args:
            debug (bool, optional): Debug mode
        &quot;&quot;&quot;
        self.debug = debug
        self.abbreviations = {}
        self.acronyms = {}
        self.currencies = {}

        self.months = [
            'january', 'february', 'march', 'april', 'may', 'june', 'july',
            'august', 'september', 'october', 'november', 'december']

        self.number_scale = [
            'thousand', 'thousands', 'million', 'millions',
            'billion', 'billions', 'trillion', 'trillions']

        path = os.path.dirname(os.path.realpath(__file__))
        with open(os.path.join(path, 'resources', 'abbreviations.json')) as jf:
            self.abbreviations = json.load(jf)
        with open(os.path.join(path, 'resources', 'acronyms.json')) as jf:
            self.acronyms = json.load(jf)
        with open(os.path.join(path, 'resources', 'currencies.json')) as jf:
            self.currencies = json.load(jf)
        with open(os.path.join(path, 'resources', 'domains.json')) as jf:
            self.domains = json.load(jf)

    def normalise(self, text):
        &quot;&quot;&quot;Normalise text.

        The function covers numbers, email addresses, ascii characters, etc.

        Args:
            text (str): Input string

        Returns:
            textn (srt): Normalised text
            tokens ([tuples]): List of tuples to track back normalisation

        Examples:
            &gt;&gt;&gt; textn, tokens = tn.normalise(&quot;My email is, a@b.com.&quot;)
            tokens: (Original, Normalised, Display)
            my email is a at b dot com
            [('My', ['my'], 'My'), ('email', ['email'], 'email'),
            ('is,', ['is'], 'is'),
            ('a@b.com.', ['a', 'at', 'b', 'dot', 'com'], 'a@b.com')]
        &quot;&quot;&quot;
        return self.__normalise(text)

    def normalise_file(self, path):
        &quot;&quot;&quot;Normalise text from a file.

        The function covers numbers, email addresses, ascii characters, etc.

        Args:
            path (str): Path to a file

        Returns:
            textn (srt): Normalised text, or None if file does not exists
            tokens ([tuples]): List of tuples to track back normalisation,
                or None if file doesnot exists

        Raises:
            Exception: If file cannot be read

        Examples:
            &gt;&gt;&gt; textn = tn.normalise_file('./trans.txt')
        &quot;&quot;&quot;
        try:
            if os.path.isfile(path):
                with codecs.open(path, encoding='utf-8') as f:
                        return self.__normalise(f.readline())
            else:
                return None, None
        except Exception as e:
            raise Exception('ERR Normalise_file: {}'.format(e))

    def __normalise(self, text):
        text = self.__prenormalise(text)
        tmp = []
        for idx, t in enumerate(text.split()):
            tmp.append((t, idx))
        original = copy.deepcopy(tmp)

        # Preprocessing
        tokens = self.__preprocess(tmp)
        
        # Convert to result format
        ret_text, ret_tokens = self.__generate_results(original, tokens)
        return ret_text, ret_tokens

    def __prenormalise(self, text):
        text = text.replace('\n', '').replace('\r', '')
        text = re.sub(r'\b\?\b', ' ', text)
        text = re.sub(r'\b\!\b', ' ', text)
        text = re.sub(r'\b\&quot;\b', ' ', text)
        text = re.sub(r'\b\--\b', ' ', text)

        chars = list(text)
        for i, c in enumerate(chars):
            if i &lt; 1 or i &gt; len(chars)-1:
                continue
            if c == ',':
                if not(chars[i-1].isnumeric() and
                   chars[i-1].isnumeric()):
                    chars[i] = ', '
            text = ''.join(chars)
        return text

    def __preprocess(self, tokens):
        # Remove spaces and some special encoding
        for idx, t in enumerate(tokens):
            i = t[1]
            t = t[0]
            t = t.replace('&amp;amp;', '&amp;')

            hints = ['[Music]', '[Laughter]', '[Applause]']
            for hint in hints:
                t = t.replace(hint, '')

            del tokens[idx]
            tokens.insert(idx, (t.strip(), i))

        # Remove last dot
        if len(tokens):
            if tokens[-1][0].endswith('.'):
                i = tokens[-1][1]
                t = tokens[-1][0]
                del tokens[-1]
                tokens.append((t[:-1], i))

        return tokens
    
   
    def __rstrip(self, token):
        for i in range(5):
            if len(token):
                if token[-1] in [',', '.', ';', '!', '?', ':', '&quot;']:
                    token = token[:-1]
                else:
                    break
        return token

    def __lstrip(self, token):
        for i in range(5):
            if len(token):
                if token[0] in [',', '.', ';', '!', '?', ':', '&quot;', '\'']:
                    token = token[1:]
                else:
                    break
        return token


    def __generate_results(self, original, normalised):
        words = []
        for t in normalised:
            if len(t[0]):
                words.append(t[0])
        text = ' '.join(words)

        tokens = []
        if len(original):
            for t in original:
                idx = t[1]
                words = []
                for t2 in normalised:
                    if idx == t2[1]:
                        words.append(t2[0])
                display_text = self.__rstrip(t[0])
                display_text = self.__lstrip(display_text)
                tokens.append((t[0], words, display_text))
        else:
            tokens.append(('', '', ''))

        return text, tokens   

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--textfile', type=str, required=True, help='input directory or file')
    args = parser.parse_args()
    tn = TextNormaliser(False)
    with open(args.textfile) as fd:
        lines = fd.readlines()
        for line in lines:
            line = line.strip()
            normalised, tokens = tn.normalise(line)
            print(normalised)



</code></pre>
",Preprocessing of the text & Tokenization,text normalizer large text file slow python working text normalizer work fine small text file take long time large text file mb anything change code make run faster large text file guess would something preprocess tmp prenormalise text thank time
comparing lists of tokenized words to a set of words,"<p>I want to know if reviews are related to the subject or not, so I built a set of words that have a relation with subject.</p>
<pre><code>effi_set = {&quot;reminders&quot;,&quot;medication&quot;, &quot;Alarm&quot;
&quot;diet&quot;, &quot;carbohydrate&quot;,&quot;nutrition&quot;,&quot;weight&quot;,&quot;IBM&quot;, &quot;sport&quot;, &quot;activity&quot;, &quot;fitbit&quot;,&quot;blood&quot;,&quot;insulin&quot;,
&quot;Hb1ac&quot; , &quot;data exportation&quot;,&quot;feedback&quot;, &quot;monitoring&quot;,&quot;recording &quot;,&quot;monitor&quot;, &quot;record&quot;,
&quot;passwords&quot;,&quot;security&quot;,&quot;backup&quot;,&quot;protection&quot;,
&quot;information&quot;,&quot;education&quot;,&quot;complication&quot;,&quot;risk&quot;,&quot;prevent&quot;,&quot;contact&quot;,&quot;consultation&quot;,
&quot;facebook&quot;,&quot;twitter&quot;,&quot;social media&quot;,&quot;mail&quot;,&quot;FAQ&quot;,&quot;doctor&quot;,
&quot;data&quot;,&quot;offline&quot;,&quot;language&quot;,&quot;location&quot;,&quot;region&quot;,&quot;country&quot;,
&quot;devise&quot;,&quot;glucometer&quot;,&quot;bluetooth&quot;,&quot;automation&quot;,&quot;carb&quot;,&quot;barcode&quot;,&quot;food&quot;,&quot;syncronize&quot;,&quot;PHR&quot;,&quot;import&quot;}
</code></pre>
<p>I tokenize each review to compare the tokenized words to subj_set</p>
<pre><code>for line in df[&quot;content&quot;]:
    tokenized_words =word_tokenize(line)
    for item in tokenized_words:
        if item not in effi_set:
            df[&quot;efficient&quot;] = False
        else:
            df[&quot;efficient&quot;] = True
</code></pre>
<p>the result was that all reviews all false which is not the case.</p>
",Preprocessing of the text & Tokenization,comparing list tokenized word set word want know review related subject built set word relation subject tokenize review compare tokenized word subj set result wa review false case
Is there a simple way to tell SpaCy to ignore stop words when using .similarity method?,"<p>So right now I have a really simple program that will take a sentence and find the sentence in a given book that is most semantically similar and prints out that sentence along with the next few sentences.</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_lg')

#load alice in wonderland
from gutenberg.acquire import load_etext
from gutenberg.cleanup import strip_headers
text = strip_headers(load_etext(11)).strip()

alice = nlp(text)

sentences = list(alice.sents)

mysent = nlp(unicode(""example sentence, could be whatever""))

best_match = None
best_similarity_value = 0
for sent in sentences:
    similarity = sent.similarity(mysent)
    if similarity &gt; best_similarity_value:
        best_similarity_value = similarity
        best_match = sent

print sentences[sentences.index(best_match):sentences.index(best_match)+10]
</code></pre>

<p>I want to get better results by telling SpaCy to ignore the stop words when doing this process, but I don't know the best way to go about this. Like I could create a new blank list and append each word that isn't a stop word to the list</p>

<pre><code>for sentence in sentences:
    for word in sentence:
        if word.is_stop == 'False':
            newlist.append(word)
</code></pre>

<p>but I would have to make it more complicated than the code above because I would have to keep the integrity of the original list of sentences (because the indexes would have to be the same if I wanted to print out the full sentences later). Plus if I did it this way, I would have to run this new list of lists back through SpaCy in order to use the .similarity method.</p>

<p>I feel like there must be a better way of going about this, and I'd really appreciate any guidance. Even if there isn't a better way than appending each non-stop word to a new list, I'd appreciate any help in creating a list of lists so that the indexes will be identical to the original ""sentences"" variable.</p>

<p>Thanks so much!</p>
",Preprocessing of the text & Tokenization,simple way tell spacy ignore stop word using similarity method right really simple program take sentence find sentence given book semantically similar print sentence along next sentence want get better result telling spacy ignore stop word process know best way go like could create new blank list append word stop word list would make complicated code would keep integrity original list sentence index would wanted print full sentence later plus way would run new list list back spacy order use similarity method feel like must better way going really appreciate guidance even better way appending non stop word new list appreciate help creating list list index identical original sentence variable thanks much
What is the difference between giving a string and a list of string(s) to keras tokenizer?,"<p>I am working with <code>keras.preprocessing</code> for tokenize sentences, I encountered an unexpected case in <code>keras.preprocessing.text.Tokenize</code>. When I give it string, the output of <code>word_index</code> is a dictionary of  single characters and their indexes  but for list the output of <code>word_index</code> is dictionary of words (spllited by space).<br />
Why this happen?<br />
<strong>String for tokenizer input:</strong></p>
<pre><code>from keras.preprocessing.text import Tokenizer

text = &quot;Keras is a deep learning and neural networks API by François Chollet&quot; 
tokenizer = Tokenizer()
tokenizer.fit_on_texts(text) #input of tokenizer as string

print(tokenizer.word_index)

&gt;&gt;&gt; {'e': 1, 'a': 2, 'n': 3, 'r': 4, 's': 5, 'i': 6, 'l': 7, 'o': 8, 'k': 9, 'd': 10, 'p': 11, 't': 12, 'g': 13,
     'u': 14, 'w': 15, 'b': 16, 'y': 17, 'f': 18, 'ç': 19, 'c': 20, 'h': 21}
</code></pre>
<p><strong>List for tokenizer input:</strong></p>
<pre><code>from keras.preprocessing.text import Tokenizer

text = [&quot;Keras is a deep learning and neural networks API by François Chollet&quot;] 
tokenizer = Tokenizer()
tokenizer.fit_on_texts(text) #input of tokenizer as list

print(tokenizer.word_index)

&gt;&gt;&gt; {'keras': 1, 'is': 2, 'a': 3, 'deep': 4, 'learning': 5, 'and': 6, 'neural': 7, 'networks': 8,
     'api': 9, 'by': 10, 'françois': 11, 'chollet': 12}
</code></pre>
",Preprocessing of the text & Tokenization,difference giving string list string kera tokenizer working tokenize sentence encountered unexpected case give string output dictionary single character index list output dictionary word spllited space happen string tokenizer input list tokenizer input
Remove punctuation from numpy string arrays,"<p>The <code>string</code> package is useful for stripping punctuation from individual strings as demonstrated below:</p>
<pre><code>import string
stripPunct = str.maketrans('', '', string.punctuation)

word = 'foo.bar.baz'

word.translate(stripPunct)

Output: 'foobarbaz'
</code></pre>
<p>But what is the method to apply this exact same method to every string in a numpy array of strings?</p>
<pre><code>myArr =   np.array(['foo.bar.baz', 'foo.bar.baz', 'foo.bar.baz'], dtype='&lt;U15')


myArr.translate(stripPunct)
AttributeError: 'numpy.ndarray' object has no attribute 'translate'
</code></pre>
",Preprocessing of the text & Tokenization,remove punctuation numpy string array package useful stripping punctuation individual string demonstrated method apply exact method every string numpy array string
python text processing: identify nouns from individual words,"<p>I have a list of words and would like to keep only nouns.</p>

<p>This is not a duplicate of <a href=""https://stackoverflow.com/questions/33587667/extracting-all-nouns-from-a-text-file-using-nltk/33588238"">Extracting all Nouns from a text file using nltk</a></p>

<p>In the linked question a piece of text is processed. The accepted answer proposes a tagger. I'm aware of the different options for tagging text (nlkt, textblob, spacy), but I can't use them, since my data doesn't consist of sentences. I only have a list of individual words:</p>

<pre><code>would
research
part
technologies
size
articles
analyzes
line
</code></pre>

<p><code>nltk</code> has a wide selection of corpora. I found <code>verbnet</code> with a comprehensive list of verbs. But so far I didn't see anything similar for nouns. Is there something like a dictionary, where I can look up if a word is a noun, verb, adjective, etc ?</p>

<p>This could probably done by some online service. Microsoft translate for example returns a lot of information in their responses: <a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/translator/reference/v3-0-dictionary-lookup?tabs=curl"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cognitive-services/translator/reference/v3-0-dictionary-lookup?tabs=curl</a>
But this is a paid service. I would prefer a python package.</p>

<p>Regarding the ambiguity of words: Ideally I would like a dictionary that can tell me all the functions a word can have. ""fish"" for example is both noun and verb. ""eat"" is only verb, ""dog"" is only noun. I'm aware that this is not an exact science. A working solution would simply remove all words that can't be nouns. </p>
",Preprocessing of the text & Tokenization,python text processing identify noun individual word list word would like keep noun duplicate paid service would prefer python package regarding ambiguity word ideally would like dictionary tell function word fish example noun verb eat verb dog noun aware exact science working solution would simply remove word noun
Insert space if uppercase letter is preceded and followed by one lowercase letter - Python,"<p>Is there a way to insert aspace if it contains a uppercase letter (but not the first letter)?</p>
<p>For example, given <strong>&quot;RegularExpression&quot;</strong> I´d like to obtain <strong>&quot;Regular Expression&quot;</strong>.</p>
<p>I tried the following regex:</p>
<pre><code>re.sub(&quot;[a-z]{1}[A-Z][a-z]{1}&quot;, &quot; &quot;,&quot;regularExpression&quot;) 
</code></pre>
<p>Unfortunately, this deletes the matching pattern:</p>
<pre><code>regula pression
</code></pre>
<p>I would prefer a regex solution, yet would be thankful for any working solution.
Thanks!</p>
",Preprocessing of the text & Tokenization,insert space uppercase letter preceded followed one lowercase letter python way insert aspace contains uppercase letter first letter example given regularexpression like obtain regular expression tried following regex unfortunately deletes matching pattern would prefer regex solution yet would thankful working solution thanks
Splitting a text column&#39;s row into multiple rows,"<p>I have a dataset like this :
<a href=""https://i.sstatic.net/zMsQz.png"" rel=""nofollow noreferrer"">how the dataset looks like</a></p>
<p>What I want is to divide the status information column with the names in it to create new columns one for the names present and another for their respective comments and then fill the rest of the columns accordingly, something like this:</p>
<p><a href=""https://i.sstatic.net/VmQnR.png"" rel=""nofollow noreferrer"">desired outcome</a></p>
<p>I am able to clean the text of punctuations and numbers and create a dictionary from the status information column with names as key and comments as values, but it still show the whole staement with all the names.</p>
<p>Here is the code I tried so far:</p>
<pre><code>names = ['tom','robin']
months = ['jan','feb','mar','apr','may', 'jun','jul','aug','sep','oct','nov','dec',
          'january','february','march','april','june','july','august','september',
'october','november','december']


a = data['Status information'][3].lower()
b = [char for char in a if char not in string.digits]
b = ''.join(b)

for i in months:
    if b.find(i) != -1:
        b= b.replace(i,' ')

for i in string.punctuation:
    if b.find(i) != -1:
        b= b.replace(i,' ')
x={} 
for i in names:
    if b.find(i) != -1:
        x[i] = b.split(i)
        
print(b)
print(x)
</code></pre>
<p>Output:</p>
<pre><code>tom  i want only server    robin cal busy    robin reg for server
{'tom': ['   ', '  i want only server    robin cal busy    robin reg for server'], 
'robin': ['   tom  i want only server    ', ' cal busy    ', ' reg for server']}
</code></pre>
",Preprocessing of the text & Tokenization,splitting text column row multiple row dataset like dataset look like want divide status information column name create new column one name present another respective comment fill rest column accordingly something like desired outcome able clean text punctuation number create dictionary status information column name key comment value still show whole staement name code tried far output
How do I identify an object is of type NLTK Tree and then parse it?,"<p>I am trying to get GPE locations from a message after tokenizing it.</p>
<pre><code>from nltk import ne_chunk 

print(ne_chunk(pos_words[0])) 
</code></pre>
<p>Output:</p>
<pre><code>  Weather/NNP
  update/VB
  a/DT
  cold/JJ
  front/NN
  from/IN
  (GPE Cuba/NNP)
  that/WDT
  could/MD
  pass/VB
  over/RP
  (PERSON Haiti/NNP))
</code></pre>
<p>I want to get the output Cuba as a string. How can I access that?</p>
<p>Edit: I am trying to generalize the extraction to the dataframe by making a list of locations. This is the function I made. However, it splits multi-word locations like New York into [New, York]</p>
<pre><code>    locations = []
    for i in range(len(pos_words)): 
        chunks = ne_chunk(pos_words[i]) 
        for c in chunks: 
            if isinstance(c, Tree) and c.label() == 'GPE': 
                # The object is &lt;class 'nltk.tree.Tree'&gt; and label is Geopolitical Entity
                locations.extend([w for w,_ in c.leaves()])
    
    return locations
</code></pre>
",Preprocessing of the text & Tokenization,identify object type nltk tree parse trying get gpe location message tokenizing output want get output cuba string access edit trying generalize extraction dataframe making list location function made however split multi word location like new york new york
StanfordNLP lemmatization cannot handle -ing words,"<p>I've been experimenting with Stanford NLP toolkit and its lemmatization capabilities. I am surprised how it lemmatize some words. For example:</p>

<pre><code>depressing -&gt; depressing
depressed -&gt; depressed
depresses -&gt; depress
</code></pre>

<p>It is not able to transform <code>depressing</code> and <code>depressed</code> into the same lemma. Simmilar happens with <code>confusing</code> and <code>confused</code>, <code>hopelessly</code> and <code>hopeless</code>. I am getting the feeling that the only thing it is able to do is remove the <code>s</code> if the word is in such form (e.g. <code>feels -&gt; feel</code>). Is such behaviour normal for Lematizatiors in English? I would expect that they would be able to transform such variations of common words into a same lemma.</p>

<p>If this is normal, should I rather use stemmers? And, is there a way to use stemmers like Porter (Snowball, etc.) in StanfordNLP? There is no mention of stemmers in their documentation; however, there are some <code>CoreAnnotations.StemAnnotation</code> in the API. If not possible with StanfordNLP which stemmers do you recommend for use in Java?</p>
",Preprocessing of the text & Tokenization,stanfordnlp lemmatization handle ing word experimenting stanford nlp toolkit lemmatization capability surprised lemmatize word example able transform lemma simmilar happens getting feeling thing able remove word form e g behaviour normal lematizatiors english would expect would able transform variation common word lemma normal rather use stemmer way use stemmer like porter snowball etc stanfordnlp mention stemmer documentation however api possible stanfordnlp stemmer recommend use java
How to convert contractions words back in NLP,"<p>I need to convert abbreviations back using NLP.<br />
Like <code>what's</code> to <code>what is</code>, <code>it's</code> to <code>it is</code>, etc.<br />
I want to use it to preprocess the raw sentence.</p>
<p>Actually, I also confused about whether I should do this or just simply remove the <code>'</code> and convert <code>what's</code> to <code>whats</code>. Otherwise, anyway, <code>is</code> will be removed as a stop word in a later step.</p>
<p>In another hand, should we consider <code>whats</code> and <code>what</code> as <code>lemma</code>?<br />
Or, we should use stemmer to cut the <code>s</code> off?</p>
<p>BTW, I don't think abbreviation is the right term here, but I'm not good at English as well. So, please introduce me the formal NLP or linguistics term we used for <code>what's</code>, <code>how's</code>, etc.</p>
",Preprocessing of the text & Tokenization,convert contraction word back nlp need convert abbreviation back using nlp like etc want use preprocess raw sentence actually also confused whether simply remove convert otherwise anyway removed stop word later step another hand consider use stemmer cut btw think abbreviation right term good english well please introduce formal nlp linguistics term used etc
Unsure of how to get started with using NLP for analyzing user feedback,"<p>I have ~138k records of user feedback that I'd like to analyze to understand broad patterns in what our users are most often saying. Each one has a rating between 1-5 stars, so I don't need to do any sort of sentiment analysis. I'm mostly interested in splitting the dataset into &gt;=4 stars to see what we're doing well and &lt;= 3 stars to see what we need to improve upon.</p>
<p>One key problem I'm running into is that I expect to see a lot of n-grams. Some of these I know, like &quot;HOV lane&quot;, &quot;carpool lane&quot;, &quot;detour time&quot;, &quot;out of my way&quot;, etc. But I also want to detect common bi- and tri-grams programmatically. I've been playing around with Spacy a bit, but it doesn't seem to have any capability to do analysis on the corpus level, only on the document level.</p>
<p>Ideally my pipeline would look something like this (I think):</p>
<ol>
<li><p>Import a list of known n-grams into the tokenizer</p>
</li>
<li><p>Process each string into a tokenized document, removing punctuation,
stopwords, etc, while respecting the known n-grams during
tokenization (ie, &quot;HOV lane&quot; should be a single noun token)</p>
</li>
<li><p>Identify the most common bi- and tri- grams in the corpus that I
missed</p>
</li>
<li><p>Re-tokenize using the found n-grams</p>
</li>
<li><p>Split by rating (&gt;=4 and &lt;=3)</p>
</li>
<li><p>Find the most common topics for each split of data in the corpus</p>
</li>
</ol>
<p>I can't seem to find a single tool, or even a collection of tools, that will allow me to do what I want here. Am I approaching this the wrong way somehow? Any pointers on how to get started would be greatly appreciated!</p>
",Preprocessing of the text & Tokenization,unsure get started using nlp analyzing user feedback k record user feedback like analyze understand broad pattern user often saying one ha rating star need sort sentiment analysis mostly interested splitting dataset star see well star see need improve upon one key problem running expect see lot n gram know like hov lane carpool lane detour time way etc also want detect common bi tri gram programmatically playing around spacy bit seem capability analysis corpus level document level ideally pipeline would look something like think import list known n gram tokenizer process string tokenized document removing punctuation stopwords etc respecting known n gram tokenization ie hov lane single noun token identify common bi tri gram corpus missed tokenize using found n gram split rating find common topic split data corpus seem find single tool even collection tool allow want approaching wrong way somehow pointer get started would greatly appreciated
Should BERT embeddings be made on tokens or sentences?,"<p>I am making a sentence classification model and using BERT word embeddings in it. Due to very large dataset, I combined all the sentences together in one string and made embeddings on the tokens generated from those.</p>
<pre><code>s = &quot; &quot;.join(text_list)
len(s)
</code></pre>
<p>Here <code>s</code> is the string and <code>text_list</code> contains the sentences on which I want to make my word embeddings.</p>
<p>I then tokenize the string</p>
<pre><code>stokens = tokenizer.tokenize(s)
</code></pre>
<p>My question is, will BERT perform better on whole sentence given at a time or making embeddings on tokens for whole string is also fine?</p>
<p>Here is the code for my embedding generator</p>
<pre><code>pool = []
all = []
i=0
while i!=600000:
  stokens = stokens[i:i+500]
  stokens = [&quot;[CLS]&quot;] + stokens + [&quot;[SEP]&quot;]
  input_ids = get_ids(stokens, tokenizer, max_seq_length)
  input_masks = get_masks(stokens, max_seq_length)
  input_segments = get_segments(stokens, max_seq_length)
  a, b= embedd(input_ids, input_masks, input_segments)
  pool.append(a)
  all.append(b)
  print(i)
  i+=500
</code></pre>
<p>What essentially I am doing here is, I have the string length of 600000 and I take 500 tokens at a time and generate embdedings for it and append it in a list call <code>pool</code>.</p>
",Preprocessing of the text & Tokenization,bert embeddings made token sentence making sentence classification model using bert word embeddings due large dataset combined sentence together one string made embeddings token generated string contains sentence want make word embeddings tokenize string question bert perform better whole sentence given time making embeddings token whole string also fine code embedding generator essentially string length take token time generate embdedings append list call
find all related words from a root word [NLP python],"<p>Is it possible to find all the related words from a root word (Kind of like a reverse stemming).
For example, study -&gt; [study , studies, studying, studied]</p>
",Preprocessing of the text & Tokenization,find related word root word nlp python possible find related word root word kind like reverse stemming example study study study studying studied
Append the count of the occurrence of the word in python Dataframe,"<p>My original data</p>
<p><a href=""https://i.sstatic.net/uu2B0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uu2B0.png"" alt="""" /></a></p>
<p>I want to convert the text data into a dataframe which will contain the 500 words like the below picture in which each sentence will contain the occurrence of that word in the particular sentence (Row of a dataframe.)</p>
<p>Final Output_data</p>
<p><a href=""https://i.sstatic.net/dG3jU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dG3jU.png"" alt="""" /></a></p>
<p>I have performed text preprocessing and all with NLTK.</p>
",Preprocessing of the text & Tokenization,append count occurrence word python dataframe original data want convert text data dataframe contain word like picture sentence contain occurrence word particular sentence row dataframe final output data performed text preprocessing nltk
Bigram Finder for Pandas Dataframe,"<p>I have a list of bigrams.<br>
I have a pandas dataframe containing a row for each document in my corpus.  What I am looking to do is get the bigrams that match from my list in each document into a new column in my dataframe.
What is the best way to accomplish this task?  I have been searching for answers on stack overflow but haven't found something that gives me a specific answer I am looking for. I need the new column to contain every bigram found from my bigram list.  </p>

<p>Any help would be appreciated! </p>

<p>The output what I have below is what I am looking for, although on my real example, I have used stop words so exact bigrams aren't found like the output below.  Is there a way to do with with some sort of string contains maybe?</p>

<pre><code>import pandas as pd 
data = [['help me with my python pandas please'], ['machine learning is fun using svd with sklearn']] 
# Create the pandas DataFrame 
df = pd.DataFrame(data, columns = ['Message']) 
import numpy as np
bigrams =[('python', 'pandas'),
 ('function', 'input'),
 ('help', 'jupyter'),
 ('sklearn', 'svd')]
def matcher(x):
    for i in bigrams:
        if i.lower() in x.lower():
            return i
    else:
        return np.nan

df['Match'] = df['Message'].apply(matcher)
df
</code></pre>
",Preprocessing of the text & Tokenization,bigram finder panda dataframe list bigram panda dataframe containing row document corpus looking get bigram match list document new column dataframe best way accomplish task searching answer stack overflow found something give specific answer looking need new column contain every bigram found bigram list help would appreciated output looking although real example used stop word exact bigram found like output way sort string contains maybe
Find and sort most similar to a list of specific words to a corpus of documents,"<p>How do count and score multiple lists of words to a corpus of multiple documents so you can perform sorting in a few different ways?</p>
<ol>
<li>find a doc in corpus and <strong>find and sort most similar to words in a list</strong></li>
</ol>
<pre><code>sort by most red
'i ate a red apple.'
'the kid read the book the little red riding hood', 
</code></pre>
<ol start=""2"">
<li>Also <strong>be able to find the closest documents to a given document</strong>.</li>
</ol>
<pre><code>most similar to doc 0
'i ate a red apple.'
'the kid read the book the little red riding hood', 
</code></pre>
<p>for example</p>
<pre><code>colors  = ['red', 'blue', 'yellow' , 'purple']
things = ['apple', 'pickle', 'tomato' , 'rainbow', 'book']

corpus = ['i ate a red apple.', 'There are so many colors in the rainbow.', 'the monster was purple and green.', 'the pickle is very green', 'the kid read the book the little red riding hood', 'in the book the wizard of oz there was a yellow brick road.', 'tom has a green thumb and likes working in a garden.' ]

colors  = ['red', 'blue', 'yellow' , 'purple']
things = ['apple', 'pickle', 'tomato' , 'rainbow', 'book']
 
     0    1    2    3    4    5    6
</code></pre>
<p>do I make a counter</p>
<pre><code># 0 'i ate a red apple.'
['red': 1, 'blue': 0, 'yellow' : 0, 'purple': 0]
['apple': 1, 'pickle': 0, 'tomato': 0, 'rainbow': 0, 'book': 0]

# 1 'There are so many colors in the rainbow.'
['red': 0, 'blue': 0, 'yellow' : 0, 'purple': 0]
['apple': 0, 'pickle': 0, 'tomato': 0, 'rainbow': 1, 'book': 0]

# 2 the monster was purple and green.'
['red': 0, 'blue': 0, 'yellow' : 0, 'purple': 1]
['apple': 0, 'pickle': 0, 'tomato': 0, 'rainbow': 0, 'book': 0]

# 3 'the pickle is very green', 
['red': 0, 'blue': 0, 'yellow' : 0, 'purple': 0]
['apple': 0, 'pickle': 1, 'tomato': 0, 'rainbow': 0, 'book': 0]

# 4 'the kid read the book the little red riding hood', 
['red': 1 'blue': 0, 'yellow' : 0, 'purple': 0]
['apple': 0, 'pickle': 0, 'tomato': 0, 'rainbow': 0, 'book': 1]

# 5 'in the book the wizard of oz there was a yellow brick road.', 
['red': 0, 'blue': 0, 'yellow' : 1, 'purple': 0]
['apple': 0, 'pickle': 0, 'tomato': 0, 'rainbow': 0, 'book': 1]

# 6 'tom has a green thumb and likes working in a garden.' 
['red': 0, 'blue': 0, 'yellow' : 0, 'purple': 0]
['apple': 0, 'pickle': 0, 'tomato': 0, 'rainbow': 0, 'book': 0]
</code></pre>
<p>or an array for color and one for things</p>
<pre><code># colors
         0    1    2    3    4    5    6
red      1    0    0    0    1    0    0
blue     0    0    0    0    0    0    0
yellow   0    0    0    0    0    1    0
purple   0    0    1    0    0    0    0
</code></pre>
<pre><code># things
          0    1    2    3    4    5    6
apple     1    0    0    0    1    0    0
pickle    0    0    0    1    0    0    0
tomato    0    0    0    0    0    0    0
rainbow   0    0    1    0    0    0    0
book      0    0    0    0    1    1    0
</code></pre>
<p>Then find most simular or sort by closest number</p>
<pre><code>sort by most red
'i ate a red apple.'
'the kid read the book the little red riding hood', 
</code></pre>
<pre><code>most similar to doc 0
'i ate a red apple.'
'the kid read the book the little red riding hood', 
</code></pre>
<p>Or should I use doc2vec or something completely different?</p>
",Preprocessing of the text & Tokenization,find sort similar list specific word corpus document count score multiple list word corpus multiple document perform sorting different way find doc corpus find sort similar word list also able find closest document given document example make counter array color one thing find simular sort closest number use doc vec something completely different
How to perform LSA on a huge dataset that does not fit into memory with Python?,"<p>I have similar questions before but I haven't found a solution that works for me specifically. So I have a million documents and lets say each document has around 20-30 words in it. I want to lemmatize, remove stopwords and use 100,000 words to build a tf-idf matrix and then do SVD on it.
How can I do this using Python within reasonable time and without running into memory errors ?</p>
<p>If someone has any idea that would be great.</p>
",Preprocessing of the text & Tokenization,perform lsa huge dataset doe fit memory python similar question found solution work specifically million document let say document ha around word want lemmatize remove stopwords use word build tf idf matrix svd using python within reasonable time without running memory error someone ha idea would great
TF Keras Text Processing - Classification Model,"<p>I posted a question earlier about a TensorFlow script that I wrote that wasn't working properly.  I've narrowed down the problem, and deleted the old comment, I hope this one is easier to follow.  I'm basically trying to follow the classic example of predicting movie comments on IMDB which are either positive or negative.  I've looked at many variations of this, and my main problem is that my model is returning the same label prediction (the label being the sentiment, positive/negative).  I'm 99% sure the problem is in my model build/compile:</p>

<pre><code>NUM_WORDS = 10000
SEQ_LEN = 512
EMBEDDING_SIZE = 300
BATCH_SIZE = 500
EPOCHS = 20

**model = tf.keras.Sequential([
        tf.keras.layers.Embedding(NUM_WORDS, 32, input_length = BATCH_SIZE),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(250, activation = 'relu'),
        tf.keras.layers.Dense(1, activation = 'sigmoid')])**

model.summary()

model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

es = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', mode = 'max')

callbacks = [es]
history = model.fit(train_seqs, train_df['adq'].values,
                    batch_size = BATCH_SIZE,
                    epochs = EPOCHS,
                    validation_split = 0.2,
                    callbacks = callbacks)

model.evaluate(test_seqs, test_df['adq'].values)


model.save('model.ps1')
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol = pickle.HIGHEST_PROTOCOL)

del model
del tokenizer


loaded_model = tf.keras.models.load_model('model.ps1')

with open('tokenizer.pickle', 'rb') as f:
    loaded_tokenizer = pickle.load(f)

def prepare_predict_data(tokenizer, comments):
    seqs = tokenizer.texts_to_sequences(comments)
    seqs = tf.keras.preprocessing.sequence.pad_sequences(seqs, maxlen = SEQ_LEN, padding = 'post')
    return seqs
</code></pre>

<p>My text preprocessing cleanup, tokenization, and padding are all working, and I have 38000 or so comments in my training set, each with an sequence length of 512, again the padding looks good.  Can you please let me know where i'm going wrong?  Thanks so much for your help!</p>
",Preprocessing of the text & Tokenization,tf kera text processing classification model posted question earlier tensorflow script wrote working properly narrowed problem deleted old comment hope one easier follow basically trying follow classic example predicting movie comment imdb either positive negative looked many variation main problem model returning label prediction label sentiment positive negative sure problem model build compile text preprocessing cleanup tokenization padding working comment training set sequence length padding look good please let know going wrong thanks much help
How to save word vectors in spacy,"<p>I have the following code. The goal is to get a vector representation of each word in the list. My intention is to use these word vectors for other application purpose like word clustering.</p>
<pre><code>import numpy as np
import pandas as pd
from sklearn.preprocessing import normalize
import en_vectors_web_lg
nlp = en_vectors_web_lg.load() 

def vectorize(text):
    return nlp(text, disable=['parser', 'tagger', 'ner']).vector

category=['Dell','Python','Asus','Apple','C','perl','Java','iphone','nokia','LG','Lenovo']
for ntext in category:
    doc = nlp(ntext)

    vectors = normalize(np.stack(vectorize(t) for t in doc.text))

</code></pre>
<p>I realize i am doing something wrong in the code above. How to save the word vectors of each word in the list 'category'</p>
",Preprocessing of the text & Tokenization,save word vector spacy following code goal get vector representation word list intention use word vector application purpose like word clustering realize something wrong code save word vector word list category
&quot;Stop words&quot; list for English?,"<p>I'm generating some statistics for some English-language text and I would like to skip uninteresting words such as ""a"" and ""the"".</p>

<ul>
<li>Where can I find some lists of these uninteresting words?</li>
<li>Is a list of these words the same as a list of the most frequently used words in English?</li>
</ul>

<p>update: these are apparently called ""stop words"" and not ""skip words"".</p>
",Preprocessing of the text & Tokenization,stop word list english generating statistic english language text would like skip uninteresting word find list uninteresting word list word list frequently used word english update apparently called stop word skip word
"ruby &amp; nlp: how to remove stop words and non-words (e.g. link, emoji) and count uniq words?","<p>Some sample data</p>
<p><code>NEW LISTING: @AaveAave AAVE ($LEND) will SOON be listed on @OKEx! Quiz &amp;amp; Net Buy 🎁 up to 3,000 $LEND: ▶️ Follow us + @AaveAave ▶️ Join Quiz: https://t.co/{.....} ▶️ RT answer &amp;amp; @OKEx #OKExDeFi #OKExAave ▶️ Deposit + Net Buying rebate Listing details: https://t.co/{.....} hhttps://t.co/{.....} </code></p>
<p><code>🚨 🚨 🚨 🚨 🚨 🚨 🚨 🚨 🚨 🚨 100,000,000 #USDT (100,568,399 USD) transferred from Tether Treasury to #Binance Tx: https://t.co/{.....} </code></p>
<p>What I plan to do</p>
<ul>
<li>remove all stop words</li>
<li>remove all non-words (link and emoji)</li>
<li>count word number and unique word number</li>
</ul>
<p>My question: I know how to do this in R (using <code>tidytext</code>), but what is the best practice doing the above in Ruby? I searached around but didn't know any popular related gems.</p>
<p>Thanks to your help</p>
<hr />
<p>If it's <code>tidytext</code>, the above can be pretty much done as shown below</p>
<p><a href=""https://i.sstatic.net/YjYtv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YjYtv.png"" alt=""enter image description here"" /></a></p>
",Preprocessing of the text & Tokenization,ruby nlp remove stop word non word e g link emoji count uniq word sample data plan remove stop word remove non word link emoji count word number unique word number question know r using best practice ruby searached around know popular related gem thanks help pretty much done shown
How to detokenize spacy text without doc context?,"<p>I have a sequence to sequence model trained on tokens formed by spacy's tokenization. This is both encoder and decoder.</p>

<p>The output is a stream of tokens from a seq2seq model. I want to detokenize the text to form natural text.</p>

<p>Example:</p>

<p><strong>Input to Seq2Seq:</strong> Some text </p>

<p><strong>Output from Seq2Seq:</strong> This does n't work .</p>

<p>Is there any API in spacy to reverse tokenization done by rules in its tokenizer?</p>
",Preprocessing of the text & Tokenization,detokenize spacy text without doc context sequence sequence model trained token formed spacy tokenization encoder decoder output stream token seq seq model want detokenize text form natural text example input seq seq text output seq seq doe n work api spacy reverse tokenization done rule tokenizer
Is it necessary to lemmatize the text and remove stopwords for NER?,"<p>I'm working on the project where there are several paragraphs of text in which I must extract certain entities using NER. (I'm using <strong>SPACY</strong>)</p>
<p>Here is the sample data:</p>
<pre><code>Mr XYZ, XX year old, who was the victim of motor vehicle accident that 
occurred on XX-XX-XX date, he sustained numerous injuries. 
He has been seen in the hospital for the left shoulder and low-back.
</code></pre>
<p>I must Extract bodypart for which he is being treated, i.e left shoulder &amp; low-back and accident date.
Now if I lemmatize this text the meaning of &quot;left shoulder&quot; changes as &quot;leave shoulder&quot;. And if I remove stop words, the bodypart &quot;low-back&quot; will not be recognised as entity as &quot;back&quot; will be removed by stopwords the meaning of entire text changes.
I know that certain stopwords can be removed, but what if other words having importance is removed.
I'm not sure whether stopwords &amp; lemmation is helping me.</p>
<p>Is is ok if I dont do text pre-processing ? or should I proceed with the same?</p>
",Preprocessing of the text & Tokenization,necessary lemmatize text remove stopwords ner working project several paragraph text must extract certain entity using ner using spacy sample data must extract bodypart treated e left shoulder low back accident date lemmatize text meaning left shoulder change leave shoulder remove stop word bodypart low back recognised entity back removed stopwords meaning entire text change know certain stopwords removed word importance removed sure whether stopwords lemmation helping ok dont text pre processing proceed
Remove stop words from spaCy Doc object,"<p>I'm trying to figure out how to remove stop words from a spaCy <code>Doc</code> object while retaining the original parent object with all its attributes.</p>
<pre class=""lang-py prettyprint-override""><code>import en_core_web_md
nlp = en_core_web_md.load()

sentence = &quot;The frigate was decommissioned following Britain's declaration of peace with France in 1763, but returned to service in 1766 for patrol duties in the Caribbean&quot;

tokens = nlp(sentence)
print(&quot;Parent type:&quot;, type(tokens))
print(&quot;Token type:&quot;, type(tokens[0]))
print(&quot;Sentence vector:&quot;, tokens.vector)
print(&quot;Word vector:&quot;, tokens[0].vector)
</code></pre>
<p>returns:</p>
<pre><code>Parent type: &lt;class 'spacy.tokens.doc.Doc'&gt;
Token type: &lt;class 'spacy.tokens.token.Token'&gt;
Sentence vector: [ 8.35970342e-02  1.38482109e-01  7.71872401e-02 -7.14236796e-02
...]
Word vector: [ 2.7204e-01 -6.2030e-02 -1.8840e-01  2.3225e-02 -1.8158e-02  6.7192e-03
...]

</code></pre>
<p>Typical solutions to removing stop words consist in using a list comprehension:</p>
<pre class=""lang-py prettyprint-override""><code>noStopWords = [t for t in tokens if not t.is_stop]
print(&quot;Parent type:&quot;, type(noStopWords))
print(&quot;Token type:&quot;, type(noStopWords[0]))
try:
    print(&quot;Sentence vector:&quot;, noStopWords.vector)
except AttributeError as e:
    print(e)
try:
    print(&quot;Word vector:&quot;, noStopWords[0].vector)
except AttributeError as e:
    print(e)
</code></pre>
<p>Since, now, the parent object is a list of <code>Token</code> objects, and no longer a <code>Doc</code> object, it no longer has the original attributes, so the code returns:</p>
<pre><code>Parent type: &lt;class 'list'&gt;
Token type: &lt;class 'spacy.tokens.token.Token'&gt;
'list' object has no attribute 'vector'
Word vector: [ 9.4139e-01 -5.9546e-01  5.5007e-01  3.7544e-01  2.3021e-02 -4.4260e-01
...]
</code></pre>
<p>So the rather terrible way I could find is to rebuild a string from the tokens, and reprocess it. This sucks as it's double work, and the <code>nlp</code> method is already slow.</p>
<pre class=""lang-py prettyprint-override""><code>noStopWordsDoc = nlp(' '.join([t.text for t in noStopWords]))
print(&quot;Parent type:&quot;, type(noStopWordsDoc))
print(&quot;Token type:&quot;, type(noStopWordsDoc[0]))
try:
    print(&quot;Sentence vector:&quot;, noStopWordsDoc.vector)
except AttributeError as e:
    print(e)
try:
    print(&quot;Word vector:&quot;, noStopWordsDoc[0].vector)
except AttributeError as e:
    print(e)
</code></pre>
<pre><code>Parent type: &lt;class 'spacy.tokens.doc.Doc'&gt;
Token type: &lt;class 'spacy.tokens.token.Token'&gt;
Sentence vector: [ 9.78216752e-02  1.06186338e-01  1.66255698e-01 -9.38376933e-02
...]
</code></pre>
<p>Now, there must be a better way, right?</p>
",Preprocessing of the text & Tokenization,remove stop word spacy doc object trying figure remove stop word spacy object retaining original parent object attribute return typical solution removing stop word consist using list comprehension since parent object list object longer object longer ha original attribute code return rather terrible way could find rebuild string token reprocess suck double work method already slow must better way right
Removing stopwords when the sentence contains special characters,"<p>I have a .xlsx file with some raw text. I'm reading the file into a DataFrame, then trying to remove symbols and stopwords from it. I do have functions for both needs already implemented, but I keep running into the following problems:</p>
<ul>
<li><p>If I remove symbols before removing stopwords, things like &quot;isnt&quot;, &quot;theyre&quot;, etc., stay in the dataframe.</p>
</li>
<li><p>If I remove stopwords before symbols, things like &quot;(the&quot; aren't counted as stopwords and stay on the dataframe.</p>
</li>
</ul>
<p>Here's what symbol removal looks like:</p>
<pre><code>regex = r'[^\w\s]'
self.dataframe = self.dataframe.replace(regex, '', regex=True)
</code></pre>
<p>And stopword removal:</p>
<pre><code>self.dataframe[col] = column.apply(lambda x: ' '.join(
            [item for item in x.split() if item not in stops]))
</code></pre>
<p>Is there an elegant solution for this? Any suggestions are also appreciated.</p>
",Preprocessing of the text & Tokenization,removing stopwords sentence contains special character xlsx file raw text reading file dataframe trying remove symbol stopwords function need already implemented keep running following problem remove symbol removing stopwords thing like isnt theyre etc stay dataframe remove stopwords symbol thing like counted stopwords stay dataframe symbol removal look like stopword removal elegant solution suggestion also appreciated
Settting PTBTokenizer&#39;s normalizeSpace to false with pycorenlp,"<p>I use <a href=""https://github.com/smilli/py-corenlp"" rel=""nofollow"">pycorenlp</a> to tokenize a text. By <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/PTBTokenizer.html"" rel=""nofollow"">default</a>, any space in tokens (e.g., phone numbers, fractions) gets turned into U+00A0 (<a href=""https://en.wikipedia.org/wiki/Non-breaking_space"" rel=""nofollow"">non-breaking space</a>), coded as <a href=""http://www.fileformat.info/info/unicode/char/00a0/index.htm"" rel=""nofollow""><code>\xC2\xA0</code> in UTF-8</a>. I <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/PTBTokenizer.html"" rel=""nofollow"">read</a> that I should set <code>normalizeSpace</code> to <code>False</code>. How can I do that with pycorenlp? </p>

<p>Here is the code I use:</p>

<pre><code>'''
From https://github.com/smilli/py-corenlp/blob/master/example.py
'''

from pycorenlp import StanfordCoreNLP
import pprint

if __name__ == '__main__':
    nlp = StanfordCoreNLP('http://localhost:9000')
    text = 'I ate 1 1/2 lobster'
    output = nlp.annotate(text, properties={
        'annotators': 'tokenize,ssplit',
        'outputFormat': 'json',
        'timeout': '50000',
        'PTBTokenizer.normalizeSpace': 'false'
        #'normalizeSpace': 'false'
    })
    pp = pprint.PrettyPrinter(indent=4)
    pp.pprint(output)
    print(""output['sentences'][0]['tokens'][2]: {0}"".
          format(output['sentences'][0]['tokens'][2]))
</code></pre>

<p>The output contains <code>u'word': u'1\xa01/2'</code>, which shows that spaces are replaced by non-breaking spaces, which I do not want.</p>

<p>The Stanford Core NLP Server was launched using:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer 9000
</code></pre>

<p>I unsuccessfully tried adding the following in the <code>properties</code> dictionary that is passed as a parameter to <code>nlp.annotate()</code>:</p>

<ul>
<li><code>'tokenizer.normalizeSpace': 'false'</code></li>
<li><code>'tokenize.normalizeSpace': 'false'</code></li>
<li><code>'normalizeSpace': 'false'</code></li>
<li><code>'PTBTokenizer.normalizeSpace': 'false'</code></li>
</ul>
",Preprocessing of the text & Tokenization,settting ptbtokenizer normalizespace false pycorenlp use pycorenlp tokenize text default space token e g phone number fraction get turned u non breaking space coded utf read set pycorenlp code use output contains show space replaced non breaking space want stanford core nlp server wa launched using unsuccessfully tried adding following dictionary passed parameter
number of tokenized sentences does not match number of sentences in text,"<p>I have some problems with the <code>nltk.sent_tokenize</code> function.</p>
<p>My text (that I want to tokenize) consist of 54116 sentences that are separated by a dot. I removed other punctuation.<br />
I would like to tokenize my text on a sentence level by using <code>nltk.sent_tokenize</code>.</p>
<p>However, if I apply <code>tokenized_text = sent_tokenize(mytext)</code>, the length of <code>tokenized_text</code> is only 51582 instead of 54116.</p>
<p>Any ideas, why this could happen?</p>
<p>Kind regards</p>
",Preprocessing of the text & Tokenization,number tokenized sentence doe match number sentence text problem function text want tokenize consist sentence separated dot removed punctuation would like tokenize text sentence level using however apply length instead idea could happen kind regard
HuggingFace Transformers: BertTokenizer changing characters,"<p>I have downloaded the Norwegian BERT-model from <a href=""https://github.com/botxo/nordic_bert"" rel=""nofollow noreferrer"">https://github.com/botxo/nordic_bert</a>, and loaded it in using:</p>
<pre><code>import transformers as t

model_class = t.BertModel
tokenizer_class = t.BertTokenizer

tokenizer = tokenizer_class.from_pretrained(/PATH/TO/MODEL/FOLDER)
model = model_class.from_pretrained(/PATH/TO/MODEL)
model.eval()
</code></pre>
<p>This works very well, however when i try to tokenize a given sentence, some nordic characters such as &quot;ø&quot; and &quot;æ&quot; remain the same, whereas all words having the char &quot;å&quot; is replaced with &quot;a&quot;.
For instance:</p>
<pre><code>s = &quot;æ ø å løpe få ærfugl&quot;
print(tokenizer.tokenize(s))
</code></pre>
<p>Yields:</p>
<pre><code>['æ', 'ø', 'a', 'løp', '##e', 'fa', 'ær', '##fugl']
</code></pre>
<p>Thanks</p>
",Preprocessing of the text & Tokenization,huggingface transformer berttokenizer changing character downloaded norwegian bert model loaded using work well however try tokenize given sentence nordic character remain whereas word char replaced instance yield thanks
Use NLTK tokenizer in Keras workflow,"<p>I am using the Keras tokenizer to tokenize sentences and than to create sequences of word indexes that I can use in the training of neural networks:</p>
<pre><code>from keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
</code></pre>
<p>How can I use the casual_tokenize NLTK tokenizer to achieve a similar result? The NLTK toeknizer does not offer a texts_to_sequences() method.</p>
",Preprocessing of the text & Tokenization,use nltk tokenizer kera workflow using kera tokenizer tokenize sentence create sequence word index use training neural network use casual tokenize nltk tokenizer achieve similar result nltk toeknizer doe offer text sequence method
Explicit likelihood of WordPiece used for pre-processing of BERT,"<p>At each iteration the WordPiece algorithm for subword tokenization merges the two symbols which increase the likelihood the most. Now, in the literature it is only mentioned that this likelihood is the likelihood of the language model  (e.g., the same likelihood used during decoding, in case of NMT). Does anyone know which likelihood was used for pre-processing of BERT?</p>
",Preprocessing of the text & Tokenization,explicit likelihood wordpiece used pre processing bert iteration wordpiece algorithm subword tokenization merges two symbol increase likelihood literature mentioned likelihood likelihood language model e g likelihood used decoding case nmt doe anyone know likelihood wa used pre processing bert
How can I use NLP to extract the main food word from an ingredient?,"<p>Let's say I have the following list of ingredients:</p>
<pre><code>1 fresh baguette, cut into sandwich parts
1 tablespoon good quality unsalted butter, per sandwich (so 4 tbsp if making 4 sandwiches)
1 apple or gala preferably (2 apples for 4 sandwiches)
1 slice of english blue cheese, preferably a stilton (again, or 4 slices for 4 sandwiches)
</code></pre>
<p>What I want to do is extract the main food word from the list of ingredients. So the result should be:</p>
<pre><code>baguette
butter
apple
cheese
</code></pre>
<p>How can I use NLP to do this? Or is there something else I can use to do this?</p>
",Preprocessing of the text & Tokenization,use nlp extract main food word ingredient let say following list ingredient want extract main food word list ingredient result use nlp something else use
text cleaning in python,"<p>If I need to clean text in R, I could chain the multiple operations by:</p>
<pre class=""lang-r prettyprint-override""><code>ssids$text%&gt;% 
  str_replace_all( &quot;[^\x01-\x7E]&quot;, &quot;&quot;) %&gt;% #remove hex encoding
  replace_non_ascii %&gt;%
  str_replace_all(&quot;\\b([a-z]+)\\b(?:\\s+\\1\\b)+&quot;,&quot;\\1&quot;) %&gt;% #remove duplicates word appearing together 
  str_replace_all('[:digit:]', '')%&gt;% #remove digits
  str_replace_all( &quot;[^[:alnum:]///' ]&quot;, &quot;&quot;) %&gt;% #remove special characters 
  str_replace_all( &quot;[:punct:]&quot;, &quot;&quot;) %&gt;% #remove punctuation
  scrubber() %&gt;% # clean and replace other textual anomalies
  str_to_lower() # convert text to lower case
</code></pre>
<p>The above code cleans the <code>text</code> column within the <code>ssids</code> dataframe. It performs each operation and sends the results down to the next operation....</p>
<p>How do I do this in a similar manner in python?</p>
",Preprocessing of the text & Tokenization,text cleaning python need clean text r could chain multiple operation code clean column within dataframe performs operation sends result next operation similar manner python
How to apply label encoding to text data(list of list),"<p>I am novice python data analyst trying to preprocess the text data (jsonl format) before it goes into Neural networks for topic modelling(VAE). I was able to clean the data and turn it into numpy array, further I wanted to apply label encoding to the cleaned text data but fail to do so. **How can one apply label encoding to list of list data format **?. The input data into label encoding is list of list and ouput has to be in same format.</p>
<pre><code>numpy array format (type: &lt;class 'numpy.ndarray'&gt;)
[array([1131,  713,  857, 1130..........]) 
 array([ 142, 1346, 1918, 1893,   61,   62, 1922,967......]) ]) 
 array([135, 148,  14, 104, 154, 159, 136,  94, 149, 135, 117,  62, 130....]) 
 array([135, 148,  14, 104, 154, 159, 136......])...................................]
</code></pre>
<p>The code is this way(after cleaning):(list of list -strings)</p>
<pre><code>    dictionary = gensim.corpora.Dictionary(process_texts) # creating a dictionary
    label_covid_data =[list(filter(lambda x: x != -1, dictionary.doc2idx(doc))) for doc in     process_texts] # converint it into numeric according to dictionary
    covid_train_data,covid_test_data = train_test_split(label_covid_data, test_size=0.2, random_state = 3456) # dividing into train and test data
    covid_train_narray = np.array([np.array(i) for i in covid_train_data]) # converting into numpy array format
    label = preprocessing.LabelEncoder() # applying label encoding 
    covid_data_labels = label.fit_transform([label.fit_transform(i) for i in covid_train_narray])
</code></pre>
<p>Error I am getting:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\sklearn\preprocessing\_label.py in _encode(values, uniques, encode, check_unknown)
    111         try:
--&gt; 112             res = _encode_python(values, uniques, encode)
    113         except TypeError:

    C:\ProgramData\Anaconda3\lib\site-packages\sklearn\preprocessing\_label.py in _encode_python(values, uniques, encode)
     59     if uniques is None:
---&gt; 60         uniques = sorted(set(values))
     61         uniques = np.array(uniques, dtype=values.dtype)
TypeError: unhashable type: 'numpy.ndarray'
During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
&lt;ipython-input-217-ebce4e37aad8&gt; in &lt;module&gt;
  4 label = preprocessing.LabelEncoder()
  5 #movie_line_labels = label.fit_transform(covid_train_narray[0])
 ----&gt; 6 covid_data_labels = label.fit_transform([label.fit_transform(i) for i in covid_train_narray])
  7 covid_data_labels
C:\ProgramData\Anaconda3\lib\site-packages\sklearn\preprocessing\_label.py in fit_transform(self, y)
250         &quot;&quot;&quot;
251         y = column_or_1d(y, warn=True)
 --&gt; 252         self.classes_, y = _encode(y, encode=True)
253         return y
254 

C:\ProgramData\Anaconda3\lib\site-packages\sklearn\preprocessing\_label.py in _encode(values, uniques, encode, check_unknown)
    112             res = _encode_python(values, uniques, encode)
    113         except TypeError:
--&gt;     114             raise TypeError(&quot;argument must be a string or number&quot;)
    115         return res
    116     else:

    TypeError: argument must be a string or number



     
</code></pre>
",Preprocessing of the text & Tokenization,apply label encoding text data list list novice python data analyst trying preprocess text data jsonl format go neural network topic modelling vae wa able clean data turn numpy array wanted apply label encoding cleaned text data fail one apply label encoding list list data format input data label encoding list list ouput ha format code way cleaning list list string error getting
KeyError when cleaning tweets column using stop words in python,"<p>I have a data frame of tweets and I'm trying to clean my 'tweet' column- remove stop words and use lemmatization.</p>
<p>Below is my code:</p>
<pre><code>stop_words = set(stopwords.words('english'))
lemmatizer= WordNetLemmatizer()

sentence = df['tweet'].apply(nltk.sent_tokenize)

 0 [ 'country year happy']
 1 [ 'wish happy year']
 2 [ 'live year together']

for i in range(len(sentence)): 
    words=nltk.word_tokenize(str(sentence[i]))
    words=[lemmatizer.lemmatize(word) for word in words if word not in 
          set(stopwords.words('english'))]
    sentence[i]=' '.join(words)
</code></pre>
<p>The code above gives me the following error: (I included all the traceback)</p>
<pre><code> KeyError  Traceback (most recent call last)
&lt;ipython-input-384-f4bb836363e1&gt; in &lt;module&gt;
  1 for i in range(len(sentence)):
----&gt; 2     words=nltk.word_tokenize(str(sentence[i]))
  3     words=[lemmatizer.lemmatize(word) for word in words if word not in 
      set(stopwords.words('english'))]
  4     sentence[i]=' '.join(words)

~\anaconda3\lib\site-packages\pandas\core\series.py in __getitem__(self, key)
   869         key = com.apply_if_callable(key, self)
   870         try:
   --&gt; 871     result = self.index.get_value(self, key)
   872 
   873             if not is_scalar(result):

~\anaconda3\lib\site-packages\pandas\core\indexes\base.py in get_value(self, 
  series, key)
  4403         k = self._convert_scalar_indexer(k, kind=&quot;getitem&quot;)
  4404         try:
  -&gt; 4405             return self._engine.get_value(s, k, 
  tz=getattr(series.dtype, &quot;tz&quot;, None))
  4406         except KeyError as e1:
  4407             if len(self) &gt; 0 and (self.holds_integer() or 
  self.is_boolean()):

  pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_value()

  pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_value()

  pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()

  pandas\_libs\hashtable_class_helper.pxi in 
  pandas._libs.hashtable.Int64HashTable.get_item()

  pandas\_libs\hashtable_class_helper.pxi in 
  pandas._libs.hashtable.Int64HashTable.get_item()

  KeyError: 34
</code></pre>
<p>How can I fix the error?</p>
<p>Also, how can I get the result in my data frame- add another column with the results?</p>
",Preprocessing of the text & Tokenization,keyerror cleaning tweet column using stop word python data frame tweet trying clean tweet column remove stop word use lemmatization code code give following error included traceback fix error also get result data frame add another column result
I encountered this problem when trying to a List of tokenized words to a String in Python,"<p>This is the code:</p>
<pre><code>import spacy
nlp = spacy.load('en_core_web_sm')

def listtostring(s): #Start the function to covert the content of the list into a string
    str1 = ''
    return str1.join(s)


new = &quot;&quot;
responese = []

reply = nlp(str(input(&quot;Input your queries:&quot;)))

for word in reply: #Removing the stop words and putting the non-stop words into the list 
    if word.is_stop == False:
        if spacy.tokens.token.Token == &quot;&quot;:
            break
        responese.append(word)

s = responese

print(listtostring(s)) #Print the result of the string
</code></pre>
<p>When I tried running the code, it gave me this error:</p>
<p><code>TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found.*</code></p>
<p>I have tried using some of the sample codes from <a href=""https://www.geeksforgeeks.org/python-program-to-convert-a-list-to-string/"" rel=""nofollow noreferrer"">here</a> but all of them did not work.</p>
<p>So is there anyway to untokenize the words in the list or any alternatives to solve this problem?</p>
",Preprocessing of the text & Tokenization,encountered problem trying list tokenized word string python code tried running code gave error tried using sample code work anyway untokenize word list alternative solve problem
How to go about creating a parse tree in Python for grammar,"<p>I am trying to create a grammar parser for a general sentence structure I am studying. I have a lot of the logic down as the grammar structure is similar between most sentences. i.e. I have hardcoded if statements that check for indices of certain keywords and tokenize them appropriately which works to a degree. However, I would like to find a way to take this as a much more object oriented approach with a tree structure.</p>
<p>example sentences: <code>bob one three enemies flank south</code></p>
<p>Example tokenization:</p>
<p><code>bob</code> is a &quot;name&quot; token,  <code>one</code> is additional name information since it comes right after the first word in the sentence.</p>
<p><code>enemies</code> is an &quot;enemy declaration&quot; token and <code>three</code> is additional enemy information since it comes right before the word &quot;enemies&quot;.</p>
<p><code>flank</code> is an action token and <code>south</code> is an &quot;action&quot; direction since it comes one word after the action.</p>
<p>These rules are consistent among all of the sentences in the language I am parsing.</p>
<p>So ideally what I want to do is a tree model architecture that parses the whole sentence, one word at a time, and tokenizes the special keywords like &quot;enemies&quot; or &quot;flank&quot;, but I also need a way to capture the additional information that is attached to each keyword in proximity (preceding or succeeding), as given in the examples above. This is likely where my if statement logic would come in.</p>
<p>How can I set up an OOP approach to appending extra information into these classes. My early thoughts are to create <code>ClassTokens</code> for the keywords, but how would I handle attaching the descriptive words? Should I make classes for each of those words too or is this redundant?</p>
<p>Also, I imagine I need a separate class like a &quot;Parser&quot; that parses the entire sentence and maintains the sentence state as well as some lookahead or look prior functions? Just trying to brainstorm ideas and get some thoughts on how to make this flow together. Thank you.</p>
<p>E.g.</p>
<pre><code>class Name:
      def __init__(self, name_keyword: str):
           self.name = name_word
           self.additional = []
          
      def add_additional(self, add_word: str):
           # if stuff then add it to self.additional

class Parser:
      def __init__(self, full_sentence: str):
           self.sentence = full_sentence.split() # initialize full sentence of words
           self.index = 0 # starting index

      def lookahead(self, n: int):
          return self.sentence[self.index + n]
   
      def lookprior(self, n: int):
          return self.sentence[self.index - n]
      
</code></pre>
",Preprocessing of the text & Tokenization,go creating parse tree python grammar trying create grammar parser general sentence structure studying lot logic grammar structure similar sentence e hardcoded statement check index certain keywords tokenize appropriately work degree however would like find way take much object oriented approach tree structure example sentence example tokenization name token additional name information since come right first word sentence enemy declaration token additional enemy information since come right word enemy action token action direction since come one word action rule consistent among sentence language parsing ideally want tree model architecture par whole sentence one word time tokenizes special keywords like enemy flank also need way capture additional information attached keyword proximity preceding succeeding given example likely statement logic would come set oop approach appending extra information class early thought create keywords would handle attaching descriptive word make class word redundant also imagine need separate class like parser par entire sentence maintains sentence state well lookahead look prior function trying brainstorm idea get thought make flow together thank e g
Equate strings based on meaning,"<p>Is there a way to equate strings in python based on their meaning despite not being similar.
For example,</p>
<ol>
<li>temp. Max</li>
<li>maximum ambient temperature</li>
</ol>
<p>I've tried using <strong>fuzzywuzzy</strong> and <strong>difflib</strong> and although they are generally good for this using token matching, they also provide false positives when I threshold the outputs over a large number of strings.
Is there some other method using <strong>NLP</strong> or <strong>tokenizatio</strong>n that I'm missing here?</p>
<p>Edit:
The answer provided by A CO does solve the problem mentioned above but is there any way to match specific substrings using word2vec from a key?
e.g. Key = max temp
Sent = the maximum ambient temperature expected tomorrow in California is 34 degrees.</p>
<p>So here I'd like to get the substring &quot;maximum ambient temperature&quot;. Any tips on that?</p>
",Preprocessing of the text & Tokenization,equate string based meaning way equate string python based meaning despite similar example temp max maximum ambient temperature tried using fuzzywuzzy difflib although generally good using token matching also provide false positive threshold output large number string method using nlp tokenization missing edit answer provided co doe solve problem mentioned way match specific substring using word vec key e g key max temp sent maximum ambient temperature expected tomorrow california degree like get substring maximum ambient temperature tip
"Python NLP/regex: How to split a long text into two parts, but ensure that sentences are not split up?","<p>I have a text with 50000 characters. I am using an APP to process the text but the APP can only process text up to 10000 characters in length. So I have to separate the text into at least 5 parts.</p>
<p>The simple way to separate the text is text[:10000], text[10000:20000], ..., This way may split a sentence into two parts, which is not what I want.</p>
<p>Another way is using tokenize.sent_tokenize(text) to separate sentences, but the output of this way is a list of all separated sentences. It is too ineffective because I do not want to separate all sentences.</p>
<p>Is there any effective ways to separate a long text into several parts?</p>
",Preprocessing of the text & Tokenization,python nlp regex split long text two part ensure sentence split text character using app process text app process text character length separate text least part simple way separate text text text way may split sentence two part want another way using tokenize sent tokenize text separate sentence output way list separated sentence ineffective want separate sentence effective way separate long text several part
ENTITIES matching conflicting in LUIS,"<p>I have two entities created in LUIS. One entity to identify the AlphaNumeric word and another one to identify a word with a pattern. Both entities are created using a regular expression.</p>
<p>To identify alphanumeric I used - <b>\w+\d+</b> regular expression.</p>
<p>To identify the word with a pattern I used - <b>^venid\d+</b> (words like venid12345, venid32310...)</p>
<p>These two entities are mapped to two different INTENTS. But actually how much I trained the LUIS, still the first entity is only getting recognized. How to overcome this?</p>
",Preprocessing of the text & Tokenization,entity matching conflicting luis two entity created luis one entity identify alphanumeric word another one identify word pattern entity created using regular expression identify alphanumeric used w regular expression identify word pattern used venid word like venid venid two entity mapped two different intent actually much trained luis still first entity getting recognized overcome
spaCy: optimizing tokenization,"<p>I'm currently trying to tokenize a text file where each line is the body text of a tweet:</p>

<pre><code>""According to data reported to FINRA, short volume percent for $SALT clocked in at 39.19% on 12-29-17 http://www.volumebot.com/?s=SALT""
""@Good2go @krueb The chart I posted definitely supports ng going lower.  Gobstopper&amp;#39; 2.12, might even be conservative.""
""@Crypt0Fortune Its not dumping as bad as it used to....""
""$XVG.X LOL. Someone just triggered a cascade of stop-loss orders and scooped up morons&amp;#39; coins. Oldest trick in the stock trader&amp;#39;s book.""
</code></pre>

<p>The file is 59,397 lines long (a day's worth of data) and I'm using spaCy for pre-processing/tokenization. It's currently taking me around 8.5 minutes and I was wondering if there were any way of optimising the following code to be quicker as 8.5 minutes seems awfully long for this process:</p>

<pre><code>def token_loop(path):
    store = []
    files = [f for f in listdir(path) if isfile(join(path, f))]

    start_time = time.monotonic()
    for filename in files:
        with open(""./data/""+filename) as f:
            for line in f:
                tokens = nlp(line.lower())
                tokens = [token.lemma_ for token in tokens if not token.orth_.isspace() and token.is_alpha and not token.is_stop and len(token.orth_) != 1]
                store.append(tokens)

    end_time = time.monotonic()
    print(""Time taken to tokenize:"",timedelta(seconds=end_time - start_time))

    return store
</code></pre>

<p>Although it says files, it's currently only looping over 1 file.</p>

<p>Just to note, I only need this to tokenize the content; I don't need any extra tagging etc.</p>
",Preprocessing of the text & Tokenization,spacy optimizing tokenization currently trying tokenize text file line body text tweet file line long day worth data using spacy pre processing tokenization currently taking around minute wa wondering way optimising following code quicker minute seems awfully long process although say file currently looping file note need tokenize content need extra tagging etc
what is use case of Tokenization and Lemmatization in NLP when we have CountVectorizer and Tfidfvectorizer,"<p>i am learning the NLP and gone through;tokenization,Lemmatization Parts of speech and other basics.
I came to know CountVectorizer  and Tfidfvectorizer are there from sklearn which having internal ability to apply tokenization, Lemmatization.</p>
<p>so question is :</p>
<p>when i need use the core NLP activities to get the vocabulary instead of using CountVectorizer  and Tfidfvectorizer?</p>
",Preprocessing of the text & Tokenization,use case tokenization lemmatization nlp countvectorizer tfidfvectorizer learning nlp gone tokenization lemmatization part speech basic came know countvectorizer tfidfvectorizer sklearn internal ability apply tokenization lemmatization question need use core nlp activity get vocabulary instead using countvectorizer tfidfvectorizer
Tokenization for Google&#39;s Public Word2Vec,"<p>I'm trying to tokenize my text in a way that is compatible with Google's pretrained word2vec <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">https://code.google.com/archive/p/word2vec/</a> however I could not find any code associated with the preprocessing nor training here.  I did find this link: <a href=""https://github.com/dav/word2vec"" rel=""nofollow noreferrer"">https://github.com/dav/word2vec</a> but it seems it also does not have preprocessing.  For example, in Google's word2vec they have vocab items like <code>####.##</code> which I assume means four numbers followed by a dot followed by two numbers.  I have managed to produce a tokenizer which produces somewhat the expected format but it would be nice to have something official.  Anyone know of anything?</p>
",Preprocessing of the text & Tokenization,tokenization google public word vec trying tokenize text way compatible google pretrained word vec however could find code associated preprocessing training find link seems also doe preprocessing example google word vec vocab item like assume mean four number followed dot followed two number managed produce tokenizer produce somewhat expected format would nice something official anyone know anything
String matching keywords and key phrases in Python,"<p>I am trying to perform a smart dynamic lookup with strings in Python for a NLP-like task. I have a large amount of similar-structure sentences that I would like to parse through each, and tokenize parts of the sentence. For example, I first parse a string such as <code>&quot;bob goes to the grocery store&quot;</code>.</p>
<p>I am taking this string in, splitting it into words and my goal is to look up matching words in a keyword list. Let's say I have a list of single keywords such as <code>&quot;store&quot;</code> and a list of keyword phrases such as <code>&quot;grocery store&quot;</code>.</p>
<pre><code>sample = 'bob goes to the grocery store'
keywords = ['store', 'restaurant', 'shop', 'office']
keyphrases = ['grocery store', 'computer store', 'coffee shop']

for word in sample.split():
    # do dynamic length lookups
</code></pre>
<p>Now the issue is this Sometimes my sentences might be simply <code>&quot;bob goes to the store&quot;</code> instead of <code>&quot;bob goes to the grocery store&quot;</code>.</p>
<p>I want to find the keyword <code>&quot;store&quot;</code> for sure but if there are descriptive words such as <code>&quot;grocery&quot;</code> or <code>&quot;computer&quot;</code> before the word store I would like to capture that as well. That is why I have the keyphrases list as well. I am trying to figure out a way to basically capture a keyword at the very least then if there are words related to it that might be a possible <code>&quot;phrase&quot;</code> I want to capture those too.</p>
<p>Maybe an alternative is to have some sort of adjective list instead of a phrase list of multiple words?</p>
<p>How could I go about doing these sort of variable length lookups where I look at more than just a single word if one is captured, or is there an entirely different method I should be considering?</p>
",Preprocessing of the text & Tokenization,string matching keywords key phrase python trying perform smart dynamic lookup string python nlp like task large amount similar structure sentence would like parse tokenize part sentence example first parse string taking string splitting word goal look matching word keyword list let say list single keywords list keyword phrase issue sometimes sentence might simply instead want find keyword sure descriptive word word store would like capture well keyphrases list well trying figure way basically capture keyword least word related might possible want capture maybe alternative sort adjective list instead phrase list multiple word could go sort variable length lookup look single word one captured entirely different method considering
gender classification of blog authors,"<p>I am working on gender classification of blog authors.I am using Weka for classification. The classifiers are SVM and Naive Bayesian classifier, but my accuracy is between 50% to 60% and
now am doubt about my feature set. The features I am using are POStags, words ending with ale,ably etc.,blog words, stemming etc..
I am referencing this paper</p>
<p><a href=""https://www.aclweb.org/anthology/D10-1021/"" rel=""nofollow noreferrer"">Paper Link</a></p>
<p>Does anyone have any ideas what a better feature set would be?</p>
",Preprocessing of the text & Tokenization,gender classification blog author working gender classification blog author using weka classification classifier svm naive bayesian classifier accuracy doubt feature set feature using postags word ending ale ably etc blog word stemming etc referencing paper paper link doe anyone idea better feature set would
Remove custom stopwords,"<p>I am trying to remove stopwords during an NLP pre-processing step. I use the <code>remove_stopwords()</code> function from <code>gensim</code> but would also like to add my own stopwords</p>
<pre><code># under this method, these custom stopwords still show up after processing
custom_stops = [&quot;stopword1&quot;, &quot;stopword2&quot;]
data_text['text'].apply(lambda x: [item for item in x if item not in custom_stops])
# remove stopwords with gensim
data_text['filtered_text'] = data_text['text'].apply(lambda x: remove_stopwords(x.lower()))
# split the sentences into a list
data_text['filtered_text'] = data_text['filtered_text'].apply(lambda x: str.split(x))
</code></pre>
",Preprocessing of the text & Tokenization,remove custom stopwords trying remove stopwords nlp pre processing step use function would also like add stopwords
Summing rows that contain strings from reference table in R,"<p>For a list of strings that exist as rows within a table, I want to identify the frequency of those strings within rows of another data table in R. Simultaneously, I want to sum the values of rows that contain these strings.</p>
<p>For example, my reference table which contains a list of strings would look something like this:</p>
<pre><code>+-----------------------------+
|String                       |
+-----------------------------+
|Dixon                        |
+-----------------------------+
|Nina Kraviz                  |
+-----------------------------+
|DJ Tennis                    |
+-----------------------------+
</code></pre>
<p>And my table that I want to analyze would look something like this:</p>
<pre><code>+--------------------------------+
|String                |Score    |
+--------------------------------+
|Nina Kraviz @ Hyde    |100      |
+--------------------------------+
|DJ Tennis?            |200      |
+--------------------------------+
|From Dixon            |100      |
+--------------------------------+
|From Kevin Saunderson |100      |
+--------------------------------+
|Dixon                 |300      |
+--------------------------------+
|Nina Kraviz           |200      |
+--------------------------------+
</code></pre>
<p>I want my resulting table to look like this:</p>
<pre><code>+---------------------------------+
|String             |Score        |
+---------------------------------+
|Dixon              |400          |
+---------------------------------+
|Nina Kraviz        |300          |
+---------------------------------+
|DJ Tennis          |200          |
+---------------------------------+
</code></pre>
<p>I've tried using n-grams and tokenizing but it's not working in a way that makes it easy, as artists names can contain 1, 2 or 3 words commonly. Any help would be appreciated.</p>
",Preprocessing of the text & Tokenization,summing row contain string reference table r list string exist row within table want identify frequency string within row another data table r simultaneously want sum value row contain string example reference table contains list string would look something like table want analyze would look something like want resulting table look like tried using n gram tokenizing working way make easy artist name contain word commonly help would appreciated
My code removed all punctuation from text but do we need few of them for sentimental analysis?,"<pre><code>def remove_punctuation(review):
    lst = []
    for text in review:
        if text not in string.punctuation:
            lst.append(text)
    return &quot;&quot;.join(lst)
df.Review = df.Review.apply(lambda x: remove_punctuation(x))
</code></pre>
<p>I am working on the sentimental analysis of amazon product reviews. I am preprocessing the reviews' text and used the above function to remove punctuation. It has removed all of them, but my question is that do we consider some of them for sentimental analysis. Like !. Is it the right approach.</p>
<p><strong>Thanks for your help and time.</strong></p>
",Preprocessing of the text & Tokenization,code removed punctuation text need sentimental analysis working sentimental analysis amazon product review preprocessing review text used function remove punctuation ha removed question consider sentimental analysis like right approach thanks help time
NLP- sentiment analysis using word vectors,"<p>I have a code that does the following:</p>
<ul>
<li>Generate word vectors using brown corpus fron nltk</li>
<li>maintain 2 list, one having few positive sentimental words (eg: good, happy, nice) and other negative sentimental words (ed. bad, sad, unnhappy)</li>
<li>Define a statement whose sentiment we wish to obtain.</li>
<li>perform preprocessing on this statement (tokenize, lowercase, remove special characters, remove stopwords, lemmatize words</li>
<li>Generate word vectors for all these words and store it in a list</li>
<li>I have a test sentence of 7 words and I wish to determine its sentiment. First I define two lists:</li>
</ul>
<ol>
<li>good_words=[good, excellent, happy]</li>
<li>bad_words=[bad,terrible,sad]</li>
</ol>
<p>Now I run a loop taking i words at a time where i ranges from 1 to sentence length. For a particular i, I have few windows of words that span the test sentence. For each window, I take average of word vectors of the window and compute euclidian distance of this windowed vector and the 2 lists.For example i= 3, and test sentence: food looks fresh healthy. I will have 2 windows: food looks fresh and looks fresh healthy for i =3. Now I take mean of vectors of the words in each window and compute euclidian distance with the good_words and bad_words. So corresponding to each word in both lists I will have 2 values(for 2 windows). Now I take mean of these 2 values for each word in the lists and whichever word has least distance lies closest to the test sentence.</p>
<p>I wish to show that window size(i) = 3 or 4 shows highest accuracy in determining the sentiment of test sentence but I am facing difficulty in achieving it. Any leads on how I can produce my results would be highly appreciated.</p>
<p>Thanks in advance.</p>
<pre><code>b = Word2Vec(brown.sents(), window=5, min_count=5, negative=15,  size=50, iter= 10, workers=multiprocessing.cpu_count())
pos_words=['good','happy','nice','excellent','satisfied']
neg_words=['bad','sad','unhappy','disgusted','afraid','fearful','angry']
pos_vec=[b[word] for word in pos_words]
neg_vec=[b[word] for word in neg_words]


test=&quot;Sound quality on both end is excellent.&quot;
tokenized_word= word_tokenize(test)
lower_tokens= convert_lowercase(tokenized_word)
alpha_tokens= remove_specialchar(lower_tokens)
rem_tokens= removestopwords(alpha_tokens)
lemma_tokens= lemmatize(rem_tokens)
word_vec=[b[word] for word in lemma_tokens]

for i in range(0,len(lemma_tokens)):
    windowed_vec=[]
    for j in range(0,len(lemma_tokens)-i):
        windowed_vec.append(np.mean([word_vec[j+k] for k in range(0,i+1)],axis=0))
    gen_pos_arr=[]
    gen_neg_arr=[]
    for p in range(0,len(pos_vec)):
        gen_pos_arr.append([euclidian_distance(vec,pos_vec[p]) for vec in windowed_vec])
    for q in range(0,len(neg_vec)):
        gen_neg_arr.append([euclidian_distance(vec,neg_vec[q]) for vec in windowed_vec])
    gen_pos_arr_mean=[]
    gen_pos_arr_mean.append([np.mean(x) for x in gen_pos_arr])
    gen_neg_arr_mean=[]
    gen_neg_arr_mean.append([np.mean(x) for x in gen_neg_arr])
    min_value=np.min([np.min(gen_pos_arr_mean),np.min(gen_neg_arr_mean)])
    for v in gen_pos_arr_mean:
        print('min value:',min_value)
        if min_value in v:
            print('pos',v)
            plt.scatter(i,min_value,color='blue')
            plt.text(i,min_value,pos_words[gen_pos_arr_mean[0].index(min_value)])
        else:
            print('neg',v)
            plt.scatter(i,min_value,color='red')
            plt.text(i,min_value,neg_words[gen_neg_arr_mean[0].index(min_value)])
print(test)
plt.title('')
plt.xlabel('window size')
plt.ylabel('avg of distances of windows from sentiment words')
plt.show()
</code></pre>
",Preprocessing of the text & Tokenization,nlp sentiment analysis using word vector code doe following generate word vector using brown corpus fron nltk maintain list one positive sentimental word eg good happy nice negative sentimental word ed bad sad unnhappy define statement whose sentiment wish obtain perform preprocessing statement tokenize lowercase remove special character remove stopwords lemmatize word generate word vector word store list test sentence word wish determine sentiment first define two list good word good excellent happy bad word bad terrible sad run loop taking word time range sentence length particular window word span test sentence window take average word vector window compute euclidian distance windowed vector list example test sentence food look fresh healthy window food look fresh look fresh healthy take mean vector word window compute euclidian distance good word bad word corresponding word list value window take mean value word list whichever word ha least distance lie closest test sentence wish show window size show highest accuracy determining sentiment test sentence facing difficulty achieving lead produce result would highly appreciated thanks advance
NLP search with elasticsearch and dense vectors,"<p>I'm developing an NLP search engine based on vectors and the MUSE (Multilingual Universal Sentence Encoder) model from Google.</p>
<p>The process is very simple:</p>
<p>1 - I have a folder with 34 pdf documents (190 MB)</p>
<p>2 - I convert this documents to plain text</p>
<p>3 - I tokenize the document´s plain text to sentences</p>
<p>4 - I use MUSE to encode every sentence in a 512 dimension vector</p>
<p>5 - I store the sentences + the vectors in elasticsearch, the vectors are stored in the &quot;dense_vector&quot;
field</p>
<p>6 - When I do the search, I vectorize the search input (with MUSE, obviously) and I use the cosineSimilarity function from elastic to get the most relevant sentences.</p>
<p>So far so good, <strong>the search engine works with very decent results</strong>.</p>
<p>The problem here is the size of the index in elasticsearch, to store the 34 documents (190MB) took 560MB!, <strong>the index with the sentences + vectors is almost 3 times the size of the original documents in pdf</strong>.</p>
<p>I have read a lot of publications about the state of the art in NLP search engines and all of them point to sentence encoding (embeddings) and calculate the cosin to get the most relevant results, but any of them talks about a real world implementation.</p>
<p>Any of you have experience with something similar?, how do you deal with vectors storage?</p>
<p>Thanks!</p>
",Preprocessing of the text & Tokenization,nlp search elasticsearch dense vector developing nlp search engine based vector muse multilingual universal sentence encoder model google process simple folder pdf document mb convert document plain text tokenize document plain text sentence use muse encode every sentence dimension vector store sentence vector elasticsearch vector stored dense vector field search vectorize search input muse obviously use cosinesimilarity function elastic get relevant sentence far good search engine work decent result problem size index elasticsearch store document mb took mb index sentence vector almost time size original document pdf read lot publication state art nlp search engine point sentence encoding embeddings calculate cosin get relevant result talk real world implementation experience something similar deal vector storage thanks
Abstract regular expressions,"<p>I am working on processing queries in russian language (translating them into SQL code to be executed). I tokenize the query, do morphological analysis, from lemmas I get links to database objects. So now I want to use patterns like regular expressions to get things like conditions, <code>ORDER BY</code> expressions and so on. But the thing is, regex can only be used with list of characters (strings).</p>

<p>Is there a library/solution for Python (preferably) which works like regular expressions but for any kinds of objects (list of tokens with grammatical properties or database objects), not just strings?</p>

<p>So, as an example, I want to write patterns which would look something this:</p>

<pre><code>[db-column]((','|'and')[db-column])*
</code></pre>

<p>this pattern would match a list of database objects like this: columnA, columnB and columnC.</p>
",Preprocessing of the text & Tokenization,abstract regular expression working processing query russian language translating sql code executed tokenize query morphological analysis lemma get link database object want use pattern like regular expression get thing like condition expression thing regex used list character string library solution python preferably work like regular expression kind object list token grammatical property database object string example want write pattern would look something pattern would match list database object like columna columnb columnc
Elasticsearch Analyzers for text Analysis,"<p>i am new to Elasticsearch and willing to use for a full-text search engine.
For Text analysis i need to work with (multilingual) language Analyzers. Elasticsearch offers built in language Analyzers but i am not sure if they cover preprocessing steps like: removing stop words, stemming, removing unwanted characters etc. I will be working with multiple-field, because all (descriptions) languages are indexed in the same fiel in a document. Is a mapping like this correct in this case?</p>
<pre><code>{
&quot;mappings&quot;: {
    &quot;properties&quot;: {
        &quot;description&quot;: {
            &quot;type&quot;: &quot;text&quot;,
            &quot;analyzer&quot;: &quot;english&quot;
        },
        &quot;description&quot;: {
            &quot;type&quot;: &quot;text&quot;,
            &quot;analyzer&quot;: &quot;german&quot;
        },
        &quot;description&quot;: {
            &quot;type&quot;: &quot;text&quot;,
            &quot;analyzer&quot;: &quot;french&quot;
        }
    }
 }
</code></pre>
<p>i am confused how to use language analyzers to analyze the input-text and when do we use mappings instead of settings?</p>
",Preprocessing of the text & Tokenization,elasticsearch analyzer text analysis new elasticsearch willing use full text search engine text analysis need work multilingual language analyzer elasticsearch offer built language analyzer sure cover preprocessing step like removing stop word stemming removing unwanted character etc working multiple field description language indexed fiel document mapping like correct case confused use language analyzer analyze input text use mapping instead setting
Preprocessing corpus stored in DataFrame with NLTK,"<p>I'm learning NLP and I'm trying to understand how to perform pre-processing on a corpus stored in a pandas DataFrame.
So let's say I have this:</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd

doc1 = """"""""Whitey on the Moon"" is a 1970 spoken word poem by Gil Scott-Heron. It was released as the ninth track on Scott-Heron's debut album Small Talk at 125th and Lenox. It tells of medical debt and poverty experienced during the Apollo Moon landings. The poem critiques the resources spent on the space program while Black Americans were experiencing marginalization. ""Whitey on the Moon"" was prominently featured in the 2018 biographical film about Neil Armstrong, First Man.""""""
doc2 = """"""St Anselm's Church is a Roman Catholic church which is part of the Personal Ordinariate of Our Lady of Walsingham in Pembury, Kent, England. It was originally founded in the 1960s as a chapel-of-ease before becoming its own quasi-parish within the personal ordinariate in 2011, following a conversion of a large number of disaffected Anglicans in Royal Tunbridge Wells.""""""
doc3 = """"""Nymphargus grandisonae (common name: giant glass frog, red-spotted glassfrog) is a species of frog in the family Centrolenidae. It is found in Andes of Colombia and Ecuador. Its natural habitats are tropical moist montane forests (cloud forests); larvae develop in streams and still-water pools. Its habitat is threatened by habitat loss, introduced fish, and agricultural pollution, but it is still a common species not considered threatened by the IUCN.""""""

df = pd.DataFrame({'text': [doc1, doc2, doc3]})
</code></pre>

<p>Which results in:</p>

<pre><code>+---+---------------------------------------------------+
|   |                                              text |
+---+---------------------------------------------------+
| 0 | ""Whitey on the Moon"" is a 1970 spoken word poe... |
+---+---------------------------------------------------+
| 1 | St Anselm's Church is a Roman Catholic church ... |
+---+---------------------------------------------------+
| 2 | Nymphargus grandisonae (common name: giant gla... |
+---+---------------------------------------------------+
</code></pre>

<p>Now, I load what I need and tokenize the text:</p>

<pre class=""lang-py prettyprint-override""><code>import nltk
import string
from nltk.tokenize import sent_tokenize, word_tokenize
nltk.download('punkt')
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

df['tokenized_text'] = df['text'].apply(word_tokenize)
df
</code></pre>

<p>Which gives the following output:</p>

<pre><code>+---+---------------------------------------------------+---------------------------------------------------+
|   |                                              text |                                    tokenized_text |
+---+---------------------------------------------------+---------------------------------------------------+
| 0 | ""Whitey on the Moon"" is a 1970 spoken word poe... | [``, Whitey, on, the, Moon, '', is, a, 1970, s... |
+---+---------------------------------------------------+---------------------------------------------------+
| 1 | St Anselm's Church is a Roman Catholic church ... | [St, Anselm, 's, Church, is, a, Roman, Catholi... |
+---+---------------------------------------------------+---------------------------------------------------+
| 2 | Nymphargus grandisonae (common name: giant gla... | [Nymphargus, grandisonae, (, common, name, :, ... |
+---+---------------------------------------------------+---------------------------------------------------+
</code></pre>

<p>Now, my problem occurs when removing stop words:</p>

<pre class=""lang-py prettyprint-override""><code>df['tokenized_text'] = df['tokenized_text'].apply(lambda words: [word for word in words if word not  in [stop_words] + list(string.punctuation)])
</code></pre>

<p>Which looks like nothing happened:</p>

<pre><code>+---+---------------------------------------------------+---------------------------------------------------+
|   |                                              text |                                    tokenized_text |
+---+---------------------------------------------------+---------------------------------------------------+
| 0 | ""Whitey on the Moon"" is a 1970 spoken word poe... | [``, Whitey, on, the, Moon, '', is, a, 1970, s... |
+---+---------------------------------------------------+---------------------------------------------------+
| 1 | St Anselm's Church is a Roman Catholic church ... | [St, Anselm, 's, Church, is, a, Roman, Catholi... |
+---+---------------------------------------------------+---------------------------------------------------+
| 2 | Nymphargus grandisonae (common name: giant gla... | [Nymphargus, grandisonae, common, name, giant,... |
+---+---------------------------------------------------+---------------------------------------------------+
</code></pre>

<p>Can someone help me understand what happens and what I should do instead?</p>

<p>After that, I'd like to apply lemmatization, but that doesn't work in the current state:</p>

<pre class=""lang-py prettyprint-override""><code>lemmatizer = WordNetLemmatizer
df['tokenized_text'] = df['tokenized_text'].apply(lemmatizer.lemmatize)
</code></pre>

<p>yields:</p>

<pre><code>TypeError: lemmatize() missing 1 required positional argument: 'word'
</code></pre>

<p>Thanks!</p>
",Preprocessing of the text & Tokenization,preprocessing corpus stored dataframe nltk learning nlp trying understand perform pre processing corpus stored panda dataframe let say result load need tokenize text give following output problem occurs removing stop word look like nothing happened someone help understand happens instead like apply lemmatization work current state yield thanks
spacy tokenize apostrophe,"<p>I am trying to properly split words to fit my corpus. I'm already using <a href=""https://stackoverflow.com/questions/51012476/spacy-custom-tokenizer-to-include-only-hyphen-words-as-tokens-using-infix-regex"">this approach</a> which fixes hyphenated words, what I can't seem to figure out is <strong>how to keep words with apostrophes for contractions like: <em>can't, won't, don't, he's</em>, etc. together as one token in spacy.</strong></p>

<p><strong>More specifically I am searching how to do this for Dutch words: <em>zo'n, auto's, massa's,</em> etc. but this problem should be language-independent.</strong></p>

<p>I have the following tokenizer:</p>

<pre><code>def custom_tokenizer(nlp):
    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)
    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)
    infix_re = re.compile(r'''[.\,\?\:\;\...\‘\’\'\`\“\”\""\'~]''')

    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,
                     suffix_search=suffix_re.search,
                     infix_finditer=infix_re.finditer,
                     token_match=None)
nlp = spacy.load('nl_core_news_sm')
nlp.tokenizer = custom_tokenizer(nlp)
</code></pre>

<p>with this the tokens I get are:</p>

<p>'Mijn','eigen','huis','staat',<strong>'zo',""'"",'n'</strong>,'zes','meter','onder','het','wateroppervlak','van','de','Noordzee','.'</p>

<p>...but the tokens I expected should be:</p>

<p>'Mijn','eigen','huis','staat',<strong>""zo'n""</strong>,'zes','meter','onder','het','wateroppervlak','van','de','Noordzee','.'</p>

<p>I know it is possible to add custom rules like:</p>

<pre><code>case = [{ORTH: ""zo""}, {ORTH: ""'n"", LEMMA: ""een""}]
tokenizer.add_special_case(""zo'n"",case)
</code></pre>

<p>But I am looking for a more general solution.</p>

<p>I've tried editing the <code>infix_re</code> regex from the other thread, but I doesn't seem to have any impact on the issue. Is there any setting or change I can do to fix this?</p>
",Preprocessing of the text & Tokenization,spacy tokenize apostrophe trying properly split word fit corpus already using z meter onder het wateroppervlak van de noordzee token expected mijn eigen huis staat zo n z meter onder het wateroppervlak van de noordzee know possible add custom rule like looking general solution tried editing regex thread seem impact issue setting change fix
spacy stemming on pandas df column not working,"<p>How to apply stemming on Pandas Dataframe column</p>

<p>am using this function for stemming which is working perfect on string</p>

<pre><code>xx='kenichan dived times ball managed save 50 rest'

def make_to_base(x):
    x_list = []
    doc = nlp(x)
    for token in doc:
        lemma=str(token.lemma_)
        if lemma=='-PRON-' or lemma=='be':
            lemma=token.text
        x_list.append(lemma)
    print("" "".join(x_list))    
make_to_base(xx)
</code></pre>

<p>But when i am applying this function on my pandas dataframe column it is not working neither giving any error</p>

<pre><code>x = list(df['text']) #my df column
x = str(x)#converting into string otherwise it is giving error
make_to_base(x)
</code></pre>

<p>i've tried different thing but nothing working. like this</p>

<pre><code>df[""texts""] =  df.text.apply(lambda x: make_to_base(x))

make_to_base(df['text'])
</code></pre>

<p>my dataset looks like this:</p>

<pre><code>df['text'].head()
Out[17]: 
0    Hope you are having a good week. Just checking in
1                              K..give back my thanks.
2          Am also doing in cbe only. But have to pay.
3    complimentary 4 STAR Ibiza Holiday or £10,000 ...
4    okmail: Dear Dave this is your final notice to...
Name: text, dtype: object
</code></pre>
",Preprocessing of the text & Tokenization,spacy stemming panda df column working apply stemming panda dataframe column using function stemming working perfect string applying function panda dataframe column working neither giving error tried different thing nothing working like dataset look like
Improve Accuracy of Email Classification?,"<p>I am building an email classification model. Currently, I am using NLTK's stopwords and lemmatization during the pre-processing of data. Following are the parameters for TF-IDF vectorizer that I am using:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(sublinear_tf= True, 
                       min_df = 5, 
                       norm= 'l2', 
                       ngram_range= (1,2), 
                       stop_words ='english')
</code></pre>

<p>I am using LogisticRegression for classification.</p>

<pre><code>from sklearn.linear_model import LogisticRegression  # Logistic Regression - (Best Performance Till Now)
X_train, X_test, y_train, y_test = train_test_split(df['Rejoined_Lemmatize'], df['Product'], random_state = 0, test_size = 0.2)
X_train_counts = tfidf.fit_transform(X_train)

clf = LogisticRegression(random_state=0).fit(X_train_counts, y_train)
y_pred = clf.predict(tfidf.transform(X_test)) # Predicting using our Model

print(metrics.classification_report(y_test,y_pred, labels= df.Product, target_names=df['Product'].unique())) # Print Results
</code></pre>

<p>I am getting the following results from the above code:</p>

<pre><code>                                precision    recall  f1-score   support

    Bank account or service       0.45      0.52      0.48        46
Checking or savings account       0.60      0.52      0.56        56
            Money transfers       0.60      0.52      0.56        56
               Student loan       0.60      0.52      0.56        56
              Consumer Loan       0.86      0.86      0.86        64
                Payday loan       0.91      0.96      0.94        55
            Debt collection       0.88      0.71      0.79        62
                   Mortgage       0.88      0.71      0.79        62
           Credit reporting       0.86      0.86      0.86        64
               Prepaid card       0.81      0.80      0.81        65
                Credit card       0.60      0.52      0.56        56

                   accuracy                           0.79    198000
                  macro avg       0.79      0.79      0.78    198000
               weighted avg       0.80      0.79      0.79    198000
</code></pre>

<p>How Can I improve this accuracy??</p>

<p>Note - I am working on the ""<strong>Consumer Complaints Dataset</strong>"". I am only using 3300 rows from that database and I have balanced my database i.e 300 emails from each category</p>

<p>11 categories * 300 emails = 3300 rows.</p>
",Preprocessing of the text & Tokenization,improve accuracy email classification building email classification model currently using nltk stopwords lemmatization pre processing data following parameter tf idf vectorizer using using logisticregression classification getting following result code improve accuracy note working consumer complaint dataset using row database balanced database e email category category email row
How to Prepare Text for Machine Learning pipeline with compiled Regular Expressions and/or List Comprehension?,"<p>I am trying to prepare text in a Python function for a machine learning pipeline and I am having trouble to get the correct output. So, I want to lowercase all words, replace symbols by spaces, delete symbols and remove stopwords from nltk. I tried all kinds of different approaches from list comprehension to regex pattern matching, but I still can't get it right. Please help out! Here are the necessary imports and the basic function:</p>

<pre><code>import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

import re
</code></pre>

<p>Here is the function:</p>

<pre><code>REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def text_prepare(text):
""""""
    text: a string

    return: modified initial string
""""""

lower = text.lower() # lowercase text
space_replace = REPLACE_BY_SPACE_RE.sub("" "",lower) #replace REPLACE_BY_SPACE_RE symbols by space in text
nosymb = BAD_SYMBOLS_RE.sub("""",space_replace) # delete symbols which are in BAD_SYMBOLS_RE from text
text = [word for word in nosymb if word not in STOPWORDS] # delete                                                                                                             stopwords from text

return text
</code></pre>

<p>Here is a test function:</p>

<pre><code>def test_text_prepare():
    examples = [""SQL Server - any equivalent of Excel's CHOOSE function?"",
            ""How to free c++ memory vector&lt;int&gt; * arr?""]
    answers = [""sql server equivalent excels choose function"", 
           ""free c++ memory vectorint arr""]
    for ex, ans in zip(examples, answers):
    if text_prepare(ex) != ans:
        return ""Wrong answer for the case: '%s'"" % ex
return 'Basic tests are passed.'
</code></pre>

<p>Here is my test result:</p>

<pre><code>print(test_text_prepare())
Wrong answer for the case: 'SQL Server - any equivalent of Excel's CHOOSE function?'
</code></pre>
",Preprocessing of the text & Tokenization,prepare text machine learning pipeline compiled regular expression list comprehension trying prepare text python function machine learning pipeline trouble get correct output want lowercase word replace symbol space delete symbol remove stopwords nltk tried kind different approach list comprehension regex pattern matching still get right please help necessary import basic function function test function test result
Lemmatization for Root Word,"<p>I'm just starting to learn/play with NLP, and have come across the following behavior:</p>

<p>Using nltk, I extract the part-of-speech for words, and then lemmatize as follows:</p>

<pre><code>import nltk
from nltk import WordNetLemmatizer
from nltk.corpus import wordnet

wordnet_lemmatizer = WordNetLemmatizer()

def get_wordnet_pos(word):
    """"""Map POS tag to first character lemmatize() accepts""""""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {""J"": wordnet.ADJ,
                ""N"": wordnet.NOUN,
                ""V"": wordnet.VERB,
                ""R"": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)


def pos_and_lemmatize(str):
    return [wordnet_lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(str)]
</code></pre>

<p>Doing <code>pos_and_lemmatize</code> (or just <code>wordnet_lemmatizer.lemmatize</code>) on the following words: <code>variability, variables, variance, variation</code> just returns the same word, instead of something like <code>vary</code>. Any way to accomplish this goal?</p>
",Preprocessing of the text & Tokenization,lemmatization root word starting learn play nlp come across following behavior using nltk extract part speech word lemmatize follows following word return word instead something like way accomplish goal
Text preprocessing for text classification using fastText,"<p>What text preprocessing produces the best results for supervised text classification using <a href=""https://github.com/facebookresearch/fastText"" rel=""nofollow noreferrer"">fastText</a>?</p>

<p>The official documentation shows a only a <a href=""https://fasttext.cc/docs/en/supervised-tutorial.html#preprocessing-the-data"" rel=""nofollow noreferrer"">simple prepocessing</a> consisting of lower-casing and separating punctuations. Would classic preprocessing like lemmatization, stopwords removal, masking numbers would help?</p>
",Preprocessing of the text & Tokenization,text preprocessing text classification using fasttext text preprocessing produce best result supervised text classification using fasttext official documentation show simple prepocessing consisting lower casing separating punctuation would classic preprocessing like lemmatization stopwords removal masking number would help
removing NLTK StopWords,"<p>I am trying to remove stop words of my data set.</p>

<pre><code>stopwordsw = nltk.corpus.stopwords.words('german')

def remove_stopwords(txt_clean):
      txt_clean =  [Word for Word in txt_clean if Word not in stopwords]
      return txt_clean

data['Tweet_sw'] = data['Tweet_clean'].apply(lambda x: remove_stopwords(x))
data.head()
</code></pre>

<p>I have two problems with that. </p>

<p>First, the output is given character by character (separated by a comma), although I run the check against the list of stopwords with words. </p>

<p>I can solve this problem with a join command, but I don't understand why it is split into characters. </p>

<p>The second and real problem is that the removal of stop words does not work. Words that are clearly in the list are not removed from the sentences. </p>

<p>Where is my mistake in this?</p>

<p><a href=""https://i.sstatic.net/nv1kL.png"" rel=""nofollow noreferrer"">image</a></p>
",Preprocessing of the text & Tokenization,removing nltk stopwords trying remove stop word data set two problem first output given character character separated comma although run check list stopwords word solve problem join command understand split character second real problem removal stop word doe work word clearly list removed sentence mistake image
lemmatize() missing 1 required positional argument: &#39;word&#39;,"<p>When I try to pass this on to the lemmatizer like this:</p>

<pre><code>def lemmatization(token_txt):
    text = [wn.lemmatize(word) for word in token_txt]
   # text = [[wn.lemmatize(word) for word in l] for l in token_text]
    return text

data['Tweet_lem'] = data['Tweet_sw'].apply(lambda x:lemmatization(x))
data.head()
</code></pre>

<p>i get following error</p>

<pre><code>TypeError: lemmatize() missing 1 required positional argument: 'word'
</code></pre>

<p>when i let it run like this:</p>

<pre><code>def lemmatization(token_txt):
      # text = [wn.lemmatize(word) for word in token_txt]
        text = [[wn.lemmatize(word) for word in l] for l in token_text]
        return text


data['Tweet_lem'] = data['Tweet_sw'].apply(lambda x:lemmatization(x))
data.head()
</code></pre>

<p>i get this error</p>

<blockquote>
  <p>NameError: name 'token_text' is not defined</p>
</blockquote>

<p>What do i have to do? </p>

<p>I am trying to apply the function on punctuation and stop words removed sentences. the steps stemming and tokenization are not applied. </p>

<p><a href=""https://i.sstatic.net/o16eG.png"" rel=""nofollow noreferrer"">DataSet</a></p>

<p><a href=""https://i.sstatic.net/VcDrT.png"" rel=""nofollow noreferrer"">DataSet new</a></p>

<p><a href=""https://i.sstatic.net/IIEPv.png"" rel=""nofollow noreferrer"">Full example first part</a></p>

<p><a href=""https://i.sstatic.net/sdiRO.png"" rel=""nofollow noreferrer"">Full example second part</a></p>
",Preprocessing of the text & Tokenization,lemmatize missing required positional argument word try pas lemmatizer like get following error let run like get error nameerror name token text defined trying apply function punctuation stop word removed sentence step stemming tokenization applied dataset dataset new full example first part full example second part
Replace a list of words with one unique word in R,"<p>I am working on a text analysis with R and have a dataset (text corpus) with various sentences about different fruits. For example: ""<strong>apple</strong>"", ""<strong>banana</strong>"" , ""<strong>orange</strong>"", ""<strong>pear</strong>"", etc. </p>

<p>Since it is not relevant for the analysis whether someone writes about ""apples"" or ""bananas"", I want to replace all different fruits with one specific word, for example ""<strong>allfruits</strong>"".</p>

<p>I thought about using regex but I am facing two issues;</p>

<p>1) I want to avoid separate code lines for each kind of fruit. Thus, is there a way to define a list or a vector that I can use so that the function replaces <strong>all words</strong> in that list (apple, bananas, pear, etc.) with one specific word ""<strong>allfruits</strong>""? </p>

<p>2) I want to avoid that words that are <em>NOT</em> a fruit but contain the same string as a fruit (e.g. the word ""appletini) get replaced by the function. </p>

<p>Example: 
If I have a sentence that says: ""<strong><em>Apple</strong> is my favourite fruit, <strong>appletini</strong> is my favourite drink. I also like <strong>bananas</strong>!</em>""
I want following to happen: <strong><em>allfruits</strong> is my favourite fruit, appletini is my favourite drink. I also like <strong>allfruits</strong>!</em></p>

<p>I am not sure whether it is possible to write this with a gsub function. Thus, all help is much appreciated. </p>

<p>Thank you!</p>
",Preprocessing of the text & Tokenization,replace list word one unique word r working text analysis r dataset text corpus various sentence different fruit example apple banana orange pear etc since relevant analysis whether someone writes apple banana want replace different fruit one specific word example allfruits thought using regex facing two issue want avoid separate code line kind fruit thus way define list vector use function replaces word list apple banana pear etc one specific word allfruits want avoid word fruit contain string fruit e g word appletini get replaced function example sentence say apple favourite fruit appletini favourite drink also like banana want following happen allfruits favourite fruit appletini favourite drink also like allfruits sure whether possible write gsub function thus help much appreciated thank
tokenise on white space ONLY | sklearn - TfidfVectorizer | Python,"<p>Is there a way for me to use <code>TfidfVectorizer()</code> from <code>sklearn.feature_extraction.text</code> such that</p>

<ol>
<li><p>NOTHING is removed </p></li>
<li><p>The ONLY token separator is white space</p></li>
</ol>

<p>In the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">documentation</a> it states the argument <code>token_pattern</code> takes a regular expression denoting what constitutes a “token”. It further states by default <strong>(1) punctuation is completely ignored</strong> and <strong>(2) always treated as a token separator.</strong></p>

<p>I've managed to deal with (1) by having <code>token_pattern = r""(?u)\b\w\w+\b|!|\?|\""|\'|.|-|_""</code> <em>(am I missing any punctuation btw?)</em> but I haven't been able to deal with (2). I'm unfamilar with regex so I'm guessing it has something to do with that? Below is an example</p>

<pre><code>sentence_list = ['-docstart-', 
                'eu rejects german call to boycott british lamb .',
                 'brussels 1996-08-22']
vectorizer = TfidfVectorizer(token_pattern = r""(?u)\b\w\w+\b|!|\?|\""|\'|.|-|_"")
vectors = vectorizer.fit_transform(sentence_list)
print(vectorizer.get_feature_names())
</code></pre>

<pre><code>### What I get
[' ', '-', '.', '08', '1996', '22', 'boycott', 'british', 'brussels', 'call', 'docstart', 'eu', 'german', 'lamb', 'rejects', 'to']

### What I want
['.', '1996-08-22', 'boycott', 'british', 'brussels', 'call', '-docstart-', 'eu', 'german', 'lamb', 'rejects', 'to']

</code></pre>

<p>So essentially what I really want is just for the token separator to be ONLY white space and for NOTHING ELSE to happen. Thanks for taking your time to read this. Any help would be greatly appreciated. Thank you!</p>
",Preprocessing of the text & Tokenization,tokenise white space sklearn tfidfvectorizer python way use nothing removed token separator white space documentation state argument take regular expression denoting constitutes token state default punctuation completely ignored always treated token separator managed deal missing punctuation btw able deal unfamilar regex guessing ha something example essentially really want token separator white space nothing else happen thanks taking time read help would greatly appreciated thank
CountVectorizer takes too long to fit_transform,"<pre><code>def tokenize(text):
    text = re.sub('[^a-zA-Z0-9]', ' ', text)
    words = word_tokenize(text)
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(w.lower().strip()) for w in words if w not in stopwords.words()]
    return words

pipeline = Pipeline([
('vect', CountVectorizer(tokenizer=tokenize)),
#     ('tfidf', TfidfTransformer()),
#     ('clf', MultiOutputClassifier(RandomForestClassifier()))
])
</code></pre>

<p>Given above code, CountVectorizer takes too long (ran for 60 minutes but it did not finished) to fit but if I remove line <code>if w not in stopwords.words()</code> it just take 5 minutes to fit, what could be problem and possible solution with this code. I am using stop words from nltk.corpus.</p>

<p>Note: tokenize function works fine, using separately for any text input.</p>

<p>Thank you</p>
",Preprocessing of the text & Tokenization,countvectorizer take long fit transform given code countvectorizer take long ran minute finished fit remove line take minute fit could problem possible solution code using stop word nltk corpus note tokenize function work fine using separately text input thank
"preprocessing tweets, remove @ and # , eliminate stop words and remove user from list of list in python","<p>i wrote the code below but now I want to p reprocess, so I transformed to lower, I wrote some word to eliminate stop words but it does not work, and I want to remove @ and # and also remove user , can you help me?  </p>

<pre><code>


! pip install wget
import wget
url = 'https://raw.githubusercontent.com/dirkhovy/NLPclass/master/data/tweets_en.txt'
wget.download(url, 'tweets_en.txt')
tweets = [line.strip() for line in open('tweets_en.txt', encoding='utf8')]

import spacy
from collections import Counter
# your code here
import itertools
nlp = spacy.load('en')

#Creates a list of lists of tokens
tokens = [[token.text for token in nlp(sentence)] for sentence in tweets[:200]]
print(tokens)


#to lower
token_l=[[w.lower() for w in line] for line in tokens]
token_l[:1]

#remove #

#remove stop word

#remove user

#remove @

from nltk.corpus import stopwords

# filtered_words = [[w for w in line] for line in tokens if w not in # stopwords.words('english')]


</code></pre>
",Preprocessing of the text & Tokenization,preprocessing tweet remove eliminate stop word remove user list list python wrote code want p reprocess transformed lower wrote word eliminate stop word doe work want remove also remove user help
Deep LSTM accuracy not crossing 50%,"<p>I am working on a classification problem of the semeval 2017 task 4A <a href=""http://alt.qcri.org/semeval2017/task4/index.php?id=data-and-tools"" rel=""nofollow noreferrer"">dataset can be found here</a>
and I am using deep LSTM network for it. In pre-processing, I have done lower casing-&gt;tokenization-&gt;lemmatization-&gt;removing stop words-&gt;removing punctuations. For word embeddings, I have used WORD2VEC model. There are 18,000 samples in my training set and 2000 samples in testing.</p>
<p>The code for my model is</p>
<pre><code>model = Sequential()
model.add(Embedding(max_words, 30, input_length=max_len))
model.add(BatchNormalization())
model.add(Activation('tanh'))
model.add(Dropout(0.3))
model.add(Bidirectional(LSTM(32, use_bias=True, return_sequences=True)))
model.add(BatchNormalization())
model.add(Activation('tanh'))
model.add(Dropout(0.5))
model.add(Bidirectional(LSTM(32, use_bias=True, return_sequences=True), input_shape=(128, 1,64)))
model.add(BatchNormalization())
model.add(Activation('tanh'))
model.add(SeqSelfAttention(attention_activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))
model.summary()
</code></pre>
<p>The value of <strong>max_words</strong> is 2000 and <strong>max_len</strong> is 300</p>
<p>But even after this, my testing accuracy is not crossing 50%. I can't figure out the problem.
PS - I am using validation technique too. The loss function is '<strong>Binary Crossentropy</strong>' and optimizer is '<strong>Adam</strong>'.</p>
",Preprocessing of the text & Tokenization,deep lstm accuracy crossing working classification problem semeval task dataset found using deep lstm network pre processing done lower casing tokenization lemmatization removing stop word removing punctuation word embeddings used word vec model sample training set sample testing code model value max word max len even testing accuracy crossing figure problem p using validation technique loss function binary crossentropy optimizer adam
Find (space separated) compound words in a DataFrame,"<p>I have two dataframes:</p>

<p>the first is called Roster:</p>

<pre><code>| u_id | Skills                                         |
|------|------------------------------------------------|
| 1    | ai, deep learning, machine learning, nlp       |
| 2    | computer vision, statistics, python, css       |
| 3    | development, keras, numpy, supervised learning |
</code></pre>

<p>the second is called Taxonomy:</p>

<pre><code>| Skill_ID                  | Skill_Name |
|---------------------------|------------|
| AI                        | some       |
| Computer Engineering      | other      |
| Machine Learning          | data       |
| Statistics                | here       |
| Robotics                  | blah       |
| Exploratory Data Analysis | blah       |
</code></pre>

<p>I need look inside Roster[""Skills""] for any Skill_ID from Taxonomy[""Skill_ID""].  There will likely be several matches so to handle that I want to aggregate all matches in a list within an adjacent cell in the Roster dataFrame.  </p>

<p>I started by tokenizing Roster[""Skills""] but then realized I would miss all compound words such as ""computer engineering"".  I suppose I could lower case all data, then write a search without any other NLP but having trouble with the code. </p>

<p>latest attempt:</p>

<pre><code>all_skills = []

for row in roster_skills:
    for skill in taxonomy_skill_id:
        if skill in row:
            all_skills.append(skill)
            skills_list_len = range(len(all_skills))
            for n in skills_list_len:
                roster1[n] = all_skills
</code></pre>

<p>ValueError: Length of values does not match length of index</p>
",Preprocessing of the text & Tokenization,find space separated compound word dataframe two dataframes first called roster second called taxonomy need look inside roster skill skill id taxonomy skill id likely several match handle want aggregate match list within adjacent cell roster dataframe started tokenizing roster skill realized would miss compound word computer engineering suppose could lower case data write search without nlp trouble code latest attempt valueerror length value doe match length index
Effect word embeddings in CNN,"<p>When I use a CNN with word embeddings, the CNN filters the areas in n-dimensional space that are important for class classification.  Using pre-processing like stop word removal and steaming logically reduces the training time. But what about the accuracy of the model. Will, it improved? Because normally the CNN should filter the unimportant words anyway.  </p>
",Preprocessing of the text & Tokenization,effect word embeddings cnn use cnn word embeddings cnn filter area n dimensional space important class classification using pre processing like stop word removal steaming logically reduces training time accuracy model improved normally cnn filter unimportant word anyway
How to find the root of a word from its present participle or other variations in Python?,"<p>I'm working on a NLP project, and right now, I'm stuck on detecting antonyms for certain phrases that aren't in their ""standard"" forms (like verbs, adjectives, nouns) instead of present-participles, past tense, or something to that effect. For instance, if I have the phrase ""arriving"" or ""arrived"", I need to convert it to ""arrive"". Similarly, ""came"" should be ""come"". Lastly, “dissatisfied” should be “dissatisfy”. Can anyone help me out with this? I have tried several stemmers and lemmanizers in NLTK with Python, to no avail; most of them don’t produce the correct root. I’ve also thought about the ConceptNet semantic network and other dictionary APIs, but it seems far too complicated for what I need. Any advice is helpful. Thanks!</p>
",Preprocessing of the text & Tokenization,find root word present participle variation python working nlp project right stuck detecting antonym certain phrase standard form like verb adjective noun instead present participle past tense something effect instance phrase arriving arrived need convert arrive similarly came come lastly dissatisfied dissatisfy anyone help tried several stemmer lemmanizers nltk python avail produce correct root also thought conceptnet semantic network dictionary apis seems far complicated need advice helpful thanks
Stop words are not being removed using python,"<p>I am trying to remove stop words from the list of tokens I have. But, it seems like the words are not removed. What would be the problem? Thanks.</p>

<p>Tried:</p>

<pre><code>Trans = []
    with open('data.txt', 'r') as myfile:
        file = myfile.read()
            #start readin from the start of the charecter
        myfile.seek(0)
        for row in myfile:
            split = row.split()
            Trans.append(split)
        myfile.close()


    stop_words = list(get_stop_words('en'))         
    nltk_words = list(stopwords.words('english')) 
    stop_words.extend(nltk_words)

    output = [w for w in Trans if not w in stop_words]


    Input: 

    [['Apparent',
      'magnitude',
      'is',
      'a',
      'measure',
      'of',
      'the',
      'brightness',
      'of',
      'a',
      'star',
      'or',
      'other']]

    output:

    It returns the same words as input.
</code></pre>
",Preprocessing of the text & Tokenization,stop word removed using python trying remove stop word list token seems like word removed would problem thanks tried
Unexpected lemmatize result from gensim,"<p>I used following code lemmatize texts that were already excluding stop words and kept words longer than 3. However, after using following code, it split existing words such as 'wheres' to ['where', 's']; 'youre' to ['-PRON-','be']. I didn't expect 's', '-PRON-', 'be' these results in my text, what caused this behaviour and what I can do?</p>

<pre><code>def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
""""""https://spacy.io/api/annotation""""""

texts_out = []
for sent in texts:
    doc = nlp("" "".join(sent)) 
    texts_out.append([token.lemma_ for token in doc]) # though rare, if only keep the tokens with given posttags, add 'if token.pos_ in allowed_postags'
return texts_out

# Initialize spacy 'en' model, keeping only tagger component (for efficiency)
nlp = spacy.load('en', disable=['parser', 'ner'])

data_lemmatized = lemmatization(data_words_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
</code></pre>
",Preprocessing of the text & Tokenization,unexpected lemmatize result gensim used following code lemmatize text already excluding stop word kept word longer however using following code split existing word wheres youre pron expect pron result text caused behaviour
spaCy - Text Processing on custom model,"<p>I am fairly new to machine learning and NLP in general. I am trying to wrap my head around how to do proper text pre-processing (cleaning the text).</p>

<p>I have built a custom text classification model, with two labels: <code>offensive</code> and <code>clean</code>. I have the below method that I run on all input text, before serving it to my model. (both before training it, and also when using it for testing).</p>

<p>The method will remove stopwords, punctuations and lemmatize the text.</p>

<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.lang.en.stop_words import STOP_WORDS
import string

def normalize(text, lowercase, remove_stopwords, remove_punctuation):
    nlp = spacy.load(""en_core_web_sm"", disable=['parser', 'tagger', 'ner'])
    stops = spacy.lang.en.stop_words.STOP_WORDS

    if lowercase:
        text = text.lower()
    text = nlp(text)
    if remove_punctuation:
        text = [t for t in text if t.text not in string.punctuation]
    lemmatized = list()
    for word in text:
        lemma = word.lemma_.strip()
        if lemma:
            if not remove_stopwords or (remove_stopwords and lemma not in stops):
                lemmatized.append(lemma)

    return "" "".join(lemmatized)
</code></pre>

<p>Consider below input string:</p>

<p>Input: <code>You're such a sweet person. All the best!</code></p>

<p>If I clean that text, using my method:</p>

<pre class=""lang-py prettyprint-override""><code>test_text = ""You're such a sweet person. All the best!""
test_text = normalize(test_text, lowercase=True, remove_stopwords=True, remove_punctuation=True)
</code></pre>

<p>It will return: <code>-PRON- sweet person</code></p>

<p>Now, I have tested my model with both version, and this is the result:</p>

<p><code>You're such a sweet person. All the best</code>:</p>

<ul>
<li><code>{'PROFANITY': 0.07376033067703247, 'CLEAN': 0.9841629266738892}</code></li>
</ul>

<p><code>-PRON- sweet person</code></p>

<ul>
<li><code>{'PROFANITY': 0.926033616065979, 'CLEAN': 0.010466966778039932}</code></li>
</ul>

<p>As you can see, the results vary greatly. If I don't clean the text, before serving it to the model, it gets the profanity / clean score correct. The text is not profane.</p>

<p>However, if I clean the text before serving it to the model, the profanity / clean score is not correct.</p>

<p>Am I doing something wrong? I have a dataset with around 18k rows, that consists of labeled sentences. All sentences look like the below, and will be cleaned using my <code>normalize</code> method before being served to the model training:</p>

<pre><code>IS_OFFENSIVE,TEXT
--------------------
1,you are a bitch!
0,you are very sweet!
0,I love you
1,""I think that is correct, idiot!""
</code></pre>

<h1>Edit 1</h1>

<p>This is my code for training my model.</p>

<pre class=""lang-py prettyprint-override""><code>def convert():
    TRAINING_DATA = defaultdict(list)
    # Open CSV file.
    with open('train/profanity/data/profanity_cleaned_data_cleaned.csv', mode='r') as csv_file:
        csv_reader = csv.DictReader(csv_file)
        line_count = 1
        for row in csv_reader:
            if line_count &gt; 0 and line_count &lt; 500:
                if row['is_offensive'] == '0':
                    CLEAN = bool(1)
                    PROFANITY = bool(0)
                else:
                    CLEAN = bool(0)
                    PROFANITY = bool(1)

                TRAINING_DATA['csv'].append([str(row['text']), {
                    ""CLEAN"": CLEAN, ""PROFANITY"": PROFANITY}])

                line_count += 1

    return TRAINING_DATA['csv']

def train():
    output_dir = 'train/profanity/model/'
    TRAINING_DATA = convert_csv_to_dataset.convert()

    nlp = spacy.blank(""en"")
    category = nlp.create_pipe(""textcat"")
    category.add_label(""PROFANITY"")
    category.add_label(""CLEAN"")
    nlp.add_pipe(category)

    # Start the training
    nlp.begin_training()

    # Loop for 10 iterations
    for itn in range(10):
        # Shuffle the training data
        random.shuffle(TRAINING_DATA)
        losses = {}

        # Batch the examples and iterate over them
        for batch in tqdm(spacy.util.minibatch(TRAINING_DATA, size=1)):
            texts = [nlp(text) for text, entities in batch]
            annotations = [{""cats"": entities} for text, entities in batch]
            nlp.update(texts, annotations, losses=losses)
        # if itn % 20 == 0:
        #    print(losses)

    nlp.to_disk(output_dir)
    print(""Saved model to"", output_dir)
</code></pre>

<p>The file <code>profanity_cleaned_data_cleaned.csv</code> has been preprocessed using the <code>normalize</code> method.</p>
",Preprocessing of the text & Tokenization,spacy text processing custom model fairly new machine learning nlp general trying wrap head around proper text pre processing cleaning text built custom text classification model two label method run input text serving model training also using testing method remove stopwords punctuation lemmatize text consider input string input clean text using method return tested model version result see result vary greatly clean text serving model get profanity clean score correct text profane however clean text serving model profanity clean score correct something wrong dataset around k row consists labeled sentence sentence look like cleaned using method served model training edit code training model file ha preprocessed using method
Joint segmentation of the sentence using NLP tools,"<p>I am trying to look for an example in joint segmentation (I'm not sure if it's a correct term) using some advanced NLP tools.
For example, if I have a sentence like this:</p>

<pre><code>I like cats and/or dogs.
</code></pre>

<p>And I would like to have something like this:</p>

<pre><code>1. I like cats.
2. I like dogs.
</code></pre>

<p>Is there a way to do this?</p>

<p>Or maybe a more complicated example. If I have:</p>

<pre><code>The steering system shall ensure easy and safe handling of the vehicle up to its maximum design speed
or in case of a trailer up to its technically permitted maximum speed.
</code></pre>

<p>Is it possible to get something like:</p>

<pre><code>1. The steering system shall ensure easy and safe handling of the vehicle up to its maximum design speed.
2. In case of a trailer, the steering system shall ensure easy and safe handling of the vehicle up to its technically permitted maximum speed.
</code></pre>

<p>There are going to be many long sentences like that, so I guess, regular expressions is not the best solution. Or is it? I cannot find much info about this topic.</p>
",Preprocessing of the text & Tokenization,joint segmentation sentence using nlp tool trying look example joint segmentation sure correct term using advanced nlp tool example sentence like would like something like way maybe complicated example possible get something like going many long sentence like guess regular expression best solution find much info topic
How to find a capital letter words from an NLTK corpus using regex?,"<p>I'd like to make a word list with a regular expression which is consists of all capital letters. 
the data set is a bunch of biological theses text files called corpus. </p>

<p>The result for <code>len(corpus.fileids())</code> is 487 which means that there are 487 theses in the corpus.</p>

<p>The main reason for this is to collect word list to filter biological words like gene name and etc(ATP, BRCA)</p>

<p>here are some codes that I've been trying. 
(p.s. I'm using python3)</p>

<p>I'm stuck with making functions to call out all files in the corpus.
for a single file, I think this would work.</p>

<pre><code>capital = re.findall(r'[A-Z]+', GNICorpus)
</code></pre>

<p>but the thing is that I have to go through all the words in the theses txt files in the corpus and have no idea. 
1st trial</p>

<pre><code>import re
import nltk
from nltk.corpus import*
x = [
    (file)
    for file in Corpus.fileids() 
    for w in Corpus.words(file) 
    if w.upper()
]
</code></pre>

<p>2nd trial</p>

<pre><code>   capital = re.findall(r'[A-Z]+', Corpus)
   capital
</code></pre>

<p>Third trial</p>

<pre><code>for fileid in Corpus.fileids():
    words = Corpus.words(fileid)
    capital = re.findall(r'[A-Z]+', words)
</code></pre>
",Preprocessing of the text & Tokenization,find capital letter word nltk corpus using regex like make word list regular expression consists capital letter data set bunch biological thesis text file called corpus result mean thesis corpus main reason collect word list filter biological word like gene name etc atp brca code trying p using python stuck making function call file corpus single file think would work thing go word thesis txt file corpus idea st trial nd trial third trial
How to ignore punctuation in-between words using word_tokenize in NLTK?,"<p>I'm looking to ignore characters in-between words using NLTK word_tokenize.</p>

<p>If I have a a sentence:</p>

<pre><code>test = 'Should I trade on the S&amp;P? This works with a phone number 333-445-6635 and email test@testing.com'
</code></pre>

<p>The word_tokenize method is splitting the S&amp;P into </p>

<pre><code>'S','&amp;','P','?'
</code></pre>

<p>Is there a way to have this library ignore punctuation between words or letters?
Expected output: <code>'S&amp;P','?'</code></p>
",Preprocessing of the text & Tokenization,ignore punctuation word using word tokenize nltk looking ignore character word using nltk word tokenize sentence word tokenize method splitting p way library ignore punctuation word letter expected output
what is meant by empty rows as feature vectors in text analysis?,"<p>I am doing the movie review sentiment analysis using the data available for Kaggle dataset here using Python. <a href=""https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only/data"" rel=""nofollow noreferrer"">https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only/data</a></p>

<p>I do not have errors here but trying to understand why the rows collecting feature vectors are empty for some cases.</p>

<p>After preprocessing my text such as removing stop words, missing data removal, removing punctuations I tokenize my text into sequences using the following codes.</p>

<pre><code>tokenizer = Tokenizer(num_words=nwords, filters='!""#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n', lower=True,split=' ')
tokenizer.fit_on_texts(df_train[""cleansed_txt""].values)
tokenizer.fit_on_texts(df_test[""cleansed_txt""].values)
X_train = tokenizer.texts_to_sequences(df_train[""cleansed_txt""].values)
X_test = tokenizer.texts_to_sequences(df_test[""cleansed_txt""].values)
</code></pre>

<p>And when I check how does my X_train looks like I find it to be this way. My question is what should I understand from these numbers. Why are some of them empty?</p>

<p><a href=""https://i.sstatic.net/Foesy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Foesy.jpg"" alt=""enter image description here""></a></p>
",Preprocessing of the text & Tokenization,meant empty row feature vector text analysis movie review sentiment analysis using data available kaggle dataset using python error trying understand row collecting feature vector empty case preprocessing text removing stop word missing data removal removing punctuation tokenize text sequence using following code check doe x train look like find way question understand number empty
Finding the List of words in List of Sentences and return the matching sentences,"<p>From the List of Sentences and List of words, how to return the list of Sentences, only if all three words are matching from words Lists (Trigrams).</p>

<p>Please suggest. Below are example lists.</p>

<pre><code>listwords = [['people','suffering','acute'], ['Covid-19','Corona','like'], ['people','must','collectively']]

listsent = ['The number of people suffering acute hunger could almost double.',
            'Lockdowns and global economic recession have',
            'one more shock – like Covid-19 – to push them over the edge',
            'people must collectively act now to mitigate the impact']
</code></pre>

<p>Output list should be first &amp; last sentences, as they have three matching words in listwords.  </p>

<p>Expected output is:</p>

<pre><code>['The number of people suffering acute hunger could almost double.',
 'people must collectively act now to mitigate the impact']
</code></pre>
",Preprocessing of the text & Tokenization,finding list word list sentence return matching sentence list sentence list word return list sentence three word matching word list trigram please suggest example list output list first last sentence three matching word listwords expected output
UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode characters in position 62-11168: ordinal not in range(128),"<p>Help me figure out what's wrong with this. I am running Text summarization using Transformers</p>

<p>~/Bart_T5-summarization$ python app.py
No handlers could be found for logger ""transformers.data.metrics""
Traceback (most recent call last):
  File ""app.py"", line 6, in 
    from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig
  File ""/home/darshan/.local/lib/python2.7/site-packages/transformers/<strong>init</strong>.py"", line 42, in 
    from .tokenization_auto import AutoTokenizer
  File ""/home/darshan/.local/lib/python2.7/site-packages/transformers/tokenization_auto.py"", line 28, in 
    from .tokenization_xlm import XLMTokenizer
  File ""/home/darshan/.local/lib/python2.7/site-packages/transformers/tokenization_xlm.py"", line 27, in 
    import sacremoses as sm
  File ""/home/darshan/.local/lib/python2.7/site-packages/sacremoses/<strong>init</strong>.py"", line 2, in 
    from sacremoses.tokenize import *
  File ""/home/darshan/.local/lib/python2.7/site-packages/sacremoses/tokenize.py"", line 16, in 
    class MosesTokenizer(object):
  File ""/home/darshan/.local/lib/python2.7/site-packages/sacremoses/tokenize.py"", line 41, in MosesTokenizer
    PAD_NOT_ISALNUM = r""([^{}\s.'`\,-])"".format(IsAlnum), r"" \1 ""</p>

<p><a href=""https://i.sstatic.net/8wADZ.png"" rel=""nofollow noreferrer"">enter image description here</a>
<strong>UnicodeEncodeError: 'ascii' codec can't encode characters in position 62-11168: ordinal not in range(128)</strong></p>
",Preprocessing of the text & Tokenization,unicodeencodeerror ascii codec encode character position ordinal range help figure wrong running text summarization using transformer bart summarization python app py handler could found logger transformer data metric traceback recent call last file app py line transformer import barttokenizer bartforconditionalgeneration bartconfig file home darshan local lib python site package transformer init py line tokenization auto import autotokenizer file home darshan local lib python site package transformer tokenization auto py line tokenization xlm import xlmtokenizer file home darshan local lib python site package transformer tokenization xlm py line import sacremoses sm file home darshan local lib python site package sacremoses init py line sacremoses tokenize import file home darshan local lib python site package sacremoses tokenize py line class mosestokenizer object file home darshan local lib python site package sacremoses tokenize py line mosestokenizer pad isalnum r format isalnum r enter image description unicodeencodeerror ascii codec encode character position ordinal range
Why there difference in training accuracy when using sklearn pipeline and not using it?,"<p>Here I'm talking about training accuracy that we see in classification report.</p>

<p>I trained a model using sklearn's pipeline and I got acuracy  78%. After that I traind model without using pipelineI got accuray 98%. I have a doubt that why there so much difference between accuracy?</p>

<p>Training model using pipepilne:
Functions for text preprocessing:</p>

<pre><code>def text_preprocess(mess):
    #nopunc = [char for char in mess if char not in string.punctuation]
    #nopunc = ''.join(nopunc)
    stopwords = list(string.punctuation) + nltk.corpus.stopwords.words('english')
    mess = str(mess)
    nopunc = mess.lower()
    nopunc = re.sub('((www\.[^\s]+)|(https?://[^\s]+))', 'URL', nopunc) # remove URLs
    nopunc = re.sub('@[^\s]+', 'AT_USER', nopunc) # remove usernames
    nopunc = re.sub(r'#([^\s]+)', r'\1', nopunc) # remove the # in #hashtag
    nopunc = nltk.tokenize.word_tokenize(nopunc) # remove repeated characters (helloooooooo into hello)
    #return [word for word in nopunc if word not in nltk.corpus.stopwords.words('english')]
    return [word for word in nopunc if word not in stopwords]

pipeline = Pipeline([
    ('bow', CountVectorizer(analyzer=text_preprocess)),
    ('tfidf', TfidfTransformer()),
    ('classifier', RandomForestClassifier(n_estimators=100))
])
</code></pre>

<p>Training without pipeline:</p>

<pre><code>bow_transformer = CountVectorizer(analyzer=text_preprocess).fit(train['Sentimenttext'])
messages_bow = bow_transformer.transform(train['Sentimenttext'])
tfidf_transformer = TfidfTransformer().fit(messages_bow)
messages_tfidf = tfidf_transformer.transform(messages_bow)
spam_detect_model = RandomForestClassifier().fit(messages_tfidf, train['Sentiment'])
</code></pre>
",Preprocessing of the text & Tokenization,difference training accuracy using sklearn pipeline using talking training accuracy see classification report trained model using sklearn pipeline got acuracy traind model without using pipelinei got accuray doubt much difference accuracy training model using pipepilne function text preprocessing training without pipeline
What&#39;s the best way to classify text data in ML?,"<p>Let's say i have a dataset consisting of a review column with exactly 100 words for each review, then it may be easy to train my model as i can simply tokenize each of the 100 words for each reviews then convert it into a numerical array and then feed it into a Sequential model with input_shape=(1,100). But in the real world, reviews are never the same size. If I use a function such as CountVectorizer, then the structure of the sentence is not reserved, and one hot encoding may not be efficient enough.</p>

<p>So what is the proper way to preprocess this particular dataset so that i feed it into a trainable NN</p>
",Preprocessing of the text & Tokenization,best way classify text data ml let say dataset consisting review column exactly word review may easy train model simply tokenize word review convert numerical array feed sequential model input shape real world review never size use function countvectorizer structure sentence reserved one hot encoding may efficient enough proper way preprocess particular dataset feed trainable nn
How do I loop through a directory of PDF files and write the information to a Pandas Dataframe in Python?,"<p>I am very new to Python, Pandas, and NLP but have taken a few intro courses. I have a directory of 3 PDF files (will be over a hundred once I get the full data set). I want to open each file and make two columns in a Pandas dataframe that I can eventually use for some NLP work. The two columns needed are an ID column with the name of the PDF and the second column is just all of the text/information located within that PDF</p>

<p>This is the code I used to go through one file at a time:</p>

<pre><code>import PyPDF2 as pdf
i = 0
    while i &lt; pdf_reader.getNumPages():
        pageinfo = pdf_reader.getPage(i)
        print(pageinfo.extractText())
        i = i + 1
</code></pre>

<p>This is the code that I used to name my directory and print out the file names:</p>

<pre><code>import os
directory = os.listdir('test_files/')
directory = os.listdir('test_files/')
for entry in directory:
    print(entry)
</code></pre>

<p>**Update, this is what I have so far. Does it seem close? </p>

<pre><code>directory = os.listdir('test_files/')
for entry in directory:
    file = open(entry,'rb')
    pdf_reader = pdf.PdfFileReader(file)
    i = 0
    while i &lt; pdf_reader.getNumPages():
        pageinfo = pdf_reader.getPage(i)
        i = i + 1
    data = {'PDF_ID':[entry],
       'Text_Data': [pageinfo.extractText()]}
    df = pd.DataFrame(data, columns = ['PDF_ID','Text_Data'])
</code></pre>

<p>would be ideal, but I haven't found the best way combine them and create a dataframe at the same time. I already have a function created that will clean and tokenize the text, but one file at a time is not ideal. Thanks!</p>
",Preprocessing of the text & Tokenization,loop directory pdf file write information panda dataframe python new python panda nlp taken intro course directory pdf file hundred get full data set want open file make two column panda dataframe eventually use nlp work two column needed id column name pdf second column text information located within pdf code used go one file time code used name directory print file name update far doe seem close would ideal found best way combine create dataframe time already function created clean tokenize text one file time ideal thanks
NLP: Apply CountVectorizer to column containing a list of features,"<p>I want to apply <code>CountVectorizer</code> to column that contains a list of words and phrases. In other words, the corpus is not a string but a list. The problem is that <code>CountVectorizer</code> or any other related function I encountered expect a string as an input. It doesn't make sense to join the list into one string and them tokenize because some phrases contain 2 words. Any ideas?   </p>

<p>example:</p>

<pre><code>ID      corpus
1       [""Harry Potter"",""Batman""]
2       [""Batman"", ""Superman"", ""Lord of the Rings""]
</code></pre>

<p>desired result:</p>

<pre><code>ID   Harry Potter    Batman    Superman    Lord of the Rings
1    1               1         0           0
2    0               1         1           1
</code></pre>
",Preprocessing of the text & Tokenization,nlp apply countvectorizer column containing list feature want apply column contains list word phrase word corpus string list problem related function encountered expect string input make sense join list one string tokenize phrase contain word idea example desired result
AttributeError: &#39;NoneType&#39; object has no attribute &#39;lower&#39; in Python. How to preprocess before tokenizing the text content?,"<p>The data set I am using looks like this. It is a video captioning data set with captions under the column 'caption' with multiple captions for a single video clip.</p>

<pre><code>video_id       caption
mv89psg6zh4    A bird is bathing in a sink.
mv89psg6zh4    A faucet is running while a bird stands.
mv89psg6zh4    A bird gets washed.
mv89psg6zh4    A parakeet is taking a shower in a sink.
mv89psg6zh4    The bird is taking a bath under the faucet.
mv89psg6zh4    A bird is standing in a sink drinking water.
R2DvpPTfl-E    PLAYING GAME ON LAPTOP.
R2DvpPTfl-E    THE MAN IS WATCHING LAPTOP.
l7x8uIdg2XU    A woman is pouring ingredients into a bowl.
l7x8uIdg2XU    A woman is adding milk to some pasta.
l7x8uIdg2XU    A person adds ingredients to pasta. 
l7x8uIdg2XU    the girls are doing the cooking.
</code></pre>

<p>It is working on the ""CandidateA"" json File <a href=""https://drive.google.com/file/d/1pnhCcLVO7Sx5GtCIrPz7fsn_VQh_Gq9P/view?usp=sharing"" rel=""nofollow noreferrer"">here</a>
However, it is not working on the ""Referencedf"" json file which looks like this (the complete file can be found <a href=""https://drive.google.com/file/d/1FElwUxlGvrS26BQoNf0uwMbzH7dJT-On/view?usp=sharing"" rel=""nofollow noreferrer"">here</a>):</p>

<pre><code>(Excerpt only):
[{""video_id"":""mv89psg6zh4_33_46"",""caption"":""A bird in a sink keeps getting under the running water from a faucet.""},{""video_id"":""mv89psg6zh4_33_46"",""caption"":""A bird is bathing in a sink.""},{""video_id"":""mv89psg6zh4_33_46"",""caption"":""A bird is splashing around under a running faucet.""},{""video_id"":""60x_yxy7Sfw_1_7"",""caption"":""A MAN IS WATCHING A LAPTOP.""},{""video_id"":""60x_yxy7Sfw_1_7"",""caption"":""A man is sitting at his computer.""}]
</code></pre>

<p>This is the following code I am applying:</p>

<pre><code>import json
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

with open(""Referencedf.json"", 'r') as f:
    datastore = json.load(f)

captions = []
video_id = []

for item in datastore:
    captions.append(item['caption'])

tokenizer = Tokenizer(oov_token=""&lt;OOV&gt;"")
tokenizer.fit_on_texts(captions)
</code></pre>

<p>The error I am getting is this:</p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-25-63fee6e467f1&gt; in &lt;module&gt;
      1 tokenizer = Tokenizer(oov_token=""&lt;OOV&gt;"")
----&gt; 2 tokenizer.fit_on_texts(captions)
      3 word_index = tokenizer.word_index
      4 print(len(word_index))

~\anaconda3\lib\site-packages\keras_preprocessing\text.py in fit_on_texts(self, texts)
    221                                             self.filters,
    222                                             self.lower,
--&gt; 223                                             self.split)
    224             for w in seq:
    225                 if w in self.word_counts:

~\anaconda3\lib\site-packages\keras_preprocessing\text.py in text_to_word_sequence(text, filters, lower, split)
     41     """"""
     42     if lower:
---&gt; 43         text = text.lower()
     44 
     45     if sys.version_info &lt; (3,):

AttributeError: 'NoneType' object has no attribute 'lower'
</code></pre>

<p>Edit: </p>

<p>As suggested by @MahindraSinghMeena, I removed the Null rows from the dataframe beforehand only so as to avoid the error by using</p>

<pre><code>df = df.dropna()
</code></pre>
",Preprocessing of the text & Tokenization,attributeerror nonetype object ha attribute lower python preprocess tokenizing text content data set using look like video captioning data set caption column caption multiple caption single video clip working candidatea json file however working referencedf json file look like complete file found following code applying error getting edit suggested mahindrasinghmeena removed null row dataframe beforehand avoid error using
Indexing original tokens in solr,"<p>Let's say I have a field type as the following:</p>

<pre><code>&lt;fieldType name=""text_body"" class=""solr.TextField"" positionIncrementGap=""100"" multiValued=""true""&gt;
  &lt;analyzer type=""index""&gt;
    &lt;tokenizer class=""solr.WhitespaceTokenizerFactory""/&gt;
    &lt;filter class=""solr.WordDelimiterGraphFilterFactory"" generateWordParts=""1"" catenateWords=""1"" preserveOriginal=""1""/&gt;
    &lt;filter class=""solr.FlattenGraphFilterFactory""/&gt;
    &lt;filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""stopwords.txt"" /&gt;
    &lt;filter class=""solr.PorterStemFilterFactory""/&gt;
    &lt;filter class=""solr.LowerCaseFilterFactory""/&gt;
  &lt;/analyzer&gt;
&lt;/fieldType&gt;
</code></pre>

<p>My goal is to index, for each token, the original token as well as the token after passing all the token filters.
For example, for the text: </p>

<pre><code>""My dog is barking #DOGS""
</code></pre>

<p>The current field type (as mentioned above) will index the following tokens:</p>

<pre><code>""my"", ""dog"", ""bark"", ""dogs"", ""#dogs"" 
</code></pre>

<p>""is"" will be dropped because of the stopWords filter, and ""barking"" will become ""bark"" because of the stemming filter.</p>

<p>I would like that the following tokens will be indexed: </p>

<pre><code>""My"", ""my"", ""dog"", ""barking"", ""bark"", ""dogs"", ""#DOGS"".
</code></pre>

<p>I considered the ""perserveOriginal"" parameter in the WordDelimiterGraphFilterFactory but it's only works for tokens with special characters, and also the ""original token"" passes the other filters after that.</p>

<p>I know that the obvious way is to write a custom TokenFilter that indexes the tokens at their original form right after the tokenizer, but my question is if there is something built in Solr that allows it.</p>

<p>I'm using Solr 6.5.1</p>

<p>Thanks :)</p>
",Preprocessing of the text & Tokenization,indexing original token solr let say field type following goal index token original token well token passing token filter example text current field type mentioned index following token dropped stopwords filter barking become bark stemming filter would like following token indexed considered perserveoriginal parameter worddelimitergraphfilterfactory work token special character also original token pass filter know obvious way write custom tokenfilter index token original form right tokenizer question something built solr allows using solr thanks
How does the num_words parameter of keras Tokenizer work?,"<p>When tokenizing a text sequence in keras using Tokenizer class, we can specify a param 'num_words' to consider only the [top] n words in the dataset. My doubt here is</p>

<ol>
<li>What does the [top] values mean? Does it mean the frequency of the word or any other value like tf-idf?</li>
<li>Is the [top] value computed on each document level or by considering the entire dataset?</li>
</ol>

<p>Directing to any good resources or explanation with example will be very useful.</p>
",Preprocessing of the text & Tokenization,doe num word parameter kera tokenizer work tokenizing text sequence kera using tokenizer class specify param num word consider top n word dataset doubt doe top value mean doe mean frequency word value like tf idf top value computed document level considering entire dataset directing good resource explanation example useful
How to find matching words using regex?,"<p>I have strings in a text file with more than 2000 lines, like:</p>

<pre><code>cool.add.come.ADD_COPY
add.cool.warm.ADD_IN
warm.cool.warm.MINUS
cool.add.go.MINUS_COPY
</code></pre>

<p>I have a list of more than 200 matching words, like:</p>

<pre><code>store=['ADD_COPY','add.cool.warm.ADD_IN', 'warm.cool.warm.MINUS', 'MINUS_COPY']
</code></pre>

<p>I'm using regular expression in the code</p>

<pre><code>def all(store, file):
    lst=[]
    for match in re.finditer(r'[\w.]+', file):
        words = match.group()
            if words in store:
                lst.append(words) 
    return lst 
</code></pre>

<p>Then I check in a loop for requirement.</p>

<p>Output I'm getting:</p>

<pre><code>add.cool.warm.ADD_IN
warm.cool.warm.MINUS
</code></pre>

<p>If I change the identifiers to <code>\w+</code> then I get only:</p>

<pre><code>ADD_COPY
MINUS_COPY
</code></pre>

<p>Required output:</p>

<pre><code>add.cool.warm.ADD_IN
warm.cool.warm.MINUS   
ADD_COPY
MINUS_COPY
</code></pre>
",Preprocessing of the text & Tokenization,find matching word using regex string text file line like list matching word like using regular expression code check loop requirement output getting change identifier get required output
Wordpiece Tokenization Model,"<p>Can somebody tell me how exactly the wordpiece model work ? I am having some hard time trying to understand how exactly the wordpiece model is working. I understand the BPE that it is based on merging according the highest frequency pairs. After digging for hours on the internet and reading the paper. It is mentioned that in wordpiece we make the final merge according to what maximize the likelihood of the Language model we created.
How is this language model is created ? Is it created by Probability of Pair equal to Count of Pair / Total Count of Pairs or what ?
What i understand is that we want to measure which token pair minus separate tokens is the largest , like if we have ""de"" = 9 , ""d"" = 15 ""e"" = 12 and ""th"" = 10 , ""t"" = 12 ""h""= 12 , then we choose to merge token ""t"" and ""h"" as its 10-24 > 9-27. Am i right ? Please somebody correct me</p>
",Preprocessing of the text & Tokenization,wordpiece tokenization model somebody tell exactly wordpiece model work hard time trying understand exactly wordpiece model working understand bpe based merging according highest frequency pair digging hour internet reading paper mentioned wordpiece make final merge according maximize likelihood language model created language model created created probability pair equal count pair total count pair understand want measure token pair minus separate token largest like de e th h choose merge token h right please somebody correct
python NLP and regex missing white space between words,"<p>I'm trying to process a large text file using NLTK and regular expression. After a few codes, I found that I have missing space between some words. Below are my codes and examples of those words clustered together because of the missing white space. I'd really appreciate it if you could help me see which part of the re code is causing the problem.</p>

<p>The data I started with is:</p>

<p>news
99¢ Only Stores_6-9-2008 99¢ Only Stores(r) Issues Precautionary Recall...
APP Pharmaceuticals_3-26-2011 APP Pharmaceuticals Issues a Nationwide Volunt...
Abbott Diabetes Care_12-22-2010 Abbott Diabetes Care Announces Recall of Certa...
Abbott Nutrition_1-19-2009 Abbott Nutrition Announces Voluntary Recall of...
Abbott_5-25-2007 Abbott Announces Voluntary Nationwide Recall o...</p>

<pre class=""lang-py prettyprint-override""><code>
import pandas as pd
pd.set_option('max_colwidth',50)

data_df = pd.DataFrame.from_dict(data_combined).transpose()
data_df.columns = ['news']
data_df = data_df.sort_index()

def clean_text_round1 (text):

    text=text.split()
    re_punc = re.compile('[%s]' % re.escape(string.punctuation))
    text = [re_punc.sub('', w) for w in text]
    text = [word for word in text if word.isalpha()]  
    #lem = WordNetLemmatizer()
    #text = [lem.lemmatize(word) for word in text]
    porter = PorterStemmer()
    text = [porter.stem(word) for word in text]
    text = [word for word in text if len(word) &gt; 3]

    return text

round1 = lambda x: clean_text_round1(x)

data_clean = pd.DataFrame(data_df.news.apply(round1))


def clean_text_round2(data_clean):

    data_clean = re.sub('^[A-Z]\w*', '', data_clean)
    data_clean = re.sub('[%s]' % re.escape(string.punctuation), '', data_clean)
    data_clean = re.sub('[‘’“”…]', '', data_clean)
    data_clean = re.sub('(\n|¢)', '', data_clean)
    data_clean = re.sub('\[.*?\]', '', data_clean)    
    data_clean = re.sub('\w*\d\w*', '', data_clean)
    data_clean = data_clean.lower().strip()    
    return data_clean

round2 = lambda x: clean_text_round2(x)

data_clean = pd.DataFrame(data_df.news.apply(round2))
</code></pre>

<p>Some examples of the wrong output:</p>

<p>abbottabbott, abbottdocument, alexandraspmcommunicationscom, americacustomers</p>
",Preprocessing of the text & Tokenization,python nlp regex missing white space word trying process large text file using nltk regular expression code found missing space word code example word clustered together missing white space really appreciate could help see part code causing problem data started news store store r issue precautionary recall app pharmaceutical app pharmaceutical issue nationwide volunt abbott diabetes care abbott diabetes care announces recall certa abbott nutrition abbott nutrition announces voluntary recall abbott abbott announces voluntary nationwide recall example wrong output abbottabbott abbottdocument alexandraspmcommunicationscom americacustomers
NLP: How do I combine stemming and tagging?,"<p>I'm trying to write code which passes in text that has been tokenized and had the stop words filtered out, and then stems and tags it. However, I'm not sure in what order I should stem and tag. This is what I have at the moment:</p>

<pre><code>#### Stemming
ps = PorterStemmer()    # PorterStemmer imported from nltk.stem

stemText = []

for word in swFiltText:    # Tagged text w/o stop words
    stemText.append(ps.stem(word))


#### POS Tagging
def tagging():
    tagTot = []
    try:
        for i in stemText:
            words = nltk.word_tokenize(i)    # I need to tokenize again (idk why?)
            tagged = nltk.pos_tag(words)
            tagTot = tagTot + tagged    # Combine tagged words into list

    except Exception as e:
        print(str(e))
    return tagTot

tagText = tagging()
</code></pre>

<p>At first glance, this works just fine. However, because I stemmed first, <code>pos_tag</code> often mislabels words. For example, it marked ""hous"" as an adjective, when the original word was really the noun ""house"". But when I try to stem after tagging, it gives me an error about how <code>pos_tag</code> can't deal with 'tuples' - I'm guessing this has something to do with the way that the stemmer formats the word list as <code>[('come', 'VB'), ('hous', 'JJ')</code>, etc.</p>

<p>Should I be using a different stemmer/tagger? Or is the error in my code?</p>

<p>Thanks in advance!</p>
",Preprocessing of the text & Tokenization,nlp combine stemming tagging trying write code pass text ha tokenized stop word filtered stem tag however sure order stem tag moment first glance work fine however stemmed first often mislabels word example marked hous adjective original word wa really noun house try stem tagging give error deal tuples guessing ha something way stemmer format word list etc using different stemmer tagger error code thanks advance
How to create custom one hot encoding by keywords on text sequences,"<p>I have a list of text sequences that look like this:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>    sequences = [
    ['okay', ''],
    ['ahead', 'fred', ''],
    ['i', 'dont', 'remember', 'you', 'want', 'to', 'go', ''],
    ['um', ''],
    ['let', 'me', 'think', '']
]</code></pre>
</div>
</div>
</p>

<p>I want to create a one hot vector for each sequence that counts the occurrence of certain words from a list. The list of words to look for is here:</p>

<pre><code>
    keywords = ['i', 'you', 'we']

</code></pre>

<p>Ultimately, I want to loop through each text sequence and return the following (where 0 means the keyword was not present and 1 means it was):</p>

<pre><code>
    seq_to_vec = [
        [0,0,0],
        [0,0,0],
        [1,1,0],
        [0,0,0],
        [0,0,0]
    ]

</code></pre>

<p>How do I do this?</p>
",Preprocessing of the text & Tokenization,create custom one hot encoding keywords text sequence list text sequence look like want create one hot vector sequence count occurrence certain word list list word look ultimately want loop text sequence return following mean keyword wa present mean wa
Regex for a consecutive character occurring at least three times in a string in Python,"<p>I have a problem with text preprocessing for tweets.</p>

<p>I would like to replace a character repeated at least three times in a string with the following manner:</p>

<pre><code>so        -----&gt; so
loooooove -----&gt; love
sweeeeeet -----&gt; sweet
</code></pre>

<p>What I did is as follows:</p>

<pre><code>text = ""this is so soooo sweeeeeeet. I loooove it!""
re.sub(r'([a-z])\1+', r'\1',text)
</code></pre>

<p>and </p>

<pre><code>re.sub(r'(\w)(\1{3,})',r'\1',text)
</code></pre>

<p>which is referred from <a href=""https://stackoverflow.com/questions/15544480/regex-for-repeating-characters-in-a-string-in-python"">regex for repeating characters in a string in Python</a></p>

<p>Both returned the same result as follows:</p>

<pre><code>'this is so so swet. I love it!'
</code></pre>

<p>Any advice is appreciated.</p>
",Preprocessing of the text & Tokenization,regex consecutive character occurring least three time string python problem text preprocessing tweet would like replace character repeated least three time string following manner follows referred href repeating character string python returned result follows advice appreciated
How to extract one topic for one document using LDA,"<p>We are aware of that LDA is designed to work with a number of documents and extract k topics from them. However our goal is to extract one single topic for one single document using LDA. Our first approach is:</p>

<ol>
<li>Clean data (lowercase, lemmatize, remove stopwords, punctuations, number, etc).</li>
<li>Vectorize using Countvectorize.</li>
<li>LDA (1 topic)</li>
</ol>

<p>One interesting observation is that the top 10 keywords of the output of LDA are exactly those 10 words that are most frequently appeared in the document(totally count based). We would like to ask two questions:</p>

<ol>
<li>Does this observation make any sense? </li>
<li>Any other ways to achieve our goal?</li>
</ol>
",Preprocessing of the text & Tokenization,extract one topic one document using lda aware lda designed work number document extract k topic however goal extract one single topic one single document using lda first approach clean data lowercase lemmatize remove stopwords punctuation number etc vectorize using countvectorize lda topic one interesting observation top keywords output lda exactly word frequently appeared document totally count based would like ask two question doe observation make sense way achieve goal
Compare two dataframe columns of sentence strings and create new values for a third frame,"<p>Here, I have two dataframe columns. A and B. For each row[i], All of B is contained in A, Now i am trying to test for B in A, and return 1 for all the words in the matching phrase, and 0 for all other words in A outside phrase B, thus creating a new dataframe of 0's and 1's.</p>

<pre><code>    Why would it be competitive, so it's wond...        if the teabaggers hadn't ousted Sen
    Had he refused to attempt something so partisa...   Had he refused to attempt something so partisa...
    ""This study would then have to be conducted an...   This study would then have to be conducted and 

</code></pre>

<p>Expected dataframe.</p>

<pre><code>['0', '0', '0', '0' , '0', '1', '1', '1', '1', '1', '1'........]

</code></pre>

<p>I have primarily tried two methods, but in the first method which i got here on stackoverflow, it was testing for individual words in B and not the entire phrase of column B,  so that i would have results like this</p>

<pre><code>['0', '1', '0', '0' , '0', '1', '1', '1', '1', '1', '1', ........]

</code></pre>

<p>where a value in B such as ""is"" or ""and"" was always liable to occur outside the phrase and return a bad result.</p>

<p>I also tried regular expressions, which worked perfectly for a single instance, but i was not able to apply it over the dataframes with good results. It was sort of a ratchet job and it would return endless rows of 1's or run out of memory.   </p>

<pre><code>rx = '({})'.format('|'.join(re.escape(el)for el in B))
     # Generator to yield replaced sentences, rep_lace is a column of 1's for each word in B
it = (re.sub(rx, rep_lace, sentence)for sentence in A)
     # Build list of paired new sentences and old to filter out where not the same
results.append([new_sentence for old_sentence, new_sentence in zip(A, it) if old_sentence != new_sentence])
nw_results = ' '.join([str(elem) for elem in results])
ew_results= nw_results.split("" "")
new_results = ['0' if i is not '1' else i for i in ew_results]
labels =([int(e) for e in new_results]) 

</code></pre>

<p>I hope that i gave a clear enough explanation.</p>
",Preprocessing of the text & Tokenization,compare two dataframe column sentence string create new value third frame two dataframe column b row b contained trying test b return word matching phrase word outside phrase b thus creating new dataframe expected dataframe primarily tried two method first method got stackoverflow wa testing individual word b entire phrase column b would result like value b wa always liable occur outside phrase return bad result also tried regular expression worked perfectly single instance wa able apply dataframes good result wa sort ratchet job would return endless row run memory hope gave clear enough explanation
how to fix stopwords preprocessing inconsistency?,"<p>I get a UserWarning thrown every time I execute this function. Here user_input is a list of words, and article_sentences a list of lists of words.</p>

<p>I've tried to remove all stop words out of the list beforehand but this didn't change anything.</p>

<pre class=""lang-py prettyprint-override""><code>def generate_response(user_input):
    sidekick_response = ''
    article_sentences.append(user_input)

    word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english')
    all_word_vectors = word_vectorizer.fit_transform(article_sentences) # this is the problematic line
    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)
    similar_sentence_number = similar_vector_values.argsort()[0][-2]
</code></pre>

<p>this is a part of a function for a simple chatbot I found here: <a href=""https://stackabuse.com/python-for-nlp-creating-a-rule-based-chatbot/"" rel=""nofollow noreferrer"">https://stackabuse.com/python-for-nlp-creating-a-rule-based-chatbot/</a>
it should return a sorted list of sentences sorted by how much they match the user_input, which it does but it also throws this <code>UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words</code>.</p>
",Preprocessing of the text & Tokenization,fix stopwords preprocessing inconsistency get userwarning thrown every time execute function user input list word article sentence list list word tried remove stop word list beforehand change anything part function simple chatbot found return sorted list sentence sorted much match user input doe also throw
Wordpiece tokenization versus conventional lemmatization?,"<p>I'm looking at NLP preprocessing. At some point I want to implement a context-sensitive word embedding, as a way of discerning word sense, and I was thinking about using the output from BERT to do so. I noticed BERT uses WordPiece tokenization (for example, ""playing"" -> ""play"" + ""##ing"").</p>

<p>Right now, I have my text preprocessed using a standard tokenizer that splits on spaces / some punctuation, and then I have a lemmatizer (""playing"" ->""play""). I'm wondering what the benefit of WordPiece tokenization is over a standard tokenization + lemmatization. I know WordPiece helps with out of vocabulary words, but is there anything else? That is, even if I don't end up using BERT, should I consider replacing my tokenizer + lemmatizer with wordpiece tokenization? In what situations would that be useful?</p>
",Preprocessing of the text & Tokenization,wordpiece tokenization versus conventional lemmatization looking nlp preprocessing point want implement context sensitive word embedding way discerning word sense wa thinking using output bert noticed bert us wordpiece tokenization example playing play ing right text preprocessed using standard tokenizer split space punctuation lemmatizer playing play wondering benefit wordpiece tokenization standard tokenization lemmatization know wordpiece help vocabulary word anything else even end using bert consider replacing tokenizer lemmatizer wordpiece tokenization situation would useful
Count frequency of multi-word terms in large texts with Python,"<p>I have a dictionary with close to a million multi-word terms (terms containing spaces). This looks something like</p>

<pre class=""lang-py prettyprint-override""><code>[..., 
'multilayer ceramic', 
'multilayer ceramic capacitor', 
'multilayer optical disk', 
'multilayer perceptron', 
...]
</code></pre>

<p>I would like to count their frequency in many gigabytes of texts. </p>

<p>As a small example consider counting these four multi-word expressions in a Wikipedia page:</p>

<pre class=""lang-py prettyprint-override""><code>payload = {'action': 'query', 'titles': 'Ceramic_capacitor', 'explaintext':1, 'prop':'extracts', 'format': 'json'}
r = requests.get('https://en.wikipedia.org/w/api.php', params=payload)
sampletext = r.json()['query']['pages']['9221221']['extract'].lower()
sampledict = ['multilayer ceramic', 'multilayer ceramic capacitor', 'multilayer optical disk', 'multilayer perceptron']

termfreqdic = {}
for term in sampledict:
    termfreqdic[term] = sampletext.count(term)
print(termfreqdic)
</code></pre>

<p>This gives something like <code>{'multilayer ceramic': 7, 'multilayer ceramic capacitor': 2, 'multilayer optical disk': 0, 'multilayer perceptron': 0}</code> but it seems sub-optimal if the dictionary contains a million entries.</p>

<p>I've tried with very large regular expressions:</p>

<pre class=""lang-py prettyprint-override""><code>termlist = [re.escape(w) for w in open('termlistfile.txt').read().strip().split('\n')]
termregex = re.compile(r'\b'+r'\b|\b'.join(termlist), re.I)
termfreqdic = {}
for i,li in enumerate(open(f)):
    for m in termregex.finditer(li):
        termfreqdic[m.group(0)]=termfreqdic.get(m.group(0),0)+1
open('counted.tsv','w').write('\n'.join([a+'\t'+v for a,v in termfreqdic.items()]))
</code></pre>

<p>This is dead slow (6 minutes for 1000 lines of text on a recent i7).
But if I use <code>regex</code> instead of <code>re</code> by replacing the first two lines, it goes down to around 12s per 1000 lines of text, which is still very slow for my needs:</p>

<pre class=""lang-py prettyprint-override""><code>termlist = open(termlistfile).read().strip().split('\n')
termregex = regex.compile(r""\L&lt;options&gt;"", options=termlist)
...
</code></pre>

<p>Note that this does not do exactly what I want as one term may be a subterm of another as in the example 'multilayer ceramic' and 'multilayer ceramic capacitor' (which also excludes approaches of first tokenizing as in <a href=""https://stackoverflow.com/questions/47663870/find-multi-word-terms-in-a-tokenized-text-in-python"">Find multi-word terms in a tokenized text in Python</a>).</p>

<p>This looks like a common problem of sequence matching, in text corpora or also in genetic strings, that must have well-known solutions. Maybe it can be solved with some <a href=""https://en.wikipedia.org/wiki/Trie"" rel=""nofollow noreferrer"">trie</a> of words (I don't mind the initial compilation of the term list to be slow)? Alas, I don't seem to be looking for the right terms. Maybe someone can point me in the right direction?</p>
",Preprocessing of the text & Tokenization,count frequency multi word term large text python dictionary close million multi word term term containing space look something like would like count frequency many gigabyte text small example consider counting four multi word expression wikipedia page give something like seems sub optimal dictionary contains million entry tried large regular expression dead slow minute line text recent use instead replacing first two line go around per line text still slow need note doe exactly want one term may subterm another example multilayer ceramic multilayer ceramic capacitor also excludes approach first tokenizing trie word mind initial compilation term list slow ala seem looking right term maybe someone point right direction
"Expected str instance, spacy.tokens.token.Token found","<p>I am executing a data extraction use-case. To preprocess and tokenize my data, I am using both spacy English and German tokenizers, because the sentences are in both the languages. Here's my code:</p>

<pre><code>import spacy
from spacy.lang.de import German
from spacy.lang.en import English
from spacy.lang.de import STOP_WORDS as stp_wrds_de
from spacy.lang.en.stop_words import STOP_WORDS as stp_wrds_en
import string

punctuations = string.punctuation

# German Parser
parser_de = German()
# English Parser
parser_en = English()

def spacy_tokenizer_de(document):
    # Token object for splitting text into 'units'
    tokens = parser_de(document)
    # Lemmatization: Grammatical conversion of words
    tokens = [word.lemma_.strip() if word.lemma_ != '-PRON-' else word for word in tokens]
    # Remove punctuations
    tokens = [word for word in tokens if word not in punctuations]

    tokens_de_str = converttostr(tokens,' ')
    tokens_en = spacy_tokenizer_en(tokens_de_str)
    print(""Tokens EN: {}"".format(tokens_en))
    tokens_en = converttostr(tokens_en,' ')
    return tokens_en

def converttostr(input_seq, separator):
   # Join all the strings in list
   final_str = separator.join(input_seq)
   return final_str

def spacy_tokenizer_en(document):
    tokens = parser_en(document)
    tokens = [word.lemma_.strip() if word.lemma_ != '-PRON-' else word for word in tokens]
    return tokens
</code></pre>

<p>Here's a further elucidation of the above code:
<br>
<strong>1. spacy_tokenizer_de():</strong> Method to parse and tokenize document in German
<br>
<strong>2. spacy_tokenizer_en():</strong> Method to parse and tokenize document in English
<br>
<strong>3. converttostr():</strong> Converts list of tokens to a string, so that the English spacy tokenizer can read the input (only accepts document/string format) and tokenize the data.
<br> </p>

<p>However, some sentences when parsed, lead to the following error:
<a href=""https://i.sstatic.net/1W1jx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1W1jx.png"" alt=""enter image description here""></a>   </p>

<p>Why is a spacy token object coming up in such scenarios, whereas, some of the sentences are being processed successfully? Can anyone please help here?</p>
",Preprocessing of the text & Tokenization,expected str instance spacy token token token found executing data extraction use case preprocess tokenize data using spacy english german tokenizers sentence language code elucidation code spacy tokenizer de method parse tokenize document german spacy tokenizer en method parse tokenize document english converttostr convert list token string english spacy tokenizer read input accepts document string format tokenize data however sentence parsed lead following error spacy token object coming scenario whereas sentence processed successfully anyone please help
How to remove last alphabet in a word using regular expression python,"<p>'''
string = (&quot; qtoday X fridayq qblue aqb&quot;)
string = re.sub('^ q+', '', string)
string</p>
<h1>I want to remove only aplhabet q which is in the starting of the word and ending of a word...</h1>
",Preprocessing of the text & Tokenization,remove last alphabet word using regular expression python string qtoday x fridayq qblue aqb string sub q string string want remove aplhabet q starting word ending word
Meaningful word detection,"<p>I'm trying to filter only meaningful words from a list of words. Some of the words will be gibberish and i want to filter them out. I'm curious if there is a library for this in a common language like python or nodejs. It would be great if the library supports different languages (turkish in this case). </p>
",Preprocessing of the text & Tokenization,meaningful word detection trying filter meaningful word list word word gibberish want filter curious library common language like python nodejs would great library support different language turkish case
Check if strings are composed by list of substrings,"<p>Using python I am looking to print all words from a list that are entirely composed of smaller words in a seperated list. For example;</p>

<pre><code>list1 = ('ABCDEFGHI', 'DEFABCGHI', 'ABCABCGHIABC', 'AACFFFGHI') 

list2 = ('ABC', 'DEF', 'GHI')
</code></pre>

<p>From these two lists I am trying to get the final output to print; 
('ABCDEFGHI', 'DEFABCGHI', 'ABCABCGHIABC',)
As these strings from list1 are made up entirely of shorter strings in list 2. But the string 'AACFFFGHI' should not be printed as it is not made up of a combination of these shorter strings. </p>

<p>So to try and clarify, the strings I am looking for from list1;</p>

<ul>
<li>Should be fully composed of strings from list2</li>
<li>Can include multiple occurrences of strings from list 2</li>
<li>Does not need to use all the strings listed in list2</li>
</ul>

<p>I've been struggling with this for a few days now and can search for strings made up of individual characters in a list, but I'm struggling to find the strings that are made up of sequences of characters. Any help would be much appreciated. Marcus.</p>
",Preprocessing of the text & Tokenization,check string composed list substring using python looking print word list entirely composed smaller word seperated list example two list trying get final output print abcdefghi defabcghi abcabcghiabc string list made entirely shorter string list string aacfffghi printed made combination shorter string try clarify string looking list fully composed string list include multiple occurrence string list doe need use string listed list struggling day search string made individual character list struggling find string made sequence character help would much appreciated marcus
Deleting a word in column based on frequencies,"<p>I have a NLP project where I would like to remove the words that appear only once in the keywords. That is to say, for each row I have a list of keywords and their frequencies. </p>

<p>I would like something like </p>

<pre><code>if the frequency for the word in the whole column ['keywords'] ==1 then replace by """". 
</code></pre>

<p>I cannot test word by word. So my idea was creating a list with all the words and remove the duplicates, then for each word in this list count.sum and then delete. But I have no idea how to do that. 
Any ideas? Thanks!</p>

<p>Here's how my data looks like: </p>

<p>sample.head(4)</p>

<pre><code>    ID  keywords                                            age sex
0   1   fibre:16;quoi:1;dangers:1;combien:1;hightech:1...   62  F
1   2   restaurant:1;marrakech.shtml:1  35  M
2   3   payer:1;faq:1;taxe:1;habitation:1;macron:1;qui...   45  F
3   4   rigaud:3;laurent:3;photo:11;profile:8;photopro...   46  F
</code></pre>
",Preprocessing of the text & Tokenization,deleting word column based frequency nlp project would like remove word appear keywords say row list keywords frequency would like something like test word word idea wa creating list word remove duplicate word list count sum delete idea idea thanks data look like sample head
Spacy - lemmatization on pronouns gives some erronous output,"<p>lemmatization on pronouns via <code>[token.lemma_ for token in doc]</code> gives lemmatized word for pronouns as <code>-PRON-</code> , is this a bug?</p>
",Preprocessing of the text & Tokenization,spacy lemmatization pronoun give erronous output lemmatization pronoun via give lemmatized word pronoun bug
TF IDF Vectorizer not functioning properly,"<p>I am working on a text classification problem and using TFIDF vectorizer to generate text features. </p>

<p>Here is the code</p>

<pre><code>tfidf_vectorizer = TfidfVectorizer(use_idf=True,
                                                      # stop_words=English_Stopwords,
                                  ngram_range=(1,3),
                                   min_df=0.10, #  ignore terms that have a document frequency strictly lower than the given threshold
                                   max_df=0.80, 
                                  smooth_idf=True)        
fitted_vect = tfidf_vectorizer.fit(df_sample[TEXT_FEAT])
transformed_X_train = tfidf_vectorizer.transform(X_train)
transformed_X_val = tfidf_vectorizer.transform(X_val)
</code></pre>

<p>I looked up the vocabulary and it contains only 162 words while the stop words list is pretty huge. What is the problem here. </p>

<pre><code>print(len(fitted_vect.vocabulary_))
# 162
print(len(fitted_vect.stop_words_))
# 16969712
</code></pre>
",Preprocessing of the text & Tokenization,tf idf vectorizer functioning properly working text classification problem using tfidf vectorizer generate text feature code looked vocabulary contains word stop word list pretty huge problem
bag of words model not making sense,"<p>I made a bag of words model and when I printed it out, the output is not quite making sense. 
This is my code I used to initalise the bag of words: </p>

<pre><code>#creating the bag of words model

headline_bow = CountVectorizer()
headline_bow.fit(x)
a = headline_bow.transform(x)
b = headline_bow.get_feature_names()
print(a)
</code></pre>

<p>This is a sample of the output that comes out from the bag of words model: </p>

<pre><code>  (0, 837)  1
  (0, 1496) 1
  (0, 1952) 1
  (0, 2610) 1 
</code></pre>

<p>From my understanding, for ""(0, 837) 1"" this means that in the first list passed through the model, the 837th word in that list appears once. This makes no sense because when I print 
x[0] I get this: </p>

<pre><code>Four ways Bob Corker skewered Donald Trump
</code></pre>

<p>There is clearly not 837 words here so im confused as to whats going on. </p>

<p>Here is a sample of what x is: (a bunch of headlines) </p>

<pre><code>['Four ways Bob Corker skewered Donald Trump'
 ""Linklater's war veteran comedy speaks to modern America, says star""
 'Trump’s Fight With Corker Jeopardizes His Legislative Agenda' ...
 'Ron Paul on Trump, Anarchism &amp; the AltRight'
 'China to accept overseas trial data in bid to speed up drug approvals'
 'Vice President Mike Pence Leaves NFL Game Because of Anti-American Protests']
</code></pre>

<p>Here is the rest of my code: </p>

<pre><code>data = pd.read_csv(""/Users/amanpuranik/Desktop/fake-news-detection/data.csv"")
data = data[['Headline', ""Label""]]

x = np.array(data['Headline'])
print(x[0])
y = np.array(data[""Label""])

# tokenization of the data here'
headline_vector = []

for  headline in x:
    headline_vector.append(word_tokenize(headline))

print(headline_vector)

stopwords = set(stopwords.words('english'))

#removing stopwords at this part
filtered = [[word for word in sentence if word not in stopwords]
            for sentence in headline_vector]
#print(filtered)


stemmed2 = [[stem(word) for word in headline] for headline in filtered]
#print(stemmed2)

#lowercase
lower = [[word.lower() for word in headline] for headline in stemmed2] #start here

#conver lower into a list of strings
lower_sentences = ["" "".join(x) for x in lower]

#organising
articles = []


for headline in lower:
    articles.append(headline)

#creating the bag of words model

    headline_bow = CountVectorizer()
headline_bow.fit(lower_sentences)
a = headline_bow.transform(lower_sentences)
print(a)
</code></pre>
",Preprocessing of the text & Tokenization,bag word model making sense made bag word model printed output quite making sense code used initalise bag word sample output come bag word model understanding mean first list passed model th word list appears make sense print x get clearly word im confused whats going sample x bunch headline rest code
Find number of bigrams after filtered from stop words,"<p>Case study
<strong>Task 1</strong></p>

<ul>
<li><p>Import text corpus brown</p></li>
<li><p>Extract the list of words associated with text collections belonging to the news genre. Store the result in the variable news_words.</p></li>
<li><p>Convert each word of the list news_words into lower case, and store the result in lc_news_words.</p></li>
<li><p>Compute the length of each word present in the list lc_news_words, and store the result in the list len_news_words.</p></li>
<li><p>Compute bigrams of the list len_news_words. Store the result in the variable news_len_bigrams.</p></li>
<li><p>Compute the conditional frequency of news_len_bigrams, where condition and event refers to the length of the words. Store the result in cfd_news.</p></li>
<li><p>Determine the frequency of 6-letter words appearing next to a 4-letter word.</p></li>
</ul>

<p><strong>Task 2</strong></p>

<ul>
<li><p>Compute bigrams of the list lc_news_words, and store it in the variable lc_news_bigrams.</p></li>
<li><p>From lc_news_bigrams, filter bigrams where both words contain only alphabet characters. Store the result in lc_news_alpha_bigrams.</p></li>
<li><p>Extract the list of words associated with the corpus stopwords. Store the result in stop_words.</p></li>
<li><p>Convert each word of the list stop_words into lower case, and store the result in lc_stop_words.</p></li>
<li><p>Filter only the bigrams from lc_news_alpha_bigrams where the words are not part of lc_stop_words. Store the result in lc_news_alpha_nonstop_bigrams.</p></li>
<li><p>Print the total number of filtered bigrams.</p></li>
</ul>

<p>Task 1 passed,  but task 2 is getting failed please help me out where I am wrong!!!!</p>

<pre class=""lang-py prettyprint-override""><code>import nltk
from nltk.corpus import brown
from nltk.corpus import stopwords

news_words = brown.words(categories = 'news')
lc_news_words = [word.lower() for word in news_words]
len_news_words = [len(word) for word in lc_news_words]
news_len_bigrams = nltk.bigrams(len_news_words)
cfd_news = nltk.ConditionalFreqDist(news_len_bigrams )
print(cfd_news[4][6])

lc_news_bigrams = nltk.bigrams(lc_news_words)
lc_news_alpha_bigrams = [ (w1, w2) for w1, w2 in lc_news_bigrams if w1.isalpha() and w2.isalpha()]

stop_words = stopwords.words('english')
lc_stop_words = [word.lower() for word in stop_words]
lc_news_alpha_nonstop_bigrams = [(n1, n2) for n1, n2 in lc_news_alpha_bigrams if n1 not in lc_stop_words and n2 not in lc_stop_words]
print(len(lc_news_alpha_nonstop_bigrams))
</code></pre>

<p><strong>Results</strong></p>

<p>with english in code <code>stop_words = stopwords.words('english')</code></p>

<p>1084</p>

<p>17704</p>

<p>with out english in code <code>stop_words = stopwords.words()</code></p>

<p>1084</p>

<p>16876</p>
",Preprocessing of the text & Tokenization,find number bigram filtered stop word case study task import text corpus brown extract list word associated text collection belonging news genre store result variable news word convert word list news word lower case store result lc news word compute length word present list lc news word store result list len news word compute bigram list len news word store result variable news len bigram compute conditional frequency news len bigram condition event refers length word store result cfd news determine frequency letter word appearing next letter word task compute bigram list lc news word store variable lc news bigram lc news bigram filter bigram word contain alphabet character store result lc news alpha bigram extract list word associated corpus stopwords store result stop word convert word list stop word lower case store result lc stop word filter bigram lc news alpha bigram word part lc stop word store result lc news alpha nonstop bigram print total number filtered bigram task passed task getting failed please help wrong result english code english code
Identifying different tenses of a word in Amazon Comprehend Medical,"<p>I am using Amazon Comprehend Medical for entity detection of injuries. </p>

<p>Lets say I have a piece of text as follows:</p>

<p>John had surgery to repair a dislocated left knee and a full ACL tear.""</p>

<p>Amazon comprehend medical (ACM) is able to recognize dislocated as a medical condition. However consider the next piece of text:</p>

<p>""John is sidelined with a dislocated right kneecap.""</p>

<p>In this piece of text ACM is not able to recognize dislocated as a medical condition. Similarly, if I were to put in a piece of text like ""Left ankle sprain"", ACM is able to recognize ankle sprain as a medical condition however if I were to put in ""sprained left ankle"" it does not catch on to the word sprained as a medical condition.</p>

<p>Is there any way in which I can clean my text of change the order of the words so that those entities can be tagged accurately? </p>
",Preprocessing of the text & Tokenization,identifying different tense word amazon comprehend medical using amazon comprehend medical entity detection injury let say piece text follows john surgery repair dislocated left knee full acl tear amazon comprehend medical acm able recognize dislocated medical condition however consider next piece text john sidelined dislocated right kneecap piece text acm able recognize dislocated medical condition similarly put piece text like left ankle sprain acm able recognize ankle sprain medical condition however put sprained left ankle doe catch word sprained medical condition way clean text change order word entity tagged accurately
How do you get a sublist of elements in fixed order in Python? What about with fillers?,"<p>If I have a list of strings from a tokenized string, how do I get a sublist from the list starting at a phrase (a fixed-order list of strings, not just one string, although <code>len(phrase)</code> could be 1 or more) and ending at another phrase (again, 1 or more)?</p>

<p>How do you extend the above so that it works with fillers, like <code>start_phrase = [""I"", ""don't"", ""think"", ""FILLER"", ""would"", ""appreciate""]</code>?</p>

<p>Here's a pseudo-example of input and expected output:</p>

<pre class=""lang-py prettyprint-override""><code>lst = nltk.tokenize.word_tokenize(""This is a sentence.\
    Do you prefer sentences that are short, or sentences that are long?"")
lst.sublist([""Do"", ""you"", ""prefer"", ""FILLER"", ""that""], [""or""])
</code></pre>

<pre class=""lang-sh prettyprint-override""><code>[""are"", ""short"", "",""]
</code></pre>

<p>N.B. that word_tokenize() doesn't only have ""words"" in the list it returns, but also punctuation, like so: <code>"",""</code>.</p>
",Preprocessing of the text & Tokenization,get sublist element fixed order python filler list string tokenized string get sublist list starting phrase fixed order list string one string although could ending another phrase extend work filler like pseudo example input expected output n b word tokenize word list return also punctuation like
Transformers and BERT: dealing with possessives and apostrophes when encode,"<p>Let's consider two sentences:</p>

<pre><code>""why isn't Alex's text tokenizing? The house on the left is the Smiths' house""
</code></pre>

<p>Now let's tokenize and decode:</p>

<pre><code>from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
tokenizer.decode(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(""why isn't Alex's text tokenizing? The house on the left is the Smiths' house"")))
</code></pre>

<p>We get:</p>

<pre><code>""why isn't alex's text tokenizing? the house on the left is the smiths'house""
</code></pre>

<p><strong>My question is how dealing with missing space in some possessives like <em>smiths'house</em>?</strong></p>

<p>For me, it seems that the process of tokenization in Transformers is done not right. Let's consider output of</p>

<pre><code>tokenizer.tokenize(""why isn't Alex's text tokenizing? The house on the left is the Smiths' house"")
</code></pre>

<p>we get:</p>

<pre><code>['why', 'isn', ""'"", 't', 'alex', ""'"", 's', 'text', 'token', '##izing', '?', 'the', 'house', 'on', 'the', 'left', 'is', 'the', 'smith', '##s', ""'"", 'house']
</code></pre>

<p>So in this step, we already have lost important information about the last apostrophe. It would be much better if tokenization was done in the another way:</p>

<pre><code>['why', 'isn', ""##'"", '##t', 'alex', ""##'"", '##s', 'text', 'token', '##izing', '?', 'the', 'house', 'on', 'the', 'left', 'is', 'the', 'smith', '##s', ""##'"", 'house']
</code></pre>

<p>In this way, tokenization keeps all information about apostrophes, and we will not have problems with possessives.</p>
",Preprocessing of the text & Tokenization,transformer bert dealing possessive apostrophe encode let consider two sentence let tokenize decode get question dealing missing space possessive like smith house seems process tokenization transformer done right let consider output get step already lost important information last apostrophe would much better tokenization wa done another way way tokenization keep information apostrophe problem possessive
training fasttext models with social generated content,"<p>I am currently learning about text classification using Facebook FastText. I have found some data from Kaggle that contains characters such as �� or twitter username and hashtags. I tried searching the web however there is no clarification of how you really need to clean/pre-process your text before training a model. </p>

<p>In some blogs I've seen authors writing about tokenisation however its not mentioned in fasttext. Another point it that fasttext git has examples of clean data, such as stackoverflow but nothing for twitter or such platform. </p>

<p>Question is, what is the best practice to pre-process user(social) generated content before training a model? What needs to be redacted? </p>

<p>Thanks</p>
",Preprocessing of the text & Tokenization,training fasttext model social generated content currently learning text classification using facebook fasttext found data kaggle contains character twitter username hashtags tried searching web however clarification really need clean pre process text training model blog seen author writing tokenisation however mentioned fasttext another point fasttext git ha example clean data stackoverflow nothing twitter platform question best practice pre process user social generated content training model need redacted thanks
Splitting/Tokenizing a sentence into string words with special conditions,"<p>I am trying to implement a tokenizer to split string of words.</p>

<p>The special conditions I have are: split punctuation . , ! ? into a separate string
and split any characters that have a space in them i.e. <code>I have a dog!'-4@</code> -> <code>'I', 'have', 'a' , 'dog', !, ""'-4@""</code></p>

<p>Something like this.....</p>

<p>I don't plan on trying the nltk's package, and I have looked at <code>re.split</code> and <code>re.findall</code>, yet for both cases:
<code>re.split</code> = I don't know how to split out words with punctuation next to them such as 'Dog,'
<code>re.findall</code> = Sure it prints out all the matched string, but what about the unmatched ones? </p>

<p>IF you guys have any suggestions, I'd be very happy to try them.</p>
",Preprocessing of the text & Tokenization,splitting tokenizing sentence string word special condition trying implement tokenizer split string word special condition split punctuation separate string split character space e something like plan trying nltk package looked yet case know split word punctuation next dog sure print matched string unmatched one guy suggestion happy try
How to split a sentence from dataframes with nltk library?,"<p>I want to create bag of words models but with calculate <em>relative frequencies</em> with nltk package. My data is built with pandas dataframe.</p>

<p>Here is my data:</p>

<pre><code>text    title   authors label
0   On Saturday, September 17 at 8:30 pm EST, an e...   Another Terrorist Attack in NYC…Why Are we STI...   ['View All Posts', 'Leonora Cravotta']  Real
1   Story highlights ""This, though, is certain: to...   Hillary Clinton on police shootings: 'too many...   ['Mj Lee', 'Cnn National Politics Reporter']    Real
2   Critical Counties is a CNN series exploring 11...   Critical counties: Wake County, NC, could put ...   ['Joyce Tseng', 'Eli Watkins']  Real
3   McCain Criticized Trump for Arpaio’s Pardon… S...   NFL Superstar Unleashes 4 Word Bombshell on Re...   []  Real
4   Story highlights Obams reaffirms US commitment...   Obama in NYC: 'We all have a role to play' in ...   ['Kevin Liptak', 'Cnn White House Producer']    Real
5   Obama weighs in on the debate\n\nPresident Bar...   Obama weighs in on the debate   ['Brianna Ehley', 'Jack Shafer']    Real
</code></pre>

<p>And I've tried to convert it into string</p>

<pre><code>import nltk 
import numpy as np
import random
import bs4 as bs
import re

data = df.astype(str)
data
</code></pre>

<p>However, when I try to tokenize the word it has error like this</p>

<pre><code>corpus = nltk.sent_tokenize(data['text'])

TypeError: expected string or bytes-like object
</code></pre>

<p>But It seems doesn't work:( Has anybody know how to tokenize the sentences each rows in a column ['text']?</p>
",Preprocessing of the text & Tokenization,split sentence dataframes nltk library want create bag word model calculate relative frequency nltk package data built panda dataframe data tried convert string however try tokenize word ha error like seems work ha anybody know tokenize sentence row column text
Remove repeating special characters using regular expression if present alone but not when surrounded by words or numbers,"<p>How can I remove special characters from a string IFF it is present as an individual. I am trying to work on a tweet author classification model and my idea is that some people use special characters as a trademark and it can help model in better judgement such as </p>

<pre><code>P!nk
A$AP
</code></pre>

<p>are trademark for two individual singers. I want to remove individual and repeating special characters such as </p>

<pre><code>whatt??
This is Good. I want both dots removed.
I'm thinking....
</code></pre>

<p>But do not want to remove <code>#hashtag_for_life</code> or something like this</p>

<p>I have used </p>

<pre><code>re.sub(r'([\W_])\1+',' ','hi my % na$me is @shady #for_life')
</code></pre>

<p>but failing in individual. Can someone please provide a solution.</p>

<p><strong>EDIT:EXAMPLE</strong></p>

<p>How can I convert </p>

<pre><code>'p!nk &amp; A$AP are 2 singers..... what? are the b0th rappers? ? ? NO!! #singer ##rapper @shady'
</code></pre>

<p>to </p>

<pre><code>'p!nk A$AP are singers what are the b0th rappers NO #singer #rapper @shady'
</code></pre>

<p>It means that individual numbers are gone, individual special characters gone, trailing special characters are gone and repeating special characters are changed to single special characters which are either in the middle or at the beginning of a word.</p>
",Preprocessing of the text & Tokenization,remove repeating special character using regular expression present alone surrounded word number remove special character string iff present individual trying work tweet author classification model idea people use special character trademark help model better judgement trademark two individual singer want remove individual repeating special character want remove something like used failing individual someone please provide solution edit example convert mean individual number gone individual special character gone trailing special character gone repeating special character changed single special character either middle beginning word
Nltk lemmatizers do not recognize the plural form of chemical names,"<p>So, I must admit, I'm a total noob in nlp, and I have no idea whatsoever about nltk, I'm just trying to use a legacy code left by the previous developer. I need to lemmatize words, mostly from chemical and biotech publications. I generally use WordNetLemmatizer. Most of the time it works.</p>

<pre><code>from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatizer.lemmatize('cats')
</code></pre>

<p>returns cat.</p>

<p>But then I try</p>

<pre><code>lemmatizer.lemmatize('dehydrogenases')
</code></pre>

<p>it returns 'dehydrogenases'. I want it to return 'dehydrogenase'. How can I do that?</p>
",Preprocessing of the text & Tokenization,nltk lemmatizers recognize plural form chemical name must admit total noob nlp idea whatsoever nltk trying use legacy code left previous developer need lemmatize word mostly chemical biotech publication generally use wordnetlemmatizer time work return cat try return dehydrogenases want return dehydrogenase
Dealing with new words in gensim not found in model,"<p>Lets say I am trying to compute the average distance between a word and a document using distances() or compute cosine similarity between two documents using n_similarity(). However, lets say these new documents contain words that the original model did not. How does gensim deal with that?</p>

<p>I have been reading through the documentation and cannot find what gensim does with unfound words.</p>

<p>I would prefer gensim to not count those in towards the average. So, in the case of distances(), it should simply not return anything or something I can easily delete later before I compute the mean using numpy. In the case of n_similarity, gensim of course has to do it by itself....</p>

<p>I am asking because the documents and words that my program will have to classify will in some instances contain unknown words, names, brands etc that I do not want to be taken into consideration during classification. So, I want to know if I'll have to preprocess every document that I am trying to classify. </p>
",Preprocessing of the text & Tokenization,dealing new word gensim found model let say trying compute average distance word document using distance compute cosine similarity two document using n similarity however let say new document contain word original model doe gensim deal reading documentation find gensim doe unfound word would prefer gensim count towards average case distance simply return anything something easily delete later compute mean using numpy case n similarity gensim course ha asking document word program classify instance contain unknown word name brand etc want taken consideration classification want know preprocess every document trying classify
Python nltk incorrect sentence tokenization with custom abbrevations,"<p>I am using <a href=""http://www.nltk.org/api/nltk.tokenize.html?highlight=punkt"" rel=""nofollow noreferrer"">nltk tokenize</a> library to split up english sentences.
Many sentences contain abbreviations such as <code>e.g.</code> or <code>eg.</code> thus I updated the tokenizer with these custom abbreviations.
I found a strange tokenization behaviour with a sentence though:</p>

<pre class=""lang-py prettyprint-override""><code>import nltk

nltk.download(""punkt"")
sentence_tokenizer = nltk.data.load(""tokenizers/punkt/english.pickle"")

extra_abbreviations = ['e.g', 'eg']
sentence_tokenizer._params.abbrev_types.update(extra_abbreviations)

line = 'Required experience with client frameworks (e.g. React, Vue.js) and testing (e.g. Karma, Tape)'

for s in sentence_tokenizer.tokenize(line):
    print(s)

# OUTPUT
# Required experience with client frameworks (e.g. React, Vue.js) and testing (e.g.
# Karma, Tape)
</code></pre>

<p>So as you can see the tokenizer does not split on the first abbreviation (correct) but it does on the second (incorrect).</p>

<p>The weird thing is that if I change the word <code>Karma</code> in anything else, it works correctly.</p>

<pre class=""lang-py prettyprint-override""><code>import nltk

nltk.download(""punkt"")
sentence_tokenizer = nltk.data.load(""tokenizers/punkt/english.pickle"")

extra_abbreviations = ['e.g', 'eg']
sentence_tokenizer._params.abbrev_types.update(extra_abbreviations)

line = 'Required experience with client frameworks (e.g. React, Vue.js) and testing (e.g. SomethingElse, Tape)'

for s in sentence_tokenizer.tokenize(line):
    print(s)

# OUTPUT
# Required experience with client frameworks (e.g. React, Vue.js) and testing (e.g. SomethingElse, Tape)
</code></pre>

<p>Any clue why is this happening?</p>
",Preprocessing of the text & Tokenization,python nltk incorrect sentence tokenization custom abbrevations using nltk tokenize library split english sentence many sentence contain abbreviation thus updated tokenizer custom abbreviation found strange tokenization behaviour sentence though see tokenizer doe split first abbreviation correct doe second incorrect weird thing change word anything else work correctly clue happening
unable to get any entity after training a blank spacy model,"<p>I have been working on a legal data to recognize the custom entities within a legal document. I am training empty spacy model on my cleaned training dataset(JSON format as mentioned on spacy documentation). I have tried to remove  punctuation, special character, brackets etc from the training dataset. I have labeled the new entity by name 'dictkey' and mentioned the startindex and endindex in it's JSON in order to create the training dataset.
Below is the links for training dataset and the code which i am using. Can you please have a look on the attached training dataset if there is any issue or further cleaning required for this dataset?</p>

<p><a href=""https://techmailer.online/TRAIN_DATA3json.txt"" rel=""nofollow noreferrer"">https://techmailer.online/TRAIN_DATA3json.txt</a></p>

<p><a href=""https://techmailer.online/train_ner%20-%20Copy.py"" rel=""nofollow noreferrer"">https://techmailer.online/train_ner%20-%20Copy.py</a></p>

<pre><code>from __future__ import unicode_literals, print_function
import plac
import random
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding
import re
#import createtrainingdataset_updated_v2 as traindataset


# training data
#TRAIN_DATA = [
#    (""Who is Shaka Khan?"", {""entities"": [(7, 17, ""PERSON"")]}),
#    (""I like London and Berlin."", {""entities"": [(7, 13, ""LOC""), (18, 24, ""LOC"")]}),
#]



with open('/Users/modis1/Desktop/24-01/TRAIN_DATA3json.txt', 'r', encoding='utf-8') as openfile:  
    TRAIN_DATA = openfile.read()


@plac.annotations(
    model=(""Model name. Defaults to blank 'en' model."", ""option"", ""m"", str),
    output_dir=(""Optional output directory"", ""option"", ""o"", Path),
    n_iter=(""Number of training iterations"", ""option"", ""n"", int),
)

def main(model=None, output_dir='/Users/A-GUPTA50/Desktop/ShubhamModi/NewSavedModel/', n_iter=100):
    """"""Load the model, set up the pipeline and train the entity recognizer.""""""
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank(""en"")  # create blank Language class
        print(""Created blank 'en' model"")

    # create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if ""ner"" not in nlp.pipe_names:
        ner = nlp.create_pipe(""ner"")
        nlp.add_pipe(ner, last=True)
    # otherwise, get it so we can add labels
    else:
        ner = nlp.get_pipe(""ner"")

    # add labels
    for _, annotations in TRAIN_DATA:
        for ent in annotations.get(""entities""):
            ner.add_label(ent[2])

    # get names of other pipes to disable them during training
    pipe_exceptions = [""ner"", ""trf_wordpiecer"", ""trf_tok2vec""]
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]
    with nlp.disable_pipes(*other_pipes):  # only train NER
        # reset and initialize the weights randomly – but only if we're
        # training a new model
        if model is None:
            nlp.begin_training()
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            losses = {}
            # batch up the examples using spaCy's minibatch
            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(
                    texts,  # batch of texts
                    annotations,  # batch of annotations
                    drop=0.5,  # dropout - make it harder to memorise data
                    losses=losses,
                )
            print(""Losses"", losses)

    # test the trained model
    for text, _ in TRAIN_DATA:
#       text = re.sub(r'\\u\d{4,}','', text.rstrip())
        doc = nlp(text)
        print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])
        print(""Tokens"", [(t.text, t.ent_type_, t.ent_iob) for t in doc])

    # save model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.to_disk(output_dir)
        print(""Saved model to"", output_dir)

        # test the saved model
        print(""Loading from"", output_dir)
        nlp2 = spacy.load(output_dir)
        for text, _ in TRAIN_DATA:
            doc = nlp2(text)
            print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])
            print(""Tokens"", [(t.text, t.ent_type_, t.ent_iob) for t in doc])


if __name__ == ""__main__"":
    plac.call(main)
</code></pre>
",Preprocessing of the text & Tokenization,unable get entity training blank spacy model working legal data recognize custom entity within legal document training empty spacy model cleaned training dataset json format mentioned spacy documentation tried remove punctuation special character bracket etc training dataset labeled new entity name dictkey mentioned startindex endindex json order create training dataset link training dataset code using please look attached training dataset issue cleaning required dataset
How to detect a stopword in a given string and convert only that stopword from sentence case or uppercase to lower case,"<p>I have the following code</p>

<pre><code>import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
import re
text='Director Of IT'
pattern = re.compile(r'\b(' + r'|'.join(stopwords.words('english'))+ r')\b\s*') #remove stop words
textmod5 = pattern.sub('', text)
print(textmod5)
</code></pre>

<p>I would like to make only the stop word 'Of' into lower case 'of'. How to achieve that using the same above expression.</p>

<p>I have tried the following but with no avail</p>

<pre><code>pattern = re.compile(r'\b(' + r'|'.join(stopwords.words('english'))+ r')\b\s*'.lower)
pattern = re.compile(r'\b(' + r'|'.join(stopwords.words.lower(('english')))+ r')\b\s*')
</code></pre>

<p>What would be the correct pattern to achieve my objective</p>

<p>Current output : Director Of IT</p>

<p>Required Output : Director of IT</p>
",Preprocessing of the text & Tokenization,detect stopword given string convert stopword sentence case uppercase lower case following code would like make stop word lower case achieve using expression tried following avail would correct pattern achieve objective current output director required output director
Gensim phrase handling sentence with a lot of punctuation,"<p>Now I am trying to use <code>gensim Phrases</code> in order to learn the phrase/special meaning base on my own corpus.</p>

<p>Suppose I have the corpus related to the car brand, by removing the <strong>punctuation</strong> and <strong>stopwords</strong>, <strong>tokenizing the sentence</strong>, eg:</p>

<pre><code>sent1 = 'aston martin is a car brand'
sent2 = 'audi is a car brand'
sent3 = 'bmw is a car brand'
...
</code></pre>

<p>In this way, I would like to use <code>gensim Phrases</code> to learn so that output looks like:</p>

<pre><code>from gensim.models import Phrases
sents = [sent1, sent2, sent3, ...]
sents_stream = [sent.split() for sent in sents]
bigram = Phrases(sents_stream)

for sent in sents:
    print(bigram [sent])

# Ouput should be like:
['aston_martin', 'car', 'brand']
['audi', 'car', 'brand']
['bmw', 'car', 'brand']
...
</code></pre>

<p>However, if a lot of sentences that have a lot of punctuation:</p>

<pre><code>sent1 = 'aston martin is a car brand'
sent2 = 'audi is a car brand'
sent3 = 'bmw is a car brand'
sent4 = 'jaguar, aston martin, mini cooper are british car brand'
sent5 = 'In all brand, I love jaguar, aston martin and mini cooper'
...

</code></pre>

<p>Then the output looks like:</p>

<pre><code>from gensim.models import Phrases
sents = [sent1, sent2, sent3, sent4, sent5, ...]
sents_stream = [sent.split() for sent in sents]
bigram = Phrases(sents_stream)

for sent in sents:
    print(bigram [sent])

# Ouput should be like:
['aston', 'martin', 'car', 'brand']
['audi', 'car', 'brand']
['bmw', 'car', 'brand']
['jaguar', 'aston', 'martin_mini', 'cooper', 'british', 'car', 'brand']
['all', 'brand', 'love', 'jaguar', 'aston', 'martin_mini', 'cooper']
...
</code></pre>

<p>In this case, how should I handle the sentence with lot of punctuation to prevent <code>martin_mini</code> case and make the output looks like:</p>

<pre><code>['aston', 'martin', 'car', 'brand']
['audi', 'car', 'brand']
['bmw', 'car', 'brand']
['jaguar', 'aston_martin', 'mini_cooper', 'british', 'car', 'brand'] # Change
['all', 'brand', 'love', 'jaguar', 'aston_martin', 'mini_cooper'] # Change
...
</code></pre>

<p>Thanks so much for helping!</p>
",Preprocessing of the text & Tokenization,gensim phrase handling sentence lot punctuation trying use order learn phrase special meaning base corpus suppose corpus related car brand removing punctuation stopwords tokenizing sentence eg way would like use learn output look like however lot sentence lot punctuation output look like case handle sentence lot punctuation prevent case make output look like thanks much helping
Calculate number of filtered Bigrams,"<p>Working on Hands on problems on NLP and got stuck in TASK given below.</p>
<p>Below are the statements which are required to be executed in sequence.</p>
<p>I have completed the below steps but the fresco platform is not accepting the solution.</p>
<p>Please let me know what I did wrong in the below code and steps</p>
<h3>TASK</h3>
<p>1.Import text corpus brown</p>
<ol start=""2"">
<li><p>Extract the list of words associated with text collections belonging to the
<strong>news</strong> genre. Store the result in the variable <strong>news_words</strong>.</p>
</li>
<li><p>Convert each word of the list <strong>news_words</strong> into lower case, and store the
result in <strong>lc_news_words</strong>.</p>
</li>
<li><p>Compute bigrams of the list <strong>lc_news_words</strong>, and store it in the variable
<strong>lc_news_bigrams</strong>.</p>
</li>
<li><p>From <strong>lc_news_bigrams</strong>, filter bigrams where both words contain only
alphabet characters. Store the result in <strong>lc_news_alpha_bigrams</strong>.</p>
</li>
<li><p>Extract the list of words associated with the corpus stopwords. Store the
result in <strong>stop_words</strong>.</p>
</li>
<li><p>Convert each word of the list <strong>stop_words</strong> into lower case, and store the
result in <strong>lc_stop_words</strong>.</p>
</li>
<li><p>Filter only the bigrams from <strong>lc_news_alpha_bigrams</strong> where the words are
not part of <strong>lc_stop_words</strong>. Store the result in
<strong>lc_news_alpha_nonstop_bigrams</strong>.</p>
</li>
<li><p>Print the total number of filtered bigrams.</p>
</li>
</ol>
<p>Below is the code which I have done so far. But fresco platform is not accepting the output.</p>
<pre class=""lang-py prettyprint-override""><code>import nltk

import nltk.corpus

from nltk.corpus import brown

from nltk.util import bigrams

from nltk.corpus import stopwords

news_words = brown.words(categories='news')

lc_news_words  = [w.lower() for w in news_words]

lc_news_bigrams = list(nltk.bigrams(lc_news_words))

lc_news_alpha_bigrams = [(word1, word2) for word1, word2 in lc_news_bigrams if (word1.isalpha() and word2.isalpha()) ]

stop_words = stopwords.words('english')

lc_stop_words = [w.lower() for w in stop_words ]

lc_news_alpha_nonstop_bigrams = [ (w1, w2) for w1, w2 in lc_news_alpha_bigrams if (w1.lower() not in lc_stop_words and w2.lower() not in lc_stop_words) ] 

len((lc_news_alpha_nonstop_bigrams))
</code></pre>
",Preprocessing of the text & Tokenization,calculate number filtered bigram working hand problem nlp got stuck task given statement required executed sequence completed step fresco platform accepting solution please let know wrong code step task import text corpus brown extract list word associated text collection belonging news genre store result variable news word convert word list news word lower case store result lc news word compute bigram list lc news word store variable lc news bigram lc news bigram filter bigram word contain alphabet character store result lc news alpha bigram extract list word associated corpus stopwords store result stop word convert word list stop word lower case store result lc stop word filter bigram lc news alpha bigram word part lc stop word store result lc news alpha nonstop bigram print total number filtered bigram code done far fresco platform accepting output
NLP - Removing Stop Words and Counting Word Frequency,"<p>I currently have a working script to do a simple count of word frequency across a column <strong>(conversation_message__body)</strong> of data coming from our database. A sample of the working code and the output (image) is below.</p>

<pre><code>import pandas as pd
import numpy as np

x = df.conversation_message__body.str.split(expand=True).stack().value_counts()

y = pd.DataFrame(data=x)

y.reset_index(level=0,inplace=True)

print(y)
</code></pre>

<p>The issue is that there are many words I want to exclude from this analysis. This is a common issue in NLP as I understand. So I altered my script as seen below:</p>

<pre><code># Import stopwords with nltk.
from nltk.corpus import stopwords
import pandas as pd
import numpy as np

stop = stopwords.words('english')
newStopWords = ['hello','hi','hey','im','get']
stop.extend(newStopWords)

df['conversation_message__body'] = df.conversation_message__body.str.replace(""[^\w\s]"", """").str.lower()

df['conversation_message__body'] = df['conversation_message__body'].apply(lambda x: [item for item in x.split() if item not in stop])

x = df.conversation_message__body.str.split(expand=True).stack().value_counts()

y = pd.DataFrame(data=x)

y.reset_index(level=0,inplace=True)

print(y)

</code></pre>

<p>This is <strong>NOT</strong> working for me and returns no results. Even when I try <code>print(x)</code> to see what the initial transformation looks like, I only get back > <code>Series([], dtype: int64)</code></p>

<p>I am pretty sure there are some basics I am missing here, but I have been working on this for a while with no luck. Can anyone push me in the right direction? </p>

<p><a href=""https://i.sstatic.net/owM1N.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/owM1N.png"" alt=""enter image description here""></a></p>
",Preprocessing of the text & Tokenization,nlp removing stop word counting word frequency currently working script simple count word frequency across column conversation message body data coming database sample working code output image issue many word want exclude analysis common issue nlp understand altered script seen working return result even try see initial transformation look like get back pretty sure basic missing working luck anyone push right direction
What is the most efficient way to calculate the distance of a word with the other words in a list?,"<p>I am working on correcting Turkish words by using <strong>Levensthein</strong> distance. First of all I detect wrong written words and compare them with a list that contains all Turkish words. The list contains about 1.300.000 words. I use Levensthein distance to compare word with the words in the list. Here is the part of my code.</p>

<pre><code>index_to_track_document_order = 1
log_text = ''
main_directory = ""C:\\words.txt""
f= codecs.open(main_directory,mode=""rb"",encoding=""utf-8"")
f=f.readlines()
similarity = 0
text_to_find = 'aktarıları'
best_fit_word = ''
for line in f:
    word = word_tokenize( line, language= 'turkish')[0]
    root = word_tokenize( line, language= 'turkish')[1]
    new_similarity = textdistance.levenshtein.normalized_similarity(text_to_find , word) * 100
    if new_similarity &gt; similarity:
        similarity = new_similarity
        best_fit_word = word
        if(similarity &gt; 90):
            print(best_fit_word, str(similarity)) 
</code></pre>

<p>As I mentioned, word.txt contains more than a million records and so that my code takes more than 5 minutes to complete. How I can optimize the code so that it can complete in a shorter time. Thank you.</p>
",Preprocessing of the text & Tokenization,efficient way calculate distance word word list working correcting turkish word using levensthein distance first detect wrong written word compare list contains turkish word list contains word use levensthein distance compare word word list part code mentioned word txt contains million record code take minute complete optimize code complete shorter time thank
Is it possible to drop sentences from the text with NLTK in Python?,"<p>For example, I have a text that consists of several sentences:</p>

<p>""First sentence is not relevant. Second contains information about <strong>KPI</strong> I want to keep. Third is useless. Fourth mentions <strong>topic</strong> relevant for me"".</p>

<p>In addition, I have self-constructed dictionary with words {<strong>KPI, topic</strong>}. 
Is it somehow possible to write a code that will keep only those sentences, where at least one word is mentioned in the dictionary? So that from the above example, only 2nd and 4th sentence will remain.</p>

<p>Thanks</p>

<p>P.S. I already have a code to tokenize the text into sentences, but leaving only ""relevant"" ones is not something common, as I see.</p>
",Preprocessing of the text & Tokenization,possible drop sentence text nltk python example text consists several sentence first sentence relevant second contains information kpi want keep third useless fourth mention topic relevant addition self constructed dictionary word kpi topic somehow possible write code keep sentence least one word mentioned dictionary example nd th sentence remain thanks p already code tokenize text sentence leaving relevant one something common see
Regular Expressions - Read through Text Doc and Extract Sentences with a Specific Word,"<p>I have a series of large text documents.  I need to read through them and - if a particular word appears - extract the entire sentence.</p>

<p>So, if I'm searching for the word <code>wobble</code> and a sentence in the document is <code>Weebles wobble but they don't fall down</code>, I want to extract that sentence. </p>

<p>What is the most efficient way to do this?</p>

<p>I can think of two approaches to this:</p>

<ol>
<li><p>Search the document for the word, then extract the particular sentence; or</p></li>
<li><p>Iterate through each sentence in the document.  Check each sentence for the word.  If the sentence has the word extract the sentence.</p></li>
</ol>

<p>I would think 1 is more computationally efficient than 2. But not sure what the syntax would be.</p>

<p>Is there another approach I'm not considering?</p>

<p>Any help on efficiency and syntax appreciated.</p>
",Preprocessing of the text & Tokenization,regular expression read text doc extract sentence specific word series large text document need read particular word appears extract entire sentence searching word sentence document want extract sentence efficient way think two approach search document word extract particular sentence iterate sentence document check sentence word sentence ha word extract sentence would think computationally efficient sure syntax would another approach considering help efficiency syntax appreciated
Stemming Geographical words,"<p>What is the best stemming method to geographical entities? I want to convert the geographical entities gathered in a dataframe column to accurate region names like for example converting:</p>

<pre><code>['India','Indian','Japanese','Europe','European']
</code></pre>

<p>to</p>

<pre><code>['India','India','Japan','Europe','Europe']
</code></pre>

<p>The geographical words will be extracted from more than 50,000 news, so I'm looking for a function which could work critical situations.</p>
",Preprocessing of the text & Tokenization,stemming geographical word best stemming method geographical entity want convert geographical entity gathered dataframe column accurate region name like example converting geographical word extracted news looking function could work critical situation
Why are spelled-out numbers below twenty considered stopwords in Spacy?,"<p>Spacy considers spelled-out numbers below twenty like ""five"" or ""eleven"" to be stop words, it also considers ""twenty"", ""thirty, ... ""sixty"" to be stopwords too.
I'm questioning the reason behind this. Seems like numbers are important information we don't want to dispose of?</p>
",Preprocessing of the text & Tokenization,spelled number twenty considered stopwords spacy spacy considers spelled number twenty like five eleven stop word also considers twenty thirty sixty stopwords questioning reason behind seems like number important information want dispose
"Tokenizing the stop words generated tokens [&#39;ha&#39;, &#39;le&#39;, &#39;u&#39;, &#39;wa&#39;] not in stop_words","<p>I am making a chatbot using Python. 
Code: </p>

<pre><code>import nltk
import numpy as np
import random
import string 
f=open('/home/hostbooks/ML/stewy/speech/chatbot.txt','r',errors = 'ignore')
raw=f.read()
raw=raw.lower()# converts to lowercase

sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences 
word_tokens = nltk.word_tokenize(raw)# converts to list of words

lemmer = nltk.stem.WordNetLemmatizer()    

def LemTokens(tokens):
    return [lemmer.lemmatize(token) for token in tokens]

remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)

def LemNormalize(text):
    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))

GREETING_INPUTS = (""hello"", ""hi"", ""greetings"", ""sup"", ""what's up"",""hey"",""hii"")
GREETING_RESPONSES = [""hi"", ""hey"", ""*nods*"", ""hi there"", ""hello"", ""I am glad! You are talking to me""]


def greeting(sentence):
    for word in sentence.split():
        if word.lower() in GREETING_INPUTS:
            return random.choice(GREETING_RESPONSES)

from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.metrics.pairwise import cosine_similarity

def response(user_response):
    robo_response=''
    sent_tokens.append(user_response)    

    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')
    tfidf = TfidfVec.fit_transform(sent_tokens)
    vals = cosine_similarity(tfidf[-1], tfidf)
    idx=vals.argsort()[0][-2]
    flat = vals.flatten()
    flat.sort()
    req_tfidf = flat[-2]    

    if(req_tfidf==0):
        robo_response=robo_response+""I am sorry! I don't understand you""
        return robo_response
    else:
        robo_response = robo_response+sent_tokens[idx]
        return robo_response

flag=True
print(""ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!"")

while(flag==True):
    user_response = input()
    user_response=user_response.lower()
    if(user_response!='bye'):
        if(user_response=='thanks' or user_response=='thank you' ):
            flag=False
            print(""ROBO: You are welcome.."")
        else:
            if(greeting(user_response)!=None):
                print(""ROBO: ""+greeting(user_response))
            else:
                print(""ROBO: "",end="""")
                print(response(user_response))
                sent_tokens.remove(user_response)
    else:
        flag=False
        print(""ROBO: Bye! take care.."")
</code></pre>

<p>It is running well but with every conversation it's giving this error:</p>

<pre><code>/home/hostbooks/django1/myproject/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. 

Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.
</code></pre>

<p>These are some conversations from CMD:</p>

<p><em><strong>ROBO: a chatbot is a piece of software that conducts a conversation via auditory or textual methods.</strong></em></p>

<p><em>what is india</em></p>

<pre><code>    /home/hostbooks/django1/myproject/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words. 'stop_words.' % sorted(inconsistent))
</code></pre>

<p><em><strong>ROBO: india's wildlife, which has traditionally been viewed with tolerance in india's culture, is supported among these forests, and elsewhere, in protected habitats.</strong></em></p>

<p><em>what is chatbot</em></p>

<pre><code>    /home/hostbooks/django1/myproject/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words. 'stop_words.' % sorted(inconsistent))
</code></pre>

<p><em><strong>ROBO: a chatbot is a piece of software that conducts a conversation via auditory or textual methods.</strong></em></p>
",Preprocessing of the text & Tokenization,tokenizing stop word generated token ha le u wa stop word making chatbot using python code running well every conversation giving error conversation cmd robo chatbot piece software conduct conversation via auditory textual method india robo india wildlife ha traditionally viewed tolerance india culture supported among forest elsewhere protected habitat chatbot robo chatbot piece software conduct conversation via auditory textual method
Query related to stemming in NLP,"<p>I am working on hands-on task based on stemming under NLP using python.</p>
<p>Below is the task which would required to be executed step wise to fetch the result.</p>
<p>I have completed till step 13 and got stuck at step number 14 and 15 (see below).</p>
<p>Please help me to know how to perform the step number 14 and 15.</p>
<h3>TASK</h3>
<ol>
<li><p>Import the text corpus <strong>brown</strong>.</p>
</li>
<li><p>Extract the list of words associated with text collections belonging to the humor genre. Store the result in the variable <strong>humor_words</strong>.</p>
</li>
<li><p>Convert each word of the list humor_words into lower case, and store the result in <strong>lc_humor_words</strong>.</p>
</li>
<li><p>Find the list of unique words present in <strong>lc_humor_words</strong>. Store the result in <strong>lc_humor_uniq_words</strong>.</p>
</li>
<li><p>Import the corpus <strong>words</strong>.</p>
</li>
<li><p>Extract the list of words associated with the corpus words. Store the result in the variable <strong>wordlist_words</strong>.</p>
</li>
<li><p>Find the list of unique words present in <strong>wordlist_words</strong>. Store the result in <strong>wordlist_uniq_words</strong>.</p>
</li>
<li><p>Create an instance of PorterStemmer named, <strong>porter</strong>.</p>
</li>
<li><p>Create an instance of LancasterStemmer named, <strong>lancaster</strong>.</p>
</li>
<li><p>Stem each word present in <strong>lc_humor_uniq_words</strong> with porter instance, and store the result in the list <strong>p_stemmed</strong></p>
</li>
<li><p>Stem each word present in <strong>lc_humor_uniq_words</strong> with lancaster instance, and store the result in the <strong>listl_stemmed</strong>`</p>
</li>
<li><p>Filter the stemmed words from <strong>p_stemmed</strong> which are also present in <strong>wordlist_uniq_words</strong>. Store the result in <strong>p_stemmed_in_wordlist</strong>.</p>
</li>
<li><p>Filter the stemmed words from <strong>l_stemmed</strong> which are also present in <strong>wordlist_uniq_words</strong>. Store the result in <strong>l_stemmed_in_wordlist</strong>.</p>
</li>
<li><p>Filter the words from <strong>lc_humor_uniq_words</strong> which have the same length as its corresponding stemmed word present in <strong>p_stemmed</strong>, and also contains at least one different character from the corresponding stemmed word. Store the result in the list <strong>p_stemmed_diff</strong>.</p>
</li>
<li><p>Filter the words from <strong>lc_humor_uniq_words</strong> which have the same length as its corresponding stemmed word, present in <strong>l_stemmed</strong>, and also contains at least one different character from the corresponding stemmed word. Store the result in list <strong>l_stemmed_diff</strong>.</p>
</li>
<li><p>Print the number of words present in <strong>p_stemmed_diff</strong>.</p>
</li>
<li><p>Print the number of words present in <strong>l_stemmed_diff</strong>.</p>
</li>
</ol>
<p>-Below is the which I have completed till step 13.</p>
<pre><code>import nltk

import nltk.corpus

from nltk.corpus import brown

humor_words = brown.words(categories = 'humor')

lc_humor_words = [w.lower() for w in humor_words]

lc_humor_uniq_words = set(lc_humor_words)

from nltk.corpus import words

wordlist_words = words.words()

wordlist_uniq_words = set(wordlist_words)

from nltk.stem import PorterStemmer

porter = PorterStemmer()

from nltk.stem import LancasterStemmer

lancaster = LancasterStemmer()

p_stemmed = []

for word in lc_humor_uniq_words:

    p_stemmed.append(porter.stem(word))

l_stemmed = []

for wordd in lc_humor_uniq_words:

    l_stemmed.append(lancaster.stem(wordd))

p_stemmed_in_wordlist = [word1 for word1 in p_stemmed if word1 in wordlist_uniq_words]

l_stemmed_in_wordlist = [word2 for word2 in l_stemmed if word2 in wordlist_uniq_words]
</code></pre>
",Preprocessing of the text & Tokenization,query related stemming nlp working hand task based stemming nlp using python task would required executed step wise fetch result completed till step got stuck step number see please help know perform step number task import text corpus brown extract list word associated text collection belonging humor genre store result variable humor word convert word list humor word lower case store result lc humor word find list unique word present lc humor word store result lc humor uniq word import corpus word extract list word associated corpus word store result variable wordlist word find list unique word present wordlist word store result wordlist uniq word create instance porterstemmer named porter create instance lancasterstemmer named lancaster stem word present lc humor uniq word porter instance store result list p stemmed stem word present lc humor uniq word lancaster instance store result listl stemmed filter stemmed word p stemmed also present wordlist uniq word store result p stemmed wordlist filter stemmed word l stemmed also present wordlist uniq word store result l stemmed wordlist filter word lc humor uniq word length corresponding stemmed word present p stemmed also contains least one different character corresponding stemmed word store result list p stemmed diff filter word lc humor uniq word length corresponding stemmed word present l stemmed also contains least one different character corresponding stemmed word store result list l stemmed diff print number word present p stemmed diff print number word present l stemmed diff completed till step
Find matching questions from all records,"<p>I've a library of questions and answers and building an API in NodeJS which allows to search for answers based on the question passed as input. Following is my goal:</p>

<ol>
<li>Split the question by space</li>
<li>Tokenize it and remove stopwords</li>
<li>Query database for records where question contains one or more words from the tokenized array</li>
<li>Ideally sort in descending order total number of matches in the question. For eg: If the question A contains 'module' and 'solution' and question B contains only 'solution', then question A should be shown before question B</li>
</ol>

<p>I've been able to achieve 1 to 3, using the below code:</p>

<pre><code>let question = req.query.question;
let arrQuestions = question.split("" "");
let tokenizedQuestion = stopwords.removeStopwords(arrQuestions);

let whereClause = tokenizedQuestion.join(""%' OR answer LIKE '%"");
whereClause = "" answer LIKE '%"" + whereClause + ""%' "";

let query = ""SELECT * FROM tbl_libraries WHERE "" + whereClause;
</code></pre>

<p>I'm not able to figure out how to achieve 4. Can somebody provide pointers?</p>

<p>Thanks!</p>
",Preprocessing of the text & Tokenization,find matching question record library question answer building api nodejs allows search answer based question passed input following goal split question space tokenize remove stopwords query database record question contains one word tokenized array ideally sort descending order total number match question eg question contains module solution question b contains solution question shown question b able achieve using code able figure achieve somebody provide pointer thanks
Why does this string.punctuation code not work for stripping punctuation?,"<p>I am confused why this code doesn't work the way I want. I am reading in a txt file and printing each item (comma separated) onto a new line. Each item is surrounded with &quot;&quot; and also contains punctuation. I am trying to remove this punctuation. I am familiar with string.punctuation and have it working on a test in my example, but it fails on the items I am looping through, see below:</p>
<pre><code>def read_word_lists(path):
    import string
    with open(path, encoding='utf-8') as f:
        lines = f.readlines()
        for line in lines[0].split(','):
            line = str(line)
            line = line.strip().lower()
            print(''.join(word.strip(string.punctuation) for word in line))
            print(line)
            print(''.join(word.strip(string.punctuation) for word in '&quot;why, does this work?! and not above?&quot;'))

                
read_word_lists('file.txt')
</code></pre>
<p>The result is this:</p>
<pre><code>trying to strip punctuation:  “you never”
originial:  “you never”
test:  why does this work and not above
trying to strip punctuation:  “you always
originial:  “you always&quot;
test:  why does this work and not above
trying to strip punctuation:  ” “your problem is”
originial:  ” “your problem is”
test:  why does this work and not above
trying to strip punctuation:  “the trouble with you is”
originial:  “the trouble with you is”
test:  why does this work and not above
</code></pre>
<p>Any thoughts why the 'trying to strip punctuation' output is not working?</p>
<h3>Edit</h3>
<p>The original file looks like this, if useful:</p>
<p><code>&quot;YOU NEVER”, “YOU ALWAYS&quot;, ” “YOUR PROBLEM IS”, “THE TROUBLE WITH YOU IS”</code></p>
",Preprocessing of the text & Tokenization,doe string punctuation code work stripping punctuation confused code work way want reading txt file printing item comma separated onto new line item surrounded also contains punctuation trying remove punctuation familiar string punctuation working test example fails item looping see result thought trying strip punctuation output working edit original file look like useful
Fast way to look up String Similarity against a large list of strings?,"<p>My aim is to identify the top 10 most similar strings in a large list of strings, given an input string. This is for a web-based API so I would need a very fast response time (&lt;100ms is ideal).</p>

<p>I am operating on Python but can be flexible if there is a better way to achieve this (be it through Bash scripts or another language). I have tried a variety of approaches so far, including <a href=""https://stackoverflow.com/questions/4802137/how-to-use-sequencematcher-to-find-similarity-between-two-strings"">difflib</a> and <a href=""https://stackoverflow.com/questions/56040817/python-fuzzy-matching-strings-in-list-performance/56165274#56165274"">fuzzywuzzy</a> but the results returned have either been too slow or not returning ideal results.</p>

<p>In the case of my most recent experiment, the solution using fuzzywuzzy returns an un-ideal result set.</p>

<p>An input of ""coupons"" returns:</p>

<pre><code>                     Query         Stem  Key compare  score
34046              copson       copson    1  coupon     83
35011           couponcom    couponcom    1  coupon     80
61206             groupon      groupon    1  coupon     77
5834          able coupon   abl coupon    1  coupon     75
124231        rjr coupons   rjr coupon    1  coupon     75
35026          couponscom   couponscom    1  coupon     75
34991             couples        coupl    1  coupon     73
34993   couples dominated  coupl domin    1  coupon     71
11236        arbys coupon  arbi coupon    1  coupon     71
</code></pre>

<p>While there are related keywords in the result set,non-related keywords (""couples"")I know that better results the exist in the large corpus of strings are being overlooked due to the fact they have a larger edit distance between it and the original query. E.g. ""birthday coupon"", ""supermarket coupons""</p>

<p>Is there an approach that does something like n-gram matching to ensure a more relevant similarity? Speed is the high priority here, I deal with issues such as mis-spellings and stemming ahead of time.</p>
",Preprocessing of the text & Tokenization,fast way look string similarity large list string aim identify top similar string large list string given input string web based api would need fast response time ideal operating python flexible better way achieve bash script another language tried variety approach far including case recent experiment solution using fuzzywuzzy return un ideal result set input coupon return related keywords result set non related keywords couple know better result exist large corpus string overlooked due fact larger edit distance original query e g birthday coupon supermarket coupon approach doe something like n gram matching ensure relevant similarity speed high priority deal issue mi spelling stemming ahead time
Is it a bad idea to use the cluster ID from clustering text data using K-means as feature to your supervised learning model?,"<p>I am building a model that will predict the lead time of products flowing through a pipeline. </p>

<p>I have a lot of different features, one is a string containing a few words about the purpose of the product (often abbreviations, name of the application it will be a part of and so forth). I have previously not used this field at all when doing feature engineering. </p>

<p>I was thinking that it would be nice to do some type of clustering on this data, and then use the cluster ID as a feature for my model, perhaps the lead time is correlated with the type of info present in that field.</p>

<p>Here was my line of thinking)</p>

<p>1) Cleaning &amp; tokenizing text.</p>

<p>2) TF-IDF</p>

<p>3) Clustering</p>

<p>But after thinking more about it, is it a bad idea? Because the clustering was based on the old data, if new words are introduced in the new data this will not be captured by the clustering algorithm, and the data should perhaps be clustered differently now. Does this mean that I would have to retrain the entire model (k-means model and then the supervised model) whenever I want to predict new data points? Are there any best practices for this?</p>

<p>Are there better ways of finding clusters for text data to use as features in a supervised model?</p>
",Preprocessing of the text & Tokenization,bad idea use cluster id clustering text data using k mean feature supervised learning model building model predict lead time product flowing pipeline lot different feature one string containing word purpose product often abbreviation name application part forth previously used field feature engineering wa thinking would nice type clustering data use cluster id feature model perhaps lead time correlated type info present field wa line thinking cleaning tokenizing text tf idf clustering thinking bad idea clustering wa based old data new word introduced new data captured clustering algorithm data perhaps clustered differently doe mean would retrain entire model k mean model supervised model whenever want predict new data point best practice better way finding cluster text data use feature supervised model
Efficient way to replace incorrect words in Series of strings in Python,"<p>I'm working with text data, that is handwritten, so it has lots of ortographic errors. I'm currently working with <code>pyspellchecker</code> to clean the data and I'm using the <code>correct()</code> method to find the most likely word when a word doesn't exist. My approach was to create a dictionary with all poorly written words as keys and the most likely word as value:</p>

<pre><code>dic={}
for i in df.text:
    misspelled = spell.unknown(i.split())
    for word in misspelled:
        dic[word]=spell.correction(word)
</code></pre>

<p>Even though this is working, it is doing so very slowly. Thus, I wanted to know if there's a faster option to implement this. Do you have any ideas?</p>

<p>Edit: there are 10571 rows in df.text and strings are usually 5-15 words long. Each loop is taking around 3-5 seconds, which makes for a total of around 40000 seconds to run the whole loop.</p>
",Preprocessing of the text & Tokenization,efficient way replace incorrect word series string python working text data handwritten ha lot ortographic error currently working clean data using method find likely word word exist approach wa create dictionary poorly written word key likely word value even though working slowly thus wanted know faster option implement idea edit row df text string usually word long loop taking around second make total around second run whole loop
Regular Expression Question - Two Negative Look behinds in the same expression,"<p>I have the following problem for which I have been working on for a few hours.
I am trying to build the following RegEx :</p>

<p>I want to be able to extract the word <em>reduced</em> from sentences but not if the word is preceded by a negative expression.</p>

<p>For example</p>

<pre><code>Sentences                                |Output
1. lv function is reduced                    reduced
2. lv function is not reduced                -
3. reduced lv function                       reduced
4. no evidence of reduced lv function        -
</code></pre>

<p>Right now, I have been able to a have a function RegEx for in the cases 3 and 4 where the adjective precedes the noun of interest using a negative look behind.</p>

<p>However, for the cases 1 and 2, the negative look behind does not work.</p>

<p>Here are sentences and the current RegEx to test :</p>

<pre><code>((?&lt;!((no|not|none)(?:\D*?)))(reduced|depressed|normal)(?:\D*?))?(?:lv function|lv|systolic function|left ventricular ejection fraction)(((?:.*\bnot\b)(\D*))(reduced|depressed|normal))?

Sentences :
lv function is reduced   
lv function is not reduced 
reduced lv function
no evidence of reduced lv function   
</code></pre>

<p>Alternatively here is a link : <a href=""http://%20regexr.com/4tc61"" rel=""nofollow noreferrer"">regexr.com/4tc61</a></p>

<p>Also, I am ultimately going to be working in R.</p>

<p>Thank you all.</p>
",Preprocessing of the text & Tokenization,regular expression question two negative look behind expression following problem working hour trying build following regex want able extract word reduced sentence word preceded negative expression example right able function regex case adjective precedes noun interest using negative look behind however case negative look behind doe work sentence current regex test alternatively link regexr com tc also ultimately going working r thank
How to classify derived words that share meaning as the same tokens?,"<p>I would like to count unrelated words in an article but I have troubles with grouping words of the same meaning derived from one another.</p>

<p>For instance, I would like <code>gasoline</code> and <code>gas</code> to be treated as the same token in sentences like <code>The price of gasoline has risen.</code> and <code>""Gas"" is a colloquial form of the word gasoline in North American English. Conversely, in BE the term would be ""petrol"".</code> Therefore, if these two sentences comprised the entire article, the count for <code>gas</code> (or <code>gasoline</code>) would be 3 (<code>petrol</code> would not be counted).</p>

<p>I have tried using NLTK's stemmers and lemmatizers but to no avail. Most seem to reproduce <code>gas</code> as <code>gas</code> and <code>gasoline</code> as <code>gasolin</code> which is not helpful for my purposes at all. I understand that this is the usual behaviour. I have checked out a <a href=""https://stackoverflow.com/questions/42698919/classify-words-with-the-same-meaning"">thread</a> that seems to be a little bit similar, however the answers there are not completely applicable to my case as I require the words to be derived from one another. </p>

<p>How to treat derived words of the same meaning as same tokens in order to count them together?</p>
",Preprocessing of the text & Tokenization,classify derived word share meaning token would like count unrelated word article trouble grouping word meaning derived one another instance would like treated token sentence like therefore two sentence comprised entire article count would would counted tried using nltk stemmer lemmatizers avail seem reproduce helpful purpose understand usual behaviour checked href seems little bit similar however answer completely applicable case require word derived one another p treat derived word meaning token order count together
Text being written to single line when removing stopwords from columns,"<p>I'm trying to remove stopwords from a tab-delimited .txt file using the following code: </p>

<pre><code>import io
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


file = open('textposts_01.txt', encoding='UTF-8')
stop_words = set(stopwords.words('english'))
line = file.read()
words = line.split()
for r in words:
    if not r in stop_words:
        appendFile = open('textposts_02.txt', mode='a', encoding='UTF-8')
        appendFile.write("" ""+r)
        appendFile.close()
</code></pre>

<p>The code executes successfully, but when I view the results all of rows have been re-written onto a single line. How can I maintain the columns while removing the stopwords? </p>

<p>I found the following solution on a similar post: </p>

<pre><code>import io
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

file = open('textposts_01.txt', encoding='UTF-8')
stop_words = set(stopwords.words('english'))
line = file.read()
words = line.split()
for r in words:
    if not r in stop_words:
        appendFile = open('textposts_02.txt', mode='a', encoding='UTF-8')
        appendFile.write("" ""+r)
        appendFile.write(""\n"")
        appendFile.close()
</code></pre>

<p>But inserting a new line simply created a new line after every word so that if I started with a row like this:</p>

<pre><code>0     make a list of every person you know
</code></pre>

<p>the results looked like this: </p>

<pre><code>0
make
list
every
person
know
</code></pre>

<p>and I need the results in rows like so:</p>

<pre><code>0     make list every person
</code></pre>

<p>I've been searching a while, but haven't found any solutions. </p>
",Preprocessing of the text & Tokenization,text written single line removing stopwords column trying remove stopwords tab delimited txt file using following code code executes successfully view result row written onto single line maintain column removing stopwords found following solution similar post inserting new line simply created new line every word started row like result looked like need result row like searching found solution
input a string and compare each word with a given word using NLP Python,"<p>I want to input a string, tokenize it, and compare each word with a specific word (in this code, the word is 'play'). I have the code</p>

<pre><code>from nltk.tokenize import word_tokenize

txt = ""bat ball cocaine golf football cake leg hand me you her she he dog cat drug""

x = word_tokenize(txt)
from nltk.corpus import wordnet 


for i in range (10):
    syn = wordnet.synsets(x[i])[0]
    print (""Synset name :  "", syn.name())

    w1 = wordnet.synset('play.n.01') 
    w2 = wordnet.synset(syn) 
    print(w1.wup_similarity(w2)) 
i = i +1
</code></pre>

<p>This gives an error:</p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-127-a30645977ba6&gt; in &lt;module&gt;()
     13 
     14     w1 = wordnet.synset('play.n.01')
---&gt; 15     w2 = wordnet.synset(syn) 
     16     print(w1.wup_similarity(w2))
     17 i = i +1
</code></pre>

<p>help</p>
",Preprocessing of the text & Tokenization,input string compare word given word using nlp python want input string tokenize compare word specific word code word play code give error help
Adding a full stop &#39;.&#39; after every sentence line while using spacy NLP to perform summarisation,"<p>I want to add a full stop after every line of sentence that I am getting from my cleaned text after performing text cleaning in order to perform summarisation using heapq or gensim. If I don't get a full stop, heapq or Gensim will not understand the different sentences and would take all the sentences as one. I am using the following code :</p>

<pre><code>import en_core_web_sm
nlp = en_core_web_sm.load()
text = nlp(str1_clean_summary)

for sent in text.sents:
  print(sent.string.strip())
</code></pre>

<p>str1_clean_summary looks like this :</p>

<pre><code>many price increase options 
still believe us need prove consistently 
aim please delay end displeasingich
responds wuickly
</code></pre>

<p>This gives me sentences in a different lines but I need to add a full stop after each sentence so they are treated separately. </p>
",Preprocessing of the text & Tokenization,adding full stop every sentence line using spacy nlp perform summarisation want add full stop every line sentence getting cleaned text performing text cleaning order perform summarisation using heapq gensim get full stop heapq gensim understand different sentence would take sentence one using following code str clean summary look like give sentence different line need add full stop sentence treated separately
Generalization of strings and state machine optimization,"<p>I am looking into learning state machines that accept given samplings of a regular language, you can allow or disallow noise in the sampling, lets assume no noise for simplicity.</p>

<p>Example:</p>

<p>given the strings ['fun', 'gun'] I can construct the regex 'fun|gun' but a shorter regex description is '[fg]un', I wish to find this systematically.</p>

<p>I've seen papers teaching the language to an RNN and then extracting a state machine from that RNN in various approximational weirding ways, which is kind of strange, cause I also seem to vaguely remember from my Computer Science days that the question of optimizing a state machine is a fairly well studied and solved problem. If the second is true, why not simply start with any state machine that accepts the language and then optimize it in terms of state or something?</p>

<p>Are there in short any <em>regular expression/state machine</em> optimization algorithm in python I can use in some obscure package cause I'm not managing to find either an algorithm or a package implementing it, just some vague CS lecture slides.</p>
",Preprocessing of the text & Tokenization,generalization string state machine optimization looking learning state machine accept given sampling regular language allow disallow noise sampling let assume noise simplicity example given string fun gun construct regex fun gun shorter regex description fg un wish find seen paper teaching language rnn extracting state machine rnn various approximational weirding way kind strange cause also seem vaguely remember computer science day question optimizing state machine fairly well studied solved problem second true simply start state machine accepts language optimize term state something short regular expression state machine optimization algorithm python use obscure package cause managing find either algorithm package implementing vague c lecture slide
Creating text summary using NLP,"<p>I am in middle of applying NLP to the set of comments that I have received from my data. These comments are stored in one column. I have cleaned them altogether stored them in a list. There are no stop words, special characters etc. Now I want to create a summary from this text. What could be the best method to do that? I have already failed myself with heapq, so I dont want any solution around that. </p>

<p>My clean text is stored in list named : <code>clean_text_summary</code> and it looks like this :</p>

<pre><code>clean_text_summary = ['You are so bad - I hate your product','I am going to deregister','You are frauds'....]
</code></pre>

<p>I need to get most common things that people have talked about as a summary. </p>
",Preprocessing of the text & Tokenization,creating text summary using nlp middle applying nlp set comment received data comment stored one column cleaned altogether stored list stop word special character etc want create summary text could best method already failed heapq dont want solution around clean text stored list named look like need get common thing people talked summary
How to identify words with the same meaning in order to reduce number of tags/categories/classes in a dataset,"<p>So here is an example of a column in my data-set:</p>

<pre><code>""industries"": [""Gaming"", ""fitness and wellness""]
</code></pre>

<p>The <code>industries</code> column has hundreds of different tags, some of which can have the same meaning, for example, some rows have: <code>""Gaming""</code> and some have <code>""video games""</code> and others <code>""Games &amp; consoles""</code>.</p>

<p>I'd like to ""lemmatize"" these tags so I could query the data and not worry about minute differences in the presentation (if they are basically the same).</p>

<p>What is the standard solution in this case?</p>
",Preprocessing of the text & Tokenization,identify word meaning order reduce number tag category class dataset example column data set column ha hundred different tag meaning example row others like lemmatize tag could query data worry minute difference presentation basically standard solution case
How to generate bigram/trigram corpus only,"<p>Is there a way for Gensim to generate strictly the bigrams, trigrams in a list of words? </p>

<p>I can successfully generate the unigrams, bigrams, trigrams but I would like to extract only the bigrams, trigrams. </p>

<p>For example, in the list below:</p>

<pre><code>words = [['the', 'mayor', 'of', 'new', 'york', 'was', 'there'],[""i"",""love"",""new"",""york""],[""new"",""york"",""is"",""great""]]
</code></pre>

<p>I use </p>

<pre><code>bigram = gensim.models.Phrases(words, min_count=1, threshold=1)
bigram_mod = gensim.models.phrases.Phraser(bigram)
words_bigram = [bigram_mod[doc] for doc in words]
</code></pre>

<p>This creates a list of unigrams and bigrams as follows:</p>

<pre><code>[['the', 'mayor', 'of', 'new_york', 'was', 'there'],
 ['i', 'love', 'new_york'],
 ['new_york', 'is', 'great']]
</code></pre>

<p>My question is, is there a way (other than regular expressions) to extract strictly the bigrams, so that in this example only ""new_york"" would be a result?</p>
",Preprocessing of the text & Tokenization,generate bigram trigram corpus way gensim generate strictly bigram trigram list word successfully generate unigrams bigram trigram would like extract bigram trigram example list use creates list unigrams bigram follows question way regular expression extract strictly bigram example new york would result
text classification using keras input dimensions,"<p>I used to do text classification on sklearn. However, I have to do it using keras, so I followed this tutrial to encode the labels and so on as I have little experience with NN: <a href=""https://www.opencodez.com/python/text-classification-using-keras.htm"" rel=""nofollow noreferrer"">https://www.opencodez.com/python/text-classification-using-keras.htm</a>.
However, I found out that I have to specify the input dimensions for the first layer.I am confused how can I decide a dimension of the text as an input? is it depending on the countvectroizer I did? as I preprocess the text as following:</p>

<pre><code>corpus=[]
for i in range(0,len(dataset)):
    review=re.sub('[^a-zA-Z]',' ',dataset['cleaned_text'][i])
    review.lower()
    review=review.split()
    ps=PorterStemmer()
    review=[ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
    review=' '.join(review)
    corpus.append(review)

#the bag of word
from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer(max_features=1500).fit(corpus)
X=cv.fit_transform(corpus).toarray()
y=dataset['sentiment']
</code></pre>

<p>Also, in the tutial he defined it based on the the vocab size. as far as I understand, I think the input dimensions should be the maximum number of tokens in the all sentence. right?
Also, I tried to get encoder.vocab_size as this tutrial did <a href=""https://www.tensorflow.org/tutorials/text/text_classification_rnn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/text_classification_rnn</a>. however, I got this error <code>AttributeError: 'LabelBinarizer' object has no attribute 'vocab_size'</code></p>
",Preprocessing of the text & Tokenization,text classification using kera input dimension used text classification sklearn however using kera followed tutrial encode label little experience nn however found specify input dimension first layer confused decide dimension text input depending countvectroizer preprocess text following also tutial defined based vocab size far understand think input dimension maximum number token sentence right also tried get encoder vocab size tutrial however got error
How can I make large additions to textstem&#39;s lexicon in R?,"<p>I have a large body of free-text survey comments that I'm attempting to analyze.  I used the textstem package to perform lemmatization, but after looking at the unique tokens it identified I'd like to make further adjustments.  For example, it identified ""abuses"", ""abused"", and ""abusing"" as the lemma ""abuse"" but it left ""abusive"" untouched...I'd like to change that to ""abuse"" as well.</p>

<p>I found <a href=""https://stackoverflow.com/questions/50401056/strange-lemmatization-result-in-r-textstem-package"">this</a> post which described how to add to the lexicon on a piecemeal basis such as</p>

<pre><code>lemmas &lt;- lexicon::hash_lemmas[token==""abusive"",lemma:=""abuse""]
lemmatize_strings(words, dictionary = lemmas)
</code></pre>

<p>but in my case I'll have a dataframe with several hundred token/lemma pairs.  How can I quickly add them all to lexicon::hash_lemmas?</p>
",Preprocessing of the text & Tokenization,make large addition textstem lexicon r large body free text survey comment attempting analyze used textstem package perform lemmatization looking unique token identified like make adjustment example identified abuse abused abusing lemma abuse left abusive untouched like change abuse well found href post described add lexicon piecemeal basis p case dataframe several hundred token lemma pair quickly add lexicon hash lemma
Tokenizing a huge quantity of text in python,"<p>I have a huge list of text files to tokenize. I have the following code which works for a small dataset.  I am having trouble using the same procedure with a huge dataset, however. I am giving the example of a small dataset as below.</p>

<pre><code>In [1]: text = [[""It works""], [""This is not good""]]

In [2]: tokens = [(A.lower().replace('.', '').split(' ') for A in L) for L in text]

In [3]: tokens
Out [3]: 
[&lt;generator object &lt;genexpr&gt; at 0x7f67c2a703c0&gt;,
&lt;generator object &lt;genexpr&gt; at 0x7f67c2a70320&gt;]

In [4]: list_tokens = [tokens[i].next() for i in range(len(tokens))]
In [5]: list_tokens
Out [5]:
[['it', 'works'], ['this', 'is', 'not', 'good']]
</code></pre>

<p>While all works so well with a small dataset, I encounter problem processing a huge list of lists of strings (more than 1,000,000 lists of strings) with the same code. As I still can tokenize the strings with the huge dataset as in <code>In [3]</code>, it fails in <code>In [4]</code> (i.e. killed in terminal).  I suspect it is just because the body of the text is too big.  </p>

<p>I am here, therefore, seek for suggestions on the improvement of the procedure to obtain lists of strings in a list as what I have in <code>In [5]</code>.  </p>

<p>My actual purpose, however, is to count the words in each list. For instance, in the example of the small dataset above, I will have things as below.</p>

<pre><code>[[0,0,1,0,0,1], [1, 1, 0, 1, 1, 0]] (note: each integer denotes the count of each word)
</code></pre>

<p>If I don't have to convert generators to lists to get the desired results (i.e. word counts), that would also be good. </p>

<p>Please let me know if my question is unclear. I would love to clarify as best as I can. Thank you.  </p>
",Preprocessing of the text & Tokenization,tokenizing huge quantity text python huge list text file tokenize following code work small dataset trouble using procedure huge dataset however giving example small dataset work well small dataset encounter problem processing huge list list string list string code still tokenize string huge dataset fails e killed terminal suspect body text big therefore seek suggestion improvement procedure obtain list string list actual purpose however count word list instance example small dataset thing convert generator list get desired result e word count would also good please let know question unclear would love clarify best thank
NLP reverse tokenizing (going from tokens to nicely formatted sentence),"<p>Python's Spacy package has a statistical tokenizer that intelligently splits a sentence into tokens. My question is, is there a package that allows me to go backwards, i.e. from list of tokens to a nicely formatted sentence? Essentially, I want a function that lets me do the following:</p>

<pre><code>&gt;&gt;&gt; toks = ['hello', ',', 'i', 'ca', ""n't"", 'feel', 'my', 'feet', '!']
&gt;&gt;&gt; some_function(toks)
""Hello, I can't feel my feet!""
</code></pre>

<p>It probably needs some sort of statistical/rules-based procedure to know how spacing, capitalization or contractions should work in a proper sentence.</p>
",Preprocessing of the text & Tokenization,nlp reverse tokenizing going token nicely formatted sentence python spacy package ha statistical tokenizer intelligently split sentence token question package allows go backwards e list token nicely formatted sentence essentially want function let following probably need sort statistical rule based procedure know spacing capitalization contraction work proper sentence
Regular Expression to extract string after seeing &quot;number + one letter + [comma or whitespace]&quot; in Bigquery,"<p>I am trying to extract:</p>

<p>Abbey Grove<br>
Abbey Grove<br>
Abbey Road View<br>
Abbey Road<br>
Abbey Terrace<br>
Abbey Wood Road<br>
Abbey Grove<br></p>

<p>from</p>

<pre><code>23a, Abbey Grove
43a Abbey Grove
Block 509a Abbey Road View
511 Abbey Road
Flat 8a, Abbey Terrace
14 Abbey Wood Road
100 Abbey Grove
</code></pre>

<p>in Google Bigquery. The issue is that: </p>

<pre><code>regexp_replace(text, '[^a-zA-Z]', '')
</code></pre>

<p>gives me ""aabbeywood"" with two a's. Essentially I just want to keep all the text after a ""numeric"" or ""numeric plus one letter"" string.</p>
",Preprocessing of the text & Tokenization,regular expression extract string seeing number one letter comma whitespace bigquery trying extract abbey grove abbey grove abbey road view abbey road abbey terrace abbey wood road abbey grove google bigquery issue give aabbeywood two essentially want keep text numeric numeric plus one letter string
Regular Expression for Number+Letter combinations in Bigquery,"<p>I am attempting to extract 2A, 6B, 8, 9A, 33, 65c, 84 from the sequence below.</p>

<p>Logically this seems to require any number [0-9] attached to a single letter before either a comma or whitespace.</p>

<pre><code>2A, Beech Avenue 
Flat 6B Earl's Court Drive 
8 Ripley Drive 
9A, Grosvenor Park Way, Thirsk 
33, Dover Priory 
Block 65c,Earl's Avenue 
84, Wayfair Drive 
</code></pre>

<p>This is as far as I have got with code that works to extract numbers but single letters and spaces are proving extremely challenging: </p>

<pre><code>REGEXP_REPLACE('[^0-9]', '') FROM list
</code></pre>
",Preprocessing of the text & Tokenization,regular expression number letter combination bigquery attempting extract b c sequence logically seems require number attached single letter either comma whitespace far got code work extract number single letter space proving extremely challenging
Data cleaning for dutch text - Sentiment analysis,"<p>I am looking for few best practices to clean up the Dutch text.
What I have done so far:
1. Used regex to remove all special characters, digits etc.
2. Spacy _ NL model for lemmetization of the words
3. NLTK stopwords for dutch
4. collecting adjectives for sentiment. </p>

<p>Feature vector - Count Vector</p>

<p>But the text is not getting cleaned as expected. There is no clean line for positive and negative. </p>

<p>I am looking for some guidance or solution to solve NLP problems in Dutch.</p>
",Preprocessing of the text & Tokenization,data cleaning dutch text sentiment analysis looking best practice clean dutch text done far used regex remove special character digit etc spacy nl model lemmetization word nltk stopwords dutch collecting adjective sentiment feature vector count vector text getting cleaned expected clean line positive negative looking guidance solution solve nlp problem dutch
find trigram using NLTK,"<p>I am not very familiar with <a href=""http://nltk.org/"" rel=""nofollow"">NLTk</a> and python, and I have to do the following tasks in a program:</p>

<ol>
<li><strong>Tokenize</strong> and lowercase the input <em>text1</em></li>
<li><strong>Tokenize</strong> the input <em>text2</em></li>
<li>Find all <strong>trigrams</strong> in the input <em>text1</em></li>
</ol>

<p>Can anyone help me? </p>
",Preprocessing of the text & Tokenization,find trigram using nltk familiar nltk python following task program tokenize lowercase input text tokenize input text find trigram input text anyone help
How to cluster customers with the model (Customer -&gt; Item list -&gt; Word list in items) with an unsupervised algorithm,"<p>Need advice on a clustering model. There is a list of customers -> customers have a list of products -> Each product contains several words.
I want to cluster clients into several groups by type of activity - that is, by general topics.</p>

<p>How would you introduce such a model for clustering into vectors, for example for K-means?</p>

<p>My hypothesis is so far - turn every word into a fasttext vector, select the top 100 words for example on TF-IDF and add * 100 (the size of the fasttext vector) by 100 words, and this will turn out 10,000 columns. Maybe something more economical in computing?</p>
",Preprocessing of the text & Tokenization,cluster customer model customer item list word list item unsupervised algorithm need advice clustering model list customer customer list product product contains several word want cluster client several group type activity general topic would introduce model clustering vector example k mean hypothesis far turn every word fasttext vector select top word example tf idf add size fasttext vector word turn column maybe something economical computing
Torch use of Spacy -- Strange Errors w/ Standard Dataset,"<p>In the following code:</p>

<pre><code>from torchtext.datasets import Multi30k
from torchtext.data import Field, BucketIterator
import spacy
from spacy.tokenizer import Tokenizer

nlp = spacy.load(""en_core_web_sm"")
tokenizer = Tokenizer(nlp.vocab)


SRC = Field(tokenize = tokenizer,
            init_token = '&lt;sos&gt;',
            eos_token = '&lt;eos&gt;',
            lower = True)

TRG = SRC

train_data, valid_data, test_data = Multi30k.splits(exts = ('.en', '.en'), fields = (SRC, TRG), root='.')
</code></pre>

<p>Running this code produces the following error:</p>

<pre><code>descriptor 'lower' requires a 'str' object but received a 'spacy.tokens.doc.Doc
</code></pre>

<p>Using a debugger, I find that the error occurs at the following line (in classmethod preprocess(x) of class Field(RawField) of the Torch file 'pipeline.py'</p>

<pre><code>if self.lower:
   x = Pipeline(six.text_type.lower)(x)
</code></pre>

<p>where indeed the arg 'x' is of type spacy.tokens.doc.Doc. But, to my surprise, when I move up the stack one step I find that the line which calls preprocess(x) is this --</p>

<pre><code>setattr(ex, name, field.preprocess(val))
</code></pre>

<p>but in this case the type of 'val' is 'str', as required for use with 'lower' (!!??)</p>

<p>My code is a completely standard use case, so what am I missing??</p>

<p>If instead I use 'lower'=False, the code runs without error, and the Train/Test/Val data produced are correct (the train_data produces 29,000 pairs). However, if I then run --</p>

<pre><code>SRC.build_vocab(train_data, min_freq = 2)
</code></pre>

<p>I get a vocab with only 4 items, namely:</p>

<pre><code>Vocab stoi:
 [('&lt;unk&gt;', 0), ('&lt;pad&gt;', 1), ('&lt;sos&gt;', 2), ('&lt;eos&gt;', 3)]
</code></pre>

<p>So something is wrong here as well.</p>

<p>Finally, if I use tokenize='spacy' in the Field constructor, instead of passing a reference to a tokenizer function -- as I do in the code above, all of these problems disappear, except that I can only do this in a collab notebook. If I try to run with tokenize='spacy' on my machine, I get this error (vscode in Windows 10):</p>

<pre><code>""OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.""
</code></pre>

<p>But clearly the spacy model has been downloaded since I was able to pass the corresponding tokenizer function to the same keyword arg (I checked this also with the debugger -- the tokenizer gets passed in). </p>

<p>I'd really like to understand what's going on . . .</p>

<p>(I've set SRC=TRG to simplify, but I get identical behavior if I use a language pair, e.g. SRC=en, TRG=de)</p>
",Preprocessing of the text & Tokenization,torch use spacy strange error w standard dataset following code running code produce following error using debugger find error occurs following line classmethod preprocess x class field rawfield torch file pipeline py indeed arg x type spacy token doc doc surprise move stack one step find line call preprocess x case type val str required use lower code completely standard use case missing instead use lower false code run without error train test val data produced correct train data produce pair however run get vocab item namely something wrong well finally use tokenize spacy field constructor instead passing reference tokenizer function code problem disappear except collab notebook try run tokenize spacy machine get error vscode window clearly spacy model ha downloaded since wa able pas corresponding tokenizer function keyword arg checked also debugger tokenizer get passed really like understand going set src trg simplify get identical behavior use language pair e g src en trg de
python nltk loop printing header instead of the value,"<p>I have tokenized sentences in a csv file but when I'm trying to remove the stop words within the for loop it stops printing the words and it prints the column header for all sentences any idea where is the error in the last line ?</p>

<pre><code>for review in tokenized_docs:
    new_review = []
    for token in review:
        new_token = x.sub(u'', token)
        if not new_token == u'':
            new_review.append(new_token)
    tokenized_docs_no_punctuation.append(new_review)
    words=pd.DataFrame(tokenized_docs_no_punctuation)
    #print(words)
    print([word for word in words if word not in stops])
</code></pre>

<p>the output shows like this </p>

<p><a href=""https://i.sstatic.net/OIFn8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OIFn8.png"" alt=""on""></a></p>

<p>which should be the words instead of the column header numbers.</p>
",Preprocessing of the text & Tokenization,python nltk loop printing header instead value tokenized sentence csv file trying remove stop word within loop stop printing word print column header sentence idea error last line output show like word instead column header number
I am looking for a dutch language tokenizer for technical product review,"<p>I am trying to find out the better text cleaning method for Dutch NLP problem. I have used dutch version for pos tags and nltk for removal of stop words. But I am not getting desired results.</p>
",Preprocessing of the text & Tokenization,looking dutch language tokenizer technical product review trying find better text cleaning method dutch nlp problem used dutch version po tag nltk removal stop word getting desired result
Is there a way in the text_to_word_sequence method in Keras to also filter out stopwords using the &#39;filters&#39; parameter?,"<p>I've reviewed the official documentation of the <code>text_to_word_sequence</code> method in Keras <a href=""https://keras.io/preprocessing/text/#text_to_word_sequence"" rel=""nofollow noreferrer"">here</a></p>

<p>The code listed in the documentation is:</p>

<pre><code>keras.preprocessing.text.text_to_word_sequence(text, filters='!""#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n', lower=True, split=' ')
</code></pre>

<p>I was wondering if there is a way to add something in the <code>filters</code> parameter that would also remove stopwords (such as from the <code>nltk</code> list of stopwords) i.e.</p>

<pre><code>from nltk.corpus import stopwords
stopwords.words('English')
</code></pre>

<p>I am aware that we can also remove stopwords via regular expressions (using a <code>_sre.SRE_Pattern</code>)  as below:</p>

<pre><code>import re
pattern = re.compile(r'\b(' + r'|'.join(stopwords.words('English')) + r')\b\s*')
phrase = pattern.sub('', phrase)
</code></pre>

<p>My minimum verifiable example is:</p>

<pre><code>from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence
text_to_word_sequence(""The cat is in the hat!!!"")

Output: ['the', 'cat', 'is', 'in', 'the', 'hat']
</code></pre>

<p>I would like the output to be :</p>

<pre><code>['cat', 'hat']
</code></pre>

<p><strong>My question is this:</strong></p>

<p>Is there a way using the <code>filters</code> parameter in the <code>text_to_word_sequence</code> method to automatically filter out stopwords along with the special characters that it filters out by default? Such as by using a pattern (<code>_sre.SRE_Pattern</code>) etc?</p>
",Preprocessing of the text & Tokenization,way text word sequence method kera also filter stopwords using filter parameter reviewed official documentation method kera code listed documentation wa wondering way add something parameter would also remove stopwords list stopwords e aware also remove stopwords via regular expression using minimum verifiable example would like output question way using parameter method automatically filter stopwords along special character filter default using pattern etc
"Improve spacy lemmatization with bigrams, proper nouns, and plurals?","<p>In python, I'm building ngrams with gensim and passing the words into spacy for lemmatization. I'm finding that spacy is not working very well as it's keeping many words as plurals that shouldn't be.</p>

<p>It looks like this is mostly happening when it's mistakenly tagging nouns as proper nouns.</p>

<pre><code>import spacy
nlp = spacy.load('en', disable=['parser','ner'])

doc = nlp(u""bed_bugs bed bug beds bedbug bugs bed_bug nymph nymphs nintendo"")

for token in doc:
    print(""original: {}, Lemma: {}, POS: {}"".format(token, token.lemma_, token.pos_))
</code></pre>

<p>output:</p>

<pre><code>original: bed_bugs, Lemma: bed_bugs, POS: PROPN
original: bed, Lemma: bed, POS: NOUN
original: bug, Lemma: bug, POS: NOUN
original: beds, Lemma: bed, POS: VERB
original: bedbug, Lemma: bedbug, POS: PROPN
original: bugs, Lemma: bugs, POS: PROPN
original: bed_bug, Lemma: bed_bug, POS: X
original: nymph, Lemma: nymph, POS: PROPN
original: nymphs, Lemma: nymphs, POS: PROPN
original: nintendo, Lemma: nintendo, POS: PROPN
</code></pre>

<p>My preferred output would have these changes -</p>

<pre><code>bed_bugs -&gt; bed_bug
nymphs -&gt; nymph
bugs -&gt; bug
</code></pre>

<p>Is there a way to accomplish this with spacy or some other tool?</p>
",Preprocessing of the text & Tokenization,improve spacy lemmatization bigram proper noun plural python building ngrams gensim passing word spacy lemmatization finding spacy working well keeping many word plural look like mostly happening mistakenly tagging noun proper noun output preferred output would change way accomplish spacy tool
UnicodeDecodeError when cleaning text data,"<p>I am trying to clean some text. I am keeping alphabets and numbers only. However, my text still contains other characters. </p>

<p>This is my function:</p>

<pre><code>def review_to_wordlist(review, remove_stopwords=False, remove_numbers = False ):
# Function to convert a document to a sequence of words,
# optionally removing stop words and numbers.  Returns a list of words.
#
# 1. Remove HTML
review_text = BeautifulSoup(review).get_text()
#
# 2. Remove non-letters
if True:
    review_text = re.sub(""[^a-zA-Z0-9]"","" "", review_text)
#
# 3. Convert words to lower case and split them
words = review_text.lower().split()
#
# 4. Optionally remove stop words (false by default)
if remove_stopwords:
    stops = set(stopwords.words(""english""))
    words = [w for w in words if not w in stops]
#
# 5. Return a list of words
return(words)
</code></pre>

<p>and this is one result that I get:</p>

<blockquote>
  <p>NuTone Central Vacuum System 45� Ell Ohio Steel Tandem Natural and
  Synthetic Turf Sweeping System Unique Home Designs 36 in. x 80 in. Su
  Casa Black Surface Mount Outswing Steel Security Door with Expanded
  Metal Screen Unique Home Designs 36 in. x 80 in. Su Casa Black Surface
  Mount Outswing Steel Security Door with Expanded Metal Screen Unique
  Home Designs 36 in. x 80 in. Su Casa Black Surface Mount Outswing
  Steel Security Door with Expanded Metal Screen MP Global Best 400 in.
  x 36 in. x 1/8 in. Acoustical Recycled Fiber Underlayment with Film
  for Laminate Wood MP Global Best 400 in. x 36 in. x 1/8 in. Acoustical
  Recycled Fiber Underlayment with Film for Laminate Wood Grip-Rite
  #10-1/4 in. x 2-1/2 in. 8� Bright Steel Ring-Shank Common Nails (1 lb.-Pack)</p>
</blockquote>

<p>the error that I get is:</p>

<pre><code>UnicodeDecodeError: 'utf8' codec can't decode bytes in position 5-6: unexpected end of data


676
Husky Pneumatic 3-1/2 in. 21� Full-Head Strip Framing Nailer
5157
RIDGID 3-1/2 in. 21� Round-Head Nailer
5158
RIDGID 3-1/2 in. 21� Round-Head Nailer
</code></pre>
",Preprocessing of the text & Tokenization,unicodedecodeerror cleaning text data trying clean text keeping alphabet number however text still contains character function one result get nutone central vacuum system ell ohio steel tandem natural synthetic turf sweeping system unique home design x su casa black surface mount outswing steel security door expanded metal screen unique home design x su casa black surface mount outswing steel security door expanded metal screen unique home design x su casa black surface mount outswing steel security door expanded metal screen mp global best x x acoustical recycled fiber underlayment film laminate wood mp global best x x acoustical recycled fiber underlayment film laminate wood grip rite x bright steel ring shank common nail lb pack error get
How to create multiple coropra in R,"<p>I am trying to create multiple corpora for textanalysis in R. </p>

<p>I started by loading multiple txt files into the working directory.</p>

<pre><code>names &lt;- gsub("".txt"", """", filenames)
for (i in 1:length(filenames)){ 
d &lt;- readtext(filenames[i], encoding = ""UTF-8"") 
assign(names[i], d) } 
</code></pre>

<p>Now my goal is to create a corpus for each individual file, without having to do it for each file individually. </p>

<p>I tried two versions:</p>

<pre><code>all_txts &lt;- list.files(pattern = "".txt$"")
lapply(all_txts, function(x){
  c &lt;- corpus()})
</code></pre>

<p>and</p>

<pre><code>for ( i in 1: length(ls(pattern ="".txt""))){
  e &lt;- corpus(i)
  assingn(filenames[i], ""_corpus"")}
</code></pre>

<p>in both cases I got the error message:</p>

<pre><code>Error in corpus.default() : corpus() only works on character, corpus, Corpus, data.frame, kwic objects.
</code></pre>

<p>My end goal is to tokenize the corpora and get an Excel-Sheet for every txt file which is split up by sentence.</p>

<p>Thanks for your help in andvance</p>
",Preprocessing of the text & Tokenization,create multiple coropra r trying create multiple corpus textanalysis r started loading multiple txt file working directory goal create corpus individual file without file individually tried two version case got error message end goal tokenize corpus get excel sheet every txt file split sentence thanks help andvance
gensim lemmatize error generator raised StopIteration,"<p>I'm trying to execute simple code to lemmatize string, but there's an error about iteration.
I have found some solutions which are about reinstalling web.py, but this not worked for me.</p>

<p>python code</p>

<pre><code>from gensim.utils import lemmatize
lemmatize(""gone"")
</code></pre>

<p>error is</p>

<pre><code>---------------------------------------------------------------------------
StopIteration                             Traceback (most recent call last)
I:\Anaconda\lib\site-packages\pattern\text\__init__.py in _read(path, encoding, comment)
    608             yield line
--&gt; 609     raise StopIteration
    610 

StopIteration: 

The above exception was the direct cause of the following exception:

RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-4-9daceee1900f&gt; in &lt;module&gt;
      1 from gensim.utils import lemmatize
----&gt; 2 lemmatize(""gone"")

-------------------------------------------------------------------------------------

I:\Anaconda\lib\site-packages\pattern\text\__init__.py in &lt;genexpr&gt;(.0)
    623     def load(self):
    624         # Arnold NNP x
--&gt; 625         dict.update(self, (x.split("" "")[:2] for x in _read(self._path) if len(x.split("" "")) &gt; 1))
    626 
    627 #--- FREQUENCY -------------------------------------------------------------------------------------

RuntimeError: generator raised StopIteration
</code></pre>
",Preprocessing of the text & Tokenization,gensim lemmatize error generator raised stopiteration trying execute simple code lemmatize string error iteration found solution reinstalling web py worked python code error
How to exclude certain names and terms from stemming (Python NLTK SnowballStemmer (Porter2)),"<p>I am newly getting into NLP, Python, and posting on Stackoverflow at the same time, so please be patient with me if I might seem ignorant :). </p>

<p>I am using SnowballStemmer in Python's NLTK in order to stem words for textual analysis. While lemmatization seems to understem my tokens, the snowball porter2 stemmer, which I read is mostly preferred to the basic porter stemmer, <em>overstems</em> my tokens. I am analyzing tweets including many names and probably also places and other words which should not be stemmed, like: hillary, hannity, president, which are now reduced to hillari, hanniti, and presid (you probably guessed already whose tweets I am analyzing). </p>

<p>Is there an easy way to exclude certain terms from stemming? Conversely, I could also merely lemmatize tokens and include a rule for common suffixes like -ed, -s, …. Another idea might be to merely stem verbs and adjectives as well as nouns ending in s. That might also be close enough… </p>

<p>I am using below code as of now: </p>

<pre><code># LEMMATIZE AND STEM WORDS

from nltk.stem.snowball import EnglishStemmer

lemmatizer = nltk.stem.WordNetLemmatizer()
snowball = EnglishStemmer() 

def lemmatize_text(text):

    return [lemmatizer.lemmatize(w) for w in text]

def snowball_stemmer(text):

    return [snowball.stem(w) for w in text]

# APPLY FUNCTIONS

tweets['text_snowball'] = tweets.text_processed.apply(snowball_stemmer)
tweets['text_lemma'] = tweets.text_processed.apply(lemmatize_text)
</code></pre>

<p>I hope someone can help… Contrary to my past experience with all kinds of issues, I have not been able to find adequate help for my issue online so far. </p>

<p>Thanks!</p>
",Preprocessing of the text & Tokenization,exclude certain name term stemming python nltk snowballstemmer porter newly getting nlp python posting stackoverflow time please patient might seem ignorant using snowballstemmer python nltk order stem word textual analysis lemmatization seems understem token snowball porter stemmer read mostly preferred basic porter stemmer overstems token analyzing tweet including many name probably also place word stemmed like hillary hannity president reduced hillari hanniti presid probably guessed already whose tweet analyzing easy way exclude certain term stemming could also merely lemmatize token include rule common suffix like ed another idea might merely stem verb adjective well noun ending might also close enough using code hope someone help contrary past experience kind issue able find adequate help issue online far thanks
Trying to tokenize special case sentences in python with nltk,"<p>I have a Python script that uses NLTK to split text into sentences. The problem I am having is two special case scenarios and I'm not sure if I can resolve it with this Tool. First, occasionally there are characters between sentences. For instance:</p>

<pre><code>    This is the first sentence. // This is the second sentence.
</code></pre>

<p>If I tokenize with <code>sent_tokenize(text)</code> I get <code>This is the first sentence.</code> and <code>// This is the second sentence.</code> The second sentence should be <code>This is the second sentence.</code> I could just strip out the slashes but I'm looking for a cleaner way already provided by the toolkit. Perhaps specifying sentences must start with a capital, or any letter. I don't know if there are any parameters I can specify when tokenizing.</p>

<p>The toolkit might not be the best tool for my next issue but occasionally a sentence will have a main title of sorts. For example <code>Words in a Title: This is my story.</code> when tokenized should be <code>This is my story.</code> I'll probably just have to solve that some other way, get rid of everything left of the colon if there is one, or something like that.</p>
",Preprocessing of the text & Tokenization,trying tokenize special case sentence python nltk python script us nltk split text sentence problem two special case scenario sure resolve tool first occasionally character sentence instance tokenize get second sentence could strip slash looking cleaner way already provided toolkit perhaps specifying sentence must start capital letter know parameter specify tokenizing toolkit might best tool next issue occasionally sentence main title sort example tokenized probably solve way get rid everything left colon one something like
How to obtain the word list from pyspark word2vec model?,"<p>I am trying to generate word vectors using PySpark. Using gensim I can see the words and the closest words as below:</p>



<pre class=""lang-python prettyprint-override""><code>sentences = open(os.getcwd() + ""/tweets.txt"").read().splitlines()
w2v_input=[]
for i in sentences:
    tokenised=i.split()
    w2v_input.append(tokenised)
model = word2vec.Word2Vec(w2v_input)
for key in model.wv.vocab.keys():
    print key
    print model.most_similar(positive=[key])
</code></pre>

<p>Using PySpark</p>

<pre class=""lang-python prettyprint-override""><code>inp = sc.textFile(""tweet.txt"").map(lambda row: row.split("" ""))
word2vec = Word2Vec()
model = word2vec.fit(inp)
</code></pre>

<p>How can I generate the words from the vector space in model? That is the pyspark equivalent of the gensim <code>model.wv.vocab.keys()</code>?</p>

<p>Background: I need to store the words and the synonyms from the model in a map so I can use them later for finding the sentiment of a tweet. I cannot reuse the word-vector model in the map functions in pyspark as the model belongs to the spark context (error pasted below). I want the pyspark word2vec version instead of gensim because it provides better synonyms for certain test words.</p>

<pre class=""lang-python prettyprint-override""><code> Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation.SparkContext can only be used on the driver, not in code that it run on workers.
</code></pre>

<p>Any alternative solution is also welcome.</p>
",Preprocessing of the text & Tokenization,obtain word list pyspark word vec model trying generate word vector using pyspark using gensim see word closest word using pyspark generate word vector space model pyspark equivalent gensim background need store word synonym model map use later finding sentiment tweet reuse word vector model map function pyspark model belongs spark context error pasted want pyspark word vec version instead gensim provides better synonym certain test word alternative solution also welcome
Regular expression &lt;.*?&gt;,"<p>I'm new to Python, and have been looking at text cleaning examples such as <a href=""https://www.kaggle.com/shashanksai/text-preprocessing-using-python/data"" rel=""nofollow noreferrer"">this</a> on Kaggle, and had a few questions on the stemming and stopwords part. </p>

<pre><code>for sentence in final_X:
  sentence = sentence.lower()                 # Converting to lowercase
  cleanr = re.compile('&lt;.*?&gt;')
  sentence = re.sub(cleanr, ' ', sentence)        #Removing HTML tags
  sentence = re.sub(r'[?|!|\'|""|#]',r'',sentence)
  sentence = re.sub(r'[.|,|)|(|\|/]',r' ',sentence)        #Removing Punctuations
</code></pre>

<p>What does <code>cleanr = re.compile('&lt;.*?&gt;')</code> mean? I understand that it is used to remove HTML tags, but I'm confused as to what <code>&lt;.*?&gt;</code> means. </p>
",Preprocessing of the text & Tokenization,regular expression new python looking text cleaning example kaggle question stemming stopwords part doe mean understand used remove html tag confused mean
Handling \u200b (Zero width space) character in text preprocessing for NLP task,"<p>I'm preprocessing some text for a NER model I'm training, and I'm encountering this character quite a lot. This character is not removed with <code>strip()</code>:</p>

<pre><code>&gt;&gt;&gt; 'Hello world!\u200b'.strip()
'Hello world!\u200b'
</code></pre>

<p>It is not considered a whitespace for regular expressions:</p>

<pre><code>&gt;&gt;&gt; re.sub('\s+', ' ', ""hello\u200bworld!"")
'hello\u200bworld!'
</code></pre>

<p>and spaCy's tokenizer does not split tokens upon it:</p>

<pre><code>&gt;&gt;&gt; [t.text for t in nlp(""hello\u200bworld!"")]
['hello\u200bworld', '!']
</code></pre>

<p>So, how should I handle it? I can simply replace it, however I don't want to make a special case for this character, but rather replace all characters with similar characteristics.</p>

<p>Thanks.</p>
",Preprocessing of the text & Tokenization,handling u b zero width space character text preprocessing nlp task preprocessing text ner model training encountering character quite lot character removed considered whitespace regular expression spacy tokenizer doe split token upon handle simply replace however want make special case character rather replace character similar characteristic thanks
Annotating the vocabulary using Word2vec model,"<p>I am trying to annotate the vocabulary in the corpus.</p>

<ol>
<li><p>I have trained the word2vec model on the corpus</p></li>
<li><p>I have grouped the words which are related based on the score as key as the first word as the key and remaining words as a list of 2-tuple of word and scores with respect to the key </p></li>
</ol>

<p>example:
'coffee'---key 
values are </p>

<pre><code>[('tea', 0.8139282),
 ('latte', 0.76456803),
 ('coffe', 0.7607962),
 ('lattes', 0.756057),
 ('starbucks', 0.7158153),
 ('espresso', 0.71386236),
 ('mocha', 0.69999266),
 ('coffees', 0.6816252),
 ('frappucino', 0.67192864),
 ('cuppa', 0.66720986),
 ('cappucino', 0.6664002),
 ('chai', 0.6623157),
 ('decaf', 0.65980726),
 ('frappuccino', 0.65150374),
 ('venti', 0.6486204),
 ('expresso', 0.6369579),
 ('macchiato', 0.6280453),
 ('scone', 0.62476856),
 ('sippy', 0.6236704),
 ('cappuccino', 0.61718297),
 ('iced', 0.6130485),
 ('hazelnut', 0.6023698),
 ('mug', 0.6004759),
'
'
'
'
'
</code></pre>

<p>as i  know the coffee is releated to latte ,green_tea ,espresso,starbucks.. from the above data
I would like to label each word as below </p>

<p>latte [COHYPO] green_tea [COHYPO] espresso [HYPO] Starbucks [RELATED] tim_horton [RELATED] </p>

<p>COHYPO-<a href=""https://en.wiktionary.org/wiki/cohyponym"" rel=""nofollow noreferrer"">https://en.wiktionary.org/wiki/cohyponym</a></p>

<p>[HYPO] -<a href=""https://en.wiktionary.org/wiki/hyponyme"" rel=""nofollow noreferrer"">https://en.wiktionary.org/wiki/hyponyme</a></p>

<p>[RELATED] -the word is repeated </p>

<p>[MORPHO]-Morphological variant (example :Computer and computers )</p>

<p>[Partof]- indicates that the annotated word is a part of the word of interest</p>

<p>Any suggestion or ideas by which I can approach this problem </p>
",Preprocessing of the text & Tokenization,annotating vocabulary using word vec model trying annotate vocabulary corpus trained word vec model corpus grouped word related based score key first word key remaining word list tuple word score respect key example coffee key value know coffee releated latte green tea espresso starbucks data would like label word latte cohypo green tea cohypo espresso hypo starbucks related tim horton related cohypo hypo related word repeated morpho morphological variant example computer computer partof indicates annotated word part word interest suggestion idea approach problem
TfIdfVectorizer not tokenizing properly,"<p>As far as I'm concerned, there is no question like this. I'm working on a NLP and sentiment analysis project in Kaggle and first of all I'm preparing my data. 
The dataframe is a text column followed by a number from 0 to 9 which categorizes which cluster does the row (the document) belongs.
I'm using TF-IDF Vectorizer in sklearn. I want to get rid of anything that's not an english language word, so I'm using the following:</p>

<pre><code>class LemmaTokenizer(object):
    def __init__(self):
        self.wnl = WordNetLemmatizer()
    def __call__(self, doc):
        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]

s_words = list(nltk.corpus.stopwords.words(""english""))

c = TfidfVectorizer(sublinear_tf=False,
                    stop_words=s_words,
                    token_pattern =r""(?ui)\\b\\w*[a-z]+\\w*\\b"",
                    tokenizer = LemmaTokenizer(),
                    analyzer = ""word"",
                    strip_accents = ""unicode"")

#a_df is the original dataframe
X = a_df['Text']
X_text = c.fit_transform(X)
</code></pre>

<p>which as far as I know, when calling <code>c.get_feature_names()</code> should return only the tokens which are proper words, without numbers or punctuation symbols.
I found the regex in a post in StackOverflow, but using a simpler one like <code>[a-zA-Z]+</code> will do exactly the same (this is, nothing).
When I call the feature names, I get stuff like</p>

<pre><code>[""''abalone"",
""#"",
""?"",
""$"",
""'"",
""'0"",
""'01"",
""'accidentally"",
...]
</code></pre>

<p>Those are just examples, but it's representative of the output I get, instead of just the words.
I've been stuck with this for days trying different regular expressions or methods to call. Even hardcoded some of the outputs for the features on the stop words.
I'm asking this because later I'm using <code>LDA</code> to get the topics of each cluster and get punctuation symbols as the ""topics"".
I hope I'm not duplicating another post. Anymore information I need to provide will do gladly. Thank you in advance!</p>
",Preprocessing of the text & Tokenization,tfidfvectorizer tokenizing properly far concerned question like working nlp sentiment analysis project kaggle first preparing data dataframe text column followed number categorizes cluster doe row document belongs using tf idf vectorizer sklearn want get rid anything english language word using following far know calling return token proper word without number punctuation symbol found regex post stackoverflow using simpler one like exactly nothing call feature name get stuff like example representative output get instead word stuck day trying different regular expression method call even hardcoded output feature stop word asking later using get topic cluster get punctuation symbol topic hope duplicating another post anymore information need provide gladly thank advance
How to handle labels when using the BERTs&#39; wordpiece tokenizer,"<p>I am trying to do multi-class sequence classification using the BERT uncased based model and tensorflow/keras. However, I have an issue when it comes to labeling my data following the BERT wordpiece tokenizer. I am unsure as to how I should modify my labels following the tokenization procedure.</p>

<p>I have read several open and closed issues on Github about this problem and I've also read the BERT paper published by Google. Specifically in section 4.3 of the paper there is an explanation of how to adjust the labels but I'm having trouble translating it to my case. I've also read the official BERT repository README which has a section on tokenization and mentions how to create a type of dictionary that maps the original tokens to the new tokens and that this can be used as a way to project my labels.</p>

<p>I have used the code provided in the README and managed to create labels in the way I think they should be. However, I am not sure if this is the correct way to do it. Below is an example of a tokenized sentence and it's labels before and after using the BERT tokenizer. Just a side-note. I have adjusted some of the code in the tokenizer so that it does not tokenize certain words based on punctuation as I would like them to remain whole.</p>

<p>This is the code to create the mapping:</p>

<pre><code>bert_tokens = []
label_to_token_mapping = []

bert_tokens.append(""[CLS]"")

for token in original_tokens:
   label_to_token_mapping.append(len(bert_tokens))
   bert_tokens.extend(tokenizer.tokenize(token, ignore_set=ignore_set))

bert_tokens.append(""[SEP]"")

</code></pre>

<pre><code>original_tokens = ['The', &lt;start&gt;', 'eng-30-01258617-a', '&lt;end&gt;', 'frailty']
tokens = ['[CLS]', 'the', '&lt;start&gt;', 'eng-30-01258617-a', '&lt;end&gt;', 'frail', '##ty', '[SEP]']
labels = [0,2, 3, 4, 1]
label_to_token_mapping = [1, 2, 3, 4, 5]
</code></pre>

<p>Using the mapping I adjust my label array and it becomes like the following:</p>

<pre><code>labels = [0, 2, 3, 4, 1, 1]
</code></pre>

<p>Following this I add padding labels (let's say that the maximum sequence length is 10) and so finally my label array looks like this:</p>

<pre><code>labels = [0, 2, 3, 4, 1, 1, 5, 5, 5, 5, 5]
</code></pre>

<p>As you can see since the last token (labeled 1) was split into two pieces I now label both word pieces as '1'. </p>

<p>I am not sure if this is correct. In section 4.3 of the paper they are labelled as 'X' but I'm not sure if this is what I should also do in my case. So in the paper (<a href=""https://arxiv.org/abs/1810.04805"" rel=""noreferrer"">https://arxiv.org/abs/1810.04805</a>) the following example is given: </p>

<pre><code>Jim    Hen    ##son  was  a puppet  ##eer
I-PER  I-PER    X     O   O   O       X
</code></pre>

<p>My final goal is to input a sentence into the model and as a result get back an array which can look something like [0, 0, 1, 1, 2, 3, 4, 5, 5, 5]. So one label per word piece. Then I can reconstruct the words back together to get the original length of the sentence and therefore the way the prediction values should actually look like.</p>

<p>Also, after training the model for a couple of epochs I attempt to make predictions and get weird values. For example a word is marked with the label '5' for padding and padding values get marked with the label '1'. This makes me think that there is something wrong with the way I create labels. Initially I did not adjust the labels so I would leave the labels as they were originally even after tokenizing the original sentence. This did not give me good results.</p>

<p>Any help would be greatly appreciated as I've been trying hard to find what I should do online but I haven't been able to figure it out yet. Thank you in advance!</p>

<p>Also, the following is the code I use to create my model:</p>

<pre><code>from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.keras.models import Model
from tensorflow.python.keras import backend as K
import tensorflow_hub as hub
import tensorflow as tf


class BertLayer(tf.layers.Layer):
    def __init__(self, bert_path, max_seq_length, n_fine_tune_layers=10, **kwargs):
        self.n_fine_tune_layers = n_fine_tune_layers
        self.trainable = True
        self.output_size = 768
        self.bert_path = bert_path
        self.max_seq_length = max_seq_length

        super(BertLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.bert = hub.Module(
            self.bert_path,
            trainable=self.trainable,
            name=""{}_module"".format(self.name)
        )
        trainable_vars = self.bert.variables

        # Remove unused layers
        # trainable_vars = [var for var in trainable_vars if not ""/cls/"" in var.name]
        trainable_vars = [var for var in trainable_vars
                          if not (""/cls/"" in var.name) and not (""/pooler/"" in var.name)]

        # Select how many layers to fine tune
        trainable_vars = trainable_vars[-self.n_fine_tune_layers:]

        # Add to trainable weights
        for var in trainable_vars:
            self._trainable_weights.append(var)

        for var in self.bert.variables:
            if var not in self._trainable_weights:
                self._non_trainable_weights.append(var)

        super(BertLayer, self).build(input_shape)

    def call(self, inputs):
        inputs = [K.cast(x, dtype=""int32"") for x in inputs]
        input_ids, input_mask, segment_ids = inputs
        bert_inputs = dict(
            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids
        )
        result = self.bert(inputs=bert_inputs, signature=""tokens"", as_dict=True)[
            ""sequence_output""
        ]
        return result

    def compute_output_shape(self, input_shape):
        return input_shape[0], self.output_size

    def get_config(self):
        config = {'bert_path': self.bert_path, 'max_seq_length': self.max_seq_length}
        base_config = super(BertLayer, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


# Build model
def build_model(bert_path, max_seq_length):
    in_id = Input(shape=(None,), name=""input_ids"")
    in_mask = Input(shape=(None,), name=""input_masks"")
    in_segment = Input(shape=(None,), name=""segment_ids"")
    bert_inputs = [in_id, in_mask, in_segment]

    bert_output = BertLayer(bert_path=bert_path, n_fine_tune_layers=3, max_seq_length=max_seq_length)(bert_inputs)
    dense = Dense(128, activation='relu')(bert_output)
    pred = Dense(8, activation='softmax',)(dense)

    model = Model(inputs=bert_inputs, outputs=pred)
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])
    model.summary()

    return model


def initialize_vars(sess):
    sess.run(tf.local_variables_initializer())
    sess.run(tf.global_variables_initializer())
    sess.run(tf.tables_initializer())
    K.set_session(sess)
</code></pre>
",Preprocessing of the text & Tokenization,handle label using berts wordpiece tokenizer trying multi class sequence classification using bert uncased based model tensorflow kera however issue come labeling data following bert wordpiece tokenizer unsure modify label following tokenization procedure read several open closed issue github problem also read bert paper published google specifically section paper explanation adjust label trouble translating case also read official bert repository readme ha section tokenization mention create type dictionary map original token new token used way project label used code provided readme managed create label way think however sure correct way example tokenized sentence label using bert tokenizer side note adjusted code tokenizer doe tokenize certain word based punctuation would like remain whole code create mapping using mapping adjust label array becomes like following following add padding label let say maximum sequence length finally label array look like see since last token labeled wa split two piece label word piece sure correct section paper labelled x sure also case paper following example given final goal input sentence model result get back array look something like one label per word piece reconstruct word back together get original length sentence therefore way prediction value actually look like also training model couple epoch attempt make prediction get weird value example word marked label padding padding value get marked label make think something wrong way create label initially adjust label would leave label originally even tokenizing original sentence give good result help would greatly appreciated trying hard find online able figure yet thank advance also following code use create model
Tokenize multiple sentences to rows in python pandas,"<p>I have a text dataframe like this,</p>

<pre><code>id      text
1       Thanks.  I appreciate your help.  I really like this chat service as it is very convenient.  I hope you have a wonderful day! thanks!
2       Got it. Thanks for the help; good nite.
</code></pre>

<p>I want to split those text sentences and match them to each id. My expected output is,</p>

<pre><code>id      text
1       Thanks.
1       I appreciate your help.
1       I really like this chat service as it is very convenient.
1       I hope you have a wonderful day!
1       thanks!
2       Got it.
2       Thanks for the help;
2       good nite.
</code></pre>

<p>Is there any nltk functions that can handle this problem?</p>
",Preprocessing of the text & Tokenization,tokenize multiple sentence row python panda text dataframe like want split text sentence match id expected output nltk function handle problem
applying regular expression keeping commas in R,"<p>I want to apply text cleaning with regular expressions on a dataset. However I want to keep commas because I will need to divide the text after cleaning it based on the commas (,).
the problem is that I am not extremely familiar with regex (I generally use quanteda and treat words separately as uni-grams, but in this case I can't because I need to treat the each X-gram as it is based on the commas.)</p>

<p>the dataset looks like this:</p>

<pre><code>   ID         Key
   1         ""Hello, dog_ food, This is it2, water""
   2         ""wow! nice, love, yes""
   3         ""1997""
   4           
   5         ""blabla, 34 l lol, @IceCream, #nice #wow d, seriously Not""
              ....
   .
   .
</code></pre>

<p>Among the things I want to do I want to get rid of words shorter than 2 letters, get rid of anything that is not alphanumeric stem words that are uni-grams.</p>

<p>I tried with these commands to obtain only low-key alphanumerics and get rid of words shorter than 2 letters but I end up cancelling the commas too, and I am not sure how to do to avoid it</p>

<pre><code>data$keys &lt;- to_lower(data$keys)
data$keys &lt;- str_replace_all(data$keys, ""[^[:alnum:]]"", "" "")
 data$keys &lt;- gsub("" *\\b[[:alpha:]]{1,2}\\b *"", "" "", data$keys) # Remove 1-2 letter words
 data$keys &lt;- gsub(""^ +| +$|( ) +"", ""\\1"", data$keys)
</code></pre>

<p>the expected output should be something like</p>

<pre><code>  ID         Key
   1         ""hello, dog food, this, water""
   2         ""wow nice, love, yes""
   3         ""1997""
   4           
   5         ""blabla, lol, icecream, nice wow, seriously not""
              ....
   .
   .
   .
</code></pre>

<p>so basically, everything lowercase, removing 2 letters words, removing any symbol that is not alphanumeric.</p>

<p>thank you very much in advance in advance for your help!</p>
",Preprocessing of the text & Tokenization,applying regular expression keeping comma r want apply text cleaning regular expression dataset however want keep comma need divide text cleaning based comma problem extremely familiar regex generally use quanteda treat word separately uni gram case need treat x gram based comma dataset look like among thing want want get rid word shorter letter get rid anything alphanumeric stem word uni gram tried command obtain low key alphanumerics get rid word shorter letter end cancelling comma sure avoid expected output something like basically everything lowercase removing letter word removing symbol alphanumeric thank much advance advance help
Sentence split using spacy sentenizer,"<p>I am using spaCy's sentencizer to split the sentences. </p>

<pre><code>from spacy.lang.en import English
nlp = English()
sbd = nlp.create_pipe('sentencizer')
nlp.add_pipe(sbd)

text=""Please read the analysis. (You'll be amazed.)""
doc = nlp(text)

sents_list = []
for sent in doc.sents:
   sents_list.append(sent.text)

print(sents_list)
print([token.text for token in doc])
</code></pre>

<p>OUTPUT</p>

<pre><code>['Please read the analysis. (', 
""You'll be amazed.)""]

['Please', 'read', 'the', 'analysis', '.', '(', 'You', ""'ll"", 'be', 
'amazed', '.', ')']
</code></pre>

<p>Tokenization is done correctly but I am not sure it's not splitting the 2nd sentence along with ( and taking this as an end in the first sentence.</p>
",Preprocessing of the text & Tokenization,sentence split using spacy sentenizer using spacy sentencizer split sentence output tokenization done correctly sure splitting nd sentence along taking end first sentence
"Getting negative score for model.docvecs.similarity_unseen_docs(document_1, document_2)","<p>I'm trying to find out the similarity between 2 documents i.e 'document_1' and 'document_2'.
I'm using <strong>Doc2Vec Gensim's <em>keyedvectors.py</em></strong> for finding similarity score.</p>

<pre><code>score = model.docvecs.similarity_unseen_docs(trainedModel, document_1, document_2)
print(score)
</code></pre>

<p>Where score is negative.</p>

<p>Here document_1 and document_2 are result of <em>NLTK's word_tokenize()</em></p>

<p><strong>What does Negative score mean when we try to find similarity between two ""<em>tokenized</em>"" documents?</strong></p>

<p><strong>P.S:</strong> Trained the model on 10 documents(2 Pages each)=20 Pages MS
word documents. </p>
",Preprocessing of the text & Tokenization,getting negative score model docvecs similarity unseen doc document document trying find similarity document e document document using doc vec gensim keyedvectors py finding similarity score score negative document document result nltk word tokenize doe negative score mean try find similarity two tokenized document p trained model document page page word document
Is there correct steps in preprocessing text for linear regression?,"<p>i've combined two different datasets so that one column has text and another column has the sentiment score (binary 0, 1) </p>

<p>I'm trying to make a linear regression model that predicts sentiment based on words used in the text,
so far to preprocess the text, i changed the text to lowercase for all texts.</p>

<p>i'm wondering what the next step is after this? i've read up a bit but i'm thinking i may not have the steps in the correct order.</p>

<pre><code>1. lowercase                         1. lowercase
2. remove punctuation               2. tokenize
3.tokenize                          3. remove punctuation
</code></pre>

<p>which way is more correct, if i remove the punctuation first i might lose details such as don't and can't. </p>
",Preprocessing of the text & Tokenization,correct step preprocessing text linear regression combined two different datasets one column ha text another column ha sentiment score binary trying make linear regression model predicts sentiment based word used text far preprocess text changed text lowercase text wondering next step read bit thinking may step correct order way correct remove punctuation first might lose detail
how to lemmatize string containing numbers and special characters?,"<p>I need to lemmatize strings containing numbers and alphanumeric characters. Example:</p>

<pre><code>'strawberries1234!@ apples123@'
</code></pre>

<p>I also need to preserve those special characters and numbers. So the output for the above example would be,</p>

<pre><code>'strawberry1234!@ apple123@'
</code></pre>

<p>I need to do this on a large dataset, so the code needs to be as efficient as possible.</p>

<p>Thanks.</p>
",Preprocessing of the text & Tokenization,lemmatize string containing number special character need lemmatize string containing number alphanumeric character example also need preserve special character number output example would need large dataset code need efficient possible thanks
Extracting Keywords using TF-IDF,"<p>I'm tackling the problem of Keyword Extraction using TF-IDF in an article .
The pipeline that I follow goes as follows :</p>

<ol>
<li>Input Text</li>
<li>Tokenize into sentences to build vocabulary</li>
<li>Apply CountVectorizer to build a count vector for each sentence .</li>
<li>Apply TfidfTransformer to assign weights for the same .</li>
</ol>

<p>However , the problem I'm facing with this is that the scores I'm receiving for each token is in context with the sentence and what I want is the score of the token in context to the whole article . So how do I go about achieving that ?</p>

<p>For eg :
This is my toy text .</p>

<blockquote>
  <p>""Rashid Siddiqui kept hearing those words from his fellow Muslim pilgrims lying mangled on the ground in 118-degree heat, under a searing Saudi sun. Barefoot, topless and dazed, Mr. Siddiqui had somehow escaped being crushed by the surging crowd.It was Sept. 24, 2015, the third morning of the hajj, the annual five-day pilgrimage to Mecca by millions of Muslims from around the world. By some estimates, it was the deadliest day in hajj history and one of the worst accidents in the world in decades. An American from Atlanta, Mr. Siddiqui, 42, had been walking through a sprawling valley of tens of thousands of pilgrim tents. His destination: Jamarat Bridge, where pilgrims throw pebbles at three large pillars in a ritual symbolizing the stoning of the devil. He was less than a mile from the bridge when the crush began.""</p>
</blockquote>

<p>And this is my weight matrix .</p>

<pre><code>[[ 0.24922681  0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.24922681  0.          0.
   0.          0.          0.24922681  0.24922681  0.          0.24922681
   0.24922681  0.          0.          0.24922681  0.          0.24922681
   0.24922681  0.          0.          0.          0.          0.
   0.24922681  0.          0.          0.          0.          0.20107462
   0.          0.24922681  0.          0.24922681  0.24922681  0.
   0.1669101   0.          0.          0.24922681  0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.24922681  0.          0.        ]

 [ 0.          0.22910137  0.22910137  0.          0.          0.
   0.22910137  0.          0.22910137  0.          0.          0.22910137
   0.          0.22910137  0.18483754  0.22910137  0.          0.          0.
   0.          0.          0.22910137  0.          0.          0.
   0.18483754  0.          0.          0.          0.          0.          0.
   0.          0.          0.22910137  0.          0.22910137  0.22910137
   0.18483754  0.          0.22910137  0.          0.          0.22910137
   0.          0.          0.          0.          0.          0.
   0.22910137  0.15343186  0.          0.          0.          0.22910137
   0.          0.          0.          0.          0.          0.22910137
   0.          0.          0.          0.18483754  0.        ]

 [ 0.          0.          0.          0.22910137  0.22910137  0.22910137
   0.          0.22910137  0.          0.          0.          0.          0.
   0.          0.18483754  0.          0.22910137  0.22910137  0.          0.
   0.          0.          0.22910137  0.          0.          0.18483754
   0.          0.          0.22910137  0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.18483754
   0.          0.          0.          0.22910137  0.          0.          0.
   0.          0.          0.          0.          0.          0.15343186
   0.22910137  0.          0.          0.          0.          0.22910137
   0.22910137  0.22910137  0.          0.          0.22910137  0.22910137
   0.          0.18483754  0.22910137]
</code></pre>

<p>Now what my question is that are these weights for the token with respect to the sentence or with respect to the whole article ? If it's with respect to the sentence , then how do i make it with respect to the whole article? </p>

<p>What I'm trying to achieve is a kind of unsupervised technique using tfidf for extracting keywords for a single article!! </p>
",Preprocessing of the text & Tokenization,extracting keywords using tf idf tackling problem keyword extraction using tf idf article pipeline follow go follows input text tokenize sentence build vocabulary apply countvectorizer build count vector sentence apply tfidftransformer assign weight however problem facing score receiving token context sentence want score token context whole article go achieving eg toy text rashid siddiqui kept hearing word fellow muslim pilgrim lying mangled ground degree heat searing saudi sun barefoot topless dazed mr siddiqui somehow escaped crushed surging crowd wa sept third morning hajj annual five day pilgrimage mecca million muslim around world estimate wa deadliest day hajj history one worst accident world decade american atlanta mr siddiqui walking sprawling valley ten thousand pilgrim tent destination jamarat bridge pilgrim throw pebble three large pillar ritual symbolizing stoning devil wa le mile bridge crush began weight matrix question weight token respect sentence respect whole article respect sentence make respect whole article trying achieve kind unsupervised technique using tfidf extracting keywords single article
getting verbal noun from noun,"<p>A verbal noun is a noun formed from or otherwise corresponding to a verb.  </p>

<p>I am looking to write an algorithm which when given a noun returns the corresponding verb (if the input noun is a verbal noun).<br>
My initial thought was to apply a stemmer to the noun, then search a verb list for a verb which has the same stem.<br>
Before doing this, I created a small test data set.<br>
It shows that sometimes this approach will not work:<br>
For example:<br>
'to explain' and 'explanation' do not have the same stem.<br>
'to decide' and 'decision' do not have the same stem.</p>

<pre><code>from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer('english')

l=[('to increase', 'increase'),
('to inhibit', 'inhibition'),
('to activate', 'activation'),
  ('to explain', 'explanation'),
  ('to correlate', 'correlation'),
  ('to decide', 'decision'),
   ('to insert', 'insertion')
  ]

for p in l:
    print(stemmer.stem(p[0]), ' &lt;-&gt; ', stemmer.stem(p[1]))

#to increas  &lt;-&gt;  increas
#to inhibit  &lt;-&gt;  inhibit
#to activ  &lt;-&gt;  activ
#to explain  &lt;-&gt;  explan
#to correl  &lt;-&gt;  correl
#to decid  &lt;-&gt;  decis
#to insert  &lt;-&gt;  insert

</code></pre>

<p>Does anyone know of a method which will work in cases of derivative nouns that do not have the same stem?</p>
",Preprocessing of the text & Tokenization,getting verbal noun noun verbal noun noun formed otherwise corresponding verb looking write algorithm given noun return corresponding verb input noun verbal noun initial thought wa apply stemmer noun search verb list verb ha stem created small test data set show sometimes approach work example explain explanation stem decide decision stem doe anyone know method work case derivative noun stem
Subsitute decimal point in floating point number with comma Python,"<p>For a given string, how can we substitute the decimal point in a floating point number with a comma?</p>

<p>For example: <code>12.33</code> should become <code>12,33</code></p>

<p>Please note that I can not simply replace all the <code>.</code> with <code>,</code> since the text also contains full stops to separate two sentences.</p>

<p>I need this for a text cleaning purpose.</p>
",Preprocessing of the text & Tokenization,subsitute decimal point floating point number comma python given string substitute decimal point floating point number comma example become please note simply replace since text also contains full stop separate two sentence need text cleaning purpose
Stream tokenizing with position indexes in Java,"<p>When tokenizing a stream of bytes in Java, I need to extract information about indexes of the tokenized words. For example, given:</p>

<p>String ""Hello world! Java8 program."" -> stream -> ...</p>

<p>I would need something like:</p>

<pre><code>[(Hello, 0, 5), (world, 6, 11), (Java8, 13, 18), (program, 19, 25)]
</code></pre>

<p>The problem is similar to <a href=""https://stackoverflow.com/questions/12398706/java-nlp-extracting-indicies-when-tokenizing-text"">Java NLP: Extracting Indicies When Tokenizing Text</a></p>

<p>The difference is the program input is stream with unknown length. I wanted to apply <a href=""https://opennlp.apache.org/docs/1.8.0/apidocs/opennlp-tools/opennlp/tools/tokenize/SimpleTokenizer.html"" rel=""nofollow noreferrer"">SimpleTokenizer</a> to this task but unfortunately it accepts only String as an input. Since stream could be very long I can't directly transform stream into String (which would force program to stop until stream would be closed). I can't process stream in chunks evaluated on the char buffer since I can break the words in that process. Can you help me with finding suitable solution? Current code can perform only tokenization without including indexes:</p>

<pre><code>public static void main(String[] args) throws IOException, ParseException {
    final Reader input = initializeInput();
    final Writer output = initializeOutput();
    try {
        final long count = process(input, output);
        output.flush();

    } finally {
        input.close();
        output.close();
    }
}

protected static long process(Reader input, Writer output) throws IOException {
    final StreamTokenizer st = new StreamTokenizer(input);
    st.eolIsSignificant(false);

    long count = 0;
    int token;
    while ((token = st.nextToken()) != StreamTokenizer.TT_EOF) {
        if (token == StreamTokenizer.TT_WORD) {
            final String word = st.sval;
            count++;
            output.write(word);
            output.flush();
        }
    }

    return count;
}

private static String asString(CharSequence stem) {
    if (stem == null)
        return ""-"";
    return stem.toString();
}

private static Writer initializeOutput()
        throws IOException, ParseException {
    final Writer output;

    output = new OutputStreamWriter(System.out, ""UTF-8"");

    return output;
}

private static Reader initializeInput()
        throws IOException {
    final Reader input;
    input = new InputStreamReader(System.in, ""UTF-8"");
    return input;
}
</code></pre>
",Preprocessing of the text & Tokenization,stream tokenizing position index java tokenizing stream byte java need extract information index tokenized word example given string hello world java program stream would need something like problem similar simpletokenizer task unfortunately accepts string input since stream could long directly transform stream string would force program stop stream would closed process stream chunk evaluated char buffer since break word process help finding suitable solution current code perform tokenization without including index
Extracting Country Name from Author Affiliations,"<p>I am currently exploring the possibility of extracting country name from Author Affiliations (PubMed Articles) my sample data looks like:</p>

<p><code>Mechanical and Production Engineering Department, National University of Singapore.</code></p>

<p><code>Cancer Research Campaign Mammalian Cell DNA Repair Group, Department of Zoology, Cambridge, U.K.</code></p>

<p><code>Cancer Research Campaign Mammalian Cell DNA Repair Group, Department of Zoology, Cambridge, UK.</code></p>

<p><code>Lilly Research Laboratories, Eli Lilly and Company, Indianapolis, IN 46285.</code></p>

<p>Initially I tried to remove punctuations and split the vector into words and then compared it with a list of country names from Wikipedia but I am not successful at this. </p>

<p>Can anyone please suggest me a better way of doing it? I would prefer the solution in <code>R</code> as I have to do further analysis and generate graphics in <code>R</code>.  </p>
",Preprocessing of the text & Tokenization,extracting country name author affiliation currently exploring possibility extracting country name author affiliation pubmed article sample data look like initially tried remove punctuation split vector word compared list country name wikipedia successful anyone please suggest better way would prefer solution analysis generate graphic
text preprocessing using string operations on tensors,"<p>I am trying to write a function that takes two string tensors from Keras input layers and do some string operations on them.</p>

<p>My first question is how to loop over each item in the tensors and split the first input (full sentence) using the second input (phrase of two words) and retrieve the indices of the split words.</p>

<pre><code>Sentence = ""this is an example to split on this dummy phrase and get an index""
Phrase = ""this dummy phrase""
output = [7, 9]
</code></pre>

<p>The function that I have (which  deals with strings)</p>

<pre><code>def get_phrase_inside_sentence_indices(sentence, phrase):
    split_sentence = sentence.split(phrase)
    split_phrase_len = len(phrase.split("" ""))

     first_part_of_sentence = split_sentence[0].strip().split("" "")
     second_index = split_phrase_len + len(first_part_of_sentence ) - 1     
     first_index = find_reverse_index(first_part_of_sentence, first_part_of_sentence [-1]) + 1
    return first_index, second_index

def find_reverse_index(given_list, value):
    return len(given_list) - given_list[-1::-1].index(value) - 1
</code></pre>

<p>How to convert this function to work on tensors or to loop over tensors and apply this function. I have a batch of sentences of (256,1) and a batch of corresponding phrases of (256,1) as my inputs.</p>

<p>My second question is how to convert the tensor to be a unicoded string:
The output from the layer is:</p>

<pre><code>tf.Tensor([[b'this is an example to split on this dummy phrase and get an index']...], shape=(256, 1), dtype=string)
</code></pre>

<p>but I want to get rid of the ""b"" at the beginning.</p>
",Preprocessing of the text & Tokenization,text preprocessing using string operation tensor trying write function take two string tensor kera input layer string operation first question loop item tensor split first input full sentence using second input phrase two word retrieve index split word function deal string convert function work tensor loop tensor apply function batch sentence batch corresponding phrase input second question convert tensor unicoded string output layer want get rid b beginning
Extracting sentence from a dataframe with description column based on a phrase,"<p>I have a dataframe with a 'description' column with details about the product. Each of the description in the column has long paragraphs. Like</p>

<p>""This is a superb product. I so so loved this superb product that I wanna gift to all. This is like the quality and packaging. I like it very much""</p>

<p>How do I locate/extract the sentence which has the phrase ""superb product"", and place it in a new column?</p>

<p>So for this case the result will be
<a href=""https://i.sstatic.net/yKyeU.png"" rel=""nofollow noreferrer"">expected output</a></p>

<p>I have used this, </p>

<pre><code>searched_words=['superb product','SUPERB PRODUCT']


print(df['description'].apply(lambda text: [sent for sent in sent_tokenize(text)
                           if any(True for w in word_tokenize(sent) 
                                     if stemmer.stem(w.lower()) in searched_words)]))
</code></pre>

<p>The output for this is not suitable. Though it works if I put just one word in "" Searched Word"" List.</p>
",Preprocessing of the text & Tokenization,extracting sentence dataframe description column based phrase dataframe description column detail product description column ha long paragraph like superb product loved superb product wan na gift like quality packaging like much locate extract sentence ha phrase superb product place new column case result expected output used output suitable though work put one word searched word list
Why is spacy failing at tokenizing a particular quotation mark?,"<p>I am running spacy on a paragraph of text and it's not extracting text in quote the same way for each, and I don't understand why that is</p>

<pre><code>nlp = spacy.load(""en_core_web_lg"")

doc = nlp(""""""A seasoned TV exec, Greenblatt spent eight years as chairman of NBC Entertainment before WarnerMedia. He helped revive the broadcast network's primetime lineup with shows like ""The Voice,"" ""This Is Us,"" and ""The Good Place,"" and pushed the channel to the top of the broadcast-rating ranks with 18-49-year-olds, Variety reported. He also drove Showtime's move into original programming, with series like ""Dexter,"" ""Weeds,"" and ""Californication."" And he was a key programming exec at Fox Broadcasting in the 1990s."""""")
</code></pre>

<p>Here's the whole output:</p>

<pre><code>A
seasoned
TV
exec
,
Greenblatt
spent
eight years
as
chairman
of
NBC Entertainment
before
WarnerMedia
.
He
helped
revive
the
broadcast
network
's
primetime
lineup
with
shows
like
""
The Voice
,
""
""
This
Is
Us
,
""
and
""The Good Place
,
""
and
pushed
the
channel
to
the
top
of
the
broadcast
-
rating
ranks
with
18-49-year-olds
,
Variety
reported
.
He
also
drove
Showtime
's
move
into
original
programming
,
with
series
like
""
Dexter
,
""
""
Weeds
,
""
and
""
Californication
.
""
And
he
was
a
key
programming
exec
at
Fox Broadcasting
in
the 1990s
.
</code></pre>

<p>The one that bothers me the most is The Good Place, which is extracted as <code>""The Good Place</code>. Since the quotation is part of the token, I then can't extract text in quote with a <a href=""https://spacy.io/usage/rule-based-matching"" rel=""nofollow noreferrer"">Token Matcher</a> later on… Any idea what's going on here?</p>
",Preprocessing of the text & Tokenization,spacy failing tokenizing particular quotation mark running spacy paragraph text extracting text quote way understand whole output one bother good place extracted since quotation part token extract text quote token matcher later idea going
Filtering specific words from a string based on a words position in a text,"<p>I have several part of speech rules and they are triggered only if the text being looked at matches the rule. However I'm curious if there is a way to remove ""any"" word that appears between a phrase that would otherwise trigger the rule. I tried using stop words, but it strips the text too much to where the rule becomes non-sensical. Heres an example.</p>

<pre><code>Text: I want to attack this player's base.

attack_rule = [
                ('nn', 'i'),
                ('vbp', 'want'),
                ('to', 'to'),
                ('vb', ('exterminate', 'waste', 'attack', 'shoot'))
                ]
</code></pre>

<p>The text will trigger this rule, however if the text is written as such:</p>

<pre><code>Text2: I f***ing want to attack this player's base.
Text2: I want to f***ing attack this player's base.
</code></pre>

<p>The rule won't trigger. So I'm wondering if there is a way to filter expletive/fillers from text that would otherwise trigger a rule? Ideally by position.</p>

<p>I'm currently using nltk's POS tagger. If there is a way to either make sure the word doesn't have a contextual effect on the sentence (like a superlative) which would seem way harder. Or just remove a word if it appears between text that would otherwise trigger a rule.</p>

<p>I tried using stop words but like I said it filtered far too much, especially when the object of the sentence was one of the most important parts.</p>

<pre><code>He will attack all of them &lt;- Stop words present
 he attack &lt;- Filtered stop words
</code></pre>
",Preprocessing of the text & Tokenization,filtering specific word string based word position text several part speech rule triggered text looked match rule however curious way remove word appears phrase would otherwise trigger rule tried using stop word strip text much rule becomes non sensical example text trigger rule however text written rule trigger wondering way filter expletive filler text would otherwise trigger rule ideally position currently using nltk po tagger way either make sure word contextual effect sentence like superlative would seem way harder remove word appears text would otherwise trigger rule tried using stop word like said filtered far much especially object sentence wa one important part
how to use column of tokenized sentences to further tokenize into words,"<p>i have tokenized a text in a column into a new column 'token_sentences' of sentence tokens.
i want to use 'token_sentences' column to create a new column 'token_words' containing tokenized words.</p>

<p>df i am using</p>

<pre><code>article_id      article_text                                       
1           Maria Sharapova has basically no friends as te...   
2           Roger Federer advance...    
3           Roger Federer has revealed that organisers of ...   
4           Kei Nishikori will try to end his long losing ...
</code></pre>

<p>added <code>token_sentences</code> column</p>

<pre><code>article_id      article_text                                      token_sentences                          
1           Maria Sharapova has basically no friends as te...    [Maria Sharapova has basically no friends as te    
2           Roger Federer advance...                             [Roger Federer advance...
3           Roger Federer has revealed that organisers of ...    [Roger Federer has revealed that organisers of...
4           Kei Nishikori will try to end his long losing ...    [Kei Nishikori will try to end his long losing...

</code></pre>

<p>which is a list of sentences in every row.
i am unable to flatten the list in <code>token_sentences</code> column to be able to used in the next step</p>

<p>i want use <code>token_sentences</code> column 
to make the df look like </p>

<pre><code>article_id  article_text    token_sentences                         token_words                       
1           Maria...        [""Maria Sharapova.."",[""...""]]           [Maria, Sharapova, has, basically, no, friends,...]       
2           Roger...        [""Roger Federer advanced  ..."",[""...""]] [Roger,Federer,...]
3           Roger...        [""Roger Federer..."",[""...""]]            [Roger ,Federer,...]
4           Kei ...         [""Kei Nishikori will try..."",[""...""]]   [Kei,Nishikori,will,try,...]

</code></pre>
",Preprocessing of the text & Tokenization,use column tokenized sentence tokenize word tokenized text column new column token sentence sentence token want use token sentence column create new column token word containing tokenized word df using added column list sentence every row unable flatten list column able used next step want use column make df look like
Iterating Over Numpy Array for NLP Application,"<p>I have a Word2Vec model that I'm building where I have a vocab_list of about 30k words.  I have a list of sentences (sentence_list) about 150k large.  I am trying to remove tokens (words) from the sentences that weren't included in vocab_list.  The task seemed simple, but nesting for loops and reallocating memory is slow using the below code.  This task took approx. 1hr to run so I don't want to repeat it.  </p>

<p>Is there a cleaner way to try this? </p>

<pre><code>import numpy as np
from datetime import datetime

start=datetime.now()
timing=[]
result=[]
counter=0
for sent in sentences_list:
    counter+=1
    if counter %1000==0 or counter==1:
        print(counter, 'row of', len(sentences_list), ' Elapsed time: ', datetime.now()-start)
        timing.append([counter, datetime.now()-start])
    final_tokens=[]
    for token in sent:
        if token in vocab_list:
            final_tokens.append(token)
    #if len(final_tokens)&gt;0:
    result.append(final_tokens)
print(counter, 'row of', len(sentences_list),' Elapsed time: ', datetime.now()-start)
timing.append([counter, datetime.now()-start])
sentences=result
del result
timing=pd.DataFrame(timing, columns=['Counter', 'Elapsed_Time'])
</code></pre>
",Preprocessing of the text & Tokenization,iterating numpy array nlp application word vec model building vocab list k word list sentence sentence list k large trying remove token word sentence included vocab list task seemed simple nesting loop reallocating memory slow using code task took approx hr run want repeat cleaner way try
Removing punctuation from a list of strings that aren&#39;t part of a word,"<p>I am writing a program to detect collocations of bigrams (2 words that appear together more often than by chance, ex: hot dog). To do this properly, I have to remove all punctuation marks that would be stored as their own element but keep punctuations that are part of a word. For example, the bigram ['U.S. flag'] should keep the periods in U.S. but ['U.S. ,'] should have the comma removed. I've written a for loop that iterates through a list of punctuations and should remove the matching element, but that doesn't change anything. Additionally, I've used regex to remove most punctuations but if I remove periods then words with periods in them also get ruined. Any suggestions for an efficient way to remove these would be deeply appreciated!</p>

<p>Here's my code so far:</p>

<pre><code>f = open('Collocations.txt').read()

punctuation = [',', '.', '!', '?', '""', ':', ""'"", ';', '@', '&amp;', '$', '#', '*', '^', '%', '{', '}']
filteredf = re.sub(r'[,"":@#?!&amp;$%}{]', '', f)

f = f.split()

print(len(f))
for i, j in zip (punctuation, f):
    if i == j:
        ind = f.index(j)
        f.remove(f[ind])
print(len(f))


# removes first element in the temp list to prepare to make bigrams
temp = list()
temp2 = list()
temp  = filteredf.split()
temp2 = filteredf.split()
temp2.remove(temp2[0]) 

# forms a list of bigrams
bi = list()
for i, j in zip(temp, temp2):
    x = i + "" "" + j
    bi.append(x)
#print(len(bi))
unigrams = dict()
for i in temp:
    unigrams[i] = unigrams.get(i, 0) + 1 
#print(len(unigrams))



bigrams = dict()
for i in bi:
    bigrams[i] = bigrams.get(i, 0) + 1
#print(len(bigramenter code here`
</code></pre>
",Preprocessing of the text & Tokenization,removing punctuation list string part word writing program detect collocation bigram word appear together often chance ex hot dog properly remove punctuation mark would stored element keep punctuation part word example bigram u flag keep period u u comma removed written loop iterates list punctuation remove matching element change anything additionally used regex remove punctuation remove period word period also get ruined suggestion efficient way remove would deeply appreciated code far
Stanford CoreNLP lemmatization with predefined parts of speech,"<p>My purpose is to generate syntactic dependencies and lemmas while I have tokens with predefined parts of speech (POS). The problem is that I didn't find a way to pass POS to the CoreNLP pipeline. I can generate dependencies by passing POS tagged tokens to the standalone <code>LexicalizedParser</code>, as described in the <a href=""https://nlp.stanford.edu/software/parser-faq.html#f"" rel=""nofollow noreferrer"">Stanford Parser FAQ</a>, but how can I get the lemmas? I didn't find a standalone lemmatizer that accepts POS, nor a way to pass POS to the pipeline.</p>
",Preprocessing of the text & Tokenization,stanford corenlp lemmatization predefined part speech purpose generate syntactic dependency lemma token predefined part speech po problem find way pas po corenlp pipeline generate dependency passing po tagged token standalone described stanford parser faq get lemma find standalone lemmatizer accepts po way pas po pipeline
"remove headers,footers,quotes in dataframes that contain documents","<p>Trying to preprocess this dataset to use it in an algorithm for NLP.</p>

<p>I have two dataframes. One with train data and another with test data.</p>

<p>Each looks kind of like this:</p>

<pre><code>df.head()

                        news                               genre
0   From: mathew &lt;mathew@mantis.co.uk&gt;\n Subject: ...   alt.atheism
1   From: mathew &lt;mathew@mantis.co.uk&gt;\n Subject: ...   alt.atheism
2   From: I3150101@dbstu1.rz.tu-bs.de (Benedikt Ro...   alt.atheism
</code></pre>

<p>each row in <code>news</code> is a document.</p>

<p>Example:</p>

<pre><code>df.news[4]
</code></pre>

<blockquote>
  <p>'From: strom@Watson.Ibm.Com (Rob Strom)\n Subject: Re: [soc.motss, et
  al.] ""Princeton axes matching funds for Boy Scouts""\n Distribution:
  usa\n Organization: IBM Research\n Lines: 15\n \n In article
  , n4hy@harder.ccr-p.ida.org
  (Bob McGwier) writes:\n \n |> [1] HOWEVER, I hate economic terrorism
  and political correctness\n |> worse than I hate this policy.  \n \n
  \n |> [2] A more effective approach is to stop donating\n |> to ANY
  organizating that directly or indirectly supports gay rights issues\n
  |> until they end the boycott on funding of scouts.  \n \n Can
  somebody reconcile the apparent contradiction between [1] and [2]?\n
  \n -- \n Rob Strom, strom@watson.ibm.com, (914) 784-7641\n IBM
  Research, 30 Saw Mill River Road, P.O. Box 704, Yorktown Heights, NY 
  10598\n'</p>
</blockquote>

<p>How can I improve the documents to get better results?</p>

<p>When I was working with Bunch type of data it was easy as you could simply do during import of the dataset:</p>

<pre><code> remove=('headers', 'footers', 'quotes')
</code></pre>

<p>Can you tell me how to remove those in the case we have here?</p>
",Preprocessing of the text & Tokenization,remove header footer quote dataframes contain document trying preprocess dataset use algorithm nlp two dataframes one train data another test data look kind like row document example strom watson ibm com rob strom n subject soc mot et al princeton ax matching fund boy scout n distribution usa n organization ibm research n line n n article n hy harder ccr p ida org bob mcgwier writes n n however hate economic terrorism political correctness n worse hate policy n n n effective approach stop donating n organizating directly indirectly support gay right issue n end boycott funding scout n n somebody reconcile apparent contradiction n n n rob strom strom watson ibm com n ibm research saw mill river road p box yorktown height ny n improve document get better result wa working bunch type data wa easy could simply import dataset tell remove case
How to customize spaCy&#39;s tokenizer to preclude splitting phrases described by a regular expression,"<p>For example I want the tokenizer to tokenize 'New York' as ['New York'] instead of the default ['New', 'York'].</p>

<p>The docs suggest adding regular expressions when creating a custom tokenizer. </p>

<p>So I did the following:</p>

<pre><code>import re
import spacy
from spacy.tokenizer import Tokenizer

target = re.compile(r'New York')

def custom_tokenizer(nlp):

    dflt_prefix = nlp.Defaults.prefixes
    dflt_suffix = nlp.Defaults.suffixes
    dflt_infix = nlp.Defaults.infixes

    prefix_re = spacy.util.compile_prefix_regex(dflt_prefix).search
    suffix_re = spacy.util.compile_suffix_regex(dflt_suffix).search
    infix_re = spacy.util.compile_infix_regex(dflt_infix).finditer

    return Tokenizer(nlp.vocab, prefix_search=prefix_re,
                                suffix_search=suffix_re,
                                infix_finditer=infix_re,
                                token_match=target.match)

nlp = spacy.load(""en_core_web_sm"")
nlp.tokenizer = custom_tokenizer(nlp)
doc = nlp(u""New York"")
print([t.text for t in doc])

</code></pre>

<p>I used the defaults so that normal behaviour continues unless the function target (the argument for the token_match parameter) returns true.</p>

<p>But I am still getting ['New', 'York']. Any help appreciated.</p>
",Preprocessing of the text & Tokenization,customize spacy tokenizer preclude splitting phrase described regular expression example want tokenizer tokenize new york new york instead default new york doc suggest adding regular expression creating custom tokenizer following used default normal behaviour continues unless function target argument token match parameter return true still getting new york help appreciated
Matching set of words with set of sentences in python nlp,"<p>I have a use case where I want to match one list of words with a list of sentences and bring the most relevant sentences</p>

<p>I am working in python. What I have already tried is using KMeans where we cluster our set of documents into the clusters and then predict the sentence that in which structure it resides. But in my case I have already available list of words available.</p>

<pre><code>def getMostRelevantSentences():
    Sentences = [""This is the most beautiful place in the world."",
            ""This man has more skills to show in cricket than any other game."",
            ""Hi there! how was your ladakh trip last month?"",
            ""Isn’t cricket supposed to be a team sport? I feel people should decide first whether cricket is a team game or an individual sport.""]

    words = [""cricket"",""sports"",""team"",""play"",""match""]

    #TODO: now this should return me the 2nd and last item from the Sentences list as the words list mostly matches with them
</code></pre>

<p>So from the above code I want to return the sentences which are closely matching with the words provided. I don't want to use the supervised machine learning here. Any help will be appreciated.</p>
",Preprocessing of the text & Tokenization,matching set word set sentence python nlp use case want match one list word list sentence bring relevant sentence working python already tried using kmeans cluster set document cluster predict sentence structure resides case already available list word available code want return sentence closely matching word provided want use supervised machine learning help appreciated
Get the most used combination of words on social networks,"<p>I want to know if there is a way ( an <strong>API</strong> or an online <strong>tool</strong>) that gives for an entred word a list of most used together words on twitter or any social media</p>
",Preprocessing of the text & Tokenization,get used combination word social network want know way api online tool give entred word list used together word twitter social medium
NLP -- tokenizing correctly words like &#39;new york&#39; or &#39;hip hop&#39;,"<p>I'm working on a NLP project using as dataset amazon digital music reviews. I'm preprocessing all the reviews by lemmatizing, stemming, tokenizing, removing punctuations and stopwords...</p>

<p>However I got stuck in a problem. Is there a way to preprocessing the text by saying to python:</p>

<blockquote>
  <p>`if there is 'new york', 'los angeles', 'hip hop' like words, then do not split them but melt: 'new_york', 'los_angeles', 'hip_hop'</p>
</blockquote>

<p>?</p>

<p>I do not want to map manually all of them and I tried to play with bigrams and with pos but with no success. </p>

<p>Can you help me?</p>
",Preprocessing of the text & Tokenization,nlp tokenizing correctly word like new york hip hop working nlp project using dataset amazon digital music review preprocessing review lemmatizing stemming tokenizing removing punctuation stopwords however got stuck problem way preprocessing text saying python new york los angeles hip hop like word split melt new york los angeles hip hop want map manually tried play bigram po success help
Understanding how word embedding with Fasttext works for my case,"<p>I'm looking for some guidance with Fasttext and NLP to help understand how the model proceed to calculate the vector of a sentence.</p>

<p><strong>Context:</strong></p>

<p>I'm using the fasttext method get_sentence_vector() to calculate the vector of a query sentence that I will call P1, as well as for a set of sentences (P2, P3, P4, P5, ..., Pn). Sentences can have one or more words. Then, I calculate the distance between the vector of the sentence P1 with that of each of the other sentences to finally obtain the list of the sentences closest to P1. Please note that i'm doing a preprocessing only on P1 (removal of numbers and punctuation + tokenization and lemmatization with SpaCy). The goal is to get the sentences that come closest in terms of meaning</p>

<hr>

<p>The problem is that I do not understand the results I get for different cases:</p>

<p><strong>case 1: P1 = ""biofertilizers""</strong></p>

<ul>
<li>distance between vectors ""biofertilizers"" and ""chemical fertilizers"" : 0.48</li>
<li>distance between vectors ""biofertilizers"" and ""bio-fertilizers"" : 0.49</li>
</ul>

<p>Here, i don't understand how vector calculated with fasttext of ""biofertilizers"" is closer to ""chemical fertilizers"" than ""bio-fertilizers"". Is the dash counted during the vector calculation ? Bio-fertilizers should logically be closer don't you think ?</p>

<p><strong>case 2: P1 = ""laptop""</strong></p>

<ul>
<li>distance between vectors ""laptop"" and ""battery chargers for laptop computers"" : 0.16</li>
<li>distance between vectors ""laptop"" and ""tablet computers"" : 0.27</li>
</ul>

<p>This is not correct because ""tablet computers"" is closest to ""laptop"" in term of meaning than ""battery chargers for laptop computers"". Is it because the latter contain the word ""laptop"" that the distance is lower ?</p>

<p><strong>case 3: P1 = ""knives"":</strong></p>

<p>The distance between ""knives"" and ""tableware, except forks, knives and spoons"" is low and these two sentences are considered to be close. This should not be the case because their meanings are opposed. So I assume that Fasttext does not assimilate the negation words like 'Except' or 'not' during the vector calculation?</p>

<hr>

<p>How does Fasttext arrive at these results when calculating distances between vectors?</p>

<p>I am also interested in hearing other suggestions for calculating the degree of semantic proximity between two sentences.</p>
",Preprocessing of the text & Tokenization,understanding word embedding fasttext work case looking guidance fasttext nlp help understand model proceed calculate vector sentence context using fasttext method get sentence vector calculate vector query sentence call p well set sentence p p p p pn sentence one word calculate distance vector sentence p sentence finally obtain list sentence closest p please note preprocessing p removal number punctuation tokenization lemmatization spacy goal get sentence come closest term meaning problem understand result get different case case p biofertilizers distance vector biofertilizers chemical fertilizer distance vector biofertilizers bio fertilizer understand vector calculated fasttext biofertilizers closer chemical fertilizer bio fertilizer dash counted vector calculation bio fertilizer logically closer think case p laptop distance vector laptop battery charger laptop computer distance vector laptop tablet computer correct tablet computer closest laptop term meaning battery charger laptop computer latter contain word laptop distance lower case p knife distance knife tableware except fork knife spoon low two sentence considered close case meaning opposed assume fasttext doe assimilate negation word like except vector calculation doe fasttext arrive result calculating distance vector also interested hearing suggestion calculating degree semantic proximity two sentence
How to pre-process texts to match Googles pre-trained word2vec model?,"<p>I am wondering which steps I have to execute on my corpus to pre-process it the same style like google did for their massive, pre-trained word2vec model (<a href=""https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/"" rel=""nofollow noreferrer"">https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/</a> )</p>

<p>According to the website they did the following:</p>

<ul>
<li>bigrams/ trigrams</li>
<li>removal of some stop words (only common ones like: a, and, of)</li>
<li>removal of some numbers (only without surrounding letters)</li>
</ul>

<p>Is there any source which details all steps?</p>

<p>Did they also  e.g. ...</p>

<ul>
<li>remove some punctuation</li>
<li>lowercase some letters</li>
<li>stem or lemmatize
?</li>
</ul>
",Preprocessing of the text & Tokenization,pre process text match google pre trained word vec model wondering step execute corpus pre process style like google massive pre trained word vec model according website following bigram trigram removal stop word common one like removal number without surrounding letter source detail step also e g remove punctuation lowercase letter stem lemmatize
How can I merge or concatenate two sequential models using Keras?,"<p>I have a dataset with two text fields which after tokenization I have made two sequential models which I am trying to combine or merge but i am facing errors while merging.</p>

<p>I have built two sequential models already and I am trying to merge them without using Keras Functional API.</p>

<pre><code># define the model
model1 = Sequential()
model1.add(Embedding(vocabulary_size_1, embedding_size, input_length=MAXLEN))
model1.add(Flatten())
model1.add(Dense(op_units, activation='softmax'))

# define the model
model2 = Sequential()
model2.add(Embedding(vocabulary_size_2, embedding_size, input_length=MAXLEN))
model2.add(Flatten())
model2.add(Dense(op_units, activation='softmax'))

merged = concatenate(axis=1)
merged_model=merged([model1.output, model2.ouput])

TypeError                                 Traceback (most recent call last)
&lt;ipython-input-76-79cf08fec6fc&gt; in &lt;module&gt;
----&gt; 1 merged = concatenate(axis=1)
      2 merged_model=merged([model1.output, model2.ouput])

TypeError: concatenate() missing 1 required positional argument: 'inputs'
</code></pre>

<p>I am expecting an approach without using Keras Functional API</p>
",Preprocessing of the text & Tokenization,merge concatenate two sequential model using kera dataset two text field tokenization made two sequential model trying combine merge facing error merging built two sequential model already trying merge without using kera functional api expecting approach without using kera functional api
How to find all the related keywords for a root word?,"<p>I am trying to figure out a way to find all the keywords that come from the same root word (in some sense the opposite action of stemming). Currently, I am using R for coding, but I am open to switching to a different language if it helps.</p>

<p>For instance, I have the root word ""rent"" and I would like to be able to find ""renting"", ""renter"", ""rental"", ""rents"" and so on.</p>
",Preprocessing of the text & Tokenization,find related keywords root word trying figure way find keywords come root word sense opposite action stemming currently using r coding open switching different language help instance root word rent would like able find renting renter rental rent
Stop words changed negative review to positive ones. What is a good way to remove stop words in text summarization process?,"<p><a href=""https://i.sstatic.net/dPFai.png"" rel=""nofollow noreferrer"">dataframe</a>I try to remove stop words (English) from two columns in a data frame. See screenshot. However, I found that after applying this process, the meaning of the review was changed. e.g. Not recommend was changed to recommended. What is the best way to remove stop words while remaining the idea of the original text unchanged? This is my code and results:</p>

<pre><code>from nltk import word_tokenize
from nltk.corpus import stopwords
stop = set(stopwords.words('english'))

df['Text_after_removed_stopwords'] = df['Text'].apply(lambda x: ' 
'.join([word for word in x.split() if word not in (stop)]))
print()
print('###Text after removed 
stopwords###'+'\n'+df['Text_after_removed_stopwords'][1])
print()
print('###Text before removed stopwords###'+'\n'+ df['Text'][1])
print()
df['Summary_after_removed_stopwords'] = df['Summary'].apply(lambda 
x: ' '.join([word for word in x.split() if word not in (stop)]))
print('###Summary after removed stopwords###'+ ' 
\n'+df['Summary_after_removed_stopwords'][1])
print()
print('###Summary before removed stopwords###'+'\n'+df['Summary'][ 
1])


###Text after removed stopwords###
product arrived labeled jumbo salted peanutsthe peanuts actually 
small sized unsalted sure error vendor intended represent product 
jumbo

###Text before removed stopwords###
product arrived labeled as jumbo salted peanutsthe peanuts were 
actually small sized unsalted not sure if this was an error or if 
the vendor intended to represent the product as jumbo

###Summary after removed stopwords###
advertised

###Summary before removed stopwords###
not as advertised
</code></pre>
",Preprocessing of the text & Tokenization,stop word changed negative review positive one good way remove stop word text summarization process dataframei try remove stop word english two column data frame see screenshot however found applying process meaning review wa changed e g recommend wa changed recommended best way remove stop word remaining idea original text unchanged code result
How to solve &quot;ValueError: setting an array element with a sequence&quot;,"<p>Here is an example of my data set</p>

<pre><code>d = {'TEXT': ['History: A 59  year  old female, was sent to R/O lung nodule. Findings:  Lungs and airway:  The study reveals a speculated nodule with pleural tagging at anterior basal segment of LLL, measured 1.9x1.4x2.0 cm in size. Pleural tagging is seen. Partial encasement of subsegmental bronchi is seen.  CA lung is considered.','History: A 59  year  old woman with history of lung cancer S/P left lower lobectomy with close to pleural margin and left adrenal nodule , was sent for evaluation before post  operative RT. Findings: Comparison is made to the prior study on 03/02/2009. Chest:   The study reveals evidence of left lower lobectomy with compensatory hyperinflation of the LUL.']}
df2 = pd.DataFrame(data=d)
</code></pre>

<p>I want to implement Latent Diritchlet allocation (LDA) for context generation for each sentence. I have separately trained my model for it and want to test on these data.</p>

<p>To reach to LDA, I tokenize the text into sentences as I am interested to classify each sentence with a topic. After sentence tokenization, I implement TFIDF and then to LDA. While reaching upto LDA, I get this error. Following is my code.</p>

<pre><code>df2[""sent_token""] = df2[""TEXT""].apply(nltk.sent_tokenize)
vectoriser = TfidfVectorizer(tokenizer=identity_tokenizer,stop_words='english',lowercase=False)
df2['tfidf1'] = vectoriser.fit_transform(df2['sent_token'])
lda = LatentDirichletAllocation(n_components =5)
df2['tfidf_lda']= lda.fit_transform(df2['tfidf1'])
</code></pre>

<p>Here is where I get this error ""ValueError: setting an array element with a sequence."" While going through similar errors, <a href=""https://stackoverflow.com/questions/4674473/valueerror-setting-an-array-element-with-a-sequence"">ValueError: setting an array element with a sequence</a> I found it may be because the rows have a different number of sentences resulting in different length or sequences. But this is the heterogeneity I have and I am not really sure what is the problem. Please help!!</p>
",Preprocessing of the text & Tokenization,solve valueerror setting array element sequence example data set want implement latent diritchlet allocation lda context generation sentence separately trained model want test data reach lda tokenize text sentence interested classify sentence topic sentence tokenization implement tfidf lda reaching upto lda get error following code get error valueerror setting array element sequence going similar error href setting array element sequence found may row different number sentence resulting different length sequence heterogeneity really sure problem please help
Get each unique word in a csv file tokenized,"<p><a href=""https://i.sstatic.net/53rEL.png"" rel=""nofollow noreferrer"">Here is the CSV table</a>There are two columns in a CSV table. One is summaries and the other one is texts. Both columns were typeOfList before I combined them together, converted to data frame and saved as a CSV file. BTW, the texts in the table have already been cleaned (removed all marks and converted to lower cases):</p>

<p>I want to loop through each cell in the table, split summaries and texts into words and tokenize each word. How can I do it?</p>

<p>I tried with python CSV reader and df.apply(word_tokenize). I tried also newList=set(summaries+texts), but then I could not tokenize them. 
Any solutions to solve the problem, no matter of using CSV file, data frame or list. Thanks for your help in advance!</p>

<p>note: The real table has more than 50,000 rows.</p>

<p>===some update== </p>

<p>here is the code I have tried. </p>

<pre><code>import pandas as pd
data= pd.read_csv('test.csv')

data.head()

newTry=data.apply(lambda x: "" "".join(x), axis=1)
type(newTry)

print (newTry)

import nltk

for sentence in newTry: 
    new=sentence.split() 

    print(new)
 print(set(new))
</code></pre>

<p><a href=""https://i.sstatic.net/7g1MN.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>Please refer to the output in the screenshot. There are duplicate words in the list, and some square bracket. How should I removed them? I tried with set, but it gives only one sentence value.</p>
",Preprocessing of the text & Tokenization,get unique word csv file tokenized csv tablethere two column csv table one summary one text column typeoflist combined together converted data frame saved csv file btw text table already cleaned removed mark converted lower case want loop cell table split summary text word tokenize word tried python csv reader df apply word tokenize tried also newlist set summary text could tokenize solution solve problem matter using csv file data frame list thanks help advance note real table ha row update code tried enter image description please refer output screenshot duplicate word list square bracket removed tried set give one sentence value
How to extract a substring from a given text using NLP?,"<p>I am trying to extract the substring from a text that does not add any value to the answer. I tried it with n-grams but not getting satisfying results. </p>

<p>I am trying to find the similarity between two text using google universal sentence encoder. I observed that If I clean the text before passing it to the encoder I get better results. I want to extract the text that is repeating from the question as it does not add any value to the answer. </p>

<pre><code>def extract_answer(question,answer):
   &lt;&lt; some code goes here &gt;&gt;
   return extracted_text

Question = ""Why is the plasma membrane called a selectively permeable membrane?""

Answer = ""The cell membrane or the plasma membrane is known as a selectively permeable membrane because it regulates the movement of substances in and out of the cell. This means that the plasma membrane allows the entry of only some substances and prevents the movement of some other materials.""

extracted_answer = extract_answer(Question,Answer)

print(extracted_answer) 



Sample 1
---------

Input
-------
Question: Why is the plasma membrane called a selectively permeable membrane?
Answer: The cell membrane or the plasma membrane is known as a selectively permeable membrane because it regulates the movement of substances in and out of the cell. This means that the plasma membrane allows the entry of only some substances and prevents the movement of some other materials.

Expected Output
---------------

Output: it regulates the movement of substances in and out of the cell. This means that the plasma membrane allows the entry of only some substances and prevents the movement of some other materials.


Sample 2
----------  

Input
-------
Question: Why is the diver able to cross the river?
Answer: The swimmer is able to cross the river because the particles of matter have space between them. 

Expected Output
---------------

Output: particles of matter have space between them.
</code></pre>
",Preprocessing of the text & Tokenization,extract substring given text using nlp trying extract substring text doe add value answer tried n gram getting satisfying result trying find similarity two text using google universal sentence encoder observed clean text passing encoder get better result want extract text repeating question doe add value answer
NLP Negation detection and Stop Words,"<p>I'd like to improve my sentiment analysis via negation detection. I'm implementing sentiment analysis using a bag of words approach highlight by professor, hence why I'm not using CoreNLP's sentiment annotator, yet. However, I noticed it has a problem.</p>

<p>Given the sentence, ""I'm not disappointed in them"", I would expect, at worst, a neutral sentiment, or weakly positive sentiment, both from the sentiment annotator, and my own bag of words implementation. The sentiment annotator reports this sentence as negative.</p>

<pre><code>I: PRP  Neutral
'm: VBP Neutral
not: RB Negative
disappointed: VBN   Negative
in: IN  Neutral
them: PRP   Neutral
.: .    Neutral
Negative
1
</code></pre>

<p>The last two lines show the sentence's sentiment label and the numerical sentiment score.</p>

<p>How do I improve the sentiment annotator's chances of getting this right, and how can I use CoreNLP to detect negation like what's shown, negation across sentences, and references to an entity across multiple sentences (which seems like the coref and dcoref annotators)?</p>

<p>Also, potentially useful would be getting rid of stop words. The lemma annotator seems to take care of stemming, but which annotator does stop words?</p>
",Preprocessing of the text & Tokenization,nlp negation detection stop word like improve sentiment analysis via negation detection implementing sentiment analysis using bag word approach highlight professor hence using corenlp sentiment annotator yet however noticed ha problem given sentence disappointed would expect worst neutral sentiment weakly positive sentiment sentiment annotator bag word implementation sentiment annotator report sentence negative last two line show sentence sentiment label numerical sentiment score improve sentiment annotator chance getting right use corenlp detect negation like shown negation across sentence reference entity across multiple sentence seems like coref dcoref annotator also potentially useful would getting rid stop word lemma annotator seems take care stemming annotator doe stop word
Want to remove numbers from pandas dataframe and implement CountVectorizer,"<p>I have a data in the following form:</p>

<pre><code>    author  text
0   garyvee     A lot of people misunderstand Gary’s message o...
1   jasonfried  ""I can’t remember having a goal. An actual goa...
2   biz         ""Tools that can create media that looks and so...

</code></pre>

<p>I tried the following to clean the text:</p>

<pre><code>text_data.loc[:,""text""] = text_data.text.apply(lambda x : str.lower(x))
text_data.loc[:,""text""] = text_data.text.apply(lambda x : "" "".join(re.findall('[\w]+',x)))
</code></pre>

<p>I got output but it contains digits I dont want that for the Text Analysis</p>

<pre><code>0    a lot of people misunderstand gary s message o...
1    i can t remember having a goal an actual goal ...
2    tools that can create media that looks and sou...
Name: text, dtype: object
</code></pre>

<p>but while removing numbers in the text string:</p>

<pre><code>text_data.loc[:,""text""] = text_data.text.apply(lambda x : "" "".join(re.sub('^[0-9\.]*$','',x)))
</code></pre>

<p>I got the output:</p>

<pre><code>0    a l o t o f p e o p l e m i s u n d e r s t a ...
1    i c a n t r e m e m b e r h a v i n g a g o a ...
2    t o o l s t h a t c a n c r e a t e m e d i a ...
Name: text, dtype: object
</code></pre>

<p>How to avoid it? How to implement CountVectorizer?</p>
",Preprocessing of the text & Tokenization,want remove number panda dataframe implement countvectorizer data following form tried following clean text got output contains digit dont want text analysis removing number text string got output avoid implement countvectorizer
Maintain proper nouns and capitalised words while stemming,"<p>I am designing a text processing program and need to stem the words for exploratory analysis later. One of my processes is to stem the words and I have to use Porter Stemmer.</p>

<p>I have designed a DataFrame structure to store my data. Furthermore, I have also designed a function to apply to the DataFrame. When I apply the function to the DataFrame, the stemming works but it does not keep the capitalised (or proper nouns) words. </p>

<p>A snippet of my code:</p>

<pre><code>from nltk.stem.porter import PorterStemmer

def stemming(word):
    stemmer = PorterStemmer()
    word = str(word)
    if word.title():
        stemmer.stem(word).capitalize()
    elif word.isupper():
        stemmer.stem(word).upper()
    else:
        stemmer.stem(word)
    return word

dfBody['body'] = dfBody['body'].apply(lambda x: [stemming(y) for y in x])
</code></pre>

<p>This is my result with that has no capitalised words:
<a href=""https://i.sstatic.net/Mgza6.png"" rel=""nofollow noreferrer"">output</a></p>

<p>Sample of dataset (my dataset is very large):</p>

<pre><code>file    body
PP3169 ['performing', 'Maker', 'USA', 'computer', 'Conference', 'NIPS']
</code></pre>

<p>Expected output (after applying stemming function):</p>

<pre><code>file    body
PP3169 ['perform', 'Make', 'USA', 'comput', 'Confer', 'NIPS']
</code></pre>

<p>Any advice will be greatly appreciated!</p>
",Preprocessing of the text & Tokenization,maintain proper noun capitalised word stemming designing text processing program need stem word exploratory analysis later one process stem word use porter stemmer designed dataframe structure store data furthermore also designed function apply dataframe apply function dataframe stemming work doe keep capitalised proper noun word snippet code result ha capitalised word output sample dataset dataset large expected output applying stemming function advice greatly appreciated
How to creat a dictionary where the key is the stem and the value is a list of words with that stem?,"<p>In python, I already have a list of words and a list of stem. How to create a dictionary where the key is the stem and the value is a list of words with that stem, like this:  </p>

<p><code>{‘achiev’: [‘achieved’, ‘achieve’] ‘accident’: [‘accidentally’, ‘accidental’] … }</code></p>
",Preprocessing of the text & Tokenization,creat dictionary key stem value list word stem python already list word list stem create dictionary key stem value list word stem like
how to modify Wordnet Lemmatizer to lemmitize specific words?,"<p>I am applying wordNet lemmatizer into my corpus and I need to define the pos tagger for lemmatizer:</p>

<pre><code>stemmer = PorterStemmer()
def lemmitize(document):
    return stemmer.stem(WordNetLemmatizer().lemmatize(document, pos='v'))

def preprocess(document):
output = []
    for token in gensim.utils.simple_preprocess(document):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; 3:
            print(""lemmitize: "", lemmitize(token))
            output.append(lemmitize(token))
    return output
</code></pre>

<p>Now as you can see I am defining pos for verb (and I know wordNet default pos is a noun), however when I lemmatized my document:</p>

<pre><code>the left door closed at the night  
</code></pre>

<p>I am getting out put as: </p>

<pre><code>output:  ['leav', 'door', 'close', 'night']
</code></pre>

<p>which this is not what i was expecting. In my above sentences, <code>left</code> points to which door (e.g. right or left). If I choose <code>pos ='n'</code> this problem may solve but it will then act as a wornNet default and there will be no effects on words like <code>taken</code>. </p>

<p>I found a similar issue in <a href=""https://stackoverflow.com/questions/22999273/python-nltk-lemmatization-of-the-word-further-with-wordnet"">here</a> and I modified the exception list in <code>nltk_data/corpora/wordnet/verb.exc</code> and I changed <code>left leave</code> to <code>left left</code> but still, I am getting the same results as <code>leav</code>.<br>
Now I am wondering if there is any solution to this problem or in the best case, is there any way that I can add a custom dictionary of some words (only limited to my document) that wordNet does not lemmatize them like:</p>

<pre><code>my_dict_list = [left, ...]
</code></pre>
",Preprocessing of the text & Tokenization,modify wordnet lemmatizer lemmitize specific word applying wordnet lemmatizer corpus need define po tagger lemmatizer see defining po verb know wordnet default po noun however lemmatized document getting put wa expecting sentence point door e g right left choose problem may solve act wornnet default effect word like found similar issue href modified exception list changed still getting result br wondering solution problem best case way add custom dictionary word limited document wordnet doe lemmatize like
POS tagging before/after punctuation removal?,"<p>A possibly very basic question about NLP best practices.</p>

<p>Does punctuation affect the behaviour of NLTK's Parts-of-Speech tagger? Or is it fine to remove punctuation from a sentence before passing it to the POS tagger?</p>
",Preprocessing of the text & Tokenization,po tagging punctuation removal possibly basic question nlp best practice doe punctuation affect behaviour nltk part speech tagger fine remove punctuation sentence passing po tagger
processing before or after train test split,"<p>I am using this excellent article to learn Machine learning.</p>

<p><a href=""https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/"" rel=""noreferrer"">https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/</a></p>

<p>The author has tokenized the X and y data after splitting it up.</p>

<pre><code>X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=42
)

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

vocab_size = len(tokenizer.word_index) + 1

maxlen = 200

X_train = pad_sequences(X_train, padding=""post"", maxlen=maxlen)
X_test = pad_sequences(X_test, padding=""post"", maxlen=maxlen)
</code></pre>

<p>If I tokenize it before using train_test_split class, I can save a few lines of code.</p>

<pre><code>tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X)

X_t = tokenizer.texts_to_sequences(X)
vocab_size = len(tokenizer.word_index) + 1
maxlen = 200

X = pad_sequences(X_t, padding=""post"", maxlen=maxlen)
</code></pre>

<p>I just want to confirm that my approach is correct and I do not expect any surprises later in the script.</p>
",Preprocessing of the text & Tokenization,processing train test split using excellent article learn machine learning author ha tokenized x data splitting tokenize using train test split class save line code want confirm approach correct expect surprise later script
Lemmatization using StanfordCoreNLP,"<p>I found this code which lemmatizes a text.<br>
This text is split into sentences and then tokenized.<br>
And finally the tokens are lemmatized.</p>

<p>My problem is that I don't need to do the steps of <code>splitting</code> and <code>tokenize</code> because I already did that in my program.<br>
I just want to integrate the step of lemmatization to my program, since I already have a list of words which I must lemmatize.</p>

<p>Here is the program that I want to integrate, without the steps which occur before the lemmatization. </p>

<pre><code>import java.util.LinkedList;
import edu.stanford.nlp.ling.CoreAnnotations.LemmaAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class StanfordLemmatizer {

protected StanfordCoreNLP pipeline;

public StanfordLemmatizer() {
    // Create StanfordCoreNLP object properties, with POS tagging
    // (required for lemmatization), and lemmatization
    Properties props;
    props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma"");

    /*
     * This is a pipeline that takes in a string and returns various analyzed linguistic forms. 
     * The String is tokenized via a tokenizer (such as PTBTokenizerAnnotator), 
     * and then other sequence model style annotation can be used to add things like lemmas, 
     * POS tags, and named entities. These are returned as a list of CoreLabels. 
     * Other analysis components build and store parse trees, dependency graphs, etc. 
     * 
     * This class is designed to apply multiple Annotators to an Annotation. 
     * The idea is that you first build up the pipeline by adding Annotators, 
     * and then you take the objects you wish to annotate and pass them in and 
     * get in return a fully annotated object.
     * 
     *  StanfordCoreNLP loads a lot of models, so you probably
     *  only want to do this once per execution
     */
    this.pipeline = new StanfordCoreNLP(props);
}

public List&lt;String&gt; lemmatize(String documentText)
{
    List&lt;String&gt; lemmas = new LinkedList&lt;String&gt;();
    // Create an empty Annotation just with the given textd
    Annotation document = new Annotation(documentText);
    // run all Annotators on this text
    this.pipeline.annotate(document);
    // Iterate over all of the sentences found
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
    for(CoreMap sentence: sentences) {
        // Iterate over all tokens in a sentence
        for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
            // Retrieve and add the lemma for each word into the
            // list of lemmas
            lemmas.add(token.get(LemmaAnnotation.class));
        }
    }
    return lemmas;
}




public static void main(String[] args) {

    System.out.println(""Starting Stanford Lemmatizer"");
   String text = ""How could you be seeing into my eyes like open doors? \n""+
            ""You led me down into my core where I've became so numb \n""+
            ""Without a soul my spirit's sleeping somewhere cold \n""+
            ""Until you find it there and led it back home \n""+
            ""You woke me up inside \n""+
            ""Called my name and saved me from the dark \n""+
            ""You have bidden my blood and it ran \n""+
            ""Before I would become undone \n""+
            ""You saved me from the nothing I've almost become \n""+
            ""You were bringing me to life \n""+
            ""Now that I knew what I'm without \n""+
            ""You can've just left me \n""+
            ""You breathed into me and made me real \n""+
            ""Frozen inside without your touch \n""+
            ""Without your love, darling \n""+
            ""Only you are the life among the dead \n""+
            ""I've been living a lie, there's nothing inside \n""+
            ""You were bringing me to life."";

    StanfordLemmatizer slem = new StanfordLemmatizer();
    System.out.println(slem.lemmatize(text));
</code></pre>
",Preprocessing of the text & Tokenization,lemmatization using stanfordcorenlp found code lemmatizes text text split sentence tokenized finally token lemmatized problem need step already program want integrate step lemmatization program since already list word must lemmatize program want integrate without step occur lemmatization
Preprecessing for Embedding: transforming word tokens to integer vectors,"<p>I am trying to preprocess a corpus of text for input to a word embedding layer, which takes padded vectors of integers. </p>

<p>I know Keras/Tensorflow already has a list of functions that can do this (e.g., <a href=""https://keras.io/preprocessing/text/#one_hot"" rel=""nofollow noreferrer"">https://keras.io/preprocessing/text/#one_hot</a>). But I would like to do my custom tokenization. This will be useful if the language is not English, for example.</p>

<p>Does anyone know of examples of code that can take in a list of tokens, and transform them to vectors of integers? I would think this is a fairly common NLP task, so I wanted to check before reinventing the wheel.</p>
",Preprocessing of the text & Tokenization,preprecessing embedding transforming word token integer vector trying preprocess corpus text input word embedding layer take padded vector integer know kera tensorflow already ha list function e g would like custom tokenization useful language english example doe anyone know example code take list token transform vector integer would think fairly common nlp task wanted check reinventing wheel
How to compute word per token word distance and return the count of 0 distance in a column,"<p>I got two descriptions, one in a dataframe and other that is a list of words and I need to compute the levensthein distance of each word in the description against each word in the list and return the count of the result of the levensthein distance that is equal to 0</p>

<pre><code>import pandas as pd


definitions=['very','similarity','seem','scott','hello','names']
</code></pre>

<pre><code># initialize list of lists 
data = [['hello my name is Scott'], ['I went to the mall yesterday'], ['This seems very similar']] 

# Create the pandas DataFrame 
df = pd.DataFrame(data, columns = ['Descriptions']) 

# print dataframe. 
df 
</code></pre>

<p>Column counting the number of all words in each row that computing the Lev distances against each word in the dictionary returns 0</p>

<p>df['lev_count_0']=  Column counting the number of all words in each row that computing the Lev distances against each word in the dictionary returns 0</p>

<p>So for example, the first case will be</p>

<pre><code>edit_distance(""hello"",""very"") # This will be equal to 4
edit_distance(""hello"",""similarity"") # this will be equal to 9
edit_distance(""hello"",""seem"") # This will be equal to 4
edit_distance(""hello"",""scott"") # This will be equal to 5
edit_distance(""hello"",""hello"")# This will be equal to 0
edit_distance(""hello"",""names"") # this will be equal to 5
</code></pre>

<p>So for the first row in df['lev_count_0'] the result should be 1, since there is just one 0 comparing all words in the Descriptions against the list of Definitions</p>

<pre><code>Description               | lev_count_0
hello my name is Scott    |      1


</code></pre>
",Preprocessing of the text & Tokenization,compute word per token word distance return count distance column got two description one dataframe list word need compute levensthein distance word description word list return count result levensthein distance equal column counting number word row computing lev distance word dictionary return df lev count column counting number word row computing lev distance word dictionary return example first case first row df lev count result since one comparing word description list definition
"How to avoid double quoted string , site URL and email address from tokenization","<p>How we I stop <code>word_tokenize</code> from splittings strings like <code>""pass_word""</code>, <code>""https://www.gmail.com""</code> and <code>""tempemail@mail.com""</code>? The quotes should prevent it, but they don't. </p>

<p>I have tried with different regex options.</p>

<pre class=""lang-py prettyprint-override""><code>from nltk import word_tokenize

s = 'open ""https://www.gmail.com"" url. Enter ""tempemail@mail.com"" in email. Enter ""pass_word"" in password.'
for phrase in re.findall('""([^""]*)""', s):
    s = s.replace('""{}""'.format(phrase), phrase.replace(' ', '*'))
tokens = word_tokenize(s)
print(tokens)
</code></pre>

<p>Actual response:</p>

<pre><code>['open', 'https', ':', '//www.gmail.com', 'url', '.', 'Enter', 
 'tempemail', '@', 'mail.com', 'in', 'email', '.', 'Enter', 
 'pass_word', 'in', 'password', '.']
</code></pre>

<p>Expected response:</p>

<pre><code>['open', 'https://www.gmail.com', 'url', '.', 'Enter', 
 'tempemail@mail.com', 'in', 'email', '.', 'Enter', 
 'pass_word', 'in', 'password', '.']
</code></pre>
",Preprocessing of the text & Tokenization,avoid double quoted string site url email address tokenization stop splittings string like quote prevent tried different regex option actual response expected response
Where to add SentencePiece tokenization in AllenNlp pipeline?,"<p>I am new to allennlp, I use sentencepiece for subword tokenization in my pipeline. </p>

<p>SentencePiece needs a training step to generate a subword model, which can then be used for tokenization.</p>

<p>Is an implementation of <code>Vocabulary</code>  class the right way to do it. Little confused whether it is the right place, given there are TokenIndexers for character tokenization etc.</p>
",Preprocessing of the text & Tokenization,add sentencepiece tokenization allennlp pipeline new allennlp use sentencepiece subword tokenization pipeline sentencepiece need training step generate subword model used tokenization implementation class right way little confused whether right place given tokenindexers character tokenization etc
How are &quot;word boundaries&quot; identified in Python sklearn CountVectorizer&#39;s analyzer parameter?,"<p>Python sklearn CountVectorizer has an ""analyzer"" parameter which has a ""char_wb"" option. According to the definition, </p>

<pre><code>""Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space."". 
</code></pre>

<p>My question here is, how does CountVectorizer identify a ""word"" from a string? More specifically, are ""words"" simply space-separated strings from a sentence, or are they identified by more complex techniques like word_tokenize from nltk?</p>

<p>The reason I ask this is that I am analyzing social media data which has a whole lot of @mentions and #hashtags. Now, nltk's word_tokenize breaks up a ""@mention"" into [""@"", ""mention], and a ""#hashtag"" into [""#"", ""hashtag""]. If I feed these into CountVectorizer with ngram_range > 1, the ""#"" and ""@"" will never be captured as features. Moreover, I want character n-grams (with char_wb) to capture ""@m"" and ""#h"" as features, which won't ever happen if CountVectorizer breaks up @mentions and #hashtags into [""@"",""mentions""] and [""#"",""hashtags""].</p>

<p>What do I do?</p>
",Preprocessing of the text & Tokenization,word boundary identified python sklearn countvectorizer analyzer parameter python sklearn countvectorizer ha analyzer parameter ha char wb option according definition question doe countvectorizer identify word string specifically word simply space separated string sentence identified complex technique like word tokenize nltk reason ask analyzing social medium data ha whole lot mention hashtags nltk word tokenize break mention mention hashtag hashtag feed countvectorizer ngram range never captured feature moreover want character n gram char wb capture h feature ever happen countvectorizer break mention hashtags mention hashtags
How can I reduce time for filtering my article dataset?,"<p>I'm trying to filter my dataset which contains nearly 50K articles. From each article I want to filter out stop words and punctuation. But the process is taking long time. I've already filtered the dataset and it took 6 hours. Now I've got another dataset to filter which contains 300K articles.</p>

<p>I'm using python in anaconda environment. PC configuration: 7th Gen. Core i5, 8GB RAM and NVIDIA 940MX GPU. To filter my dataset I've wrote a code which takes each article in dataset, tokenize words and then remove stop words, punctuations and numbers. </p>

<pre class=""lang-py prettyprint-override""><code>def sentence_to_wordlist(sentence, filters=""!\""#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n?,।!‍.'0123456789০১২৩৪৫৬৭৮৯‘\u200c–“”…‘""):
    translate_dict = dict((c, ' ') for c in filters)
    translate_map = str.maketrans(translate_dict)
    wordlist = sentence.translate(translate_map).split()
    global c,x;
    return list(filter(lambda x: x not in stops, wordlist))

</code></pre>

<p>Now I want to reduce the time for this process. Is there any way to optimize this?</p>
",Preprocessing of the text & Tokenization,reduce time filtering article dataset trying filter dataset contains nearly k article article want filter stop word punctuation process taking long time already filtered dataset took hour got another dataset filter contains k article using python anaconda environment pc configuration th gen core gb ram nvidia mx gpu filter dataset wrote code take article dataset tokenize word remove stop word punctuation number want reduce time process way optimize
Delete rows with blank values after performing unnest_tokens and remove stopwords?,"<p>Here is my df:</p>

<pre><code>df &lt;- structure(list(id = 1:50, strain_id = c(6L, 6L, 7L, 12L, 19L, 
35L, 81L, 100L, 100L, 100L, 100L, 100L, 100L, 100L, 100L, 100L, 
100L, 123L, 123L, 123L, 123L, 123L, 123L, 123L, 123L, 123L, 123L, 
123L, 202L, 202L, 202L, 202L, 202L, 202L, 202L, 202L, 202L, 202L, 
202L, 246L, 246L, 246L, 246L, 246L, 246L, 246L, 246L, 246L, 246L, 
246L), name = c(""Anorexia and Cachexia"", ""Autoimmune Diseases and Inflammation"", 
""Psychiatric Symptoms"", ""Autoimmune Diseases and Inflammation"", 
""Pain"", ""Autoimmune Diseases and Inflammation"", ""Dependency and Withdrawal"", 
""Anorexia and Cachexia"", ""Spasticity"", ""Movement Disorders"", 
""Pain"", ""Glaucoma"", ""Epilepsy"", ""Asthma"", ""Dependency and Withdrawal"", 
""Psychiatric Symptoms"", ""Autoimmune Diseases and Inflammation"", 
""Nausea and Vomiting"", ""Anorexia and Cachexia"", ""Spasticity"", 
""Movement Disorders"", ""Pain"", ""Glaucoma"", ""Epilepsy"", ""Asthma"", 
""Dependency and Withdrawal"", ""Psychiatric Symptoms"", ""Autoimmune Diseases and Inflammation"", 
""Nausea and Vomiting"", ""Anorexia and Cachexia"", ""Spasticity"", 
""Movement Disorders"", ""Pain"", ""Glaucoma"", ""Epilepsy"", ""Asthma"", 
""Dependency and Withdrawal"", ""Psychiatric Symptoms"", ""Autoimmune Diseases and Inflammation"", 
""Nausea and Vomiting"", ""Anorexia and Cachexia"", ""Spasticity"", 
""Movement Disorders"", ""Pain"", ""Glaucoma"", ""Epilepsy"", ""Asthma"", 
""Dependency and Withdrawal"", ""Psychiatric Symptoms"", ""Autoimmune Diseases and Inflammation""
), rating = c(4, 4, 5, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 3, 5, 5, 
5, 3, 3, 5, 5, 4, 3, 4, 4, 4, 3, 4, 3, 3, 2, 3, 4, 4, 3, 2, 5, 
3, 3, 3, 3, 4, 4, 3, 5, 3, 1, 3, 4, 3), dose = c(3, 3, 3, 3, 
3, 3, 1, 3, 2, 1, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 
3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 1, 2, 2, 1, 3, 2, 
3, 2, 2, 3), info = c(""Affects / helps even in small doses very well at / against Anorexia and Cachexia."", 
""Affects / helps even in small doses very well at / against Autoimmune Diseases and Inflammation."", 
""Affects / helps even in small doses extremly well at / against Psychiatric Symptoms."", 
""Affects / helps even in small doses extremly well at / against Autoimmune Diseases and Inflammation."", 
""Affects / helps even in small doses very well at / against Pain."", 
""Affects / helps even in small doses extremly well at / against Autoimmune Diseases and Inflammation."", 
""Affects / helps only in heavy doses extremly well at / against Dependency and Withdrawal."", 
""Affects / helps even in small doses extremly well at / against Anorexia and Cachexia."", 
""Affects / helps in average doses very well at / against Spasticity."", 
""Affects / helps only in heavy doses extremly well at / against Movement Disorders."", 
""Affects / helps in average doses extremly well at / against Pain."", 
""Affects / helps in average doses very well at / against Glaucoma."", 
""Affects / helps in average doses very well at / against Epilepsy."", 
""Affects / helps even in small doses well at / against Asthma."", 
""Affects / helps in average doses extremly well at / against Dependency and Withdrawal."", 
""Affects / helps in average doses extremly well at / against Psychiatric Symptoms."", 
""Affects / helps in average doses extremly well at / against Autoimmune Diseases and Inflammation."", 
""Affects / helps in average doses well at / against Nausea and Vomiting."", 
""Affects / helps in average doses well at / against Anorexia and Cachexia."", 
""Affects / helps even in small doses extremly well at / against Spasticity."", 
""Affects / helps even in small doses extremly well at / against Movement Disorders."", 
""Affects / helps in average doses very well at / against Pain."", 
""Affects / helps in average doses well at / against Glaucoma."", 
""Affects / helps in average doses very well at / against Epilepsy."", 
""Affects / helps even in small doses very well at / against Asthma."", 
""Affects / helps even in small doses very well at / against Dependency and Withdrawal."", 
""Affects / helps in average doses well at / against Psychiatric Symptoms."", 
""Affects / helps in average doses very well at / against Autoimmune Diseases and Inflammation."", 
""Affects / helps in average doses well at / against Nausea and Vomiting."", 
""Affects / helps in average doses well at / against Anorexia and Cachexia."", 
""Affects / helps in average doses low at / against Spasticity."", 
""Affects / helps in average doses well at / against Movement Disorders."", 
""Affects / helps in average doses very well at / against Pain."", 
""Affects / helps in average doses very well at / against Glaucoma."", 
""Affects / helps in average doses well at / against Epilepsy."", 
""Affects / helps even in small doses low at / against Asthma."", 
""Affects / helps in average doses extremly well at / against Dependency and Withdrawal."", 
""Affects / helps in average doses well at / against Psychiatric Symptoms."", 
""Affects / helps in average doses well at / against Autoimmune Diseases and Inflammation."", 
""Affects / helps in average doses well at / against Nausea and Vomiting."", 
""Affects / helps only in heavy doses well at / against Anorexia and Cachexia."", 
""Affects / helps in average doses very well at / against Spasticity."", 
""Affects / helps in average doses very well at / against Movement Disorders."", 
""Affects / helps only in heavy doses well at / against Pain."", 
""Affects / helps even in small doses extremly well at / against Glaucoma."", 
""Affects / helps in average doses well at / against Epilepsy."", 
""Affects / helps even in small doses very low at / against Asthma."", 
""Affects / helps in average doses well at / against Dependency and Withdrawal."", 
""Affects / helps in average doses very well at / against Psychiatric Symptoms."", 
""Affects / helps even in small doses well at / against Autoimmune Diseases and Inflammation.""
), votes = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L)), row.names = c(NA, 50L), class = ""data.frame"")
</code></pre>

<p>And I need to work on the <code>name</code> column.</p>

<pre><code>df %&gt;%
  tidytext::unnest_tokens(input = name, 
                          output = word, 
                          token = ""words"", 
                          format = ""text"", 
                          drop = T, 
                          to_lower = T) %&gt;%
  dplyr::mutate(word = sapply(word, tm::removePunctuation, ucp = T),
                word = tm::removeWords(word, stopwords(""en"")),
                word = tm::stripWhitespace(word)) %&gt;%
  dplyr::filter(!word == """")
</code></pre>

<p>Please advise which function or setting should I use to avoid filtering (<code>dplyr::filter(!word == """")</code>) and remove rows with blank values.</p>

<p>In other words I want my code automatically (using a setting or function) to do filtering of rows with empty values in specific columns.</p>
",Preprocessing of the text & Tokenization,delete row blank value performing unnest token remove stopwords df need work column please advise function setting use avoid filtering remove row blank value word want code automatically using setting function filtering row empty value specific column
how to approach the project which is about analyzing call records and getting meaningful results about the topic,"<p>I am analyzing the call records and try to use doc2vec I cant find the appropriate way to apply</p>

<p>I tried to convert words to root later i will try to get rid of stop words(which are rooted).</p>

<p>I desire to understand that each  what the conversation is about(that can be a few or more words).Can you suggest me a certain way or sample project ?</p>
",Preprocessing of the text & Tokenization,approach project analyzing call record getting meaningful result topic analyzing call record try use doc vec cant find appropriate way apply tried convert word root later try get rid stop word rooted desire understand conversation word suggest certain way sample project
In R distance between two sentences: Word-level comparison by minimum edit distance,"<p>While trying to learn R, I want to implement the algorithm below in R. Consider the two lists below:</p>

<pre><code>List 1: ""crashed"", ""red"", ""car""
List 2: ""crashed"", ""blue"", ""bus""
</code></pre>

<p>I want to find out how many actions it would take to transform 'list1' into 'list2'.
As you can see I need only two actions: 
<code>1. Replace ""red"" with ""blue"".</code>
<code>2. Replace ""car"" with ""bus"".</code></p>

<p>But, how we can find the number of actions like this automatically.
We can have several actions to transform the sentences: ADD, REMOVE, or REPLACE the words in the list.
Now, I will try my best to explain how the algorithm should work:</p>

<p>At the first step: I will create a table like this: </p>

<p>rows: i= 0,1,2,3,
   columns: j = 0,1,2,3</p>

<p><code>(example: value[0,0] = 0 , value[0, 1] = 1 ...)</code></p>

<pre><code>                 crashed    red     car
         0          1        2       3

crashed  1
blue     2
bus      3
</code></pre>

<p>Now, I will try to fill the table. Please, note that each cell in the table shows the number of actions we need to do to reformat the sentence (ADD, remove, or replace). 
Consider the interaction between <strong>""crashed"" and ""crashed""</strong> (<code>value[1,1]</code>), obviously we don't need to change it so the <strong>value will be '0'.</strong> Since they are the same words. Basically, we got the <strong>diagonal value</strong> = <code>value[0,0]</code></p>

<pre><code>                 crashed    red     car
         0          1        2       3

crashed  1          0
blue     2
bus      3
</code></pre>

<p>Now, consider ""crashed"" and the second part of the sentence which is ""red"". Since they are not the same word we can use calculate the number of changes like this :</p>

<pre><code>min{value[0,1] , value[0,2] and value[1,1]} + 1 
min{ 1, 2, 0} + 1 = 1 
</code></pre>

<p>Therefore, we need to just remove ""red"".
So, the table will look like this:</p>

<pre><code>                 crashed    red     car
         0          1        2       3

crashed  1          0        1
blue     2  
bus      3
</code></pre>

<p>And we will continue like this :
""crashed"" and ""car"" will be :</p>

<pre><code>min{value[0,3], value[0,2] and value[1,2]} + 1 
min{3, 2, 1} +1 = 2
</code></pre>

<p>and the  table will be:</p>

<pre><code>                 crashed    red     car
         0          1        2       3

crashed  1          0        1       2
blue     2  
bus      3
</code></pre>

<p>And we will continue to do so. the final result will be :</p>

<pre><code>             crashed    red     car
         0      1        2       3

crashed  1      0        1       2
blue     2      1        1       2
bus      3      2        2       2 
</code></pre>

<p>As you can see the last number in the table shows the distance between two sentences: <strong>value[3,3] = 2</strong></p>

<p>Basically, the algorithm should look like this:</p>

<pre><code> if (characters_in_header_of_matrix[i]==characters_in_column_of_matrix [j] &amp; 
                                            value[i,j] == value[i+1][j-1] )

then {get the 'DIAGONAL VALUE' #diagonal value= value[i, j-1]}

else{
value[i,j] = min(value[i-1, j], value[i-1, j-1],  value[i, j-1]) + 1
 }
  endif
</code></pre>

<p>for finding the difference between the elements of two lists that you can see in the header and the column of the matrix, I have used the <code>strcmp()</code> function which will give us a boolean value(TRUE or FALSE) while comparing the words. But, I fail at implementing this. 
I'd appreciate your help on this one, thanks.</p>
",Preprocessing of the text & Tokenization,r distance two sentence word level comparison minimum edit distance trying learn r want implement algorithm r consider two list want find many action would take transform list list see need two action find number action like automatically several action transform sentence add remove replace word list try best explain algorithm work first step create table like row column j try fill table please note cell table show number action need reformat sentence add remove replace consider interaction crashed crashed obviously need change value since word basically got diagonal value consider crashed second part sentence red since word use calculate number change like therefore need remove red table look like continue like crashed car table continue final result see last number table show distance two sentence value basically algorithm look like finding difference element two list see header column matrix used function give u boolean value true false comparing word fail implementing appreciate help one thanks
Where may I find a list of words used to describe relations and relationships?,"<p>I'm in a nlp project and there are millions of sentences which contains two entity. I want to find whether two entities have relationships or not in each sentence.</p>

<p>So I want to find a word list like:</p>

<pre><code>['related to','induced by','the treatment of','The effects of','the treatment of','treated with','best for','in response to','approved for','response with','associated with','efficacy of ','in treating','applied to','efficacy in','efficacy and safety','efficacy at','impact on','approved','causing','but none of ','linked to','cause of','associated with','leading to','caused by','the relationship between','responsible for']
</code></pre>

<p>I have search github but I can't find it. </p>

<p>What should I do?</p>
",Preprocessing of the text & Tokenization,may find list word used describe relation relationship nlp project million sentence contains two entity want find whether two entity relationship sentence want find word list like search github find
How can I apply stemming into a dictionary?,"<p>I'm working in some kind of NLP. I compare a daframe of articles with inputs words. The main goal is classify text if a bunch of words were found</p>

<p>I've tried to extract the values in the dictionary and convert into a list and then apply stemming to it. The problem is that later I'll do another process to split and compare according to the keys. I think if more practical to work directly in the dictionary.</p>

<pre><code>search = {'Tecnology' : ['computer', 'digital', 'sistem'], 'Economy' : ['bank', 'money']}
words_list = list()
for key in search.keys():
    words_list.append(search[key])
search_values = [val for sublist in words_list for val in sublist]
search_values_stem = [stemmer.stem(word) for word in test]

</code></pre>

<p>I expect a dictionary stemmed to compare directly with the column of the articles stemmed</p>
",Preprocessing of the text & Tokenization,apply stemming dictionary working kind nlp compare daframe article input word main goal classify text bunch word found tried extract value dictionary convert list apply stemming problem later another process split compare according key think practical work directly dictionary expect dictionary stemmed compare directly column article stemmed
Could pipeline become transformer?,"<p>I need two processes of tokenizing and removing Stop words:</p>

<pre><code>val tokenizer = new Tokenizer()
                  .setInputCol(""seg_text"")
                  .setOutputCol(""raw_words"")

val remover = new StopWordsRemover()
                  .setInputCol(""raw_words"")
                  .setStopWords(stop_words) 
                  .setOutputCol(""words"")
</code></pre>

<p>and created pipeline:</p>

<pre><code>val pipeline = new Pipeline()
                  .setStages(Array(tokenizer, remover))
</code></pre>

<p>finally, I wish get the output by this pipeline. I don't know how, or maybe can't. </p>
",Preprocessing of the text & Tokenization,could pipeline become transformer need two process tokenizing removing stop word created pipeline finally wish get output pipeline know maybe
Regular expression in R extracts the part between two matching strings (NOT intendet),"<p>I am trying to clean some documents in R and extract in text citations. If there are two citations close to each other (with only a few words in between) R extracts the part beween the matching strings as well. It might have something to do with my regex being greedy, but I am not sure how to solve this. </p>

<p>My regex looks like this </p>

<pre><code>""\\([.A-Z].*\\,[[:space:]]([0-9]{4}[a-z]?)\\)|\\([.A-Z].*\\,[[:space:]][n.d.].*\\)|\\([.A-Z].*\\,[[:space:]]\\(?forthcoming\\)?\\)|\\([0-9]{4}[a-z]?\\)""
</code></pre>

<p>This is some example data</p>

<pre><code>s &lt;- ""Author (Author, 1996), Text""
t &lt;- ""Author (Author, 1996a), Text""
r &lt;- ""Author (Bla usw, forthcoming), Title Analysis, Paris""
k &lt;- ""Author (Author, n.d.), text""
m &lt;- ""text (Lara, Bera and Ceta, 2009), I dont want R to grab this part (Whatever, 2003) text goes on""  
n &lt;- ""Smthing (Author, forthcoming some other text I do not want extracted because it is not a citation but some random numbers and text 1234) stmth""
n &lt;- ""Smthing (Author, forthcoming) stmth""
i &lt;- ""Authors or something, A B and C. (2012a), text,""

l &lt;- list(s,t,r, k, m, i,n )
</code></pre>

<p>To check if it works I used that: </p>

<pre><code>regmatches(l, regexpr(""\\([.A-Z].*\\,[[:space:]]([0-9]{4}[a-z]?)\\)| \\([.A-Z].*\\,[[:space:]][n.d.].*\\)|\\([.A-Z].*\\,[[:space:]]\\(?forthcoming\\)?\\)|\\([0-9]{4}[a-z]?\\)"", l))
</code></pre>

<p>I only want the part in the parenthesis, but only this (there are more parenthesis in the text, so I cannot just extract those). </p>

<p>So I want it to extract </p>

<pre><code>(Author, 1996)
(Author, 1996a)
(Author, n.d.)
(Lara, Bera and Ceta, 2009)
(Whatever, 2003)
(Author, forthcoming)
(2012a)
</code></pre>

<p>but it currently extracts the middle part in this one string here as well. I want the text in both parenthesis as well as in the other strings, but not the part between them, that says ""I dont want R to grab this part"".</p>

<pre><code>(Lara, Bera and Ceta, 2009), I dont want R to grab this part (Whatever, 2003)
</code></pre>

<p>How do I prevent that?</p>
",Preprocessing of the text & Tokenization,regular expression r extract part two matching string intendet trying clean document r extract text citation two citation close word r extract part beween matching string well might something regex greedy sure solve regex look like example data check work used want part parenthesis parenthesis text extract want extract currently extract middle part one string well want text parenthesis well string part say dont want r grab part prevent
I have a dataset with one target column and two text columns.It is an nlp problem which i am trying to solve through deep learning,"<p>I am dealing with a dataset where I have 3 fields. One field is my target field and the other two field are text fields. It is basically an NLP based problem statement. I am trying to approach a deep learning mechanism but while taking into account the two text fields I am getting an error at tokenizing the X_train data post train test split.
I have already read the dataset and label encoded the target column. I have cleaned up the text columns and used stemmer to further lemmatize them. I have stored the two text columns in X and the target column in y. Then, I have performed a train test split.After that I am trying to tokenize X_train which is giving me an error. Review Text and Review Title are text columns.</p>

<pre><code>df=pd.read_csv('train_amazon.csv')
df.head(10)

df['topic'].nunique()

df['topic'].value_counts()

df['Review Text'].isnull().any()

df['Review Title'].isnull().any()

df['topic'].isnull().any()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['topic'] = le.fit_transform(df['topic'])

df.head()

le.classes_

dummy_y = pd.get_dummies(df['topic']).values

X =df.iloc[:, :-1].values
y = dummy_y

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 101)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
vocabulary_size = len(tokenizer.word_index) + 1
vocabulary_size
</code></pre>

<p>I am getting the error as follows:-</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-67-7ab7cb886988&gt; in &lt;module&gt;
      1 tokenizer = Tokenizer()
----&gt; 2 tokenizer.fit_on_texts(X_train)
      3 vocabulary_size = len(tokenizer.word_index) + 1
      4 vocabulary_size

~\Anaconda3\lib\site-packages\keras_preprocessing\text.py in fit_on_texts(self, texts)
    221                                             self.filters,
    222                                             self.lower,
--&gt; 223                                             self.split)
    224             for w in seq:
    225                 if w in self.word_counts:

~\Anaconda3\lib\site-packages\keras_preprocessing\text.py in text_to_word_sequence(text, filters, lower, split)
     41     """"""
     42     if lower:
---&gt; 43         text = text.lower()
     44 
     45     if sys.version_info &lt; (3,):

AttributeError: 'numpy.ndarray' object has no attribute 'lower'
</code></pre>

<p>My X_train has a shape (4469,2)</p>

<p>and my X_train looks like :-</p>

<pre><code>array([['use sinc seal miss', 'broken seal'],
       ['took week immedi effect 1 2 hour hour ingest includ tingl extrem slight relax probabl help anxieti much like medic numb make care less what is bother you howev product made difficult focus short term memori sever impact bout week stuff good detail orient job trust take longer sinc long term effect like unknown care take unregul supplements!!!!',
        'careless'],
       ['smell aw mean rancid could make sick sooooo annoy wish could money back',
        'rancid pill'],
       ...,
       ['didn t realiz serv size capsul purchas huge deal fault prefer take pill vitamin idea it s work help',
        'vitamin yeah'],
       ['horribl taste! wast money', 'horribl fake tast'],
       ['nasti stuff work with thick dropper doesn t work well finger bottl leav sticki mess don t lick bitter',
        'nasti']], dtype=object)
</code></pre>
",Preprocessing of the text & Tokenization,dataset one target column two text column nlp problem trying solve deep learning dealing dataset field one field target field two field text field basically nlp based problem statement trying approach deep learning mechanism taking account two text field getting error tokenizing x train data post train test split already read dataset label encoded target column cleaned text column used stemmer lemmatize stored two text column x target column performed train test split trying tokenize x train giving error review text review title text column getting error follows x train ha shape x train look like
POS tagging a single word in spaCy,"<p>spaCy POS tagger is usally used on entire sentences. Is there a way to efficiently apply a unigram POS tagging to a single word (or a list of single words)?</p>

<p>Something like this:</p>

<pre><code>words = [""apple"", ""eat"", good""]
tags = get_tags(words) 
print(tags)
&gt; [""NNP"", ""VB"", ""JJ""]
</code></pre>

<p>Thanks.</p>
",Preprocessing of the text & Tokenization,po tagging single word spacy spacy po tagger usally used entire sentence way efficiently apply unigram po tagging single word list single word something like thanks
How to locate specific sequences of words in a sentence efficiently,"<p>The problem is to find a time efficient function that receives as inputs a sentence of words and a list of sequences of varying amounts of words (also known as ngrams) and returns for every sequence a list of indexes indicating where they occur in the sentence, and do it as efficiently as possible for large amounts of sequences.</p>

<p>What I ultimately want is to replace the occurrences of ngrams in the sentence for a concatenation of the words in the sequence by ""_"".</p>

<p>For example if my sequences are [""hello"", ""world""] and [""my"", ""problem""], and the sentence is ""hello world this is my problem can you solve it please?"" the function should return ""hello_world this is my_problem can you solve it please?""</p>

<p>What I did is group the sequences by the amount of words each have and save that in a dictionary where the key is the amount and the value is a list of the sequences of that length.</p>

<p>The variable ngrams is this dictionary:</p>

<pre class=""lang-py prettyprint-override""><code>def replaceNgrams(line, ngrams):
    words = line.split()
    #Iterates backwards in the length of the sequences
    for n in list(ngrams.keys())[::-1]: #O(L*T)
        newWords = []
        if len(words) &gt;= n:
            terms = ngrams[n]
            i = 0
            while i &lt; len(words)+1-n: #O(L*Tn)
                #Gets a sequences of words from the sentences of the same length of the ngrams currently checking
                nwords = words[i:i+n].copy()
                #Checks if that sequence is in my list of sequences
                if nwords in terms: #O(Tn)
                    newWords.append(""_"".join(nwords))
                    i+=n
                else:
                    newWords.append(words[i])
                    i+=1
            newWords += words[i:].copy()
            words = newWords.copy()
    return "" "".join(words)
</code></pre>

<p>This works as desired but I have too many sequences and too many lines to apply this too and this is way too slow for me (it would take a month to finish).</p>
",Preprocessing of the text & Tokenization,locate specific sequence word sentence efficiently problem find time efficient function receives input sentence word list sequence varying amount word also known ngrams return every sequence list index indicating occur sentence efficiently possible large amount sequence ultimately want replace occurrence ngrams sentence concatenation word sequence example sequence hello world problem sentence hello world problem solve please function return hello world problem solve please group sequence amount word save dictionary key amount value list sequence length variable ngrams dictionary work desired many sequence many line apply way slow would take month finish
"How to split a sentence string into words, but also make punctuation a separate element","<p>I'm currently trying to tokenize some language data using Python and was curious if there was an efficient or built-in method for splitting strings of sentences into separate words and also separate punctuation characters. For example:</p>

<pre><code>'Hello, my name is John. What's your name?'
</code></pre>

<p>If I used <code>split()</code> on this sentence then I would get</p>

<pre><code>['Hello,', 'my', 'name', 'is', 'John.', ""What's"", 'your', 'name?']
</code></pre>

<p>What I want to get is:</p>

<pre><code>['Hello', ',', 'my', 'name', 'is', 'John', '.', ""What's"", 'your', 'name', '?']
</code></pre>

<p>I've tried to use methods such as searching the string, finding punctuation, storing their indices, removing them from the string and then splitting the string, and inserting the punctuation accordingly but this method seems too inefficient especially when dealing with large corpora.</p>

<p>Does anybody know if there's a more efficient way to do this?</p>

<p>Thank you.</p>
",Preprocessing of the text & Tokenization,split sentence string word also make punctuation separate element currently trying tokenize language data using python wa curious wa efficient built method splitting string sentence separate word also separate punctuation character example used sentence would get want get tried use method searching string finding punctuation storing index removing string splitting string inserting punctuation accordingly method seems inefficient especially dealing large corpus doe anybody know efficient way thank
Select sentences in python,"<p>I am trying to filter a text to keep only the sentences that include at least one word from a list of words. </p>

<p>The script is working but picking some wrong words. For example, my list includes the word ""RESULT"" and I am picking sentences with the word ""RESULTING"". Here is the section of the code. </p>

<pre><code>    sentences = split_into_sentences(doc)
    for sentence in sentences:
        if any(ele in sentence for ele in keywords):
            sentences_to_write.append(sentence)
</code></pre>

<p>where sentences is the list of sentences (text) and keywords is the list of keywords.</p>
",Preprocessing of the text & Tokenization,select sentence python trying filter text keep sentence include least one word list word script working picking wrong word example list includes word result picking sentence word resulting section code sentence list sentence text keywords list keywords
Gensim LDAmodel error: NaN and all topics the same,"<p>Been following the documentation <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">here</a> and as well as in this link: <a href=""https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"" rel=""nofollow noreferrer"">Machine Learning Gensim Tutorial</a> and I'm at a complete loss for why this is happening. After tokenizing and lemmatizing my sentences, I put the sentences through a phraser, created a Dictionary, and inserted all the right variables into the model. Here is a sampling of my code:</p>

<pre><code>tokens =  [[euid, sent, gensim.parsing.preprocessing.preprocess_string(sent.lower(), filters=[strip_punctuation,
        strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short, wordnet_stem])] for sent in sentences]
#these filters are all default gensim filters except for wordnet_stem, which uses a WordNetLemmatizer

 bigram = gensim.models.Phrases(bag_of_words)
bigram_mod = gensim.models.phrases.Phraser(bigram)
</code></pre>

<p>Sample token list looks like this: <code>['beautiful', 'Manager', 'tree', 'caring', 'great_place']</code> (completely made-up list)</p>

<pre><code>texts = [bigram_mod[t] for t in bag_of_words]
id2word = corpora.Dictionary(texts)
sent_wordfreq = [id2word.doc2bow(sent) for sent in texts]

lda_model = gensim.models.ldamodel.LdaModel(corpus=sent_wordfreq,
                                            id2word=id2word,
                                            num_topics=5,
                                            update_every=1,
                                            alpha='auto',
                                            per_word_topics=True)
</code></pre>

<p>Here are the topics I'm getting:</p>

<blockquote>
  <p>[(0, 'nan*""discovered"" + nan*""gained"" + nan*""send"" + ...
  (1, 'nan*""discovered"" + nan*""gained"" + nan*""send"" + ...
  and this continues on 3 more times</p>
</blockquote>

<p>So not only are all the topics the same, each's weight is nan. What could be the issue?</p>
",Preprocessing of the text & Tokenization,gensim ldamodel error nan topic following documentation well link machine learning gensim tutorial complete loss happening tokenizing lemmatizing sentence put sentence phraser created dictionary inserted right variable model sampling code sample token list look like completely made list topic getting nan discovered nan gained nan send nan discovered nan gained nan send continues time topic weight nan could issue
Grouping of text in pandas,"<p>Hello guys i am new to NLP and am totally lost. All help is much appreciated.I Have data set of dimension 5000x3. So my first column is the a timestamp and the second column is Title of the essay. I need to group them into similar titles and introduce a third column with a group number based on the title similarity and newness and frequency of  the topics occurrance . Can anyone help me with this? </p>

<pre><code>this is how my data set looks
    Timestamp       Essaytitle               
    20156032125     life of Pablo           
    21056032127     Plays of Shakespeare    
    21056032127     French Revolution       
</code></pre>

<p>This is what ive done : 1. NLP preprocessing(tokenization and stop word removal in the essay column). 2. tf_idf vectorize 3.Cosinesimilarity 4.k means clustering based off cosine similarity. I'm pretty sure what is doing is wrong. Help!!!! </p>

<pre><code>import pandas as pd
from nltk.tokenize import RegexpTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
token = RegexpTokenizer(r'[a-zA-Z0-9]+')
tf=TfidfVectorizer(lowercase=True,stop_words=words,ngram_range = (1,1),tokenizer = token.tokenize)

 data['EssayTitle']=data['EssayTitle'].astype(str)
EssayTitle_tf_idf=tf.fit_transform(data['EssayTitle'])
from sklearn.metrics.pairwise import cosine_similarity
sim=cosine_similarity(EssayTitle_tf_idf)
from sklearn.cluster import KMeans

true_k = 2
model = KMeans(n_clusters=true_k)
model.fit(EssayTitle_tf_idf)
clusters = model.labels_.tolist()
data['Clusters']=clusters
data['Clusters'].value_counts()

My expected output should be:
 Timestamp       Essaytitle           Group
20156032125     Shakespeare             2
21056032127     Plays of Shakespeare    2
21056032127     French Revolution       1
</code></pre>
",Preprocessing of the text & Tokenization,grouping text panda hello guy new nlp totally lost help much appreciated data set dimension x first column timestamp second column title essay need group similar title introduce third column group number based title similarity newness frequency topic occurrance anyone help ive done nlp preprocessing tokenization stop word removal essay column tf idf vectorize cosinesimilarity k mean clustering based cosine similarity pretty sure wrong help
Is there a tool that allow to get a desired inflected form from a base word (lemne)?,"<p>I need a tool that takes in argument a word e.g ""fast"" and another argument specifying the requested part of speech form e.g an adverb
which would output ""fastly"".</p>

<p>In fact, stemming and lemmatization are a special case of the NLP task I need.</p>

<p>If it exists, does someone know how it's called?
And how correct it is statistically?</p>

<p>The ideal tool would take any inflected form as input.
A less ideal tool but which would still help me would take a base word (lemne) as input.</p>

<p>A bonus would be to be able to not only specify part of speech (e.g verb) but to specify time conjugate and plurality e.g:
arg1: eat arg2: POS->verb time->preterit|singular
output: ate
or arg1: eat arg2: POS->verb time->past participle
output: eaten</p>
",Preprocessing of the text & Tokenization,tool allow get desired inflected form base word lemne need tool take argument word e g fast another argument specifying requested part speech form e g adverb would output fastly fact stemming lemmatization special case nlp task need exists doe someone know called correct statistically ideal tool would take inflected form input le ideal tool would still help would take base word lemne input bonus would able specify part speech e g verb specify time conjugate plurality e g arg eat arg po verb time preterit singular output ate arg eat arg po verb time past participle output eaten
Fast way to check for words in markdown?,"<p>I want to scan text for the presence of words from a list of words. This would be straightforward if the text were unformatted, but it is markdown-formatted. At the moment, I'm accomplishing this with regex:</p>

<pre class=""lang-py prettyprint-override""><code>import re

text = 'A long text string with **markdown** formatting.'
words = ['markdown', 'markup', 'marksideways']
found_words = []

for word in words:
    word_pattern = re.compile(r'(^|[ \*_])' + word + r'($|[ \*_.!?])', (re.I | re.M))
    match = word_pattern.search(text)
    if match:
        found_words.append(word)
</code></pre>

<p>I'm working with a very long list of words (a sort of denylist) and very large candidate texts, so speed is important to me. Is this a relatively efficient and speedy way to do this? Is there a better approach?</p>
",Preprocessing of the text & Tokenization,fast way check word markdown want scan text presence word list word would straightforward text unformatted markdown formatted moment accomplishing regex working long list word sort denylist large candidate text speed important relatively efficient speedy way better approach
Too much time for spell checking in python,"<p>I have a dataframe with around 200,000 rows and each line has approximetely 30 tokenized words. I am trying to fix spelling mistakes, then lemmatize them.</p>

<p>Some words are not in the dictionary so, if the frequency of them is too high, I just pass that word, if not, I correct it.</p>

<pre><code>spell = SpellChecker() 
def spelling_mistake_corrector(word):
    checkedWord = spell.correction(word)
    if freqDist[checkedWord] &gt;= freqDist[word]:
    word = checkedWord
return word

def correctorForAll(text):
    text = [spelling_mistake_corrector(word) for word in text]
    return text

lemmatizer = WordNetLemmatizer()
def lemmatize_words(text):
    text = [lemmatizer.lemmatize(word) for word in text]
    text = [word for word in text if len(word) &gt; 2] #filtering 1 and 2 letter words out
    return text

def apply_corrector_and_lemmatizer(text):
    return lemmatize_words(correctorForAll(text))

df['tokenized'] = df['tokenized'].apply(apply_corrector_and_lemmatizer)
</code></pre>

<p>The thing is: this code is running on colab for 3 hours, what should I do to improve run time? Thanks!</p>
",Preprocessing of the text & Tokenization,much time spell checking python dataframe around row line ha approximetely tokenized word trying fix spelling mistake lemmatize word dictionary frequency high pas word correct thing code running colab hour improve run time thanks
How can i make spacy not produce the -PRON- lemma?,"<p>I am using spacy in order to lemmatize a large amount of tweets.  However when i lemmatize words like ""I"", the token -PRON- is produced.  How can i avoid that?</p>
",Preprocessing of the text & Tokenization,make spacy produce pron lemma using spacy order lemmatize large amount tweet however lemmatize word like token pron produced avoid
How do I elegantly remove ellipses of n-length from strings (NLP with spacy)?,"<p>I am currently performing data cleaning on this <a href=""https://www.kaggle.com/team-ai/spam-text-message-classification"" rel=""nofollow noreferrer"">spam text message dataset</a>. There are many ellipses in these text message, for example:</p>

<pre><code>mystr = 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'
</code></pre>

<p>As you can see, there are ellipses with 2 periods (<code>..</code>) or 3 periods (<code>...</code>)</p>

<p>My initial solution was to write a function <code>spacy_tokenizer</code> that tokenizes my strings, removes stopwords as well as punctuations:</p>

<pre class=""lang-py prettyprint-override""><code>import spacy
nlp = spacy.load('en_core_web_sm')
from nltk.corpus import stopwords

stopWords = set(stopwords.words('english'))
print(stopWords)

import string
punctuations = string.punctuation
</code></pre>

<pre class=""lang-py prettyprint-override""><code>def spacy_tokenizer(sentence):
    # Create token object
    mytokens = nlp(sentence)
    # Case normalization and Lemmatization
    mytokens = [ word.lemma_.lower() if word.lemma_ != ""-PRON-"" else word.lower_ for word in mytokens ]
    # Remove stop words and punctuations
    mytokens = [ word.strip(""."") for word in mytokens if word not in stopWords and word not in punctuations ]
    # return preprocessed list of tokens
    return mytokens
</code></pre>

<p>However, this function doesn't get rid of the ellipses</p>

<pre><code>IN: print(spacy_tokenizer(mystr))
OUT: ['go', 'jurong', 'point', 'crazy', '', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '', 'cine', 'get', 'amore', 'wat', '']
</code></pre>

<p>As you can see, there are tokens with <code>len(token) = 0</code> that appear as <code>''</code></p>

<p>My workaround is to add another list comprehension to <code>spacy_tokenizer</code> that looks something like this: <code>[ word for word in mytokens if len(word) &gt; 0]</code></p>

<pre><code>def spacy_tokenizer(sentence):
    # Create token object
    mytokens = nlp(sentence)
    # Case normalization and Lemmatization
    mytokens = [ word.lemma_.lower() if word.lemma_ != ""-PRON-"" else word.lower_ for word in mytokens ]
    # Remove stop words and punctuations
    mytokens = [ word.strip(""."") for word in mytokens if word not in stopWords and word not in punctuations ]
    # remove empty strings
    mytokens = [ word for word in mytokens if len(word) &gt; 0]
    return mytokens

IN: print(spacy_tokenizer(mystr))
OUT: ['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'get', 'amore', 'wat']


</code></pre>

<p>So the new function gives the expect result, bu it's not the most elegant solution I think. Does anyone have any alternative ideas?</p>
",Preprocessing of the text & Tokenization,elegantly remove ellipsis n length string nlp spacy currently performing data cleaning spam text message dataset many ellipsis text message example see ellipsis period period initial solution wa write function tokenizes string remove stopwords well punctuation however function get rid ellipsis see token appear workaround add another list comprehension look something like new function give expect result bu elegant solution think doe anyone alternative idea
"Analyze text (lemmatization, edit distance)","<p>I need to analyze the text to exist in it banned words. Suppose the black list is the word: ""Forbid"". The word has many forms. In the text the word can be, for example: ""forbidding"", ""forbidden"", ""forbad"". To bring the word to the initial form, I use a process lemmatization. Your suggestions?</p>

<p><strong>What about typos?</strong><br>
For example: ""F0rb1d"".  I  think use damerau–Levenshtein or another. You suggestions?</p>

<p><strong>And what if the text is written as follows</strong>:<br>
""ForbiddenInformation.Privatecorrespondenceofthecompany.""  OR
""F0rb1dden1nformation.Privatecorresp0ndenceofthec0mpany."" (yes, without whitespace)</p>

<p>How to solve this problem?<br>
Preferably fast algorithm, because text are processed in real time.<br>
And maybe what some tips to improve performance (how to store, etc)?</p>
",Preprocessing of the text & Tokenization,analyze text lemmatization edit distance need analyze text exist banned word suppose black list word forbid word ha many form text word example forbidding forbidden forbad bring word initial form use process lemmatization suggestion typo example f rb think use damerau levenshtein another suggestion text written follows forbiddeninformation privatecorrespondenceofthecompany f rb dden nformation privatecorresp ndenceofthec mpany yes without whitespace solve problem preferably fast algorithm text processed real time maybe tip improve performance store etc
How to fix tokenizing phrases such as T&amp;C from being split into &#39;T&#39; &#39;&amp;&#39; &#39;C&#39;,"<p>I'm trying to clean some text data ready for NLP techniques. I require patterns such as T&amp;C and S&amp;P to be left how they are. However, when I tokenize sentences it gets split into 'T' '&amp;' 'C' rather than 'T&amp;C' altogether.</p>

<p>I've tried looking for exemptions to the rule but cannot find a general way of completing this given any sequence i.e. FT&amp;P or S&amp;ST or S&amp;T</p>

<pre><code>import pandas as pd

from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords


en_stop = set(stopwords.words('english'))
en_stop = [word for word in en_stop]
[en_stop.append(x) for x in ['shall', 'should','please']]

def rem_stopwords(txt):
    words = [w for w in word_tokenize(txt) if w not in en_stop]
    return "" "".join(words)

rem_stopwords('what is f&amp;p doing in regards')
Out[163]: ['f', '&amp;', 'p', 'regards']
</code></pre>

<p>I want the output to be ['f&amp;p', 'regards']</p>
",Preprocessing of the text & Tokenization,fix tokenizing phrase c split c trying clean text data ready nlp technique require pattern c p left however tokenize sentence get split c rather c altogether tried looking exemption rule find general way completing given sequence e ft p st want output f p regard
How I can do stemming on a text file in node.js?,"<p>I want to use NLP stemming on a text file using node.js and then store it in some output file. Please let me know if you need more information.</p>

<p>I am using npm natural and PorterStemmer to do the same, but no success.</p>

<pre><code>function doStem(data){
    var natural = require('natural');
    //do procesing using WordTokenizer &amp; PorterStemmer.
    return nData;
}
</code></pre>

<p>Please let me know if I am using the correct approach or let me know if there is anything to be corrected.</p>

<p>UPDATE:</p>

<pre><code>function doStemming(data){ 
  var natural = require('natural'); 
  var tokenizer = new natural.WordTokenizer(); 
  var tokens = tokenizer.tokenize(data); 
  stemmer = natural.PorterStemmer(data);
  var nData = stemmer.stem(tokens); 
  //console.log(nData); 
  return nData; 
} 
</code></pre>

<p>This is what looks like I am doing. Please give some idea about how to do this using tokenization and PorterStemmer onto a text file but not on any specific word</p>
",Preprocessing of the text & Tokenization,stemming text file node j want use nlp stemming text file using node j store output file please let know need information using npm natural porterstemmer success please let know using correct approach let know anything corrected update look like please give idea using tokenization porterstemmer onto text file specific word
Using LSTM for large text,"<p>I have a dataset for detecting fake news that i got from kaggle( <a href=""https://www.kaggle.com/c/fake-news/data"" rel=""nofollow noreferrer"">https://www.kaggle.com/c/fake-news/data</a> ).
I want to use LSTM for the classification</p>

<p>The mean length of words in a single article is about 750 words. I have tried to remove punctuation, stop words, removed numbers. Preprocessing the text is also taking a very long time.</p>

<p>I'd like a method to feed large text into the LSTM using keras. What should i do to reduce computation time and not lose a lot of accuracy.</p>
",Preprocessing of the text & Tokenization,using lstm large text dataset detecting fake news got kaggle want use lstm classification mean length word single article word tried remove punctuation stop word removed number preprocessing text also taking long time like method feed large text lstm using kera reduce computation time lose lot accuracy
Removing stopwords in a list of a list,"<p>I would like to remove the stopwords that are in a list of a list while keeping the format the same (i.e. a list of a list)</p>

<p>Following is the code that I have already tried</p>

<pre class=""lang-py prettyprint-override""><code>sent1 = 'I have a sentence which is a list'
sent2 = 'I have a sentence which is another list'

from nltk.corpus import stopwords
stop_words = stopwords.words('english')

lst = [sent1, sent2]
sent_lower = [t.lower() for t in lst]

filtered_words=[]
for i in sent_lower:
    i_split = i.split()
    lst = []
    for j in i_split:
        if j not in stop_words:
            lst.append(j)
            "" "".join(lst)
            filtered_words.append(lst)
</code></pre>

<p>Current Output of filtered_words:</p>

<pre class=""lang-py prettyprint-override""><code>filtered_words
[['sentence', 'list'],
 ['sentence', 'list'],
 ['sentence', 'another', 'list'],
 ['sentence', 'another', 'list'],
 ['sentence', 'another', 'list']]
</code></pre>

<p>Desired Output of filtered_words:</p>

<pre class=""lang-py prettyprint-override""><code>filtered_words
[['sentence', 'list'],
 ['sentence', 'another', 'list']]
</code></pre>

<p>I am getting a duplicate of list. What might I be doing wrong in the loop? Also is there a better way of doing this rather than writing so many for loops?</p>
",Preprocessing of the text & Tokenization,removing stopwords list list would like remove stopwords list list keeping format e list list following code already tried current output filtered word desired output filtered word getting duplicate list might wrong loop also better way rather writing many loop
Pre-processing script not removing punctuation,"<p>I have a code which is supposed to pre-process a list of text documents. That is: Given a list of text documents, it returns a list with each text document pre-processed. But for some reason, it is not working to remove punctuation. </p>

<pre><code>import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download(""stopwords"")
nltk.download('punkt')
nltk.download('wordnet')


def preprocess(docs):
  """""" 
  Given a list of documents, return each documents as a string of tokens, 
  stripping out punctuation 
  """"""
  clean_docs = [clean_text(i) for i in docs]
  tokenized_docs = [tokenize(i) for i in clean_docs]
  return tokenized_docs

def tokenize(text):
  """""" 
  Tokenizes text -- returning the tokens as a string 
  """"""
  stop_words = stopwords.words(""english"")
  nltk_tokenizer = nltk.WordPunctTokenizer().tokenize
  tokens = nltk_tokenizer(text)  
  result = "" "".join([i for i in tokens if not i in stop_words])
  return result


def clean_text(text): 
  """""" 
  Cleans text by removing case
  and stripping out punctuation. 
  """"""
  new_text = make_lowercase(text)
  new_text = remove_punct(new_text)
  return new_text

def make_lowercase(text):
  new_text = text.lower()
  return new_text

def remove_punct(text):
  text = text.split()
  punct = string.punctuation
  new_text = "" "".join(word for word in text if word not in string.punctuation)
  return new_text

# Get a list of titles  
s1 = ""[UPDATE] I am tired""
s2 = ""I am cold.""

clean_docs = preprocess([s1, s2])
print(clean_docs)

</code></pre>

<p>This prints out: </p>

<p><code>['[ update ] tired', 'cold .']</code> </p>

<p>In other words, it does not strip out punctuation because ""["", ""]"", and ""."" all appear in the final product. </p>
",Preprocessing of the text & Tokenization,pre processing script removing punctuation code supposed pre process list text document given list text document return list text document pre processed reason working remove punctuation print word doe strip punctuation appear final product
Can someone explain me what is the meaning of all the code inside these parentheses: RegexpTokenizer(r&#39;\w+|$[0-9]+|\S+&#39;)?,"<p>I've been reading a book about NLP recently and in one part the author show me how to tokenize a piece of text. </p>

<p>And then he show me this code:</p>

<pre class=""lang-py prettyprint-override""><code>sent0 = """"""Thomas Jefferson began building Monticello at the age of 26.""""""
tokenizer = RegexpTokenizer(r'\w+|$[0-9]+|\S+')
print(tokenizer.tokenize(sent0))
</code></pre>

<p>What I don't understand is the meaning of this ""r'\w+|$[0-9]+|\S+'"". Can someone explain me just that?</p>
",Preprocessing of the text & Tokenization,someone explain meaning code inside parenthesis regexptokenizer r w reading book nlp recently one part author show tokenize piece text show code understand meaning r w someone explain
RegEx processing with python,"<p>I am trying to learn python and do text analysis using NLTK at the same time.</p>

<p>I am using python to scrub text before text analysis. </p>

<p>Given the sentence: <code>The target IP was: 127.1.1.100.</code></p>

<p>I want to tokenize it into:  </p>

<pre><code>[""The"", ""target"", ""IP"", ""was"", "":"",""127.1.1.100"","".""]
</code></pre>

<p>It is important I retain all the punctuation so as to reconstruct the source doc, but I need leading/trailing punctuation separated so I can do text analysis on the individual words. I wrote the following python code which works fine, but seems kinda kludgy.</p>

<pre><code>punct = ['.', ',', ':', ';', '!', '[', ']', '(', ')', '{', '}']
def split_punctuation(sentence)-&gt; list:
    sentwords = sentence.split("" "")
    for i, word in enumerate(sentwords):
        if word_ends_with_punct(word) and len(word) &gt; 1:
            sentwords.pop(i)
            sentwords.insert(i, word[:-1])
            sentwords.insert(i+1, word[-1])
            word = word[:-1]
        if word_starts_with_punct(word) and len(word) &gt; 1:
            sentwords.pop(i)
            sentwords.insert(i, word[0:1])
            sentwords.insert(i+1, word[1:])
            word = word[1:]
    return sentwords

def word_starts_with_punct(w)-&gt; bool:
    for p in punct:
        if w.startswith(p):
            return True
    return False

def word_ends_with_punct(w)-&gt;bool:
    for p in punct:
        if w.endswith(p):
            return True
    return False
</code></pre>

<p>So looking on SO I found a regex that does what I want, kinda...
<a href=""https://ideone.com/8BGDgN"" rel=""nofollow noreferrer"">RegEx by Wiktor Stribiżew  </a></p>

<pre><code>re.sub(r'[]!""$%&amp;\'()*+,./:;=#@?[\\^_`{|}~-]+', r' \g&lt;0&gt; ', my_text).strip()
</code></pre>

<p>I was able to figure out what's going on, but in this form it separates ALL punctuation, even in the middle of words. For example, it converted todays date from:
<code>6/28/2109</code> to <code>""6 / 28 / 2019""</code>.</p>

<p>So I modified to use anchor tags at beginning/end but it seems I have to run it twice, once for beginning punctuation, and again for the end. Seems rather inefficient and was hoping somebody could show the the correct way to accomplish this. The below code is the regex version:</p>

<pre><code>def sep_punct_by_regex(sent)-&gt;list :
    words = sent.split("" "")
    new_words = []
    for w in words:
        tmp1 = re.sub(r'^[]!""$/%&amp;\'()*+,.:;=#@?[\\^_`{|}~-]+', r' \g&lt;0&gt; ', w).strip()
        tmp2 = re.sub(r'[]!""$/%&amp;\'()*+,.:;=#@?[\\^_`{|}~-]+$', r' \g&lt;0&gt; ', tmp1).strip()
        t = tmp2.split("" "")
        for x in t:
            new_words.append(x)
    return new_words
</code></pre>

<p>Note the <code>^</code> in the tmp1, and <code>$</code> in tmp2
This works as is, but the goal is to learn while building so how would I modify the RegEx for single pass? I tried the obvious (<code>^</code>) up front, and the <code>$</code> at the end, but it doesn't work. </p>
",Preprocessing of the text & Tokenization,regex processing python trying learn python text analysis using nltk time using python scrub text text analysis given sentence want tokenize important retain punctuation reconstruct source doc need leading trailing punctuation separated text analysis individual word wrote following python code work fine seems kinda kludgy looking found regex doe want kinda regex wiktor stribi ew wa able figure going form separate punctuation even middle word example converted today date modified use anchor tag beginning end seems run twice beginning punctuation end seems rather inefficient wa hoping somebody could show correct way accomplish code regex version note tmp tmp work goal learn building would modify regex single pas tried obvious front end work
"How do I either remove &#39;\n&#39; from my nltk tokens, or prevent it from appearing in the first place, after converting a string to a list?","<p>I've converted a column from a CSV to a list, and then a string for tokenization. After it's converted to a string I get '\n' throughout. I'm looking to either prevent that from happening completely, or remove it after it happens. </p>

<p>So far, I've tried replace, strip, and rstrip to no avail.</p>

<p>Here's a version where I tried .replace() after converting the list to a string. </p>

<pre><code>df = pd.read_csv('raw_da_qs.csv')
question = df['question_only']
question = question.str.replace(r'\d+','')
question = str(question.tolist())
question = question.replace('\n','')
tokenizer = nltk.tokenize.RegexpTokenizer('\w+')
tokens = tokenizer.tokenize(question)
</code></pre>

<p>and I end up with tokens like this 'nthere', and 'nsuicide'</p>
",Preprocessing of the text & Tokenization,either remove n nltk token prevent appearing first place converting string list converted column csv list string tokenization converted string get n throughout looking either prevent happening completely remove happens far tried replace strip rstrip avail version tried replace converting list string end token like nthere nsuicide
ValueError: `sequences` must be iterable in Keras,"<p>I am trying to build a sentiment analysis model but when i start training,i am getting error as  <code>ValueError: sequences must be iterable.</code>  </p>

<p><code>pad_sequences</code> is what gives error.</p>

<p><strong>code till the function containing <code>pad_sequences</code>:</strong></p>

<p>1)get the word list,remove any punctuation and convert all words tokens to lowercase:    </p>

<pre><code> def get_processed_tokens(text):
    filtered_text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    filtered_text = filtered_text.split()
    filtered_text = [token.lower() for token in filtered_text]
    return filtered_text
</code></pre>

<p>2)Creating <code>token_idx</code> dictionary that maps tokens to integers to create embeddings and filter out the ones that occur less than the threshold which is given as 5 in the training set.</p>

<pre><code>def tokenize_text(data_text, min_frequency =5):
    review_tokens = [get_processed_tokens(review) for review in data_text]
    token_list = [token for review in review_tokens  for token in review] 
    token_freq_dict = {token:token_list.count(token) for token in set(token_list)}
    most_freq_tokens = [tokens for tokens in token_freq_dict if token_freq_dict[tokens] &gt;= min_frequency]
    idx = range(len(most_freq_tokens))
    token_idx = dict(zip(most_freq_tokens, idx))
    return token_idx,len(most_freq_tokens)
</code></pre>

<p>3)createing the sequences that will be fed into the model to learn the embeddings,fixed-length sequence of (max_tokens) for each review in the dataset. pre-padding the sequences with zeros if they are less than the maximum length.</p>

<pre><code>def create_sequences(data_text,token_idx,max_tokens):
    review_tokens  = [get_processed_tokens(review) for review in data_text] 
    review_token_idx = map( lambda review: [token_idx[k] for k in review if k in token_idx.keys() ], review_tokens)    
    padded_sequences = pad_sequences(review_token_idx, maxlen=max_tokens)  ##this line gives error
    return np.array(padded_sequences)
</code></pre>
",Preprocessing of the text & Tokenization,valueerror must iterable kera trying build sentiment analysis model start training getting error give error code till function containing get word list remove punctuation convert word token lowercase creating dictionary map token integer create embeddings filter one occur le threshold given training set createing sequence fed model learn embeddings fixed length sequence max token review dataset pre padding sequence zero le maximum length
Correct multithreaded lemmatization using spaCy,"<p>I'm trying to multithread the lemmatization of my corpus using spaCy. Following the <a href=""https://spacy.io/usage/processing-pipelines#section-multithreading"" rel=""nofollow noreferrer"">documentation</a>, this is currently my approach:</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner', 'tagger'])

def lemmatize():
    for doc in nlp.pipe(corpus, batch_size=2, n_threads=10):
        yield ' '.join([token.lemma_ for token in doc])

new_corpus = list(lemmatize())
</code></pre>

<p>However, this takes the same amount of time regardless when using 10 or 1 thread (I use it on 100.000 documents), suggesting that it is not multithreaded. </p>

<p>Is my implementation wrong?</p>
",Preprocessing of the text & Tokenization,correct multithreaded lemmatization using spacy trying multithread lemmatization corpus using spacy following documentation currently approach however take amount time regardless using thread use document suggesting multithreaded implementation wrong
A stable regular expression or simple library for multi-lingual tokenization?,"<p>We have a product which requires the ability to search, and has primarily been english-focused. As such, tokenization on spaces works relatively well (despite not always being the best idea).</p>

<p>We are recently expanding into the Japanese market and have found a number of complicating factors. Japanese has 2 key gotchas: 1) wordsCanBeStrungTogetherWithoutSpaces
2) Japanese uses different punctuation <a href=""https://www.livinglanguage.com/community/discussion/1339/japanese-punctuation-marks"" rel=""nofollow noreferrer"">symbols</a> </p>

<p>We have a workaround for 1, but having a ""word"" with a few hundred characters causes some complications, so it would be ideal to solve for (2). In the strictest sense I am trying to solve for Japanese, but realistically I would like a way to at least split up sentences regardless of alphabet.  Is there a regex that is good for splitting based on a unicode range? Or will it need to be custom and including every different language?</p>

<p>Quick searching reveals <a href=""https://unicodelookup.com/#full%20stop/1"" rel=""nofollow noreferrer"">https://unicodelookup.com/#full%20stop/1</a> it seems that the various ""full stop""s are without pattern (as far as I can tell), but there aren't many, and I could build to match those.  My concern is that there are edge cases that I don't know that I don't know about.</p>
",Preprocessing of the text & Tokenization,stable regular expression simple library multi lingual tokenization product requires ability search ha primarily english focused tokenization space work relatively well despite always best idea recently expanding japanese market found number complicating factor japanese ha key gotchas wordscanbestrungtogetherwithoutspaces japanese us different punctuation symbol workaround word hundred character cause complication would ideal solve strictest sense trying solve japanese realistically would like way least split sentence regardless alphabet regex good splitting based unicode range need custom including every different language quick searching reveals seems various full stop without pattern far tell many could build match concern edge case know know
Removing Word but not Subword from a Sentence in Python,"<p>I need to remove a given vector of words from a sentence (a given String) in Python.</p>

<p>The problem is that i want to remove exactly words but not substrings or subwords.</p>

<p>note: i cannot assume that before or after the word there is a space</p>

<p>I tried the <code>.replace(word,"""")</code> function but not works</p>

<p>example: <code>s = ""I'am at home and i will work by webcam call""</code></p>

<p>when i do <code>s.replace(""am"","""")</code></p>

<p>outputs: <code>i' at home and i will work by webc call</code></p>

<p>maybe can help the tokenization?</p>
",Preprocessing of the text & Tokenization,removing word subword sentence python need remove given vector word sentence given string python problem want remove exactly word substring subwords note assume word space tried function work example output maybe help tokenization
How to Tokenize group of words in Python,"<p>I am developing a application in python which gives job recommendation based on the resume uploaded. I am trying to tokenize resume before processing further. I want to tokenize group of words. For example <strong>Data Science</strong> is a keyword when i tokenize i will get <strong>data</strong> and <strong>science</strong> separately. How to overcome this situation. Is there any library which does these extraction in python?</p>
",Preprocessing of the text & Tokenization,tokenize group word python developing application python give job recommendation based resume uploaded trying tokenize resume processing want tokenize group word example data science keyword tokenize get data science separately overcome situation library doe extraction python
tm::removePunctuation doesn&#39;t remove all punctuation in R?,"<p>I have 900K <code>strings</code> in one <code>column</code> of one <code>dataframe</code>.</p>

<p>I am trying to clean them and one of the cleaning methods I use is <code>removePunctuation</code> from the <code>tm</code> package in R.</p>

<p>Please guide me why the following punctuation doesn't get removed:</p>

<pre><code>&gt; removePunctuation(""grillin’"")
[1] ""grillin’""
&gt; removePunctuation(""’ eggs benedict day"")
[1] ""’ eggs benedict day""
</code></pre>
",Preprocessing of the text & Tokenization,tm removepunctuation remove punctuation r k one one trying clean one cleaning method use package r please guide following punctuation get removed
Providing extracted lemma for each sentences using treetaggerwrapper does not work : return list of words instead list of word for each sentences,"<p>Here is my function which is supposed to lemmatize a list of sentences but the output is a list of all words  but not a list of each lemmatized sentences.</p>

<p>Code for lemmatize function</p>

<pre><code>tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr') 
def lemmatize(corpus):
    lemmatize_list_of _sentences= []
    lemmatize_list_of _sentences2 = []
    for sentence in corpus:
        tags = tagger.tag_text(sentence)
        tags2 = treetaggerwrapper.make_tags(tags, allow_extra = True)
        lemmatize_list_of_sentences.append(tags2)
        print(lemmatize_list_of_sentences)
        for subl in lemmatize_list_of_sentences: # loop in list of sublists 
            for word in subl:
                if word.__class__.__name__ == ""Tag"":
                    lemme=word[2] #  I want also to check if lemme[2] is empty and add this 
                    lemmeOption2=lemme.split(""|"")
                    lemme=lemmeOption1[0]
                    lemmatize_list_of_sentences2.append(lemme)


    return lemmatize_list_of_sentences2 # should return a list of lists where each list contains the lemme retrieve



lemmatize_train= lemmatize(sentences_train_remove_stop_words)
lemmatize_test= lemmatize(sentences_test_remove_stop_words)
print(lemmatize_train)
</code></pre>

<p>Furthermore , i would like to add the lemmatize function a line of code  to check if the index(2) or (-1) is empty,  and if it is empty retrieve the word at the first index</p>

<p>I come up with this but how can i combine it with my lemmatize function</p>

<pre><code>for word in subl:
        lemme= word.split('\t')
        try:
            if lemme[2] == '':
                lemmatize_list_of _sentences2.append(parts[0])
            else:
                lemmatize_list_of _sentences2.append(parts[2])
        except:
            print(parts)
</code></pre>

<p>list of sentences in the file_input</p>

<pre><code>La période de rotation de la Lune est la même que sa période orbitale et elle présente donc toujours le même hémisphère. 
Cette rotation synchrone résulte des frottements qu’ont entraînés les marées causées par la Terre. 

</code></pre>

<p>After tagging the text, and print the list of sentences_tagging , I have this :</p>

<p>first sentence : </p>

<pre><code>[[Tag(word='la', pos='DET:ART', lemma='le'), Tag(word='période', pos='NOM', lemma='période'), Tag(word='rotation', pos='NOM', lemma='rotation'), Tag(word='lune', pos='NOM', lemma='lune'), Tag(word='période', pos='NOM', lemma='période'), Tag(word='orbitale', pos='ADJ', lemma='orbital'), Tag(word='présente', pos='VER:pres', lemma='présenter'), Tag(word='donc', pos='ADV', lemma='donc'), Tag(word='toujours', pos='ADV', lemma='toujours')]]
</code></pre>

<p>whole sentences: </p>

<pre><code>[[Tag(word='la', pos='DET:ART', lemma='le'), Tag(word='période', pos='NOM', lemma='période'), Tag(word='rotation', pos='NOM', lemma='rotation'), Tag(word='lune', pos='NOM', lemma='lune'), Tag(word='période', pos='NOM', lemma='période'), Tag(word='orbitale', pos='ADJ', lemma='orbital'), Tag(word='présente', pos='VER:pres', lemma='présenter'), Tag(word='donc', pos='ADV', lemma='donc'), Tag(word='toujours', pos='ADV', lemma='toujours')], [Tag(word='cette', pos='PRO:DEM', lemma='ce'), Tag(word='rotation', pos='NOM', lemma='rotation'), Tag(word='synchrone', pos='ADJ', lemma='synchrone'), Tag(word='résulte', pos='VER:pres', lemma='résulter'), Tag(word='frottements', pos='NOM', lemma='frottement'), Tag(word='entraînés', pos='VER:pper', lemma='entraîner'), Tag(word='les', pos='DET:ART', lemma='le'), Tag(word='marées', pos='NOM', lemma='marée'), Tag(word='causées', pos='VER:pper', lemma='causer')]]

</code></pre>

<p>After retrieving the lemma I have a list of word , which is not what i expected. Expected a list for each sentences.</p>

<p>Output : </p>

<pre><code>['le', 'période', 'rotation', 'lune', 'période', 'orbital', 'présenter', 'donc', 'toujours', 'ce', 'rotation', 'synchrone', 'résulter', 'frottement', 'entraîner', 'le', 'marée', 'causer']
</code></pre>

<p>Expected : to have each word of the sentence in a single string with spaces between the word.</p>

<pre><code>
['le période rotation lune période orbital présenter donc toujours','ce rotation synchrone résulter frottement entraîner le marée causer']

</code></pre>
",Preprocessing of the text & Tokenization,providing extracted lemma sentence using treetaggerwrapper doe work return list word instead list word sentence function supposed lemmatize list sentence output list word list lemmatized sentence code lemmatize function furthermore would like add lemmatize function line code check index empty empty retrieve word first index come combine lemmatize function list sentence file input tagging text print list sentence tagging first sentence whole sentence retrieving lemma list word expected expected list sentence output expected word sentence single string space word
"using stanford nlp in spark, error &quot; Class java.util.function.Function not found - continuing with a stub.&quot;","<p>I need to do some text preprocessing in spark 1.6. taking the answer from <a href=""https://stackoverflow.com/questions/30222559/simplest-method-for-text-lemmatization-in-scala-and-spark/30226592?noredirect=1#comment58130063_30226592"">Simplest method for text lemmatization in Scala and Spark</a>, it's required to <code>import java.util.Properties</code>. But by running abt compiling and assembling, I got the following error: </p>

<pre><code>[warn] Class java.util.function.Function not found - continuing with a stub.
[warn] Class java.util.function.Function not found - continuing with a stub.
[warn] Class java.util.function.Function not found - continuing with a stub.
[error] Class java.util.function.Function not found - continuing with a stub.
[error] Class java.util.function.Function not found - continuing with a stub.
[warn] four warnings found
[error] two errors found
[error] (compile:compileIncremental) Compilation failed
[error] Total time: 52 s, completed Feb 10, 2016 2:11:12 PM
</code></pre>

<p>The code is as follows: </p>

<pre><code> // ref https://stackoverflow.com/questions/30222559/simplest-methodfor-text-lemmatization-in-scala-and-spark?rq=1

 def plainTextToLemmas(text: String): Seq[String] = {

 import java.util.Properties

 import edu.stanford.nlp.ling.CoreAnnotations._
 import edu.stanford.nlp.pipeline._

 import scala.collection.JavaConversions._
 import scala.collection.mutable.ArrayBuffer
 //  val stopWords = Set(""stopWord"")

 val props = new Properties()
 props.put(""annotators"", ""tokenize, ssplit, pos, lemma"")
 val pipeline = new StanfordCoreNLP(props)
 val doc = new Annotation(text)
 pipeline.annotate(doc)
 val lemmas = new ArrayBuffer[String]()
 val sentences = doc.get(classOf[SentencesAnnotation])
 for (sentence &lt;- sentences;
     token &lt;- sentence.get(classOf[TokensAnnotation])) {
 val lemma = token.get(classOf[LemmaAnnotation])
 if (lemma.length &gt; 2) {
    lemmas += lemma.toLowerCase
 }
}
   lemmas
}
</code></pre>

<p>My sbt file is as follows: </p>

<pre><code>scalaVersion := ""2.11.7""

crossScalaVersions := Seq(""2.10.5"", ""2.11.0-M8"")

libraryDependencies ++= Seq(
  ""org.apache.spark"" % ""spark-core_2.10"" % ""1.6.0"" % ""provided"",
  ""org.apache.spark"" % ""spark-mllib_2.10"" % ""1.6.0"" % ""provided"",
  ""org.apache.spark"" % ""spark-sql_2.10"" % ""1.6.0"" % ""provided"",
  ""com.github.scopt"" % ""scopt_2.10"" % ""3.3.0"",
 )
    libraryDependencies ++= Seq(
  ""edu.stanford.nlp"" % ""stanford-corenlp"" % ""3.5.2"",
  ""edu.stanford.nlp"" % ""stanford-corenlp"" % ""3.5.2"" classifier ""models""
  //   ""edu.stanford.nlp"" % ""stanford-corenlp"" % ""3.5.2"" classifier ""models-chinese""
  //   ""edu.stanford.nlp"" % ""stanford-corenlp"" % ""3.5.2"" classifier ""models-german""
  //   ""edu.stanford.nlp"" % ""stanford-corenlp"" % ""3.5.2"" classifier ""models-spanish""
  //""com.google.code.findbugs"" % ""jsr305"" % ""2.0.3""
)
</code></pre>

<p>taking the suggestion from the site, I changed java lib version from 1.7 to 1.8, the problem is still there. </p>

<p><a href=""https://i.sstatic.net/u24x3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/u24x3.png"" alt=""enter image description here""></a></p>
",Preprocessing of the text & Tokenization,using stanford nlp spark error class java util function function found continuing stub need text preprocessing spark taking answer
"Using UIMA, Stanford Core NLP together","<p>The UIMA and the StanfordNLP produces the output after the pipeline of operation like if We want to do POS tagging so in the input text first the tokenization is done and then the POS tagging.</p>

<p>I want to use the tokenization of the UIMA and use that token in the POS tagger of the Stanford CoreNLP. But the POS tagger of Stanford CoreNLP have the requirement to run the tokenizer before POS tagger.</p>

<p>So, is it possible to use the different API in the same pipeline or not ?
Is it possible to use the UIMA tokenizer and the Stanford CoreNLP together ?</p>
",Preprocessing of the text & Tokenization,using uima stanford core nlp together uima stanfordnlp produce output pipeline operation like want po tagging input text first tokenization done po tagging want use tokenization uima use token po tagger stanford corenlp po tagger stanford corenlp requirement run tokenizer po tagger possible use different api pipeline possible use uima tokenizer stanford corenlp together
"Best way to take text from data frame, tokenize by sentence then by word","<p>Would like to take a list of comments from a data frame, first parse into a list of sentences, then on a second pass, parse by word. Need this for input to word2vec model, genism. </p>

<p>Have already used sent_tokenize from nltk to tokenize once, but then if I try to word_tokenize after that , get have an issue because it is no longer a string and expecting a string or byte like object.</p>

<pre><code>import nltk

print(df)

ID Comment
0   Today is a good day.
1   Today I went by the river. The river also flow...
2   The water by the river is blue, it also feels ...
3   Today is the last day of spring; what to do to...

df['sentences']=df['Comment'].dropna().apply(nltk.sent_tokenize)

df['word']=df['sentences'].dropna().apply(nltk.word_tokenize)
</code></pre>

<p>after trying to pass sentences into words 
TypeError: expected string or bytes-like object</p>
",Preprocessing of the text & Tokenization,best way take text data frame tokenize sentence word would like take list comment data frame first parse list sentence second pas parse word need input word vec model genism already used sent tokenize nltk tokenize try word tokenize get issue longer string expecting string byte like object trying pas sentence word typeerror expected string byte like object
Stopwords coming up in most influential words,"<p>I am running some NLP code, trying to find the most influential (positively or negatively) words in a survey. My problem is that, while I successfully add some extra stopwords to the NLTK stopwords file, they keep coming up as influential words later on.</p>

<p>So, I have a dataframe, first column contains scores, second column contains comments.</p>

<p>I add extra stopwords:</p>

<pre><code>stopwords = stopwords.words('english')
extra = ['Cat', 'Dog']
stopwords.extend(extra)
</code></pre>

<p>I check that they are added, using the len method before and after.</p>

<p>I create this function to remove punctuation and stopwords from my comments:</p>

<pre><code>def text_process(comment):
   nopunc = [char for char in comment if char not in string.punctuation]
   nopunc = ''.join(nopunc)
   return [word for word in nopunc.split() if word.lower() not in stopwords]
</code></pre>

<p>I run the model (not going to include the whole code since it doesn't make a difference):</p>

<pre><code>corpus = df['Comment']
y = df['Label']
vectorizer = CountVectorizer(analyzer=text_process)
x = vectorizer.fit_transform(corpus)
</code></pre>

<p>...</p>

<p>And then to get the most influential words:</p>

<pre><code>feature_to_coef = {word: coef for word, coef in zip(vectorizer.get_feature_names(), nb.coef_[0])}


for best_positive in sorted(
    feature_to_coef.items(), 
    key=lambda x: x[1], 
    reverse=True)[:20]:
    print (best_positive)
</code></pre>

<p>But, Cat and Dog are in the results.</p>

<p>What am I doing wrong, any ideas?</p>

<p>Thank you very much!</p>
",Preprocessing of the text & Tokenization,stopwords coming influential word running nlp code trying find influential positively negatively word survey problem successfully add extra stopwords nltk stopwords file keep coming influential word later dataframe first column contains score second column contains comment add extra stopwords check added using len method create function remove punctuation stopwords comment run model going include whole code since make difference get influential word cat dog result wrong idea thank much
What is the best way to do french text analysis in Python?,"<p>I want to do text analysis on a french texts to visualise similarities between those texts, the possible class depending on the words that are used.
I ask for your help because I just started working with Python and I would like to know the best way to do text analysis in Python taking into account that my texts are in French ?</p>

<p><strong>Are there libraries specially designed for french texts ?</strong> The uses would be to clean the data, and further to analyse the data.</p>

<p>I can already :</p>

<ul>
<li>Clean: Removed all special  characters, put every word in lowercase and suppressed STOPWORDS</li>
<li>Tag : It's not optimal on french words (Libraries tested : StanfordPostagger, Tag.pos de NLTK).</li>
<li>Lemmatize : Not optimal (--) with french words(FrenchLefffLemmatizer,  WordNetLemmatizer)  </li>
<li>Stemming: FrenchStemmer snowball</li>
</ul>

<p><strong>What I can't do with French words : pass to singular, pass verbs to the infinitive form...</strong></p>
",Preprocessing of the text & Tokenization,best way french text analysis python want text analysis french text visualise similarity text possible class depending word used ask help started working python would like know best way text analysis python taking account text french library specially designed french text us would clean data analyse data already clean removed special character put every word lowercase suppressed stopwords tag optimal french word library tested stanfordpostagger tag po de nltk lemmatize optimal french word frenchleffflemmatizer wordnetlemmatizer stemming frenchstemmer snowball french word pas singular pas verb infinitive form
How to specify additional tokens for tokenizator?,"<p>I want to tokenize text with <code>gensim.utils.tokenize()</code>. And I want to add some phrases that would be recognized as single tokens, for example: <code>'New York', 'Long Island'</code>. </p>

<p>Is it possible with gensim? If not, what other libraries is it possible to use?</p>
",Preprocessing of the text & Tokenization,specify additional token tokenizator want tokenize text want add phrase would recognized single token example possible gensim library possible use
Avoiding &lt;sos&gt; and &lt;eos&gt; being parsed by Spacy,"<p>I am stuck with a basic thing but I could not figure out how to make it work. My apologies if it is something super basic. It is just that I am very new to Spacy and do not know how to do this. Could not find any resource on the internet as well. </p>

<p>I have a bunch of sentences like so</p>

<pre class=""lang-py prettyprint-override""><code>a = ""&lt;sos&gt; Hello There! &lt;eos&gt;""
</code></pre>

<p>I am using this following lines of code to tokenize it using Spacy</p>

<pre class=""lang-py prettyprint-override""><code>import spacy
nlp = spacy.load('en_core_web_sm')
for token in nlp(a):
    print(token.text)
</code></pre>

<p>What it prints is something like this</p>

<pre><code>&lt;
sos
&gt;
Hello
There
!
&lt;
eos
&gt;
</code></pre>

<p>As you can see, it parsed the <code>&lt;sos&gt;</code> and <code>&lt;eos&gt;</code> metatags. How can I avoid that? The output I would like to see is something like the following</p>

<pre><code>&lt;sos&gt;
Hello
There
!
&lt;eos&gt;
</code></pre>

<p>I could not figure out how to achieve this. Any help will be great. </p>

<p>Thanks in advance</p>
",Preprocessing of the text & Tokenization,avoiding eos parsed spacy stuck basic thing could figure make work apology something super basic new spacy know could find resource internet well bunch sentence like using following line code tokenize using spacy print something like see parsed metatags avoid output would like see something like following could figure achieve help great thanks advance
Create synonyms and use regular expressions to find keyword,"<p><strong>Background:</strong></p>

<p>I want to use regular expressions to search for a keyword. However, my keyword has multiple synonyms. For example, the keyword <code>positive</code>  can have the following words that I consider as equal to <code>positive</code>: <code>""+"", ""pos"", ""POS"", ""Positive"", ""POSITIVE""</code> </p>

<p>I've tried looking <a href=""https://stackoverflow.com/questions/49646310/create-a-dataframe-with-nltk-synonyms"">Create a dataframe with NLTK synonyms</a> and <a href=""http://www.nltk.org/howto/wordnet.html"" rel=""nofollow noreferrer"">http://www.nltk.org/howto/wordnet.html</a> but I don't think it is quite what I am looking for   </p>

<p><strong>Goals:</strong></p>

<p>1) create synonyms for a given keyword (e.g. <code>positive</code>)</p>

<p>2) search for a keyword (e.g. <code>positive</code>) in a corpus using regular expressions</p>

<p><strong>Example:</strong></p>

<pre><code>toy_corpus = 'patient is POS which makes them ideal to treatment '
</code></pre>

<p>I think the steps to getting this would look something like this:</p>

<p>1) define synonyms for the <code>positive</code>
 e.g. <code>positive</code> = <code>[""pos"", ""POS"", ""Positive"", ""POSITIVE"", ""+""]</code></p>

<p>2) use regular expression to find the keyword <code>POS</code></p>

<p><strong>Question</strong></p>

<p>How do I go about achieving this?</p>
",Preprocessing of the text & Tokenization,create synonym use regular expression find keyword background want use regular expression search keyword however keyword ha multiple synonym example keyword following word consider equal tried looking think quite looking goal create synonym given keyword e g search keyword e g corpus using regular expression example think step getting would look something like define synonym e g use regular expression find keyword question go achieving
Latent Semantic Analysis and Stemming,"<p>Assume a very large corpus of any inflective language. Does the following make sense? By applying LSA on such corpus, words with similar concepts converge together in vector space, thus inflected word forms reffering to the same concept should ideally be identical with their lemma in the space. With such assumption, any lemmatization or stemming of queries or corpus is not necessary. Or am i totally wrong?</p>
",Preprocessing of the text & Tokenization,latent semantic analysis stemming assume large corpus inflective language doe following make sense applying lsa corpus word similar concept converge together vector space thus inflected word form reffering concept ideally identical lemma space assumption lemmatization stemming query corpus necessary totally wrong
Text normalization : Text similarity in Python. How to normalize Text spelling mismatch?,"<p>i have a dataframe with a column A as below :</p>

<pre><code>Column A
Carrefour supermarket
Carrefour hypermarket
Carrefour
carrefour
Carrfour downtown
Carrfor market
Lulu
Lulu Hyper
Lulu dxb
lulu airport
k.m trading
KM Trading
KM trade
K.M.  Trading
KM.Trading
</code></pre>

<p>I wanted to derive at the below ""column A"" :</p>

<pre><code>Column A
Carrefour
Carrefour
Carrefour
Carrefour
Carrefour
Carrefour
Lulu
Lulu
Lulu
Lulu
KM Trading
KM Trading
KM Trading
KM Trading
KM Trading
</code></pre>

<p>To do this, i code as below :</p>

<pre><code>MERCHANT_NAME_DICT = {""lulu"": ""Lulu"", ""carrefour"": ""Carrefour"",  ""km"": ""KM Trading""}

def replace_merchant_name(row):
    """"""Provided a long merchant name replace it with short name.""""""
    processed_row = re.sub(r'\s+|\.', '', row.lower()).strip()
    for key, value in MERCHANT_NAME_DICT.items():
        if key in processed_row:
            return value

    return row

frame['MERCHANT_NAME'] = frame['MERCHANT_NAME'].astype(str)
frame.MERCHANT_NAME = frame.MERCHANT_NAME.apply(lambda row: replace_merchant_name(row))
</code></pre>

<p>But i wanted to use NLP Logic and make it a generic function ( Instead of using values for mapping ). Just call the generic function and run it on any similar data column and get the desired results. I am pretty new to NLP Concepts, so looking for some help on it friends.</p>

<p>NOTE : Basically i wanted a generic NLP way coding to find all similar words from a given column ( or in a list ).</p>
",Preprocessing of the text & Tokenization,text normalization text similarity python normalize text spelling mismatch dataframe column wanted derive column code wanted use nlp logic make generic function instead using value mapping call generic function run similar data column get desired result pretty new nlp concept looking help friend note basically wanted generic nlp way coding find similar word given column list
How do I use CountVectorizer to get the count of a phrase without counting words in the phrase?,"<p>I am working on an NLP project and I hope to tokenize sentences and get counts of different tokens. Sometimes I hope a few words to be a phrase and do not count the words inside the phrase.</p>

<p>I have found CountVectorizer in scikit-learn useful in counting phrases, but I could not figure out how to remove the words inside the phrases.</p>

<p>For example:</p>

<pre class=""lang-py prettyprint-override""><code>words = ['cat', 'dog', 'walking', 'my dog']
example = ['I was walking my dog and cat in the park']
vect = CountVectorizer(vocabulary=words, ngram_range=(1,2))
dtm = vect.fit_transform(example)
print(dtm)
</code></pre>

<p>I got:</p>

<pre><code>&gt;&gt;&gt; vect.get_feature_names()
['cat', 'dog', 'walking', 'my dog']
&gt;&gt;&gt; print(dtm)
  (0, 0)    1
  (0, 1)    1
  (0, 2)    1
  (0, 3)    1
</code></pre>

<p>What I want is:</p>

<pre><code>&gt;&gt;&gt; print(dtm)
  (0, 0)    1
  (0, 2)    1
  (0, 3)    1
</code></pre>

<p>But I want to keep <code>'dog'</code> in the dictionary because it may appear on its own in other text.</p>
",Preprocessing of the text & Tokenization,use countvectorizer get count phrase without counting word phrase working nlp project hope tokenize sentence get count different token sometimes hope word phrase count word inside phrase found countvectorizer scikit learn useful counting phrase could figure remove word inside phrase example got want want keep dictionary may appear text
How to find this word &#39;অশুভ&#39; in my string using regular expression?,"<p>I am trying to find this word using regular expression. But the issue I found is whenever I tried using word border ""\b"" it doesn't work accurately. 
And if I didn't use any RegEx then it will show different output where it has been used like 'অশুভৰ' 'অশুভ_লক্ষণ'.  I want to eliminate these results and only want that word precisely.</p>

<p>This is the string: ""মেকুৰীয়ে ৰাস্তা কাটিলে অশুভ বুলি ধৰা হয়, দুৱাৰডলিত বহাটো অশুভনীয়, যি লক্ষণ অশুভৰ পৰিচায়ক""</p>
",Preprocessing of the text & Tokenization,find word string using regular expression trying find word using regular expression issue found whenever tried using word border b work accurately use regex show different output ha used like want eliminate result want word precisely string
python lemmatizer that lemmatize &quot;political&quot; and &quot;politics&quot; to the same word,"<p>I've been testing different python lemmatizers for a solution I'm building out. One difficult problem I've faced is that stemmers are producing non english words which won't work for my use case. Although stemmers get ""politics"" and ""political"" to the same stem correctly, I'd like to do this with a lemmatizer, but spacy and nltk are producing different words for ""political"" and ""politics"". Does anyone know of a more powerful lemmatizer? My ideal solution would look like this:  </p>

<pre><code>from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

print(""political = "", lemmatizer.lemmatize(""political""))
print(""politics = "", lemmatizer.lemmatize(""politics""))  
</code></pre>

<p>returning:  </p>

<pre><code>political =  political
politics =  politics  
</code></pre>

<p>Where I want to return:</p>

<pre><code>political =  politics
politics =  politics  
</code></pre>
",Preprocessing of the text & Tokenization,python lemmatizer lemmatize political politics word testing different python lemmatizers solution building one difficult problem faced stemmer producing non english word work use case although stemmer get politics political stem correctly like lemmatizer spacy nltk producing different word political politics doe anyone know powerful lemmatizer ideal solution would look like returning want return
How can I add custom signs to spaCy&#39;s punctuation functionality?,"<p>Is there any option to add custom punctuation marks, which aren't included in the default punctuation rules? (<a href=""https://github.com/explosion/spaCy/blob/develop/spacy/lang/de/punctuation.py"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/develop/spacy/lang/de/punctuation.py</a>)</p>

<p>I am using spaCy's Matcher class (<a href=""https://spacy.io/usage/rule-based-matching"" rel=""nofollow noreferrer"">https://spacy.io/usage/rule-based-matching</a>) and the attribute ""IS_PUNCT"" to remove punctuation from my text. </p>

<pre class=""lang-py prettyprint-override""><code>from spacy.matcher import Matcher

# instantiate Matcher
matcher = Matcher(nlp.vocab)

# define pattern
pattern = [{""IS_PUNCT"": False}]

# add pattern to matcher
matcher.add(""Cleaning"", None, pattern)
</code></pre>

<p>I would like to customize the punctuation rules to be able to remove ""|"" from my texts with the Matcher.</p>
",Preprocessing of the text & Tokenization,add custom sign spacy punctuation functionality option add custom punctuation mark included default punctuation rule using spacy matcher class attribute punct remove punctuation text would like customize punctuation rule able remove text matcher
How do I tokenize a string sentence in NLTK?,"<p>I am using nltk, so I want to create my own custom texts just like the default ones on nltk.books. However, I've just got up to the method like</p>

<pre><code>my_text = ['This', 'is', 'my', 'text']
</code></pre>

<p>I'd like to discover any way to input my ""text"" as:</p>

<pre><code>my_text = ""This is my text, this is a nice way to input text.""
</code></pre>

<p>Which method, python's or from nltk allows me to do this. And more important, how can I dismiss punctuation symbols?</p>
",Preprocessing of the text & Tokenization,tokenize string sentence nltk using nltk want create custom text like default one nltk book however got method like like discover way input text method python nltk allows important dismiss punctuation symbol
How does SpaCy keeps track of character and token offset during tokenization?,"<p><strong>How does SpaCy keeps track of character and token offset during tokenization?</strong></p>

<p>In SpaCy, there's a Span object that keeps the start and end offset of the token/span <a href=""https://spacy.io/api/span#init"" rel=""nofollow noreferrer"">https://spacy.io/api/span#init</a></p>

<p>There's a <a href=""https://github.com/explosion/spaCy/blob/72889a16d558848191e51f4bfb200e70d3bc413a/spacy/tokens/span.pyx#L312"" rel=""nofollow noreferrer""><code>_recalculate_indices</code></a> method seems to be retrieving the <code>token_by_start</code> and <code>token_by_end</code> but that looks like all the recalcuation is doing. </p>

<p>When looking at extraneous spaces, it's doing some <a href=""https://github.com/explosion/spaCy/blob/72889a16d558848191e51f4bfb200e70d3bc413a/spacy/tokenizer.pyx#L95"" rel=""nofollow noreferrer"">smart alignment of the spans</a>. </p>

<p>Does it recalculate after every regex execution, does it keep track of the character's movement? Does it do a post regexes execution span search?</p>
",Preprocessing of the text & Tokenization,doe spacy keep track character token offset tokenization doe spacy keep track character token offset tokenization spacy span object keep start end offset token span method seems retrieving look like recalcuation looking extraneous space smart alignment span doe recalculate every regex execution doe keep track character movement doe post regexes execution span search
"How to do corpus pre-processing, lemmatization, and vectorization in SPACY NLP?","<p>I am trying to do Tokenization, Lemmatization, and Vectorization on the folder (with .txt files) on Jupyter Notebook (Python 3) using spaCy. </p>

<p>Below are the codes I had tried to write, but I probably made a mistake. I want the entire folder to be Tokenized, Lemmatized, and Vectorized (not any particular .txt file but the whole bulk of it combined). </p>

<pre><code>#tokenization
    for token in file_list:
        print(token.text, '\t', token.pos_, '\t', token.lemma, '\t', token.lemma_)

#lemmatisation 
    def show_lemmas(file_list):
        for token in text:
            print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:&lt;{22}} {token.lemma_}') 
            show_lemmas(file_list)  

#Vectorization (Using TF-IDF to create a vectorized document term matrix)
    from sklearn.feature_extraction.text import TfidfVectorizer
    tfidf = TfidfVectorizer(max_df=0.95,min_df=2, stop_words='english')
    dtm =tfidf.fit_transform(file_list)
    dtm
</code></pre>

<p>I expect the lines of code to be able to perform text-vectorization, lemmatization, and corpus pre-processing on the folder (with numerous .txt files). Could you please help me write the codes required to achieve that?
Also, let me know if I should be doing something more (other than Vec, Tok, and Lemm) before jumping into cluster analysis?</p>
",Preprocessing of the text & Tokenization,corpus pre processing lemmatization vectorization spacy nlp trying tokenization lemmatization vectorization folder txt file jupyter notebook python using spacy code tried write probably made mistake want entire folder tokenized lemmatized vectorized particular txt file whole bulk combined expect line code able perform text vectorization lemmatization corpus pre processing folder numerous txt file could please help write code required achieve also let know something vec tok lemm jumping cluster analysis
java keyword extraction,"<p>Is there a simple to use Java library that can take a String and return a set of Strings which are the keywords/keyphrases.</p>

<p>It doesn't have to be particularly clever, just use stop words and stemming to match keywords.</p>

<p>I am looking at the KEA package <a href=""http://code.google.com/p/kea-algorithm/"" rel=""nofollow"">http://code.google.com/p/kea-algorithm/</a> but I can't figure out how to use their code.</p>

<p>Ideally something simple which has a little example documentation would be good. In the meantime I will set about writing this myself!</p>

<p>EDIT: When I say I can't see how to figure out how to use their code, I mean I can't see a simple way. The individiual classes by themselves have useful methods that will do much of the work.</p>
",Preprocessing of the text & Tokenization,java keyword extraction simple use java library take string return set string keywords keyphrases particularly clever use stop word stemming match keywords looking kea package figure use code ideally something simple ha little example documentation would good meantime set writing edit say see figure use code mean see simple way individiual class useful method much work
How can I replace a word in my list of lists that matches a word from 1st list into a word that has the same position as the 1st one in the 2nd list?,"<p>I have a string type list of lists that has words that need to be replaced. </p>

<p>This replacement can occur by checking if there is a match in list 1, if there is then the position of the match is searched in list 2. The word is picked from that position in list 2 and replaced in the original list of lists. </p>

<p>Note: The matches might not occur in all the sublists of my list of lists. </p>

<pre><code>
list_of_list = [['apple', 'ball', 'cat'], ['apple', 'table'], ['cat', 'mouse', 'bootle'], ['mobile', 'black'], ['earphone']]

list_1 = [""apple"", ""bootle"", ""earphone""]

list_2 = [""fruit"", ""object"", ""electronic""]

This is what I have tried so far, but it doesn't seem to work the way I want it to. 

for word in list_of_list:
    for idx, item in enumerate(word):
        for w in list_1:
            if (w in item):
                index_list_1= list_1.index(w)
                word_from_list_2 = list_2[index_list_1]
                word[idx] = word_from_list_2
print(list_of_list)
</code></pre>

<p>I want something like:</p>

<p>Input:</p>

<pre><code>list_of_list = [['apple', 'ball', 'cat'], ['apple', 'table'], ['cat', 'mouse', 'bootle'], ['mobile', 'black'], ['earphone']]
</code></pre>

<p>Output:</p>

<pre><code>list_of_list = [['fruit', 'ball', 'cat'], ['fruit', 'table'], ['cat', 'mouse', 'object'], ['mobile', 'black'], ['electronic']]
</code></pre>
",Preprocessing of the text & Tokenization,replace word list list match word st list word ha position st one nd list string type list list ha word need replaced replacement occur checking match list position match searched list word picked position list replaced original list list note match might occur sublists list list want something like input output
removing stop words using spacy,"<p>I am cleaning a column in my <code>data frame</code>, Sumcription, and am trying to do 3 things:</p>

<ol>
<li>Tokenize</li>
<li>Lemmantize</li>
<li><p>Remove stop words </p>

<pre><code>import spacy        
nlp = spacy.load('en_core_web_sm', parser=False, entity=False)        
df['Tokens'] = df.Sumcription.apply(lambda x: nlp(x))    
spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS        
spacy_stopwords.add('attach')
df['Lema_Token']  = df.Tokens.apply(lambda x: "" "".join([token.lemma_ for token in x if token not in spacy_stopwords]))
</code></pre></li>
</ol>

<p>However, when I print for example: </p>

<pre><code>df.Lema_Token.iloc[8]
</code></pre>

<p>The output still has the word attach in it:
<code>attach poster on the wall because it is cool</code></p>

<p>Why does it not remove the stop word?</p>

<p>I also tried this:</p>

<pre><code>df['Lema_Token_Test']  = df.Tokens.apply(lambda x: [token.lemma_ for token in x if token not in spacy_stopwords])
</code></pre>

<p>But the str <code>attach</code> still appears.</p>
",Preprocessing of the text & Tokenization,removing stop word using spacy cleaning column sumcription trying thing tokenize lemmantize remove stop word however print example output still ha word attach doe remove stop word also tried str still appears
Pos Tag Lemmatize giving only one row in output,"<p>Using Pos Tag on tokenize data, it is coming into form of word, pos_tag.
When passing the same for lemmatization, only the first value is getting lemmatized.</p>

<p>Dataframe with two columns-</p>

<pre><code>ID Text 
1  Lemmatization is an interesting part
</code></pre>

<p>After tokenize and removing stop words -</p>

<pre><code>ID Tokenize_data
1  'Lemmatization', 'interesting', 'part'



#Lemmatization with postag
#Part of Speech Tagging
df2['tag_words'] = df2.tokenize_data.apply(nltk.pos_tag)
#Treebank to Wordnet
from nltk.corpus import wordnet

def get_wordnet_pos(treebank_tag):

    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return None

from nltk.stem.wordnet import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

def tagging(text):
#tagged = nltk.pos_tag(tokens)
    for (word, tag) in text:
        wntag = get_wordnet_pos(tag)
        if wntag is None:# not supply tag in case of None
            lemma = lemmatizer.lemmatize(word) 
        else:
            lemma = lemmatizer.lemmatize(word, pos=wntag) 
        return lemma

tag1 = lambda x: tagging(x)
df2['lemma_tag'] = df2.tag_words.apply(tag1)
</code></pre>

<p>Output is coming as -</p>

<pre><code>ID Lemma_words 
1  'Lemmatize'
</code></pre>

<p>Expected -</p>

<pre><code>ID Lemma_words
1  'Lemmatize', 'interest', 'part'
</code></pre>
",Preprocessing of the text & Tokenization,po tag lemmatize giving one row output using po tag tokenize data coming form word po tag passing lemmatization first value getting lemmatized dataframe two column tokenize removing stop word output coming expected
How do I subset my SOTU dfm to Presidents Wilson and later in quanteda?,"<p>I'm working with quanteda.corpora's SOTU corpus and need to subset it to look at roughly the last century of SOTU speeches. I'm coming from tm though, so I'm not super familiar with managing dfm objects. </p>

<p>I've learned how to preprocess the corpus when in dfm format, but I'm not certain what to do next. This is what I have right now. To my understanding, this code ought to subset my corpus to include only documents that were delivered after 1913.</p>

<pre><code>library(quanteda)
library(quanteda.corpora)
dfmat_sotu &lt;- dfm(data_corpus_sotu, tolower = TRUE, remove = stopwords(""english""), remove_numbers = TRUE, remove_punct = TRUE)
dfmat_sotu &lt;- dfm_wordstem(dfmat_sotu, language = quanteda_options(""language_stemmer""))
dfmat_sotu &lt;- dfm_subset(dfmat_sotu, Date &gt; 1913-12-02)
wf_sotu &lt;- textmodel_wordfish(dfmat_sotu)

textplot_scale1d(wf_sotu)
</code></pre>

<p>The issue is that when I run this code as well as wordfish, it becomes clear that I haven't subset the corpus as intended -- it seems to only include speeches from 1978 and later. What do I need to do differently?</p>
",Preprocessing of the text & Tokenization,subset sotu dfm president wilson later quanteda working quanteda corpus sotu corpus need subset look roughly last century sotu speech coming tm though super familiar managing dfm object learned preprocess corpus dfm format certain next right understanding code ought subset corpus include document issue run code well wordfish becomes clear subset corpus intended seems include speech later need differently
NLP: Sentence segmentation by intent,"<p>I am trying to find a solution for sentence segregation by intent. As one sentence may have multiple intents, I would like to separate a piece of text into short sentences by intents.</p>

<p><strong>For example: To separate 'Email my team meeting is today, and attach the schedule file' into 'Email my team meeting is today', 'and attach the schedule file'. OR 'I want apple and orange and I will be late tonight' into 'I want apple and orange', 'and I will be late tonight'.</strong></p>

<p>I've checked chunking and sentence tokenizing, I think chunking might be a starting point but it does not give a close approach. Can someone give me some suggestion? Thanks a lot!</p>
",Preprocessing of the text & Tokenization,nlp sentence segmentation intent trying find solution sentence segregation intent one sentence may multiple intent would like separate piece text short sentence intent example separate email team meeting today attach schedule file email team meeting today attach schedule file want apple orange late tonight want apple orange late tonight checked chunking sentence tokenizing think chunking might starting point doe give close approach someone give suggestion thanks lot
What are some of the data preparation steps or techniques one needs to follow when dealing with multi-lingual data?,"<p>I'm working on multilingual word embedding code where I need to train my data on English and test it on Spanish. I'll be using the MUSE library by Facebook for the word-embeddings.
I'm looking for a way to pre-process both my data the same way. I've looked into diacritics restoration to deal with the accents. </p>

<p>I'm having trouble coming up with a way in which I can carefully remove stopwords, punctuations and weather or not I should lemmatize. </p>

<p>How can I uniformly pre-process both the languages to create a vocabulary list which I can later use with the MUSE library.</p>
",Preprocessing of the text & Tokenization,data preparation step technique one need follow dealing multi lingual data working multilingual word embedding code need train data english test spanish using muse library facebook word embeddings looking way pre process data way looked diacritic restoration deal accent trouble coming way carefully remove stopwords punctuation weather lemmatize uniformly pre process language create vocabulary list later use muse library
Pipeline for text cleaning / processing in python,"<p>I am pretty new to the python environment (jupyter notebook), and I am trying to work on a relatively huge text data. I want to process it by applying the following steps and in the same order:</p>

<p><strong>strip whitespaces,
lower case,
stemming,
remove punctuation but preserve intra-word dashes or hyphens,
remove stopwords,
remove symbols,
Strip whitespaces,</strong></p>

<p>I was hoping I could get a single function that could perform the task, instead of doing them individually, is there any single library and/or function out there that could help? if not, what could be the simplest way of defining a function to perform them just with one run?</p>
",Preprocessing of the text & Tokenization,pipeline text cleaning processing python pretty new python environment jupyter notebook trying work relatively huge text data want process applying following step order strip whitespaces lower case stemming remove punctuation preserve intra word dash hyphen remove stopwords remove symbol strip whitespaces wa hoping could get single function could perform task instead individually single library function could help could simplest way defining function perform one run
Regular expression tokenization with numbers?,"<p>I am expecting the following code;
tokenize </p>

<pre><code>this is an example 123
</code></pre>

<p>into </p>

<pre><code>['this', 'is', 'an', 'example 123'] 
</code></pre>

<p>but it doesn't see numbers part of the word. Any suggestion?</p>

<pre><code>import re
from nltk.tokenize import RegexpTokenizer
pattern=re.compile(r""[\w\s\d]+"")
tokenizer_number=RegexpTokenizer(pattern)
tokenizer_number.tokenize(""this is an example 123"")
</code></pre>
",Preprocessing of the text & Tokenization,regular expression tokenization number expecting following code tokenize see number part word suggestion
How to tokenize sentence using nlp,"<p>I'm new in NLP. I'm trying to tokenize sentence using nlp on python 3.7.So I used following code</p>

<pre><code>import nltk
text4=""This is the first sentence.A gallon of milk in the U.S. cost 
$2.99.Is this the third sentence?Yes,it is!""
x=nltk.sent_tokenize(text4)
x[0]
</code></pre>

<p>I was expecting that x[0] will return first sentence but I got </p>

<pre><code>Out[4]: 'This is the first sentence.A gallon of milk in the U.S. cost $2.99.Is this the third sentence?Yes,it is!'
</code></pre>

<p>Am I doing anything wrong?</p>
",Preprocessing of the text & Tokenization,tokenize sentence using nlp new nlp trying tokenize sentence using nlp python used following code wa expecting x return first sentence got anything wrong
Implementation of n-grams in python code for multi-class text classification,"<p>I am new to python and working on the multi-class text classification of contract documents of the construction industry. I am facing problems in the implementation of n-grams in my code which I produced form by getting help from different online sources. I want to implement unigram, bi-gram, and tri-gram in my code. Any help in this regard shall be highly appreciated.</p>

<p>I have tried bigram and trigram in my Tfidf part of my code but it is working.</p>

<pre><code>    df = pd.read_csv('projectdataayes.csv')
    df = df[pd.notnull(df['types'])]
    my_types = ['Requirement','Non-Requirement']

    #converting to lower case
    df['description'] = df.description.map(lambda x: x.lower()) 

    #Removing the punctuation
    df['description'] = df.description.str.replace('[^\w\s]', '')  

    #splitting the word into tokens
    df['description'] = df['description'].apply(tokenize.word_tokenize) 

    #stemming
    stemmer = PorterStemmer()
    df['description'] = df['description'].apply(lambda x: [stemmer.stem(y) for y in x]) 

    print(df[:10])

    ## This converts the list of words into space-separated strings
    df['description'] = df['description'].apply(lambda x: ' '.join(x))
    count_vect = CountVectorizer()  
    counts = count_vect.fit_transform(df['description']) 


    X_train, X_test, y_train, y_test = train_test_split(counts, df['types'], test_size=0.3, random_state=39) 

    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', 
    token_pattern=r'\w{1,}', ngram_range=(2,3), max_features=5000)
    tfidf_vect_ngram.fit(df['description'])
    X_train_Tfidf =  tfidf_vect_ngram.transform(X_train)
    X_test_Tfidf =  tfidf_vect_ngram.transform(X_test)

    model = MultinomialNB().fit(X_train, y_train)
</code></pre>

<p>File ""C:\Users\fhassan\anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"", line 328, in 
    tokenize(preprocess(self.decode(doc))), stop_words)</p>

<p>File ""C:\Users\fhassan\anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"", line 256, in 
    return lambda x: strip_accents(x.lower())</p>

<p>File ""C:\Users\fhassan\anaconda3\lib\site-packages\scipy\sparse\base.py"", line 686, in <strong>getattr</strong>
    raise AttributeError(attr + "" not found"")</p>

<p>AttributeError: lower not found</p>
",Preprocessing of the text & Tokenization,implementation n gram python code multi class text classification new python working multi class text classification contract document construction industry facing problem implementation n gram code produced form getting help different online source want implement unigram bi gram tri gram code help regard shall highly appreciated tried bigram trigram tfidf part code working file c user fhassan anaconda lib site package sklearn feature extraction text py line tokenize preprocess self decode doc stop word file c user fhassan anaconda lib site package sklearn feature extraction text py line return lambda x strip accent x lower file c user fhassan anaconda lib site package scipy sparse base py line raise attributeerror attr found attributeerror lower found
Any efficient way to create vocabulary of top frequent words from list of sentences?,"<p>I figured out how to use tfidf schema to capture distribution of the words along the document. However, I want to create vocabulary of top frequent and least frequent words for list of sentences.</p>

<p>Here is some part of text preprocessing:</p>

<pre><code>print(my.df) -&gt; 
(17298, 2)

print(df.columns) -&gt;
Index(['screen_name', 'text'], dtype='object')



txt = re.sub(r""[^\w\s]"","""",txt)
txt = re.sub(r""@([A-Z-a-z0-9_]+)"", """", txt)
tokens = nltk.word_tokenize(txt)
token_lemmetized = [lemmatizer.lemmatize(token).lower() for token in tokens]
df['text'] = df['text'].apply(lambda x: process(x))
</code></pre>

<p>then this is my second attempt:</p>

<pre><code>import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
import string
stop = set(stopwords.words('english'))

df['text'] = df['text'].apply(lambda x: [item for item in x if item not in stop])

all_words = list(chain.from_iterable(df['text']))
for i in all_words:
    x=Counter(df['text'][i])
    res= [word for word, count in x.items() if count == 1]
    print(res)
</code></pre>

<p>in above approach I want to create most frequent and least frequent words from list of sentences, but my attempt didn't produce that outuput? what should I do? any elegant way to make this happen? any idea? can anyone give me possible idea to make this happen? Thanks</p>

<p><strong>example data snippets</strong> :</p>

<p>here is data that I used and file can be found safely here: <a href=""https://gist.github.com/julaiti/2a7fa59bebdd3a9a0e05c6869e487dac"" rel=""nofollow noreferrer"">example data</a></p>

<p><strong>sample input  and output</strong>:</p>

<p>inputList = {""RT @GOPconvention: #Oregon votes today. That means 62 days until the @GOPconvention!"", ""RT @DWStweets: The choice for 2016 is clear: We need another Democrat in the White House. #DemDebate #WeAreDemocrats "", ""Trump's calling for trillion dollar tax cuts for Wall Street."", From Chatham Town Council to Congress, @RepRobertHurt has made a strong mark on his community. Proud of our work together on behalf of VA!}</p>

<p><strong>sample output of tokens</strong></p>

<pre><code>['rt', 'gopconvention', 'oregon', 'vote', 'today', 'that', 'mean', '62', 'day', 'until', 'gopconvention', 'http', 't', 'co', 'ooh9fvb7qs']
</code></pre>

<p><strong>output</strong>:</p>

<p>I want to create vocabulary for most frequent words and least frequent words from give data. any idea to get this done? Thanks</p>
",Preprocessing of the text & Tokenization,efficient way create vocabulary top frequent word list sentence figured use tfidf schema capture distribution word along document however want create vocabulary top frequent least frequent word list sentence part text preprocessing second attempt approach want create frequent least frequent word list sentence attempt produce outuput elegant way make happen idea anyone give possible idea make happen thanks example data snippet data used file found example data sample input output inputlist rt gopconvention oregon vote today mean day gopconvention rt dwstweets choice clear need another democrat white house demdebate wearedemocrats trump calling dollar tax cut wall street chatham town council congress reproberthurt ha made strong mark community proud work together behalf va sample output token output want create vocabulary frequent word least frequent word give data idea get done thanks
Is there an way to concat words from given word sets into grammatical sentence?,"<p>I want to build sentence of fixed length 
S = w1 w2 w3 w4 w5 ..... wN.
N is already known to me. I have a set of words for each word position wi where i = 1 to N. I have to pick one word from each wi to construct a sentence S. S must be grammatically correct; and I don't want to check the meaning of sentence S. Is there any way to achieve this using NLTK's toolkit or any other NLP toolkit?</p>

<p>I can construct sentences from the given word list using the DFS algorithm and then check whether each sentence is grammatically correct or not. But the set size for each word is large (scale of 10^3), so it takes too much time to construct a sentence for even three word sentences. </p>

<blockquote>
  <p>Suppose, I want to build three word sentence S = w1 w2 w3.</p>
  
  <p>w1 = {I, you, we, he, she,...},<br/>  w2 = {eat, drink, read, who,
  how,...},<br/>  w3 = {banana, tea, music,... }</p>
  
  <p>Valid sentences (Grammatically Correct):</p>
  
  <ul>
  <li>I eat banana.<br/></li>
  <li>I drink tea.<br/></li>
  <li>I eat music.<br/></li>
  <li>I drink banana.<br/></li>
  <li>I read banana and so on.</li>
  </ul>
  
  <p>Invalid Sentences (Grammatically Incorrect):</p>
  
  <ul>
  <li>I who banana.<br/></li>
  <li>I how book.</li>
  </ul>
</blockquote>
",Preprocessing of the text & Tokenization,way concat word given word set grammatical sentence want build sentence fixed length w w w w w wn n already known set word word position wi n pick one word wi construct sentence must grammatically correct want check meaning sentence way achieve using nltk toolkit nlp toolkit construct sentence given word list using dfs algorithm check whether sentence grammatically correct set size word large scale take much time construct sentence even three word sentence suppose want build three word sentence w w w w w eat drink read w banana tea music valid sentence grammatically correct eat banana drink tea eat music drink banana read banana invalid sentence grammatically incorrect banana book
Stanford coreNLP splitting paragraph sentences without whitespace,"<p>I faced a problem with stanford's Sentence annotator.
As an input I've got the text, which contains sentences, but there is no whitespace after dot in some parts of it. Like this:</p>
<blockquote>
<p>Dog loves cat.Cat loves mouse. Mouse hates everybody.</p>
</blockquote>
<p>So when I'm trying to use SentenceAnnotator - I'm getting 2 sentences</p>
<blockquote>
<p>Dog loves cat.Cat loves mouse.</p>
<p>Mouse hates everybody.</p>
</blockquote>
<p>Here is my code</p>
<pre><code>Annotation doc = new Annotation(t);
Properties props = new Properties();
props.setProperty(&quot;annotators&quot;, &quot;tokenize,ssplit,pos,lemma,ner,parse,coref&quot;);
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
pipeline.annotate(doc);
List&lt;CoreMap&gt; sentences = doc.get(CoreAnnotations.SentencesAnnotation.class);
</code></pre>
<p>I also tried to add property</p>
<pre><code>props.setProperty(&quot;ssplit.boundaryTokenRegex&quot;, &quot;\\.&quot;);
</code></pre>
<p>but no effect.</p>
<p>Maybe I'm missing something?
Thanks!</p>
<p>UPD
Also I tried to tokenize text using PTBTokenizer</p>
<pre><code>PTBTokenizer ptbTokenizer = new PTBTokenizer(
        new FileReader(classLoader.getResource(&quot;simplifiedParagraphs.txt&quot;).getFile())
        ,new WordTokenFactory()
        ,&quot;untokenizable=allKeep,tokenizeNLs=true,ptb3Escaping=true,strictTreebank3=true,unicodeEllipsis=true&quot;);
List&lt;String&gt; strings = ptbTokenizer.tokenize();
</code></pre>
<p>but tokenizer thinks that cat.Cat is single word and doesn't split it.</p>
",Preprocessing of the text & Tokenization,stanford corenlp splitting paragraph sentence without whitespace faced problem stanford sentence annotator input got text contains sentence whitespace dot part like dog love cat cat love mouse mouse hate everybody trying use sentenceannotator getting sentence dog love cat cat love mouse mouse hate everybody code also tried add property effect maybe missing something thanks upd also tried tokenize text using ptbtokenizer tokenizer think cat cat single word split
Tfidfvectorizer - How can I check out processed tokens?,"<p>How can I check the strings tokenized inside <code>TfidfVertorizer()</code>?  If I don't pass anything in the arguments, <code>TfidfVertorizer()</code> will tokenize the string with some pre-defined methods. I want to observe how it tokenizes strings so that I can more easily tune my model.</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
corpus = ['This is the first document.',
          'This document is the second document.',
          'And this is the third one.',
          'Is this the first document?']
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)
</code></pre>

<p>I want something like this:</p>

<pre><code>&gt;&gt;&gt;vectorizer.get_processed_tokens()
[['this', 'is', 'first', 'document'],
 ['this', 'document', 'is', 'second', 'document'],
 ['this', 'is', 'the', 'third', 'one'],
 ['is', 'this', 'the', 'first', 'document']]
</code></pre>

<p>How can I do this? </p>
",Preprocessing of the text & Tokenization,tfidfvectorizer check processed token check string tokenized inside pas anything argument tokenize string pre defined method want observe tokenizes string easily tune model want something like
Lemmatisation of web scraped data,"<p>Let's suppose that I have a text document such as the following:</p>

<pre><code>document = '&lt;p&gt; I am a sentence. I am another sentence &lt;p&gt; I am a third sentence.'
</code></pre>

<p>( or a more complex text example:</p>

<pre><code>document = '&lt;p&gt;Forde Education are looking to recruit a Teacher of Geography for an immediate start in a Doncaster Secondary school.&lt;/p&gt; &lt;p&gt;The school has a thriving and welcoming environment with very high expectations of students both in progress and behaviour.&amp;nbsp; This position will be working&amp;nbsp;until Easter with a&amp;nbsp;&lt;em&gt;&lt;strong&gt;likely extension until July 2011.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The successful candidates will need to demonstrate good practical subject knowledge  but also possess the knowledge and experience to teach to GCSE level with the possibility of teaching to A’Level to smaller groups of students.&lt;/p&gt; &lt;p&gt;All our candidate will be required to hold a relevant teaching qualifications with QTS  successful applicants will be required to provide recent relevant references and undergo a Enhanced CRB check.&lt;/p&gt; &lt;p&gt;To apply for this post or to gain information regarding similar roles please either submit your CV in application or Call Debbie Slater for more information.&amp;nbsp;&lt;/p&gt;' 
</code></pre>

<p>)</p>

<p>I am applying a series of pre-processing NLP techniques to get a ""cleaner"" version of this document by also taking the stem word for each of its words.</p>

<p>I am using the following code for this:</p>

<pre><code>stemmer_1 = PorterStemmer()
stemmer_2 = LancasterStemmer()
stemmer_3 = SnowballStemmer(language='english')

# Remove all the special characters
document = re.sub(r'\W', ' ', document)

# remove all single characters
document = re.sub(r'\b[a-zA-Z]\b', ' ', document)

# Substituting multiple spaces with single space
document = re.sub(r' +', ' ', document, flags=re.I)

# Converting to lowercase
document = document.lower()

# Tokenisation
document = document.split()

# Stemming
document = [stemmer_3.stem(word) for word in document]

# Join the words back to a single document
document = ' '.join(document)
</code></pre>

<p>This gives the following output for the text document above:</p>

<pre><code>'am sent am anoth sent am third sent'
</code></pre>

<p>(and this output for the more complex example:</p>

<pre><code>'ford educ are look to recruit teacher of geographi for an immedi start in doncast secondari school the school has thrive and welcom environ with veri high expect of student both in progress and behaviour nbsp this posit will be work nbsp until easter with nbsp em strong like extens until juli 2011 strong em the success candid will need to demonstr good practic subject knowledg but also possess the knowledg and experi to teach to gcse level with the possibl of teach to level to smaller group of student all our candid will be requir to hold relev teach qualif with qts success applic will be requir to provid recent relev refer and undergo enhanc crb check to appli for this post or to gain inform regard similar role pleas either submit your cv in applic or call debbi slater for more inform nbsp'
</code></pre>

<p>)</p>

<p>What I want to do now is to get an output like the one exactly above but after I have applied lemmatisation and not stemming.</p>

<p>However, unless I am missing something, this requires to split the original document into (sensible) sentences, apply POS tagging and then implement the lemmatisation.</p>

<p>But here things are a little bit complicated because the text data are coming from web scraping and hence you will encounter many HTML tags such as <code>&lt;br&gt;</code>, <code>&lt;p&gt;</code> etc.</p>

<p>My idea is that every time a sequence of words is ending with some common punctuation mark (fullstop, exclamation point etc) or with a HTML tag such as <code>&lt;br&gt;</code>, <code>&lt;p&gt;</code> etc then this should be considered as a separate sentence.</p>

<p>Thus for example the original document above:</p>

<pre><code>document = '&lt;p&gt; I am a sentence. I am another sentence &lt;p&gt; I am a third sentence.'
</code></pre>

<p>Should be split in something like this:</p>

<pre><code>['I am a sentence', 'I am another sentence', 'I am a third sentence']
</code></pre>

<p>and then I guess we will apply POS tagging at each sentence, split each sentence in words, apply lemmatization and <code>.join()</code> the words back to a single document as I am doing it with my code above.</p>

<p>How can I do this?</p>
",Preprocessing of the text & Tokenization,lemmatisation web scraped data let suppose text document following complex text example applying series pre processing nlp technique get cleaner version document also taking stem word word using following code give following output text document output complex example want get output like one exactly applied lemmatisation stemming however unless missing something requires split original document sensible sentence apply po tagging implement lemmatisation thing little bit complicated text data coming web scraping hence encounter many html tag etc idea every time sequence word ending common punctuation mark fullstop exclamation point etc html tag etc considered separate sentence thus example original document split something like guess apply po tagging sentence split sentence word apply lemmatization word back single document code
Add stop words in Gensim,"<p>Thanks for stopping by!  I had a quick question about appending stop words. I have a select few words that show up in my data set and I was hopping I could add them to gensims stop word list.  I've seen a lot of examples using nltk and I was hoping there would be a way to do the same in gensim.  I'll post my code below:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; 3:
            nltk.bigrams(token)
            result.append(lemmatize_stemming(token))
    return result</code></pre>
</div>
</div>
</p>
",Preprocessing of the text & Tokenization,add stop word gensim thanks stopping quick question appending stop word select word show data set wa hopping could add gensims stop word list seen lot example using nltk wa hoping would way gensim post code
how to iterate through a list inside a series,"<p>I have a <code>series</code> that contains a <code>list</code> inside. Each <code>list</code> in the <code>series</code> has a different length. The reason is because I had a <code>string</code> originally inside the <code>series</code>, and then I applied the <code>nltk</code> <code>word_tokenize()</code> function.</p>

<p>I have tried using <code>apply() lambda</code> functions but have been unsuccessful.
For example, to count the frequency of unique words I tried:</p>

<pre><code>summary_word_frequency = df.summary_word_tokens.apply(lambda x: [x.value_counts() for x in df.summary_word_tokens])
</code></pre>

<p>Some insights on my data and how it is structured:</p>

<p>In:</p>

<pre><code>print(type(df.summary_word_tokens))
print(type(df.summary_word_tokens[0]))
print(type(df.summary_word_tokens[0][1]))
</code></pre>

<p>Out:</p>

<pre><code>&lt;class 'pandas.core.series.Series'&gt;
&lt;class 'list'&gt;
&lt;class 'str'&gt;
</code></pre>

<p>In: </p>

<pre><code>print(df.summary_word_tokens.shape)
print(df.summary_word_tokens[0])
print(df.summary_word_tokens[0][1])
</code></pre>

<p>Out:</p>

<pre><code>(1000,)
['cake', 'type', 'is', 'cake', 'chocolate']
type
</code></pre>

<p>My goal is to be able to iterate through all the rows, as I want to be able to feed the tokens into an algorithm.</p>

<p>Or would you all recommend a different way to tokenize/ making the list into a series (so it'd be a series within a series)? And does it matter the way you tokenize if the problem is unsupervised? </p>

<p>Thanks</p>
",Preprocessing of the text & Tokenization,iterate list inside series contains inside ha different length reason originally inside applied function tried using function unsuccessful example count frequency unique word tried insight data structured goal able iterate row want able feed token algorithm would recommend different way tokenize making list series series within series doe matter way tokenize problem unsupervised thanks
Removing stopwords and tokenization in python,"<p>I have following input data and I would like to remove stopwords from this input and want to do tokenization:</p>

<pre><code>input = [['Hi i am going to college', 'We will meet next time possible'],
         ['My college name is jntu', 'I am into machine learning specialization'],
         ['Machine learnin is my favorite subject' ,'Here i am using python for 
              implementation']]
</code></pre>

<p>I have tried following code but not getting desired results:</p>

<pre><code>from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
import nltk
nltk.download('stopwords')

stop_words = set(stopwords.words('english')) 

word_tokens = word_tokenize(input) 

filtered_sentence = [w for w in word_tokens if not w in stop_words] 

filtered_sentence = [] 

for w in word_tokens: 
    if w not in stop_words: 
        filtered_sentence.append(w) 

#print(word_tokens) 
print(filtered_sentence) 
</code></pre>

<p>Expecting output like below:</p>

<pre><code>Output = [['Hi', 'going', 'college', 'meet','next', 'time', 'possible'],
          ['college', 'name','jntu', 'machine', 'learning', 'specialization'],
          ['Machine', 'learnin', 'favorite', 'subject' ,'using', 'python', 'implementation']]
</code></pre>
",Preprocessing of the text & Tokenization,removing stopwords tokenization python following input data would like remove stopwords input want tokenization tried following code getting desired result expecting output like
Tokenizing lists of strings to return one list of tokenized of words,"<p>Good day,</p>

<p>I have a function that should have the ability to lower and tokenize text and returns tokens. 
Here is the function below:</p>

<pre><code>def preprocess_text(text):
"""""" A function to lower and tokenize text data """""" 
# Lower the text
lower_text = text.lower()

# tokenize the text into a list of words
 tokens = nltk.tokenize.word_tokenize(lower_text)

return tokens
</code></pre>

<p>I then wish to apply the function to my actual text data called <code>data</code> which is a list with strings within it. I want to iterate over each string within <code>data</code> and apply the function to lower and tokenize the text data. </p>

<p>Finally, I wish to append the tokenized words to a final list called <code>tokenized_final</code> which should be the final list containing the tokenized words. 
Here is the next bit of code below: </p>

<pre><code># Final list with tokenized words
tokenized_final = []

# Iterating over each string in data
for x in data:
    # Calliing preprocess text function
    token = preprocess_text(x)

    tokenized_final.append(token)  
</code></pre>

<p>However, when I do all this and print the list <code>tokenized_final</code>. It outputs a big list containing lists within it. </p>

<pre><code>print (tokeninized_final)

Output:
 [['pfe', 'bulls', 'have', 'reasons', 'on'],
 ['to', 'pay', 'more', 'attention'],
 ['there', 'is', 'still']]
</code></pre>

<p>When my desired output for <code>tokenized_final</code> is to be like this in one list: </p>

<pre><code>['pfe', 'bulls', 'have', 'reasons', 'on','to', 'pay','more', 'attention','there','is', 'still']
</code></pre>

<p>Is there any way to rectify the preprocess function and apply it to the data to get the desired output. Or is there any way to do this?...
Help would truly be appreciated here. 
Thanks in advance</p>
",Preprocessing of the text & Tokenization,tokenizing list string return one list tokenized word good day function ability lower tokenize text return token function wish apply function actual text data called list string within want iterate string within apply function lower tokenize text data finally wish append tokenized word final list called final list containing tokenized word next bit code however print list output big list containing list within desired output like one list way rectify preprocess function apply data get desired output way help would truly appreciated thanks advance
How to lemmatize Norwegian using spaCy?,"<p>I'm doing the following:</p>

<pre><code>from spacy.lang.nb import Norwegian
nlp = Norwegian()
doc = nlp(u'Jeg heter Marianne Borgen og jeg er ordføreren i Oslo.')
for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)
</code></pre>

<p>Lemmatization seems to not work at all, as this is the output:</p>

<pre><code>(u'Jeg', u'Jeg', u'', u'', u'', u'Xxx', True, False)
(u'heter', u'heter', u'', u'', u'', u'xxxx', True, False)
(u'Marianne', u'Marianne', u'', u'', u'', u'Xxxxx', True, False)
(u'Borgen', u'Borgen', u'', u'', u'', u'Xxxxx', True, False)
(u'og', u'og', u'', u'', u'', u'xx', True, True)
(u'jeg', u'jeg', u'', u'', u'', u'xxx', True, True)
(u'er', u'er', u'', u'', u'', u'xx', True, True)
(u'ordf\xf8reren', u'ordf\xf8reren', u'', u'', u'', u'xxxx', True, False)
(u'i', u'i', u'', u'', u'', u'x', True, True)
(u'Oslo', u'Oslo', u'', u'', u'', u'Xxxx', True, False)
(u'.', u'.', u'', u'', u'', u'.', False, False)
</code></pre>

<p>However, looking at <a href=""https://github.com/explosion/spaCy/blob/master/spacy/lang/nb/lemmatizer/_verbs_wordforms.py"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/master/spacy/lang/nb/lemmatizer/_verbs_wordforms.py</a>, the verb <em>heter</em> should at least be transformed into <em>hete</em>.</p>

<p>So it looks like spaCy has support, but it's not working? What could be the problem?</p>
",Preprocessing of the text & Tokenization,lemmatize norwegian using spacy following lemmatization seems work output however looking verb heter least transformed hete look like spacy ha support working could problem
Python Text Summarizer - maintain sentence order,"<p>I am teaching myself python and have completed a rudimentary text summarizer. I'm nearly happy with the summarized text but want to polish the final product a bit more.</p>

<p>The code performs some standard text processing correctly (tokenization, remove stopwords, etc). The code then scores each sentence based on a weighted word frequency. I am using the heapq.nlargest() method to return the top 7 sentences which I feel does a good job based on my sample text. </p>

<p>The issue I'm facing is that the top 7 sentences are returned sorted from highest score -> lowest score. I understand the <em>why</em> this is happening. I would prefer to maintain the same sentence order as present in the original text. I've included the relevant bits of code and hope someone can guide me on a solution.</p>

<pre><code>#remove all stopwords from text, build clean list of lower case words
clean_data = []
for word in tokens:
    if str(word).lower() not in stoplist:
        clean_data.append(word.lower())

#build dictionary of all words with frequency counts: {key:value = word:count}
word_frequencies = {}
for word in clean_data:
    if word not in word_frequencies.keys():
        word_frequencies[word] = 1
    else:
        word_frequencies[word] += 1

#print(word_frequencies.items())

#update the dictionary with a weighted frequency 
maximum_frequency = max(word_frequencies.values())
#print(maximum_frequency)

for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/maximum_frequency)

#print(word_frequencies.items())

#iterate through each sentence and combine the weighted score of the underlying word
sentence_scores = {}

for sent in sentence_list:
    for word in nltk.word_tokenize(sent.lower()):
        if word in word_frequencies.keys():
            if len(sent.split(' ')) &lt; 30:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]

#print(sentence_scores.items())                    

summary_sentences = heapq.nlargest(7, sentence_scores, key = sentence_scores.get)

summary = ' '.join(summary_sentences)    

print(summary)
</code></pre>

<p>I'm testing using the following article: <a href=""https://www.bbc.com/news/world-australia-45674716"" rel=""nofollow noreferrer"">https://www.bbc.com/news/world-australia-45674716</a></p>

<p>Current output: ""Australia bank inquiry: 'They didn't care who they hurt'
The inquiry has also heard testimony about corporate fraud, bribery rings at banks, actions to deceive regulators and reckless practices. A royal commission this year, the country's highest form of public inquiry, has exposed widespread wrongdoing in the industry. The royal commission came after a decade of scandalous behaviour in Australia's financial sector, the country's largest industry. ""[The report] shines a very bright light on the poor behaviour of our financial sector,"" Treasurer Josh Frydenberg said. ""When misconduct was revealed, it either went unpunished or the consequences did not meet the seriousness of what had been done,"" he said. The bank customers who lost everything
He also criticised what he called the inadequate actions of regulators for the banks and financial firms. It has also received more than 9,300 submissions of alleged misconduct by banks, financial advisers, pension funds and insurance companies.""</p>

<p>As an example of the desired output: The third sentence above, ""A royal commission this year, the country's highest form of public inquiry, has exposed widespread wrongdoing in the industry."" actually comes before ""Australia bank inquiry: They didnt care who they hurt"" in the original article and I would like the output to maintain that sentence order.</p>
",Preprocessing of the text & Tokenization,python text summarizer maintain sentence order teaching python completed rudimentary text summarizer nearly happy summarized text want polish final product bit code performs standard text processing correctly tokenization remove stopwords etc code score sentence based weighted word frequency using heapq nlargest method return top sentence feel doe good job based sample text issue facing top sentence returned sorted highest score lowest score understand happening would prefer maintain sentence order present original text included relevant bit code hope someone guide solution testing using following article current output australia bank inquiry care hurt inquiry ha also heard testimony corporate fraud bribery ring bank action deceive regulator reckless practice royal commission year country highest form public inquiry ha exposed widespread wrongdoing industry royal commission came decade scandalous behaviour australia financial sector country largest industry report shine bright light poor behaviour financial sector treasurer josh frydenberg said misconduct wa revealed either went unpunished consequence meet seriousness done said bank customer lost everything also criticised called action regulator bank financial firm ha also received submission alleged misconduct bank financial adviser pension fund insurance company example desired output third sentence royal commission year country highest form public inquiry ha exposed widespread wrongdoing industry actually come australia bank inquiry didnt care hurt original article would like output maintain sentence order
Python converting from a panda column to a list?,"<p>I was wondering if I have a file with the following format
<a href=""https://i.sstatic.net/PWdEm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PWdEm.png"" alt=""enter image description here""></a>
and I want to put each column in a list of list since I have more than one sentences:
so the output can look like this</p>

<pre><code>[['Learning centre of The University of Lahore is established for professional development.'], 
 ['These events, destroyed the bond between them.']]
</code></pre>

<p>and the same for the verb column. This is what I tried but it put everything in a single list not a list of lists</p>

<pre><code>train_fn=""/content/data/wiki/wiki1.train.oie""


dfE = pandas.read_csv(train_fn, sep= ""\t"",
                         header=0,
                         keep_default_na=False)
train_textEI = dfE['word'].tolist()
train_textEI = [' '.join(t.split()) for t in train_textEI]
train_textEI = np.array(train_textEI, dtype=object)[:, np.newaxis]
</code></pre>

<p>it outputs each word in a list</p>

<pre><code>[['Learning'],['Center'],['of'],['The'],['University'],['of'],
 ['Lahore'],['is'],['established'],['for'],['the'],
 ['professional'],['development'],['.'],['These'],['events'],[','],
 ['destroyed'],['the'],['bond'],['between'],['them'],['.']]
</code></pre>
",Preprocessing of the text & Tokenization,python converting panda column list wa wondering file following format want put column list list since one sentence output look like verb column tried put everything single list list list output word list
How to remove stop words in OSX terminal?,"<p>For now, I have a txt file made by 6 Shakespeare books and I am going to remove stopwords based on a <a href=""https://raw.githubusercontent.com/igorbrigadir/stopwords/master/en/99webtools.txt"" rel=""nofollow noreferrer"">prepared stop word list</a>.</p>
<p>Firstly, I create an empty txt file and duplicate the original txt file's content in lowercase to the new file(because the stop word list are made by lowercase words).</p>
<pre><code>$cat new_txt.txt | tr [A-Z] [a-z] &gt; new_txt_lowercase.txt
</code></pre>
<p>Then, I use the code below to erase stop words in stop word list:</p>
<pre><code>$ grep -wvf 99webtools.txt new_txt_lowercase.txt&gt;new_txt_no_stopwords.txt

$ grep -v -f 99webtools.txt new_txt_lowercase.txt&gt;new_txt_no_stopwords.txt
</code></pre>
<p>But neither of them works.</p>
<p>Any ideas? is there anything wrong with my code?</p>
",Preprocessing of the text & Tokenization,remove stop word osx terminal txt file made shakespeare book going remove stopwords based prepared stop word list firstly create empty txt file duplicate original txt file content lowercase new file stop word list made lowercase word use code erase stop word stop word list neither work idea anything wrong code
Skip-gram word2vec loss doesn&#39;t decrease,"<p>I'm working on implementaion of word2vec architecture from scratch. But my model doesn't converge.</p>

<pre><code>class SkipGramBatcher:
  def __init__(self, text):
    self.text = text.results

  def get_batches(self, batch_size):
    n_batches = len(self.text)//batch_size
    pairs = []


    for idx in range(0, len(self.text)):
      window_size = 5
      idx_neighbors = self._get_neighbors(self.text, idx, window_size)
      #one_hot_idx = self._to_one_hot(idx)
      #idx_pairs = [(one_hot_idx, self._to_one_hot(idx_neighbor)) for idx_neighbor in idx_neighbors]
      idx_pairs = [(idx,idx_neighbor) for idx_neighbor in idx_neighbors]
      pairs.extend(idx_pairs)


    for idx in range(0, len(pairs), batch_size):
      X = [pair[0] for pair in pairs[idx:idx+batch_size]]
      Y = [pair[1] for pair in pairs[idx:idx+batch_size]]
      yield X,Y

  def _get_neighbors(self, text, idx, window_size):
    text_length = len(text)
    start = max(idx-window_size,0)
    end = min(idx+window_size+1,text_length)
    neighbors_words = set(text[start:end])

    return list(neighbors_words)

  def _to_one_hot(self, indexes):
    n_values = np.max(indexes) + 1
    return np.eye(n_values)[indexes]
</code></pre>

<p>I use text8 corpus and have applied preprocessing techniques such as stemming, lemmatization and subsampling. Also I've excluded English stop words and limited vocabulary </p>

<pre><code>vocab_size = 20000
text_len = len(text)
test_text_len = int(text_len*0.15)
preprocessed_text = PreprocessedText(text,vocab_size)
</code></pre>

<p>I use tensorflow for graph computation</p>

<pre><code>train_graph = tf.Graph()
with train_graph.as_default():
  inputs = tf.placeholder(tf.int32, [None], name='inputs')
  labels = tf.placeholder(tf.int32, [None, None], name='labels')

n_embedding =  300
with train_graph.as_default():
  embedding = tf.Variable(tf.random_uniform((vocab_size, n_embedding), -1, 1))
  embed = tf.nn.embedding_lookup(embedding, inputs)
</code></pre>

<p>And apply negative sampling</p>

<pre><code># Number of negative labels to sample
n_sampled = 100
with train_graph.as_default():
  softmax_w = tf.Variable(tf.truncated_normal((vocab_size, n_embedding))) # create softmax weight matrix here
  softmax_b = tf.Variable(tf.zeros(vocab_size), name=""softmax_bias"") # create softmax biases here

  # Calculate the loss using negative sampling

  loss = tf.nn.sampled_softmax_loss(
      weights=softmax_w,
      biases=softmax_b,
      labels=labels,
      inputs=embed,
      num_sampled=n_sampled,
      num_classes=vocab_size)

  cost = tf.reduce_mean(loss)
  optimizer = tf.train.AdamOptimizer().minimize(cost)
</code></pre>

<p>Finally I train my model</p>

<pre><code>epochs = 10
batch_size = 64
avg_loss = []

with train_graph.as_default():
    saver = tf.train.Saver()

with tf.Session(graph=train_graph) as sess:
  iteration = 1
  loss = 0
  sess.run(tf.global_variables_initializer())

  for e in range(1, epochs+1):
    batches = skip_gram_batcher.get_batches(batch_size)
    start = time.time()
    for  batch_x,batch_y  in batches:

      feed = {inputs: batch_x, 
             labels: np.array(batch_y)[:, None]}
      train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)
      loss += train_loss

      if iteration % 100 == 0: 
        end = time.time()
        print(""Epoch {}/{}"".format(e, epochs),
                ""Iteration: {}"".format(iteration),
                ""Avg. Batch loss: {:.4f}"".format(loss/iteration),
                ""{:.4f} sec/batch"".format((end-start)/100))
        #loss = 0
        avg_loss.append(loss/iteration)
        start = time.time()

      iteration += 1

  save_path = saver.save(sess, ""checkpoints/text8.ckpt"")
</code></pre>

<p>But after running this model my average batch loss doesn't decrease dramatically </p>

<p><a href=""https://i.sstatic.net/g9lAl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/g9lAl.png"" alt=""enter image description here""></a></p>

<p>I guess I should have made a mistake somewhere. Any help is apprciated</p>
",Preprocessing of the text & Tokenization,skip gram word vec loss decrease working implementaion word vec architecture scratch model converge use text corpus applied preprocessing technique stemming lemmatization subsampling also excluded english stop word limited vocabulary use tensorflow graph computation apply negative sampling finally train model running model average batch loss decrease dramatically guess made mistake somewhere help apprciated
How to perform stemming in apache spark?,"<p>I am doing a simple project using K-Means clustering in apache spark and i did some preprocessing steps like tokenization, stop words remover,and hashingTF. These are performed by spark own Tokenization(),StopWordRemover() and HasingTF(). But i want to perform stemming before applying k-means clustering.I have tried some NLP libraries in openNLP. But I don't know how to implement it in spark DataFrame.
Can someone help me how to do it.</p>
",Preprocessing of the text & Tokenization,perform stemming apache spark simple project using k mean clustering apache spark preprocessing step like tokenization stop word remover hashingtf performed spark tokenization stopwordremover hasingtf want perform stemming applying k mean clustering tried nlp library opennlp know implement spark dataframe someone help
Cosine-similarity between columns in a Spark dataframe,"<p>I have data that looks like this...</p>

<pre><code>+-----------+--------------------+
| searchterm|               title|
+-----------+--------------------+
|red ball   |A big red ball      |
|red ball   |A small blue ball   |
|...        |...                 |
+-----------+--------------------+
</code></pre>

<p>I'm trying to find the cosine similarity between the searchterm column and the title column in Scala.  I'm able to tokenize each column without issue, but most similarity implementations I have found online operate across rows and not across columns, i.e. they would compare 'a big red ball' with 'a small blue ball' rather than the cross column comparison I actually want.  Any ideas?  I'm very new to Scala, but this is how I would do it in Python.</p>

<pre><code>def get_text_cosine_similarity(row):

  # Form TF-IDF matrix
  text_arr = row[['searchterm', 'title']].values
  tfidf_vectorizer = TfidfVectorizer()
  tfidf_matrix = tfidf_vectorizer.fit_transform(text_arr)

  # Get cosine similarity 'score', assuming keyword is at index 0
  similarity_scores = cosine_similarity(tfidf_matrix[0], tfidf_matrix)

  return pd.Series(similarity_scores[0][1:])


df[['title_cs']] = df.apply(get_text_cosine_similarity, axis=1)
</code></pre>

<p>Using <code>sklearn.metrics.pairwise.cosine_similarity</code> and <code>sklearn.feature_extraction.text.TfidfVectorizer</code></p>
",Preprocessing of the text & Tokenization,cosine similarity column spark dataframe data look like trying find cosine similarity searchterm column title column scala able tokenize column without issue similarity implementation found online operate across row across column e would compare big red ball small blue ball rather cross column comparison actually want idea new scala would python using
How can I prevent spacy&#39;s tokenizer from splitting a specific substring when tokenizing a string?,"<p>How can I prevent spacy's tokenizer from splitting a specific substring when tokenizing a string?</p>

<p>More specifically, I have this sentence:</p>

<blockquote>
  <p>Once unregistered, the folder went away from the shell.</p>
</blockquote>

<p>which gets tokenized as [Once/unregistered/,/the/folder/went/away/from/the/<strong>she/ll</strong>/.] by scapy 1.6.0. I don't want the substring <code>shell</code> to be cut into two different tokens <code>she</code> and <code>ll</code>.</p>

<hr>

<p>Here is the code I use:</p>

<pre><code># To install spacy:
# sudo pip install spacy
# sudo python -m spacy.en.download parser # will take 0.5 GB

import spacy
nlp = spacy.load('en')

# https://spacy.io/docs/usage/processing-text
document = nlp(u'Once unregistered, the folder went away from the shell.')

for token in document:
    print('token.i: {2}\ttoken.idx: {0}\ttoken.pos: {3:10}token.text: {1}'.
      format(token.idx, token.text,token.i,token.pos_))
</code></pre>

<p>which outputs:</p>

<pre><code>token.i: 0      token.idx: 0    token.pos: ADV       token.text: Once
token.i: 1      token.idx: 5    token.pos: ADJ       token.text: unregistered
token.i: 2      token.idx: 17   token.pos: PUNCT     token.text: ,
token.i: 3      token.idx: 19   token.pos: DET       token.text: the
token.i: 4      token.idx: 23   token.pos: NOUN      token.text: folder
token.i: 5      token.idx: 30   token.pos: VERB      token.text: went
token.i: 6      token.idx: 35   token.pos: ADV       token.text: away
token.i: 7      token.idx: 40   token.pos: ADP       token.text: from
token.i: 8      token.idx: 45   token.pos: DET       token.text: the
token.i: 9      token.idx: 49   token.pos: PRON      token.text: she
token.i: 10     token.idx: 52   token.pos: VERB      token.text: ll
token.i: 11     token.idx: 54   token.pos: PUNCT     token.text: .
</code></pre>
",Preprocessing of the text & Tokenization,prevent spacy tokenizer splitting specific substring tokenizing string prevent spacy tokenizer splitting specific substring tokenizing string specifically sentence unregistered folder went away shell get tokenized unregistered folder went away scapy want substring cut two different token code use output
How to find topics and phrases (verbs/adjectives) that are grouped with a specific word?,"<p>I have a large tidy data set with columns containing text responses(i.e., in a grant application) and rows as the individual organization applying for the grant. I'm trying to find the topics and phrases grouped with a specific word (e.g., ""funder""/""funding""). More specifically, what adjectives and verbs are being grouped with these tokens?  </p>

<p>So for example </p>

<pre><code>text &lt;- ""This funding would help us create a new website and hire talented people.""
</code></pre>

<p>So ""funding"" can be grouped with verbs like ""create"", ""hire"", and adjective phrases like ""new website"", ""talented people"". </p>

<p>I'm doing this in R. Does anyone have a package or program in which they'd recommend doing this? I've found cleanNLP, but not sure if this is the most convenient package. Would I need to tokenize all the words? If so, wouldn't I have problems grouping phrases?</p>

<p>I'm fairly new to NLP/text mining, so I apologize for the introductory question. </p>

<p>Thank you!</p>
",Preprocessing of the text & Tokenization,find topic phrase verb adjective grouped specific word large tidy data set column containing text response e grant application row individual organization applying grant trying find topic phrase grouped specific word e g funder funding specifically adjective verb grouped token example funding grouped verb like create hire adjective phrase like new website talented people r doe anyone package program recommend found cleannlp sure convenient package would need tokenize word problem grouping phrase fairly new nlp text mining apologize introductory question thank
"How to auto-tag content, algorithms and suggestions needed","<p>I am working with some really large databases of newspaper articles, I have them in a MySQL database, and I can query them all.</p>

<p>I am now searching for ways to help me tag these articles with somewhat descriptive tags. </p>

<p>All these articles is accessible from a URL that looks like this:</p>

<pre><code>http://web.site/CATEGORY/this-is-the-title-slug
</code></pre>

<p>So at least I can use the category to figure what type of content that we are working with. However, I also want to tag based on the article-text.</p>

<p>My initial approach was doing this:</p>

<ol>
<li>Get all articles</li>
<li>Get all words, remove all punctuation, split by space, and count them by occurrence </li>
<li>Analyze them, and filter common non-descriptive words out like ""them"", ""I"", ""this"", ""these"", ""their"" etc.</li>
<li>When all the common words was filtered out, the only thing left is words that is tag-worthy.</li>
</ol>

<p>But this turned out to be a rather manual task, and not a very pretty or helpful approach. </p>

<p>This also suffered from the problem of words or names that are split by space, for example if 1.000 articles contains the name ""John Doe"", and 1.000 articles contains the name of ""John Hanson"", I would only get the word ""John"" out of it, not his first name, and last name.</p>
",Preprocessing of the text & Tokenization,auto tag content algorithm suggestion needed working really large database newspaper article mysql database query searching way help tag article somewhat descriptive tag article accessible url look like least use category figure type content working however also want tag based article text initial approach wa get article get word remove punctuation split space count occurrence analyze filter common non descriptive word like etc common word wa filtered thing left word tag worthy turned rather manual task pretty helpful approach also suffered problem word name split space example article contains name john doe article contains name john hanson would get word john first name last name
What are the form of the inputs into the BERT NLP model?,"<p>After having read the BERT paper (<a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1810.04805</a>), I am still slightly confused as to what for the input (words) take. </p>

<p>They refer to a paper which essentially tokenize words, but do not explicitly show what it looks like.</p>

<p>Thanks for the clarification.</p>

<p>Tom</p>
",Preprocessing of the text & Tokenization,form input bert nlp model read bert paper still slightly confused input word take refer paper essentially tokenize word explicitly show look like thanks clarification tom
How do you count a negative or positive word prior to a specific word - Sentiment Analysis in Python?,"<p>I'm trying to count how many times a negative word from a list appears before a specific word. For example, ""This terrible laptop."" The specified word being ""laptop"", I want the output to have ""Terrible 1"" in Python.</p>

<pre><code>def run(path):
    negWords={} #dictionary to return the count
    #load the negative lexicon
    negLex=loadLexicon('negative-words.txt')
    fin=open(path)

    for line in fin: #for every line in the file (1 review per line)
        line=line.lower().strip().split(' ')
        review_set=set() #Adding all the words in the review to a set

        for word in line: #Check if the word is present in the line
            review_set.add(word)  #As it is a set, only adds one time

        for word in review_set:
            if word in negLex:
                if word in negWords:
                    negWords[word]=negWords[word]+1
                else:
                    negWords[word] = 1

    fin.close()
    return negWords

if __name__ == ""__main__"": 
    print(run('textfile'))
</code></pre>
",Preprocessing of the text & Tokenization,count negative positive word prior specific word sentiment analysis python trying count many time negative word list appears specific word example terrible laptop specified word laptop want output terrible python
"pre-trained Word2Vec with LSTM, predict next word in sentence","<p>I have a corpus of text. For a preprocessing data I've vectorized all text using gensim Word2Vec.
I don't understand what I do exactly wrong. For the base I've took this discussion (and good tutorial) <a href=""https://stackoverflow.com/questions/42064690/using-pre-trained-word2vec-with-lstm-for-word-generation"">predict next word</a>. Code: <a href=""https://gist.github.com/maxim5/c35ef2238ae708ccb0e55624e9e0252b"" rel=""nofollow noreferrer"">Source code</a>.</p>

<p>As input I have lines of sentences. I want to take each line, then take word[0] of this line -> predict word[1 ]. Then using word[0] and word[1 ] predict word[3], and so on to the end of line.</p>

<p>In this tutorial each time predicts fix length of words.
What I do:</p>

<pre><code>def on_epoch_end(epoch, _):
    print('\nGenerating text after epoch: %d' % epoch)
    for sentence in inpt:
        word_first=sentence.split()[0]
        sample = generate_next(word_first, len(sentence))
        print('%s... -&gt; %s' % (word_first, sample))
</code></pre>

<p>I take first word and use it to generate all next. And as second parameter I give length of sentence (not <code>num_generated=10</code>) as in tutorial. But it doesn't help for me at all. Every time I'm getting output predicted sequence of words with random(in my opinion) length.</p>

<p>What am I doing wrong and how to fix it?</p>
",Preprocessing of the text & Tokenization,pre trained word vec lstm predict next word sentence corpus text preprocessing data vectorized text using gensim word vec understand exactly wrong base took discussion good tutorial source code input line sentence want take line take word line predict word using word word predict word end line tutorial time predicts fix length word take first word use generate next second parameter give length sentence tutorial help every time getting output predicted sequence word random opinion length wrong fix
nlp multilabel classification tf vs tfidf,"<p>I am trying to solve an NLP multilabel classification problem. I have a huge amount of documents that should be classified into 29 categories. </p>

<p>My approach to the problem was, after cleaning up the text, stop word removal, tokenizing etc., is to do the following:</p>

<p>To create the features matrix I looked at the frequency distribution of the terms of each document, I then created a table of these terms (where duplicate terms are removed), I then calculated the term frequency for each word in its corresponding text (<code>tf</code>). So, eventually I ended up with around a 1000 terms and their respected frequency in each document. </p>

<p>I then used <code>selectKbest</code> to narrow them down to around 490. and after scaling them I used OneVsRestClassifier(<code>SVC</code>) to do the classification. </p>

<p>I am getting an <code>F1 score</code> around <code>0.58</code> but it is not improving at all and I need to get <code>0.62</code>. </p>

<p>Am I handling the problem correctly? </p>

<p>Do I need to use <code>tfidf vectorizer</code> instead of <code>tf</code>, and how? </p>

<p>I am very new to NLP and I am not sure at all what to do next and how to improve the score. </p>

<p>Any help in this subject is priceless. </p>

<p>Thanks</p>
",Preprocessing of the text & Tokenization,nlp multilabel classification tf v tfidf trying solve nlp multilabel classification problem huge amount document classified category approach problem wa cleaning text stop word removal tokenizing etc following create feature matrix looked frequency distribution term document created table term duplicate term removed calculated term frequency word corresponding text eventually ended around term respected frequency document used narrow around scaling used onevsrestclassifier classification getting around improving need get handling problem correctly need use instead new nlp sure next improve score help subject priceless thanks
Numeric Ranges with a Regular Expression python,"<p>So I am working on a text analytics problem and I am trying to remove all the numbers between 0 and 999 with regular expression in Python. I have tried Regex Numeric Range Generator to get the regular expression but I didn't succed. I can only remove all the numbers. </p>

<p>I have tried several Regex but it didn't work. here's what I tried</p>

<pre><code># Remove numbers starting from 0 ==&gt; 999
data_to_clean = re.sub('[^[0-9]{1,3}$]', ' ', data_to_clean)
</code></pre>

<p>I have tried this also:</p>

<pre><code># Remove numbers starting from 0 ==&gt; 999
data_to_clean = re.sub('\b([0-9]|[1-8][0-9]|9[0-9]|[1-8][0-9]{2}|9[0-8][0-9]|99[0-9])\b', ' ', data_to_clean)  
</code></pre>

<p>this one:</p>

<pre><code>^([0-9]|[1-8][0-9]|9[0-9]|[1-8][0-9]{2}|9[0-8][0-9]|99[0-9])$
</code></pre>

<p>and this:</p>

<pre><code>def clean_data(data_to_clean):
    # Remove numbers starting from 0 ==&gt; 999
    data_to_clean = re.sub('[^[0-9]{1,3}$]', ' ', data_to_clean)  
    return data_to_clean
</code></pre>

<p>I have a lot of numbers but I need to delete just the ones under 3 decimals and keep the other.</p>

<p>Thank You for your help</p>
",Preprocessing of the text & Tokenization,numeric range regular expression python working text analytics problem trying remove number regular expression python tried regex numeric range generator get regular expression succed remove number tried several regex work tried tried also one lot number need delete one decimal keep thank help
How Word2Vec works? Python,"<p>I've got a question about gensim <strong>Word2Vec</strong> and documentation doesn't help me.</p>

<p>For example in my block of text I have some sentences like:</p>

<pre><code>&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt;
&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt;
&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt;
         ...
</code></pre>

<p>And in some time I have a new sentence like:</p>

<pre><code>&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt; &lt;Word4&gt;
</code></pre>

<p>How can I detect this situation? (of course Word4 is in dictionary too)</p>

<p>My solutions:
1). I tried to find most similar words for each and see - if the next word is in this is - OK, otherwise - I can find Word4. I mean I will do:</p>

<pre><code>model.most_similar('&lt;Word_i&gt;')
or
model.similar_by_vector('&lt;Word_i&gt;')
</code></pre>

<p>And in top of answer list I will get Word_i+1. But it doesn't work!
Because I thought that the words in the sentence after training will have quite similar coordinates and in top list Word_i+1 will be for Word_i.
But it's wrong. When I checked this solution and trained by all corpus of text I had a situation when Word_2 wasn't in top list for Word_1! My explanation that not the near words have quite similar coordinates, but words with contextual proximity have quite similar coordinates, it's not the same..</p>

<p>2). So my second solution is using <strong>doesnt_match()</strong>, which takes a list of words, and reports the one word which is furthest from the average of all the words.</p>

<pre><code>print(model.doesnt_match('&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt; &lt;Word4&gt;'.split()))
</code></pre>

<p>And yes - in this case the answer will be Word4! (so I detect this word)
But if I do it with:</p>

<pre><code>print(model.doesnt_match('&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt;'.split()))
</code></pre>

<p>The answer will be Word2 (for example). And if I again will explore top words for Word1 and Word3 I won't see Word2 in this lists, but this sentence (Word1 Word2 Word3) is normal.</p>

<p>So how can I detect it?</p>
",Preprocessing of the text & Tokenization,word vec work python got question gensim word vec documentation help example block text sentence like time new sentence like detect situation course word dictionary solution tried find similar word see next word ok otherwise find word mean top answer list get word work thought word sentence training quite similar coordinate top list word word wrong checked solution trained corpus text situation word top list word explanation near word quite similar coordinate word contextual proximity quite similar coordinate second solution using doesnt match take list word report one word furthest average word yes case answer word detect word answer word example explore top word word word see word list sentence word word word normal detect
Parsing a List of Tweets in Order to Utlize Gensim Word2Vec,"<p>I'm working on an NLP problem and my goal is to be able to pass my data into sklearn's algos after having used Word2Vec via Python's Gensim Library. The underlying problem I am trying to solve is binary classification of a series of tweets. To do so I am modifying the code in <a href=""https://github.com/halidebey/PyCon2018/blob/master/analysis.py"" rel=""nofollow noreferrer"">this</a> git repo.</p>

<p>Here is part of the code relating to tokenization:</p>

<pre><code>from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')
input_file[""tokens""] = input_file[""text""].apply(tokenizer.tokenize)
all_words = [word for tokens in input_file[""tokens""] for word in tokens]
sentence_lengths = [len(tokens) for tokens in input_file[""tokens""]]
vocabulary = sorted(set(all_words))
</code></pre>

<p>Now here is the part where I use Gensim's sklearn-api to try to vectorize my tweets:</p>

<pre><code>from sklearn.model_selection import train_test_split
from gensim.test.utils import common_texts
from gensim.sklearn_api import W2VTransformer
text = input_file[""text""].tolist()
labels = input_file[""label""].tolist()
X_train, X_test, y_train, y_test = train_test_split(text, labels, test_size=0.2,random_state=40)
model = W2VTransformer(size=10, min_count=1, seed=1)
X_train_w2v = model.fit(common_texts).transform(X_train)
</code></pre>

<p>This results in the following error:</p>

<pre><code>KeyError: ""word 'Great seeing you again, don't be a stranger!' not in vocabulary""
</code></pre>

<p>It seems that part of the issue is that Gensim is expecting to be fed one word at a time and instead it is getting entire tweets.</p>

<p>X_train is of type list, here are the first three elements of the list:</p>

<pre><code>[""Great seeing you again, don't be a stranger!"",
 ""Beautiful day here in sunny Prague. Not a cloud in the sky"",
 "" pfft! i wish I had a laptop like that""]
</code></pre>

<p><strong>Update</strong></p>

<p>In order to remedy this, I have tried the following:</p>

<pre><code>X_train_list = []
for sentence in X_train:
word_list = sentence.split(' ')
while("""" in word_list): 
    word_list.remove("""") 
X_train_list.append(word_list)
model = W2VTransformer(size=10, min_count=1, seed=1)
X_train_tfidf = model.fit(common_texts).transform(X_train_list)
</code></pre>

<p>This produces the following error:</p>

<pre><code>KeyError: ""word 'here' not in vocabulary""
</code></pre>

<p>To be honest, this one blows my mind! How a common word like 'here' is not in the vocabulary is beyond me. Also wondering if tweets with stray letters will throwing errors, I imagine the weird jumbles of letters that often pass for words will cause similar issues.</p>
",Preprocessing of the text & Tokenization,parsing list tweet order utlize gensim word vec working nlp problem goal able pas data sklearn algos used word vec via python gensim library underlying problem trying solve binary classification series tweet modifying code git repo part code relating tokenization part use gensim sklearn api try vectorize tweet result following error seems part issue gensim expecting fed one word time instead getting entire tweet x train type list first three element list update order remedy tried following produce following error honest one blow mind common word like vocabulary beyond also wondering tweet stray letter throwing error imagine weird jumble letter often pas word cause similar issue
Python - Regex &quot;Machine Learning&quot;,"<p>I have thousands of lines of text where I need to find money-representations e.g.:</p>

<pre><code>Lorem ipsum dolor sit amet, 100.000,00 USD sadipscing elitr, sed diam nonumy eirmod 
GBP 400 ut labore et dolore magna aliquyam erat, sed diam voluptua. At USD 20 eos et 
accusam et justo duo dolores et 100,000.00 USD  ea rebum. Stet 3,-- USD gubergren, no 
</code></pre>

<p>The Python script should return the amount converted to USD. (e.g. 100000USF, 400 GBP -> USD, etc) </p>

<p>What I did so far was manually creating Regular expressions for number - currency combinations to retreive the value, then compare the currency against a database and calculate the exchange.</p>

<p>However, this is neither efficient nor future proof (e.g. if another currency is added)
So I'm wondering wether there is an efficient machine learning algorithm that I could ""train"" with some examples and it then tries to find sich ""value - currency"" combinations?</p>
",Preprocessing of the text & Tokenization,python regex machine learning thousand line text need find money representation e g python script return amount converted usd e g usf gbp usd etc far wa manually creating regular expression number currency combination retreive value compare currency database calculate exchange however neither efficient future proof e g another currency added wondering wether efficient machine learning algorithm could train example try find sich value currency combination
Best way to map words with multiple spellings to a list of key words?,"<p>I have a pile of ngrams of variable spelling, and I want to map each ngram to it's best match word out of a list of known desired outputs.</p>

<p>For example, ['mob', 'MOB', 'mobi', 'MOBIL', 'Mobile] maps to a desired output of 'mobile'.</p>

<p>Each input from ['desk', 'Desk+Tab', 'Tab+Desk', 'Desktop', 'dsk'] maps to a desired output of 'desktop'</p>

<p>I have about 30 of these 'output' words, and a pile of about a few million ngrams (much fewer unique).</p>

<p>My current best idea was to get all unique ngrams, copy and paste that into Excel and manually build a mapping table, took too long and isn't extensible. 
Second idea was something with fuzzy (fuzzy-wuzzy) matching but it didn't match well. </p>

<p>I'm not experienced in Natural Language terminology or libraries at all so I can't find an answer to how this might be done better, faster and more extensibly when the number of unique ngrams increases or 'output' words change. </p>

<p>Any advice? </p>
",Preprocessing of the text & Tokenization,best way map word multiple spelling list key word pile ngrams variable spelling want map ngram best match word list known desired output example mob mob mobi mobil mobile map desired output mobile input desk desk tab tab desk desktop dsk map desired output desktop output word pile million ngrams much fewer unique current best idea wa get unique ngrams copy paste excel manually build mapping table took long extensible second idea wa something fuzzy fuzzy wuzzy matching match well experienced natural language terminology library find answer might done better faster extensibly number unique ngrams increase output word change advice
How to combine 10 NLP preprocessing regexes?,"<p>Here are all the things I want to do to a Pandas dataframe in one pass in python:</p>

<ol>
<li>Lowercase text</li>
<li>Remove whitespace</li>
<li>Remove numbers</li>
<li>Remove special characters</li>
<li>Remove emails</li>
<li>Remove stop words</li>
<li>Remove NAN</li>
<li>Remove weblinks</li>
<li>Expand contractions (if possible not necessary)</li>
<li>Tokenize</li>
</ol>

<p>I am using Dask to parallelize my Python dataframe functions.</p>

<p>Here's an example function:</p>

<pre><code>df2 = df.map_partitions(lambda d: d.replace(r'\t|\r|\n', '', regex=True))
</code></pre>

<p>However, I have one of the above functions for all of the above preprocessing steps I am trying to do. Is there a way to combine all the regexes? I consider using or pipes, but I don't know if that's the optimal solution.</p>
",Preprocessing of the text & Tokenization,combine nlp preprocessing regexes thing want panda dataframe one pas python lowercase text remove whitespace remove number remove special character remove email remove stop word remove nan remove weblinks expand contraction possible necessary tokenize using dask parallelize python dataframe function example function however one function preprocessing step trying way combine regexes consider using pipe know optimal solution
Tokenize words into syllables (gujarati characters) for Gujarati,"<p>I am trying to tokenize <code>Gujarati</code> (an <code>Indian</code> language) word into characters. </p>

<p>Example :
વાનર is word then I want list of characters like [વા, ન, ર]</p>

<p>I tried <code>java.text.BreakIterator</code> with <code>Gujarati</code> <code>locale</code> but it did not work. Though it works perfectly for <code>Hindi</code>.</p>

<p>Here is code :</p>

<pre><code>import java.text.*;
import java.util.*;

public class Language{
    public static void main(String[] args) {
        String text = ""વાનર"";
        Locale gujarati = new Locale(""gu"",""IN"");
        BreakIterator breaker = BreakIterator.getCharacterInstance(gujarati);
        breaker.setText(text);
        int start = breaker.first();
        for (int end = breaker.next(); end != BreakIterator.DONE; start = end, end = breaker.next()) {
            System.out.println(text.substring(start,end));
        }    
    }
}
</code></pre>

<p>Ouput:</p>

<pre><code>વ
ા
ન
ર
</code></pre>

<p>Is there any library that can do it correctly?
I am fine with languages other than <code>Java</code></p>
",Preprocessing of the text & Tokenization,tokenize word syllable gujarati character gujarati trying tokenize language word character example word want list character like tried work though work perfectly code ouput library correctly fine language
Combine slash separated words to form a string in Python,"<p>I would like to find a way to combine slash separated words from a list of strings. Here's an example:</p>

<pre><code>string1 = 'awesome/stupid'
string2 = 'red/blue/yellow' 
string3 = 'car'


def some_func(string1, strin2):

.
.
.
</code></pre>

<p>Output:</p>

<pre><code>'awesome red car'
'awesome blue car'
'awesome yellow car'    
'stupid red car'
'stupid blue car'
'stupid yellow car'
</code></pre>

<p>What I've tried splitting by '/' and adding strings, but the loop ends up being too big. The solution I'm looking for is for a general case, where I don't know the amount of words separated by slashs.
Thanks in advance.</p>
",Preprocessing of the text & Tokenization,combine slash separated word form string python would like find way combine slash separated word list string example output tried splitting adding string loop end big solution looking general case know amount word separated slash thanks advance
Remove stopwords list from list in Python (Natural Language Processing),"<p>I have been trying to remove stopwords using python 3 code but my code does not seem to work,I want to know how to remove stop words from the below list.  The example structure is as below:</p>

<pre><code>    from nltk.corpus import stopwords

    word_split1=[['amazon','brand','- 
    ','solimo','premium','almonds',',','250g','by','solimo'],
    ['hersheys','cocoa', 'powder', ',', '225g', 'by', 'hersheys'], 
    ['jbl','t450bt','extra','bass','wireless','on- 
    ear','headphones','with','mic','white','by','jbl','and']]
</code></pre>

<p>I am trying to remove stop words and tried the below is my code and i would appreciate if anyone can help me rectify the issue.. here is the code below</p>

<pre><code>    stop_words = set(stopwords.words('english'))

    filtered_words=[]
    for i in word_split1:
        if i not in stop_words:
            filtered_words.append(i)
</code></pre>

<p>I get error:</p>

<pre><code>    Traceback (most recent call last):
    File ""&lt;ipython-input-451-747407cf6734&gt;"", line 3, in &lt;module&gt;
    if i not in stop_words:
    TypeError: unhashable type: 'list'
</code></pre>
",Preprocessing of the text & Tokenization,remove stopwords list list python natural language processing trying remove stopwords using python code code doe seem work want know remove stop word list example structure trying remove stop word tried code would appreciate anyone help rectify issue code get error
Is there a better way to tokenize some strings?,"<p>I was trying to write a code for tokenization of strings in python for some NLP and came up with this code:</p>

<pre><code>str = ['I am Batman.','I loved the tea.','I will never go to that mall again!']
s= []
a=0
for line in str:
    s.append([])
    s[a].append(line.split())
    a+=1
print(s)
</code></pre>

<p>the output came out to be:</p>

<pre><code>[[['I', 'am', 'Batman.']], [['I', 'loved', 'the', 'tea.']], [['I', 'will', 'never', 'go', 'to', 'that', 'mall', 'again!']]]
</code></pre>

<p>As you can see, the list now has an extra dimension, for example, If I want the word 'Batman', I would have to type <code>s[0][0][2]</code> instead of <code>s[0][2]</code>, so I changed the code to:</p>

<pre><code>str = ['I am Batman.','I loved the tea.','I will never go to that mall again!']
s= []
a=0
m = []
for line in str:
    s.append([])
    m=(line.split())
    for word in m:
        s[a].append(word)
    a += 1
print(s)
</code></pre>

<p>which got me the correct output:</p>

<pre><code>[['I', 'am', 'Batman.'], ['I', 'loved', 'the', 'tea.'], ['I', 'will', 'never', 'go', 'to', 'that', 'mall', 'again!']]
</code></pre>

<p>But I have this feeling that this could work with a single loop, because the dataset that I will be importing would be pretty large and a complexity of <code>n</code> would be a lot better that <code>n^2</code>, so, is there a better way to do this/a way to do this with one loop?</p>
",Preprocessing of the text & Tokenization,better way tokenize string wa trying write code tokenization string python nlp came code output came see list ha extra dimension example want word batman would type instead changed code got correct output feeling could work single loop dataset importing would pretty large complexity would lot better better way way one loop
How to search multiple items in a xlsx sheet in python,"<p>I'm taking input from the user and then tokenizing it, tokenization is successful but the problem i'm facing is it does not display anything </p>

<p>i am trying to search the words in xlsx file which user inputs and then it should display that complete row in which that specific word exists.</p>

<pre><code>import xlrd
import pandas as pd
from openpyxl import load_workbook
from xlrd import open_workbook
from nltk import word_tokenize


sen = input(""Enter your sentence: "")
sent = word_tokenize(sen)
print(sent)


book = open_workbook(""Pastho dictionary.xlsx"")
for sheet in book.sheets():
    for rowidx in range(sheet.nrows):
        row = sheet.row(rowidx)
        for colidx,cell in enumerate(row):
            for i in sent:
                if cell.value == sent:#row value
                    print (""Found Row Element"")
                    print(rowidx, colidx)
                    print(cell.value)
                    print(row)
</code></pre>

<p>i expect all the input words to be searched and then display the entire row in which that word exists.</p>
",Preprocessing of the text & Tokenization,search multiple item xlsx sheet python taking input user tokenizing tokenization successful problem facing doe display anything trying search word xlsx file user input display complete row specific word exists expect input word searched display entire row word exists
Extracting Word Frequency List from a Large Corpus,"<p>I have a large English corpus named <a href=""http://ghpaetzold.github.io/subimdb/"" rel=""nofollow noreferrer"">SubIMDB</a> and I want to make a list of all the words with their frequency. Meaning that how much they have appeared in the whole corpus. This frequency list should have some characteristics:</p>

<ol>
<li>The words like boy and boys or other grammatical features such as get and getting, the same word or lemma and if there are 3 boy and 2 boys it should list them as Boy 5. However, not for the cases like Go and Went which have irregular forms(or foot and feet)</li>
<li>I want to use this frequency list as a kind of dictionary so whenever I see a word in another part of the program I want to check its frequency in this list. So, better if it is searchable without looking up all the of it.</li>
</ol>

<p>My questions are:</p>

<ol>
<li>For the first problem, what should I do? Lemmatize? or Stemming? or how can I get that?</li>
<li>For second, what kind of variable type I should set it to? like dictionary or lists or what?</li>
<li>Is is the best to save it in csv?</li>
<li>Is there any prepared toolkit for python doing this all?</li>
</ol>

<p>Thank you so much.</p>
",Preprocessing of the text & Tokenization,extracting word frequency list large corpus large english corpus named subimdb want make list word frequency meaning much appeared whole corpus frequency list characteristic word like boy boy grammatical feature get getting word lemma boy boy list boy however case like go went irregular form foot foot want use frequency list kind dictionary whenever see word another part program want check frequency list better searchable without looking question first problem lemmatize stemming get second kind variable type set like dictionary list best save csv prepared toolkit python thank much
How do you extract sentences containing one of a specified list of words followed by an adjective in Python,"<p>Want to extract all sentences containing a set of specified words followed by any adjective, e.g. ""very good""</p>

<p>I tagged each word with its part of speech to recognize any adjective. Then, I specified the pattern using a regular expression. Here is the code:</p>

<pre><code>import nltk
import os
import string
import re

s=[""This"",""Movie"",""is"",""very"",""good""];
v=[""extremely"",""very""];

tagged=nltk.pos_tag(s);

grammar= """"""Chunk: {[v[0]-v[4]]&lt;JJ&gt;}"""""";
parser=nltk.RegexpParser(grammar);
t=parser.parse(tagged); 
</code></pre>

<p>But it didn't recognize the pattern I specified, no pair was labeled with ""Chunk"".</p>
",Preprocessing of the text & Tokenization,extract sentence containing one specified list word followed adjective python want extract sentence containing set specified word followed adjective e g good tagged word part speech recognize adjective specified pattern using regular expression code recognize pattern specified pair wa labeled chunk
MetaMap java.lang.OutOfMemoryError: Java heap space,"<p>We keep encountering a <code>java.lang.OutOfMemoryError: Java heap space</code> error when running MetaMap (with Java API and UIMA wrapper). </p>

<p>Unfortunately, the logs are not very informative, so we don't know which file it's puking on.</p>

<p>In the past, we've had issues with MetaMap creating huge circular annotations when it's encountered the pipe (<code>|</code>) symbol. However, the file set we're using (<a href=""https://mimic.physionet.org"" rel=""nofollow noreferrer"">MIMIC notes</a>) don't contain any pipe symbols. Are there other characters that may be exhibiting similar behavior to the pipe symbol?</p>

<p>We could increase system RAM to circumvent the heap space issue (it's actually not able to use the maximum set heap, which is set to 6 GB, since system RAM is limited), but we would prefer to know what is causing the issue, especially since then the output file size is more manageable.</p>

<p><strong>* EDIT *</strong></p>

<p>Just to clarify: We have increased memory resources for the JVM and that does help to actually push the data through (this was tested on a local VM). The problem MetaMap has is that it creates enormous circular annotations that eat up the JVM resources (and on our current system, the OS RAM is not optimal). </p>

<p>As noted in my comment below, we preprocess the files to strip them of any characters that throw errors. The heap space error is kind of annoying though, since unlike for other errors we've encounter (e.g., spaces surrounding a lone period, as in <code>text . text</code>), these just throw a parsing error with the text that threw the error. In the case of the pipe symbol, we found it by increasing RAM (on the VM we were initially testing this on) and then looking at the annotations in the UIMA viewer. We were able to identify the problematic files, since the output file size of the XMI with circular annotations is enormous. </p>

<p>We are running some tests on the VM again to see if we can identify the issue, but if anyone has MetaMap experience to help us identify any problem characters or character sequences, that would be desirable.</p>

<p><strong>* EDIT 2 *</strong></p>

<p>Memory should not be an issue. We are running the app using <code>export JAVA_TOOL_OPTIONS='-Xms2G -Xmx6G -XX:MinHeapFreeRatio=25 -XX:+UseG1GC'</code></p>

<p>there is a fundamental issue with circular annotations we are trying to resolve. This is gobbling up resources and puking.</p>
",Preprocessing of the text & Tokenization,metamap java lang outofmemoryerror java heap space keep encountering error running metamap java api uima wrapper unfortunately log informative know file puking past issue metamap creating huge circular annotation encountered pipe symbol however file set using mimic note contain pipe symbol character may exhibiting similar behavior pipe symbol could increase system ram circumvent heap space issue actually able use maximum set heap set gb since system ram limited would prefer know causing issue especially since output file size manageable edit clarify increased memory resource jvm doe help actually push data wa tested local vm problem metamap ha creates enormous circular annotation eat jvm resource current system ram optimal noted comment preprocess file strip character throw error heap space error kind annoying though since unlike error encounter e g space surrounding lone period throw parsing error text threw error case pipe symbol found increasing ram vm initially testing looking annotation uima viewer able identify problematic file since output file size xmi circular annotation enormous running test vm see identify issue anyone ha metamap experience help u identify problem character character sequence would desirable edit memory issue running app using fundamental issue circular annotation trying resolve gobbling resource puking
Python: NLTK - regexp tokenizer produces empty output,"<p>Im trying to tokenize the text available on NLTK text book (using python 2.7) but the output is not as expected. Is there something I am missing?</p>

<pre><code>text = 'That U.S.A. poster-print costs $12.40...'

pattern = r'''(?x)     # set flag to allow verbose regexps
   ([A-Z]\.)+          # abbreviations, e.g. U.S.A.
   | \w+(-\w+)*        # words with optional internal hyphens
   | \$?\d+(\.\d+)?%?  # currency and percentages, e.g. $12.40, 82%
   | \.\.\.            # ellipsis
   | [][.,;""'?():-_`]  # these are separate tokens; includes ], [
   '''

nltk.regexp_tokenize(text, pattern)


Output: 
 [('', '', ''),
 ('A.', '', ''),
 ('', '-print', ''),
 ('', '', ''),
 ('', '', '.40'),
 ('', '', '')]

Expected:
['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']
</code></pre>
",Preprocessing of the text & Tokenization,python nltk regexp tokenizer produce empty output im trying tokenize text available nltk text book using python output expected something missing
Nested List Iteration,"<p>I was attempting some preprocessing on nested list before attempting a small word2vec and encounter an issue as follow:</p>

<pre><code>corpus = ['he is a brave king', 'she is a kind queen', 'he is a young boy', 'she is a gentle girl']

corpus = [_.split(' ') for _ in corpus]
</code></pre>

<blockquote>
  <p>[['he', 'is', 'a', 'brave', 'king'], ['she', 'is', 'a', 'kind', 'queen'], ['he', 'is', 'a', 'young', 'boy'], ['she', 'is', 'a', 'gentle', 'girl']]</p>
</blockquote>

<p>So the output above was given as a nested list &amp; I intended to remove the stopwords e.g. 'is', 'a'.</p>

<pre><code>for _ in range(0, len(corpus)):
     for x in corpus[_]:
         if x == 'is' or x == 'a':
             corpus[_].remove(x)
</code></pre>

<blockquote>
  <p>[['he', 'a', 'brave', 'king'], ['she', 'a', 'kind', 'queen'], ['he', 'a', 'young', 'boy'], ['she', 'a', 'gentle', 'girl']]</p>
</blockquote>

<p>The output seems indicating that the loop skipped to the next sub-list after removing 'is' in each sub-list instead of iterating entirely.</p>

<p>What is the reasoning behind this? Index? If so, how to resolve assuming I'd like to retain the nested structure.</p>
",Preprocessing of the text & Tokenization,nested list iteration wa attempting preprocessing nested list attempting small word vec encounter issue follow brave king kind queen young boy gentle girl output wa given nested list intended remove stopwords e g brave king kind queen young boy gentle girl output seems indicating loop skipped next sub list removing sub list instead iterating entirely reasoning behind index resolve assuming like retain nested structure
Python - From list of list of tokens to bag of words,"<p>I am struggling with computing bag of words. I have a pandas dataframe with a textual column, that I properly tokenize, remove stop words, and stem.
In the end, for each document, I have a list of strings.</p>

<p>My ultimate goal is to compute bag of words for this column, I've seen that scikit-learn has a function to do that but it works on string, not on a list of string.</p>

<p>I am doing the preprocessing myself with NLTK and would like to keep it that way...</p>

<p>Is there a way to compute bag of words based on a list of list of tokens ? e.g., something like that:</p>

<pre><code>[""hello"", ""world""]
[""hello"", ""stackoverflow"", ""hello""]
</code></pre>

<p>should be converted into</p>

<pre><code>[1, 1, 0]
[2, 0, 1]
</code></pre>

<p>with vocabulary:</p>

<pre><code>[""hello"", ""world"", ""stackoverflow""]
</code></pre>
",Preprocessing of the text & Tokenization,python list list token bag word struggling computing bag word panda dataframe textual column properly tokenize remove stop word stem end document list string ultimate goal compute bag word column seen scikit learn ha function work string list string preprocessing nltk would like keep way way compute bag word based list list token e g something like converted vocabulary
Sentiment Analysis using NLTK,"<p>If you were to do sentiment analysis on reviews text using NLTK in python what would be the high level steps to be followed. There are so many terms in NLTK like stemming, parts of speech to name a few, but I would like to know a high level approach for processing text.</p>
",Preprocessing of the text & Tokenization,sentiment analysis using nltk sentiment analysis review text using nltk python would high level step followed many term nltk like stemming part speech name would like know high level approach processing text
Removing stopwords from a large list of stop words taking forever,"<p>I am doing some NLP on a dataset and I am trying to remove stopwords.</p>

<p>I am not using the nltk built in stopwords and I am using a custom stopwords list (which is about 10k words in different languages)</p>

<p>I first defined the below function</p>

<pre><code>def clean_text(text):
    text = ''.join([word.lower() for word in text if word not in string.punctuation])
    tokens = re.split('\W+', text)
    text = [lm.lemmatize(word) for word in tokens if word not in stopwords]
    return text
</code></pre>

<p>then I applied it to the dataframe as follows:</p>

<pre><code>df_train['clean_text'] = df_train['question_text'].apply(lambda x: clean_text(x))
</code></pre>

<p>My Problem is that it is taking so long to process, so is there a faster way to do this?</p>
",Preprocessing of the text & Tokenization,removing stopwords large list stop word taking forever nlp dataset trying remove stopwords using nltk built stopwords using custom stopwords list k word different language first defined function applied dataframe follows problem taking long process faster way
Tokenizer doesn&#39;t work properly in torchtext,"<p>I got a problem in torchtext, and was struggling with it for a long time. I was trying to tokenize and numericalize the text using torchtext and spacy. I defined my tokenizer as this:</p>

<pre><code>def Sp_Tokenizer(text): 
    return [tok.text for tok in spacy_en.tokenizer(text)]
</code></pre>

<p>It worked good:</p>

<pre><code>Sp_Tokenizer('How are you today')

['How', 'are', 'you', 'today']
</code></pre>

<p>Then I passed this tokenizer into torchtext:</p>

<pre><code>TEXT = data.Field(sequential=True, tokenize=Sp_Tokenizer, lower=False)
</code></pre>

<p>and built the vocab:</p>

<pre><code>corps = ['How are you', 'I am good today', 'He is not well']
TEXT.build_vocab(corps, vectors=""glove.6B.100d"")
</code></pre>

<p>Then I tried</p>

<pre><code>TEXT.numericalize('How are you today')
</code></pre>

<p>I assumed I should get a tensor with 4 numbers (word level), however, what I got was like char level:</p>

<pre><code>tensor([[ 6,  3, 10,  2,  4, 17,  5,  2, 11,  3, 19,  2,  9,  3,  7,  4, 11]])
</code></pre>

<p>What's wrong with that? Is there anythin I can do to fix it? Thanks!</p>
",Preprocessing of the text & Tokenization,tokenizer work properly torchtext got problem torchtext wa struggling long time wa trying tokenize numericalize text using torchtext spacy defined tokenizer worked good passed tokenizer torchtext built vocab tried assumed get tensor number word level however got wa like char level wrong anythin fix thanks
nltk.org example of Sentence segmentation with Naive Bayes Classifier: how does .sent separate sentences and how does the ML algorithm improve it?,"<p>There is an example in nltk.org <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow noreferrer"">book (chapter 6)</a> where they use a NaiveBayesian algorithm to classify a punctuation symbol as finishing a sentence or not finishing one... </p>

<p>This is what they do: First they take a corpus and use the .sent method to get the sentences and build an index from them of where the punctuation symbols that separate them (the <code>boundaries</code>) are. </p>

<p>Then they ""tokenize"" the text (convert it to list of words and punctuation symbols) and apply the following algorithm/function to each token so that they get a list of <strong>features</strong> which are returned in a dictionary: </p>

<pre><code>def punct_features(tokens, i):
    return {'nextWordCapitalized': tokens[i+1][0].isupper(),
        'prevWord': tokens[i-1].lower(),
        'punct': tokens[i],
        'prevWordis1Char': len(tokens[i-1]) == 1}
</code></pre>

<p>These features will be used by the ML algorithm to classify the punctuation symbol as finishing a sentence or not (i.e as a boundary token).</p>

<p>With this fn and the 'boundaries' index, they select all the punctuation tokens, each with its features, and tag them as <code>True</code> boundary, or <code>False</code> one, thus creating a list of <strong>labeled feature-sets</strong>:</p>

<pre><code>featuresets1 = [(punct_features(tokens, i), (i in boundaries)) for i in range(1, len(tokens)-1)
               if tokens[i] in '.?!;']
print(featuresets1[:4])
</code></pre>

<p>This is an example of the outpout we could have when printing the first four sets: </p>

<pre><code>[({'nextWordCapitalized': False, 'prevWord': 'nov', 'punct': '.', 'prevWordis1Char': False}, False), 
({'nextWordCapitalized': True, 'prevWord': '29', 'punct': '.', 'prevWordis1Char': False}, True), 
({'nextWordCapitalized': True, 'prevWord': 'mr', 'punct': '.', 'prevWordis1Char': False}, False), 
({'nextWordCapitalized': True, 'prevWord': 'n', 'punct': '.', 'prevWordis1Char': True}, False)]
</code></pre>

<p>With this, they train and evaluate the punctuation classifier:</p>

<pre><code>size = int(len(featuresets) * 0.1)
train_set, test_set = featuresets[size:], featuresets[:size]
classifier = nltk.NaiveBayesClassifier.train(train_set)

nltk.classify.accuracy(classifier, test_set)
</code></pre>

<p>Now, (1) how and what would such a ML algorithm improve? I can't grasp how could it better the first simple algorithm that just checks if next token from the punctuation symbol is Uppercase and previous is lowercase. Indeed that algorithm is taken to validate that a symbol is a boundary...! And if it doesn't improve it, what could possibly be useful for?</p>

<p>And related with this: (2) is any of these two algorithms how nlpk really separates sentences? I mean, specially if the best is the first simple one, does nltk understand that sentences is just a text between two punctuation symbols that are followed by a word with first chart in uppercase and previous word in lowercase? Is this what .sent method does? Notice that this is far from how Linguistics or better said, the Oxford dictionary, defines a sentence: </p>

<blockquote>
  <p>""A set of words that is complete in itself, typically containing a
  subject and predicate, conveying a statement, question, exclamation,
  or command, and consisting of a main clause and sometimes one or more
  subordinate clauses.""</p>
</blockquote>

<p>Or (3) are the raw corpora texts like <code>treebank</code> or <code>brown</code> already divided by sentences manually? - in this case, what is the criterion to select them? </p>
",Preprocessing of the text & Tokenization,nltk org example sentence segmentation naive bayes classifier doe sent separate sentence doe ml algorithm improve example nltk org book chapter use naivebayesian algorithm classify punctuation symbol finishing sentence finishing one first take corpus use sent method get sentence build index punctuation symbol separate tokenize text convert list word punctuation symbol apply following algorithm function token get list feature returned dictionary feature used ml algorithm classify punctuation symbol finishing sentence e boundary token fn boundary index select punctuation token feature tag boundary one thus creating list labeled feature set example outpout could printing first four set train evaluate punctuation classifier would ml algorithm improve grasp could better first simple algorithm check next token punctuation symbol uppercase previous lowercase indeed algorithm taken validate symbol boundary improve could possibly useful related two algorithm nlpk really separate sentence mean specially best first simple one doe nltk understand sentence text two punctuation symbol followed word first chart uppercase previous word lowercase sent method doe notice far linguistics better said oxford dictionary defines sentence set word complete typically containing subject predicate conveying statement question exclamation command consisting main clause sometimes one subordinate clause raw corpus text like already divided sentence manually case criterion select
Loop iteration through character instead of word when trying to remove stop words from a Pandas Dataframe,"<p>I'm trying to remove stop words from strings stored in a pandas DataFrame, but for some reason instead of iterating through the words of the strings I'm iterating through every character, which gives me an unwanted result. I was not able to find any solution to this problem. </p>

<p>Can someone please explain why am I iterating through the characters instead of the words in the phrase?</p>

<p>I present the code I am using and the results I am getting bellow. The stop words and strings are in Portuguese but I don't think it influences the results.</p>

<pre><code>#List of stop words
stp = set(stopwords.words('portuguese') + list(punctuation))

print(stp)

trainData = pd.DataFrame(columns= ['text', 'response'])

corpus = []

with open('pred.txt', 'r') as f_input:
    corpus += [strip_multiple_whitespaces(line) for line in f_input]

corpus_1 = [strip_non_alphanum(line) for line in corpus]
corpus_2 = [line.rstrip() for line in corpus_1]

train_data = [line.split(' ') for line in corpus_2]

for line in train_data:
    if(line[0] == ''):
        train_data.remove(line)

tmp = pd.DataFrame({'text':train_data[::2], 'response':train_data[1::2]})

trainData = trainData.append(tmp[['text', 'response']], ignore_index=True)

trainData['text'] = trainData['text'].astype(str).str.lower()

print(trainData)

trainData['text'] = trainData['text'].apply(lambda x: [word for word in x if word not in stp])

print(trainData)
</code></pre>

<p>This is the result of printing the stop words:</p>

<pre><code>{'com', 'meu', 'fora', '/', ',', 'aos', 'tu', 'estiver', 'esteve', 'fossem', 'e', 'seu', 'já', '|', 'minha', 'te', 'foi', 'há', 'dos', 'ele', 'fôramos', 'tuas', '[', 'foram', 'para', 'quando', 'for', 'tua', 'estávamos', 'eles', 'sou', 'tiveram', 'estivemos', 'também', 'aquela', 'você', 'tenho', 'às', 'houvera', '-', 'éramos', 'mais', 'houveríamos', '^', '`', '@', 'delas', 'estivéramos', 'nas', 'dele', 'esteja', 'hajamos','hei', 'ela', 'se', ':', 'por', 'na', 'estiverem', 'houveria', 'pelos', 'estivessem', 'tenhamos', 'nos', 'até', 'nós', 'estão', 'tenha', 'teremos', 'nem', 'teu', 'ou', 'estejam', 'fomos', 'sejam', 'forem', 'estive', 'houverei', 'me', '*', 'uma', 'meus', 'houvemos', 'o', 'vocês', 'aquilo', 'não', '%', ""'"", 'ao', 'minhas', 'tinham', '+', 'do', 'aquele', 'sua', 'hajam', 'sejamos', 'a', 'este', 'num', 'era', 'terá', 'serão', 'tivesse', '=', 'houver', 'esse', 'tiverem', 'um', 'mas', 'nossa', 'está', 'houvéssemos', 'serei', 'houverão', 'estivermos', '?', '~', 'teus', 'fôssemos', 'havemos', 'deles', 'dela', 'tivéssemos', 'tivemos', 'depois', '{', 'nossos', 'nossas', 'estivera', 'seria', ')', 'houvéramos', 'seriam', 'formos', 'estas', 'tinha', 'estejamos', 'tivessem', 'eram', 'será', 'fosse', 'estes', 'teria', 'esta', 'estou', 'pelas', 'houveremos', 'tem', 'houveram', 'estamos', 'lhes', 'estivesse', 'tive', 'numa', 'seja', 'tiver', 'que', '$', 'estavam', '&lt;', 'terei', 'houverá', 'seríamos', '&gt;', 'teríamos', 'pela', 'isto', 'à', 'as', 'esses', ';', 'essas','teve', 'suas', 'de', 'em', 'qual', 'houveriam', '#', 'das', '.', '(', 'hão', 'são', 'mesmo', 'sem', 'vos', 'houve', 'lhe', 'houvermos', 'só', 'houvesse', 'seremos', '\\', '}', 'somos', 'como', 'aqueles', 'estiveram', 'temos', 'da', 'tivéramos', 'eu', '""', 'muito', '_', 'nosso', 'pelo', 'no', 'estava', ']', 'tém', 'estivéssemos', 'isso', '&amp;', '!', 'haja', 'tenham', 'elas', 'tivermos', 'terão', 'quem', 'tínhamos', 'teriam', 'os', 'houverem', 'fui', 'tivera', 'aquelas', 'entre', 'seus', 'essa', 'houvessem'}
</code></pre>

<p>This is my Dataframe before removing the stop words: </p>

<pre><code>                                                 text        response
0                               ['o', 'que', 'causa']    [causadorDe]
1       ['o', 'que', 'leva', 'á', 'existência', 'de']    [causadorDe]
2                    ['porquê', 'é', 'que', 'existe']    [causadorDe]
3   ['o', 'que', 'é', 'que', 'esta', 'contido', 'no']     [contidoEm]
4   ['o', 'que', 'é', 'que', 'esta', 'contido', 'na']     [contidoEm]
5                ['qual', 'é', 'o', 'antónimo', 'de']   [antonimoNDe]
6               ['qual', 'é', 'o', 'contrário', 'de']   [antonimoNDe]
7              ['o', 'que', 'é', 'o', 'oposto', 'de']   [antonimoNDe]
8            ['qual', 'é', 'a', 'consequência', 'de']  [finalidadeDe]
9               ['qual', 'é', 'o', 'resultado', 'de']  [finalidadeDe]
10                      ['o', 'que', 'resulta', 'de']  [finalidadeDe]
11                     ['o', 'que', 'usaria', 'para']  [finalidadeDe]
12       ['o', 'que', 'pode', 'ser', 'usado', 'para']  [finalidadeDe]
13                 ['qual', 'é', 'a', 'origem', 'de']  [originarioDe]
14                         ['de', 'onde', 'vem', 'o']  [originarioDe]
15               ['de', 'onde', 'é', 'derivado', 'o']  [originarioDe]
16                 ['qual', 'é', 'a', 'origem', 'de']  [originarioDe]
17                         ['de', 'onde', 'vem', 'a']  [originarioDe]
18               ['de', 'onde', 'é', 'derivada', 'a']  [originarioDe]
19                     ['para', 'que', 'serve', 'um']     [servePara]
20                    ['para', 'que', 'usaria', 'um']     [servePara]
21       ['qual', 'é', 'a', 'finalidade', 'de', 'um']     [servePara]
22                    ['para', 'que', 'serve', 'uma']     [servePara]
23                   ['para', 'que', 'usaria', 'uma']     [servePara]
24      ['qual', 'é', 'a', 'finalidade', 'de', 'uma']     [servePara]
</code></pre>

<p>And this is the result after trying to remove those stop words:</p>

<pre><code>                                                 text        response
0                               [ , q, u,  , c, u, s]    [causadorDe]
1   [ , q, u,  , l, v,  , á,  , x, i, s, t, ê, n, ...    [causadorDe]
2       [p, r, q, u, ê,  , é,  , q, u,  , x, i, s, t]    [causadorDe]
3   [ , q, u,  , é,  , q, u,  , s, t,  , c, n, t, ...     [contidoEm]
4   [ , q, u,  , é,  , q, u,  , s, t,  , c, n, t, ...     [contidoEm]
5       [q, u, l,  , é,  ,  , n, t, ó, n, i, m,  , d]   [antonimoNDe]
6    [q, u, l,  , é,  ,  , c, n, t, r, á, r, i,  , d]   [antonimoNDe]
7                [ , q, u,  , é,  ,  , p, s, t,  , d]   [antonimoNDe]
8   [q, u, l,  , é,  ,  , c, n, s, q, u, ê, n, c, ...  [finalidadeDe]
9       [q, u, l,  , é,  ,  , r, s, u, l, t, d,  , d]  [finalidadeDe]
10                  [ , q, u,  , r, s, u, l, t,  , d]  [finalidadeDe]
11                  [ , q, u,  , u, s, r, i,  , p, r]  [finalidadeDe]
12   [ , q, u,  , p, d,  , s, r,  , u, s, d,  , p, r]  [finalidadeDe]
13            [q, u, l,  , é,  ,  , r, i, g, m,  , d]  [originarioDe]
14                           [d,  , n, d,  , v, m,  ]  [originarioDe]
15            [d,  , n, d,  , é,  , d, r, i, v, d,  ]  [originarioDe]
16            [q, u, l,  , é,  ,  , r, i, g, m,  , d]  [originarioDe]
17                           [d,  , n, d,  , v, m,  ]  [originarioDe]
18            [d,  , n, d,  , é,  , d, r, i, v, d,  ]  [originarioDe]
19               [p, r,  , q, u,  , s, r, v,  , u, m]     [servePara]
20            [p, r,  , q, u,  , u, s, r, i,  , u, m]     [servePara]
21  [q, u, l,  , é,  ,  , f, i, n, l, i, d, d,  , ...     [servePara]
22               [p, r,  , q, u,  , s, r, v,  , u, m]     [servePara]
23            [p, r,  , q, u,  , u, s, r, i,  , u, m]     [servePara]
24  [q, u, l,  , é,  ,  , f, i, n, l, i, d, d,  , ...     [servePara]
</code></pre>
",Preprocessing of the text & Tokenization,loop iteration character instead word trying remove stop word panda dataframe trying remove stop word string stored panda dataframe reason instead iterating word string iterating every character give unwanted result wa able find solution problem someone please explain iterating character instead word phrase present code using result getting bellow stop word string portuguese think influence result result printing stop word dataframe removing stop word result trying remove stop word
Multi language Lemmatization in Python,"<p>I have a dataset in multiple languages. 
can I apply lemmatization according to its language?
I have already separated data according to its language.
Tried using WordNet lemmatization, but it only supports English language</p>

<p>For stemming in multiple languages, I am using snowballStemmer.</p>
",Preprocessing of the text & Tokenization,multi language lemmatization python dataset multiple language apply lemmatization according language already separated data according language tried using wordnet lemmatization support english language stemming multiple language using snowballstemmer
Swift NaturalLanguage framework error “Token SequenceType length is 0&quot;,"<p>I’m currently using Apple’s CreateML to generate a NLP model from a JSON file I have. Here is my code:</p>

<pre><code>import Foundation
import CreateML
import NaturalLanguage

let trainingData = try MLDataTable(contentsOf: Bundle.main.url(forResource: ""Fel"", withExtension: ""json"")!)
let model = try MLWordTagger(trainingData: trainingData, tokenColumn: ""tokens"", labelColumn: ""labels"")
let metadata = MLModelMetadata(author: ""Sai Kambampati"", shortDescription: ""The Felicity Named Entity Recognizer"", license: ""MIT"", version: ""1.0"")
try model.write(to: URL(fileURLWithPath: ""/Users/SaiKambampati/Desktop/Felicity.mlmodel""), metadata: metadata)
</code></pre>

<p>The JSON file has 116,096 lines and Xcode was able to parse it correctly. It generated a validation set from 5% of the data. However, when it began tokenizing the data and extracting features, I got an error saying:</p>

<p><code>▿ MLCreateError
  ▿ generic : 1 element
    - reason : ""Token SequenceType length is 0”</code></p>

<p>I have no idea what this means and can’t find anything online! Can anyone help please?</p>
",Preprocessing of the text & Tokenization,swift naturallanguage framework error token sequencetype length currently using apple createml generate nlp model json file code json file ha line xcode wa able parse correctly generated validation set data however began tokenizing data extracting feature got error saying idea mean find anything online anyone help please
Python and Regex to convert wrtitten numbers to numeric,"<p>I am trying to convert written numbers to numeric values. </p>

<p>For example, to extract millions from this string:</p>

<pre><code>text = 'I need $ 150000000, or 150 million,1 millions, 15 Million, 15million, 15Million, 15 m, 15 M, 15m, 15M, 15 MM, 15MM, 5 thousand'
</code></pre>

<p>To:</p>

<pre><code>'I need $ 150000000, or 150000000,1000000, 15000000, 15000000, 15000000, 15000000, 15000000, 15000000, 15000000, 15000000, 15000000, 5 thousand'
</code></pre>

<p>I use this function to remove any separators in the numbers first:</p>

<pre><code>def foldNumbers(text):
    """""" to remove "","" or ""."" from numbers """"""""
    text = re.sub('(?&lt;=[0-9])\,(?=[0-9])', """", text) # remove commas
    text = re.sub('(?&lt;=[0-9])\.(?=[0-9])', """", text) # remove points
return text
</code></pre>

<p>And I have written this regex to findall of the possible patterns for common Million notations. This 1) finds digits and does a look ahead for 2) common notation for millions, 3) The ""[a-z]?"" part is to handle optional ""s"" on million or millions where I have already removed ""'"".</p>

<pre><code>re.findall(r'(?:[\d\.]+)(?= million[a-z]?|million[a-z]?| Million[a-z]?|Million[a-z]?|m| m|M| M|MM| MM)',text)
</code></pre>

<p>which correctly matches Million numbers and returns:</p>

<pre><code>['150', '1', '15', '15', '15', '15', '15', '15', '15', '15', '15']
</code></pre>

<p>What I need to do now is to write a replacement pattern to insert ""000000"" after the digits, or to iterate through and multiply the digits by 100000. I have tried this so far:</p>

<pre><code>re.sub(r'(?:[\d\.]+)(?= million[a-z]?|million[a-z]?| Million[a-z]?|Million[a-z]?|m| m|M| M|MM| MM)', ""000000 "", text)
</code></pre>

<p>which returns:</p>

<pre><code>'I need $ 150,000,000, or 000000  million,000000  millions, 000000  Million, 000000 million, 000000 Million, 000000  m, 000000  M, 000000 m, 000000 M, 000000  MM, 000000 MM, 5 thousand'
</code></pre>

<p>I think I need to do a <a href=""https://www.regular-expressions.info/lookaround.html"" rel=""nofollow noreferrer"">look behind</a> (?&lt;=), however I haven't worked with this before and after several attempts I cant seem to work it through. </p>

<p>FYI: My plan is to tackle ""Millions"" first and then to replicate the solution for Thousands (K), Billions (B), Trillions (T) and possibly for other units such as distances, currencies etc. I have searched SO and google for any solutions in NLP, text cleaning and mining articles but did not find anything. </p>
",Preprocessing of the text & Tokenization,python regex convert wrtitten number numeric trying convert written number numeric value example extract million string use function remove separator number first written regex findall possible pattern common million notation find digit doe look ahead common notation million z part handle optional million million already removed correctly match million number return need write replacement pattern insert digit iterate multiply digit tried far return think need look behind however worked several attempt cant seem work fyi plan tackle million first replicate solution thousand k billion b possibly unit distance currency etc searched google solution nlp text cleaning mining article find anything
Python &quot;Invalid Argument&quot; error while reading large txt file,"<p>I am trying to preprocess a large <code>.txt</code> file, that is around 12GB.<br>
The following code gives an</p>

<blockquote>
  <p>Invalid Argument</p>
</blockquote>

<p>error. I think it happens because the data is too large.<br>
Is there any way to read a document this big?<br>
Do I need this big data to train the words to generate word vectors?<br>
Or is there some other error? </p>

<pre><code>with open('data/text8') as f:
    text = f.read()
</code></pre>
",Preprocessing of the text & Tokenization,python invalid argument error reading large txt file trying preprocess large file around gb following code give invalid argument error think happens data large way read document big need big data train word generate word vector error
Creating vector space,"<p>I've got a question:
I have a lot of documents and each line built by some pattern.
Of course, I have this array of patterns.</p>

<p>I want to create some vector space, then to vector this patterns by some rule (I have no ideas about what is this rule yet..) - i.e. to make this patterns like ""centroids"" of my vector space.
Then to vector each line of the current document (again by this rule) and to count the closet centroid to this line (i.e. minimum of the distance between two vectors).</p>

<p>I don't know how can I do this? 
I know about sklearn libraries and CountVectorizer/TfidfVectorizer/HashingVectorizer - but this depends on the vocabulary size. But, again, I have a lot of documents, that's why it'll be too much words in vocabulary (if do this way, but in next new document it can be new word which this vocabulary wouldn't have. That's way this is wrong way to solve my problem)
Also Keras library with it's Text Preprocessing won't solve my problem two. E.x. ""one hot"" encodes a text into a list of word indexes of size . But each document may have different size and of course the order. That's way comparing two vectors may give big distance, but in fact this vectors (words, that encoded by this vectors) are very similar.</p>
",Preprocessing of the text & Tokenization,creating vector space got question lot document line built pattern course array pattern want create vector space vector pattern rule idea rule yet e make pattern like centroid vector space vector line current document rule count closet centroid line e minimum distance two vector know know sklearn library countvectorizer tfidfvectorizer hashingvectorizer depends vocabulary size lot document much word vocabulary way next new document new word vocabulary way wrong way solve problem also kera library text preprocessing solve problem two e x one hot encodes text list word index size document may different size course order way comparing two vector may give big distance fact vector word encoded vector similar
Text Cleaning Issues,"<p>I'm learning text cleaning using python online.
I have get rid of some stop words and lower the letter.</p>

<p>but when i execute this code, it doesn't show anything.</p>

<p>I don't know why.</p>

<pre><code># we add some words to the stop word list
texts, article = [], []
for w in doc:
    # if it's not a stop word or punctuation mark, add it to our article!
    if w.text != '\n' and not w.is_stop and not w.is_punct and not w.like_num and w.text != 'I':
        # we add the lematized version of the word
        article.append(w.lemma_)
    # if it's a new line, it means we're onto our next document
    if w.text == '\n':
        texts.append(article)
        article = []
</code></pre>

<p>when i try to output texts, it's just blank.</p>
",Preprocessing of the text & Tokenization,text cleaning issue learning text cleaning using python online get rid stop word lower letter execute code show anything know try output text blank
How to find &quot;num_words&quot; or vocabulary size of Keras tokenizer when one is not assigned?,"<p>So if I were to not pass <code>num_words</code> argument when initializing <code>Tokenizer()</code>, how do I find the vocabulary size after it is used to tokenize the training dataset?</p>

<p>Why this way, I don't want to limit the tokenizer vocabulary size to know how well my Keras model perform without it. But then I need to pass on this vocabulary size as the argument in the model's first layer definition.</p>
",Preprocessing of the text & Tokenization,find num word vocabulary size kera tokenizer one assigned pas argument initializing find vocabulary size used tokenize training dataset way want limit tokenizer vocabulary size know well kera model perform without need pas vocabulary size argument model first layer definition
Add exception in Spacy tokenizer to not break the tokens with whitespaces?,"<p>I am trying to find word similarity between a list of 5 words and a list of 3500 words.                                                                            </p>

<p>The problem that I am facing:    </p>

<p>The List of 5 words I have are as below   </p>

<pre><code> List_five =['cloud','data','machine learning','virtual server','python']
</code></pre>

<p>In the list of 3500 words, there are words like     </p>

<pre><code> List_threek =['cloud computing', 'docker installation', 'virtual server'.....]                                                                     
</code></pre>

<p>The Spacy models through their 'nlp' object seem to break the tokens in the second list into cloud, computing, docket, installation.</p>

<p>This in turn causes similar words to appear inaccurately, For example when I run the following code                                                                                </p>

<pre><code>tokens = "" "".join(List_five)
doc = nlp(tokens)

top5 = "" "".join(List_threek)
doc2 = nlp(top5)

similar_words = []
for token1 in doc:
    list_to_sort = [] 
    for token2 in doc2:
    #print(token1, token2)
        list_to_sort.append((token1.text, token2.text, token1.similarity(token2)))
</code></pre>

<p>I get results like (cloud, cloud) while I expected (cloud, cloud computing). It looks like the word 'cloud computing' is broken into two separate tokens.</p>

<p>Are there any workarounds? Any help is appreciated.</p>

<p>I would want an exception where contextually linked words like 'cloud computing' is not broken into two like 'cloud' , 'computing' but retained as 'cloud computing'</p>
",Preprocessing of the text & Tokenization,add exception spacy tokenizer break token whitespaces trying find word similarity list word list word problem facing list word list word word like spacy model nlp object seem break token second list cloud computing docket installation turn cause similar word appear inaccurately example run following code get result like cloud cloud expected cloud cloud computing look like word cloud computing broken two separate token workarounds help appreciated would want exception contextually linked word like cloud computing broken two like cloud computing retained cloud computing
Any efficient way to find surrounding ADJ respect to target phrase in python?,"<p>I am doing sentiment analysis on given documents, my goal is I want to find out the closest or surrounding adjective words respect to target phrase in my sentences. I do have an idea how to extract surrounding words respect to target phrases, but How do I find out relatively close or closest adjective or <code>NNP</code> or <code>VBN</code> or other POS tag respect to target phrase.</p>

<p>Here is the sketch idea of how I may get surrounding words to respect to my target phrase.</p>

<pre><code>sentence_List= {""Obviously one of the most important features of any computer is the human interface."", ""Good for everyday computing and web browsing."",
""My problem was with DELL Customer Service"", ""I play a lot of casual games online[comma] and the touchpad is very responsive""}

target_phraseList={""human interface"",""everyday computing"",""DELL Customer Service"",""touchpad""}
</code></pre>

<p>Note that my original dataset was given as dataframe where the list of the sentence and respective target phrases were given. Here I just simulated data as follows:</p>

<pre><code>import pandas as pd
df=pd.Series(sentence_List, target_phraseList)
df=pd.DataFrame(df)
</code></pre>

<p>Here I tokenize the sentence as follow:</p>

<pre><code>from nltk.tokenize import word_tokenize
tokenized_sents = [word_tokenize(i) for i in sentence_List]
tokenized=[i for i in tokenized_sents]
</code></pre>

<p>then I try to find out surrounding words respect to my target phrases by using this <a href=""https://stackoverflow.com/questions/17645701/extract-words-surrounding-a-search-word"">loot at here</a>. However, I want to find out relatively closer or closet <code>adjective</code>, or <code>verbs</code> or <code>VBN</code> respect to my target phrase. How can I make this happen? Any idea to get this done? Thanks</p>
",Preprocessing of the text & Tokenization,efficient way find surrounding adj respect target phrase python sentiment analysis given document goal want find closest surrounding adjective word respect target phrase sentence idea extract surrounding word respect target phrase find relatively close closest adjective po tag respect target phrase sketch idea may get surrounding word respect target phrase note original dataset wa given dataframe list sentence respective target phrase given simulated data follows tokenize sentence follow try find surrounding word respect target phrase using href however want find relatively closer closet respect target phrase make happen idea get done thanks
NLTK Tokenizer encoding issue,"<p>After tokenizing, my sentence contains many weird characters. How can I remove them?
This is my code:</p>

<pre><code>def summary(filename, method):
    list_names = glob.glob(filename)
    orginal_data = []
    topic_data = []
    print(list_names)
    for file_name in list_names:
        article = []
        article_temp = io.open(file_name,""r"", encoding = ""utf-8-sig"").readlines()
        for line in article_temp:
            print(line)
            if (line.strip()):
                tokenizer =nltk.data.load('tokenizers/punkt/english.pickle')
                sentences = tokenizer.tokenize(line)
                print(sentences)
                article = article + sentences
        orginal_data.append(article)
        topic_data.append(preprocess_data(article))
    if (method == ""orig""):
        summary = generate_summary_origin(topic_data, 100, orginal_data)
    elif (method == ""best-avg""):
        summary = generate_summary_best_avg(topic_data, 100, orginal_data)
    else:
        summary = generate_summary_simplified(topic_data, 100, orginal_data)
    return summary
</code></pre>

<p>The <code>print(line)</code> prints a line of a txt. And <code>print(sentences)</code> prints the tokenized sentences in the line.</p>

<p>But sometimes the sentences contains weird characters after nltk's processing.</p>

<pre><code>Assaly, who is a fan of both Pusha T and Drake, said he and his friends 
wondered if people in the crowd might boo Pusha T during the show, but 
said he never imagined actual violence would take place.

[u'Assaly, who is a fan of both Pusha T and Drake, said he and his 
friends wondered if people in\xa0the crowd might boo Pusha\xa0T during 
the show, but said he never imagined actual violence would take 
place.']
</code></pre>

<p>Like above example, where is the <code>\xa0</code> and <code>\xa0T</code> from?</p>
",Preprocessing of the text & Tokenization,nltk tokenizer encoding issue tokenizing sentence contains many weird character remove code print line txt print tokenized sentence line sometimes sentence contains weird character nltk processing like example
Text corpus clustering,"<p>I have 27000 free text elements, each of around 2-3 sentences.  I need to cluster these by similarity.  So far, I have pretty limited success.  I have tried the following:</p>

<p>I used Python <a href=""https://www.nltk.org/"" rel=""nofollow noreferrer"">Natural Language Toolkit</a> to remove stop words, lemmatize and tokenize, then generated semantically similar words for each word in the sentence before inserting them into a Neo4j graph database.  I then tried querying that using the TF counts for each word and related word.  That didn't work very well and only resulted in being able to easily calculate the similarity between two text items.</p>

<p>I then looked at <a href=""https://github.com/graphaware/neo4j-nlp"" rel=""nofollow noreferrer"">Graphawares NLP library</a> to annotate, enrich and calculate the cosine similarity between each text item.  After 4 days of processing similarity I checked the log to find that it would take 1.5 years to process.  Apparently the community version of the plugin isn't optimised, so I guess it's not appropriate for this kind of volume of data.</p>

<p>I then wrote a custom implementation that took the same approach as the Graphaware plugin, but in much simpler form.  I used <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">scikitlearn's TfidfVectorizer</a> to calculate the cosine similarity between each text item and every other text item and saved those as relationships between the Neo4j nodes.  However, with 27000 text items that creates  27000 * 27000 = 729000000 relationships!  The intention was to take the graph into <a href=""https://gephi.org/"" rel=""nofollow noreferrer"">Grephi</a> selecting relationships of over X threshold of similarity and use modularity clustering to extract clusters.  Processing for this is around 4 days which is much better.  Processing is incomplete and is currently running.  However, I believe that Grephi has a max edge count of 1M, so I expect this to restrict what I can do.</p>

<p>So I turned to more conventional ML techniques using scikitlearn's KMeans, DBSCAN, and MeanShift algorithms.  I am getting clustering, but when it's plotted on a scatter chart there is no separation (I can show code if that would help).  Here is what I get with DBSCAN:</p>

<p><a href=""https://i.sstatic.net/AsxNJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AsxNJ.png"" alt=""enter image description here""></a></p>

<p>I get similar results with KMeans.  These algorithms run within a few seconds, which obviously makes life easier, but the results seem poor.</p>

<p>So my questions are:</p>

<ul>
<li>Is there a better approach to this?</li>
<li>Can I expect to find distinct clusters at all in free text?</li>
<li>What should my next move be?</li>
</ul>

<p>Thank you very much.</p>
",Preprocessing of the text & Tokenization,text corpus clustering free text element around sentence need cluster similarity far pretty limited success tried following used python natural language toolkit remove stop word lemmatize tokenize generated semantically similar word word sentence inserting neo j graph database tried querying using tf count word related word work well resulted able easily calculate similarity two text item looked graphawares nlp library annotate enrich calculate cosine similarity text item day processing similarity checked log find would take year process apparently community version plugin optimised guess appropriate kind volume data wrote custom implementation took approach graphaware plugin much simpler form used scikitlearn tfidfvectorizer calculate cosine similarity text item every text item saved relationship neo j node however text item creates relationship intention wa take graph grephi selecting relationship x threshold similarity use modularity clustering extract cluster processing around day much better processing incomplete currently running however believe grephi ha max edge count expect restrict turned conventional ml technique using scikitlearn kmeans dbscan meanshift algorithm getting clustering plotted scatter chart separation show code would help get dbscan get similar result kmeans algorithm run within second obviously make life easier result seem poor question better approach expect find distinct cluster free text next move thank much
How do I get started in NLP using C#?,"<p>I come from a linguistic background and I'm pretty new to the programming community (and feeling kind of lost actually).</p>

<p>I have been actively learning the basics of C# in the last weeks, which will be the language I'll be working with for a few NLP projects in my PhD program.</p>

<p>I don't know at all where and how to get started on this. I'd like to learn how to consume web services in C# for different sets of NLP tasks such as POS tagging or sentence tokenization. Everything I've looked for up to this moment feels like far away from my current understanding. So, if you happen to know anything about NLP in connection to C# that could guide me in the right direction, I would highly appreciate it.</p>
",Preprocessing of the text & Tokenization,get started nlp using c come linguistic background pretty new programming community feeling kind lost actually actively learning basic c last week language working nlp project phd program know get started like learn consume web service c different set nlp task po tagging sentence tokenization everything looked moment feel like far away current understanding happen know anything nlp connection c could guide right direction would highly appreciate
Python Html: Extract Parts of Text from html file,"<p>I'm currently working on a project where I downloaded a bunch of related html files and I gather data from them. One thing I noticed is that even though the overall format of the html files are similar, sometimes various files use different tags for storing similar information.</p>

<p>For example, in one file it could be:</p>

<pre><code>&lt;html&gt;
&lt;head&gt;
&lt;p&gt; Title: The GodFather &lt;/p&gt;
&lt;p&gt; Author: Mario Puzo &lt;/p&gt;
&lt;/head&gt;
&lt;html&gt;
</code></pre>

<p>And in another example it could be:</p>

<pre><code>&lt;html&gt;
&lt;head&gt;
&lt;p&gt; Heading &lt;/p&gt;
&lt;pre&gt; Ebook from xyz site: Please donate to our foundation at www.abc.com
Title: The GodFather
Author: Mario Puzo
&lt;/pre&gt;
&lt;/head&gt;
&lt;/html&gt;
</code></pre>

<p>
</p>

<p>I can say for sure that ""<strong>Title:</strong> "" and ""<strong>Author:</strong> "" are common in all the html files. I want to extract the text that is next to ""<strong>Title:</strong> "" and ""<strong>Author:</strong> "". 
I'm supposing I use beautiful soup to extract each html file. But to extract <strong>Title</strong> and <strong>Author</strong>, would using regular expressions be best?</p>
",Preprocessing of the text & Tokenization,python html extract part text html file currently working project downloaded bunch related html file gather data one thing noticed even though overall format html file similar sometimes various file use different tag storing similar information example one file could another example could say sure title author common html file want extract text next title author supposing use beautiful soup extract html file extract title author would using regular expression best
Why is stemming important for sentimental analysis,"<p>I am using seven lexicons to calculate sentimental scores on a data set containing forum posts. Apart from removing all noise such as whitespace, special char, digits and stopwords, why is it also important to stem the words?</p>

<p>I am using Harvard.IV, Qdap, Henry's Financial dictionary and Loughran-McDonald Financial dictionary from SentimentAnalysis package, as well as AFINN, NRC and BING dictionaries.</p>
",Preprocessing of the text & Tokenization,stemming important sentimental analysis using seven lexicon calculate sentimental score data set containing forum post apart removing noise whitespace special char digit stopwords also important stem word using harvard iv qdap henry financial dictionary loughran mcdonald financial dictionary sentimentanalysis package well afinn nrc bing dictionary
Is there a way to vectorize only words i.e not from a corpus or bag of words in python?,"<p>My use case is to vectorize words in two lists like below.</p>

<pre><code>ListA = [Japan, Electronics, Manufacturing, Science]

ListB = [China, Electronics, AI, Software, Science]
</code></pre>

<p>I understand that <code>word2vec</code> and <code>Glove</code> can vectorize words but they do that through corpus or bag of words i.e we have to pass sentences which gets broken down to tokens and then it is vectorized. </p>

<p>Is there a way to just vectorize words in a list? </p>

<p>PS. I am new to NLP side of things, hence pardon any obvious points stated.</p>
",Preprocessing of the text & Tokenization,way vectorize word e corpus bag word python use case vectorize word two list like understand vectorize word corpus bag word e pas sentence get broken token vectorized way vectorize word list p new nlp side thing hence pardon obvious point stated
How to get cosine similarity value for words from two different lists in python,"<p>I have two lists of words as below</p>

<p>List1 = [apple, samsung, nokia, LG, micromax]</p>

<p>List2 = [Panasonic, nokia, Micromax, onida]</p>

<p>I want to get a cosine similarity value for each pair </p>

<p>e.g </p>

<p>(apple - nokia) 0.003</p>

<p>(nokia - nokia) 1.0</p>

<p>How do I accomplish this ?</p>
",Preprocessing of the text & Tokenization,get cosine similarity value word two different list python two list word list apple samsung nokia lg micromax list panasonic nokia micromax onida want get cosine similarity value pair e g apple nokia nokia nokia accomplish
stemmed word using hunspell module,"<p>I am using 2 modules for NLP one is nltk and the other is hunspell. The reason of using hunspell is that I have suffix and affix rules those needs to be followed. </p>

<pre><code>from nltk.stem.porter import *
stemmer = PorterStemmer()
stemmer.stem('ladies')
</code></pre>

<blockquote>
  <p>ladi</p>
</blockquote>

<pre><code>from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatizer.lemmatize('ladies')
</code></pre>

<blockquote>
  <p>lady</p>
</blockquote>

<p>The nltk module works as expected as shown above. But hunspell module seems to support only lemmatization and there is no way to return stemmed form.</p>

<pre><code>import hunspell
hobj = hunspell.HunSpell('en_US.dic', 'en_US.aff')
hobj.stem('ladies')
</code></pre>

<p>This returns ""<strong>lady</strong>"" and not ""ladi"" as one would expect. Is there any way to return the stemmed form of a word using hunspell module?</p>
",Preprocessing of the text & Tokenization,stemmed word using hunspell module using module nlp one nltk hunspell reason using hunspell suffix affix rule need followed ladi lady nltk module work expected shown hunspell module seems support lemmatization way return stemmed form return lady ladi one would expect way return stemmed form word using hunspell module
Stemming vs Lemmatization for financial text in python [NLTK],"<p>To extract more information from annual reports (10ks), I am trying to compare companies based on the cosine similarity. One of the steps in this research is the stemming or lemmatization of words. The reason for doing this is to get the root of the words, so that when you don't have different variation words that at their core mean the same thing. For stemmer and lemmatizer, I used SnowBall stemmer and WordNetLemmatizer from the NLTK package. </p>

<p>E.g. of stemming:   ; E.g. of lemmatization
<code>
walking -&gt; walk          walking-&gt; walking
walked -&gt; walk           walked -&gt; walked
or 
owing -&gt; owe             owing -&gt; owing
owed -&gt; owe              owed -&gt; owed
</code><br>
<strong>The question is the following</strong>: should I use the stemmer or a lemmatizer for financial text? </p>

<p>The way I see it, a stemmer would be more appropiate for this kind of research. </p>

<p>Disclaimer: I know there is already a question discussing stemming vs lemmatization on stackoverflow. However, I am looking for some clarification regarding financial text in particular not as a general case.</p>
",Preprocessing of the text & Tokenization,stemming v lemmatization financial text python nltk extract information annual report k trying compare company based cosine similarity one step research stemming lemmatization word reason get root word different variation word core mean thing stemmer lemmatizer used snowball stemmer wordnetlemmatizer nltk package e g stemming e g lemmatization question following use stemmer lemmatizer financial text way see stemmer would appropiate kind research disclaimer know already question discussing stemming v lemmatization stackoverflow however looking clarification regarding financial text particular general case
Count how often words from a list appear in a string,"<p>I have a pre-defined list of words; e.g.:</p>

<pre><code>wordlist = [[""one""],[""two""],[""three""]]
</code></pre>

<p>And I have a large corpus of .txt files, also imported into python as a list, e.g.:</p>

<pre><code>corpus = [""my friend has one potato"",
""i have two bananas and three apples"",
""my dad has three apples""] 
</code></pre>

<p>I want a formula that goes through the corpus line by line and tells me the amount of words from my wordlist that are contained in each line; i.e., exemplary output would be:</p>

<pre><code>1
2
1
</code></pre>

<p>I need no differentiation between the words in the wordlist.</p>

<p>However, I want this to be a formula, so that I can easily apply it with different word lists or corpora.</p>

<p>I have not found an answer on SO or elsewhere. What I have tried is:</p>

<pre><code>wordcount_total=list()
for i in range(len(corpus)):
    row=corpus[i]
    wordcount_row=sum(1 for word in row.split() if word in wordlist)
    wordcount_total.append(wordcount_row)
</code></pre>

<p>However, this gives me:</p>

<pre><code>0
0
0
</code></pre>

<p>Many thanks to anyone willing to help!</p>
",Preprocessing of the text & Tokenization,count often word list appear string pre defined list word e g large corpus txt file also imported python list e g want formula go corpus line line tell amount word wordlist contained line e exemplary output would need differentiation word wordlist however want formula easily apply different word list corpus found answer elsewhere tried however give many thanks anyone willing help
GloVe Text Summarization returning stop word gibberish,"<p>I am trying to train a Keras test summarization model to generate a new headline for a news article that I can then compare to the published headline. I am training on GloVe 6B, then predicting against the article, which I have cleaned by removing tags, parsing, removing stop words, lemmatized, and then rejoined. My results tend to look like this:</p>

<p>Original Headline:  Ford Traveled To Maryland In August Despite Allegedly Fear Of Flying
Generated Headline:  opinion: the the the the to to</p>

<p>Article text after cleaning: Brett Kavanaugh accuser Christine Blasey Ford took polygraph test Maryland far home California despite alleged fear flying Documents released Wednesday polygraph test administered Ford Aug. 7 Hilton Hotel Linthicum Heights Maryland far Baltimore Washington International Airport A friend Kate DeVarney   Blasey Ford enjoy flying hard time place ’s escape route Christine Blasey Ford professor accusing Supreme Court nominee Brett Kavanaugh having sexually assaulted high school previously told friend alleged encounter 30 year ago lasting effect life Two longtime friend Ford ’s told CNN week previously described feeling uncomfortable struggling enclosed space escape route exit door suggested discomfort stemmed alleged encounter Kavanaugh This reason Ford enjoy flying DeVarney said airplane ultimate closed space away The fear flying   Ford able testify timely manner Senate Judiciary   In letter California Democratic Sen. Dianne Feinstein dated July 30 2018 Ford said vacation Mid Atlantic Aug. 7 day polygraph given Ford testify Senate 10 a.m. EST Thursday   </p>

<p>Here is my training code:</p>

<pre><code>from __future__ import print_function

import pandas as pd
from sklearn.model_selection import train_test_split
from keras_text_summarization.library.utility.plot_utils import plot_and_save_history
from keras_text_summarization.library.seq2seq import Seq2SeqGloVeSummarizer
from keras_text_summarization.library.applications.fake_news_loader import fit_text
import numpy as np

LOAD_EXISTING_WEIGHTS = False


def main():
    np.random.seed(42)
    data_dir_path = './data'
    very_large_data_dir_path = './very_large_data'
    report_dir_path = './reports'
    model_dir_path = './models'

    print('loading csv file ...')
    df = pd.read_csv(""dcr Man_Cleaned.csv"")

    print('extract configuration from input texts ...')
    Y = df.Title
    X = df['Joined']
    config = fit_text(X, Y)

    print('configuration extracted from input texts ...')

    summarizer = Seq2SeqGloVeSummarizer(config)
    summarizer.load_glove(very_large_data_dir_path)

    if LOAD_EXISTING_WEIGHTS:
        summarizer.load_weights(weight_file_path=Seq2SeqGloVeSummarizer.get_weight_file_path(model_dir_path=model_dir_path))

    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)

    print('training size: ', len(Xtrain))
    print('testing size: ', len(Xtest))

    print('start fitting ...')
    history = summarizer.fit(Xtrain, Ytrain, Xtest, Ytest, epochs=20, batch_size=16)

    history_plot_file_path = report_dir_path + '/' + Seq2SeqGloVeSummarizer.model_name + '-history.png'
    if LOAD_EXISTING_WEIGHTS:
        history_plot_file_path = report_dir_path + '/' + Seq2SeqGloVeSummarizer.model_name + '-history-v' + str(summarizer.version) + '.png'
    plot_and_save_history(history, summarizer.model_name, history_plot_file_path, metrics={'loss', 'acc'})


if __name__ == '__main__':
    main()
</code></pre>

<p>Any thoughts as to what is going wrong here is appreciated. </p>
",Preprocessing of the text & Tokenization,glove text summarization returning stop word gibberish trying train kera test summarization model generate new headline news article compare published headline training glove b predicting article cleaned removing tag parsing removing stop word lemmatized rejoined result tend look like original headline ford traveled maryland august despite allegedly fear flying generated headline opinion article text cleaning brett kavanaugh accuser christine blasey ford took polygraph test maryland far home california despite alleged fear flying document released wednesday polygraph test administered ford aug hilton hotel linthicum height maryland far baltimore washington international airport friend kate devarney blasey ford enjoy flying hard time place escape route christine blasey ford professor accusing supreme court nominee brett kavanaugh sexually assaulted high school previously told friend alleged encounter year ago lasting effect life two longtime friend ford told cnn week previously described feeling uncomfortable struggling enclosed space escape route exit door suggested discomfort stemmed alleged encounter kavanaugh reason ford enjoy flying devarney said airplane ultimate closed space away fear flying ford able testify timely manner senate judiciary letter california democratic sen dianne feinstein dated july ford said vacation mid atlantic aug day polygraph given ford testify senate est thursday training code thought going wrong appreciated
How to tokenize a paragraph which have numbered list into multiple sentences using python?,"<p>I'm planning to break the paragraph into multiple sentences. This paragraph contains numbered sentences like shown below:</p>

<pre><code>Hello, How are you? Hope everything is good. I'm fine. 1.Hello World. 2.Good Morning John. 

Product is good but the managemnt is very lazy very bad. I dont like company service. They are giving fake promises. Next time i will not take any product. For Amazon service i will give 5 star dey give awsome service. But for sony company i will give 0 star... 1. Doesn't support all file formats when you connect USB 2. No other apps than YouTube and Netflix (requires subscription) 3. Screen mirroring is not up to the mark ( getting connected after once in 10 attempts 4. Good screen quality 5. Audio is very good 6. Bulky compared to other similar range 7. Price bit high due to brand value 8. its 1/4 smart TV. Not a full smart TV 9. Bad customer support 10. Remote control is very horrible to operate. it might be good for non smart TV 11. See the exchange value on amazon itself. LG gets 2ooo/- more than TV's 12. Also it was mentioned like 1+1 year warranty. But either support or Amazon support aren't clear about it. 13. Product information isn't up to 30% at least.There no installation. While I contact costumer Care.
</code></pre>

<p>I had used the below code to break into sentences:</p>

<pre><code>import nltk
tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()
fp = open(""/Users/Desktop/sample.txt"", encoding='utf-8')
data = fp.read()
with open(""/Users/Desktop/output.txt"", 'a', encoding='utf-8' ) as f:
            f.write(''.join(tokenizer.tokenize(data)))
            f.close()
</code></pre>

<p>This code is splitting the paragraph based on full stops. But the numbered sentence are creating an issue. Since these have fullstops after numbers, it is splitting in an improper manner.</p>

<p>Can anyone please suggest me regarding the same?</p>
",Preprocessing of the text & Tokenization,tokenize paragraph numbered list multiple sentence using python planning break paragraph multiple sentence paragraph contains numbered sentence like shown used code break sentence code splitting paragraph based full stop numbered sentence creating issue since fullstops number splitting improper manner anyone please suggest regarding
Word_tokenize does not work after sent_tokenize in python dataframe,"<p>I trying to tokenize by data using sent_tokenize and word_tokenize.</p>

<p>Below is my dummy data</p>

<blockquote>
<pre><code>**text**
Hello world, how are you
I am fine, thank you!
</code></pre>
</blockquote>

<p>I am trying to tokenize it using below code</p>

<pre><code>import pandas as pd
from nltk.tokenize import word_tokenize, sent_tokenize
Corpus=pd.read_csv(r""C:\Users\Desktop\NLP\corpus.csv"",encoding='utf-8')

Corpus['text']=Corpus['text'].apply(sent_tokenize)
Corpus['text_new']=Corpus['text'].apply(word_tokenize)
</code></pre>

<p>but it is giving below error</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/gunjit.bedi/Desktop/NLP Project/Topic Classification.py"", line 24, in &lt;module&gt;
    Corpus['text_new']=Corpus['text'].apply(word_tokenize)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\series.py"", line 3192, in apply
    mapped = lib.map_infer(values, f, convert=convert_dtype)
  File ""pandas/_libs/src\inference.pyx"", line 1472, in pandas._libs.lib.map_infer
  File ""C:\ProgramData\Anaconda3\lib\site-packages\nltk\tokenize\__init__.py"", line 128, in word_tokenize
    sentences = [text] if preserve_line else sent_tokenize(text, language)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\nltk\tokenize\__init__.py"", line 95, in sent_tokenize
    return tokenizer.tokenize(text)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\nltk\tokenize\punkt.py"", line 1241, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\nltk\tokenize\punkt.py"", line 1291, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File ""C:\ProgramData\Anaconda3\lib\site-packages\nltk\tokenize\punkt.py"", line 1291, in &lt;listcomp&gt;
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File ""C:\ProgramData\Anaconda3\lib\site-packages\nltk\tokenize\punkt.py"", line 1281, in span_tokenize
    for sl in slices:
  File ""C:\ProgramData\Anaconda3\lib\site-packages\nltk\tokenize\punkt.py"", line 1322, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\nltk\tokenize\punkt.py"", line 313, in _pair_iter
    prev = next(it)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\nltk\tokenize\punkt.py"", line 1295, in _slices_from_text
    for match in self._lang_vars.period_context_re().finditer(text):
TypeError: expected string or bytes-like object
</code></pre>

<p>I did try a lot of things like if I comment sent_tokenize , the word_tokenize works but both of them do not work together</p>
",Preprocessing of the text & Tokenization,word tokenize doe work sent tokenize python dataframe trying tokenize data using sent tokenize word tokenize dummy data trying tokenize using code giving error try lot thing like comment sent tokenize word tokenize work work together
How do I use generator objects in spaCy?,"<p>First experience with NLP here. I have about a half million tweets. I'm trying to use spacy to remove stop words, lemmatize, etc. and then pass the processed text to a classification model. Because of the size of the data I need multiprocessing to do this in reasonable speed, but can't figure out what to do with the generator object once I have it.</p>

<p>Here I load spacy and pass the data through the standard pipeline:</p>

<pre><code>nlp = spacy.load('en')

tweets = ['This is a dummy tweet for stack overflow',
         'What do we do with generator objects?']
spacy_tweets = []
for tweet in tweets:
    doc_tweet = nlp.pipe(tweet, batch_size = 10, n_threads = 3)
    spacy_tweets.append(doc_tweet)
</code></pre>

<p>Now I'd like to take the Doc objects spaCy creates and then process the text with something like this:</p>

<pre><code>def spacy_tokenizer(tweet):
    tweet = [tok.lemma_.lower().strip() if tok.lemma_ != ""-PRON-"" else tok.lower_ for tok in tweet]
    tweet = [tok for tok in tweet if (tok not in stopwords and tok not in punctuations)] 
    return tweet
</code></pre>

<p>But this doesn't work because spaCy returns generator objects when using the .pipe() method. So when I do this:</p>

<pre><code>for tweet in spacy_tweets:
    print(tweet)
</code></pre>

<p>It prints the generator. Okay, I get that. But when I do this:</p>

<pre><code>for tweet in spacy_tweets[0]:
    print(tweet)
</code></pre>

<p>I would expect it to print the Doc object or the text of the tweet in the generator but it doesn't do that. Instead it prints each character our individually. </p>

<p>Am I approaching this wrong or is there something I need to do in order to retrieve the Doc objects from the generator objects so I can use the spaCy attributes for lemmatizing etc.?</p>
",Preprocessing of the text & Tokenization,use generator object spacy first experience nlp half million tweet trying use spacy remove stop word lemmatize etc pas processed text classification model size data need multiprocessing reasonable speed figure generator object load spacy pas data standard pipeline like take doc object spacy creates process text something like work spacy return generator object using pipe method print generator okay get would expect print doc object text tweet generator instead print character individually approaching wrong something need order retrieve doc object generator object use spacy attribute lemmatizing etc
necessary condition to fix weird lemma&#39;s?,"<p>(<strong>Executed in jupyter notbook</strong>) I'm applying lemmatization on documents that I've tokenised and I can't help but notice that the word ""us"" gets lemmatized to ""u"" every time which wouldn't make sense from a clarity point of view and could possibly lead people to understand it as something else. Am I missing out a condition for my pos function? How could I fix this problem?</p>

<p><strong>Defining the function</strong></p>

<pre><code>from nltk import pos_tag

def penn2wordNet(treebank_tags):
    wordNet_tag = {'NN':'n', 'JJ':'a',
                  'VB':'v', 'RB':'r'}
    try:
        return wordNet_tag[penntag[:2]]
    except:
        return 'n'
paired_tags = []
for doc in wordTokens:
    paired_tags.append(pos_tag(doc))
    print(paired_tags)
</code></pre>

<p><img src=""https://i.sstatic.net/NxSGZ.png"" alt=""snippet output of the code above""></p>

<p><strong>Lemmatizing the tokens</strong></p>

<pre><code>    from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()

print(wordTokens[1])
lemmatized_wordTokens = []
for index in range(len(paired_tags)):
    lemmatized_wordTokens.append(([wnl.lemmatize(word, pos=penn2wordNet(tag)) for word, tag in paired_tags[index]]))
print(lemmatized_wordTokens[1])
</code></pre>

<p><img src=""https://i.sstatic.net/ig27A.png"" alt=""output after lemmatization showing before and after""></p>
",Preprocessing of the text & Tokenization,necessary condition fix weird lemma executed jupyter notbook applying lemmatization document tokenised help notice word u get lemmatized u every time make sense clarity point view could possibly lead people understand something else missing condition po function could fix problem defining function lemmatizing token
Removing words in text files containing a character or string of letters with Python,"<p>I have a few lines of text and want to remove any word with special characters or a fixed given string in them (in python).</p>

<p>Example:</p>

<pre><code>in_lines = ['this is go:od', 
            'that example is bad', 
            'amp is a word']

# remove any word with {'amp', ':'}
out_lines = ['this is', 
             'that is bad', 
             'is a word']
</code></pre>

<p>I know how to remove words from a list that is given but cannot remove words with special characters or few letters being present. Please let me know and I'll add more information. </p>

<p>This is what I have for removing selected words:</p>

<pre><code>def remove_stop_words(lines):
   stop_words = ['am', 'is', 'are']
   results = []
   for text in lines:
        tmp = text.split(' ')
        for stop_word in stop_words:
            for x in range(0, len(tmp)):
               if tmp[x] == stop_word:
                  tmp[x] = ''
        results.append("" "".join(tmp))
   return results
out_lines = remove_stop_words(in_lines)
</code></pre>
",Preprocessing of the text & Tokenization,removing word text file containing character string letter python line text want remove word special character fixed given string python example know remove word list given remove word special character letter present please let know add information removing selected word
Add a new stemmer to nltk,"<p>I have this python function that works as expected. Is it possible to save the logic as NLP stemmer?
If yes, what changes needs to be done?</p>

<pre><code>import itertools, re
def dropdup(mytuple):
    newtup=list()
    for i in mytuple:
        i = i[:-3] if i.endswith('bai') else i
        for r in ((""tha"", ""ta""), (""i"", ""e"")):
            i = i.replace(*r)
            i = re.sub(r'(\w)\1+',r'\1', i)
        newtup.append(''.join(i for i, _ in itertools.groupby(i)))
    return tuple(newtup)

dropdup(('savithabai', 'samiiir', 'aaaabaa'))
('saveta', 'samer', 'aba')
</code></pre>

<p>I will like the users to import something like this...</p>

<pre><code>from nltk.stemmer import indianNameStemmer
</code></pre>

<p>There are a few more rules to be added to the logic. I just want to know if this is a valid (pythonic) idea.</p>
",Preprocessing of the text & Tokenization,add new stemmer nltk python function work expected possible save logic nlp stemmer yes change need done like user import something like rule added logic want know valid pythonic idea
NLTK reverse n-gram search,"<p>I have a corpus that consists of various messages. I used NLTK to create a series of bi-grams and tri-grams. I created the grams by doing pre-processing like removing stop words and things of the sort.</p>

<p>How can I take a bi-gram (or tri) and search to see if it exists in a new message? I would have to pre-process the message at some point wouldn't I? </p>

<p>Or, if I can do this another way, during the creation of the n-gram process, is it possible to index the messages and output both the n-grams and which message they apply to? </p>
",Preprocessing of the text & Tokenization,nltk reverse n gram search corpus consists various message used nltk create series bi gram tri gram created gram pre processing like removing stop word thing sort take bi gram tri search see exists new message would pre process message point another way creation n gram process possible index message output n gram message apply
Spacy - preprocessing &amp; lemmatization taking long time,"<p>I am working on text classification problem and I have tried WordNetLemmmatizer then followed by TF-IDF, CountVectorizer. Now, I am trying to clean up the text using Spacy before feeding to TF-IDF. Input file has around 20,000 records with each record having few sentences. Overall size of the file is 45MB.  </p>

<p>Lemmatization using WordNet was taking only few seconds. But below code using Spacy is taking too long. After 20 mins my laptop get hanged. Kindly advice how to optimize Spacy for text pre-processing and lemmatization.</p>

<p>I am using Spacy 2.0.12.</p>

<pre><code>import spacy
nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])

def spacy_lemma_text(text):
    doc = nlp(text)
    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']
    tokens = [tok for tok in tokens if tok not in nltk_stopwords and tok not in punctuations]
    tokens = ' '.join(tokens)
    return tokens

df['spacy_lemma_text'] = df['text'].apply(spacy_lemma_text)
</code></pre>
",Preprocessing of the text & Tokenization,spacy preprocessing lemmatization taking long time working text classification problem tried wordnetlemmmatizer followed tf idf countvectorizer trying clean text using spacy feeding tf idf input file ha around record record sentence overall size file mb lemmatization using wordnet wa taking second code using spacy taking long min laptop get hanged kindly advice optimize spacy text pre processing lemmatization using spacy
Merge Similar Strings Python,"<p>I'm having two strings</p>

<pre><code>string1 = ""apple banna kiwi mango""
string2 = ""aple banana mango lemon""
</code></pre>

<p>I want the resultant of addition of these two strings (not concatenation) i.e. result should look like</p>

<pre><code>result = ""apple banana kiwi mango lemon""
</code></pre>

<p>My current approach is rather simple.</p>

<ol>
<li>Tokenize the multiline string (the above strings are after tokenization), remove any noises (special/ newline characters/ empty strings)</li>
<li>The next step is to identify the cosine similarity of the strings, if it is above 0.9, then I add one of the string to final result</li>
</ol>

<p>Now, here is the problem. It doesn't cover the part where one string contains one half of a word and other contains the other half (or correct word in some cases) of word. I have also added <a href=""https://stackoverflow.com/questions/46860908/concatenate-two-strings-with-a-common-substring"">this</a> function in my script. But again the problem remains. Any help on how to move forward with this is appreciated.</p>

<pre><code>def text_to_vector(text):
     words = WORD.findall(text)
     return Counter(words)

def get_cosine(vec1, vec2):
     intersection = set(vec1.keys()) &amp; set(vec2.keys())
     numerator = sum([vec1[x] * vec2[x] for x in intersection])

     sum1 = sum([vec1[x]**2 for x in vec1.keys()])
     sum2 = sum([vec2[x]**2 for x in vec2.keys()])
     denominator = math.sqrt(sum1) * math.sqrt(sum2)

     if not denominator:
        return 0.0
     else:
        return float(numerator) / denominator


def merge_string(string1, string2):
    i = 0
    while not string2.startswith(string1[i:]):
        i += 1

    sFinal = string1[:i] + string2
    return sFinal

for item in c:
for j in d:
    vec1 = text_to_vector(item)
    vec2 = text_to_vector(j)
    r = get_cosine(vec1, vec2)
    if r &gt; 0.5:
        if r &gt; 0.85:
            final.append(item)
            break
        else:
            sFinal = merge_string(item, j)
            #print(""1."", len(sFinal), len(item), len(j))
            if len(sFinal) &gt;= len(item) + len(j) -8:
                sFinal = merge_string(j, item)
                final.append(sFinal)
                #print(""2."", len(sFinal), len(item), len(j))
                temp.append([item, j])
                break
</code></pre>
",Preprocessing of the text & Tokenization,merge similar string python two string want resultant addition two string concatenation e result look like current approach rather simple tokenize multiline string string tokenization remove noise special newline character empty string next step identify cosine similarity string add one string final result problem cover part one string contains one half word contains half correct word case word also added href function script problem remains help move forward appreciated p
Find number of keyword matches in pandas column that is in a list,"<p>I have a pandas dataframe that looks like the following:</p>

<pre><code>Type        Keywords 
----        --------
Animal      [Pigeon, Bird, Raccoon, Dog, Cat]
Pet         [Dog, Cat, Hamster]
Pest        [Rat, Mouse, Raccoon, Pigeon]
Farm        [Chicken, Horse, Cow, Sheep]
Predator    [Wolf, Fox, Raccoon]
</code></pre>

<p>Let's say that I have the following string:</p>

<pre><code>input = ""There is a dead rat and raccoon in my pool""
</code></pre>

<p>Given that I tokenize the string and remove stop-words so that it becomes</p>

<pre><code>input = [Dead, Rat, Raccoon, Pool]
</code></pre>

<p>I need to go through each row and find the rows that have the highest number of keyword matches. With the given example, the results would look like the following:</p>

<pre><code>Type        Keywords                            Matches
----        --------                            -------
Animal      [Pigeon, Bird, Raccoon, Dog, Cat]   1
Pet         [Dog, Cat, Hamster]                 0
Pest        [Rat, Mouse, Raccoon, Pigeon]       2
Farm        [Chicken, Horse, Cow, Sheep]        0
Predator    [Wolf, Fox, Raccoon]                1
</code></pre>

<p>The output would be the top three Type names that have the highest number of matches.</p>

<p>In the above case, since the ""Pest"" category has the highest number of matches, it would be selected as the highest match. Additionally both the Animal and Predator categories would be selected. The output in order would thus be: </p>

<pre><code>output = [Pest, Animal, Predator]
</code></pre>

<p>Doing this task with nested for loops is easy, but since I have thousands of these kinds of rows, I'm looking for a better solution. (Additionally for some reason I have encountered a lot of bugs when using non in-built functions with pandas, perhaps it's because of vectorization?)</p>

<p>I looked at the groupby and isin functions that are inbuilt in pandas, but as far as I could tell they would not be able to get me to the output that I want (I would not be surprised at all if I am incorrect in this assumption).  </p>

<p>I next investigated the usage of sets and hashmaps with pandas, but unfortunately my coding knowledge and current ability is not yet proficient enough to craft a solid solution. <a href=""https://stackoverflow.com/questions/51574485/match-keywords-in-pandas-column-with-another-list-of-elements"">This StackOverflow link</a> in particular got me much closer to what I wanted, though it didn't find the top three match row names.</p>

<p>I would greatly appreciate any help or advice.</p>
",Preprocessing of the text & Tokenization,find number keyword match panda column list panda dataframe look like following let say following string given tokenize string remove stop word becomes need go row find row highest number keyword match given example result would look like following output would top three type name highest number match case since pest category ha highest number match would selected highest match additionally animal predator category would selected output order would thus task nested loop easy since thousand kind row looking better solution additionally reason encountered lot bug using non built function panda perhaps vectorization looked groupby isin function inbuilt panda far could tell would able get output want would surprised incorrect assumption next investigated usage set hashmaps panda unfortunately coding knowledge current ability yet proficient enough craft solid solution href stackoverflow link particular got much closer wanted though find top three match row name would greatly appreciate help advice
R - Text Analysis - Misleading results,"<p>I am doing some text analysis of comments from bank customers related to mortgages and I find a couple of things I do understand.</p>

<p>1) After cleaning data without applying Stemming Words and checking the dimension of the TDM the number of terms (2173) is smaller than the number of documents (2373)(This is before remove stop words and being the TDM a 1-gram).</p>

<p>2) Also, I wanted to check the 2-words frequency (rowSums(Matrix)) of the bi-gram tokenizing the TDM. The issue is that for example I have gotten as the most repeated result the 2-words ""Proble miss"". Since this grouping was already strange, I have gone to the dataset, ""Control +F"", to try to find and i could not. Questions: it seems that the code some how has stemmed these words, how is it possible? (From the top 25 bi-words, this one is the only one that seems to be stemmed). Is this not supposed to ONLY create bi-grams that are always together?</p>

<pre><code>{file_cleaning &lt;-  replace_number(files$VERBATIM)
file_cleaning &lt;-  replace_abbreviation(file_cleaning)
file_cleaning &lt;-  replace_contraction(file_cleaning)
file_cleaning &lt;- tolower(file_cleaning)
file_cleaning &lt;- removePunctuation(file_cleaning)
file_cleaning[467]
file_cleaned &lt;- stripWhitespace(file_cleaning)

custom_stops &lt;- c(""Bank"")
file_cleaning_stops &lt;- c(custom_stops, stopwords(""en""))
file_cleaned_stopped&lt;- removeWords(file_cleaning,file_cleaning_stops)

file_cleaned_corups&lt;- VCorpus(VectorSource(file_cleaned))
file_cleaned_tdm &lt;-TermDocumentMatrix(file_cleaned_corups)
dim(file_cleaned_tdm) # Number of terms &lt;number of documents
file_cleaned_mx &lt;- as.matrix(file_cleaned_tdm)

file_cleaned_corups&lt;- VCorpus(VectorSource(file_cleaned_stopped))
file_cleaned_tdm &lt;-TermDocumentMatrix(file_cleaned_corups)
file_cleaned_mx &lt;- as.matrix(file_cleaned_tdm)

dim(file_cleaned_mx)
file_cleaned_mx[220:225, 475:478]

coffee_m &lt;- as.matrix(coffee_tdm)

term_frequency &lt;- rowSums(file_cleaned_mx)
term_frequency &lt;- sort(term_frequency, decreasing = TRUE)
term_frequency[1:10]


BigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigram_dtm &lt;- TermDocumentMatrix(file_cleaned_corups, control = list(tokenize = BigramTokenizer))
dim(bigram_dtm)

bigram_bi_mx &lt;- as.matrix(bigram_dtm)
term_frequency &lt;- rowSums(bigram_bi_mx)
term_frequency &lt;- sort(term_frequency, decreasing = TRUE)
term_frequency[1:15]

freq_bigrams &lt;- findFreqTerms(bigram_dtm, 25)
freq_bigrams}
</code></pre>

<p>SAMPLE of DATASET:</p>

<pre><code>&gt; dput(droplevels(head(files,4)))

structure(list(Score = c(10L, 10L, 10L, 7L), Comments = structure(c(4L,

3L, 1L, 2L), .Label = c(""They are nice an quick. 3 years with them, and no issue."",

""Staff not very friendly."",

""I have to called them 3 times. They are very slow."",

""Quick and easy. High value.""

), class = ""factor"")), row.names = c(NA, 4L), class = ""data.frame"")
</code></pre>
",Preprocessing of the text & Tokenization,r text analysis misleading result text analysis comment bank customer related mortgage find couple thing understand cleaning data without applying stemming word checking dimension tdm number term smaller number document remove stop word tdm gram also wanted check word frequency rowsums matrix bi gram tokenizing tdm issue example gotten repeated result word proble miss since grouping wa already strange gone dataset control f try find could question seems code ha stemmed word possible top bi word one one seems stemmed supposed create bi gram always together sample dataset
How to generate the result of bigrams with highest probabilities with a list of individual alphabetical strings as input,"<p>I am learning natural language processing for bigram topic. At this stage, I am having difficulty in the Python computation, but I try.</p>

<p>I will be using this corpus that has not been subjected to tokenization as my main raw dataset. I can generate the bigram results using nltk module. However, my question is how to compute in Python to generate the bigrams containing more than two specific words. More specifically, I wish to find all the bigrams, which are available in corpus_A, that contain words from the word_of_interest. </p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>corpus = [""he is not giving up so easily but he feels lonely all the time his mental is strong and he always meet new friends to get motivation and inspiration to success he stands firm for academic integrity when he was young he hope that santa would give him more friends after he is a grown up man he stops wishing for santa clauss to arrival  he and his friend always eat out but they clean their hand to remove sand first before eating""]</p>
      
      <p>word_of_interest = ['santa', 'and', 'hand', 'stands', 'handy', 'sand']</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>I want to get the bigram for each of the individual words from the list of word_of_interest. Next, I want to get the frequency for each bigram available based on their appearance in the corpus_A. With the frequency available, I want to sort and print out the bigram based on their probability from highest to lower. </p>

<p>I have tried out codes from on-line search but it does not give me an output. The codes are mentioned below:</p>

<pre><code>for i in corpus:
    bigrams_i = BigramCollocationFinder.from_words(corpus, window_size=5)
    bigram_j = lambda i[x] not in i
    x += 1
print(bigram_j)
</code></pre>

<p>Unfortunately, the output did not return what I am planning to achieve. </p>

<p>Please advice me. The output that I want will have the bigram with the specific words from the word_of_interest and their probabilities sorted as shown below.</p>

<pre><code>[((santa, clauss), 0.89), ((he, and), 0.67), ((stands, firm), 0.34))] 
</code></pre>
",Preprocessing of the text & Tokenization,generate result bigram highest probability list individual alphabetical string input learning natural language processing bigram topic stage difficulty python computation try using corpus ha subjected tokenization main raw dataset generate bigram result using nltk module however question compute python generate bigram containing two specific word specifically wish find bigram available corpus contain word word interest corpus giving easily feel lonely time mental strong always meet new friend get motivation inspiration success stand firm academic integrity wa young hope santa would give friend grown man stop wishing santa clauss arrival friend always eat clean hand remove sand first eating word interest santa hand stand handy sand want get bigram individual word list word interest next want get frequency bigram available based appearance corpus frequency available want sort print bigram based probability highest lower tried code line search doe give output code mentioned unfortunately output return planning achieve please advice output want bigram specific word word interest probability sorted shown
How to un-stem a word in Python?,"<p>I want to know if there is anyway that I can un-stem them to a normal form?</p>

<p>The problem is that I have thousands of words in different forms e.g. eat, eaten, ate, eating and so on and I need to count the frequency of each word. All of these - eat, eaten, ate, eating etc will count towards eat and hence, I used stemming.</p>

<p>But the next part of the problem requires me to find similar words in data and I am using nltk's synsets to calculate Wu-Palmer Similarity among the words. The problem is that nltk's synsets wont work on stemmed words, or at least in this code they won't. <a href=""https://stackoverflow.com/questions/18871706/check-if-two-words-are-related-to-each-other"">check if two words are related to each other</a></p>

<p>How should I do it? Is there a way to un-stem a word?</p>
",Preprocessing of the text & Tokenization,un stem word python want know anyway un stem normal form problem thousand word different form e g eat eaten ate eating need count frequency word eat eaten ate eating etc count towards eat hence used stemming next part problem requires find similar word data using nltk synset calculate wu palmer similarity among word problem nltk synset wont work stemmed word least code href two word related way un stem word
Python nltk stemmers never remove prefixes,"<p>I'm trying to preprocess words to remove common prefixes like ""un"" and ""re"", however all of nltk's common stemmers seem to completely ignore prefixes:</p>

<pre><code>from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer

PorterStemmer().stem('unhappy')
# u'unhappi'

SnowballStemmer('english').stem('unhappy')
# u'unhappi'

LancasterStemmer().stem('unhappy')
# 'unhappy'

PorterStemmer().stem('reactivate')
# u'reactiv'

SnowballStemmer('english').stem('reactivate')
# u'reactiv'

LancasterStemmer().stem('reactivate')
# 'react'
</code></pre>

<p>Isn't part of the job of a stemmer to remove common prefixes as well as suffixes? Is there another stemmer which does this reliably?</p>
",Preprocessing of the text & Tokenization,python nltk stemmer never remove prefix trying preprocess word remove common prefix like un however nltk common stemmer seem completely ignore prefix part job stemmer remove common prefix well suffix another stemmer doe reliably
prevent word split based on - in sentences,"<p>This:</p>

<pre><code>import re

title = 'Decreased glucose-6-phosphate dehydrogenase activity along with oxidative stress affects visual contrast sensitivity in alcoholics.'

words = list(filter(None, re.split('\W+', title)))
for word in words:
    print(word)
</code></pre>

<p>results in:</p>

<pre><code>Decreased
glucose
6
phosphate
dehydrogenase
activity
along
with
oxidative
stress
affects
visual
contrast
sensitivity
in
alcoholics
</code></pre>

<p>Ideally, I would like to prevent the splitting of words like:</p>

<pre><code>glucose-6-phosphate 
</code></pre>

<p>Is there a better way to obtain separate words of a sentence like this in Python? Should I adopt the regular expression or is there something OOTB? Thanks.</p>
",Preprocessing of the text & Tokenization,prevent word split based sentence result ideally would like prevent splitting word like better way obtain separate word sentence like python adopt regular expression something ootb thanks
How does spacy lemmatizer works?,"<p>For lemmatization spacy has a <a href=""https://github.com/explosion/spaCy/tree/master/spacy/en/lemmatizer"" rel=""noreferrer"">lists of words</a>:  adjectives, adverbs, verbs... and also lists for exceptions: adverbs_irreg... for the regular ones there is a set of <a href=""https://github.com/explosion/spaCy/blob/master/spacy/en/lemmatizer/_lemma_rules.py"" rel=""noreferrer"">rules</a></p>

<p>Let's take as example the word ""wider""</p>

<p>As it is an adjective the rule for lemmatization should be take from this list:</p>

<pre><code>ADJECTIVE_RULES = [
    [""er"", """"],
    [""est"", """"],
    [""er"", ""e""],
    [""est"", ""e""]
] 
</code></pre>

<p>As I understand the process will be like this:</p>

<p>1) Get the POS tag of the word to know whether it is a noun, a verb...<br>
2) If the word is in the list of irregular cases is replaced directly if not one of the rules is applied.</p>

<p>Now, how is decided to use ""er"" -> ""e"" instead of ""er""-> """" to get ""wide"" and not ""wid""? </p>

<p><a href=""http://textanalysisonline.com/spacy-word-lemmatize"" rel=""noreferrer"">Here</a> it can be tested.</p>
",Preprocessing of the text & Tokenization,doe spacy lemmatizer work lemmatization spacy ha list word adjective adverb verb also list exception adverb irreg regular one set rule let take example word wider adjective rule lemmatization take list understand process like get po tag word know whether noun verb word list irregular case replaced directly one rule applied decided use er e instead er get wide wid tested
How to tokenize every sentence into indivdual words in row of dataframe and average the polarity for every word in sentence?,"<p>I have a df that looks like this: </p>

<pre><code>       text
0   Thanks, I’ll have a read!
1   Am I too late
</code></pre>

<p>How do I apply TextBlob tokenization to every word in sentence and average the polarity scores of every word in each sentence? </p>

<p>for example, I can do this with a single sentence in a variable: </p>

<pre><code>from textblob import TextBlob
import import statistics as s

#tokenize word in sentence
a = TextBlob(""""""Thanks, I'll have a read!"""""")
print a.words

    WordList(['Thanks', 'I', ""'ll"", 'have', 'a', 'read'])

#get polarity of every word
    for i in a.words:
        print( a.sentiment.polarity)

    0.25
    0.25
    0.25
    0.25
    0.25
    0.25


#calculating the mean of the scores
c=[]
for i in a.words: 
    c.append(a.sentiment.polarity)
    d = s.mean(c)
    print (d)

0.25
</code></pre>

<p>How do I apply the <code>a.words</code> to every row of dataframe column for sentence? </p>

<p>New df: </p>

<pre><code>      text                        score
0   Thanks, I’ll have a read!      0.25
1   Am I too late                  0.24
</code></pre>

<p>closet I come is that I can get polarity of every sentence using this function on the dataframe:</p>

<pre><code>def sentiment_calc(text):
    try:
        return TextBlob(text).sentiment.polarity
    except:
        return None

df_sentences['sentiment'] = df_sentences['text'].apply(sentiment_calc)
</code></pre>

<p>Thank you in advance. </p>
",Preprocessing of the text & Tokenization,tokenize every sentence indivdual word row dataframe average polarity every word sentence df look like apply textblob tokenization every word sentence average polarity score every word sentence example single sentence variable apply every row dataframe column sentence new df closet come get polarity every sentence using function dataframe thank advance
Removing German stop words in R,"<p>I have survey data with a comments column. I am looking to so sentiment analysis on the responses. The problem is there are many languages in the data and I can't figure out how to eliminate multiple language stopwords from the set</p>

<p>'nps' is my data source, nps$customer.feedback is the comments column.</p>

<p>First I tokenize the data</p>

<pre><code>#TOKENISE
comments &lt;- nps %&gt;% 
  filter(!is.na(cusotmer.feedback)) %&gt;% 
  select(cat, Comment) %&gt;% 
  group_by(row_number(), cat) 

  comments &lt;- comments %&gt;% ungroup()
</code></pre>

<p>Getting rid of stopwords</p>

<pre><code>nps_words &lt;-  nps_words %&gt;% anti_join(stop_words, by = c('word'))
</code></pre>

<p>Then use Stemming and get_sentimets(""bing"") to show word counts by sentiment.</p>

<pre><code> #stemgraph
  nps_words %&gt;% 
  mutate(word = wordStem(word)) %&gt;% 
  inner_join(get_sentiments(""bing"") %&gt;% mutate(word = wordStem(word)), by = 
  c('word')) %&gt;%
  count(cat, word, sentiment) %&gt;%
  group_by(cat, sentiment) %&gt;%
  top_n(7) %&gt;%
  ungroup() %&gt;%
  ggplot(aes(x=reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  facet_wrap( ~cat, scales = ""free"")  +
  scale_fill_brewer(palette = ""Set1"") +
  labs(title = ""Word counts by Sentiment by Category - Bing (Stemmed)"", x = 
  `""Words"", y = ""Count"")`
</code></pre>

<p>However, ""di"" and ""die"" appear in 'negative' graph due to German text being analyzed.</p>

<p>Can someone help?</p>

<p>My goal is to get eliminate German stopwords using the above code. </p>
",Preprocessing of the text & Tokenization,removing german stop word r survey data comment column looking sentiment analysis response problem many language data figure eliminate multiple language stopwords set np data source np customer feedback comment column first tokenize data getting rid stopwords use stemming get sentimets bing show word count sentiment however di die appear negative graph due german text analyzed someone help goal get eliminate german stopwords using code
Text mining - Stemming method without `tm` package,"<p>I´m dealing with a text mining task. Today, I have a problem with the stemming method.
I have several paragraphs in this format. These are character object, not list neither Corpus object from tm package.</p>

<p>[1] "" andres oppenheimer intelectuales influyentes latinoamerica segun revista foreign policy editor columnista miami herald sigue recorriendo continente presenta reportajes cnn tradicional ciclo periodistico argentina presentando libro salvese pueda analiza futuro mundo automatizacion robotizacion "" </p>

<p>I have a dictionary where some words has to be match in the corpus above. The problem is that I couldn´t do it through the stemming method. My syntax is the following: </p>

<pre><code>lexicon&lt;- read.xlsx(""lexicon nf.xlsx"",sheetName = ""lex"",colIndex = 1,header = T)
lexicon$palabra&lt;- as.character(lexicon$palabra)
match&lt;- paste(lexicon$palabra[order(-nchar(lexicon$palabra))],collapse = ""|^"")
</code></pre>

<p>If I try:</p>

<pre><code>match&lt;- paste(lexicon$palabra[order(-nchar(lexicon$palabra))],collapse = ""|"")
</code></pre>

<p>It matches the word in any position, but this is not what I want. I know that if a split the words of the corpus by, for instance the space, I can make the match as I need, but this is a more complicated aproach. I wish to do it directly from the paragraph, But without turn it into a Corpus object. </p>

<p>Any idea? Thank you very much for your help!</p>
",Preprocessing of the text & Tokenization,text mining stemming method without package dealing text mining task today problem stemming method several paragraph format character object list neither corpus object tm package andres oppenheimer intelectuales influyentes latinoamerica segun revista foreign policy editor columnista miami herald sigue recorriendo continente presenta reportajes cnn tradicional ciclo periodistico argentina presentando libro salvese pueda analiza futuro mundo automatizacion robotizacion dictionary word ha match corpus problem stemming method syntax following try match word position want know split word corpus instance space make match need complicated aproach wish directly paragraph without turn corpus object idea thank much help
"Python: How remove punctuation in text corpus, but not remove it in special words (e.g. c++, c#, .net, etc)","<p>I have a big pandas dataset with job descriptions. I want to tokenize it, but before this I should remove stopwords and punctuation. I have no problems with stopwords. </p>

<p>If I will use regex for removing punctuation, I can lose very important words that describe jobs (e.g. c++ developer, c#, .net, etc.).</p>

<p>List of such important words is very big, because it consists not only programming languages names but also companies names.</p>

<p>For exmaple, I want the next way of removing punctuation:</p>

<p>Before:</p>

<blockquote>
  <p>Hi! We are looking for smart, young and hard-working c++ developer. Our perfect candidate should know: - c++, c#, .NET in expert level;</p>
</blockquote>

<p>After:</p>

<blockquote>
  <p>Hi We are looking for smart young and hard-working c++ developer Our perfect candidate should know  c++ c# .NET in expert level</p>
</blockquote>

<p>Can you advise me advance tockenizers or methods for removing punctuation?</p>
",Preprocessing of the text & Tokenization,python remove punctuation text corpus remove special word e g c c net etc big panda dataset job description want tokenize remove stopwords punctuation problem stopwords use regex removing punctuation lose important word describe job e g c developer c net etc list important word big consists programming language name also company name exmaple want next way removing punctuation hi looking smart young hard working c developer perfect candidate know c c net expert level hi looking smart young hard working c developer perfect candidate know c c net expert level advise advance tockenizers method removing punctuation
extracting n grams from huge text,"<p>For example we have following text:</p>

<blockquote>
  <p>""Spark is a framework for writing fast, distributed programs. Spark
  solves similar problems as Hadoop MapReduce does but with a fast
  in-memory approach and a clean functional style API. ...""</p>
</blockquote>

<p>I need all possible section of this text respectively, for one word by one word, then two by two, three by three to five to five.
like this:</p>

<blockquote>
  <p>ones : ['Spark', 'is', 'a', 'framework', 'for', 'writing, 'fast',
  'distributed', 'programs', ...]</p>
  
  <p>twos : ['Spark is', 'is a', 'a framework', 'framework for', 'for writing'
  ...] </p>
  
  <p>threes : ['Spark is a', 'is a framework', 'a framework for', 
  'framework for writing', 'for writing fast', ...]</p>
  
  <p>. . .</p>
  
  <p>fives : ['Spark is a framework for', 'is a framework for writing',
  'a framework for writing fast','framework for writing fast distributed', ...]</p>
</blockquote>

<p>Please note that the text to be processed is huge text( about 100GB).
I need the best solution for this process. May be it should be processed multi thread in parallel.</p>

<p>I don't need whole list at once, it can be streaming.</p>
",Preprocessing of the text & Tokenization,extracting n gram huge text example following text spark framework writing fast distributed program spark solves similar problem hadoop mapreduce doe fast memory approach clean functional style api need possible section text respectively one word one word two two three three five five like one spark framework writing fast distributed program two spark framework framework writing three spark framework framework framework writing writing fast five spark framework framework writing framework writing fast framework writing fast distributed please note text processed huge text gb need best solution process may processed multi thread parallel need whole list streaming
training sentence tokenizer in spaCy,"<p>I'm trying to tokenize sentences using spacy. </p>

<p>The text includes lots of abbreviations and comments which ends with a period. Also, the text was obtained with OCR and sometimes there are line breaks in the middle of sentences. Spacy doesn't seem to be performing so well in these situations.</p>

<p>I have extracted some examples of how I want these sentences to be split. Is there any way to train spacy's sentence tokenizer?</p>
",Preprocessing of the text & Tokenization,training sentence tokenizer spacy trying tokenize sentence using spacy text includes lot abbreviation comment end period also text wa obtained ocr sometimes line break middle sentence spacy seem performing well situation extracted example want sentence split way train spacy sentence tokenizer
How to ignore characters while tokenizing Keras,"<p>I am trying to train &amp; build a tokenizer using Keras &amp; here is the snippet of code where I am doing this: </p>

<pre><code>from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense

txt1=""""""What makes this problem difficult is that the sequences can vary in length,
be comprised of a very large vocabulary of input symbols and may require the model 
to learn the long term context or dependencies between symbols in the input sequence.""""""

#txt1 is used for fitting 
tk = Tokenizer(nb_words=2000, lower=True, split="" "",char_level=False)
tk.fit_on_texts(txt1)

#convert text to sequencech
t= tk.texts_to_sequences(txt1)

#padding to feed the sequence to keras model
t=pad_sequences(t, maxlen=10)
</code></pre>

<p>Upon testing which words the Tokenizer has learned, it gives that it has only learned characters but not words. </p>

<pre><code>print(tk.word_index)
</code></pre>

<p>output: </p>

<pre><code>{'e': 1, 't': 2, 'n': 3, 'a': 4, 's': 5, 'o': 6, 'i': 7, 'r': 8, 'l': 9, 'h': 10, 'm': 11, 'c': 12, 'u': 13, 'b': 14, 'd': 15, 'y': 16, 'p': 17, 'f': 18, 'q': 19, 'v': 20, 'g': 21, 'w': 22, 'k': 23, 'x': 24}
</code></pre>

<p>why it does not have any words ?</p>

<p>Furthermore, if I print t, it clearly shows that, words are ignored and each word is tokenized char by char</p>

<pre><code>print(t)  
</code></pre>

<p>Output: </p>

<pre><code>[[ 0  0  0 ...  0  0 22]
 [ 0  0  0 ...  0  0 10]
 [ 0  0  0 ...  0  0  4]
 ...
 [ 0  0  0 ...  0  0 12]
 [ 0  0  0 ...  0  0  1]
 [ 0  0  0 ...  0  0  0]]
</code></pre>
",Preprocessing of the text & Tokenization,ignore character tokenizing kera trying train build tokenizer using kera snippet code upon testing word tokenizer ha learned give ha learned character word output doe word furthermore print clearly show word ignored word tokenized char char output
Remove punctuation from list of sentences in a pandas data frame,"<p>I have email messages in a pandas data frame. Before applying <strong>sent_tokenize</strong>, I could <strong>remove the punctuation</strong> like this. </p>

<pre><code>def removePunctuation(fullCorpus):
punctuationRemoved = fullCorpus['text'].str.replace(r'[^\w\s]+', '')
return  punctuationRemoved
</code></pre>

<p>After applying sent_tokenize the data frame looks like below. How can I remove the punctuation while keeping the sentences as tokenized in the lists?</p>

<blockquote>
  <p>sent_tokenize</p>
</blockquote>

<pre><code>def tokenizeSentences(fullCorpus):
sent_tokenized = fullCorpus['body_text'].apply(sent_tokenize)
return sent_tokenized
</code></pre>

<blockquote>
  <p>Sample of data frame after tokenizing into sentences</p>
</blockquote>

<pre><code>[Nah I don't think he goes to usf, he lives around here though]                                                                                                                                                                                                                          

[Even my brother is not like to speak with me., They treat me like aids patent.]                                                                                                                                                                                                         

[I HAVE A DATE ON SUNDAY WITH WILL!, !]                                                                                                                                                                                                                                                  

[As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers., Press *9 to copy your friends Callertune]                                                                                                                      

[WINNER!!, As a valued network customer you have been selected to receivea £900 prize reward!, To claim call 09061701461., Claim code KL341., Valid 12 hours only.]
</code></pre>
",Preprocessing of the text & Tokenization,remove punctuation list sentence panda data frame email message panda data frame applying sent tokenize could remove punctuation like applying sent tokenize data frame look like remove punctuation keeping sentence tokenized list sent tokenize sample data frame tokenizing sentence
SpaCy custom stopwords not working properly,"<p>There is ONE word not being recognized as stopword, despite being on the list.
I'm working with spacy 2.0.11, python 3.7, conda env, Debian 9.5</p>

<pre><code>import spacy
from spacy.lang.es.stop_words import STOP_WORDS
nlp = spacy.load('es', disable=['tagger', 'parser', 'ner'])
STOP_WORDS.add('y')
</code></pre>

<p>Doing some tests:</p>

<pre><code>&gt;&gt;&gt; word = 'y'
&gt;&gt;&gt; word in STOP_WORDS
True
&gt;&gt;&gt; nlp(word)[0].is_stop
False
&gt;&gt;&gt; len(STOP_WORDS)
305
&gt;&gt;&gt; [word for word in STOP_WORDS if not nlp(word)[0].is_stop]
['y']
</code></pre>

<p>So, from 305 listed in STOP_WORDS, one is not flagged as such. I don't know what I'm doing wrong... Maybe it's a bug?</p>
",Preprocessing of the text & Tokenization,spacy custom stopwords working properly one word recognized stopword despite list working spacy python conda env debian test listed stop word one flagged know wrong maybe bug
"Where can I find list of words like [&#39;Dr.&#39;, &#39;Mrs&#39;, &#39;D.C.&#39;, &#39;Inc.&#39;,&#39;.com&#39;] that are to be ignored while splitting text by Period(punctuation)?","<p>I have text from huge Text/PDF file. I am working on the text to do sentence tokenization using the Period (punctuation). But, I am facing issues with cases like ['Dr.', 'Mrs', 'D.C.', 'Inc.','.com']. To deal with this, I am looking for complete list of such words. Where can I find corpus of all these prefixes/abbreviations/suffixes? 
Thanks.   </p>
",Preprocessing of the text & Tokenization,find list word like dr mr c inc com ignored splitting text period punctuation text huge text pdf file working text sentence tokenization using period punctuation facing issue case like dr mr c inc com deal looking complete list word find corpus prefix abbreviation suffix thanks
Detect stopword after lemma in Spacy,"<p>How to detect if word is a stopword after stemming and lemmatization in <code>spaCy</code>?</p>

<p>Assume sentence</p>

<pre><code>s = ""something good\nsomethings 2 bad""
</code></pre>

<p>In this case <code>something</code> is a stopword. Obviously (to me?) <code>Something</code> and <code>somethings</code> are also stopwords, but it needs to stemmed before. Following script will say that the first is true, but latter isn't.</p>

<pre><code>import spacy
from spacy.tokenizer import Tokenizer
nlp = spacy.load('en')
tokenizer = Tokenizer(nlp.vocab)

s = ""something good\nSomething 2 somethings""
tokens = tokenizer(s)

for token in tokens:
  print(token.lemma_, token.is_stop)
</code></pre>

<p>Returns:</p>

<pre><code>something True
good False
""\n"" False
Something False
2 False
somethings False
</code></pre>

<p>Is there a way to detect that through <code>spaCy</code> API?</p>
",Preprocessing of the text & Tokenization,detect stopword lemma spacy detect word stopword stemming lemmatization assume sentence case stopword obviously also stopwords need stemmed following script say first true latter return way detect api
Determine if a sentence is an inquiry,"<p>How can I detect if a search query is in the form of a question?</p>

<p>For example, a customer might search for ""how do I track my order"" (notice no question mark).</p>

<p>I'm guessing most direct questions would conform to a particular grammar.</p>

<p>Very simple guessing approach:</p>

<pre><code>START WORDS = [who, what, when, where, why, how, is, can, does, do]

isQuestion(sentence):
  sentence ends with '?'
  OR sentence starts with one of START WORDS
</code></pre>

<p>START WORDS list could be longer. The scope is a website search box, so I imagine the list shouldn't need to include too many words.</p>

<p>Is there a library that can do this better than my simple guessing approach? Any improvements on my approach?</p>
",Preprocessing of the text & Tokenization,determine sentence inquiry detect search query form question example customer might search track order notice question mark guessing direct question would conform particular grammar simple guessing approach start word list could longer scope website search box imagine list need include many word library better simple guessing approach improvement approach
Text similarity approaches do not reflect &quot;real&quot; similarity between texts,"<p>I am comparing the content of CVs (.txt files with stop-words already removed) with really compact job descriptions (JDs), like this:</p>

<blockquote>
  <p>project management,
  leadership,
  sales,
  SAP,
  marketing </p>
</blockquote>

<p>The CVs have around 600 words and the JDs only the words highlighted above.</p>

<p>The problem that I am currently experiencing, and I am sure this is due to my lack of knowledge, is that when I apply similarity measures to it, I get confuse results. For example I have the CV number 1 which contains all the words from the JD, sometimes repeated more than once. I also have CV 2 which only contains the word <strong><em>project</em></strong> in comparsion to the JD.  Even though, when I apply <strong><em>cosine similarity, diff, jaccard distance and edit distance</em></strong>, all these measures return to me a higher degree of similarity between the CV2 and the JD, which for me is strange, because only one word is equal between them, while the CV1 possesses all the words from the JD. </p>

<p>I am applying the wrong measures to assess similarity? I am sorry if this is a naive question, I am a beginner with programming. </p>

<p>Codes follow</p>

<p><strong>Diff</strong></p>

<pre><code>    from difflib import SequenceMatcher
    def similar(a, b):
        return SequenceMatcher(None, a, b).ratio()
    similar('job.txt','LucasQuadros.txt')
    0.43478260869565216
    similar('job.txt','BrunaA.Fernandes.txt')
    0.2962962962962963
</code></pre>

<p><strong>Cosine</strong></p>

<pre><code>    from sklearn.feature_extraction.text import TfidfVectorizer
    document= ('job.txt','LucasQuadros.txt','BrunaA.Fernandes')
    tfidf = TfidfVectorizer().fit_transform(document)
    matrix= tfidf * tfidf.T
    matrix.todense()
    matrix([[1.        , 0.36644682, 0.        ],
    [0.36644682, 1.        , 0.        ],
    [0.        , 0.        , 1.        ]])
</code></pre>

<p><strong>Edit distance</strong></p>

<pre><code>    import nltk
    w1= ('job.txt')
    w2= ('LucasQuadros.txt')
    w3= ('BrunaA.Fernandes.txt')
    nltk.edit_distance(w1,w2)
    11
    nltk.edit_distance(w1,w3)
    16
</code></pre>

<p><strong>Jaccard distance</strong></p>

<pre><code>    import nltk
    a1= set('job.txt')
    a2= set('LucasQuadros.txt')
    a3= set('BrunaA.Fernandes.txt')
    nltk.jaccard_distance(a1,a2)
    0.7142857142857143
    nltk.jaccard_distance(a1,a3)
    0.8125
</code></pre>

<p>As you guys can see, the 'LucasQuadros.txt'(CV1) has a higher similarity with the 'job.txt'(Job Description), even though it only contains one word from the job description.</p>
",Preprocessing of the text & Tokenization,text similarity approach reflect real similarity text comparing content cv txt file stop word already removed really compact job description jds like project management leadership sale sap marketing cv around word jds word highlighted problem currently experiencing sure due lack knowledge apply similarity measure get confuse result example cv number contains word jd sometimes repeated also cv contains word project comparsion jd even though apply cosine similarity diff jaccard distance edit distance measure return higher degree similarity cv jd strange one word equal cv posse word jd applying wrong measure ass similarity sorry naive question beginner programming code follow diff cosine edit distance jaccard distance guy see lucasquadros txt cv ha higher similarity job txt job description even though contains one word job description
Get list of all words that can be stemmed to a particular stemming,"<p>I was wondering if it was possible to get a list of longer words that could be stemmed to the same word. Basically we have a list of stemmed words, and we would like to make a new corpus with all words that could have generated this stemming. For example you can do:</p>

<pre><code>import nltk
ps = nltk.stem.PorterStemmer()
print(ps.stem('rowing'))
'row'
print(ps.stem('rows'))
'row'
print(ps.stem('rowed'))
'row'   
</code></pre>

<p>In our case we have the word <code>row</code> and we like put this in some function and get the list <code>['rowing','rows', 'rowed']</code> (this assumes those are the only words that stem to row). Is this implemented in nltk? The hack solution is to just add every common ending to every stemmed word we have, but we would like to avoid doing that. I used PorterStemmer in the example, but if this inverse look up works for any stemmer that would work for us. Thanks!</p>
",Preprocessing of the text & Tokenization,get list word stemmed particular stemming wa wondering wa possible get list longer word could stemmed word basically list stemmed word would like make new corpus word could generated stemming example case word like put function get list assumes word stem row implemented nltk hack solution add every common ending every stemmed word would like avoid used porterstemmer example inverse look work stemmer would work u thanks
OpenNLP Tokenizer does not detect words that belong together?,"<p>I am new to NLP and I came across OpenNLP. From my understanding <code>tokenization</code> means segmenting text into words and sentences. Words are often separated by white spaces but not all white spaces are equal. For example Los Angeles in an individual thought regardless of the white space. But whenever I run the OpenNLP Tokenizer, it creates two distinct tokens for Los Angeles: <strong>Los</strong> &amp; <strong>Angeles</strong>. Here is my code (I got the model en-token.bin from the old OpenNLP site).</p>

<pre><code>InputStream inputStream = new FileInputStream(""C:\\apache-opennlp-1.9.0\\Models\\en-token.bin""); 
TokenizerModel tokenModel = new TokenizerModel(inputStream);
//Instantiating the TokenizerME class 
TokenizerME tokenizer = new TokenizerME(tokenModel);

String tokens[] = tokenizer.tokenize(sentence2);

for(String token : tokens) {
     System.out.println(token);
}
</code></pre>

<p>Here is the output:</p>

<pre><code>The
city
of
Los
Angeles
is
one
of
the
most
beautiful
places
in
California
</code></pre>

<p>I tested some other tokenizers online and they produce the same output. If it is not tokenization, what would be the process to identify these two words belong together?</p>
",Preprocessing of the text & Tokenization,opennlp tokenizer doe detect word belong together new nlp came across opennlp understanding mean segmenting text word sentence word often separated white space white space equal example los angeles individual thought regardless white space whenever run opennlp tokenizer creates two distinct token los angeles los angeles code got model en token bin old opennlp site output tested tokenizers online produce output tokenization would process identify two word belong together
Quanteda: error message while tokenizing &quot;unable to find an inherited method for function ‘tokens’ for signature ‘&quot;corpus&quot;’&quot;,"<p>I have been trying to tokenise and clean my 400 txt documents before using structured topic modelling (STM). I wanted to remove punctuations, stopwords, symbols, etc. However, I get the following error message: 
<em>""Error in (function (classes, fdef, mtable): unable to find an inherited method for function ‘tokens’ for signature ‘""corpus""’""</em>. This is my original code:</p>

<pre><code>answers2 &lt;- tokens(answers_corpus, what = c(""word""), remove_numbers = TRUE, remove_punct = TRUE,
   remove_symbols = TRUE, remove_separators = TRUE,
   remove_twitter = TRUE, remove_hyphens = TRUE, remove_url = TRUE,
   ngrams = 1L, verbose = quanteda_options(""verbose""), include_docvars = TRUE, text_field = ""text"")
</code></pre>

<p>I also tried to tokenize a simple string text - just to check if it was an encoding problem while importing my txt files - but I got the same error message, plus a couple of extra ones when I tried to tokenise the the text directly, without converting it to corpus: ""<em>Error: Unable to locate Ciao bella ciao</em>"" and ""<em>Error: No language specified!</em>"". Here is my example code in case someone wants to replicate the error message:</p>

<pre><code>prova &lt;- c(""Ciao bella ciao"")
prova2 &lt;- ""Ciao bella ciao""
prova_corpus &lt;- corpus(prova)
prova2_corpus &lt;- corpus(prova2)
prova_tok &lt;- tokens(prova2_corpus)
prova2_tok &lt;- tokens(prova_corpus)
</code></pre>

<p>The packages that are loaded are: data.table, ggplot2, quanteda, readtext, stm, stringi, stringr, tm, textstem. Any suggestion on how I could proceed to tokenise and clean my texts?</p>
",Preprocessing of the text & Tokenization,quanteda error message tokenizing unable find inherited method function token signature corpus trying tokenise clean txt document using structured topic modelling stm wanted remove punctuation stopwords symbol etc however get following error message error function class fdef mtable unable find inherited method function token signature corpus original code also tried tokenize simple string text check wa encoding problem importing txt file got error message plus couple extra one tried tokenise text directly without converting corpus error unable locate ciao bella ciao error language specified example code case someone want replicate error message package loaded data table ggplot quanteda readtext stm stringi stringr tm textstem suggestion could proceed tokenise clean text
Splitting a List and removing corresponding elements in Python,"<p>I have a List of Parts of Speech tagged words (each element is in the format of ""word|tag"") and I am trying to find a way to delete the corresponding ""tag"" after I delete a certain ""word."" More specifically, my algorithm can only deal with the ""word"" portion of each element, so I first split my current ""word""|""tag"" list into two separate lists of words and tags. After I remove certain unnecessary words from the Words list though, I want to concatenate the corresponding tags. How can I effectively delete the corresponding tag from a different list? Or is there a better way to do this? I tried running my cleaning algorithm with the tagged words initially, but couldn't find a way to ignore the tags from each word.</p>

<p>My issue may be more clear by showing my code:</p>

<pre><code>my_list = ['I|PN', 'am|V', 'very|ADV', 'happy|ADJ']

tags = []
words = []
for i, x in enumerate(my_list):
    front, mid, end = x.partition('|')
    words.append(front)
    tags.append(mid+end)
</code></pre>

<p>Current Output (after I run the words list through my cleaning algorithm):</p>

<pre><code>words = ['I', 'very', 'happy']
tags = ['PN', 'V', 'ADV', 'ADJ']
</code></pre>

<p>Clearly, I can not concatenate these lists element-wise anymore because I did not delete the corresponding tag from the removed word. </p>

<p>Desired Output:</p>

<pre><code>words = ['I', 'very', 'happy']
tags = ['PN', 'ADV', 'ADJ']
</code></pre>

<p>How can I achieve the above output?</p>
",Preprocessing of the text & Tokenization,splitting list removing corresponding element python list part speech tagged word element format word tag trying find way delete corresponding tag delete certain word specifically algorithm deal word portion element first split current word tag list two separate list word tag remove certain unnecessary word word list though want concatenate corresponding tag effectively delete corresponding tag different list better way tried running cleaning algorithm tagged word initially find way ignore tag word issue may clear showing code current output run word list cleaning algorithm clearly concatenate list element wise anymore delete corresponding tag removed word desired output achieve output
How to consider word pairs/phrases for Word2Vec and other pre-processing,"<p>So it's my first time using Word2Vec and Im using a wikipedia dump with WikiCorpus to pre-process the file before training my Word2Vec model. I want to use the following pre-processing techniques:</p>

<ol>
<li>Convert all letters to lowercase (I think WikiCorpus does this already).</li>
<li>Remove all punctuation (Done by WikiCorpus).</li>
<li>Consider word pairs/phrases as a single word, for example 'Big Apple' -> 'big_apple', not 'big', 'apple'.</li>
<li>Convert all digits to their word forms, so '3' -> 'three' </li>
</ol>

<p>At the moment I have no idea how to do the last two. I know about num2text but not sure how to incorporate with WikiCorpus or Word2vec. Can anyone help?</p>
",Preprocessing of the text & Tokenization,consider word pair phrase word vec pre processing first time using word vec im using wikipedia dump wikicorpus pre process file training word vec model want use following pre processing technique convert letter lowercase think wikicorpus doe already remove punctuation done wikicorpus consider word pair phrase single word example big apple big apple big apple convert digit word form three moment idea last two know num text sure incorporate wikicorpus word vec anyone help
How to create new entity and use it to find the entity in my test data? How to make my tokenize works?,"<p>I would like to make a new entity: let's call it ""medicine"" and then train it using my corpora. From there, identify all the entities of ""medicine"". Somehow my code is not working, could anyone help me? </p>

<pre><code>import nltk


test= input(""Please enter your file name"")
test1= input(""Please enter your second file name"")

with open(test, ""r"") as file:  
    new = file.read().splitlines()


with open(test1, ""r"") as file2:
    new1= file2.read().splitlines()


for s in new:
    for x in new1:
        sample = s.replace('value', x)

        sample1 = ''.join(str(v) for v in sample)

        print(sample1)


        sentences = nltk.sent_tokenize(sample1)
        tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]
        tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]
        chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)


        print(sentences)

def extract_entity_names(t):
    entity_names = []

    if hasattr(t, 'label') and t.label:
        if t.label() == 'NE':
            entity_names.append(' '.join([child[0] for child in t]))
        else:
            for child in t:
                entity_names.extend(extract_entity_names(child))

    return entity_names
</code></pre>
",Preprocessing of the text & Tokenization,create new entity use find entity test data make tokenize work would like make new entity let call medicine train using corpus identify entity medicine somehow code working could anyone help
How to improve Precision and Recall on Imbalanced Dataset in Python,"<p>I built a supervised model to classify medical text data (my output predicts the positive or negative occurrence of a disease). The data is very imbalanced (130 positive cases compared to 1600 negative cases, which is understandable since the disease is rare). I first cleaned the data (removed unnecessary words, lemmatization, etc..) and applied POS afterwards. I then applied TfidfVectorizer and TfidfTransformer to this cleaned data. For classification, I tried both SVM and Random Forest, but achieved only 56% precision and 58% recall for the positive data even after tuning their parameters with GridSearchCV (I also made class_weight = 'balanced'). Does anyone have advice as to how to improve this low precision and recall? Thank you very much.</p>

<p>Here is my current Pipeline (obviously I only use one of the classifiers when I run it, but I displayed both just to show their parameters).</p>

<pre><code>pipeline = Pipeline([ 

('vectors', TfidfVectorizer(ngram_range = (2,3),norm = 'l1', token_pattern = r""\w+\b\|\w+"" ,min_df = 2, max_features = 1000).fit(data['final'])),

('classifier', RandomForestClassifier(n_estimators = 51, min_samples_split = 8, min_samples_leaf = 2, max_depth = 14, class_weight= 'balanced')),

('classifier', SVC(C = 1000, gamma = 1, class_weight = 'balanced', kernel='linear')),

])
</code></pre>
",Preprocessing of the text & Tokenization,improve precision recall imbalanced dataset python built supervised model classify medical text data output predicts positive negative occurrence disease data imbalanced positive case compared negative case understandable since disease rare first cleaned data removed unnecessary word lemmatization etc applied po afterwards applied tfidfvectorizer tfidftransformer cleaned data classification tried svm random forest achieved precision recall positive data even tuning parameter gridsearchcv also made class weight balanced doe anyone advice improve low precision recall thank much current pipeline obviously use one classifier run displayed show parameter
How can I use regular expressions in my vocabulary for CountVectorizer?,"<p>How do I make ""First word in the doc was [target word]"" a feature? </p>

<p>Consider these two sentences:</p>

<pre><code>example = [""At the moment, my girlfriend is Jenny. She is working as an artist at the moment."",
       ""My girlfriend is Susie. She is working as an accountant at the moment.""]
</code></pre>

<p>If I were trying to measure relationship commitment, I'd want to be able to treat the phrase ""at the moment"" as a feature <em>only</em> when it shows up at the beginning like that.</p>

<p>I would <em>love</em> to be able to use regex's in the vocabulary...</p>

<pre><code>phrases = [""^at the moment"", 'work']
vect = CountVectorizer(vocabulary=phrases, ngram_range=(1, 3), token_pattern=r'\w{1,}')
dtm = vect.fit_transform(example)
</code></pre>

<p>But that doesn't seem to work. </p>

<p>I have also tried this, but get a 'vocabulary is empty' error...</p>

<pre><code>CountVectorizer(token_pattern = r""(?u)^currently"")
</code></pre>

<p>What's the right way to do this? Do I need a custom vectorizer? Any simple tutorials you can link me to? This is my first sklearn project, and I've been Googling this for hours. Any help much appreciated!</p>
",Preprocessing of the text & Tokenization,use regular expression vocabulary countvectorizer make first word doc wa target word feature consider two sentence trying measure relationship commitment want able treat phrase moment feature show beginning like would love able use regex vocabulary seem work also tried get vocabulary empty error right way need custom vectorizer simple tutorial link first sklearn project googling hour help much appreciated
How to pass user defined function inside TfidfVectorizer.fit_transform(),"<p>I have function for text preprocessing which is simply removing stopwords as:</p>

<pre><code>def text_preprocessing():
    df['text'] = df['text'].apply(word_tokenize)
    df['text']=df['text'].apply(lambda x: [item for item in x if item not in stopwords])
    new_array=[]
    for keywords in df['text']: #converts list of words into string
         P="" "".join(str(x) for x in keywords)
         new_array.append(P)
    df['text'] = new_array
    return df['text']
</code></pre>

<p>I want to pass <code>text_preprocessing()</code> into another function <code>tf_idf()</code> which gives feature matrix what I essentially did as:-</p>

<pre><code>def tf_idf():
    tfidf = TfidfVectorizer()
    feature_array = tfidf.fit_transform(text_preprocessing)
    keywords_data=pd.DataFrame(feature_array.toarray(), columns=tfidf.get_feature_names())
    return keywords_data
</code></pre>

<p>I got an error as <code>TypeError: 'function' object is not iterable</code></p>
",Preprocessing of the text & Tokenization,pas user defined function inside tfidfvectorizer fit transform function text preprocessing simply removing stopwords want pas another function give feature matrix essentially got error
Is there a good stemmer for Hebrew?,"<p>I am looking for a good stemmer for Hebrew - I found nothing at all using Google...</p>

<p>On the <a href=""http://code972.com/blog/2010/06/19-finding-hebrew-lemmas-hebmorph-part-2"" rel=""noreferrer"">HebMorph site</a> it says that:</p>

<p><code>Stem and Lemma originally have different meanings, but for Semitic languages they seem to be used interchangeably.</code></p>

<p>Does that mean that for NLP purposes, I could use lemmas instead of stems? Keeping in mind that: <code>Stemmers are much simpler, smaller and usually faster then lemmatizers, and for many applications their results are good enough. Using a lemmatizer for that is a waste of resources.</code> (<a href=""https://stackoverflow.com/questions/17317418/stemmers-vs-lemmatizers"">source</a> )</p>

<p>Thank you.</p>
",Preprocessing of the text & Tokenization,good stemmer hebrew looking good stemmer hebrew found nothing using google hebmorph site say doe mean nlp purpose could use lemma instead stem keeping mind href p thank
Only words or numbers re pattern. Tokenize with CountVectorizer,"<p>I'm using python <code>CountVectorizer</code> to tokenize sentences and at the same time filter non-existant words like ""1s2"".</p>

<p>Which re pattern should I use to select only English words and numbers? The following regex pattern gets me pretty close: </p>

<pre><code>pattern = '(?u)(?:\\b[a-zA-Z]+\\b)*(?:\\b[\d]+\\b)*'

vectorizer = CountVectorizer(ngram_range=(1, 1),
                             stop_words=None,
                             token_pattern=pattern)
tokenize = vectorizer.build_tokenizer()

tokenize('this is a test test1 and 12.')

['this', '', 'is', '', 'a', '', 'test', '', '', '', '',
 '', '', '', '', 'and', '', '12', '', '']
</code></pre>

<p>but I can't understand why it gives me so many empty list items (<code>''</code>).</p>

<p>Also, how can I keep the punctuation? In end I would like to result like this:</p>

<pre><code>tokenize('this is a test test1 and 12.')

['this','is','a','test','and','12','.']
</code></pre>
",Preprocessing of the text & Tokenization,word number pattern tokenize countvectorizer using python tokenize sentence time filter non existant word like pattern use select english word number following regex pattern get pretty close understand give many empty list item also keep punctuation end would like result like
R - NLP - text cleaning,"<p>I am new to text mining and, currently, I stuck with this kind of pattern </p>

<pre><code>pattern = c(
    ""&lt;f0&gt;&lt;U+009F&gt;&lt;U+0098&gt;&lt;U+00AD&gt;"", 
    ""&lt;f0&gt;&lt;U+009F&gt;&lt;U+0099&gt;&lt;U+008F&gt;"",
    ""&lt;f0&gt;&lt;U+009F&gt;&lt;U+008F&gt;&lt;U+00BF&gt; "",
    ""&lt;f0&gt;&lt;U+009F&gt;&lt;U+0098&gt;&lt;U+0082&gt;"", 
    "" &lt;f0&gt;&lt;U+009F&gt;&lt;U+00A4&gt;&lt;U+00B7&gt;"",
    ""  &lt;f0&gt;&lt;U+009F&gt;&lt;U+008F&gt;&lt;U+00BD&gt;&lt;U+200D&gt;&lt;U+2640&gt;&lt;U+FE0F&gt;\r\nBody"",
    "" &lt;f0&gt;&lt;U+009F&gt;&lt;U+00A4&gt;&lt;U+00A3&gt;"", 
    "" &lt;f0&gt;&lt;U+009F&gt;&lt;U+0099&gt;&lt;U+0084&gt; "", 
    ""  &lt;f0&gt;&lt;U+009F&gt;&lt;U+0099&gt;&lt;U+0084&gt;"",
    ""  &lt;f0&gt;&lt;U+009F&gt;&lt;U+0099&gt;&lt;U+0083&gt;"",
      ""&lt;f0&gt;&lt;U+009F&gt;&lt;U+0098&gt;&lt;U+00B4&gt;"",
     ""Hello"")
</code></pre>

<p>I would like to receive only pattern = ""Hello"" and exclude all the other text.</p>

<p>I tried the following but I failed immediately: </p>

<pre><code>gsub(c, ""&lt;f0&gt;&lt;U+00F&gt;&lt;U+[0-9]&gt;&lt;U+[a-zA-Z0-9]&gt;*, replacement = """")
</code></pre>

<p>So, I tried to break it down:</p>

<pre><code>a = gsub(c, pattern = ""&lt;f0&gt;"", replacement = """")
</code></pre>

<p>->result <code>&lt;fo&gt;</code> drops, so it is a good sign but when I do the next step</p>

<pre><code>gsub(a, pattern = ""&lt;U+009F&gt;"", replacement = """")
</code></pre>

<p>->result: <code>&lt;U+009F&gt;</code> remains.
Do you have some ideas?
I appreciate any kind of suggestions!
Thanks in advance!</p>
",Preprocessing of the text & Tokenization,r nlp text cleaning new text mining currently stuck kind pattern would like receive pattern hello exclude text tried following failed immediately tried break result drop good sign next step result remains idea appreciate kind suggestion thanks advance
How to extend the stopword list from NLTK and remove stop words with the extended list?,"<p>I have tried two ways of removing stopwords, both of which I run into issues:</p>

<p>Method 1:</p>

<pre><code>cachedStopWords = stopwords.words(""english"")
words_to_remove = """"""with some your just have from it's /via &amp;amp; that they your there this into providing would can't""""""
remove = tu.removal_set(words_to_remove, query)
remove2 = tu.removal_set(cachedStopWords, query)
</code></pre>

<p>In this case, only the first remove function works. remove2 doesn't work.</p>

<p>Method 2:</p>

<pre><code>lines = tu.lines_cleanup([sentence for sentence in sentence_list], remove=remove)
words = '\n'.join(lines).split()
print words # list of words
</code></pre>

<p>output looks like this <code>[""Hello"", ""Good"", ""day""]</code></p>

<p>I try to remove stopwords from words. This is my code:</p>

<pre><code>for word in words:
    if word in cachedStopwords:
        continue
    else:
        new_words='\n'.join(word)

print new_words
</code></pre>

<p>The output looks like this:</p>

<pre><code>H
e
l
l
o
</code></pre>

<p>Cant figure out what is wrong with the above 2 methods. Please advice.</p>
",Preprocessing of the text & Tokenization,extend stopword list nltk remove stop word extended list tried two way removing stopwords run issue method case first remove function work remove work method output look like try remove stopwords word code output look like cant figure wrong method please advice
"Finding duplicates in multiple HUGE lists in Python (compare 2, 3, 4, 5 lists)","<p>So I'm currently working on 5 dictionaries and very possibly more in the futur, with at least 257000+ entries each. Consider them as 5 huge text files(size: 10-20 Mb) with, say, 10-30 characters each line would be fine.
An example of an entry be like:</p>

<pre><code>abaissements volontaires,abaissement volontaire.N+NA:mp
</code></pre>

<p>My mission is to find out duplicate words between/among different dictionaires.
So first of all, I have to process the file to get, for example, only <strong><em>abaissements volontaires</em></strong> from the example. After this part, my idea is to have a list that contains elements like:</p>

<pre><code>dict_word_list = [[dict_A, [word1, word2, ...]], [dict_B, [word1, word2, ...]]]
</code></pre>

<p>The choice of lists over dicts is simply because dicts are unordered in Python and I have to know the name of the corresponding dictionary of each word list, so I put the corresponding dictionary names in element 0 of each list.</p>

<p>My question is how to find out duplicates between/among these huge lists and at the same time keep dictionary names? 
I tried <strong><em>if not in list</em></strong> but due to the file size and a very old processor(an intel core i3 in an old shabby laptop at work and I cannot use my own laptop due to confidentiality issues) , the program simply bugs there.</p>

<p>Maybe <strong><em>set</em></strong> would be a solution, but how do I shuffle the comparison? I would like to have results like:</p>

<blockquote>
  <p>Duplicates dict_A, dict_B: [word1, word2, word3, ...]</p>
  
  <p>Duplicates dict_B, dict_C: [word1, word2, word3, ...]</p>
  
  <p>Duplicates dict_A, dict_B, dict_C: [word1, word2, word3, ...]</p>
</blockquote>
",Preprocessing of the text & Tokenization,finding duplicate multiple huge list python compare list currently working dictionary possibly futur least entry consider huge text file size mb say character line would fine example entry like mission find duplicate word among different dictionaires first process file get example abaissements volontaires example part idea list contains element like choice list dicts simply dicts unordered python know name corresponding dictionary word list put corresponding dictionary name element list question find duplicate among huge list time keep dictionary name tried list due file size old processor intel core old shabby laptop work use laptop due confidentiality issue program simply bug maybe set would solution shuffle comparison would like result like duplicate dict dict b word word word duplicate dict b dict c word word word duplicate dict dict b dict c word word word
"Chris Manning is amazing&quot; and labels &quot;PER PER O O&quot; would become ([[1,9], [2,9], [3,8], [4,8]], [1, 1, 4, 4]). how &quot;chris&quot; is encoded in [1,9]?","<p>how ""chris"" is encoded in [1,9] ?  Below is code link</p>

<p><a href=""https://github.com/roypan/CS224n/blob/master/assignment3/q2_rnn.py"" rel=""nofollow noreferrer"">https://github.com/roypan/CS224n/blob/master/assignment3/q2_rnn.py</a></p>

<p>data: is a list of (sentence, labels) tuples. @sentence is a list
            containing the words in the sentence and @label is a list of
            output labels. Each word is itself a list of
            @n_features features. For example, the sentence ""Chris
            Manning is amazing"" and labels ""PER PER O O"" would become
            ([[1,9], [2,9], [3,8], [4,8]], [1, 1, 4, 4]). Here ""Chris""
            the word has been featurized as ""[1, 9]"", and ""[1, 1, 4, 4]""
            is the list of labels.</p>
",Preprocessing of the text & Tokenization,chris manning amazing label per per would become chris encoded chris encoded code link data list sentence label tuples sentence list containing word sentence label list output label word list n feature feature example sentence chris manning amazing label per per would become chris word ha featurized list label
Regex for Parsing JSON,"<p>I have a column of data I'm reading in Tableau directly from Redshift. This column contains a JSON object. It looks like this:</p>

<pre><code>{""Age"": 58, ""City"": ""Wisconsin Rapids"", ""Race"": ""Other"", ""State"": ""Wisconsin"", ""Gender"": ""Female"", ""Country"": ""United States""}
</code></pre>

<p>I wish to extract this data by generating a column with a calculated field for each data point of interest using Tableau's REGEXP_EXTRACT function. I.e. an Age column, a City column etc.</p>

<p>How do I write a line of regular expressions to get the value of 58 for Age, Wisoncsin Rapids for City, etc.</p>

<p>Thanks!</p>
",Preprocessing of the text & Tokenization,regex parsing json column data reading tableau directly redshift column contains json object look like wish extract data generating column calculated field data point interest using tableau regexp extract function e age column city column etc write line regular expression get value age wisoncsin rapid city etc thanks
What does one do with the POS labelled as &#39;Conjunction&#39; while WordNet lemmatization?,"<p>Simplified tags after the POS tagging by NLTK have been calculated.</p>

<pre><code>simplified = [(word, simplify_wsj_tag(tag)) for word, tag in posTagged]
print(simplifiedTags)
#[('And', 'CONJ'), ('now', 'ADV'), ('for', 'ADP'), ('something', 'NOUN'), ('completely', 'ADV'), ('different', 'ADJ')]
</code></pre>

<p>Now the lemma for each word has to be found. Each of these, except conjuction, can be mapped to a wordnet POS class - noun, adjective, adverb, verb. What is supposed to be done with the words labelled as Conjuction? Which is the closest relative of conjuction amongst all the four classes? Or are they supposed to be dropped from the sentence all together? </p>
",Preprocessing of the text & Tokenization,doe one po labelled conjunction wordnet lemmatization simplified tag po tagging nltk calculated lemma word ha found except conjuction mapped wordnet po class noun adjective adverb verb supposed done word labelled conjuction closest relative conjuction amongst four class supposed dropped sentence together
Python: Tokenizing with phrases,"<p>I have blocks of text I want to tokenize, but I don't want to tokenize on whitespace and punctuation, as seems to be the standard with tools like <a href=""http://nltk.googlecode.com/svn/trunk/doc/howto/tokenize.html"" rel=""noreferrer"">NLTK</a>. There are particular phrases that I want to be tokenized as a single token, instead of the regular tokenization.  </p>

<p>For example, given the sentence ""The West Wing is an American television serial drama created by Aaron Sorkin that was originally broadcast on NBC from September 22, 1999 to May 14, 2006,"" and adding the phrase to the tokenizer ""<a href=""http://en.wikipedia.org/wiki/The_West_Wing"" rel=""noreferrer"">the west wing</a>,"" the resulting tokens would be:  </p>

<ul>
<li>the west wing</li>
<li>is</li>
<li>an</li>
<li>american </li>
<li>...</li>
</ul>

<p>What's the best way to accomplish this?  I'd prefer to stay within the bounds of tools like NLTK.</p>
",Preprocessing of the text & Tokenization,python tokenizing phrase block text want tokenize want tokenize whitespace punctuation seems standard tool like nltk particular phrase want tokenized single token instead regular tokenization example given sentence west wing american television serial drama created aaron sorkin wa originally broadcast nbc september may adding phrase tokenizer west wing resulting token would west wing american best way accomplish prefer stay within bound tool like nltk
Hierarchical training for doc2vec: how would assigning same labels to sentences of the same document work?,"<p>What is the effect of assigning the same label to a bunch of sentences in doc2vec? I have a collection of documents that I want to learn vectors using gensim for a ""file"" classification task where file refers to a collection of documents for a given ID. I have several ways of labeling in mind and I want to know what would be the difference between them and which is the best - </p>

<ul>
<li><p>Take a document d1, assign label <code>doc1</code> to the tags and train. Repeat for others</p></li>
<li><p>Take a document d1, assign label <code>doc1</code> to the tags. Then tokenize document into sentences and assign label <code>doc1</code> to its tags and then train with both full document and individual sentences. Repeat for others</p></li>
</ul>

<p>For example (ignore that the sentence isn't tokenized) -</p>

<pre><code>Document -  ""It is small. It is rare"" 
TaggedDocument(words=[""It is small. It is rare""], tags=['doc1'])
TaggedDocument(words=[""It is small.""], tags=['doc1'])
TaggedDocument(words=[""It is rare.""], tags=['doc1'])
</code></pre>

<ul>
<li>Similar to above, but also assign a unique label for each sentence along with <code>doc1</code>. The full document has the all the sentence tags along with <code>doc1</code>.</li>
</ul>

<p>Example - </p>

<pre><code>Document -  ""It is small. It is rare"" 
TaggedDocument(words=[""It is small. It is rare""], tags=['doc1', 'doc1_sentence1', 'doc1_sentence2'])
TaggedDocument(words=[""It is small.""], tags=['doc1', 'doc1_sentence1'])
TaggedDocument(words=[""It is rare.""], tags=['doc1', 'doc1_sentence2'])
</code></pre>

<p>I also have some additional categorical tags that I'd be assigning. So what would be the best approach?</p>
",Preprocessing of the text & Tokenization,hierarchical training doc vec would assigning label sentence document work effect assigning label bunch sentence doc vec collection document want learn vector using gensim file classification task file refers collection document given id several way labeling mind want know would difference best take document assign label tag train repeat others take document assign label tag tokenize document sentence assign label tag train full document individual sentence repeat others example ignore sentence tokenized similar also assign unique label sentence along full document ha sentence tag along example also additional categorical tag assigning would best approach
How to get stemmers to recognize Identification and Identifier similarly?,"<p>Why does NLTK's stemmers identify a different stem for Identification and Identifier?
For Identification, both the Snowball and Porter stemmers return identif, but for Identifier, I get identifi. Are there any other stemmers that would be a bit more inclusive of different forms of words? </p>
",Preprocessing of the text & Tokenization,get stemmer recognize identification identifier similarly doe nltk stemmer identify different stem identification identifier identification snowball porter stemmer return identif identifier get identifi stemmer would bit inclusive different form word
Tokenization not working the same for both case.,"<p>I have a document </p>

<pre><code>doc = nlp('x-xxmessage-id:')
</code></pre>

<p>When I want to extract the tokens of this one I get 'x', 'xx', 'message' and 'id', ':'. Everything goes well. 
Then I create a new document </p>

<pre><code>test_doc = nlp('id')
</code></pre>

<p>If I try to extract the tokens of test_doc, I will get 'i' and 'd'. Is there any way to get past this problem? Because I want to get the same token as above and this is creating problems in the text processing. </p>
",Preprocessing of the text & Tokenization,tokenization working case document want extract token one get x xx message id everything go well create new document try extract token test doc get way get past problem want get token creating problem text processing
Convert emoji title to unicode,"<p>I using <a href=""https://github.com/haccer/twint"" rel=""nofollow noreferrer"">Twint</a> to extract tweets resulted from a particular search (that gives me about 100k tweets). 
The problem is that Twint outputs the tweet content with the emoji title and not its specific unicode. This is one example:</p>

<pre><code>@LulapeloBrasil presidente minha eterna gratidão a tudo que senhor fez, faz e fará ao nosso povo. Seguiremos lutando pelos nossos ideais! &lt;Emoji: Heavy red heart&gt;  &lt;Emoji: Flexed biceps (dark skin tone)&gt; #LulaLivre #EusouLula #LulaValeALuta #OcupaSaoBernardo
</code></pre>

<p>This is bad because I want to tokenize the tweet for further analysis (e.g. emoji usage) and a traditional tweet tokenizer (e.g. nltk TweetTokenizer) won't tokenize properly. </p>

<p>Do you have any suggestion about how can I convert these emojis titles to their respective unicode (I'm able to extract the titles only using <code>re</code>)?</p>

<p>Where can I get the data that <a href=""https://emojipedia.org/emojipedia/"" rel=""nofollow noreferrer"">emojepedia</a> uses? Or where can I download a list of all twitter emojis containing their unicode code and titles?</p>
",Preprocessing of the text & Tokenization,convert emoji title unicode using twint extract tweet resulted particular search give k tweet problem twint output tweet content emoji title specific unicode one example bad want tokenize tweet analysis e g emoji usage traditional tweet tokenizer e g nltk tweettokenizer tokenize properly suggestion convert emojis title respective unicode able extract title using get data emojepedia us download list twitter emojis containing unicode code title
String and word manipulation in Python,"<p>Example: </p>

<p>I have a sentence 'Face book is a social networking company', which I want to clean by concatenating 'Face' and 'book' into 'Facebook'. I would like to check and perform this for numerous sentences. Any suggestions on how can I do this?</p>

<p>I thought of something on the lines of this: first tokenzing the sentence and then looping over every word and check if the token (word) after 'face' is 'book' and then delete the two elements and all 'Facebook'.</p>
",Preprocessing of the text & Tokenization,string word manipulation python example sentence face book social networking company want clean concatenating face book facebook would like check perform numerous sentence suggestion thought something line first tokenzing sentence looping every word check token word face book delete two element facebook
Trying to remove large amounts of text from pandas series with a list,"<p>My problem is basically as follows. I have a pandas dataframe, with a column which contains fairly large amounts of text (generally 20 to 200 words). This dataframe is about 600k rows. On top of that I have a list of words, which is about 150k items long, which need to be filtered out of the strings in the dataframe. I am currently using this method to do this:</p>

<pre><code>for word in uncommon_words:
    reports['Report_Clean_Filtered'] = reports['Report_Clean'].str.replace(word, '')
</code></pre>

<p>Where uncommon_words is the list of words and reports is the dataframe.</p>

<p>My estimation is that this will take around 27 hours on my machine. Is there a better (or at least faster) way to do this? I have a very open mind! :)</p>
",Preprocessing of the text & Tokenization,trying remove large amount text panda series list problem basically follows panda dataframe column contains fairly large amount text generally word dataframe k row top list word k item long need filtered string dataframe currently using method uncommon word list word report dataframe estimation take around hour machine better least faster way open mind
How to remove punctuations of a list in Python?,"<p>I am totally a newbie to nltk and python. I have been given a task to extract all the texts from an url. I have tried and able to extract text from a specified url after reading the nltk documentation. My main concern is how to do I remove the special characters (like .,-,"""",'',!,) from the extracted list. The below mentioned code is not working for the text inside the <code>&lt;li&gt;</code> <code>&lt;/li&gt;</code> tag of a html web page. Thus, always dot <code>.</code> is appended to the last word of the text inside the <code>&lt;li&gt;</code> tag. Any help is deeply appreciated. The source code is as follows.</p>

<pre><code>from bs4 import BeautifulSoup 
import urllib.request
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
response = urllib.request.urlopen('https://en.wikipedia.org/wiki/Electronics') 
f=open('corpus.txt','w+')
html = response.read() 
soup = BeautifulSoup(html,""html.parser"") 
text = soup.get_text(strip=True)
tokens = [t for t in text.split()]
clean_tokens = tokens[:] 
sr = stopwords.words('english') 
for token in tokens: 
if token in sr: 
   clean_tokens.remove(token) 
   freq = nltk.FreqDist(clean_tokens) 
   for normalize,val in freq.items(): 
       lemmatizer=WordNetLemmatizer()
       corpus_refi=lemmatizer.lemmatize(str(normalize) + ':' + str(val), pos=""a"")
       corpus_refi=corpus_refi.lower()
       print(corpus_refi)  
</code></pre>
",Preprocessing of the text & Tokenization,remove punctuation list python totally newbie nltk python given task extract text url tried able extract text specified url reading nltk documentation main concern remove special character like extracted list mentioned code working text inside tag html web page thus always dot appended last word text inside tag help deeply appreciated source code follows
Spacy - Tokenize quoted string,"<p>I am using spacy 2.0 and using a quoted string as input.  </p>

<p>Example string</p>

<pre><code>""The quoted text 'AA XX' should be tokenized""
</code></pre>

<p>and expecting to extract </p>

<pre><code>[The, quoted, text, 'AA XX', should, be, tokenized]
</code></pre>

<p>I however get some strange results while experimenting.  Noun chunks and ents looses one of the quote.  </p>

<pre><code>import spacy
nlp = spacy.load('en')
s = ""The quoted text 'AA XX' should be tokenized""
doc = nlp(s)
print([t for t in doc])
print([t for t in doc.noun_chunks])
print([t for t in doc.ents])
</code></pre>

<p>Result</p>

<pre><code>[The, quoted, text, ', AA, XX, ', should, be, tokenized]
[The quoted text 'AA XX]
[AA XX']
</code></pre>

<p>What is the best way to address what I need</p>
",Preprocessing of the text & Tokenization,spacy tokenize quoted string using spacy using quoted string input example string expecting extract however get strange result experimenting noun chunk ents loos one quote result best way address need
"How I can iterate through a bunch of documents and execute spacy&#39;s nlp for each of them, without getting a memory error?","<p>I have 90 documents with around 40 pages each (raw text). I want to tokenize them with spacy.</p>

<pre><code>nlp = spacy.load('de')
tokenized_list = []

for document in doc_collection:
    temp_doc = nlp(document)
    tokenized_list.append(temp_doc)
</code></pre>

<p>It's working for a low number of documents, but if i want to tokenize all, then it gives a ""MemoryError"".</p>

<pre><code>""...site-packages\numpy\core\shape_base.py"", line 234, in vstack
    return _nx.concatenate([atleast_2d(_m) for _m in tup], 0)

MemoryError""
</code></pre>

<p>Does somebody know how I can fix it? </p>

<p>Update:
I can execute it over and over again without changing the documents and it get stuck sometimes in this document sometimes in that - really weird... Does somebody know a similar problem?</p>
",Preprocessing of the text & Tokenization,iterate bunch document execute spacy nlp without getting memory error document around page raw text want tokenize spacy working low number document want tokenize give memoryerror doe somebody know fix update execute without changing document get stuck sometimes document sometimes really weird doe somebody know similar problem
removing stop words from corpus in R is too slow,"<p>I have loaded my corpus which includes 16 text files but it has taken about 2 hours to remove stop words from this corpus.
the total size of the corpus is 31Mb. 
Do you know how can I fix this problem? </p>

<pre><code>multidocMBTI &lt;- Corpus(DirSource(""F:/my master course/Principle of analytics/DATA03""))
multidocMBTI &lt;- tm_map(multidocMBTI, removeWords, stopwords(""english""))
</code></pre>
",Preprocessing of the text & Tokenization,removing stop word corpus r slow loaded corpus includes text file ha taken hour remove stop word corpus total size corpus mb know fix problem
How to reduce memory usage of spaCy lemmatization?,"<p>I am relatively new to spaCy, and I am trying to tokenize + lemmatize 200 MB of text for an NLP project.</p>

<p>I first tried <code>nlp = spacy.load('en')</code> which resulted in a <code>MemoryError</code>.</p>

<p>After doing some research, I tried <code>nlp = spacy.blank('en')</code> to eliminate unnecessary functionality but it is still taking all 16 GB of my memory (and eventually resulting in <code>MemoryError</code>). This seems excessive to me.</p>

<p>What can I do to reduce the memory usage of spaCy or is it not possible? Thank you!</p>

<p>Original code:</p>

<pre><code>nlp = spacy.blank('en')
train['spacy_tokens'] = train['text'].apply(lambda x: nlp(x))

def lemmatize(x):
    intermediate_lemmas = [token.lemma_.lower() for token in x
            if not token.is_punct]
    return [lemma for lemma in intermediate_lemmas
           if lemma not in stop_words
           and lemma != ""-PRON-""
           and lemma != "" ""
           ]

train['lemmas'] = train['spacy_tokens'].apply(lambda x: lemmatize(x))
print(train.head())
</code></pre>
",Preprocessing of the text & Tokenization,reduce memory usage spacy lemmatization relatively new spacy trying tokenize lemmatize mb text nlp project first tried resulted research tried eliminate unnecessary functionality still taking gb memory eventually resulting seems excessive reduce memory usage spacy possible thank original code
chinese tokenizer stanford core nlp,"<p>Can somebody help me use the stanford core nlp to tokenize chinese text in java.
This is my code so far:</p>

<pre><code>File file = new File(""example.txt"");
   file.createNewFile();
   FileWriter fileWriter = new FileWriter(file);
   fileWriter.write(""这是很好"");
   fileWriter.flush();
   fileWriter.close();
   FileReader fileReader = new FileReader(file);

   InputStreamReader isReader = new InputStreamReader(new FileInputStream(file),""UTF-8"");

   CHTBTokenizer chineseTokenizer = new CHTBTokenizer(isReader);

   String nextToken = """";
   while((nextToken = chineseTokenizer.getNext())!=null)
       System.out.println(nextToken);
</code></pre>

<p>But instead of getting 3 seperate tokens I'm getting the whole sentence as a single token.
Can somebody help me out?</p>
",Preprocessing of the text & Tokenization,chinese tokenizer stanford core nlp somebody help use stanford core nlp tokenize chinese text java code far instead getting seperate token getting whole sentence single token somebody help
Handling compound words (2-grams) using NLTK,"<p>I'm trying to identify user similarities by comparing the keywords used in their profile (from a website). For example, <code>Alice = pizza, music, movies</code>, <code>Bob = cooking, guitar, movie</code> and <code>Eve = knitting, running, gym</code>. Ideally, <code>Alice</code> and <code>Bob</code> are the most similar. I put down some simple code to calculate the similarity. To account for possible plural/singular version of the keywords I use something like:</p>

<pre><code>from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
wnl = WordNetLemmatizer()
w1 = [""movies"", ""movie""] 
tokens = [token.lower() for token in word_tokenize("" "".join(w1))]
lemmatized_words = [wnl.lemmatize(token) for token in tokens]
</code></pre>

<p>So that, <code>lemmatized_words = [""movie"", ""movie""]</code>. 
Afterwards, I do some pairwise keywords comparison using <a href=""https://spacy.io/usage/vectors-similarity"" rel=""nofollow noreferrer""><code>spacy</code></a>, such as:</p>

<pre><code>import spacy
nlp = spacy.load('en')
t1 = nlp(u""pizza"")
t2 = nlp(u""food"")
sim = t1.similarity(t2)
</code></pre>

<p>Now, the problem starts when I have to deal with compound  words such as: <code>artificial intelligence</code>, <code>data science</code>, <code>whole food</code>, etc. By tokenizing, I would simply split those words into 2 (e.g. <code>artificial</code> and <code>intelligence</code>), but this would affect my similarity measure. What is (would be) the best approach to take into account those type of words?</p>
",Preprocessing of the text & Tokenization,handling compound word gram using nltk trying identify user similarity comparing keywords used profile website example ideally similar put simple code calculate similarity account possible plural singular version keywords use something like afterwards pairwise keywords comparison using problem start deal compound word etc tokenizing would simply split word e g would affect similarity measure would best approach take account type word
What&#39;s a good measure for classifying text documents?,"<p>I have written an application that measures text importance. It takes a text article, splits it into words, drops stopwords, performs stemming, and counts word-frequency and document-frequency. Word-frequency is a measure that counts how many times the given word appeared in all documents, and document-frequency is a measure that counts how many documents the given word appeared.</p>

<p>Here's an example with two text articles:</p>

<ul>
<li>Article I) ""A fox jumps over another fox.""</li>
<li>Article II) ""A hunter saw a fox.""</li>
</ul>

<p>Article I gets split into words (afters stemming and dropping stopwords): </p>

<ul>
<li>[""fox"", ""jump"", ""another"", ""fox""].</li>
</ul>

<p>Article II gets split into words:</p>

<ul>
<li>[""hunter"", ""see"", ""fox""].</li>
</ul>

<p>These two articles produce the following word-frequency and document-frequency counters:</p>

<ul>
<li><code>fox</code> (word-frequency: 3, document-frequency: 2)</li>
<li><code>jump</code> (word-frequency: 1, document-frequency: 1) </li>
<li><code>another</code> (word-frequency: 1, document-frequency: 1) </li>
<li><code>hunter</code> (word-frequency: 1, document-frequency: 1) </li>
<li><code>see</code> (word-frequency: 1, document-frequency: 1) </li>
</ul>

<p>Given a new text article, how do I measure how similar this article is to previous articles?</p>

<p>I've read about df-idf measure but it doesn't apply here as I'm dropping stopwords, so words like ""a"" and ""the"" don't appear in the counters.</p>

<p>For example, I have a new text article that says ""hunters love foxes"", how do I come up with a measure that says this article is pretty similar to ones previously seen?</p>

<p>Another example, I have a new text article that says ""deer are funny"", then this one is a totally new article and similarity should be 0.</p>

<p>I imagine I somehow need to sum word-frequency and document-frequency counter values but what's a good formula to use?</p>
",Preprocessing of the text & Tokenization,good measure classifying text document written application measure text importance take text article split word drop stopwords performs stemming count word frequency document frequency word frequency measure count many time given word appeared document document frequency measure count many document given word appeared example two text article article fox jump another fox article ii hunter saw fox article get split word afters stemming dropping stopwords fox jump another fox article ii get split word hunter see fox two article produce following word frequency document frequency counter word frequency document frequency word frequency document frequency word frequency document frequency word frequency document frequency word frequency document frequency given new text article measure similar article previous article read df idf measure apply dropping stopwords word like appear counter example new text article say hunter love fox come measure say article pretty similar one previously seen another example new text article say deer funny one totally new article similarity imagine somehow need sum word frequency document frequency counter value good formula use
is there is any stemmer available for indian language,"<p>is there is any implementation of stemmers for indian languages like(hindi,telugu) are available ....</p>
",Preprocessing of the text & Tokenization,stemmer available indian language implementation stemmer indian language like hindi telugu available
"What&#39;s the difference between Stanford Tagger, Parser and CoreNLP?","<p>I'm currently using different tools from Stanford NLP Group and trying to understand the differences between them. It seems to me that somehow they intersect each other, since I can use same features in different tools (e.g. tokenize, and POS-Tag a sentence can be done by Stanford POS-Tagger, Parser and CoreNLP).</p>

<p>I'd like to know what's the actual difference between each tool and in which situations I should use each of them.</p>
",Preprocessing of the text & Tokenization,difference stanford tagger parser corenlp currently using different tool stanford nlp group trying understand difference seems somehow intersect since use feature different tool e g tokenize po tag sentence done stanford po tagger parser corenlp like know actual difference tool situation use
Microsoft Natural Language List: is there an equivalent to &quot;no language&quot; or &quot;raw unicode&quot; or &quot;Language neutral&quot;?,"<p>Is there something equivalent to ""non-language"" or ""raw"" in the Microsoft Natural Language List, which would have the effect of causing the word-breaking (i.e. tokenizing) algorithms to use only the space as the delimiter?</p>

<p>Edit: Or is there a way to tell Microsoft's technology to use the period as a token delimiter <em>only when it is attached to a lexeme</em>?</p>

<p>The specific problem (for us) is that the full-text search in SQL Server is using the period as a delimiter when tokenizing the text. But our text contains meaningful ""non-lexical"" character strings like <code>JC7D.14GR.2345DG</code> which we would like to search for without searching for each chunk <code>(JC7D and 14GR and 2345DG)</code> or <code>(JC7D NEAR 14GR NEAR 2345DG)</code> as that separate-chunk approach can yield false positives when these values appear in a list in close proximity to one another.</p>
",Preprocessing of the text & Tokenization,microsoft natural language list equivalent language raw unicode language neutral something equivalent non language raw microsoft natural language list would effect causing word breaking e tokenizing algorithm use space delimiter edit way tell microsoft technology use period token delimiter attached lexeme specific problem u full text search sql server using period delimiter tokenizing text text contains meaningful non lexical character string like would like search without searching chunk separate chunk approach yield false positive value appear list close proximity one another
why does my nltk.pos_tag tag letter rather than words,"<p>I'm new in nltk. I tried to use nltk.pos_tag to tag words like this:[[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('text', 'NN'), ('for', 'IN'), ('test', 'NN'), ('.', '.')], [('And', 'CC'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('learn', 'VB'), ('how', 'WRB'), ('to', 'TO'), ('use', 'VB'), ('nltk', 'NN'), ('.', '.')]]
I followed the instruction and I found a lot of people have got the right result. my code as follows:
    sentences=""Why TCP is more reliable than UDP ""
    words=nltk.word_tokenize(sentences)
    print(words)
    tags=[]
    for i in range(0,len(words)):
        tags.append(nltk.pos_tag(words[i]))
    print(tags)
however, my result is:</p>

<p>['Why', 'TCP', 'is', 'more', 'reliable', 'than', 'UDP']
[[('W', 'NNP'), ('h', 'NN'), ('y', 'NN')], [('T', 'NNP'), ('C', 'NNP').....</p>

<p>I really wish some genius could help me.</p>
",Preprocessing of the text & Tokenization,doe nltk po tag tag letter rather word new nltk tried use nltk po tag tag word like dt vbz dt text nn test nn cc prp want vbp learn vb use vb nltk nn followed instruction found lot people got right result code follows sentence tcp reliable udp word nltk word tokenize sentence print word tag range len word tag append nltk po tag word print tag however result tcp reliable udp w nnp h nn nn nnp c nnp really wish genius could help
Generating possible sentences from a scrambled list of n-grams (python),"<p>Sample input stream: <code>[ ('t','h'), ('h','e'), ('e', ' '), (' ','f') , ('f','o'), ('o','x'), ('x',' '), (' ','a'), ('a','t'), ('t','e'), ('e', &lt;p&gt;) ]</code></p>

<hr>

<p>Suppose you have a sentence <code>{ABCABA}</code>, where each letter is either a character or word, depending on tokenization.</p>

<p>Then your bag-of-bigrams is <code>{(AB), (BC), (CA), (AB), (BA)}</code>.</p>

<p>From here, I need an algorithm to list all the possible permutations of sentences with the same length as the original sentence, given these bigrams. Here, <code>{ABCABA}</code> (the original sequence) and <code>(ABABCA)</code> are both valid, possible sentences, but <code>{ACBABA}</code> is not. This example is for bigrams, but I also need this to work for any <code>$n$</code>. Any ideas?</p>
",Preprocessing of the text & Tokenization,generating possible sentence scrambled list n gram python sample input stream suppose sentence letter either character word depending tokenization bag bigram need algorithm list possible permutation sentence length original sentence given bigram original sequence valid possible sentence example bigram also need work idea
Giving your own custom tags to tokenize data in nltk?,"<p>While running the below code</p>

<pre><code>from nltk import word_tokenize, pos_tag, ne_chunk


sentence = ""Antacids is given to Jhon,Sodium Bicarbonate is given to Carl,Folic Acid to Jeery  all works at Google "" 
print(ne_chunk(pos_tag(word_tokenize(sentence))))
</code></pre>

<p>i am getting this output</p>

<pre><code>(S
  (GPE Antacids/NNP)
  is/VBZ
  given/VBN
  to/TO
  (PERSON Jhon/NNP)
  ,/,
  (PERSON Sodium/NNP Bicarbonate/NNP)
  is/VBZ
  given/VBN
  to/TO
  (GPE Carl/NNP)
  ,/,
  (PERSON Folic/NNP Acid/NNP)
  to/TO
  (GPE Jeery/NNP)
  all/DT
  works/NNS
  at/IN
  (ORGANIZATION Google/NNP))
</code></pre>

<p>I want to assign the medicines like (Antacid,Sodium, Folic) to same category.</p>

<p>Which library I can use for this purpose ? </p>
",Preprocessing of the text & Tokenization,giving custom tag tokenize data nltk running code getting output want assign medicine like antacid sodium folic category library use purpose
Online clustering of news articles,"<p>Is there a common online algorithm to classify news dynamically? I have a huge data set of news classified by topics. I consider each of that topics a <em>cluster</em>. Now I need to classify breaking news. Probably, I will need to generate new topics, or new <em>clusters</em>, dynamically.</p>

<p>The algorithm I'm using is the following:</p>

<p>1) I go through a group of feeds from news sites and I recognize news links.</p>

<p>2) For each new link, I extract the content using dragnet, and then I tokenize it.</p>

<p>3) I find the vector representation of all the old news and the last one using TfidfVectorizer from sklearn.</p>

<p>4) I find the nearest neighbor in my dataset computing euclidean distance from the last news vector representation and all the vector representations of the old news.</p>

<p>5) If that distance is smaller than a threshold, I put it in the cluster that the neighbor belongs. Otherwise, I create a new <em>cluster</em>, with the breaking news.</p>

<p>Each time a news arrive, I re-fit all the data using a TfidfVectorizer, because new dimensions can be founded. I can't wait to re-fit once per day, because <strong>I need to detect breaking events, which can be related to unknown topics</strong>. Is there a common approach more efficient than the one I am using?</p>
",Preprocessing of the text & Tokenization,online clustering news article common online algorithm classify news dynamically huge data set news classified topic consider topic cluster need classify breaking news probably need generate new topic new cluster dynamically algorithm using following go group feed news site recognize news link new link extract content using dragnet tokenize find vector representation old news last one using tfidfvectorizer sklearn find nearest neighbor dataset computing euclidean distance last news vector representation vector representation old news distance smaller threshold put cluster neighbor belongs otherwise create new cluster breaking news time news arrive fit data using tfidfvectorizer new dimension founded wait fit per day need detect breaking event related unknown topic common approach efficient one using
R: How to create clusters based on row strings,"<p>I m trying to create clusters from data based on the string value of each row. I m using the R langage. What I m calling a ""cluster"" is a big thematic (= family) that can define each keywords. I imagine something autogenearated based on the keyword, maybe by using lemmatization or ngram.</p>

<p>For example both keywords ""cloud services"" and ""the cloud service"" should be in the ""service"" cluster.</p>

<p>Here is my input vector:</p>

<pre><code>keywords_df &lt;- c(""cloud storage"", ""cloud computing"", ""google cloud storage"", ""the cloud service"", 
        ""free cloud storage"", ""what is cloud computing"", ""best cloud storage"",""cloud computing definition"", 
        ""amazon cloud services"", ""cloud service providers"", ""cloud services"", ""google cloud computing"", ""cloud computing services"", ""benefits of cloud computing"")
</code></pre>

<p>Here is the expected output dataframe:</p>

<pre><code>| Keyword                   |  Thematic |
|---------------------------|:---------:|
|cloud storage              |storage  |
|cloud computing            |computing|
|google cloud storage       |storage  |
|the cloud service          |service  |
|free cloud storage         |storage  |
|what is cloud computing    |computing|
|best cloud storage         |storage  |
|cloud computing definition |computing|
|amazon cloud service       |service |
|cloud service providers        |services |
|cloud service              |service |
|google cloud computing     |computing|
|cloud computing services   |service |
|benefits of cloud computing|computing|
</code></pre>

<p>The goal is to clean up the data in the ""keyword"" column and auto extract a kind of lemm or ngram.</p>

<p>Here is what I have done for now :</p>

<ol>
<li><p>Create the ""Thematic"" column based on keyword column:</p>

<pre><code>keywords_df &lt;- mutate(keywords_df,Thematic=Keyword)
keywords_df$Thematic &lt;- as.character(keywords_df$Thematic)
</code></pre></li>
<li><p>Remove Stopwords:</p>

<pre><code>stopwords_list&lt;-(c(""cloud"")) #Remove the main word
stopwords &lt;- stopwords(kind = ""en"")
stopwords &lt;- append(stopwords,stopwords_list)
x  = keywords_df$Thematic        
x  =  removeWords(x,stopwords)
keywords_df$Thematic &lt;- x  
</code></pre></li>
</ol>
",Preprocessing of the text & Tokenization,r create cluster based row string trying create cluster data based string value row using r langage calling cluster big thematic family define keywords imagine something autogenearated based keyword maybe using lemmatization ngram example keywords cloud service cloud service service cluster input vector expected output dataframe goal clean data keyword column auto extract kind lemm ngram done create thematic column based keyword column remove stopwords
How to separate two concatenaded words,"<p>I  have a review dataset and I want to process it using NLP techniques. I did all the preprocessing stages (remove stop words, stemming, etc.). My problem is that there are some words, which are connected to each other and my function doesn't understand those. Here is an example:</p>

<pre><code>Great services. I had a nicemeal and I love it a lot. 
</code></pre>

<p>How can I correct it from <strong>nicemeal</strong> to <strong>nice meal</strong>?</p>
",Preprocessing of the text & Tokenization,separate two concatenaded word review dataset want process using nlp technique preprocessing stage remove stop word stemming etc problem word connected function understand example correct nicemeal nice meal
Tokenize TEI-like text,"<p>I'm trying to use spaCy to tokenize a text document, where named entities are wrapped in XML tags. E.g. <a href=""https://en.wikipedia.org/wiki/Text_Encoding_Initiative"" rel=""nofollow noreferrer"">TEI</a>-like <code>&lt;personName&gt;Harry&lt;/personName&gt; goes to &lt;orgName&gt;Hogwarts&lt;/orgName&gt;</code>.</p>

<pre><code>import spacy

nlp = spacy.load('en')
txt = '&lt;personName&gt;Harry&lt;/personName&gt; goes to &lt;orgName&gt;Hogwarts&lt;/orgName&gt;. &lt;personName&gt;Sally&lt;/personName&gt; lives in &lt;locationName&gt;London&lt;/locationName&gt;.'
doc = nlp(txt)
sents = list(doc.sents)
for i, s in enumerate(doc.sents):
    print(""{}: {}"".format(i, s))
</code></pre>

<p>However, the XML tags cause a sentence split:</p>

<pre><code>0: &lt;personName&gt;
1: Harry&lt;/personName&gt; goes to &lt;orgName&gt;
2: Hogwarts&lt;/orgName&gt;.
3: &lt;personName&gt;
4: Sally&lt;/personName&gt; lives in &lt;
5: locationName&gt;
6: London&lt;/locationName&gt;.
</code></pre>

<p>How can I get only 2 sentences?
I know that spaCy has a support for a <a href=""https://spacy.io/usage/linguistic-features#section-tokenization"" rel=""nofollow noreferrer"">custom tokenizer</a> but since the rest of the text is standard, I'd like to keep using the built-in one or perhaps build on top of it to recognize the XML annotations.</p>
",Preprocessing of the text & Tokenization,tokenize tei like text trying use spacy tokenize text document named entity wrapped xml tag e g tei like however xml tag cause sentence split get sentence know spacy ha support custom tokenizer since rest text standard like keep using built one perhaps build top recognize xml annotation
R - Analyze pronouns,"<p>I have a sentence like this:</p>

<pre><code>sent &lt;- ""She likes long walks on the beach with her dogs.""
</code></pre>

<p>Let's say I tokenize the sentence by word. What NLP tools can I use to get data on the pronouns in this sentence, such as subject (first person, second person, third person) and type (possessive, reflexive, etc.)?</p>
",Preprocessing of the text & Tokenization,r analyze pronoun sentence like let say tokenize sentence word nlp tool use get data pronoun sentence subject first person second person third person type possessive reflexive etc
TypeError: slice indices must be integers or None or have an __index__ method NLP,"<p>I am running an example for NLP, using a stemmer function as a class method.</p>

<pre><code>import nltk

class IndexedText(object):
    def __init__(self, stemmer, text):
        self._text = text 
        self._stemmer = stemmer
        self._index = nltk.Index((self._stem(word), i) for (i, word) in enumerate(text))
    def concordance(self, word, width=40):
        key = self._stem(word)
        wc = width/4 # words of context 
        print (self._index[key])
        for i in self._index[key]:
            lcontext = ' '.join(self._text[i-wc:i]) 
            rcontext = ' '.join(self._text[i:i+wc]) 
            ldisplay = '%*s' % (width, lcontext[-width:]) 
            rdisplay = '%-*s' % (width, rcontext[:width]) 
            print (ldisplay, rdisplay)
    def _stem(self, word):
        return self._stemmer.stem(word).lower()



 porter = nltk.PorterStemmer()

 grail = nltk.corpus.webtext.words('grail.txt')

 text = IndexedText(porter, grail)
</code></pre>

<p>Now i'm using concordance function on word 'lie' as follows :</p>

<pre><code>text.concordance('lie')
</code></pre>

<p>And it gives me the error as below :</p>

<pre><code>TypeError: slice indices must be integers or None or have an __index__ method
</code></pre>

<p>Where as the index['lie'] yields the output as all integers :
[1824, 6451, 7038, 7080, 8450, 13860, 13965, 16684]</p>
",Preprocessing of the text & Tokenization,typeerror slice index must integer none index method nlp running example nlp using stemmer function class method using concordance function word lie follows give error index lie yield output integer
Determine if a list of words is in order in a String in Python?,"<p>I want to write a python function to determine if a list of words exists in order in a string. If it does not, return the longest list of words that are in order in that string.</p>

<p>For example, let's suppose I have this sentence: <code>The boy was walking his big dog down the street.</code> And I have a list of words <code>[boy, was, his, dog, street, the, down]</code>. It is clear that in this case, <code>boy, was</code> and <code>dog, street, the down</code> both appear in succession in the sentence. Thus, my function should return these words in the order they appear, so: <code>boy was</code> and <code>dog down the street</code>. </p>

<p>Does anyone have an idea of how to do this in an efficient way?</p>

<p>Thanks</p>

<p>Edit from comment:
You just need to return the sets of words that appear in the string in order and are also in the list. Of course, they should be as long as they can be. That's why in the example I returned <code>dog down the street</code> since all of those words are in my list and also appear next to each other in the string.</p>
",Preprocessing of the text & Tokenization,determine list word order string python want write python function determine list word exists order string doe return longest list word order string example let suppose sentence list word clear case appear succession sentence thus function return word order appear doe anyone idea efficient way thanks edit comment need return set word appear string order also list course long example returned since word list also appear next string
Find number of words before a string,"<p>In a given text, how can I find the number of words preceding a string?</p>

<p>For example: <code>""how to format this article? put return between paragraphs'</code></p>

<p>I wish to determine that <code>'put return between paragraphs'</code> starts from the 6th word or, in other words, that there are 5 words before it.</p>

<p>Alternatively, is there a way to take into account customized tokens during tokenization such that I can treat <code>'put return between paragraphs'</code> as one token and the rest of individual words as a token each?</p>

<p>Update: This has nothing to do with newline.</p>
",Preprocessing of the text & Tokenization,find number word string given text find number word preceding string example wish determine start th word word word alternatively way take account customized token tokenization treat one token rest individual word token update ha nothing newline
How to parse and extract price-related information from unstructured text data using NLP techniques in Python?,"<p>I want to extract actual and discount prices from unstructured text; the following are some (simulated) samples of that input text:</p>

<blockquote>
  <ol>
  <li>Based on the user rating we recommend to offer the product with a discount of Rs.2500 from an initial price of Rs.10000.This was
  provided to establish long term relationship with the customers.</li>
  <li>For better relationship we recommend to reduce the product price for the customer to Rs.15000 from Rs.20000. Total discount for the
  case is Rs.5000</li>
  <li>As provided previously the cost can be reduced 5% from the initial price Rs.22000. This results in an overall discount of Rs.1100</li>
  </ol>
</blockquote>

<p>My desired output from the above 3 texts is like this:</p>

<pre><code>No ActualPrice  Discount
1 10000         2500
2 20000         5000
3 22000         1100
</code></pre>

<p>I have used some regular expressions to extract the amount, but as the data is in an unstructured manner regexes don't help much as the patterns are not consistent.
Is there any NLP way to handle these kind of scenarios in Python? I have knowledge of basic NLP techniques (tokenization, ngrams, POS tagging, stopword removal, stemming etc.) </p>
",Preprocessing of the text & Tokenization,parse extract price related information unstructured text data using nlp technique python want extract actual discount price unstructured text following simulated sample input text based user rating recommend offer product discount r initial price r wa provided establish long term relationship customer better relationship recommend reduce product price customer r r total discount case r provided previously cost reduced initial price r result overall discount r desired output text like used regular expression extract amount data unstructured manner regexes help much pattern consistent nlp way handle kind scenario python knowledge basic nlp technique tokenization ngrams po tagging stopword removal stemming etc
quanteda kwic regex operation,"<p>Further <strong>edit to original question</strong>.<br>
Question originated by expectation that regexes would work identically or nearly to ""grep"" or to some programming language. This below is what I expected and the fact that it did not happen generated my question (using cygwin):</p>

<pre><code>echo ""regex unusual operation will deport into a different"" &gt; out.txt
grep ""will * dep"" out.txt
""regex unusual operation will deport into a different""
</code></pre>

<p><hr />
<em>Originary question</em><br>
Trying to follow <a href=""https://github.com/kbenoit/ITAUR/blob/master/README.md"" rel=""nofollow noreferrer"">https://github.com/kbenoit/ITAUR/blob/master/README.md</a><br>
to learn Quanteda after seeing that everybody that uses this package finds it very good.<br>
In <a href=""https://github.com/kbenoit/ITAUR/blob/master/1_demo/demo.R"" rel=""nofollow noreferrer"">demo.R</a>, line 22 I find the line:</p>

<pre><code>kwic(immigCorpus, ""deport"", window = 3)  
</code></pre>

<p>Its output is -</p>

<pre><code>[BNP, 157]        The BNP will | deport | all foreigners convicted  
[BNP, 1946]                . 2. | Deport | all illegal immigrants    
[BNP, 1952] immigrants We shall | deport | all illegal immigrants  
[BNP, 2585]  Criminals We shall | deport | all criminal entrants  
</code></pre>

<p>To try/learn the basics I execute</p>

<pre><code>kwic(immigCorpus, ""will *depo"", window = 3, valuetype = ""regex"")
</code></pre>

<p>expecting to get</p>

<pre><code>[BNP, 157]        The BNP will | deport | all foreigners convicted
</code></pre>

<p>but I get:</p>

<pre><code>kwic object with 0 rows
</code></pre>

<p>Similar attempts like</p>

<pre><code>kwic(immigCorpus, "".*will *depo.*"", window = 3, valuetype = ""regex"")
</code></pre>

<p>Get the same result:</p>

<pre><code>kwic object with 0 rows
</code></pre>

<p>Why is that? Tokenization? if so how should I write the regex?</p>

<p>PS Thanks for this great package</p>
",Preprocessing of the text & Tokenization,quanteda kwic regex operation edit original question question originated expectation regexes would work identically nearly grep programming language expected fact happen generated question using cygwin originary question trying follow learn quanteda seeing everybody us package find good demo r line find line output try learn basic execute expecting get get similar attempt like get result tokenization write regex p thanks great package
Spacy Stopwords based on Frequency,"<p>i'm currently searching for an easy solution to add custom stopwords to spacy. These stopwords shall be determined on basis of the absolute frequency of the words in the whole corpus. E.g., in my domain-specific texts, the term ""patient"" should be considered a stopword as it occurs in 70% of all documents.</p>

<p>My first idea was to implement this by the help of pandas apply, but this would require to write my own tokenizing function. Is there a possibility to customize Spacy?</p>

<p>Thank you for any advice</p>
",Preprocessing of the text & Tokenization,spacy stopwords based frequency currently searching easy solution add custom stopwords spacy stopwords shall determined basis absolute frequency word whole corpus e g domain specific text term patient considered stopword occurs document first idea wa implement help panda apply would require write tokenizing function possibility customize spacy thank advice
"Classifier with 3000 labels and 1000000 rows, memory error","<p>Im building a classifier based on a dataset that has 10^6 rows, with about 15 words per row and a total of about 3000 labels. I already did preprocessing(including stemming, splitting etc.), my windows is 64 bit, python 64 bit version is also installed. I have 16 gigs of RAM and a i7 processor. At the bottom you'll find the whole script. </p>

<p>The problem is the memory error and i dont know how to fix it. My bag of words shouldn't be getting that much bigger with a bigger dataset(only a finite number of words), but a matrix of 10^6 x 15000 (i built my bag of words with 15000 max words ) is still really really big. Can anyone help me with the best way to go about this, is there a way to split the bag of words up and use it in a batch wise manner? </p>

<pre><code>import numpy as np
import pandas as pd
import re
from nltk.stem.snowball import SnowballStemmer
from sklearn.preprocessing import LabelEncoder
#from sklearn.feature_extraction import DictVectorizer
from stop_words import get_stop_words

stop_words = get_stop_words('german')

# Importing the dataset
df = pd.read_excel('filename', delimiter = '\t', quoting = 3)
df = df.sample(frac=1).reset_index(drop=True)
#Aanpassen van de kolomnamen voor overzicht
namenKolommen =  list(df.columns.values)
newcols = {
        namenKolommen[2] : 'Short Description 1',
        namenKolommen[3] : 'Short Description 2',
        namenKolommen[4] : 'Type Description',
        namenKolommen[5] : 'Long Description',
        namenKolommen[11] : 'Manufacturer',
        namenKolommen[7] : 'L1',
        }
df.rename(columns = newcols, inplace=True)
print('Start corpus')

AllLabels = df['L1'] 

le1_y = LabelEncoder()
y = le1_y.fit_transform(AllLabels)

Text_input = df['Short Description 1'].fillna('') + ' ' + df['Short Description 2'].fillna('')+ ' ' + df['Type Description'].fillna('') + ' ' + df['Long Description'].fillna('') + ' ' + df['Manufacturer'].fillna('')
Text_input.to_csv('Opgeschoonde lijst.csv')


corpus = []

for i in range(0,len(Text_input)):
    review = re.sub('[^a-zA-Züä0-9()ß-]',' ', str( Text_input[i]))
    #str is tip van internet, blijkbaar klopte datatype niet in die cell
    review = review.lower()
    review = review.split()
    stemmer = SnowballStemmer(""german"")
    review= [stemmer.stem(word) for word in review if not word in set(stop_words)]
    review =  ' '.join(review )
    corpus.append(review)

import pickle
with open('Opgeschoonde_Lijst_Met_Stemming', 'wb') as fp:
    pickle.dump(corpus, fp)

print('Start predicting model')

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features = 15000)
X = cv.fit_transform(corpus).toarray()

#splitten in test en train sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 18)

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)

#Predicting the test results
y_pred = classifier.predict(X_test)
y_pred_strings = le1_y.inverse_transform(y_pred)

#Making the Confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
 accuracy1=np.trace(cm)/len(X_test)
</code></pre>
",Preprocessing of the text & Tokenization,classifier label row memory error im building classifier based dataset ha row word per row total label already preprocessing including stemming splitting etc window bit python bit version also installed gig ram processor bottom find whole script problem memory error dont know fix bag word getting much bigger bigger dataset finite number word matrix x built bag word max word still really really big anyone help best way go way split bag word use batch wise manner
nltk: How to lemmatize taking surrounding words into context?,"<p>The following code prints out <code>leaf</code>:</p>

<pre><code>from nltk.stem.wordnet import WordNetLemmatizer

lem = WordNetLemmatizer()
print(lem.lemmatize('leaves'))
</code></pre>

<p>This may or may not be accurate depending on the surrounding context, e.g. <code>Mary leaves the room</code> vs. <code>Dew drops fall from the leaves</code>. How can I tell NLTK to lemmatize words taking surrounding context into account?</p>
",Preprocessing of the text & Tokenization,nltk lemmatize taking surrounding word context following code print may may accurate depending surrounding context e g v tell nltk lemmatize word taking surrounding context account
Text Processing Tools for German and Spanish Languages,"<p>I'm trying to process text in German and Spanish languages. Working on English text is straight forward because of myriad NLP packages on this language. But it's not easy for other languages. I Found some packages for German text but I don't know which one is more accurate. Also, It's more difficult to find NLP package for Spanish text considering that there are some special characters in this language. Some steps that I need to do on the text are: Sentence Splitting, Tokenizing, Pos tagging and Stemming. In other words, I am looking for something that works on one or both of these two languages in Java.</p>

<p>Any information on this topic is appreciated.. </p>
",Preprocessing of the text & Tokenization,text processing tool german spanish language trying process text german spanish language working english text straight forward myriad nlp package language easy language found package german text know one accurate also difficult find nlp package spanish text considering special character language step need text sentence splitting tokenizing po tagging stemming word looking something work one two language java information topic appreciated
NLP: What is the appropriate way to use engineered features in a sklearn pipeline?,"<p>I'm conducting a text classification task for my first time (twitter sentiment analysis), but I'm unsure of how to incorporate engineered features into my sklearn pipeline.</p>

<p>So far I have tried transformations before outputting a classifier. For example:</p>

<pre><code>model = Pipeline([('t', 'mean_vectorizer'), ('logreg', LogisticRegression())])
</code></pre>

<p>but all of these basic pipelines yield very low scores. So I want to start conducting grid searches and incorporating my own features. </p>

<p>So far, my data set (X_train) is such that the rows are tweets (single string). This is the format handled by mean_vectorizer (and tfidf_vectorizer if I use it). </p>

<p><strong>Incorporating new features</strong></p>

<p>Take for example 1 new feature, a boolean value for whether or not a positive word exists (just a basic example). I would create a (len(X_train), 1)-dimensional array of boolean values corresponding to each tweet.</p>

<p>My ideas:</p>

<ol>
<li><p>After preprocessing the tweets, tokenize them and replace the words with values from a word2index dict. Pad the tweets to equal length, and then concatenate this array with my features. Then pass this into the Pipeline as normal. </p></li>
<li><p>Maybe there is a way in which these features can be passed individually into the Pipeline?</p></li>
<li><p>Maybe the transformations will have issues using an array of integers instead of strings?</p></li>
</ol>

<p><strong>Question</strong></p>

<p>Could someone please advise on the best way to go forward with this using sklearn. </p>

<p>Assume that the data is a list of sentences (training separate from testing) and each sentence is a single string.</p>

<p>I think this will be a really helpful for other people starting out with NLP, so please be as general as possible.</p>
",Preprocessing of the text & Tokenization,nlp appropriate way use engineered feature sklearn pipeline conducting text classification task first time twitter sentiment analysis unsure incorporate engineered feature sklearn pipeline far tried transformation outputting classifier example basic pipeline yield low score want start conducting grid search incorporating feature far data set x train row tweet single string format handled mean vectorizer tfidf vectorizer use incorporating new feature take example new feature boolean value whether positive word exists basic example would create len x train dimensional array boolean value corresponding tweet idea preprocessing tweet tokenize replace word value word index dict pad tweet equal length concatenate array feature pas pipeline normal maybe way feature passed individually pipeline maybe transformation issue using array integer instead string question could someone please advise best way go forward using sklearn assume data list sentence training separate testing sentence single string think really helpful people starting nlp please general possible
Is there method .predict in official python bindings for fastText,"<p>I know there are unofficial bindings with .predict method in python(fasttext, pyfasttext) but they do not work with recent models trained on on official FastText bash tool or do not have all the options. Official python bindings have only load_model(path)and tokenize(text) methods described , which sounds strange as with this you can not do any predictions. Am I missing something here? </p>
",Preprocessing of the text & Tokenization,method predict official python binding fasttext know unofficial binding predict method python fasttext pyfasttext work recent model trained official fasttext bash tool option official python binding load model path tokenize text method described sound strange prediction missing something
spacy: how to save the text preprocessed to save time in the future?,"<p>I have a large collection of papers, and i want to save the result of each</p>

<pre><code>doc = nlp(txt)
</code></pre>

<p>to avoit do the same work in the future and only do the similarity between docs, or for example:</p>

<ul>
<li>now i have 5 texts</li>
<li>i have the similarity bewteen each text (total: 10 similarities)</li>
<li><p>i add a new text and i need to know the similarity bewteen the saved documents and the new document, and try to do:</p>

<p>docToSave = nlp(textToProcess)</p></li>
</ul>

<p>NOTE: When i run the before statement, i get the same input text and i think i could have the text after removed stemming, stopwords, lematization, tokenization, etc.</p>

<p>Its posible save the preprocesed text (doc) to save time in the future (save the preprocessed tex) and load this in the future, to could be posible do:</p>

<pre><code>SELECT data1 FROM docs;
     doc1 = data1.doc
     SELECT data2 FROM docs WHERE id&gt;data1.id:
          doc2 = data2.doc
          sim = doc1.similarity(doc2)
          saveSimilarity(sim)
</code></pre>
",Preprocessing of the text & Tokenization,spacy save text preprocessed save time future large collection paper want save result avoit work future similarity doc example text similarity bewteen text total similarity add new text need know similarity bewteen saved document new document try doctosave nlp texttoprocess note run statement get input text think could text removed stemming stopwords lematization tokenization etc posible save preprocesed text doc save time future save preprocessed tex load future could posible
udpipe_accuracy() always gives the same error &quot; The CoNLL-U line &#39;....&#39; does not contain 10 columns!&quot;,"<p>This is regarding the R package <code>udpipe</code> for NLP. I am using it to tokenize, tag, lemmatize and perform dependency parsing on text files.</p>

<p>I am not sure which template the <code>conllu</code> file is needed for the function</p>

<pre><code>udpipe_accuracy
</code></pre>

<p>I loaded a CSV file of 10 columns but the error persists.</p>

<p>I could not search any questions on SO on this package and also there is no tag of udpipe.</p>
",Preprocessing of the text & Tokenization,udpipe accuracy always give error conll u line doe contain column regarding r package nlp using tokenize tag lemmatize perform dependency parsing text file sure template file needed function loaded csv file column error persists could search question package also tag udpipe
How to access a two dimensional array in Java,"<p>I have a response text from an NLP API which look like this.
[[0.9999999999999995,""doc1"",""doc1""]]
There is no proper documentation for this and so i am finding it difficult to access the real number from the text programmatically without tokenizing the string.
Can someone help me out please.</p>
",Preprocessing of the text & Tokenization,access two dimensional array java response text nlp api look like doc doc proper documentation finding difficult access real number text programmatically without tokenizing string someone help please
How to use Stemmer or Lemmatizer to stem specific word,"<p>I am currently trying to stem a big corpus(aprox. 800k sentences). I've managed to stem only the basic one. The problem now is that I want to stem only a specific word for example this method only applies if the lemma is a substring of the original word. For instance, the suffix for the word apples are apple and 's'. But if not a substring, it will not split it like the word teeth into tooth.</p>

<p>I've also read about lemmatizer WordNet, where we can add a parameter for pos such as verb, noun or adjective. Is there a way that I can apply the method above?</p>

<p>Thanks in advance!</p>
",Preprocessing of the text & Tokenization,use stemmer lemmatizer stem specific word currently trying stem big corpus aprox k sentence managed stem basic one problem want stem specific word example method applies lemma substring original word instance suffix word apple apple substring split like word teeth tooth also read lemmatizer wordnet add parameter po verb noun adjective way apply method thanks advance
What separators does the hive ngram UDF use to tokenize?,"<p>I am performing some sentiment analysis.</p>

<p>I need to count the vocabulary (distinct words) in within a text.</p>

<p>The ngram UDF seems to do a great job at determining the unigrams. I want to know what separators it uses to determine the unigrams/ tokens. This is important if I want to mimic the vocabulary count using the split UDF instead. For example, given the following text (a product review)</p>

<blockquote>
  <p>I was aboslutely shocked to see how much 1 oz really was. At $7.60, I mistakenly assumed it would be a decent sized can. As locally I am able to buy a medium sized tube of wasabi paste for around $3, but never used it fast enough so it would get old. I figured a powder would be better, so I can mix it as I needed it. When I opened the box and dug thru the packing and saw this little little can, I started looking for the hidden cameras ...  thought this HAD to be a joke. Nope .. and it's NOT returnable either. SO I HAVE LEARNED MY LESSON. Please just be aware if you should decide you want this EXPENSIVE wasabi powder.</p>
</blockquote>

<p>The ngram UDG counts 82 unigrams/ tokens</p>

<pre><code>SELECT count(*) FROM 
(SELECT explode(ngrams(sentences(upper(reviewtext)),1,9999999))  
FROM  amazon.Food_review_part_small WHERE asin = 'B0000CNU1X' AND reviewerid ='A1UCAVBNJUZMPR') t;
82
</code></pre>

<p>However, using the split UDF with space, comma, period, hyphen and double quotation marks as separators, there are 85 unigrams/tokens</p>

<pre><code>select  count(distinct(te)) FROM amazon.Food_review_part_small 
lateral view explode(split(upper(reviewtext), '[\\s,.-]|\""')) t as te
WHERE te &lt;&gt; '' AND asin = 'B0000CNU1X' AND reviewerid ='A1UCAVBNJUZMPR';
85
</code></pre>

<p>Of course there is little to no documentation that i can find. Does anyone know what separators the ngram UDF uses to determine unigram tokens ?</p>
",Preprocessing of the text & Tokenization,separator doe hive ngram udf use tokenize performing sentiment analysis need count vocabulary distinct word within text ngram udf seems great job determining unigrams want know separator us determine unigrams token important want mimic vocabulary count using split udf instead example given following text product review wa aboslutely shocked see much really wa mistakenly assumed would decent sized locally able buy medium sized tube wasabi paste around never used fast enough would get old figured powder would better mix needed opened box dug thru packing saw little little started looking hidden camera thought joke nope returnable either learned lesson please aware decide want expensive wasabi powder ngram udg count unigrams token however using split udf space comma period hyphen double quotation mark separator unigrams token course little documentation find doe anyone know separator ngram udf us determine unigram token
Find negation of particular keywords in text,"<p>I am working on information extraction from medical texts (very new to NLP!). At the moment I am interested to find and extract the medications which are mentioned in a predefined list of drugs. For example, consider the text:</p>

<blockquote>
  <p>""John was prescribed aspirin due to hight temperature""</p>
</blockquote>

<p>Thus, given the list of medications (in Python language): </p>

<pre><code>list_of_meds = ['aspirin', 'ibuprofen', 'paracetamol']
</code></pre>

<p>The extracted drug is <code>aspirin</code>. That's fine. </p>

<p>Now consider another case:</p>

<blockquote>
  <p>""John was prescribed ibuprofen, because he could not tolerate paracetamol""</p>
</blockquote>

<p>Now, if I extract the drugs using the list (for example with regular expression), then the extracted drugs are <code>ibuprofen</code> and <code>paracetamol</code>.</p>

<p><strong>QUESTION</strong> How to separate actually prescribed and untolerated drugs? Is there a way to label prescribed (used) and other mentioned drugs?</p>
",Preprocessing of the text & Tokenization,find negation particular keywords text working information extraction medical text new nlp moment interested find extract medication mentioned predefined list drug example consider text john wa prescribed aspirin due hight temperature thus given list medication python language extracted drug fine consider another case john wa prescribed ibuprofen could tolerate paracetamol extract drug using list example regular expression extracted drug question separate actually prescribed untolerated drug way label prescribed used mentioned drug
When the stop word removal process is executed in sklearn TfidfVectorizer?,"<p>If I pass a list of custom stopwords to <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer""><code>TfidfVectorizer</code></a>, when will the stopwords be removed exactly? According to <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">the documentation</a>:</p>

<blockquote>
  <p><strong>stop_words</strong> : <code>string</code> {‘english’}, <code>list</code>, or <code>None</code> (default)</p>
  
  <p>...</p>
  
  <p>If a list, that list is assumed to contain stop words, all of which
  will be removed from the resulting tokens. Only applies if <code>analyzer == 'word'</code>.</p>
</blockquote>

<p>so it seems that the process happens <em>after</em> the tokenization, am I right? The doubt arises because if the tokenization also involves stemming, I think there is the risk to erroneously skip (not remove) a stopword because, after stemming, it is not recognized anymore.</p>
",Preprocessing of the text & Tokenization,stop word removal process executed sklearn tfidfvectorizer pas list custom stopwords stopwords removed exactly according documentation stop word english default list list assumed contain stop word removed resulting token applies seems process happens tokenization right doubt arises tokenization also involves stemming think risk skip remove stopword stemming recognized anymore
Convert the word &quot; I&#39;m&quot; to &quot;I am&quot; using python,"<p>Convert the word <code>"" I'm""</code> to <code>""I am""</code> and similarly <code>""How're""</code> to <code>""How are""</code> using python. I used lemmatization for this but did not get adequate results. I used the following code:</p>

<pre><code>sen=""You are great, My Lord. I'm studying with co-workers. How're you?"" 
all_words=regexp_tokenize(sen, ""[\w']+"")
lemmatiser = WordNetLemmatizer()
all_words_lem=[]
for i in all_words:
x=lemmatiser.lemmatize(i, pos=""v"")
all_words_lem.append(x)
</code></pre>
",Preprocessing of the text & Tokenization,convert word using python convert word similarly using python used lemmatization get adequate result used following code
Scikit - TF-IDF empty vocabulary,"<p>I have to calculate the distance/similarity of two or more texts. Some texts are genuinely really small or do not form proper english words etc, ""A1024515"". This means that it should accept every single word in the list. </p>

<p>As a test case, I have used the following list as a corpus of words. </p>

<pre><code>words= ['A', 'A', 'A']

vect = TfidfVectorizer(min_df =0)
dtm = vect.fit_transform(words)
df_tf_idf = pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names())
</code></pre>

<p>However, I get the following error </p>

<pre><code>ValueError: empty vocabulary; perhaps the documents only contain stop words
</code></pre>

<p><strong>How can I ensure that the list is accepted as possible words and ensure stop words are not removed from the corpus?</strong></p>
",Preprocessing of the text & Tokenization,scikit tf idf empty vocabulary calculate distance similarity two text text genuinely really small form proper english word etc mean accept every single word list test case used following list corpus word however get following error ensure list accepted possible word ensure stop word removed corpus
Multi Classify into categories based on Email Subject and Body in python,"<p>My dataset as like below.</p>

<p><a href=""https://i.sstatic.net/fRPoM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fRPoM.png"" alt=""enter image description here""></a></p>

<p>Subject column refers to Email SUbject and Problem description and Problem details column refers to Email body. </p>

<p>Based on both subject and emaail body keywords, i need to classify to which Queue it should belong to. </p>

<p>Previous queue column consists of 25+ different categories. </p>

<p>My dataframe shape is of (60697, 4).</p>

<p>Please advise on the approach i need to follow to classify. Which ML models i need to use to train the data and test the data.</p>

<p>I know a bit to use natural language tokenization concepts. </p>

<p>Classification is more like gmail inbox classification: Primary, Social and Promotions. However, here I have to categorize into 25+.</p>
",Preprocessing of the text & Tokenization,multi classify category based email subject body python dataset like subject column refers email subject problem description problem detail column refers email body based subject emaail body keywords need classify queue belong previous queue column consists different category dataframe shape please advise approach need follow classify ml model need use train data test data know bit use natural language tokenization concept classification like gmail inbox classification primary social however categorize
"You must feed a value for placeholder tensor &#39;Placeholder&#39; with dtype float and shape [?,48,50]","<p>I am trying to implement an LSTM in Tensorflow, but I have problems with the imput shape.</p>

<p>I have a corpus, I tokenize it and remove the punctuation, then I make sequences in this way:</p>

<pre><code>length = 50 + 1
sequences = list()
for i in range(length, len(tokens)):
    # select sequence of tokens
    seq = tokens[i-length:i]
    # convert into a line
    line = ' '.join(seq)
    # store
    sequences.append(line)
print('Total Sequences: %d' % len(sequences))
</code></pre>

<p>I then divide the sequences in train and test like this:</p>

<pre><code>sequences = array(sequences)
print(len(sequences))
print(sequences)
sequences_train = sequences[:10000] #17314
sequences_test = sequences[10000:12000]
X_train, y_train = sequences_train[:,:-1], sequences_train[:,-1]
X_test, y_test = sequences_test[:,:-1], sequences_test[:,-1]
y = to_categorical(y_train, num_classes=vocab_size)
</code></pre>

<p>Then I create the Neural Network:</p>

<pre><code>num_input = 50
timesteps = 48 
num_hidden = 128 # hidden layer num of features
num_classes = 48 
num_features = X_train.shape[1]
#num_classes = 1


# tf Graph input
X = tf.placeholder(tf.float32, [None, timesteps, num_input])
Y = tf.placeholder(tf.float32, [None, num_classes])

# Define weights
weights = {
'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))
}
biases = {
'out': tf.Variable(tf.random_normal([num_classes]))
}

def RNN(x, weights, biases):

    # Prepare data shape to match `rnn` function requirements
    # Current data input shape: (batch_size, timesteps, n_input)

    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)
    x = tf.unstack(x, timesteps, 1)


    # Define a lstm cell with tensorflow
    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)

    # Get lstm cell output
    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)

    # Linear activation, using rnn inner loop last output
    return tf.matmul(outputs[-1], weights['out']) + biases['out']
logits = RNN(X, weights, biases)
prediction = tf.nn.softmax(logits)

# Define loss and optimizer
loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
train_op = optimizer.minimize(loss_op)

# Evaluate model (with test logits, for dropout to be disabled)
correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

# Initialize the variables (i.e. assign their default value)
init = tf.global_variables_initializer()

# network params 
batch_size = 48 
max_epochs = 30 
display_step = 1
num_epochs = 0
epoch = 0

# batch sampling
np.random.seed(0)
train_indices = np.arange(len(X_train))

# lists 
train_cost, val_cost, pred_undrsamp = [],[],[]
y_out_train, train_batch_x, y_batch_idx, y_out_training = [],[],[],[]

# restricting memory usage, TensorFlow is greedy and will use all memory otherwise
gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)

with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:    
    try:
        sess.run(tf.global_variables_initializer())
        print(""Starting...\n"")
        print(str(""---------------"")*7)

        # total batches
        total_batch = int(X_train.shape[0]/batch_size)
        for epoch in range(max_epochs):

            # shuffling
            np.random.shuffle(train_indices)

            # loop over all batches            
            for i in range(total_batch):

                batch_idx = train_indices[batch_size*i:batch_size*(i+1)]
                batch_xs = X_train[batch_idx]
                batch_ys = y_train[batch_idx]
                batch_xs = batch_xs.reshape((1, timesteps, num_input))
                batch_ys = batch_ys.reshape((1, 48))

                feed_dict_train = {X: batch_xs, Y: batch_ys}
                fetches_train = [train_op, loss_op, prediction]
                #print(X.shape, batch_xs.shape, Y.shape, batch_ys.shape)

                # running the train_op
                res = sess.run(fetches=fetches_train, feed_dict=feed_dict_train)


                train_cost += [res[1]]   
                y_out_train = list(res[2])

                # storing for evaluation 
                y_batch_idx += list(batch_ys)
                y_out_training += list(y_out_train)

            # compute validation loss and accuracy
            if epoch % display_step == 0:
            num_epochs += 1

                ### VALIDATING ###
                # deciding which parts to fetch
                fetches_test = [loss_op, prediction]

                #X_test = X_test.reshape((1, timesteps, num_input))
                #y_test = y_test.reshape((1, 48))

                # what to feed our accuracy op
                feed_dict_valid = {X: X_test, Y: y_test}
                print(X.shape, X_test.shape, Y.shape, y_test.shape)

                # running the validation
                res_t = sess.run(fetches_test, feed_dict=feed_dict_valid)

                # storing for evaluation 
                val_cost += [res_t[0]]
                pred_undrsamp = list(res_t[1]) # y_out validation


                print(""Epoch %i, Train cost: %0.3f,\t Val cost: %0.3f, Val ROC curve (area = %0.3f)\t"" % ((epoch+1), train_cost[-1],val_cost[-1], roc_auc_score(y_test, pred_undrsamp)))

        print(str(""---------------"")*7, ""\n\n"",""Optimization Finished!"")
        print(""\n"",""Total epochs: %i\t Final validation ROC_AUC: %0.3f\t""% \
              ((epoch+1),roc_auc_score(y_test, pred_undrsamp)),""\n"")
        print(get_plot_ROC(y_test, pred_undrsamp, 'Feedforward NN using RandomUnderSampling'))



    except KeyboardInterrupt:
        print('KeyboardInterrupt')

print('Done')
</code></pre>

<p>I have two problems: if I don't test during the trainig, my train_cost is huge, but the program runs, otherwise, if I test the network (so I don't comment out this:</p>

<pre><code>fetches_test = [loss_op, prediction]

#X_test = X_test.reshape((1, timesteps, num_input))
#y_test = y_test.reshape((1, 48))


feed_dict_valid = {X: X_test, Y: y_test}

res_t = sess.run(fetches_test, feed_dict=feed_dict_valid)

# storing for evaluation 
val_cost += [res_t[0]]
pred_undrsamp = list(res_t[1]) # y_out validation
</code></pre>

<p>I get this error:</p>

<pre><code>InvalidArgumentError                      Traceback (most recent call last)
~\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1326     try:
-&gt; 1327       return fn(*args)
   1328     except errors.OpError as e:

~\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1305                                    feed_dict, fetch_list, target_list,
-&gt; 1306                                    status, run_metadata)
   1307 

~\Anaconda3\lib\contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---&gt; 66                 next(self.gen)
     67             except StopIteration:

 ~\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py in raise_exception_on_not_ok_status()
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--&gt; 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:

InvalidArgumentError: You must feed a value for placeholder tensor 
'Placeholder' with dtype float and shape [?,48,50]
     [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[?,48,50], 
_device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-24-018940a98f31&gt; in &lt;module&gt;()
     50 
     51                 # running the train_op
 ---&gt; 52                 res = sess.run(fetches=fetches_train, 
feed_dict=feed_dict_train)
     53 
     54                 # storing cross entropy, predictions (second fetch 
argument, so index=1)
</code></pre>
",Preprocessing of the text & Tokenization,must feed value placeholder tensor placeholder dtype float shape trying implement lstm tensorflow problem imput shape corpus tokenize remove punctuation make sequence way divide sequence train test like create neural network two problem test trainig train cost huge program run otherwise test network comment get error
"You must feed a value for placeholder tensor &#39;Placeholder&#39; with dtype float and shape [?,48,50]","<p>I am trying to implement an LSTM in Tensorflow, but I have problems with the imput shape.</p>

<p>I have a corpus, I tokenize it and remove the punctuation, then I make sequences in this way:</p>

<pre><code>length = 50 + 1
sequences = list()
for i in range(length, len(tokens)):
    # select sequence of tokens
    seq = tokens[i-length:i]
    # convert into a line
    line = ' '.join(seq)
    # store
    sequences.append(line)
print('Total Sequences: %d' % len(sequences))
</code></pre>

<p>I then divide the sequences in train and test like this:</p>

<pre><code>sequences = array(sequences)
print(len(sequences))
print(sequences)
sequences_train = sequences[:10000] #17314
sequences_test = sequences[10000:12000]
X_train, y_train = sequences_train[:,:-1], sequences_train[:,-1]
X_test, y_test = sequences_test[:,:-1], sequences_test[:,-1]
y = to_categorical(y_train, num_classes=vocab_size)
</code></pre>

<p>Then I create the Neural Network:</p>

<pre><code>num_input = 50
timesteps = 48 
num_hidden = 128 # hidden layer num of features
num_classes = 48 
num_features = X_train.shape[1]
#num_classes = 1


# tf Graph input
X = tf.placeholder(tf.float32, [None, timesteps, num_input])
Y = tf.placeholder(tf.float32, [None, num_classes])

# Define weights
weights = {
'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))
}
biases = {
'out': tf.Variable(tf.random_normal([num_classes]))
}

def RNN(x, weights, biases):

    # Prepare data shape to match `rnn` function requirements
    # Current data input shape: (batch_size, timesteps, n_input)

    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)
    x = tf.unstack(x, timesteps, 1)


    # Define a lstm cell with tensorflow
    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)

    # Get lstm cell output
    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)

    # Linear activation, using rnn inner loop last output
    return tf.matmul(outputs[-1], weights['out']) + biases['out']
logits = RNN(X, weights, biases)
prediction = tf.nn.softmax(logits)

# Define loss and optimizer
loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
train_op = optimizer.minimize(loss_op)

# Evaluate model (with test logits, for dropout to be disabled)
correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

# Initialize the variables (i.e. assign their default value)
init = tf.global_variables_initializer()

# network params 
batch_size = 48 
max_epochs = 30 
display_step = 1
num_epochs = 0
epoch = 0

# batch sampling
np.random.seed(0)
train_indices = np.arange(len(X_train))

# lists 
train_cost, val_cost, pred_undrsamp = [],[],[]
y_out_train, train_batch_x, y_batch_idx, y_out_training = [],[],[],[]

# restricting memory usage, TensorFlow is greedy and will use all memory otherwise
gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)

with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:    
    try:
        sess.run(tf.global_variables_initializer())
        print(""Starting...\n"")
        print(str(""---------------"")*7)

        # total batches
        total_batch = int(X_train.shape[0]/batch_size)
        for epoch in range(max_epochs):

            # shuffling
            np.random.shuffle(train_indices)

            # loop over all batches            
            for i in range(total_batch):

                batch_idx = train_indices[batch_size*i:batch_size*(i+1)]
                batch_xs = X_train[batch_idx]
                batch_ys = y_train[batch_idx]
                batch_xs = batch_xs.reshape((1, timesteps, num_input))
                batch_ys = batch_ys.reshape((1, 48))

                feed_dict_train = {X: batch_xs, Y: batch_ys}
                fetches_train = [train_op, loss_op, prediction]
                #print(X.shape, batch_xs.shape, Y.shape, batch_ys.shape)

                # running the train_op
                res = sess.run(fetches=fetches_train, feed_dict=feed_dict_train)


                train_cost += [res[1]]   
                y_out_train = list(res[2])

                # storing for evaluation 
                y_batch_idx += list(batch_ys)
                y_out_training += list(y_out_train)

            # compute validation loss and accuracy
            if epoch % display_step == 0:
            num_epochs += 1

                ### VALIDATING ###
                # deciding which parts to fetch
                fetches_test = [loss_op, prediction]

                #X_test = X_test.reshape((1, timesteps, num_input))
                #y_test = y_test.reshape((1, 48))

                # what to feed our accuracy op
                feed_dict_valid = {X: X_test, Y: y_test}
                print(X.shape, X_test.shape, Y.shape, y_test.shape)

                # running the validation
                res_t = sess.run(fetches_test, feed_dict=feed_dict_valid)

                # storing for evaluation 
                val_cost += [res_t[0]]
                pred_undrsamp = list(res_t[1]) # y_out validation


                print(""Epoch %i, Train cost: %0.3f,\t Val cost: %0.3f, Val ROC curve (area = %0.3f)\t"" % ((epoch+1), train_cost[-1],val_cost[-1], roc_auc_score(y_test, pred_undrsamp)))

        print(str(""---------------"")*7, ""\n\n"",""Optimization Finished!"")
        print(""\n"",""Total epochs: %i\t Final validation ROC_AUC: %0.3f\t""% \
              ((epoch+1),roc_auc_score(y_test, pred_undrsamp)),""\n"")
        print(get_plot_ROC(y_test, pred_undrsamp, 'Feedforward NN using RandomUnderSampling'))



    except KeyboardInterrupt:
        print('KeyboardInterrupt')

print('Done')
</code></pre>

<p>I have two problems: if I don't test during the trainig, my train_cost is huge, but the program runs, otherwise, if I test the network (so I don't comment out this:</p>

<pre><code>fetches_test = [loss_op, prediction]

#X_test = X_test.reshape((1, timesteps, num_input))
#y_test = y_test.reshape((1, 48))


feed_dict_valid = {X: X_test, Y: y_test}

res_t = sess.run(fetches_test, feed_dict=feed_dict_valid)

# storing for evaluation 
val_cost += [res_t[0]]
pred_undrsamp = list(res_t[1]) # y_out validation
</code></pre>

<p>I get this error:</p>

<pre><code>InvalidArgumentError                      Traceback (most recent call last)
~\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1326     try:
-&gt; 1327       return fn(*args)
   1328     except errors.OpError as e:

~\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1305                                    feed_dict, fetch_list, target_list,
-&gt; 1306                                    status, run_metadata)
   1307 

~\Anaconda3\lib\contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---&gt; 66                 next(self.gen)
     67             except StopIteration:

 ~\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py in raise_exception_on_not_ok_status()
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--&gt; 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:

InvalidArgumentError: You must feed a value for placeholder tensor 
'Placeholder' with dtype float and shape [?,48,50]
     [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[?,48,50], 
_device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-24-018940a98f31&gt; in &lt;module&gt;()
     50 
     51                 # running the train_op
 ---&gt; 52                 res = sess.run(fetches=fetches_train, 
feed_dict=feed_dict_train)
     53 
     54                 # storing cross entropy, predictions (second fetch 
argument, so index=1)
</code></pre>
",Preprocessing of the text & Tokenization,must feed value placeholder tensor placeholder dtype float shape trying implement lstm tensorflow problem imput shape corpus tokenize remove punctuation make sequence way divide sequence train test like create neural network two problem test trainig train cost huge program run otherwise test network comment get error
Pattern of regular expressions while using Look Behind or Look Ahead Functions to find a match,"<p>I am trying to split a sentence correctly bases on normal grammatical rules in python.</p>

<p>The sentence I want to split is </p>

<pre><code>s = """"""Mr. Smith bought cheapsite.com for 1.5 million dollars,
i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a
probability of .9 it isn't.""""""
</code></pre>

<p>The expected output is </p>

<pre><code>Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it.

Did he mind?

Adam Jones Jr. thinks he didn't.

In any case, this isn't true...

Well, with a probability of .9 it isn't.
</code></pre>

<p>To achieve this I am using regular , after a lot of searching I came upon the following regex which does the trick.The new_str was jut to remove some \n from 's'</p>

<pre><code>m = re.split(r'(?&lt;!\w\.\w.)(?&lt;![A-Z][a-z]\.)(?&lt;=\.|\?)\s',new_str)

for i in m:
    print (i)



Mr. Smith bought cheapsite.com for 1.5 million dollars,i.e. he paid a lot for it.
Did he mind?
Adam Jones Jr. thinks he didn't.
In any case, this isn't true...
Well, with aprobability of .9 it isn't.
</code></pre>

<p>So the way I understand the reg ex is that we are first selecting </p>

<p>1) All the characters like i.e</p>

<p>2) From the filtered spaces from the first selection ,we select those characters 
which dont have words like Mr. Mrs. etc</p>

<p>3) From the filtered 2nd step we select only those subjects where we have either dot or question and are preceded by a space.</p>

<p>So I tried to change the order as below</p>

<p>1) Filter out all the titles first.</p>

<p>2) From the filtered step select those that are preceded by space</p>

<p>3) remove all phrases like i.e</p>

<p>but when I do that the blank after is also split</p>

<pre><code>m = re.split(r'(?&lt;![A-Z][a-z]\.)(?&lt;=\.|\?)\s(?&lt;!\w\.\w.)',new_str)

for i in m:
    print (i)


Mr. Smith bought cheapsite.com for 1.5 million dollars,i.e.
he paid a lot for it.
Did he mind?
Adam Jones Jr. thinks he didn't.
In any case, this isn't true...
Well, with aprobability of .9 it isn't.
</code></pre>

<p>Shouldn't the last step in the modified procedure be capable in identifying phrases like i.e ,why is it failing to detect it ?</p>
",Preprocessing of the text & Tokenization,pattern regular expression using look behind look ahead function find match trying split sentence correctly base normal grammatical rule python sentence want split expected output achieve using regular lot searching came upon following regex doe trick new str wa jut remove n way understand reg ex first selecting character like e filtered space first selection select character dont word like mr mr etc filtered nd step select subject either dot question preceded space tried change order filter title first filtered step select preceded space remove phrase like e blank also split last step modified procedure capable identifying phrase like e failing detect
Is there a quicker snowball stemmer in python 3.6 than NLTK&#39;s?,"<p>I am currently using NLTK's SnowballStemmer to stem the words in my documents and this was working fine when I had 68 documents. Now I have 4000 documents and this is way too slow. I read another post where someone suggested to use  <code>PyStemmer</code>, but this is not offered on Python 3.6 Are there any other packages that would do the trick? Or maybe there's something I can do in the code to speed up the process.</p>

<p>Code:</p>

<pre><code>eng_stemmer = nltk.stem.SnowballStemmer('english')
...
class StemmedCountVectorizer(CountVectorizer):
    def build_analyzer(self):
        analyzer = super(StemmedCountVectorizer, self).build_analyzer()
        return lambda doc: ([eng_stemmer.stem(w) for w in analyzer(doc)])
</code></pre>
",Preprocessing of the text & Tokenization,quicker snowball stemmer python nltk currently using nltk snowballstemmer stem word document wa working fine document document way slow read another post someone suggested use offered python package would trick maybe something code speed process code
Efficient custom stemming in R package tm,"<p>This is a code that (seems to) work but I'm looking for a way to shorten my code. Which implements custom stemming or grouping of words. </p>

<pre><code># Simple reproducible example:
library(tm)
vec &lt;- c(""partners, very good"", ""partnery SOso Goodish!"", ""partna goodies"", 
         ""Good night"")
corp &lt;- Corpus(VectorSource(vec))
corp &lt;- tm_map(corp, tolower)
corp &lt;- tm_map(corp, removePunctuation)

# Custom stemming (how to shorten this code and avoid reptition)
corp &lt;- tm_map(
  corp, 
  content_transformer(gsub), 
  pattern = ""good[^ ]*"", 
  replacement = ""good""
)
corp &lt;- tm_map(
  corp, 
  content_transformer(gsub), 
  pattern = ""partn[^ ]*"", 
  replacement = ""partn""
)
</code></pre>

<p>Background: I can't use standard stemming methods since: </p>

<ol>
<li>No stemming algorithms for the language of the text I'm analysing
have been developed.</li>
<li>I'm using the code to group terms that are closely related in
meaning (but not spelling) for later feeding the data to clustering algorithms.</li>
</ol>

<p><strong>EDIT</strong> </p>

<p>I've reached a somewhat satisfactory and more scalable solution but I still have a feeling this is not the way this should be done...</p>

<pre><code># Make a list of pattern/replacment pairs
steml &lt;- list(
  c(""good[^ ]*"", ""good""),
  c(""partn[^ ]*"", ""partn"")
)

for (pair in seq_along(steml)) {
  corp &lt;- tm_map(
    corp, 
    content_transformer(gsub), 
    pattern     = steml[[pair]][1],
    replacement = steml[[pair]][2]
  )
}
</code></pre>
",Preprocessing of the text & Tokenization,efficient custom stemming r package tm code seems work looking way shorten code implement custom stemming grouping word background use standard stemming method since stemming algorithm language text analysing developed using code group term closely related meaning spelling later feeding data clustering algorithm edit reached somewhat satisfactory scalable solution still feeling way done
How to append lines from file into a list while keeping the number of lines - python 3,"<p>I am trying to stem words from a file that contains about 90000 lines (each line has three to several hundred words.
I want to append the lines to a list after stemming the words. I was able to insert the stemmed words into a list, which contains one line. I want to insert the words into the list while keeping the 90000 lines. Any ideas?</p>

<p>clean_sentence = []
    with open(folder_path+text_file_name, 'r', encoding='utf-8') as f:</p>

<pre><code>    for line in f:
        sentence = line.split()

        for word in sentence:
            if word.endswith('er'):
                clean_sentence.append(word[:-2])
            else:
                clean_sentence.append(word)
        x = ' '.join(clean_sentence)

    with open('StemmingOutFile.txt','w', encoding=""utf8"") as StemmingOutFile:
        StemmingOutFile.write(x)
</code></pre>

<p>The file is not in English, but here is an example that illustrates the issue at hand: current code yields: </p>

<pre class=""lang-none prettyprint-override""><code>why don't you like to watch TV? are there any more fruits? why not?
</code></pre>

<p>I want the output file to be:</p>

<pre class=""lang-none prettyprint-override""><code>why don't you like to watch TV?

are there any more fruits?

why not? 
</code></pre>
",Preprocessing of the text & Tokenization,append line file list keeping number line python trying stem word file contains line line ha three several hundred word want append line list stemming word wa able insert stemmed word list contains one line want insert word list keeping line idea clean sentence open folder path text file name r encoding utf f file english example illustrates issue hand current code yield want output file
Why adding documents to gensim Dictionary gets slow when reaching 2 million words?,"<p>I noticed that when adding documents to a gensim Dictionary, execution time jumps from 0.2s to more than 6s when reaching 2 million words.</p>

<p>The code below is a quick example. I loop through int and add the number to the dictionary at each iteraion.</p>

<pre><code>from gensim import corpora
import time



dict_transcript = corpora.Dictionary()


for i in range(1,10000000):

    start_time = time.time()

    doc = [str(i)]

    dict_transcript.add_documents([doc])

    print(""Iter ""+str(i)+"" done in "" + str(time.time() - start_time) + ' w/ '+str(len(doc)) + ' words and dico size ' +
          str(len(dict_transcript)))
</code></pre>

<p>I do get the following output when reaching 2 million words:</p>

<pre><code>Iter 1999999 done in 0.0 w/ 1 words and dico size 1999999
Iter 2000000 done in 0.0 w/ 1 words and dico size 2000000
Iter 2000001 done in 0.0 w/ 1 words and dico size 2000001
Iter 2000002 done in 7.940511226654053 w/ 1 words and dico size 2000001
</code></pre>

<p>Is there any reason why? And does anyone know how to bypass that problem?
I'm using this dictionary on a big corpus that I tokenize into bigrams so I'm expecting the dictionary to be a few million rows.</p>

<p>Many thanks</p>
",Preprocessing of the text & Tokenization,adding document gensim dictionary get slow reaching million word noticed adding document gensim dictionary execution time jump reaching million word code quick example loop int add number dictionary iteraion get following output reaching million word reason doe anyone know bypass problem using dictionary big corpus tokenize bigram expecting dictionary million row many thanks
Why the num of languages in `nltk.corpus` stop words is different depending on the OS?,"<p>I'm running the same code in two different machines (Mac and Linux) and despite both machines run the same version of <code>nltk</code> they offer stop words lists in a different number of languages (14 for Mac, 17 Linux).</p>

<pre><code>import nltk
nltk.__version__
from nltk.corpus import stopwords
stopwords.ensure_loaded
stopwords_dict = {lang:stopwords.words(lang) for lang in stopwords.__dict__.get('_fileids')}
stopwords_dict.keys()
len(stopwords_dict.keys())
</code></pre>

<p>Same <code>nltk</code> version in both machines <code>'3.2.5'</code>, but different number of languages:</p>

<p><strong>Mac</strong>:</p>

<pre><code>dict_keys(['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish'])

14
</code></pre>

<p><strong>Linux</strong>:</p>

<pre><code>dict_keys(['kazakh', 'swedish', 'spanish', 'danish', 'english', 'italian', 'portuguese', 'dutch', 'turkish', 'arabic', 'romanian', 'russian', 'french', 'hungarian', 'german', 'finnish', 'norwegian'])
17
</code></pre>

<p>I unsucessfully tried to find the answer in the <code>nltk</code> documentation.</p>
",Preprocessing of the text & Tokenization,num language stop word different depending running code two different machine mac linux despite machine run version offer stop word list different number language mac linux version machine different number language mac linux unsucessfully tried find answer documentation
Word2Vec on Spark Scala,"<p>I'm trying to use Word2Vec from mllib, in order to apply a kmeans subsequently. I'm using scala 2.10.5 and spark 1.6.3. This is my code (after a Tokenization):</p>

<pre><code>val word2Vec = new Word2Vec()
  .setMinCount(2)
  .setInputCol(""FilteredFeauturesEntities"")
  .setOutputCol(""Word2VecFeatures"")
  .setVectorSize(1000)

val model = word2Vec.fit(CleanedTokenizedDataFrame)
val word2VecDataFrame = model.transform(CleanedTokenizedDataFrame)

word2VecDataFrame.show()
</code></pre>

<p>I'm not getting a special error but my job don't reach the finishing lines.
This is the log output :</p>

<pre><code>18/02/05 15:39:32 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 23) in 3143 ms on dhadlx122.haas.xxxxxx (2/9)
18/02/05 15:39:32 INFO TaskSetManager: Starting task 5.1 in stage 4.0 (TID 28, dhadlx121.haas.xxxxxx, partition 5,NODE_LOCAL, 2329 bytes)
18/02/05 15:39:32 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 20) in 3217 ms on dhadlx121.haas.xxxxxx (3/9)
18/02/05 15:39:32 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 22) in 3309 ms on dhadlx123.haas.xxxxxx (4/9)
18/02/05 15:39:32 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 21) in 3677 ms on dhadlx121.haas.xxxxxx (5/9)
18/02/05 15:39:33 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 25) in 3901 ms on dhadlx126.haas.xxxxxx (6/9)
18/02/05 15:39:33 INFO YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (dhadlx127.haas.xxxxxx:48384) with ID 6
18/02/05 15:39:33 INFO BlockManagerMasterEndpoint: Registering block manager dhadlx127.haas.xxxxxx:37909 with 5.3 GB RAM, BlockManagerId(6, dhadlx127.haas.xxxxxx, 37909)
18/02/05 15:39:33 INFO TaskSetManager: Lost task 5.1 in stage 4.0 (TID 28) on executor dhadlx121.haas.xxxxxx: java.lang.NullPointerException (null) [duplicate 1]
18/02/05 15:39:33 INFO TaskSetManager: Starting task 5.2 in stage 4.0 (TID 29, dhadlx128.haas.xxxxxx, partition 5,RACK_LOCAL, 2329 bytes)
18/02/05 15:39:33 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 27) in 2948 ms on dhadlx125.haas.xxxxxx (7/9)
18/02/05 15:39:34 INFO TaskSetManager: Lost task 5.2 in stage 4.0 (TID 29) on executor dhadlx128.haas.xxxxxx: java.lang.NullPointerException (null) [duplicate 2]
18/02/05 15:39:34 INFO TaskSetManager: Starting task 5.3 in stage 4.0 (TID 30, dhadlx127.haas.xxxxxx, partition 5,RACK_LOCAL, 2329 bytes)
18/02/05 15:39:35 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on dhadlx127.haas.xxxxxx:37909 (size: 26.4 KB, free: 5.3 GB)
18/02/05 15:39:35 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 19) in 6321 ms on dhadlx120.haas.xxxxxx (8/9)
18/02/05 15:39:36 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on dhadlx127.haas.xxxxxx:37909 (size: 58.9 KB, free: 5.3 GB)
18/02/05 15:39:40 INFO TaskSetManager: Lost task 5.3 in stage 4.0 (TID 30) on executor dhadlx127.haas.xxxxxx: java.lang.NullPointerException (null) [duplicate 3]
18/02/05 15:39:40 ERROR TaskSetManager: Task 5 in stage 4.0 failed 4 times; aborting job
18/02/05 15:39:40 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool
18/02/05 15:39:40 INFO YarnScheduler: Cancelling stage 4
18/02/05 15:39:40 INFO DAGScheduler: ShuffleMapStage 4 (map at Word2Vec.scala:161) failed in 11.037 s
18/02/05 15:39:40 INFO DAGScheduler: Job 3 failed: collect at Word2Vec.scala:170, took 11.058049 s
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 4.0 failed 4 times, most recent failure: Lost task 5.3 in stage 4.0 (TID 30, dhadlx127.haas.xxxxxx): java.lang.NullPointerException
    at java.util.regex.Matcher.getTextLength(Matcher.java:1283)
    at java.util.regex.Matcher.reset(Matcher.java:309)
    at java.util.regex.Matcher.&lt;init&gt;(Matcher.java:229)
    at java.util.regex.Pattern.matcher(Pattern.java:1093)
    at scala.util.matching.Regex.replaceAllIn(Regex.scala:385)
    at SemanticAnalysis.App$$anonfun$extractPattern$1$1.apply(App.scala:63)
    at SemanticAnalysis.App$$anonfun$extractPattern$1$1.apply(App.scala:63)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
    at org.apache.spark.sql.execution.Project$$anonfun$1$$anonfun$apply$1.apply(basicOperators.scala:51)
    at org.apache.spark.sql.execution.Project$$anonfun$1$$anonfun$apply$1.apply(basicOperators.scala:49)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
    at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:189)
    at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
    at org.apache.spark.scheduler.Task.run(Task.scala:89)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:247)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1433)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1421)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1420)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1420)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:801)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:801)
    at scala.Option.foreach(Option.scala:236)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:801)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1642)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1590)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:622)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1831)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1844)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1857)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1928)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:934)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:323)
    at org.apache.spark.rdd.RDD.collect(RDD.scala:933)
    at org.apache.spark.mllib.feature.Word2Vec.learnVocab(Word2Vec.scala:170)
    at org.apache.spark.mllib.feature.Word2Vec.fit(Word2Vec.scala:284)
    at org.apache.spark.ml.feature.Word2Vec.fit(Word2Vec.scala:149)
    at SemanticAnalysis.App$.main(App.scala:126)
    at SemanticAnalysis.App.main(App.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:750)
    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.NullPointerException
    at java.util.regex.Matcher.getTextLength(Matcher.java:1283)
    at java.util.regex.Matcher.reset(Matcher.java:309)
    at java.util.regex.Matcher.&lt;init&gt;(Matcher.java:229)
    at java.util.regex.Pattern.matcher(Pattern.java:1093)
    at scala.util.matching.Regex.replaceAllIn(Regex.scala:385)
    at SemanticAnalysis.App$$anonfun$extractPattern$1$1.apply(App.scala:63)
    at SemanticAnalysis.App$$anonfun$extractPattern$1$1.apply(App.scala:63)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
    at org.apache.spark.sql.execution.Project$$anonfun$1$$anonfun$apply$1.apply(basicOperators.scala:51)
    at org.apache.spark.sql.execution.Project$$anonfun$1$$anonfun$apply$1.apply(basicOperators.scala:49)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
    at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:189)
    at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
    at org.apache.spark.scheduler.Task.run(Task.scala:89)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:247)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:748)
18/02/05 15:39:40 INFO SparkContext: Invoking stop() from shutdown hook
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static/sql,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
18/02/05 15:39:40 INFO SparkUI: Stopped Spark web UI at http://xxx.xx.xx.xxx:xxxx
18/02/05 15:39:40 INFO YarnClientSchedulerBackend: Interrupting monitor thread
18/02/05 15:39:40 INFO YarnClientSchedulerBackend: Shutting down all executors
18/02/05 15:39:40 INFO YarnClientSchedulerBackend: Asking each executor to shut down
18/02/05 15:39:40 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
services=List(),
started=false)
18/02/05 15:39:40 INFO YarnClientSchedulerBackend: Stopped
18/02/05 15:39:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/02/05 15:39:40 INFO MemoryStore: MemoryStore cleared
18/02/05 15:39:40 INFO BlockManager: BlockManager stopped
18/02/05 15:39:40 INFO BlockManagerMaster: BlockManagerMaster stopped
18/02/05 15:39:40 INFO SparkContext: Successfully stopped SparkContext
18/02/05 15:39:40 INFO ShutdownHookManager: Shutdown hook called
18/02/05 15:39:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-e769e7c5-4336-45bd-97cd-e0731803f45f
18/02/05 15:39:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-f427cf4c-4236-4e57-a304-6be2a52932f3
18/02/05 15:39:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/02/05 15:39:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-f427cf4c-4236-4e57-a304-6be2a52932f3/httpd-0ab9e5ee-930e-4a48-be77-f5a6d2b01250
18/02/05 15:39:40 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
18/02/05 15:39:40 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
18/02/05 15:39:40 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
</code></pre>

<p>Moreover, the same code works for a small example, in the same working environment : </p>

<pre><code>package BIGDATA

/**
* @author ${user.name}
*/

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.{Row, SQLContext}
import org.apache.spark.sql.types.{ArrayType, StringType, StructField, StructType}
import org.apache.spark.ml.feature.StopWordsRemover
import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}
import org.apache.spark.ml.feature.Word2Vec
import org.apache.spark.ml.clustering.KMeans
import org.apache.spark.mllib.linalg.{VectorUDT, Vectors}


object App {

  def main(args : Array[String]) {

val conf = new SparkConf()
  .setAppName(""SEMANTIC ANALYSIS - TEST"")

val sc = new SparkContext(conf)
val hiveContext = new HiveContext(sc)
import hiveContext.implicits._

println(""===================================================="")
println(""READING DATA"")
println(""===================================================="")


val pattern: scala.util.matching.Regex = ""(([\\w\\.-]+@[\\w\\.-]+)|((X|A|x|a)\\d{6})|(MA\\d{7}\\w|MA\\d{7}|FR\\d{8}\\w)|(w+\\..*(\\.com|fr))|([|\\[\\]!\\(\\)?,;:@&amp;*#_=\\/]*))"".r

def extractPattern(pattern: scala.util.matching.Regex) = udf(
  (title: String) =&gt; pattern.replaceAllIn(title, """")
)

val df = Seq(
  (8, ""Hi I heard about Spark x163021. Now, let’s use trained model by loading it. We need to import KMeansModel in order to use it for loading the model from file.""),
  (64, ""I wish Java could use case classes. Above is a very naive example in which we use training dataset as input data too. In real world we will train a model, save it and later use it for predicting clusters of input data.""),
  (-27, ""Logistic regression models are neat. Here is how you can save a trained model and later load it for prediction."")
).toDF(""number"", ""word"").select($""number"", $""word"",
  extractPattern(pattern)($""word"").alias(""NewWord""))


println(""===================================================="")
println(""FEATURE TRANSFORMERS"")
println(""===================================================="")

val tokenizer = new Tokenizer()
  .setInputCol(""NewWord"")
  .setOutputCol(""FeauturesEntities"")

val TokenizedDataFrame = tokenizer.transform(df)

val remover = new StopWordsRemover()
  .setInputCol(""FeauturesEntities"")
  .setOutputCol(""FilteredFeauturesEntities"")

val CleanedTokenizedDataFrame = remover.transform(TokenizedDataFrame)

CleanedTokenizedDataFrame.show()


println(""===================================================="")
println(""WORD2VEC : LEARN A MAPPING FROM WORDS TO VECTORS"")
println(""===================================================="")


// Learn a mapping from words to Vectors.
val word2Vec = new Word2Vec()
  .setMinCount(2)
  .setInputCol(""FilteredFeauturesEntities"")
  .setOutputCol(""Word2VecFeatures"")
  .setVectorSize(1000)

val model = word2Vec.fit(CleanedTokenizedDataFrame)
val word2VecDataFrame = model.transform(CleanedTokenizedDataFrame)

word2VecDataFrame.show()

  }

}
</code></pre>

<p>What's wrong with the first example ? thx !</p>
",Preprocessing of the text & Tokenization,word vec spark scala trying use word vec mllib order apply kmeans subsequently using scala spark code tokenization getting special error job reach finishing line log output moreover code work small example working environment wrong first example thx
Entity Recognition and Sentiment Analysis using NLP,"<p>So, this question might be a little naive, but I thought asking the friendly people of Stackoverflow wouldn't hurt.</p>
<p>My current company has been using a third party API for NLP for a while now. We basically URL encode a string and send it over, and they extract certain entities for us (we have a list of entities that we're looking for) and return a json mapping of entity : sentiment. We've recently decided to bring this project in house instead.</p>
<p>I've been studying NLTK, Stanford NLP and lingpipe for the past 2 days now, and can't figure out if I'm basically reinventing the wheel doing this project.</p>
<p>We already have massive tables containing the original unstructured text and another table containing the extracted entities from that text and their sentiment. The entities are single words. For example:</p>
<blockquote>
<p>Unstructured text : Now for the bed. It wasn't the best.</p>
<p>Entity : Bed</p>
<p>Sentiment : Negative</p>
</blockquote>
<p>I believe that implies we have training data (unstructured text) as well as entity and sentiments. Now how I can go about using this training data on one of the NLP frameworks and getting what we want? No clue. I've sort of got the steps, but not sure:</p>
<ol>
<li>Tokenize sentences</li>
<li>Tokenize words</li>
<li>Find the noun in the sentence (POS tagging)</li>
<li>Find the sentiment of that sentence.</li>
</ol>
<p>But that should fail for the case I mentioned above since it talks about the bed in 2 different sentences?</p>
<p>So the question - Does any one know what the best framework would be for accomplishing the above tasks, and any tutorials on the same (Note: I'm not asking for a solution). If you've done this stuff before, is this task too large to take on? I've looked up some commercial APIs but they're absurdly expensive to use (we're a tiny startup).</p>
<p>Thanks stackoverflow!</p>
",Preprocessing of the text & Tokenization,entity recognition sentiment analysis using nlp question might little naive thought asking friendly people stackoverflow hurt current company ha using third party api nlp basically url encode string send extract certain entity u list entity looking return json mapping entity sentiment recently decided bring project house instead studying nltk stanford nlp lingpipe past day figure basically reinventing wheel project already massive table containing original unstructured text another table containing extracted entity text sentiment entity single word example unstructured text bed best entity bed sentiment negative believe implies training data unstructured text well entity sentiment go using training data one nlp framework getting want clue sort got step sure tokenize sentence tokenize word find noun sentence po tagging find sentiment sentence fail case mentioned since talk bed different sentence question doe one know best framework would accomplishing task tutorial note asking solution done stuff task large take looked commercial apis absurdly expensive use tiny startup thanks stackoverflow
Tokenization of sentence final punctuations when they are not followed by whitespace,"<p>We are trying to select a tokenizer to utilize in our annotation pipeline. Currently, I am testing the PTBTokenizer with some twitter texts.
In our data we have some texts in which there is no whitespace after the sentence final dot:""This is a test.And some more.""
The PTBTokenizer does not recognize ""test"",""."",""And"" as separate tokens, instead it takes the ""test.And""as a single token. Is there any option or setting in the tokenizer which makes it to recognize the sentence final punctuations as single tokens even though it is not followed by a white space?</p>

<p>Thank you.</p>
",Preprocessing of the text & Tokenization,tokenization sentence final punctuation followed whitespace trying select tokenizer utilize annotation pipeline currently testing ptbtokenizer twitter text data text whitespace sentence final dot test ptbtokenizer doe recognize test separate token instead take test single token option setting tokenizer make recognize sentence final punctuation single token even though followed white space thank
Create dfm step by step with quanteda,"<p>I want to analyze a big (n=500,000) corpus of documents. I am using <code>quanteda</code> in the expectation that  <a href=""https://stackoverflow.com/questions/25330753/more-efficient-means-of-creating-a-corpus-and-dtm-with-4m-rows"">will be faster</a> than <code>tm_map()</code> from <code>tm</code>. I want to proceed step by step instead of using the automated way with <code>dfm()</code>. I have reasons for this: in one case, I don't want to tokenize before removing stopwords as this would result in many useless bigrams, in another I have to preprocess the text with language-specific procedures.</p>

<p>I would like this sequence to be implemented:<br>
1) remove the punctuation and numbers<br>
2) remove stopwords (i.e. before the tokenization to avoid useless tokens)<br>
3) tokenize using unigrams and bigrams<br>
4) create the dfm</p>

<p>My attempt:</p>

<pre><code>&gt; library(quanteda)
&gt; packageVersion(""quanteda"")
[1] ‘0.9.8’
&gt; text &lt;- ie2010Corpus$documents$texts
&gt; text.corpus &lt;- quanteda:::corpus(text, docnames=rownames(ie2010Corpus$documents))

&gt; class(text.corpus)
[1] ""corpus"" ""list""

&gt; stopw &lt;- c(""a"",""the"", ""all"", ""some"")
&gt; TextNoStop &lt;- removeFeatures(text.corpus, features = stopw)
# Error in UseMethod(""selectFeatures"") : 
# no applicable method for 'selectFeatures' applied to an object of class ""c('corpus', 'list')""

# This is how I would theoretically continue: 
&gt; token &lt;- tokenize(TextNoStop, removePunct=TRUE, removeNumbers=TRUE)
&gt; token2 &lt;- ngrams(token,c(1,2))
</code></pre>

<p><strong>Bonus question</strong>
How do I remove sparse tokens in <code>quanteda</code>? (i.e. equivalent of <code>removeSparseTerms()</code> in <code>tm</code>.</p>

<hr>

<p><strong>UPDATE</strong>
At the light of @Ken's answer, here is the code to proceed step by step with <code>quanteda</code>:</p>

<pre><code>library(quanteda)
packageVersion(""quanteda"")
[1] ‘0.9.8’
</code></pre>

<p>1) Remove custom punctuation and numbers. E.g. notice that the ""\n"" in the ie2010 corpus</p>

<pre><code>text.corpus &lt;- ie2010Corpus
texts(text.corpus)[1]      # Use texts() to extrapolate text
# 2010_BUDGET_01_Brian_Lenihan_FF
# ""When I presented the supplementary budget to this House last April, I said we
# could work our way through this period of severe economic distress. Today, I
# can report that notwithstanding the difficulties of the past eight months, we
# are now on the road to economic recovery.\nIt is

texts(text.corpus)[1] &lt;- gsub(""\\s"","" "",text.corpus[1])    # remove all spaces (incl \n, \t, \r...)
texts(text.corpus)[1]
2010_BUDGET_01_Brian_Lenihan_FF
# ""When I presented the supplementary budget to this House last April, I said we
# could work our way through this period of severe economic distress. Today, I
# can report that notwithstanding the difficulties of the past eight months, we
# are now on the road to economic recovery. It is of e
</code></pre>

<p>A further note on the reason why one may prefer to preprocess. My present corpus is in Italian, a language that has articles connected to the words with an apostrophe. Thus, the straight <code>dfm()</code> can lead to inexact tokenization.
e.g.: </p>

<pre><code>broken.tokens &lt;- dfm(corpus(c(""L'abile presidente Renzi. Un'abile mossa di Berlusconi""), removePunct=TRUE))
</code></pre>

<p>will produce two separated tokens for the same word (""un'abile"" and ""l'abile""), hence the need of an additional step with <code>gsub()</code> here.</p>

<p>2) In <code>quanteda</code> it is not possible to remove stopwords directly in the text before the tokenization. In my previous example ""l"" and ""un"" have to be removed not to produce misleading bigrams. This can be handled in <code>tm</code> with <code>tm_map(..., removeWords)</code>. </p>

<p>3) Tokenization </p>

<pre><code>token &lt;- tokenize(text.corpus[1], removePunct=TRUE, removeNumbers=TRUE, ngrams = 1:2)
</code></pre>

<p>4) Create the dfm:</p>

<pre><code>dfm &lt;- dfm(token)
</code></pre>

<p>5) Remove sparse features</p>

<pre><code>dfm &lt;- trim(dfm, minCount = 5)
</code></pre>
",Preprocessing of the text & Tokenization,create dfm step step quanteda want analyze big n corpus document using expectation note reason one may prefer preprocess present corpus italian language ha article connected word apostrophe thus straight lead inexact tokenization e g produce two separated token word un abile l abile hence need additional step possible remove stopwords directly text tokenization previous example l un removed produce misleading bigram handled tokenization create dfm remove sparse feature
auto correct misspelled words in a list of lists - python,"<p>I have a list of lists containing words/tokens.</p>

<pre><code>Eg:- [[u'note', u'subject', u'mussage', u'aar', u'install'],[ u'accomplishment', u'survice', u'hte', u'skinn', u'damaged', u'location', u'shown'] 
</code></pre>

<p>Need a python script which auto corrects the misspelled words and provides the results as.  </p>

<pre><code>Eg:- [[u'note', u'subject', u'message', u'air', u'install'],[ u'accomplishment', u'service', u'the', u'skin', u'damaged', u'location', u'shown']
</code></pre>

<p>I have around 2 million lists each having more than 5000 words/tokens. How to do a script which completes the job in very short time</p>
",Preprocessing of the text & Tokenization,auto correct misspelled word list list python list list containing word token need python script auto corrects misspelled word provides result around million list word token script completes job short time
Fast lexicon lookup with phrases and stemming in python,"<p>I'm building a text classifier in python and I have a list of key phrases for each class. For example the classes can be ""travel"" and ""science"" and lists could contain: </p>

<ul>
<li>travel: ""New York"", ""South Korea"", ""Seoul"", etc.</li>
<li>science: ""scientist"", ""chemical"", etc.</li>
</ul>

<p>I'm looking for the best way to match phrases from such lists in python. </p>

<p>For instance, the result for the document:</p>

<blockquote>
  <p>A famous scientist traveled from New York to Seoul, South Korea</p>
</blockquote>

<p>should be:
""science"": 1
""travel"": 3</p>

<p>Even if the ""in"" operator for strings is well optimized, there are a few situations which should be handled as well:</p>

<ul>
<li>word boundaries: at some point I can have ""to"" in the dictionary, and wouldn't want to match ""to"" in ""tomorrow"". Tokenization would work in this case, but some custom logic would be required for phrases, maybe sub-list lookup in the list of tokens.</li>
<li>stemming: ""scientists discovered"" should also be matched when there is ""scientist discover"" in the list</li>
</ul>

<p>Is there a python library which could handle this efficiently? If I need to implement it from scratch, what would be the best way to handle the above issues in terms of performance?</p>
",Preprocessing of the text & Tokenization,fast lexicon lookup phrase stemming python building text classifier python list key phrase class example class travel science list could contain travel new york south korea seoul etc science scientist chemical etc looking best way match phrase list python instance result document famous scientist traveled new york seoul south korea science travel even operator string well optimized situation handled well word boundary point dictionary want match tomorrow tokenization would work case custom logic would required phrase maybe sub list lookup list token stemming scientist discovered also matched scientist discover list python library could handle efficiently need implement scratch would best way handle issue term performance
Remove repeated n-grams from text with NLTK,"<p>I am starting to use <code>nltk</code> with <code>python</code> to analyse chat corpora. To start, I would like to identify the most use words, then I would like to use LDA to identify the topics of the conversations.
I clean the texts as:</p>

<pre><code>stop = set(stopwords.words('english'))
stop.update(['.', ',', '""', ""'"", '?', '!', ':', ';', '(', ')', '[',']', '{', '}', '@', '#', 'http', 'https', '/', '://', '_'])
p_stemmer = PorterStemmer()
texts = ['bla bla xxxxxx', 'hahahah', 'haha xx', ... 'hello xx']
</code></pre>

<p>Many of the words that I analyse consists of <code>xxxxx</code>, <code>xxx</code> and <code>x</code> or <code>hahaha</code> or <code>hahahaha</code>. After the preprocessing I obtain separated values for <code>xxxx</code> and <code>xx</code> etc. How can I consider <code>xxxx</code> and <code>xx</code> (or <code>hahaha</code> and <code>ha</code>  as the same words? Is there any function that allow me to do that?</p>
",Preprocessing of the text & Tokenization,remove repeated n gram text nltk starting use analyse chat corpus start would like identify use word would like use lda identify topic conversation clean text many word analyse consists preprocessing obtain separated value etc consider word function allow
R unnest_tokens and calculate positions (start and end location) of each token,"<p>How to get the position of all the tokens after using unnest_tokens?
Here is a simple example - </p>

<pre><code>df&lt;-data.frame(id=1,
               doc=c(""Patient:   [** Name **], [** Name **] Acct.#:         
[** Medical_Record_Number **]        MR #:     [** Medical_Record_Number **]
Location: [** Location **] ""))
</code></pre>

<p>Tokenize by white space using tidytext - </p>

<pre><code>library(tidytext)
tokens_df&lt;-df %&gt;% 
unnest_tokens(tokens,doc,token = stringr::str_split, 
pattern = ""\\s"",
to_lower = F, drop = F)
</code></pre>

<p>How to get the position of all the tokens?</p>

<pre><code>id  tokens  start  end
 1  Patient: 1      8
 1           9      9
 1  [**      12     14
 1  Name     16     19  
</code></pre>
",Preprocessing of the text & Tokenization,r unnest token calculate position start end location token get position token using unnest token simple example tokenize white space using tidytext get position token
Tokenize and count tokens in grouped Pandas dataframe,"<p>I have a large (~20 million) dataframe containing the posts made by users on various communities. The columns include 'community' and 'text'. Each row corresponds to a post; each community will typically have one thousand to several hundred thousand associated posts. </p>

<p>I would like to output a dictionary containing community names as keys and associated Counters counting the number of occurrences of each token in that community. I thought the easiest way to do this would be to group the Pandas dataframe by community, concatenate the 'text' into a string, clean/tokenize, and apply the Counter, as follows:</p>

<pre><code>def run(data):
    counts = {}
    data_grouped = data.groupby('community')
    for community, group in data_grouped:
        community_doc = group['text'].str.cat(sep = '')
        community_doc = community_doc.lower()
        community_doc = community_doc.translate(None, string.punctuation)
        tokens = nltk.word_tokenize(community_doc)
        tokens = [token for token in tokens if not token in stopwords.words('english')]
        count = Counter(tokens).most_common()
        counts[community] = count
    return counts
</code></pre>

<p>However, the code runs incredibly slowly -- often 20-30 minutes per community. Is there a more efficient way to do this?</p>
",Preprocessing of the text & Tokenization,tokenize count token grouped panda dataframe large million dataframe containing post made user various community column include community text row corresponds post community typically one thousand several hundred thousand associated post would like output dictionary containing community name key associated counter counting number occurrence token community thought easiest way would group panda dataframe community concatenate text string clean tokenize apply counter follows however code run incredibly slowly often minute per community efficient way
Determining if two words are derived from the same root in Python,"<p>I'd like to write a function <code>same_base(word1, word2)</code> that returns <code>True</code> when <code>word1</code> and <code>word2</code> are two English words derived from the same root word. I realize that words can have multiple senses; I want the algorithm to be overzealous, returning <code>True</code> whenever it is <em>possible</em> to view the words as originating from the same place. Some false positives are OK; false negatives are not.</p>

<p>Typically, stemming and lemmatization would be used for this. Here's what I've tried:</p>

<ul>
<li>Check if the words stem to the same thing, using, for instance, the Porter Stemmer. This doesn't catch <code>sung</code> and <code>sing</code>, <code>dig</code> and <code>dug</code>, <code>medication</code> and <code>medicine</code>.</li>
<li>Check if the words lemmatize to the same thing. It's unclear what arguments to pass to the lemmatizer (i.e., for part of speech). The WordNet lemmatizer, at least, seems to be too conservative.</li>
</ul>

<p>Does such a tool exist? Do I just need an extremely aggressive stemmer / lemmatizer combo — and if so, where would I find one?</p>
",Preprocessing of the text & Tokenization,determining two word derived root python like write function return two english word derived root word realize word multiple sens want algorithm overzealous returning whenever possible view word originating place false positive ok false negative typically stemming lemmatization would used tried check word stem thing using instance porter stemmer catch check word lemmatize thing unclear argument pas lemmatizer e part speech wordnet lemmatizer least seems conservative doe tool exist need extremely aggressive stemmer lemmatizer combo would find one
How to use clustering to group sentences with similar intents?,"<p>I'm trying to develop an program in Python that can process raw chat data and cluster sentences with similar intents so they can be used as training examples to build a new chatbot. The goal is to make it as quick and automatic (i.e. no parameters to enter manually) as possible.</p>

<p>1- For feature extraction, I tokenize each sentence, stem its words and vectorize it using Sklearn's TfidfVectorizer.</p>

<p>2- Then I perform clustering on those sentence vectors with Sklearn's DBSCAN. I chose this clustering algorithm because it doesn't require the user to specify the desired number of clusters (like the k parameter in k-means). It throws away a lot of sentences (considering them as outliers), but at least its clusters are homogeneous.</p>

<p>The overall algorithm works on relatively small datasets (10000 sentences) and generates meaningful clusters, but there are a few issues:</p>

<ul>
<li><p>On large datasets (e.g. 800000 sentences), DBSCAN fails because it requires too much memory, even with parallel processing on a powerful machine in the cloud. I need a less computationally-expensive method, but I can't find another algorithm that doesn't make weird and heterogeneous sentence clusters. What other options are there? What algorithm can handle large amounts of high-dimensional data?</p></li>
<li><p>The clusters that are generated by DBSCAN are sentences that have similar wording (due to my feature extraction method), but the targeted words don't always represent intents. How can I improve my feature extraction so it better captures the intent of a sentence? I tried Doc2vec but it didn't seem to work well with small datasets made of documents that are the size of a sentence...</p></li>
</ul>
",Preprocessing of the text & Tokenization,use clustering group sentence similar intent trying develop program python process raw chat data cluster sentence similar intent used training example build new chatbot goal make quick automatic e parameter enter manually possible feature extraction tokenize sentence stem word vectorize using sklearn tfidfvectorizer perform clustering sentence vector sklearn dbscan chose clustering algorithm require user specify desired number cluster like k parameter k mean throw away lot sentence considering outlier least cluster homogeneous overall algorithm work relatively small datasets sentence generates meaningful cluster issue large datasets e g sentence dbscan fails requires much memory even parallel processing powerful machine cloud need le computationally expensive method find another algorithm make weird heterogeneous sentence cluster option algorithm handle large amount high dimensional data cluster generated dbscan sentence similar wording due feature extraction method targeted word always represent intent improve feature extraction better capture intent sentence tried doc vec seem work well small datasets made document size sentence
"Negation in R, how can I replace words following a negation in R?","<p>I'm following up on a question that has been asked <a href=""https://stackoverflow.com/questions/21811580/negation-handling-in-r-how-can-i-replace-a-word-following-a-negation-in-r"">here</a> about how to add the prefix ""not_"" to a word following a negation.</p>

<p>In the comments, MrFlick proposed a solution using a regular expression <code>gsub(""(?&lt;=(?:\\bnot|n't) )(\\w+)\\b"", ""not_\\1"", x, perl=T)</code>.</p>

<p>I would like to edit this regular expression in order to add the <em>not_</em> prefix to all the words following ""not"" or ""n't"" until there is some punctuation.</p>

<p>If I'm editing cptn's example, I'd like:</p>

<pre><code>x &lt;- ""They didn't sell the company, and it went bankrupt"" 
</code></pre>

<p>To be transformed into:</p>

<pre><code>""They didn't not_sell not_the not_company, and it went bankrupt""
</code></pre>

<p>Can the use of backreference still do the trick here? If so, any example would be much appreciated. Thanks!</p>
",Preprocessing of the text & Tokenization,negation r replace word following negation r following question ha asked comment mrflick proposed solution using regular expression would like edit regular expression order add prefix word following n punctuation editing cptn example like transformed use backreference still trick example would much appreciated thanks
Issues in lemmatization (nltk),"<p>I am using nltk lemmatizer as follows.</p>

<pre><code>from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
mystring = ""the sand rock needed to be mixed and shaked well before using it for construction works""
splits=mystring.split()
mystring = "" "".join(lemmatizer.lemmatize(w) for w in splits)
print(mystring)
</code></pre>

<p>I am expecting the output to be</p>

<pre><code>sand rock need to be mix and shake well before use it for construction work
</code></pre>

<p>However, in the output I get (mentioned below) it seems like words such as <code>needed, mixed, shaked, using</code> have not changed to its base form.</p>

<pre><code>sand rock needed to be mixed and shaked well before using it for construction work
</code></pre>

<p>Is there a way to resolve this problem?</p>
",Preprocessing of the text & Tokenization,issue lemmatization nltk using nltk lemmatizer follows expecting output however output get mentioned seems like word changed base form way resolve problem
Get the word from stem (stemming),"<p>I am using porter stemmer as follows to get the stem of my words.</p>

<pre><code>from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()
def stem_tokens(tokens, stemmer):
    stemmed = []
    for item in tokens:
        stemmed.append(stemmer.stem(item))
    return stemmed
</code></pre>

<p>Now, I want to know the possibility of some word from the stem to make it readable. For example <code>environ</code> to <code>environment</code> or <code>educ</code> to <code>education</code> etc. Is it possible to do?</p>
",Preprocessing of the text & Tokenization,get word stem stemming using porter stemmer follows get stem word want know possibility word stem make readable example etc possible
Deleting stop-words and punctuation signs,"<p>I parse information from a news website. Each news is a dictionary that is stored inside translated_news variable. Each news has its title, url and country.
Then I try to iterate over each news title and delete stop-words and punctuation signs. I've written this code:</p>

<pre><code>for new in translated_news:
    tk = tokenize(new['title'])
    # delete punctuation signs &amp; stop-words
    for t in tk:
        if (t in punkts) or (t+'\n' in stops):
            tk.remove(t)
tokens.append(tk)
</code></pre>

<p>Tokenize is a function that returns a list of tokens. Here's an example of the output:</p>

<pre><code>['medium', ':', 'russian', 'athlete', 'will', 'be', 'admit', 'to', 'the', '2018', 'olympics', 'in', 'neutral', 'status']
</code></pre>

<p>Here's the same output, but with deleted stop-words and punctuation:</p>

<pre><code>['medium', 'russian', 'athlete', 'be', 'admit', 'the', 'olympics', 'neutral', 'status']
</code></pre>

<p>The problem is: even though the words 'the' and 'be' are included in my stop-words list, they were not deleted from the news title. However, on other titles it sometimes works correctly:</p>

<p><code>['wada', 'acknowledge', 'the', 'reliable', 'information', 'provide', 'to', 'rodchenkov']
['wada', 'acknowledge', 'reliable', 'information', 'provide', 'rodchenkov']</code></p>

<p>Here 'the' was deleted from the title.
I don't understand what is wrong with the code and why sometimes the output is perfect and sometimes not. </p>
",Preprocessing of the text & Tokenization,deleting stop word punctuation sign parse information news website news dictionary stored inside translated news variable news ha title url country try iterate news title delete stop word punctuation sign written code tokenize function return list token example output output deleted stop word punctuation problem even though word included stop word list deleted news title however title sometimes work correctly wa deleted title understand wrong code sometimes output perfect sometimes
"spaCy and text cleaning, getting rid of &#39;&lt;br /&gt;&lt;br /&gt;&#39;","<p>I am working with spaCy and python trying to clean some text for sklearn. I run the loop:</p>

<pre><code>for text in df.text_all:
    text = str(text)
    text = nlp(text)
    cleaned = [token.lemma_ for token in text if token.is_punct==False and token.is_stop==False]
    cleaned_text.append(' '.join(cleaned))
</code></pre>

<p>And it works pretty well but it leaves in <code>&lt;br /&gt;&lt;br /&gt;</code> inside of some text. I thought that would be taken out by the <code>token.is_punct==False</code> filter but no. I looked for something like html tags but couldn't find anything. Does anyone know what I can do? </p>
",Preprocessing of the text & Tokenization,spacy text cleaning getting rid br br working spacy python trying clean text sklearn run loop work pretty well leaf inside text thought would taken filter looked something like html tag find anything doe anyone know
is there any text preprocessing which can detect san andreas as a single word Api for python,"<p>I am working twitter data for analysis but before analysis, i want to preprocess data. i tried nltk.word_tokenize(), but they don't remove punctuation and they don't understand san andreas and a single word but in tweets, there are many words like this. here is code </p>

<p><code>def inital():
    return 0
def get_vocab(lst):
    vocab_count=defaultdict(inital)
    i=0
    for txt in lst:
        if type(txt)!=float:
            txt=txt.decode(""utf8"")
            for words in nltk.word_tokenize(txt):
                vocab_count[words]+=1
    return vocab_count</code></p>
",Preprocessing of the text & Tokenization,text preprocessing detect san andreas single word api python working twitter data analysis analysis want preprocess data tried nltk word tokenize remove punctuation understand san andreas single word tweet many word like code
NLTK words vs word_tokenize,"<p>I'm exploring some of NLTK's corpora and came across the following behaviour: <strong>word_tokenize() and words produce different sets of words()</strong>.</p>

<p>Here is an example using webtext:</p>

<pre><code>from nltk.corpus import webtext
</code></pre>

<p>When I run the following,</p>

<pre><code>len(set(word_tokenize(webtext.raw('wine.txt'))))
</code></pre>

<p>I get: 3488</p>

<p>When I run the following,</p>

<pre><code>len(set(webtext.words('wine.txt')))
</code></pre>

<p>I get: 3414</p>

<p>All I can find in the documentation is that word_tokenize is a list of punctuation and words. But it also says words is a list of punctuation and words. I'm wondering, what's going on here? Why are they different?</p>

<p>I've already tried looking at the set differences.</p>

<pre><code>U = set(word_tokenize(webtext.raw('wine.txt')))
V = set(webtext.words('wine.txt'))

tok_not_in_words = U.difference(V) # in tokenize but not in words
words_not_in_tok = V.difference(U) # in words but not in tokenize
</code></pre>

<p>All I can see is that word_tokenize contains hyphenated words and words splits the hyphenated words.</p>

<p>Any help is appreciated. Thank you!  </p>
",Preprocessing of the text & Tokenization,nltk word v word tokenize exploring nltk corpus came across following behaviour word tokenize word produce different set word example using webtext run following get run following get find documentation word tokenize list punctuation word also say word list punctuation word wondering going different already tried looking set difference see word tokenize contains hyphenated word word split hyphenated word help appreciated thank
Remove a verb as a stopword,"<p>There are some words which are used sometimes as a verb and sometimes as other part of speech.</p>

<p><strong>Example</strong> </p>

<p>A sentence with the meaning of the word as verb:</p>

<pre><code>I blame myself for what happened
</code></pre>

<p>And a sentence with the meaning of word as noun:</p>

<pre><code>For what happened the blame is yours
</code></pre>

<p>The word I want to detect is known to me, in the example above is ""blame"". I would like to detect and remove as stopwords only when it has meaning like a verb.</p>

<p>Is there any easy way to make it?</p>
",Preprocessing of the text & Tokenization,remove verb stopword word used sometimes verb sometimes part speech example sentence meaning word verb sentence meaning word noun word want detect known example blame would like detect remove stopwords ha meaning like verb easy way make
How can you programmatically add contractions to a sentence?,"<p>I've seen many posts about expanding contractions to tokenize them correctly but I'd like to be able to add a contraction while retaining semantic accuracy. Are there any libraries addressing this or solutions available to this problem?</p>

<p>Positive Examples:</p>

<ul>
<li>""You are going to the restaurant."" becomes ""You're going to the
restaurant."" </li>
<li>""I am a happy boy."" becomes ""I'm a happy boy.""</li>
</ul>

<p>Negative Examples:</p>

<ul>
<li>""Is that all you have?"" should not become ""Is that all you've?"" </li>
<li>""Is that who you are?"" should not become ""Is that who you're?"" </li>
<li>""You have a big heart."" should not become ""You've a big heart.""</li>
</ul>
",Preprocessing of the text & Tokenization,programmatically add contraction sentence seen many post expanding contraction tokenize correctly like able add contraction retaining semantic accuracy library addressing solution available problem positive example going restaurant becomes going restaurant happy boy becomes happy boy negative example become become big heart become big heart
Compute word n-grams on original text or after lemma/stemming process?,"<p>I'm thinking about use word n-grams techniques on a raw text. But I have a doubt:</p>

<p>does it have sense use word n-grams after applying lemma/stemming on text? If not, why should I use word n-grams only on raw files? What are pros and cons?</p>
",Preprocessing of the text & Tokenization,compute word n gram original text lemma stemming process thinking use word n gram technique raw text doubt doe sense use word n gram applying lemma stemming text use word n gram raw file pro con
Getting numpy vector from a trained Doc2Vec model for each document,"<p>This is my first time using Doc2Vec
I'm trying to classify works of an author. I have trained a model with Labeled Sentences (paragraphs, or strings of specified length), with words = the list of words in the paragraph, and tags = author's name. In my case I only have two authors.
I tried accessing the docvecs attribute from the trained model but it only contains two elements, corresponding to the two tags I have when I trained the model. I'm trying to get the doc2vec numpy representations of each paragraph I fed in to the training so I can use that as training data later on. How can I do this?
Thanks.</p>
",Preprocessing of the text & Tokenization,getting numpy vector trained doc vec model document first time using doc vec trying classify work author trained model labeled sentence paragraph string specified length word list word paragraph tag author name case two author tried accessing docvecs attribute trained model contains two element corresponding two tag trained model trying get doc vec numpy representation paragraph fed training use training data later thanks
How to prepare &#39;hashed&#39; text data for classifier,"<pre><code>sequence = 'I am Bob'
</code></pre>

<p>The sequence has been altered in some way, like e.g. letters have been replaced with the subsequent letter (a->b), and spaces with 'ss': </p>

<pre><code>hashed_seq = 'JssbnssCpc' #for lack of a better word I'm using 'hashed'
</code></pre>

<p>In my experience, for text classification, you tokenize the sentence/words with some dictionary, e.g.:</p>

<pre><code>vocab = {'&lt;PAD&gt;': 0, 'I': 1, 'Bob': 2, 'am': 3}
tokenized_seq = [1,3,2]
</code></pre>

<p>How would I prepare (tokenize etc) the data for text classification in my case without decoding them? I only have the data after the algorithm has altered them.</p>

<p>Language: Python</p>
",Preprocessing of the text & Tokenization,prepare hashed text data classifier sequence ha altered way like e g letter replaced subsequent letter b space experience text classification tokenize sentence word dictionary e g would prepare tokenize etc data text classification case without decoding data algorithm ha altered language python
LDA: Good rule-of-thumb for minimum number of words per document?,"<p>I am creating a semantic space using LDA or Hierarchical LDA. I'm working form a collection of ~45000 texts, some of which are very long (e.g., Moby Dick). Others are relatively short.</p>

<p>My plan is to break the texts into smaller chunks and consider each of these chunks as a ""document"" for purposes of generating the semantic space.</p>

<p>What is a good rule-of-thumb for how long to make each document? My understanding is that LDA [much moreso than LSI] prefers to work with longer texts, so rather than just breaking the texts into paragraphs, I am currently planning on breaking them into paragraphs and adjoining paragraphs until I get 200 terms (after text pre-processing, removing stop-words, etc.)</p>

<p>Is 200 a good number for this?</p>

<p>This will yield something like 3,800,000 million documents for training.</p>

<p>Does the answer differ between LDA and HDP-LDA?</p>
",Preprocessing of the text & Tokenization,lda good rule thumb minimum number word per document creating semantic space using lda hierarchical lda working form collection text long e g moby dick others relatively short plan break text smaller chunk consider chunk document purpose generating semantic space good rule thumb long make document understanding lda much moreso lsi prefers work longer text rather breaking text paragraph currently planning breaking paragraph adjoining paragraph get term text pre processing removing stop word etc good number yield something like million document training doe answer differ lda hdp lda
Does stemming a lemma provide any useful output?,"<p>I am currently working with a neural network model that take words and phrases as features in a bag of words style model (i.e. a vector filled with ones and zeros depending if a word or phrase appears in the input data). </p>

<p>To clean some of the words and reduce the number of unique words or phrases that will be input to the model a colleague and I used stemming AND lemmatization with the nltk python module. After I thought about it, this did not seem to make sense, but stemming the lemmas seemed to reduce the number of unique inputs the most, so we ran with it.</p>

<p>My question is, is it harmful to stem a lemma? Would it harm the results? Is there a point to doing this?</p>

<p>I have not found anyone asking a question like this <em>anywhere</em> so I am assuming it simply does not make sense.</p>
",Preprocessing of the text & Tokenization,doe stemming lemma provide useful output currently working neural network model take word phrase feature bag word style model e vector filled one zero depending word phrase appears input data clean word reduce number unique word phrase input model colleague used stemming lemmatization nltk python module thought seem make sense stemming lemma seemed reduce number unique input ran question harmful stem lemma would harm result point found anyone asking question like anywhere assuming simply doe make sense
Reducing computation time of counting word frequency in a corpus (python),"<pre><code>def foo(self, n):
    wordFreqDict = defaultdict(int)
    resultList = []

    for sentence in self.allSentences:
        for word in self.wordSet:
            if word in sentence:
                wordFreqDict[word] += 1

    for candidateWord in wordFreqDict:
        if wordFreqDict[candidateWord] &gt;= n:
            resultList.append(candidateWord)

    return resultList
</code></pre>

<p>My function basically return the list of words where a word is in the list iff it occurs in at least n sentences. What I am trying to do is just iterate through the self.allSentences is a list of list of words(sentence) and iterate through the self.wordSet(all unique words in the corpus) and add the frequency of each word to the dictionary (wordFreqDict).</p>

<p>Then loop through the wordFreqDict and add a word with frequency >= n into the resultList.</p>

<p>I am assuming that this will work, but it's taking too long time to check the result.</p>

<p>Is there a way to make it for efficient and make the computation time shorter?</p>

<p>EDIT:</p>

<p>Here is how the self.allSentences is computed</p>

<pre><code>def storeAllSentences(self, corpus):
    for sentence in corpus:
        self.allSentences.append(sentence)
</code></pre>

<p>and the self.wordSet:</p>

<pre><code>def getUniqueWordSet(self, corpus):
    for sentence in self.allSentences:
        for word in sentence:
            self.wordSet.append(word)

    self.wordSet = set(self.wordSet)
</code></pre>
",Preprocessing of the text & Tokenization,reducing computation time counting word frequency corpus python function basically return list word word list iff occurs least n sentence trying iterate self allsentences list list word sentence iterate self wordset unique word corpus add frequency word dictionary wordfreqdict loop wordfreqdict add word frequency n resultlist assuming work taking long time check result way make efficient make computation time shorter edit self allsentences computed self wordset
How can i clean urdu data corpus Python without nltk,"<p>I have a corpus of more that 10000 words in urdu. Now what i want is to clean my data. There appear a special uni coded data in my text like ""!؟ـ،"" whenever i use regular expressions it gives me error that your data is not in encoded form. 
Kindly provide me some help to clean my data. 
Thank you</p>

<p>Here is my sample data: </p>

<blockquote>
  <p>ظہیر</p>
  
  <p>احمد</p>
  
  <p>ماہرہ</p>
  
  <p>خان</p>
  
  <p>کی،</p>
  
  <p>تصاویر،</p>
  
  <p>نے</p>
  
  <p>دائیں</p>
  
  <p>اور</p>
  
  <p>بائیں</p>
  
  <p>والوں</p>
  
  <p>کو</p>
  
  <p>آسمانوں</p>
  
  <p>پر</p>
  
  <p>پہنچایا</p>
  
  <p>،ہوا</p>
  
  <p>ہے۔</p>
  
  <p>دائیں؟</p>
  
  <p>والے</p>
</blockquote>
",Preprocessing of the text & Tokenization,clean urdu data corpus python without nltk corpus word urdu want clean data appear special uni coded data text like whenever use regular expression give error data encoded form kindly provide help clean data thank sample data
How to keep non-alphanumeric symbols when tokenizing words in R?,"<p>I am using the <code>tokenizers</code> package in <strong>R</strong> for tokenizing a text, but non-alphanumeric symbols such as ""@"" or ""&amp;"" are lost and I need to keep them. Here is the function I am using:</p>

<pre><code>tokenize_ngrams(""My number &amp; email address user@website.com"", lowercase = FALSE, n = 3, n_min = 1,stopwords = character(), ngram_delim = "" "", simplify = FALSE)
</code></pre>

<p>I know <code>tokenize_character_shingles</code> has the <code>strip_non_alphanum</code> argument that allows keeping the punctuation, but the tokenization is applied to characters, not words.</p>

<p>Anyone knows how to handle this issue?</p>
",Preprocessing of the text & Tokenization,keep non alphanumeric symbol tokenizing word r using package r tokenizing text non alphanumeric symbol lost need keep function using know ha argument allows keeping punctuation tokenization applied character word anyone know handle issue
Can PostgreSQL&#39;s to_tsvector function return tokens/words and not lexemes?,"<p>PostgreSQL's <code>to_tsvector</code> function is extremely useful but in regards to my data set it does a little more than I want it to.</p>

<p>For instance:</p>

<pre><code>select * 
from to_tsvector('english', 'This is my favourite game. I enjoy everything about it.');
</code></pre>

<p>produces: <code>'enjoy':7 'everyth':8 'favourit':4 'game':5</code></p>

<p>I am not fussed about stop-words getting filtered out, that is fine. But some words get completely ruined, like <code>everything</code> and <code>favourite</code>.</p>

<p>Is there a way to modify this behaviour or is there a different function that does this?</p>

<p>PS: Yes, I can write my own query that does this (and I have) but I want a faster method.</p>
",Preprocessing of the text & Tokenization,postgresql tsvector function return token word lexeme postgresql function extremely useful regard data set doe little want instance produce fussed stop word getting filtered fine word get completely ruined like way modify behaviour different function doe p yes write query doe want faster method
Stemmer function in R Slow,"<p>I am trying to run stemmer function on a dataset(uploaded through data.table package) in R of around 40000 rows,but its taking forever to run. My code looks like this:</p>

<pre><code>data[, Description := map(Description, function(k) stemmer(k))]
</code></pre>

<p>If manually stop the process, it shows more than 50 warnings as:
<a href=""https://i.sstatic.net/mYKsE.jpg"" rel=""nofollow noreferrer"">Image Link</a></p>

<p>Is there an alternative to run it faster. My computer has 8Gb ram.</p>
",Preprocessing of the text & Tokenization,stemmer function r slow trying run stemmer function dataset uploaded data table package r around row taking forever run code look like manually stop process show warning image link alternative run faster computer ha gb ram
How to put key-words in NLTK tokenize?,"<p>Input:""My favorite game is call of duty.""</p>

<p>And I set ""call of duty"" as a key-words, this phrase will be one word in tokenize process.</p>

<p>Finally want to get the result:['my','favorite','game','is','call of duty']</p>

<p>So, how to set the key-words in python NLP ?</p>
",Preprocessing of the text & Tokenization,put key word nltk tokenize input favorite game call duty set call duty key word phrase one word tokenize process finally want get result favorite game call duty set key word python nlp
Count if a word occurs in each row of a 4 million observation data set,"<p>I am using R and writing a script that counts if one of ~2000 words occurs in each row of a 4 million observation data file. The data set with observations (df) contains two columns, one with text (df$lead_paragraph), and one with a date (df$date).</p>

<p>Using the following, I can count if any of the words in a list (p) occur in each row of the lead_paragraph column of the df file, and output the answer as a new column.</p>

<pre><code>   df$pcount&lt;-((rowSums(sapply(p, grepl, df$lead_paragraph, 
   ignore.case=TRUE) == TRUE, na.rm=T) &gt; 0) * 1)
</code></pre>

<p>However, if I include too many words in the list p, running the code crashes R.</p>

<p>My alternate strategy is to simply break this into pieces, but I was wondering if there is a better, more elegant coding solution to use here. My inclination is to use a for loop, but everything I am reading suggests this is not preferred in R. I am pretty new to R and not a very good coder, so my apologies if this is not clear. </p>

<pre><code>    df$pcount1&lt;-((rowSums(sapply(p[1:100], grepl, df$lead_paragraph, 
    ignore.case=TRUE) == TRUE, na.rm=T) &gt; 0) * 1)
    df$pcount2&lt;-((rowSums(sapply(p[101:200], grepl, df$lead_paragraph, 
    ignore.case=TRUE) == TRUE, na.rm=T) &gt; 0) * 1) 
    ...
    df$pcount22&lt;-((rowSums(sapply(p[2101:2200], grepl, df$lead_paragraph, 
    ignore.case=TRUE) == TRUE, na.rm=T) &gt; 0) * 1)
</code></pre>
",Preprocessing of the text & Tokenization,count word occurs row million observation data set using r writing script count one word occurs row million observation data file data set observation df contains two column one text df lead paragraph one date df date using following count word list p occur row lead paragraph column df file output answer new column however include many word list p running code crash r alternate strategy simply break piece wa wondering better elegant coding solution use use loop everything reading suggests preferred r pretty new r good coder apology clear
How to do large-scale replacement/tokenization in R tm_map gsub from a list?,"<p>Has anyone managed to create a massive find/replace function/working code snippet that exchanges out known bigrams in a dataframe?</p>

<p>Here's an example. I'm able to don onesie-twosie replacements but I really want to leverage a known lexicon of about 800 terms I want to find-replace to turn them into word units prior to DTM generation. For example, I want to turn ""Google Analytics"" into ""google-analytics"".</p>

<p>I know it's theoretically possible; essentially, a custom stopwords list functionally does almost the same thing, except without the replacement. And it seems stupid to just have 800 gsubs.</p>

<p>Here's my current code. Any help/pointers/URLs/RTFMs would be greatly appreciated.</p>

<pre><code>mystopwords &lt;- read.csv(stopwords.file, header = FALSE)
mystopwords &lt;- as.character(mystopwords$V1)
mystopwords &lt;- c(mystopwords, stopwords())

# load the file

df &lt;- readLines(file.name)

# transform to corpus

doc.vec &lt;- VectorSource(df)
doc.corpus &lt;- Corpus(doc.vec)
# summary(doc.corpus)

## Hit known phrases

docs &lt;- tm_map(doc.corpus, content_transformer(gsub), pattern = ""Google Analytics"", replacement = ""google-analytics"")

## Clean up and fix text - note, no stemming

doc.corpus &lt;- tm_map(doc.corpus, content_transformer(tolower))
doc.corpus &lt;- tm_map(doc.corpus, removePunctuation,preserve_intra_word_dashes = TRUE)
doc.corpus &lt;- tm_map(doc.corpus, removeNumbers)
doc.corpus &lt;- tm_map(doc.corpus, removeWords, c(stopwords(""english""),mystopwords))
doc.corpus &lt;- tm_map(doc.corpus, stripWhitespace)
</code></pre>
",Preprocessing of the text & Tokenization,large scale replacement tokenization r tm map gsub list ha anyone managed create massive find replace function working code snippet exchange known bigram dataframe example able onesie twosie replacement really want leverage known lexicon term want find replace turn word unit prior dtm generation example want turn google analytics google analytics know theoretically possible essentially custom stopwords list functionally doe almost thing except without replacement seems stupid gsubs current code help pointer url rtfms would greatly appreciated
how to map documents to controls,"<p>I have about 500 documents (1 page) that have been mapped to about 3000 short paragraphs (1-2 sentences). These paragraphs describe how the document needs to be reviewed. Each document can and is typically mapped to several paragraphs.</p>

<p>For example, if the document is about the procedures to follow for a certain production process, the paragraphs are about who needs to review the document, what needs to be reviewed, what is the goal of the review, how frequently it should be done etc.</p>

<p>I want to develop a model that can suggest the possible paragraphs from a given document. I have chosen to follow the below approach:</p>

<p>Prepare the data (tokenize, remove stop words, lemmatize etc.)
Consider all the paragraphs as one single output, i.e. concatenate them
Use a sequence to sequence model (tensorflow encoder/decoder RNN model) to map the document to the concatenated paragraphs
Use the outputted sequence to find the closest paragraphs as a suggestion
Given the small sample size, the model in step 3 does not converge.</p>

<p>I am trying to improve the modeling approach (e.g. one can map each sentence in the document to each paragraph to increase the sample size) or to find alternative approaches. What are some typical models for such a problem?</p>
",Preprocessing of the text & Tokenization,map document control document page mapped short paragraph sentence paragraph describe document need reviewed document typically mapped several paragraph example document procedure follow certain production process paragraph need review document need reviewed goal review frequently done etc want develop model suggest possible paragraph given document chosen follow approach prepare data tokenize remove stop word lemmatize etc consider paragraph one single output e concatenate use sequence sequence model tensorflow encoder decoder rnn model map document concatenated paragraph use outputted sequence find closest paragraph suggestion given small sample size model step doe converge trying improve modeling approach e g one map sentence document paragraph increase sample size find alternative approach typical model problem
How to preprocess text for embedding?,"<p>In the traditional ""one-hot"" representation of words as vectors you have a vector of the same dimension as the cardinality of your vocabulary. To reduce dimensionality usually stopwords are removed, as well as applying stemming, lemmatizing, etc. to normalize the features you want to perform some NLP task on.</p>

<p>I'm having trouble understanding whether/how to preprocess text to be embedded (e.g. word2vec). My goal is to use these word embeddings as features for a NN to classify texts into topic A, not topic A, and then perform event extraction on them on documents of topic A (using a second NN).</p>

<p>My first instinct is to preprocess removing stopwords, lemmatizing stemming, etc. But as I learn about NN a bit more I realize that applied to natural language, the CBOW and skip-gram models would in fact require the whole set of words to be present --to be able to predict a word from context one would need to know the actual context, not a reduced form of the context after normalizing... right?). The actual sequence of POS tags seems to be key for a human-feeling prediction of words.</p>

<p>I've found <a href=""https://groups.google.com/forum/#!topic/word2vec-toolkit/TI-TQC-b53w"" rel=""noreferrer"">some guidance online</a> but I'm still curious to know what the community here thinks:</p>

<ol>
<li>Are there any recent commonly accepted best practices regarding punctuation, stemming, lemmatizing, stopwords, numbers, lowercase etc?</li>
<li>If so, what are they? Is it better in general to process as little as possible, or more on the heavier side to normalize the text? Is there a trade-off?</li>
</ol>

<p>My thoughts: </p>

<p>It is better to remove punctuation (but e.g. in Spanish don't remove the accents because the do convey contextual information), change written numbers to numeric, do not lowercase everything (useful for entity extraction), no stemming, no lemmatizing. </p>

<p>Does this sound right?</p>
",Preprocessing of the text & Tokenization,preprocess text embedding traditional one hot representation word vector vector dimension cardinality vocabulary reduce dimensionality usually stopwords removed well applying stemming lemmatizing etc normalize feature want perform nlp task trouble understanding whether preprocess text embedded e g word vec goal use word embeddings feature nn classify text topic topic perform event extraction document topic using second nn first instinct preprocess removing stopwords lemmatizing stemming etc learn nn bit realize applied natural language cbow skip gram model would fact require whole set word present able predict word context one would need know actual context reduced form context normalizing right actual sequence po tag seems key human feeling prediction word found guidance online still curious know community think recent commonly accepted best practice regarding punctuation stemming lemmatizing stopwords number lowercase etc better general process little possible heavier side normalize text trade thought better remove punctuation e g spanish remove accent convey contextual information change written number numeric lowercase everything useful entity extraction stemming lemmatizing doe sound right
How to replace tokens (words) with stemmed versions of words from my own table?,"<p>I got data like this (simplified):</p>

<pre><code>library(quanteda)
</code></pre>

<p>sample data</p>

<pre><code>myText &lt;- c(""ala ma kotka"", ""kasia ma pieska"")  
myDF &lt;- data.frame(myText)
myDF$myText &lt;- as.character(myDF$myText)
</code></pre>

<p>tokenization</p>

<pre><code>tokens &lt;- tokens(myDF$myText, what = ""word"",  
             remove_numbers = TRUE, remove_punct = TRUE,
             remove_symbols = TRUE, remove_hyphens = TRUE)
</code></pre>

<p>stemming with my own data
sample dictionary</p>

<pre><code>Origin &lt;- c(""kot"", ""pies"")
Word &lt;- c(""kotek"",""piesek"")

myDict &lt;- data.frame(Origin, Word)

myDict$Origin &lt;- as.character(myDict$Origin)
myDict$Word &lt;- as.character(myDict$Word)
</code></pre>

<p>what i got</p>

<pre><code>tokens[1]
[1] ""Ala""   ""ma""    ""kotka""
</code></pre>

<p>what i would like to get</p>

<pre><code>tokens[1]
[1] ""Ala""   ""ma""    ""kot""
tokens[2]
[1] ""Kasia""   ""ma""    ""pies""
</code></pre>
",Preprocessing of the text & Tokenization,replace token word stemmed version word table got data like simplified sample data tokenization stemming data sample dictionary got would like get
"error in initCoreNLP , specially &quot;annoators&quot;","<p>I have used coreNLP package &amp; stanford Parser thorugh rJAVA,NLP,openNLP,coreNLP package</p>

<p>Here is my code</p>

<pre><code>sent_token_annotator &lt;- Maxent_Sent_Token_Annotator()
word_token_annotator &lt;- Maxent_Word_Token_Annotator()
parse_annotator &lt;- Parse_Annotator()
initCoreNLP(mem = ""8g"", annotators = c(""tokenize"", ""ssplit"",""pos"",""lemma""))
</code></pre>

<p>during yesterday, everything is work, </p>

<p>but today, It dosen't work suddenly. showing :</p>

<p><strong>Error in initCoreNLP(mem = ""8g"", annotators = c(""tokenize"", ""ssplit"",  : 
  unused argument (annotators = c(""tokenize"", ""ssplit"", ""pos"", ""lemma""))</strong></p>

<p>In my code, first 3 lines are succesfully run, but last line makes error </p>

<p>I tried to check memory ( CPU i5 : ram : 8gb),  reinstall R &amp;R studio. </p>

<p>also, same code in other computer, thie error has apeeared too. 
Is there some error in code? 
or some error in NLP server? 
I cannot solve this problem. 
how can I solve it?</p>
",Preprocessing of the text & Tokenization,error initcorenlp specially annoators used corenlp package stanford parser thorugh rjava nlp opennlp corenlp package code yesterday everything work today dosen work suddenly showing error initcorenlp mem g annotator c tokenize ssplit unused argument annotator c tokenize ssplit po lemma code first line succesfully run last line make error tried check memory cpu ram gb reinstall r r studio also code computer thie error ha apeeared error code error nlp server solve problem solve
Remove stop words from the parsed content using OpenNLP,"<p>I have parsed the document using OpenNLP parser code provided in this <a href=""http://www.programcreek.com/2012/05/opennlp-tutorial/"" rel=""nofollow"">link</a> and I got the following output:</p>

<pre><code>(TOP (S (NP (NN Programcreek)) (VP (VBZ is) (NP (DT a) (ADJP (RB very) (JJ huge) (CC and) (JJ useful)) (NN website)))))
</code></pre>

<p>From this I want to extract only meaningful words, meaning I want to remove all stopwords because I want to do classification further based on these meaningful words. Can you please suggest to me how to remove stopwords from the parsed output?</p>

<p>Finally I want to get the below output</p>

<pre><code>   (TOP (S (NP (NN Programcreek)) (JJ useful)) (NN website)))))
</code></pre>

<p>Please help me with this, if it is not possible with OpenNLP then suggest me any other Java library for natural language processing. Because my main aim is to parse the document and get the meaningful words only.</p>
",Preprocessing of the text & Tokenization,remove stop word parsed content using opennlp parsed document using opennlp parser code provided link got following output want extract meaningful word meaning want remove stopwords want classification based meaningful word please suggest remove stopwords parsed output finally want get output please help possible opennlp suggest java library natural language processing main aim parse document get meaningful word
"What is the efficient way to drop stop words (e.g., of a the etc.) from a text using Java","<p>Is there a way to drop stop words (e.g., 'of' 'a' 'the' etc.) before using a JAVA based document classifiers (such as OpenNLP) etc. Or if you are doing it yourself (with JAVA) what could be the most efficient way to do it (Given that string comparison is inefficient). Also, Given that each document itself is not that big, i.e.,on average around 100 words, but the number of documents is assumed to be large. </p>

<pre><code>E.g., 
// Populate the stop words to a list
List&lt;String&gt; stopWordsList = ArrayList&lt;&gt;();

// Iterate through a list of documents
String currentDoc = getCurrentDoc();

String[] wordsArray = currentDoc.split("" "");    

 for ( String word : wordsArray ) {

      if (stopWordsList.contains(word)){
           // Drop it
      }
  }
</code></pre>
",Preprocessing of the text & Tokenization,efficient way drop stop word e g etc text using java way drop stop word e g etc using java based document classifier opennlp etc java could efficient way given string comparison inefficient also given document big e average around word number document assumed large
Sentence matching with gensim word2vec: manually populated model doesn&#39;t work,"<p>I'm trying to solve a problem of sentence comparison using naive approach of summing up word vectors and comparing the results. My goal is to match people by interest, so the dataset consists of names and short sentences describing their hobbies. The batches are fairly small, few hundreds of people, so i wanted to give it a try before digging into doc2vec.</p>

<p>I prepare the data by cleaning it completely, removing stop words, tokenizing and lemmatizing. I use pre-trained model for word vectors which returns adequate results when finding similarities for some test words. Also tried summing up the sentence words to find similarities in the original model - the matches do make sense. The similarities would be around general sense of the phrase.</p>

<p>For sentence matching I'm trying the following: create an empty model</p>

<pre><code>b = gs.models.Word2Vec(min_count=1, size=300, sample=0, hs=0)
</code></pre>

<p>Build vocab out of names (or person id's), no training</p>

<pre><code>#first create vocab with an empty vector
test = [['test']]
b.build_vocab(test)
b.wv.syn0[b.wv.vocab['test'].index] = b.wv.syn0[b.wv.vocab['test'].index]*0

#populate vocab from an array
b.build_vocab([personIds], update=True)
</code></pre>

<p>Summ each sentence's word vectors and store the results into the model for a corresponding id</p>

<pre><code>#sentences are pulled from pandas dataset df. 'a' is a pre-trained model i use to get vectors for each word

def summ(phrase, start_model):
    '''
    vector addition function
    '''
    #starting with a vector of 0's
    sum_vec = start_model.word_vec(""cat_NOUN"")*0
    for word in phrase:
        sum_vec += start_model.word_vec(word)
    return sum_vec

for i, row in df.iterrows():
    try:
        personId = row[""ID""]
        summVec = summ(df.iloc[i,1],a)
        #updating syn0 for each name/id in vocabulary
        b.wv.syn0[b.wv.vocab[personId].index] = summVec
    except:
        pass
</code></pre>

<p>I understand that i shouldn't be expecting much accuracy here, but the t-SNE print doesn't show any clustering whatsoever. Finding similarities method also fails to find matches (&lt;0.2 similarity coefficient basically for everything). [<img src=""https://i.sstatic.net/w81nX.png"" alt=""]plot of the entire model[1]""></p>

<p>Wondering if anyone has an idea of where did i go wrong? Is my approach valid at all?</p>
",Preprocessing of the text & Tokenization,sentence matching gensim word vec manually populated model work trying solve problem sentence comparison using naive approach summing word vector comparing result goal match people interest dataset consists name short sentence describing hobby batch fairly small hundred people wanted give try digging doc vec prepare data cleaning completely removing stop word tokenizing lemmatizing use pre trained model word vector return adequate result finding similarity test word also tried summing sentence word find similarity original model match make sense similarity would around general sense phrase sentence matching trying following create empty model build vocab name person id training summ sentence word vector store result model corresponding id understand expecting much accuracy sne print show clustering whatsoever finding similarity method also fails find match similarity coefficient basically everything wondering anyone ha idea go wrong approach valid
Python match word to word list after removing repeating characters,"<p>I have a list of words with positive and negative sentiment e.g. <code>['happy', 'sad']</code></p>

<p>Now when processing tweets I'm removing repeating characters like this (allowing only 2 repetitions):</p>

<pre><code>happpppyyy -&gt; happyy

saaad -&gt; saad
</code></pre>

<p>The check if e.g. <code>saad</code> is part of the word list should now return <code>True</code> because it is similar to <code>sad</code>. </p>

<p>How can I implement this behaviour?</p>
",Preprocessing of the text & Tokenization,python match word word list removing repeating character list word positive negative sentiment e g processing tweet removing repeating character like allowing repetition check e g part word list return similar implement behaviour
NLP classification training model,"<p>I am trying to train a model to classify tweets using opennlp. My question is should I perform tokenization, stop word removal etc on the tweets which I am using for training the model or should I use the tweet directly without performing the sanitization? </p>
",Preprocessing of the text & Tokenization,nlp classification training model trying train model classify tweet using opennlp question perform tokenization stop word removal etc tweet using training model use tweet directly without performing sanitization
Entity over-generalisation on Api.ai,"<p>We’ve been having a great deal of difficulty with chatbot entities over-generalising on Api.ai, i.e. returning values that have not been specified for that entity when using the “Define Synonyms” feature on custom entities, even when the “Allow automated expansion” flag is turned off.</p>

<p>Our key example is an entity we use for confirming a user choice called confirm_accept. We had an entry: “that’s it”, with synonyms: “thats it”, “that is it”, “that’s it thanks”, “thats it thanks”, “that is it thanks”. This entity value was being returned unexpectedly in expressions where just a stray “it” was appearing.</p>

<p>In general, we have seen a lot of inappropriate entity generalisation which seems to indicate there is some form of stop word removal and stemming/lemmatization going on during entity identification... and which can’t be turned off. </p>

<p>This returns poor entity classifications, making it difficult to create entities for which very precise values are important, e.g. where a single word or character can make a big difference in meaning. Our key use case involves a lot of address processing, so it is important we get back only values we have specified.</p>

<p>Types of over-generalisations we’ve seen include:</p>

<ul>
<li>inappropriate identification of determiners (a, an, the, this, that, etc.) as part of entities: as in “it” returning “that’s it”</li>
<li>stemmed words: as in stray mentions of “driving”, returning “drive” (a valid street type entity)</li>
<li>inappropriate plural stems: a stray mention of “children” returning “child”, or a stray “will” returning “wills” (which in our case “child” and “wills” are street name entities, so we don’t want “children” or “will” to be returned)</li>
</ul>

<p>This is currently making it difficult to create a production quality chatbot using the Api.ai service.</p>

<p>Anyone had more luck at either getting a response from Api.ai or solving the over-generalisation problem?</p>
",Preprocessing of the text & Tokenization,entity generalisation api ai great deal difficulty chatbot entity generalising api ai e returning value specified entity using define synonym feature custom entity even allow automated expansion flag turned key example entity use confirming user choice called confirm accept entry synonym thats thanks thats thanks thanks entity value wa returned unexpectedly expression stray wa appearing general seen lot inappropriate entity generalisation seems indicate form stop word removal stemming lemmatization going entity identification turned return poor entity classification making difficult create entity precise value important e g single word character make big difference meaning key use case involves lot address processing important get back value specified type generalisation seen include inappropriate identification determiner etc part entity returning stemmed word stray mention driving returning drive valid street type entity inappropriate plural stem stray mention child returning child stray returning case child street name entity want child returned currently making difficult create production quality chatbot using api ai service anyone luck either getting response api ai solving generalisation problem
why stop word removal be null? (php),"<p>I'm the beginner NLP programmer in PHP.
I just want to discuss about the stop word removal.</p>

<p>this is my practice:</p>

<p>I have the following declaration of a variable <code>$words = ""he's the young man"";</code></p>

<p>and then I remove the common words like this</p>

<pre><code> $common_words = $this-&gt;common_words();
 $ncwords = preg_replace('/\b('.implode('|',$common_words).')\b/','',$data); 
 // I have save the array common_words in another function
</code></pre>

<p>and I explode my no common words</p>

<pre><code>$a_ncwords=explode("" "", $ncwords);
</code></pre>

<p>But, when I print <code>$a_ncwords</code>, like so <code>print_r($a_ncwords);</code></p>

<p>I get a result like this:</p>

<pre><code>Array ( [0] =&gt; [1] =&gt; [2] =&gt; young [3] =&gt; man )
</code></pre>

<p>why are the <code>index[0]</code> and <code>index[1]</code> array values null?</p>
",Preprocessing of the text & Tokenization,stop word removal null php beginner nlp programmer php want discus stop word removal practice following declaration variable remove common word like explode common word print like get result like array value null
Fuziness In UIMA ruta,"<p>Is there any option of fuzziness in case of word matching, or ignoring some special cases.</p>
<p>For ex:</p>
<pre><code>STRINGLIST AMIMALLIST = {&quot;LION&quot;,&quot;TIGER&quot;,&quot;MONKEY&quot;};
DECLARE ANIMAL;


Document {-&gt; MARKFAST(ANIMAL, AMIMALLIST, true)};
</code></pre>
<p>I need to match words with list in case I face some special character like</p>
<p><strong>Tiger-</strong> or <strong>MONKEY$</strong></p>
<p>According to <a href=""https://uima.apache.org/d/ruta-current/tools.ruta.book.html"" rel=""nofollow noreferrer"">documentation</a> There are different evaluator any idea how to use?
Or can I use <strong>SCORE</strong> or <strong>MARKSCORE</strong></p>
",Preprocessing of the text & Tokenization,fuziness uima ruta option fuzziness case word matching ignoring special case ex need match word list case face special character like tiger monkey according documentation different evaluator idea use use score markscore
NLP - When to lowercase text during preprocessing,"<p>I want to build a model for language modelling, which should predict the next words in a sentence, given the previous word(s) and/or the previous sentence.</p>

<p><strong>Use case:</strong> I want to automate writing reports. So the model should automatically complete the sentence I am writing. Therefore, it is important that nouns and the words at the beginning of a sentence are capitalized. </p>

<p><strong>Data</strong>: The data is in German and contains a lot of technical jargon.</p>

<p>My text corpus is in <strong>German</strong> and I am currently working on the preprocessing. Because my model should predict gramatically correct sentences I have decided to use/not use the following preprocessing steps:</p>

<ul>
<li>no stopword removal</li>
<li><p>no lemmatization</p></li>
<li><p>replace all expressions with numbers by NUMBER</p></li>
<li>normalisation of synonyms and abbreviations </li>
<li>replace rare words with RARE</li>
</ul>

<p>However, I am not sure whether to convert the corpus to lowercase. When searching the web I found different opinions. Although lower-casing is quite common it will cause my model to wrongly predict the capitalization of nouns, sentence beginnings etc.</p>

<p>I also found the idea to convert only the words at the beginning of a sentence to lower-case on the following <a href=""https://nlp.stanford.edu/IR-book/html/htmledition/capitalizationcase-folding-1.html"" rel=""noreferrer"">Stanford page</a>.</p>

<p>What is the best strategy for this use-case? Should I convert the text to lower-case and change the words to the correct case after prediction? Should I leave the capitalization as it is? Should I only lowercase words at the beginning of a sentence?</p>

<p>Thanks a lot for any suggestions and experiences!</p>
",Preprocessing of the text & Tokenization,nlp lowercase text preprocessing want build model language modelling predict next word sentence given previous word previous sentence use case want automate writing report model automatically complete sentence writing therefore important noun word beginning sentence capitalized data data german contains lot technical jargon text corpus german currently working preprocessing model predict gramatically correct sentence decided use use following preprocessing step stopword removal lemmatization replace expression number number normalisation synonym abbreviation replace rare word rare however sure whether convert corpus lowercase searching web found different opinion although lower casing quite common cause model wrongly predict capitalization noun sentence beginning etc also found idea convert word beginning sentence lower case following stanford page best strategy use case convert text lower case change word correct case prediction leave capitalization lowercase word beginning sentence thanks lot suggestion experience
Existing API for NLP in C++?,"<p>Is/are there existing C++ NLP API(s) out there? The closest thing I have found is <code>CLucene</code>, a port of <code>Lucene</code>. However, it seems a bit obsolete and the documentation is far from complete.</p>

<p>Ideally, this/these API(s) would permit tokenization, stemming and PoS tagging.</p>
",Preprocessing of the text & Tokenization,existing api nlp c existing c nlp api closest thing found port however seems bit obsolete documentation far complete ideally api would permit tokenization stemming po tagging
Template language for natural language text mutations,"<p>Stuck upon a rather trivial task that seems to lead to a wider problem.</p>

<p>Need to be able to generate light variations of a same short text. Some word forms depend on the speaker's gender, some can be replaced with synonyms.</p>

<p>Pseudo code:</p>

<pre><code>I {random:decided|made up my mind} to {random:try|test|give a try to}
this {new|fresh} {cool|awesome} {service|web service|online tool}.
</code></pre>

<p>I'm looking for an ""industry standard"" templating language to describe such texts and possible variations. Thinking further, I might want <em>global</em> variables (like the gender one), <em>cross-links</em> for dependencies picked earlier in the sentence.</p>

<p>This looks close to regular expressions syntax. Ideally it would be more readable/writable by non-programmers.</p>

<p>Perhaps the problem is well-known, with a solid state solution like some programming language specifically for the task?</p>
",Preprocessing of the text & Tokenization,template language natural language text mutation stuck upon rather trivial task seems lead wider problem need able generate light variation short text word form depend speaker gender replaced synonym pseudo code looking industry standard templating language describe text possible variation thinking might want global variable like gender one cross link dependency picked earlier sentence look close regular expression syntax ideally would readable writable non programmer perhaps problem well known solid state solution like programming language specifically task
MarkLogic - Tokenize Search Phrase based on XML Field as a dictionary of phrases,"<p>I have a list of ""known phrases"" stored in an XML Document under an element named <strong>label</strong>. I am trying to figure out how to write a function, that can tokenize a search phrase into all of its <strong>label</strong> pieces (if available).</p>

<p>For instance. I have a Label for North Korea, and ICBM.</p>

<p>If the user types in North Korea ICBM, I would expect to get back two tokens, one for each label as opposed to North and Korea and ICBM.
In another example if the user types in New York City, I would expect only one token (label) of ""New York City"". </p>

<p>If there is no labels found, it would return the default tokenization of each word. </p>

<p>I tried to start writing this, but am not sure how to do this properly without a while loop facility, and am pretty new to xQuery in general. </p>

<p>The code below was how I started, but quickly realized it would not work for scaling out search terms.
Basically, it checks to see if the full phrase is in the <strong>Label</strong> fields. If it is not, it starts to strip away from the back of the search phrase checking what's left for a label.</p>

<pre><code>  let $label-query := cts:element-value-query(fn:QName('','label'), $searchTerm, ('case-insensitive', 'whitespace-sensitive'))

  let $results := cts:search(fn:collection('typea'),$label-query)

  let $test :=
    if (fn:empty($results)) then
        let $tokens := (fn:tokenize($searchTerm, "" ""))
        let $tokenCount := fn:count($tokens)
            let $lastWord := $tokens[last()]
            let $firstPhrase := $tokens[position() ne (last())]

            let $_ :=
                if (fn:count($firstPhrase)  = 1 ) then
                    ()
                else
                      let $label-query2 := cts:element-value-query(fn:QName('','label'), $firstPhrase, ('case-insensitive', 'whitespace-sensitive'))
                      let $results2 := cts:search(fn:collection('typea'),$label-query2)
                        return
                            if (fn:empty($results2)) then
                                xdmp:log('second empty')
                            else
                                xdmp:log($results2)

        let $l := xdmp:log(  $firstPhrase  )
        return $tokens

    else
        let $_ := xdmp:log('full')
        return element {'result'} {$results}
</code></pre>

<p>Does anyone have any advice how I could implement this recursively or perhaps any alternate strategies. I am essentially trying to say, break this sentence up into all of the phrases found that exist in the <strong>Label</strong> fields of the <strong>typea</strong> collection. If there are no labels found, tokenize by word. </p>

<p>Thanks I look forward to your guidance. </p>

<hr>

<p>Update to help clarify my ultimate intention. </p>

<p>Below is the document referring to North Korea. </p>

<p><a href=""https://i.sstatic.net/0YybE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0YybE.png"" alt=""enter image description here""></a></p>

<p>The goal is to parse the search phrase, and use extra information found in these documents to aid in search.</p>

<p>Meaning if the person types in <strong>DPRK</strong> or <strong>North Korea</strong> they should both search the same way. It should also include <strong>Narrower</strong> Labels as an Or Condition on the search, and will more likely than not be updated to include other relationships that will also be included in search. (IE: Kim Jong Un is Notably Associated with North Korea.)</p>

<p>So in short I would like to reconcile the multi phrase search terms using the <strong>label</strong> field, and then if it was found, use the information from <strong>all labels</strong> + the <strong>narrower labels</strong> as well from that document. </p>

<hr>

<p>Edit 2: Trying to use <code>cts:highlight</code> to get the phrases. Once I have the phrases I will do an element lookup to get to the right document, and then get the associated documents data for submission to query building. </p>

<p>The issue now is that the cts:highlight does not always return the full phrase under one <code>&lt;phrase&gt;</code> tag. </p>

<pre><code>let $phrases :=   cts:highlight(&lt;nod&gt;New York City FC&lt;/nod&gt;,      cts:or-query((//label)),      &lt;phrase&gt;{ $cts:text }&lt;/phrase&gt;)
</code></pre>

<p><a href=""https://i.sstatic.net/Q3E1z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Q3E1z.png"" alt=""enter image description here""></a></p>
",Preprocessing of the text & Tokenization,marklogic tokenize search phrase based xml field dictionary phrase list known phrase stored xml document element named label trying figure write function tokenize search phrase label piece available instance label north korea icbm user type north korea icbm would expect get back two token one label opposed north korea icbm another example user type new york city would expect one token label new york city label found would return default tokenization word tried start writing sure properly without loop facility pretty new xquery general code wa started quickly realized would work scaling search term basically check see full phrase label field start strip away back search phrase checking left label doe anyone advice could implement recursively perhaps alternate strategy essentially trying say break sentence phrase found exist label field typea collection label found tokenize word thanks look forward guidance update help clarify ultimate intention document referring north korea goal parse search phrase use extra information found document aid search meaning person type dprk north korea search way also include narrower label condition search likely updated include relationship also included search ie kim jong un notably associated north korea short would like reconcile multi phrase search term using label field wa found use information label narrower label well document edit trying use get phrase phrase element lookup get right document get associated document data submission query building issue ct highlight doe always return full phrase one tag
Stemming words in a Python list,"<p>Have a list ""l"" with distinct words like this:</p>

<pre><code>'gone',
'done',
'crawled',
'laughed',
'cried'
</code></pre>

<p>I try to apply stemming on this list just that way:</p>

<pre><code>from stemming.porter2 import stem
l = [[stem(word) for word in sentence.split(' ')] for sentence in l]
</code></pre>

<p>But nothing seems to happen and nothing changes. What am I doing wrong with the stemming procedure?</p>
",Preprocessing of the text & Tokenization,stemming word python list list l distinct word like try apply stemming list way nothing seems happen nothing change wrong stemming procedure
Nltk&#39;s wordnet lemmatizer not lemmatizing all words,"<p>I'm trying to lemmatize words in a text. Like for example 'pickled' should turn to 'pickle', 'ran' to 'run', 'raisins' to 'raisin' etc.</p>

<p>I'm using nltk's <code>WordNet Lemmatizer</code> as follows:</p>

<pre><code>from nltk.stem import WordNetLemmatizer
&gt;&gt;&gt; 
&gt;&gt;&gt; lem = WordNetLemmatizer()
&gt;&gt;&gt; print(lem.lemmatize(""cats""))
cat
&gt;&gt;&gt; print(lem.lemmatize(""pickled""))
pickled
&gt;&gt;&gt; print(lem.lemmatize(""ran""))
ran
</code></pre>

<p>So, as you can see for <code>'pickled'</code> and <code>'ran'</code>, the output isn't coming as expected. How to get <code>'pickle'</code> and <code>'run'</code> for these without having to specify <code>'v'</code> (verb) etc. for the words.</p>
",Preprocessing of the text & Tokenization,nltk wordnet lemmatizer lemmatizing word trying lemmatize word text like example pickled turn pickle ran run raisin raisin etc using nltk follows see output coming expected get without specify verb etc word
NLTK - Download all nltk data except corpara from command line without Downloader UI,"<p>We can download all nltk data using:</p>

<pre><code>&gt; import nltk
&gt; nltk.download('all')
</code></pre>

<p>Or specific data using:</p>

<pre><code>&gt; nltk.download('punkt')
&gt; nltk.download('maxent_treebank_pos_tagger')
</code></pre>

<p>But I want to download all data except 'corpara' files, 
for example -  all chunkers, grammers, models, stemmers, taggers, tokenizers, etc</p>

<p>is there any way to do so without Downloader UI? something like, </p>

<pre><code>&gt; nltk.download('all-taggers')
</code></pre>
",Preprocessing of the text & Tokenization,nltk download nltk data except corpara command line without downloader ui download nltk data using specific data using want download data except corpara file example chunkers grammers model stemmer tagger tokenizers etc way without downloader ui something like
nltk word_tokenize: why do sentence tokenization before word tokenization?,"<p>As noted in the <a href=""https://github.com/nltk/nltk/blob/develop/nltk/tokenize/__init__.py#L113"" rel=""nofollow noreferrer"">source code</a>, <code>word_tokenize</code> runs a sentence tokenizer(Punkt) before running the word tokenizer(Treebank):</p>

<pre class=""lang-python prettyprint-override""><code># Standard word tokenizer.
_treebank_word_tokenizer = TreebankWordTokenizer()

def word_tokenize(text, language='english', preserve_line=False):
    """"""
    Return a tokenized copy of *text*,
    using NLTK's recommended word tokenizer
    (currently an improved :class:`.TreebankWordTokenizer`
    along with :class:`.PunktSentenceTokenizer`
    for the specified language).
    :param text: text to split into words
    :param text: str
    :param language: the model name in the Punkt corpus
    :type language: str
    :param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.
    :type preserver_line: bool
    """"""
    sentences = [text] if preserve_line else sent_tokenize(text, language)
    return [token for sent in sentences
            for token in _treebank_word_tokenizer.tokenize(sent)]
</code></pre>

<p>What is the benefit of doing sentence tokenization prior to word tokenization? </p>
",Preprocessing of the text & Tokenization,nltk word tokenize sentence tokenization word tokenization noted source code run sentence tokenizer punkt running word tokenizer treebank benefit sentence tokenization prior word tokenization
Stemming full strings on Python,"<p>I need to perform stemming on portuguese strings.  To do so, i'm tokening the string using nltk.word_tokenize() function a then stemming each word individually. After that, I rebuild the string. It's working, but not performing well. How can i make it faster? The string length is about 2 million words.</p>

<pre><code>    tokenAux=""""
    tokens = nltk.word_tokenize(portugueseString)
        for token in tokens:
            tokenAux = token
            tokenAux = stemmer.stem(token)    
            textAux = textAux + "" ""+ tokenAux
    print(textAux)
</code></pre>

<p>Sorry for bad english and thanks!</p>
",Preprocessing of the text & Tokenization,stemming full string python need perform stemming portuguese string tokening string using nltk word tokenize function stemming word individually rebuild string working performing well make faster string length million word sorry bad english thanks
NLP: Within Sentence Segmentation / Boundary Detection,"<p>I am interested if there are libraries that break a sentence into small pieces based on content.</p>

<p>E.g. </p>

<blockquote>
  <p>input: sentence: ""During our stay at the hotel we had a clean room,
  very nice bathroom, breathtaking view out the window and a delicious
  breakfast in the morning.""</p>
  
  <p>output: list of sentence segments: [""During our stay at the hotel"" ,
  ""we had a clean room"" , ""very nice bathroom"" , ""breathtaking view out
  the window"" , ""and a delicious breakfast in the morning.""]</p>
</blockquote>

<p>So basically I am looking for a <em>within sentence boundary detection/ segmentation</em> based on <em>meaning</em>. <strong>My goal is to take a sentence and separate it into bit of pieces that have their own 'meaning' without the rest of sentence.</strong> </p>

<p>By no way I am interested in sentence-boundary-detection, since any one can a dozen of those, but that does not work for within sentence segmentation.</p>

<p>Thank you in advance</p>
",Preprocessing of the text & Tokenization,nlp within sentence segmentation boundary detection interested library break sentence small piece based content e g input sentence stay hotel clean room nice bathroom breathtaking view window delicious breakfast morning output list sentence segment stay hotel clean room nice bathroom breathtaking view window delicious breakfast morning basically looking within sentence boundary detection segmentation based meaning goal take sentence separate bit piece meaning without rest sentence way interested sentence boundary detection since one dozen doe work within sentence segmentation thank advance
How to reduce &quot;add up&quot; to &quot;add&quot;?,"<p>I'm new to NLP; Searching for something that reduces some ""words"" like ""add up"",""sign up"",""log in"", etc. to their main part ""add"", ""sign"", and ""log"". </p>

<p>I tried Lemmatization and Stemming, but none of them brought me the results that I wanted to have.</p>

<p>Thanks for your help :)</p>
",Preprocessing of the text & Tokenization,reduce add add new nlp searching something reduces word like add sign log etc main part add sign log tried lemmatization stemming none brought result wanted thanks help
How to standardize the bag of words for train and test?,"<p>I am trying to classify based on the bag-of-words model from NLP.</p>

<ol>
<li>Did pre-processing of the train data using NLTK (punctuation, stop words removal, lower case, stemming etc.)</li>
<li>Created tf-idf matrix for train.</li>
<li>Did pre-processing of test.</li>
<li>Created tf-idf matrix for test data.</li>
<li>Train and Test data have different bag of words so the no of features are different, so we cannot use a classification algo like knn.</li>
<li>I merged the train and test data together and created the tf-idf matrix. This solved the above problem of different bag of words. But the resultant matrix was too huge to process.</li>
</ol>

<p>Here are my questions:</p>

<ol>
<li>Is there a way to create the exact bag of words for train and test?</li>
<li>If there is not and my approach of adding train and test is correct, should I go for a dimensionality reduction algo like LDA? </li>
</ol>
",Preprocessing of the text & Tokenization,standardize bag word train test trying classify based bag word model nlp pre processing train data using nltk punctuation stop word removal lower case stemming etc created tf idf matrix train pre processing test created tf idf matrix test data train test data different bag word feature different use classification algo like knn merged train test data together created tf idf matrix solved problem different bag word resultant matrix wa huge process question way create exact bag word train test approach adding train test correct go dimensionality reduction algo like lda
Matching TV and Movie File names with NLP/Machine Learning?,"<p>So i've wondered if there would be a way to tokenize/tag TV or Movie Files using NLP/Machine Learing.</p>

<p>I know there are a lot of regexp approaches out there which do this already but shouldn't it be possible to get this done with NLP/Machine Learning as well?</p>

<p>Example: 
<code>The.Heart.Guy.S01E07.Die.Belastungsprobe.German.DL.720p.HDTV.x264-GDR</code></p>

<p>Should be something like:</p>

<p><code>The Heart Guy SHOW-NAME
1 SEASON
7 EPISODE
Die Belastungsprobe EP-NAME
German DL LANGUAGE
720p RESOLUTION
HDTV SOURCE
x264 CODEC
GDR GROUP</code></p>

<p>Anyone ever tried something like this? Or any hints where one should start or if it's even possible to get something like this working.</p>
",Preprocessing of the text & Tokenization,matching tv movie file name nlp machine learning wondered would way tokenize tag tv movie file using nlp machine learing know lot regexp approach already possible get done nlp machine learning well example something like anyone ever tried something like hint one start even possible get something like working
Natural Language Processing using apache OpenNLP,"<p>How do I find the accuracy of a data in % in Natural Language Processing?</p>

<p>Trying to find accuracy on movie review data</p>

<p>Once the fresh movie review comes, to decide whether the review is positive or not, process followed as below </p>

<p>For training purpose used movie review file of size 400 MB which has 200 K + review comments and its corresponding ratings.</p>

<p>Based on the ratings in the file implemented the bag of words approach
for e.g.: if rating > 5 then put it into the good bag else bad bag</p>

<p>While implementing bag of words approach used below NLP techniques</p>

<ol>
<li>Tokenization</li>
<li>Stop word Removal</li>
<li>Stemming</li>
<li>Special character removal etc</li>
</ol>

<p>Once the bag is ready, with fresh review comments using NLP techniques as stated above then looking up in the bags calculating mean on good and bad bags whichever is greater based on that deciding whether the fresh review is positive or not.</p>

<p>Please suggest how to figure out the accuracy in %?</p>
",Preprocessing of the text & Tokenization,natural language processing using apache opennlp find accuracy data natural language processing trying find accuracy movie review data fresh movie review come decide whether review positive process followed training purpose used movie review file size mb ha k review comment corresponding rating based rating file implemented bag word approach e g rating put good bag else bad bag implementing bag word approach used nlp technique tokenization stop word removal stemming special character removal etc bag ready fresh review comment using nlp technique stated looking bag calculating mean good bad bag whichever greater based deciding whether fresh review positive please suggest figure accuracy
How to match a SVO pattern with Textacy,"<p>How do you use Textacy's <code>pos_regex_match()</code> method to find subject-verb-object triples using their pseudo-regular-expression syntax? And yes, I'm aware of <code>textacy.extract.subject_verb_object_triples()</code>, but this function is very inaccurate and finds very little, so I'm attempting to build something more robust.</p>

<p>For the text:</p>

<pre><code>text = ""He recently wrote the sky is full of stars.""
</code></pre>

<p>I'm trying:</p>

<pre><code>svo_pattern = r'&lt;DET&gt;? &lt;NOUN|PROPN|PRON&gt;+ &lt;VERB&gt;?&lt;ADV&gt;*&lt;VERB&gt;+ &lt;DET&gt;? &lt;NOUN|PROPN|PRON&gt;+'
doc = textacy.Doc(text)
for sent in sents:
    matches = list(textacy.extract.pos_regex_matches(sent, svo_pattern))
    print(matches)
</code></pre>

<p>but it doesn't find anything. What's the flaw in my pattern? I've played with several variations of it, but nothing matches.</p>
",Preprocessing of the text & Tokenization,match svo pattern textacy use textacy method find subject verb object triple using pseudo regular expression syntax yes aware function inaccurate find little attempting build something robust text trying find anything flaw pattern played several variation nothing match
How to resolve mkcls taking up lots of memory and time for word alignment using GIZA++?,"<p>I am using the <a href=""http://code.google.com/p/giza-pp/"" rel=""nofollow"">GIZA++</a> for aligning word from the bitexts from the <a href=""http://www.statmt.org/europarl/"" rel=""nofollow"">Europarl corpus</a>.</p>

<p>Before i train the alignment model using <code>GIZA++</code>, i need to use the <a href=""http://www-i6.informatik.rwth-aachen.de/web/Software/mkcls.html"" rel=""nofollow"">mkcls</a> script to making classes that is necessary for Hidden Markov Model algorithm as such:</p>

<pre><code>mkcls -n10 -pcorp.tok.low.src -Vcorp.tok.low.src.vcb.classes
</code></pre>

<p>I have tried it with a small size 1000 lines corpus and it works properly and completed in a few minutes. <strong>Now i'm trying it on corpus with 1,500,000 lines and it's taking up 100% of one of the my CPU <code>(Six-Core AMD Opteron(tm) Processor 2431 × 12)</code></strong> </p>

<p>Before making the classes, i have taken the necessary step to tokenize, lower all upper cases and filter out lines with more than 40 words.</p>

<p><strong>Does anyone have similar experience on the <code>mkcls</code> for GIZA++? How is it solved? If anyone had done the same on the Europarl corpus, how long did it take you to run the <code>mkcls</code>?</strong></p>
",Preprocessing of the text & Tokenization,resolve mkcls taking lot memory time word alignment using giza using giza aligning word bitexts europarl corpus train alignment model using need use mkcls script making class necessary hidden markov model algorithm tried small size line corpus work properly completed minute trying corpus line taking one cpu making class taken necessary step tokenize lower upper case filter line word doe anyone similar experience giza solved anyone done europarl corpus long take run
Assign document to a category using document similarity,"<p>I'm developing a NLP project in python.</p>

<p>I'm getting ""conversation"" from social networks. A conversation is made up of post_text + comment_text + reply_text (with comment_text and reply_text as optional).</p>

<p>I've also a list of categories, arguments, and I want to ""connect"" conversation to an argument (or get a weight for each argument).</p>

<p>For each category, I get the summary on Wikipedia, using <code>wikipedia</code> python package. So, they represent my training documents (right?).</p>

<p>Now, I've writed down some steps to follow, but maybe I'm wrong.</p>

<ul>
<li>Each training document must be transformed to Vector Space Model. I've to remove stopwords and common words. So, I've a list of vocabulary.</li>
<li>Each conversation must be transformed to vector space model and each token must be assigned to its vocabulary index. I can save all vector space models in a matrix.</li>
<li>Now, I've to perform tf-idf (for example) on all matrix rows.

<ul>
<li>In tf-idf I've to calculate tf, idf and normalize matrix?</li>
</ul></li>
<li>So, each row represents tf-idf for each conversation. Now, I've to perform cosine-similarity (for example) to get similarity between each conversation and one training document. I've to iterate it to get similarity between conversations and each training document.</li>
</ul>

<p>What do you think about the steps? Is there any guide/how to/book I've to read to understand better this problem?</p>
",Preprocessing of the text & Tokenization,assign document category using document similarity developing nlp project python getting conversation social network conversation made post text comment text reply text comment text reply text optional also list category argument want connect conversation argument get weight argument category get summary wikipedia using python package represent training document right writed step follow maybe wrong training document must transformed vector space model remove stopwords common word list vocabulary conversation must transformed vector space model token must assigned vocabulary index save vector space model matrix perform tf idf example matrix row tf idf calculate tf idf normalize matrix row represents tf idf conversation perform cosine similarity example get similarity conversation one training document iterate get similarity conversation training document think step guide book read understand better problem
how to preserve number of records in word2vec?,"<p>I have 45000 text records in my dataframe. I wanted to convert those 45000 records into word vectors so that I can train a classifier on the word vector. I am not tokenizing the sentences. I just split the each entry into list of words.</p>

<p>After training word2vec model with 300 features, the shape of the model resulted in only 26000. How can I preserve all of my 45000 records ? </p>

<p>In the classifier model, I need all of those 45000 records, so that it can match 45000 output labels.</p>
",Preprocessing of the text & Tokenization,preserve number record word vec text record dataframe wanted convert record word vector train classifier word vector tokenizing sentence split entry list word training word vec model feature shape model resulted preserve record classifier model need record match output label
Regular expression to match part of a word,"<p>Is it possible to match a pattern of a word with a certain margin of error.
For example a words ""Tike"", ""Make"", ""Bake"", ""Tame"" fits the pattern of the word take ""Take"" with one misspelled character.</p>
",Preprocessing of the text & Tokenization,regular expression match part word possible match pattern word certain margin error example word tike make bake tame fit pattern word take take one misspelled character
Remove stopwords of a sentence using Rcpp,"<p>I would like to replace the words of a sentence based on the stopwords of another vector using <code>Rcpp</code>. An example in <code>R</code> follows:</p>

<pre><code>library(stringr)
library(tm)

x &lt;- ""I love eating burgers every day and night""
idVector &lt;- str_split(x,pattern = "" "")[[1]]
idVector &lt;- idVector[!idVector %in% stopwords(kind = ""en"")]
idVector &lt;- paste0(idVector, collapse = "" "")
print(idVector)
</code></pre>

<p>I would be interested to create a <code>Rcpp</code> function and perform a group by over the rows of a <code>data.table</code> in order to clean a huge amount of sentences. Using the current approach it takes significant amount of time.</p>

<p>Thanks in advance!</p>
",Preprocessing of the text & Tokenization,remove stopwords sentence using rcpp would like replace word sentence based stopwords another vector using example follows would interested create function perform group row order clean huge amount sentence using current approach take significant amount time thanks advance
How to remove stop words from documents in gensim?,"<p>I'm building a NLP chat application using Doc2Vec technique in Python using its <code>gensim</code> package. I have already done tokenizing and stemming. I want to remove the stop words (to test if it works better) from both the training set as well as the question which user throws. </p>

<p>Here is my code.</p>

<pre><code>import gensim
import nltk
from gensim import models
from gensim import utils
from gensim import corpora
from nltk.stem import PorterStemmer
ps = PorterStemmer()

sentence0 = models.doc2vec.LabeledSentence(words=[u'sampl',u'what',u'is'],tags=[""SENT_0""])
sentence1 = models.doc2vec.LabeledSentence(words=[u'sampl',u'tell',u'me',u'about'],tags=[""SENT_1""])
sentence2 = models.doc2vec.LabeledSentence(words=[u'elig',u'what',u'is',u'my'],tags=[""SENT_2""])
sentence3 = models.doc2vec.LabeledSentence(words=[u'limit', u'what',u'is',u'my'],tags=[""SENT_3""])
sentence4 = models.doc2vec.LabeledSentence(words=[u'claim',u'how',u'much',u'can',u'I'],tags=[""SENT_4""])
sentence5 = models.doc2vec.LabeledSentence(words=[u'retir',u'i',u'am',u'how',u'much',u'can',u'elig',u'claim'],tags=[""SENT_5""])
sentence6 = models.doc2vec.LabeledSentence(words=[u'resign',u'i',u'have',u'how',u'much',u'can',u'i',u'claim',u'elig'],tags=[""SENT_6""])
sentence7 = models.doc2vec.LabeledSentence(words=[u'promot',u'what',u'is',u'my',u'elig',u'post',u'my'],tags=[""SENT_7""])
sentence8 = models.doc2vec.LabeledSentence(words=[u'claim',u'can,',u'i',u'for'],tags=[""SENT_8""])
sentence9 = models.doc2vec.LabeledSentence(words=[u'product',u'coverag',u'cover',u'what',u'all',u'are'],tags=[""SENT_9""])
sentence10 = models.doc2vec.LabeledSentence(words=[u'hotel',u'coverag',u'cover',u'what',u'all',u'are'],tags=[""SENT_10""])
sentence11 = models.doc2vec.LabeledSentence(words=[u'onlin',u'product',u'can',u'i',u'for',u'bought',u'through',u'claim',u'sampl'],tags=[""SENT_11""])
sentence12 = models.doc2vec.LabeledSentence(words=[u'reimburs',u'guidelin',u'where',u'do',u'i',u'apply',u'form',u'sampl'],tags=[""SENT_12""])
sentence13 = models.doc2vec.LabeledSentence(words=[u'reimburs',u'procedur',u'rule',u'and',u'regul',u'what',u'is',u'the',u'for'],tags=[""SENT_13""])
sentence14 = models.doc2vec.LabeledSentence(words=[u'can',u'i',u'submit',u'expenditur',u'on',u'behalf',u'of',u'my',u'friend',u'and',u'famili',u'claim',u'and',u'reimburs'],tags=[""SENT_14""])
sentence15 = models.doc2vec.LabeledSentence(words=[u'invoic',u'bills',u'procedur',u'can',u'i',u'submit',u'from',u'shopper stop',u'claim'],tags=[""SENT_15""])
sentence16 = models.doc2vec.LabeledSentence(words=[u'invoic',u'bills',u'can',u'i',u'submit',u'from',u'pantaloon',u'claim'],tags=[""SENT_16""])
sentence17 = models.doc2vec.LabeledSentence(words=[u'invoic',u'procedur',u'can',u'i',u'submit',u'invoic',u'from',u'spencer',u'claim'],tags=[""SENT_17""])

# User asks a question.

document = input(""Ask a question:"")
tokenized_document = list(gensim.utils.tokenize(document, lowercase = True, deacc = True))
#print(type(tokenized_document))
stemmed_document = []
for w in tokenized_document:
    stemmed_document.append(ps.stem(w))
sentence19 = models.doc2vec.LabeledSentence(words= stemmed_document, tags=[""SENT_19""])

# Building vocab.
sentences = [sentence0,sentence1,sentence2,sentence3, sentence4, sentence5,sentence6, sentence7, sentence8, sentence9, sentence10, sentence11, sentence12, sentence13, sentence14, sentence15, sentence16, sentence17, sentence19]

#I tried to remove the stop words but it didn't work out as LabeledSentence object has no attribute lower.
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
          for document in sentences]
..
</code></pre>

<p>Is there a way I can remove stop words from <code>sentences</code> directly and get a new set of vocab without stop words ?</p>
",Preprocessing of the text & Tokenization,remove stop word document gensim building nlp chat application using doc vec technique python using package already done tokenizing stemming want remove stop word test work better training set well question user throw code way remove stop word directly get new set vocab without stop word
How to give more weight to Proper Nouns in scikit TfidfVectorizer,"<p>I am using <code>sci-kit</code>'s <code>TdidfVectorizer</code> to extract keywords from a list of scientific articles. There is an argument for stop_words, but I was wondering if I could give more weight/score to proper nouns such as ""Bohr"" or ""Japan"".</p>

<p>Will I have to implement my own custom <code>tfidf vectorizer</code> or can I still use this built in one?</p>

<pre><code>tf = TfidfVectorizer(strip_accents='ascii', 
                     analyzer='word',
                     ngram_range=(1,1),
                     min_df = 0,
                     stop_words = stopwords,
                     lowercase = True)
</code></pre>
",Preprocessing of the text & Tokenization,give weight proper noun scikit tfidfvectorizer using extract keywords list scientific article argument stop word wa wondering could give weight score proper noun bohr japan implement custom still use built one
Add spelling/grammatical error to data,"<p>Basically, I want to increase the dataset I already have, containing series of sentences, and to do so I planned to add grammatical/spelling error to each sentence in the dataset. The data set i have is completely clean, i.e, without any spelling/grammatical errors. How can that be done keeping in mind that not too many errors should be added to any particular sentence and hence prevent the change in its meaning/sense.</p>
",Preprocessing of the text & Tokenization,add spelling grammatical error data basically want increase dataset already containing series sentence planned add grammatical spelling error sentence dataset data set completely clean e without spelling grammatical error done keeping mind many error added particular sentence hence prevent change meaning sense
AI - String/Text Classification/Categorization (e.g. a string/text is classified as a company name),"<p>My problem is to filter out all the names of persons in a table, i.e. names of companies, schools, institutions will be left in the database.</p>

<p>I tried a simple solution wherein I was given a list of the name of companies, schools, etc. And I searched for the most common terms there. (Note: I did not search for the common strings in a name, since that would cost a lot). I assigned weight to those terms, and also to the most common substrings. With that, if the string has a corp, inc, school, univ in it then it's very highly possible that it's not a name of a person.</p>

<p>Now, my problem is how can I make it into an AI. Moreover, I will have to make it possible such that classifications of companies only, schools only, etc. will be easier.</p>

<p>For example</p>

<pre><code>XYZ Brewery Corporation -&gt; company
Harvard University -&gt; school
Department of Health -&gt; government agency
</code></pre>

<p>The only AI techniques I know are Naive-Bayes, K-Means, Hierarchical, FCM, ANN. Those techniques commonly get numerical values, so, I don't know how to make it into an AI. The only AI techniques that I know that handles strings extensively are Levenshtein, Stemming, Needleman-Wunch and Jaro-Winkler.</p>

<p>Is my first approach incorrect? How can incorporate the techniques that I know? Do I have to learn a new technique? I'm basically new to AI since I am still a student. However, this is not an assignment but it's for a company project (actually I am the only computer science major in our group, so it's very heavy on my part). By the way, if you are curious on what language I use, I am using C# since I am planning to make it just a stand-alone application and the users are using Windows.</p>
",Preprocessing of the text & Tokenization,ai string text classification categorization e g string text classified company name problem filter name person table e name company school institution left database tried simple solution wherein wa given list name company school etc searched common term note search common string name since would cost lot assigned weight term also common substring string ha corp inc school univ highly possible name person problem make ai moreover make possible classification company school etc easier example ai technique know naive bayes k mean hierarchical fcm ann technique commonly get numerical value know make ai ai technique know handle string extensively levenshtein stemming needleman wunch jaro winkler first approach incorrect incorporate technique know learn new technique basically new ai since still student however assignment company project actually computer science major group heavy part way curious language use using c since planning make stand alone application user using window
how to append the results in list?,"<p>I have CSV file contains reviews and I'm doing some NLP preprocess (tokenizing, stemming ..etc) I only have a problem in the final result. here is my code:</p>

<pre><code>def clean_doc(r):
            stop_free = "" "".join([i for i in r.lower().split() if i not in stop])
            punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
            normalized = "" "".join(lemma.lemmatize(word) for word in punc_free.split())
            stemmed = """".join([stemmer.stem(i) for i in normalized])
            return stemmed
##----------------------------------------------------------------------
with open('test.csv', 'r') as csvfile:
 data = csv.reader(csvfile, delimiter=',')
 stemmer = PorterStemmer()
 stop = set(stopwords.words('english'))
 exclude = set(string.punctuation) 
 lemma = WordNetLemmatizer()
 texts = []
 next(data)
 for row in data:
    r = row[1]
    texts.append(clean_doc(r))
 print(texts)
</code></pre>

<p>the variable <strong>(texts)</strong> doesn't print anything. can anyone tell me where am I wrong?..Thanks in advance</p>
",Preprocessing of the text & Tokenization,append result list csv file contains review nlp preprocess tokenizing stemming etc problem final result code variable text print anything anyone tell wrong thanks advance
"Unsure about word embeddings, POS re: using a Neural Net for NLP classification.","<p>Im planning on using an NN for sarcasm detection on a number of tweets. Im unsure of how to prepare the word embeddings I will train the NN on. If I tokenize the strings and tag emoticons, capitalisation, user tags, hashtags etc, how do i then combine the resulting strings with word embeddings? do i train the word embeddings on the resulting corpus of tweets?</p>
",Preprocessing of the text & Tokenization,unsure word embeddings po using neural net nlp classification im planning using nn sarcasm detection number tweet im unsure prepare word embeddings train nn tokenize string tag emoticon capitalisation user tag hashtags etc combine resulting string word embeddings train word embeddings resulting corpus tweet
"Parsing XML files in R for NLP, but code returns empty","<p>I'm trying to preprocess large TEI-encoded XML files, in order to later use ggplot2 to visualize results. I am using code from Matt Jockers, which all seems to be working. But instead of returning data, I get a list of 6 NULL values (6 being the number of files). Why am I getting empty lists??</p>

<p>Code is here for reference, though files are limited-access</p>

<pre><code>setwd(""C:/Users/Amanda/Desktop/WWO/WWO"")
input.dir &lt;- ""Wroth and Sidney Herbert/R corpus""

files.v &lt;- dir(path=input.dir, pattern="".*xml"")
file.path(input.dir, files.v)


getTEIWordTableList &lt;- function(doc.object){
 paras &lt;- getNodeSet(doc.object,
              ""/tei:TEI/tei:text/tei:body//tei:p"",
             c(tei = ""http://www.tei-c.org/ns/1.0""))
 words &lt;- paste(sapply(paras,xmlValue), collapse="" "")
 words.lower &lt;- tolower(words)
 words.l &lt;- strsplit(words.lower, ""\\W"")
 word.v &lt;- unlist(words.l)
 book.freqs.t &lt;- table(word.v[which(word.v!="""")])
book.freqs.rel.t &lt;- 100*(book.freqs.t/sum(book.freqs.t))
return(book.freqs.rel.t)}

book.freqs.l &lt;- list() 

for(i in 1:length(files.v)){
doc.object &lt;- xmlTreeParse(file.path(input.dir, files.v[i]),
                    useInternalNodes=TRUE)
worddata &lt;- getTEIWordTableList(doc.object)
book.freqs.l[[files.v[i]]] &lt;- worddata}
</code></pre>

<p>Thanks in advance! I'm new to XML parsing. </p>

<p>EDIT: adding first chunk of XML text</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;?xml-model href=""../schema/wwp-store.rng""    type=""application/xml""schematypens=""http://relaxng.org/ns/structure/1.0""?&gt;&lt;?xml-model href=""../schema/wwp-store.isosch"" type=""application/xml""schematypens=""http://purl.oclc.org/dsdl/schematron""?&gt;&lt;!-- $Id: sidney.antonie.xml 29592 2016-09-16 13:49:08Zsyd $ --&gt;&lt;TEI xmlns=""http://www.wwp.northeastern.edu/ns/textbase""xmlns:xi=""http://www.w3.org/2001/XInclude""&gt;
&lt;teiHeader xml:id=""TR00204.hdr""&gt;
    &lt;fileDesc&gt;
      &lt;titleStmt&gt;
        &lt;title type=""main""&gt;The Tragedie of Antony, 1595&lt;/title&gt;
        &lt;author&gt;&lt;persName ref=""p:msidney.enw"" type=""person-female""&gt;Sidney, Mary, Countess of Pembroke&lt;/persName&gt;, trans. (from &lt;persName ref=""p:rgarnier.pbo"" type=""person-male""&gt;Robert Garnier&lt;/persName&gt;)&lt;/author&gt;
    &lt;sponsor&gt;Brown University&lt;/sponsor&gt;
    &lt;sponsor&gt;Northeastern University&lt;/sponsor&gt;
    &lt;funder&gt;U.S. National Endowment for the Humanities&lt;/funder&gt;
</code></pre>

<p>
      
",Preprocessing of the text & Tokenization,parsing xml file r nlp code return empty trying preprocess large tei encoded xml file order later use ggplot visualize result using code matt jockers seems working instead returning data get list null value number file getting empty list code reference though file limited access thanks advance new xml parsing edit adding first chunk xml text
Using PyParsing to Distinguish Pattern at End of String,"<p>I am writing a script in python to identify and strip reference numbers from a natural text document. For instance, ""The patient used a successful treatment4-6"", where 4-6 are reference numbers. </p>

<p>I have successfully created a grammar that can distinguish a string of alphabet characters (the word) as well as the reference numbers:</p>

<pre><code>real_word = Word(pyparsing.alphas)
first_punctuation = Word('.!?:,;')
second_punctuation = Word('.!?:,;-')
nums = Word(pyparsing.nums)

number_then_punctuation = WordStart() + real_word + nums + 
    second_punctuation + pyparsing.ZeroOrMore(nums | 
    second_punctuation) + WordEnd()
</code></pre>

<p>However, I would like to extend this to recognize words that might have dashes, or other characters in them. I think the easiest solution to this is to create a grammar to recognize the reference number pattern in a token, and then to strip it from the token (without caring about what the 'word' portion looks like), such that:</p>

<pre><code>number_then_punctuation = nums + second_punctuation + 
    pyparsing.ZeroOrMore(nums | second_punctuation) + WordEnd()
</code></pre>

<p>However, this fails when I try to identify the pattern in the token using parseString, because there's no pattern for the word preceding the reference number. In a token, how can I skip to the reference number pattern, saving both the pattern as well as the preceding 'word' in a list? I can use searchString to find the pattern, but this doesn't save the preceding word. </p>

<p>In my above example, I would like to return ['treatment', '4-6']. I can do this using searchString, and then python's str.find() method:</p>

<pre><code>string_test = 'treatment4-5'
x = number_then_punctuation.searchString(string_test).asList()[0][0]
index = string_test.find(x)
split = [string_test[:index], string_test[index:]]
</code></pre>

<p>But I'm hoping that there's a way to do this built in to pyparsing.</p>

<p>Thanks</p>
",Preprocessing of the text & Tokenization,using pyparsing distinguish pattern end string writing script python identify strip reference number natural text document instance patient used successful treatment reference number successfully created grammar distinguish string alphabet character word well reference number however would like extend recognize word might dash character think easiest solution create grammar recognize reference number pattern token strip token without caring word portion look like however fails try identify pattern token using parsestring pattern word preceding reference number token skip reference number pattern saving pattern well preceding word list use searchstring find pattern save preceding word example would like return treatment using searchstring python str find method hoping way built pyparsing thanks
Tokenizing a string having double quotes,"<p>I have a string in the form : </p>

<pre><code>'I am going to visit ""Huge Hotel"" and the ""Grand River""'
</code></pre>

<p>I want it tokenized as </p>

<pre><code>['I', 'am', 'going',..., 'Huge Hotel','and' ,'the' ,'Grand River']
</code></pre>

<p>As seen <em>'Huge Hotel'</em> and <em>'Grand River'</em> are taken as a single word as they were present in quotes.</p>

<pre><code>import nltk
text = 'I am going to visit ""Huge Hotel"" and the ""Grand River""'
b = nltk.word_tokenize(text)
</code></pre>

<p>I have written above code but it does'nt work</p>
",Preprocessing of the text & Tokenization,tokenizing string double quote string form want tokenized seen huge hotel grand river taken single word present quote written code doe nt work
Stemming and Lemmatization in Spark and Scala,"<p>I have used Stanford NLP Library to perform stemming and lemmatization on a sentence. 
For example, Car is an easy way for commute. But there are too many cars on roads these days. </p>

<p>So the expected output is: </p>

<pre><code>car be easy way commute car road day
</code></pre>

<p>But I am getting this :</p>

<pre><code>ArrayBuffer(car, easy, way, for, commute, but, there, too, many, car, road, these, day)
</code></pre>

<p>Here is the code </p>

<pre><code>val stopWords = sc.broadcast(
  scala.io.Source.fromFile(""src/main/common-english-words.txt"").getLines().toSet).value

def plainTextToLemmas(text: String, stopWords: Set[String]): Seq[String] = {
  val props = new Properties()
  props.put(""annotators"", ""tokenize, ssplit, pos, lemma"")
  val pipeline = new StanfordCoreNLP(props)
  val doc = new Annotation(text)
  pipeline.annotate(doc)
  val lemmas = new ArrayBuffer[String]()
  val sentences = doc.get(classOf[SentencesAnnotation])
  for (sentence &lt;- sentences; token &lt;- sentence.get(classOf[TokensAnnotation])) {
    val lemma = token.get(classOf[LemmaAnnotation])
    if (lemma.length &gt; 2 &amp;&amp; !stopWords.contains(lemma)) {
      lemmas += lemma.toLowerCase
    }
  }
  lemmas
}
val lemmatized = stringRDD.map(plainTextToLemmas(_, stopWords))
lemmatized.foreach(println)
</code></pre>

<p>I have taken it from the advanced analytics book on Spark, it seems like the stop words are not removed, and ""is"" is not converted to ""be"". Can we add or delete rules from these libraries?</p>

<p><a href=""http://www.textfixer.com/resources/common-english-words.txt"" rel=""nofollow noreferrer"">http://www.textfixer.com/resources/common-english-words.txt</a></p>
",Preprocessing of the text & Tokenization,stemming lemmatization spark scala used stanford nlp library perform stemming lemmatization sentence example car easy way commute many car road day expected output getting code taken advanced analytics book spark seems like stop word removed converted add delete rule library
Removing all punctuation marks in python,"<p>I need to remove all punctuation marks in the string, as a part of bigger program. 
It is working when I write it serepatly for each mark like this:</p>

<pre><code>words = [word.replace(""."", """") for word in words]
</code></pre>

<p>But when I am trying to do it in the loop, it is not working. </p>

<pre><code>line = ""I was going to leave her, but in the very last moment I had changed 
my mind. Interesting thing, many nice ways to use.""
words = line.lower().split()
for punc in [""."","",""]:
    if punc in words:
        words = [word.replace(punc, """") for word in words]
print words
</code></pre>

<p>Can you please tell me, what I am doing wrong?</p>
",Preprocessing of the text & Tokenization,removing punctuation mark python need remove punctuation mark string part bigger program working write serepatly mark like trying loop working please tell wrong
Embeddings vs text cleaning (NLP),"<p>I am a graduate student focusing on ML and NLP. I have a lot of data (8 million lines) and the text is usually badly written and contains so many spelling mistakes.
So i must go through some text cleaning  and vectorizing. To do so, i considered two approaches:</p>

<p>First one:</p>

<p>cleaning text by replacing bad words using hunspell package which is a spell checker and morphological analyzer
+
tokenization
+
convert sentences to vectors using tf-idf</p>

<p>The problem here is that sometimes, Hunspell fails  to provide the correct word and changes the misspelled word with another word that don't have the same meaning. Furthermore, hunspell does not reconize acronyms or abbreviation (which are very important in my case) and tends to replace them.</p>

<p>Second approache:</p>

<p>tokenization
+
using some embeddings methode (like word2vec) to convert words into vectors without cleaning text</p>

<p>I need to know if there is some (theoretical or empirical) way to compare this two approaches :)</p>

<p>Please do not hesitate to respond If you have any ideas to share, I'd love to discuss them with you.
Thank you in advance</p>
",Preprocessing of the text & Tokenization,embeddings v text cleaning nlp graduate student focusing ml nlp lot data million line text usually badly written contains many spelling mistake must go text cleaning vectorizing considered two approach first one cleaning text replacing bad word using hunspell package spell checker morphological analyzer tokenization convert sentence vector using tf idf problem sometimes hunspell fails provide correct word change misspelled word another word meaning furthermore hunspell doe reconize acronym abbreviation important case tends replace second approache tokenization using embeddings methode like word vec convert word vector without cleaning text need know theoretical empirical way compare two approach please hesitate respond idea share love discus thank advance
Is there an open-source self-learning stemmer?,"<p>I need to implement some sort of stemmer/lemmatizer. I have some words in different forms (a few thousands). It's not a morphological dictionary, just a small part of it. Is it a good idea to learn a stemmer automatically from the file a have? Is there any open-source implementations that can be used?</p>
",Preprocessing of the text & Tokenization,open source self learning stemmer need implement sort stemmer lemmatizer word different form thousand morphological dictionary small part good idea learn stemmer automatically file open source implementation used
Search a python list for matches to a custom list of stem words of varying length,"<p>I'm trying to search word-tokenized abstracts for custom stem words using python. The following code is almost what I want. That is, do any of the values in stem_words appears once or more in word_tokenized_abstract?</p>

<pre><code>if(any(word in stem_words for word in word_tokenized_abstract)):
    do stuff
</code></pre>

<p>where...</p>

<ul>
<li>stem_words is a list of strings only</li>
<li>word_tokenized_abstract is a list of strings only</li>
</ul>

<p>I based the above at <a href=""https://stackoverflow.com/questions/10668282/one-liner-to-check-if-at-least-one-item-in-list-exists-in-another-list"">one-liner to check if at least one item in list exists in another list?</a></p>

<p>My issue is that my stem_words are of different lengths. I've tried the following code (a modification of the above) which did not work for me. I've tried a few other modifications but they either don't work or cause a crash.</p>

<pre><code>if(any(word in stem_words for word[0:len(word)] in word_tokenized_abstract)):
    do stuff
</code></pre>

<p>That is, do any of the values word_tokenized_abstract begin with any of the values in <code>stem_words</code>?</p>

<p>if it helps, my <code>stem_words = ['pancrea', 'muscul', 'derma', 'ovar']</code></p>

<p>Thanks! I apologize if this question has been answered previously but I couldn't find it.</p>
",Preprocessing of the text & Tokenization,search python list match custom list stem word varying length trying search word tokenized abstract custom stem word using python following code almost want value stem word appears word tokenized abstract stem word list string word tokenized abstract list string based href check least one item list exists another list issue stem word different length tried following code modification work tried modification either work cause crash value word tokenized abstract begin value help thanks apologize question ha answered previously find
AtrributeError: Dict has no object class_name,"<p>Im trying to parse a website, tokenize it and store different sentences in an array, so this becomes an array of strings. I need to access the classes returned from the json. For example, i have to classes: nhate and hate. If the class is hate and the confidence for that class is > 0.50, then do something. However i cannot access the classes.</p>

<pre><code>words = text.split(""."")
c=0
for i in words:

  if not words[c]:
      words[c] = ""this was empty before."" 
  classes = natural_language_classifier.classify('90e7b4x199-nlc-36073',words[c])
  result = json.dumps(classes, indent=2)
  if (classes.class_name == 'hate' and classes.confidence &gt; 0.50):
    print(json.dumps(classes, indent=2)) 
  c=c+1
</code></pre>

<p>The error that im getting is:</p>

<pre><code>Traceback (most recent call last):                        
File ""parse.py"", line 45, in &lt;module&gt;                        
if (classes.class_name == 'hate' and classes.confidence &gt; 0.50) 
AttributeError: 'dict' object has no attribute 'class_name'
</code></pre>

<p><strong>Edited</strong>: The json that im getting is like this: </p>

<pre><code>{
  ""classifier_id"": ""10D41B-nlc-1"",
  ""url"": ""https://gateway.watsonplatform.net/natural-language-classifier    /api/v1/classifiers/10D41B-nlc-1/classify?text=How%20hot%20wil/10D41B-nlc-1"",
  ""text"": ""How hot will it be today?"",
  ""top_class"": ""nhate"",
  ""classes"": [
    {
      ""class_name"": ""nhate"",
      ""confidence"": 0.9998201258549781
    },
    {
      ""class_name"": ""hate"",
      ""confidence"": 0.00017987414502176904
    }
  ]
}
</code></pre>

<p><em>Edited</em></p>

<p>print(classes) gives me:</p>

<pre><code>    {u'url': u'https://gateway.watsonplatform.net/natural-language-classifier/api/v1
    /classifiers/90e7b4x199-nlc-36073', 
    u'text': u' A Partnership With Abu Shaklak Printing House', 
    u'classes': 
    [{u'class_name': u'nhate.', u'confidence': 0.9398546
    187612434}, {u'class_name': u'hate.', u'confidence':   0.0449277873541271}, {u'cla
    ss_name': u'Feels good man', u'confidence': 0.015217593884629425}],     u'classifier
    _id': u'90e7b4x199-nlc-36073', u'top_class': u'nhate.'}
</code></pre>
",Preprocessing of the text & Tokenization,atrributeerror dict ha object class name im trying parse website tokenize store different sentence array becomes array string need access class returned json example class nhate hate class hate confidence class something however access class error im getting edited json im getting like edited print class give
How to reverse the regex in contractions tokenization?,"<p>In <code>nlp</code> tokenization, the contractions are sometimes split up as such:</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; s = 'he cannot fly'
&gt;&gt;&gt; pattern, substitution  = r""(?i)\b(can)(not)\b"", r"" \1 \2 ""
&gt;&gt;&gt; re.sub(pattern, substitution, s)
'he  can not  fly'
</code></pre>

<p>To reverse it (i.e. detokenization), I've tried this:</p>

<pre><code>&gt;&gt;&gt; rev_pattern, rev_substitution  = r""(?i)\b(can)\s(not)\b"", r"" \1\2 ""
&gt;&gt;&gt; re.sub(rev_pattern, rev_substitution, s)
'he cannot fly'
</code></pre>

<p>The question is <code>r""(?i)\b(can)\s(not)\b""</code> and <code>r"" \1\2 ""</code> the reverse of the original pattern substitution? Is there other way to reverse this?</p>

<p>In this case, I've manually coded the <code>\s</code> into the pattern. The main problem is there're a bunch of these regexes that are manually coded for tokenization and I've to manually add the <code>\s</code> for all of them:</p>

<pre><code>CONTRACTIONS2 = [re.compile(r""(?i)\b(can)(not)\b""),
                 re.compile(r""(?i)\b(d)('ye)\b""),
                 re.compile(r""(?i)\b(gim)(me)\b""),
                 re.compile(r""(?i)\b(gon)(na)\b""),
                 re.compile(r""(?i)\b(got)(ta)\b""),
                 re.compile(r""(?i)\b(lem)(me)\b""),
                 re.compile(r""(?i)\b(mor)('n)\b""),
                 re.compile(r""(?i)\b(wan)(na) "")]
CONTRACTIONS3 = [re.compile(r""(?i) ('t)(is)\b""),
                 re.compile(r""(?i) ('t)(was)\b"")]
CONTRACTIONS4 = [re.compile(r""(?i)\b(whad)(dd)(ya)\b""),
                 re.compile(r""(?i)\b(wha)(t)(cha)\b"")]
</code></pre>

<p>Is there a way to automatically iterate through the list of regexes and add the <code>\s</code> in between the group without hardcoding the detokenzation regexes</p>

<p>I know that the original tokenization substitution is -> `r' \1 \2 ', so to undo that, I've to change it back to r' \1\2 '. </p>
",Preprocessing of the text & Tokenization,reverse regex contraction tokenization tokenization contraction sometimes split reverse e detokenization tried question reverse original pattern substitution way reverse case manually coded pattern main problem bunch regexes manually coded tokenization manually add way automatically iterate list regexes add group without hardcoding detokenzation regexes know original tokenization substitution r undo change back r
Accessing the confidence parameters in IBM Watson nl-classifier,"<p>I'm building a web application where when i input a web page, it will get the textual content of web page, tokenize it and give each line to the trained classifier, what i want is to do something with those lines whose confidence parameters > 0.98. Below is an example taken from bluemix website:</p>

<pre><code>     {
  ""classifier_id"": ""10D41B-nlc-1"",
  ""url"": ""https://gateway.watsonplatform.net/natural-language-classifier    /api/v1/classifiers/10D41B-nlc-1/classify?text=How%20hot%20wil/10D41B-nlc-1"",
  ""text"": ""How hot will it be today?"",
  ""top_class"": ""temperature"",
  ""classes"": [
    {
      ""class_name"": ""temperature"",
      ""confidence"": 0.9998201258549781
    },
    {
      ""class_name"": ""conditions"",
      ""confidence"": 0.00017987414502176904
    }
  ]
}
</code></pre>

<p>Now in the above example, i want to something with class_name: temperature where confidence > 0.95.</p>

<pre><code> #if class.temperature.confidence &gt; 0.98
     #do something with it   
</code></pre>

<p>Is it possible?</p>
",Preprocessing of the text & Tokenization,accessing confidence parameter ibm watson nl classifier building web application input web page get textual content web page tokenize give line trained classifier want something line whose confidence parameter example taken bluemix website example want something class name temperature confidence possible
OpenNLP POSTagger output from command line,"<p>I want to use <a href=""http://opennlp.apache.org/cgi-bin/download.cgi"" rel=""nofollow noreferrer"">OpenNLP</a> in order to tokenize Thai words. I downloaded OpenNLP and <a href=""http://opennlp.sourceforge.net/models/thai/?C=M;O=A"" rel=""nofollow noreferrer"">Thai tokenize model</a> and run the following</p>

<pre><code>./bin/opennlp POSTagger -lang th -model thai.tok.bin &lt; sentence.txt &gt; output.txt
</code></pre>

<p>I put <code>thai.tok.bin</code> that I downloaded on the directory that I call from and run the following. <code>sentence.txt</code> has this text inside <code>กินอะไรยังนาย</code>. However, the output I got has only these text:</p>

<pre><code>Usage: opennlp POSTagger model &lt; sentences
Execution time: 0.000 seconds
</code></pre>

<p>I'm pretty new to <code>OpenNLP</code>, please let me know if anyone knows how to get output from it.</p>
",Preprocessing of the text & Tokenization,opennlp postagger output command line want use opennlp order tokenize thai word downloaded opennlp thai tokenize model run following put downloaded directory call run following ha text inside however output got ha text pretty new please let know anyone know get output
Thai Language Maxent Model Generation,"<p>While creating models using ApacheOpenNlp for languages such as Thai, is it good to tokenize and use the tokenized corpus to train the MxentModel, or the untokenized sentences should be used.</p>
",Preprocessing of the text & Tokenization,thai language maxent model generation creating model using apacheopennlp language thai good tokenize use tokenized corpus train mxentmodel untokenized sentence used
Python: linguistic normalization,"<p>I'm looking for a lemmatization module/lib that will transfer a sentence like:<br>
 ""this is <strong>great</strong>"" to ""this is <strong>good</strong>"".</p>

<p>I'm familiar with some of the tools available in nltk such as stemming and lemmatization, however it's not exactly what I'm looking for</p>

<p>My goal is to minimize the variety of ways saying the same thing.</p>
",Preprocessing of the text & Tokenization,python linguistic normalization looking lemmatization module lib transfer sentence like great good familiar tool available nltk stemming lemmatization however exactly looking goal minimize variety way saying thing
stopwords in sentence tokenizer,"<p>i am using stopwords and sentence tokenizer but when i print filtered sentence that gives me result including stopwords. the problem is it not ignore stopwords in output . how to remove stopwords in sentence tokenizer ?  </p>

<pre><code> userinput1  = input (""Enter file name:"")
    myfile1 = open(userinput1).read()
    stop_words = set(stopwords.words(""english""))
    word1 = nltk.sent_tokenize(myfile1)
    filtration_sentence = []
    for w in word1:
        word = sent_tokenize(myfile1)
        filtered_sentence = [w for w in word if not w in stop_words]
        print(filtered_sentence)

    userinput2  = input (""Enter file name:"")
    myfile2 = open(userinput2).read()
    stop_words = set(stopwords.words(""english""))
    word2 = nltk.sent_tokenize(myfile2)
    filtration_sentence = []
    for w in word2:
        word = sent_tokenize(myfile2)
        filtered_sentence = [w for w in word if not w in stop_words]
        print(filtered_sentence)

    stemmer = nltk.stem.porter.PorterStemmer()
    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)

    def stem_tokens(tokens):
        return [stemmer.stem(item) for item in tokens]

    '''remove punctuation, lowercase, stem'''
    def normalize(text):
        return stem_tokens(nltk.sent_tokenize(text.lower().translate(remove_punctuation_map)))
    vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')

    def cosine_sim(myfile1, myfile2):
        tfidf = vectorizer.fit_transform([myfile1, myfile2])
        return ((tfidf * tfidf.T).A)[0,1]
    print(cosine_sim(myfile1,myfile2))
</code></pre>
",Preprocessing of the text & Tokenization,stopwords sentence tokenizer using stopwords sentence tokenizer print filtered sentence give result including stopwords problem ignore stopwords output remove stopwords sentence tokenizer
How can I get the correct position of each tag in several sentences in the indexed depedency of Stanford Parser?,"<p>Normally I can do it by splitting sentences and tokenize it but there's an example:</p>

<p>""There comes the soldier... I... you must go.""</p>

<p>Tagging</p>

<p>There/EX
comes/VBZ
the/DT
soldier/NN
.../:
I./NNP
./.
./.</p>

<p>you/PRP
must/MD
go/VB
./.
Parse</p>

<p>(ROOT
  (S
    (NP (EX There))
    (VP (VBZ comes)
      (NP
        (NP (DT the) (NN soldier))
        (: ...)
        (NP (NNP I.) (. .))))
    (. .)))</p>

<p>(ROOT
  (S
    (NP (PRP you))
    (VP (MD must)
      (VP (VB go)))
    (. .)))</p>

<p>Universal dependencies</p>

<p>expl(comes-2, There-1)
root(ROOT-0, comes-2)
det(soldier-4, the-3)
dobj(comes-2, soldier-4)
dep(soldier-4, I.-6)</p>

<p>nsubj(go-3, you-1)
aux(go-3, must-2)
root(ROOT-0, go-3)</p>

<p>The sentence doesn't stop at the first ""..."", but at the second one. So easily splitting sentences and count the number of tokens will not help in this case. (Because it will regard this as three sentences.)</p>

<p>Is there any other way that I can know to which parse tree belongs a token? Or a parse tree is which substring of the example? Or directly the position of a tag in this example(three sentences) ?</p>
",Preprocessing of the text & Tokenization,get correct position tag several sentence indexed depedency stanford parser normally splitting sentence tokenize example come soldier must go tagging ex come vbz dt soldier nn nnp prp must md go vb parse root np ex vp vbz come np np dt nn soldier np nnp root np prp vp md must vp vb go universal dependency expl come root root come det soldier dobj come soldier dep soldier nsubj go aux go must root root go sentence stop first second one easily splitting sentence count number token help case regard three sentence way know parse tree belongs token parse tree substring example directly position tag example three sentence
Speed up dictionary merging with soft conjunction logic,"<p>I have a look-up table which contains <code>&lt;word: dictionary&gt;</code>pairs.
Then, given a word list, 
I can produce a dictionary list using this look-up table.
(Each time, the length of this word list is not fixed). 
Values in these dictionaries represent log probability of some keys. </p>

<p>Here is an example:</p>

<p>Given a word list </p>

<p><code>['fruit','animal','plant']</code>,</p>

<p>we can check out the look-up table and have </p>

<p><code>dict_list = [{'apple':-1, 'flower':-2}, {'apple':-3, 'dog':-1}, {'apple':-2, 'flower':-1}]</code>.</p>

<p>We can see from the list that we have a set of keys: <code>{'apple', 'flower', 'dog'}</code> </p>

<p>For each key, I want to give a sum of each value in the dict_list. And if a key is not existed in one dictionary, then we add a small value -10 to the value (you can regard -10 as an very small log probability).</p>

<p>The result dictionary looks like:
<code>dict_merge = {'apple':-6, 'flower':-13, 'dog':-21}</code>, 
because <code>'apple' = (-1) + (-3) + (-2)</code>, <code>'flower' = (-2) + (-10) + (-1)</code>, <code>'dog' = (-10) + (-1) + (-10)</code></p>

<p>Here is my python3 code:</p>

<pre><code>dict_list = [{'apple':-1, 'flower':-2}, {'apple':-3, 'dog':-1}, {'apple':-2, 'flower':-1}]

key_list = []
for dic in dict_list:
    key_list.extend(dic.keys())

dict_merge = dict.fromkeys(key_list, 0)
for key in dict_merge:
    for dic in dict_list:
        dict_merge[key] += dic.get(key, -10)
</code></pre>

<p><strong>This code works, but if the sizes of some dictionaries in <code>dict_list</code> are super large (for example 100,000), then it could take over 200ms, which is not acceptable in practice.</strong></p>

<p>The main computation is in the <code>for key in dict_merge</code> loop, imagine it is a loop of size 100,000. </p>

<p>Is there any speed-up solutions? Thanks! And, thanks for reading~ maybe too long and too annoying...</p>

<p>P.S. 
There are only a few dictionaries in the look-up table have super large size. So there could be some chances here.</p>
",Preprocessing of the text & Tokenization,speed dictionary merging soft conjunction logic look table contains pair given word list produce dictionary list using look table time length word list fixed value dictionary represent log probability key example given word list check look table see list set key key want give sum value dict list key existed one dictionary add small value value regard small log probability result dictionary look like python code code work size dictionary super large example could take acceptable practice main computation loop imagine loop size speed solution thanks thanks reading maybe long annoying p dictionary look table super large size could chance
Is there a library for splitting sentence into a list of words in it?,"<p>I'm looking at nltk for python, but it splits(tokenize) <code>won't</code> as <code>['wo',""n't""]</code>. Are there libraries that do this more robustly? </p>

<p>I know i can build a regex of some sort to solve this problem, but I'm looking for a library/tool because it would be a more directed approach. For example, after a basic regex with periods and commas, I realized words like 'Mr. ' will break the system. </p>

<p>(@artsiom)</p>

<p>If the sentence was ""you won't?"", split() will give me [""you"", ""won't?""]. So there's an extra '?' that I have to deal with.
I'm looking for a tried and tested method which do away with the kinks like the above mentioned and also the lot many exceptions that I'm sure exist. Of course, I'll resort to a split(regex) if I don't find any.</p>
",Preprocessing of the text & Tokenization,library splitting sentence list word looking nltk python split tokenize library robustly know build regex sort solve problem looking library tool would directed approach example basic regex period comma realized word like mr break system artsiom sentence wa split give extra deal looking tried tested method away kink like mentioned also lot many exception sure exist course resort split regex find
fast way to tokenize data in Python like TfidfVectorizer does,"<p>i used TfidfVectoriser to create tf-idf matrix</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
transformer = TfidfVectorizer(smooth_idf=False,stop_words=stopwords.words('english'))   
tfidf = transformer.fit_transform(raw_documents=sentences)
</code></pre>

<p>Now I want to transform each element of my <code>sentences</code> list to list of tokens, which were used in tfidfvectoriser. I tried to extract it directly from <code>tfidf</code> object via this function</p>

<pre><code>def get_token(id, transformer_name, tfidf_obj):
   return(np.array(transformer_name.get_feature_names())[\
      tfidf_obj[id].toarray().reshape((tfidf_obj.shape[1],))!=0])
</code></pre>

<p>Where <code>id</code> is index of each sentence. In this function i tried to extract given row from tf-idf object, find non-zero elements and extract corresponding elements from <code>transformer_name.get_feature_names()</code> . Looks too complex =). Also this solution works very slow =/</p>

<p>Is there any way to get tokens using tfidfvectorizer's preprocessing and tokenization functions?</p>
",Preprocessing of the text & Tokenization,fast way tokenize data python like tfidfvectorizer doe used tfidfvectoriser create tf idf matrix want transform element list list token used tfidfvectoriser tried extract directly object via function index sentence function tried extract given row tf idf object find non zero element extract corresponding element look complex also solution work slow way get token using tfidfvectorizer preprocessing tokenization function
Determine most important sentences of a text,"<p>I am working on a tool that will allow users to summarize a selected text.</p>

<p>I want to do this by determining x amount of most important sentences of the text (decided by the user/calculated based on text length), and then for each of those ""core sentences"", I want to accompany the sentence by x amount of most related/similar sentences to that core sentence. That way, I hope to cover multiple important pieces of a text in a few lines, rather than one big piece (subject) of the text. I am aware that not every text will have multiple subjects available to sufficiently have multiple core sentences, the amount of core sentences and related sentences will depend on the text itself.</p>

<p>For determining these important sentences I have currently based myself on the example of <a href=""https://thetokenizer.com/2013/04/28/build-your-own-summary-tool/"" rel=""nofollow noreferrer"">this guide</a>, which uses intersection scores between sentences to rank each sentence of the text. So far this has lead to decent results, but sometimes the results aren't as qualitative.</p>

<p>Therefore, I am looking for other methods to extract the most important sentences. After a bit of searching, the <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein distance</a> popped up several times as a way to compare strings.</p>

<p>Could I use the <strong>Levenshtein distance</strong>  to calculate the LD between each sentence and add the total LD-amount for each sentence, returning the x amount of sentences with the lowest aggregated Levenshtein distance number - would this result in a representative ranking of the most important sentences of the text?</p>

<p>If not, should I stick to the intersection method or should I consider an alternative?</p>

<p>I am also considering using <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">tf–idf</a> to ""preprocess"" a sentence in order to only keep valuable words in the text's sentences.</p>
",Preprocessing of the text & Tokenization,determine important sentence text working tool allow user summarize selected text want determining x amount important sentence text decided user calculated based text length core sentence want accompany sentence x amount related similar sentence core sentence way hope cover multiple important piece text line rather one big piece subject text aware every text multiple subject available sufficiently multiple core sentence amount core sentence related sentence depend text determining important sentence currently based example guide us intersection score sentence rank sentence text far ha lead decent result sometimes result qualitative therefore looking method extract important sentence bit searching levenshtein distance popped several time way compare string could use levenshtein distance calculate ld sentence add total ld amount sentence returning x amount sentence lowest aggregated levenshtein distance number would result representative ranking important sentence text stick intersection method consider alternative also considering using tf idf preprocess sentence order keep valuable word text sentence
Is there an easy way in Rapid Miner to find within what range of occurences are the words that are most predictive?,"<p>As stated in the question, other than visually looking through the example set fot a number close to 1. and then checking the occurrences of that attribute in the word list, is there an easy way to find the answer to my question? A graph that would show the most predicted words?</p>

<p>Thanks</p>
",Preprocessing of the text & Tokenization,easy way rapid miner find within range occurences word predictive stated question visually looking example set fot number close checking occurrence attribute word list easy way find answer question graph would show predicted word thanks
all possible wordform completions of a (biomedical) word&#39;s stem,"<p>I'm familiar with word stemming and completion from the tm package in R.  </p>

<p>I'm trying to come up with a quick and dirty method for finding all variants of a given word (within some corpus.)  For example, I'd like to get ""leukocytes"" and ""leuckocytic"" if my input is ""leukocyte"".</p>

<p>If I had to do it right now, I would probably just go with something like:</p>

<pre><code>library(tm)
library(RWeka)
dictionary &lt;- unique(unlist(lapply(crude, words)))
grep(pattern = LovinsStemmer(""company""), 
    ignore.case = T, x = dictionary, value = T)
</code></pre>

<p>I used Lovins because Snowball's Porter doesn't seem to be aggressive enough.</p>

<p>I'm open to suggestions for other stemmers, scripting languages (Python?), or entirely different approaches.</p>
",Preprocessing of the text & Tokenization,possible wordform completion biomedical word stem familiar word stemming completion tm package r trying come quick dirty method finding variant given word within corpus example like get leukocyte leuckocytic input leukocyte right would probably go something like used lovins snowball porter seem aggressive enough open suggestion stemmer scripting language python entirely different approach
Extracting user details from SMS messages in R,"<p>I ve a dataset of mobile SMSs, specifically I want to extract certain details like data balance, date of expiry using R, I ve done the tokenization using NLP and regex, but the thing is I cannot extract the date if the SMS format is something like given below:
<strong>Your 46.35 MB data will expire on 2017-02-08.Buy any of the data plans to continue enjoying the service. Text HELP to 229 for more information.</strong>
Notice that there is no space between 2017-02-08, period(.) and buy, so while tokenizing R treats it as a single token. If I replace period with a space then extracting the data amount(46.35 MB) would become difficult.
Can anyone pls suggest if I can use some text modelling or train the machine to extract these details even if the format of the SMS changes with time
<strong>My SMSs format(structure) would differ. So can I train the dataset?how to proceed?</strong>
My output should be
Data balance, expiry date along with other details going forward in future.</p>
",Preprocessing of the text & Tokenization,extracting user detail sm message r dataset mobile sm specifically want extract certain detail like data balance date expiry using r done tokenization using nlp regex thing extract date sm format something like given mb data expire buy data plan continue enjoying service text help information notice space period buy tokenizing r treat single token replace period space extracting data amount mb would become difficult anyone pls suggest use text modelling train machine extract detail even format sm change time sm format structure would differ train dataset proceed output data balance expiry date along detail going forward future
Python Dealing with Newline Characters When Converting From List to String,"<p>I am having a problem concerning newline characters and return characters. Ugh this is hard to explain for me, but I will try.</p>

<p>I have data that exists in list form. The members of the list have newline characters in them such that.</p>

<pre><code> example_list = [""I've always loved jumping\n\n""]
</code></pre>

<p>In order to tokenize this sentence using NLP though NLTK I need the sentence to be in a string. NLTK will ignore newline characters and other escape characters when it tokenizes according to some tests I ran and evidence from the nltk tutorial.</p>

<p>The problem is when i try to convert example_list to a string i get this output</p>

<pre><code> str(example_list)
 '[""I\'ve always loved jumping\\n\\n""]'
</code></pre>

<p>Notice that all newline characters have now become an escaped forward slash. Trying to tokenize this yields a terrible result where NLTK thinks that jumping\n\n is one big word because it thinks that the newline characters with two slashes are actually text. </p>

<p>Does anyone know any tricks or good practices to ensure that newline characters never exist in my lists or that these are disregarded or not ""double escaped"" when converting to a string.</p>

<p>Lastly, Does anyone have any suggestions on learning material relating to how python processes newline characters and how these characters interact with different datatypes and such because it is so confusing.</p>

<p>Thanks a ton!</p>
",Preprocessing of the text & Tokenization,python dealing newline character converting list string problem concerning newline character return character ugh hard explain try data exists list form member list newline character order tokenize sentence using nlp though nltk need sentence string nltk ignore newline character escape character tokenizes according test ran evidence nltk tutorial problem try convert example list string get output notice newline character become escaped forward slash trying tokenize yield terrible result nltk think jumping n n one big word think newline character two slash actually text doe anyone know trick good practice ensure newline character never exist list disregarded double escaped converting string lastly doe anyone suggestion learning material relating python process newline character character interact different datatypes confusing thanks ton
PorterStemmer with verbs ending in -es and -ed java,"<p>i am using PorterStemmer in java to get the base form of a verb, But i found a problem with the verbs ""goes"" and ""gambles"". Instead of stemming it to ""go"" and ""gamble"", it stems them to ""goe"" and ""gambl"". Is there a better tool that can handle verbs that ends with -es and -ed to retrieve the base form of a verb? P.S JAWS with wordnet java does that too.
Here is my code:</p>

<pre><code>public class verb
{
    public static void main(String[] args)
    {
        PorterStemmer ps = new PorterStemmer();
        ps.setCurrent(""gambles"");
        ps.stem();
        System.out.println(ps.getCurrent());        
    }
}
</code></pre>

<p>Here is the output in console: 
<code>gambl</code></p>
",Preprocessing of the text & Tokenization,porterstemmer verb ending e ed java using porterstemmer java get base form verb found problem verb go gamble instead stemming go gamble stem goe gambl better tool handle verb end e ed retrieve base form verb p jaw wordnet java doe code output console
Matching a large set of words against a list of small sets,"<p>I feel like there could be an algorithm for this but I just don't know what it'd be called.</p>

<p>Let's say you have a 'large' set of words, <br>
<code>('apple', 'orange', 'potato', 'tomato', 'river', 'mountain', 'forest')</code></p>

<p>and a list of smaller sets that will be considered requirements:<br>
<code>[('apple'), 
('potato', 'tomato'), 
('cockroach', 'dynamite')]</code></p>

<p>Is there a way to hash/precompute the list of smaller sets such that you can tell which sets of required words are fulfilled without having to go through them one-by-one?<br>
In this example, the function would tell you the first two requirements were met ('apple') and ('potato','tomato').</p>
",Preprocessing of the text & Tokenization,matching large set word list small set feel like could algorithm know called let say large set word list smaller set considered requirement way hash precompute list smaller set tell set required word fulfilled without go one one example function would tell first two requirement met apple potato tomato
Using word2vec on phrases,"<p>I have a text file with phrases on each line. If I run the word2vec on this file it gives me a numerical vector by tokenizing the file into words. Like this,</p>

<pre><code>the -0.464252 0.177642 -1.212928 0.737752 0.990782 1.530809 1.053639 
0.182065 0.753926 0.082467  
of -0.281145 0.060403 -0.877230 0.566957 0.748220 1.108621 0.711598 
0.135636 0.489113 0.059783  
to -0.352605 0.101068 -0.995506 0.600547 0.809564 1.360837 0.905638 
0.114751 0.596093 0.067007 
</code></pre>

<p>Instead, I want it to assume each line as a word and output a single vector for each line. Something like this,</p>

<pre><code>Suspension of sitting -0.244289 0.111375 -0.722939 0.366711 0.590016 0.904601 0.622145 0.098230 0.431038 0.008134
</code></pre>

<p>This is the package I'm using. '<a href=""https://github.com/danielfrg/word2vec"" rel=""nofollow noreferrer"">https://github.com/danielfrg/word2vec</a>'</p>

<p>How do I accomplish this?</p>
",Preprocessing of the text & Tokenization,using word vec phrase text file phrase line run word vec file give numerical vector tokenizing file word like instead want assume line word output single vector line something like package using accomplish
how to read list so that it prints the exact number of lines of that not more?,"<p>EDIT: I have a text file that contains a Persian sentence, a tab and theد an English word in each line. I omitted stop words and punctuation and put the result in a list (witoutStops). Now I have to see if the words in ""s"" are in each line of the witoutStop list, put ""1"" instead of that and if not put ""0"". For example if the list has 10 lines, the output file should have 10 lines of 1 and 0, too with 6 columns (5 are for the words in ""s"" list and 1 is for the English word). But the problem is, it returns 30 lines. How can I fix it?</p>

<pre><code>from hazm import*
from collections import Counter
import collections
import math

punctuation = '!""#$%&amp;\'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~،؟«؛«'

file1 = ""stopwords.txt""
file2 = ""test files/golTest.txt""


witoutStops = []
corpuslines = []

def RemStopWords(file1, file2):  
    with open(file1, encoding = ""utf-8"") as stopfile:
        normalizer = Normalizer()
        stopwords = stopfile.read()
        stopwords = normalizer.normalize(stopwords)
        with open(file2, encoding = ""utf-8"") as trainfile:
            for line in trainfile:
                tmp = line.strip().split(""\t"")
                tmp[0] = normalizer.normalize(tmp[0])
                for i in punctuation:  # delete punctuations
                    if i in tmp[0]:
                        tmp[0] = tmp[0].replace(i, """")
                corpuslines.append(tmp)
                for row in corpuslines:
                    line = """"
                    tokens = row[0].split()# delete stop words
                    for token in tokens: 
                        if token not in stopwords:
                            line += token + "" ""
                line = line.strip() + ""\t"" + row[1] + ""\n""
                witoutStops.append (line)
#print (witoutStops)
#print (corpuslines)

s = ['آبی', 'منابع', 'سبز', 'رنگ', 'زرد']

def vector():
RemStopWords(file1, file2)
for line in witoutStops:
    with open (""Train.arff"", ""a"", encoding = ""utf-8"") as f:   
    line = line.split(""\t"")
    words = line[0].split()
    for i in s:
        if any([i == word for word in words]): 
            f.write('1,')
        else: 
            f.write('0,')
</code></pre>

<p>Example of the file (and witoutStop list):</p>

<p><a href=""https://i.sstatic.net/0sWy5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0sWy5.png"" alt=""enter image description here""></a></p>

<p>the output should have 5 columns (the words in ""s"" list) + a column of the English word. And 10 rows.</p>

<p>Hint: This is a part of a bigger code. so It has some other functions to extract the words for ""s"" list (which are actually 1000 most frequent words of the file). I put these 5 words here as an example here.</p>
",Preprocessing of the text & Tokenization,read list print exact number line edit text file contains persian sentence tab english word line omitted stop word punctuation put result list witoutstops see word line witoutstop list put instead put example list ha line output file line column word list english word problem return line fix example file witoutstop list output column word list column english word row hint part bigger code ha function extract word list actually frequent word file put word example
R tm package stemCompletion &#39;Out of Memory&#39;,"<p>I have been trying to work through the following tutorial: <a href=""http://www.rdatamining.com/examples/text-mining"" rel=""nofollow"">http://www.rdatamining.com/examples/text-mining</a> however, instead of using the twitter data I have been using .csv file (unfortunately the contents are sensitive and cannot be made public).</p>

<p>The .csv file has two columns a user key in column A and a piece of narrative text (Response) in column B. The file has been opened with the following code,</p>

<pre><code>Data &lt;- read.csv(file=""PATH/FILE.csv"", header=TRUE, sep="","", stringsAsFactors=FALSE)
Data &lt;- Data[!(Data$Response==""""), ]
df&lt;- do.call(""rbind"", lapply(Data$Response, as.list))
</code></pre>

<p>df is a 'list of 91' with each item in the list being of type ""character"".</p>

<p>The tutorial is followed from the line library(tm) with no differences except the addition of <code>NarrativeCorpus &lt;- tm_map(NarrativeCorpus, PlainTextDocument)</code> after <code>myCorpus &lt;- tm_map(myCorpus, removeWords, myStopwords)</code>, which I found was needed for stemming.</p>

<p>The code fails at stem completion: <code>myCorpus &lt;- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)</code> with the error,</p>

<blockquote>
  <p>Error in grep(sprintf(""^%s"", w), dictionary, value = TRUE) : 
     invalid regular expression, reason 'Out of memory'</p>
</blockquote>

<p>I have tried to look on-line and on stack overflow with little luck.</p>

<p>I have tried converting the reference dictionary into a list of unique words then back into a corpus (to reduce its size) but to no avail.</p>

<p>I am using R 64-bit 3.2.3 with RStudio Desktop 0.99.891 on a Windows 7 laptop with 4GB RAM. All packages are up to date (according to RStudio).</p>

<p>This is my first SO post so I welcome advise on what I should have included and why, etc..</p>
",Preprocessing of the text & Tokenization,r tm package stemcompletion memory trying work following tutorial however instead using twitter data using csv file unfortunately content sensitive made public csv file ha two column user key column piece narrative text response column b file ha opened following code df list item list type character tutorial followed line library tm difference except addition found wa needed stemming code fails stem completion error error grep sprintf w dictionary value true invalid regular expression reason memory tried look line stack overflow little luck tried converting reference dictionary list unique word back corpus reduce size avail using r bit rstudio desktop window laptop gb ram package date according rstudio first post welcome advise included etc
How to implement re.search() in my code?,"<p>I am working on a binary classification problem with text data. I want to classify the words of the text based on their appearances in some well-defined Word class features I have chosen.
For now, I have been searching the occurrence of the entire word of text in each word class and incrementing the count of that word class on match. This count is further being used to calculate frequency of each word class. Here is my code:</p>

<pre><code>import nltk
import re

def wordClassFeatures(text):
    home = """"""woke home sleep today eat tired wake watch
        watched dinner ate bed day house tv early boring
        yesterday watching sit""""""

    conversation = """"""know people think person tell feel friends
talk new talking mean ask understand feelings care thinking
friend relationship realize question answer saying""""""


    countHome = countConversation =0

    totalWords = len(text.split())

    text = text.lower()
    text = nltk.word_tokenize(text)
    conversation = nltk.word_tokenize(conversation)
    home = nltk.word_tokenize(home)
'''
    for word in text:
        if word in conversation: #this is my current approach
            countConversation += 1
        if word in home:
            countHome += 1
'''

    for word in text:
        if re.search(word, conversation): #this is what I want to implement
            countConversation += 1
        if re.search(word, home):
            countHome += 1

    countConversation /= 1.0*totalWords
    countHome /= 1.0*totalWords

    return(countHome,countConversation)

text = """""" Long time no see. Like always I was rewriting it from scratch a couple of times. But nevertheless 
it's still java and now it uses metropolis sampling to help that poor path tracing converge. Btw. I did MLT on 
yesterday evening after 2 beers (it had to be Ballmer peak). Altough the implementation is still very fresh it 
easily outperforms standard path tracing, what is to be seen especially when difficult caustics are involved. 
I've implemented spectral rendering too, it was very easy actually, cause all computations on wavelengths are 
linear just like rgb. But then I realised that even if it does feel more physically correct to do so, whats the 
point? 3d applications are operating in rgb color space, and because I cant represent a rgb color as spectrum 
interchangeably I have to approximate it, so as long as I'm not running a physical simulation or something I don't
see the benefits (please correct me if I'm wrong), thus I abandoned that.""""""

print(wordClassFeatures(text))
</code></pre>

<p>The demerit of this is that I now have an extra overhead of stemming each word of all the word classes since the words in the text must match explicitly to fall into a word class. Therefore, I am now trying to input each word of the text as a regular expression and search for it in each word class.
This throws the error: </p>

<pre><code>line 362, in wordClassFeatures
if re.search(conversation, word):
  File ""/root/anaconda3/lib/python3.6/re.py"", line 182, in search
    return _compile(pattern, flags).search(string)
  File ""/root/anaconda3/lib/python3.6/re.py"", line 289, in _compile
    p, loc = _cache[type(pattern), pattern, flags]
TypeError: unhashable type: 'list'
</code></pre>

<p>I know there's a major mistake in the syntax but I couldn't find it on the net as most of the syntax for re.search are in the format:</p>

<blockquote>
  <p>re.search(""thank|appreciate|advance"", x)</p>
</blockquote>

<p>Is there any way to properly implement this?</p>
",Preprocessing of the text & Tokenization,implement search code working binary classification problem text data want classify word text based appearance well defined word class feature chosen searching occurrence entire word text word class incrementing count word class match count used calculate frequency word class code demerit extra overhead stemming word word class since word text must match explicitly fall word class therefore trying input word text regular expression search word class throw error know major mistake syntax find net syntax search format search thank appreciate advance x way properly implement
Text preprocessing in Python,"<p>I would like to build a text corpus for a NLP project in Python. I've seen this text format in the LSHTC4 Kaggle challenge: </p>

<pre><code>5 0:10 8:1 18:2 54:1 442:2 3784:1 5640:1 43501:1 
</code></pre>

<p>The first number corresponds to the label.</p>

<p>Each set of numbers separated by ‘:‘ correspond to a (feature,value) pair of the vector, where the first number is the feature’s id and the second number its frequency (for example feature with the id 18 appears 2 times in the instance).</p>

<p>I don't know if this is a common way to pre-process the text data to a numeric vector. I can't find the pre-processing procedure in the challenge, the data were already pre-processed.</p>
",Preprocessing of the text & Tokenization,text preprocessing python would like build text corpus nlp project python seen text format lshtc kaggle challenge first number corresponds label set number separated correspond feature value pair vector first number feature id second number frequency example feature id appears time instance know common way pre process text data numeric vector find pre processing procedure challenge data already pre processed
Python Arabic NLP,"<p>I'm in the process of assessing the capabilities of the NLTK in processing Arabic text in a research to analyze and extract sentiments.</p>

<p>Question is as follows:</p>

<ol>
<li>Is the NTLK capable of handling and allows the analysis of Arabic text?</li>
<li>Is python capable of manipulating\tokenizing Arabic text?</li>
<li>Will I be able to parse and store Arabic text using Python?</li>
</ol>

<p>If python and NTLK aren't the tools for this job, what tools would you recommend (if existent)?</p>

<p>Thank you.</p>

<hr>

<h3>EDIT</h3>

<p>Based on research:</p>

<ol>
<li>NTLK is only capable of stemming Arabic text: <a href=""http://text-processing.com/demo/"" rel=""noreferrer"">Link</a></li>
<li>Python is capable of handling Arabic text since it supports UTF-8 unicode: <a href=""http://www.spencegreen.com/2008/12/19/python-arabic-unicode/"" rel=""noreferrer"">Link</a></li>
<li>Parsing and Lemmatization of Arabic text can be done using: 
SNLPG (The Stanford Natural Language Processing Group) Statistical Parser: <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""noreferrer"">Link</a></li>
</ol>
",Preprocessing of the text & Tokenization,python arabic nlp process assessing capability nltk processing arabic text research analyze extract sentiment question follows ntlk capable handling allows analysis arabic text python capable manipulating tokenizing arabic text able parse store arabic text using python python ntlk tool job tool would recommend existent thank edit based research ntlk capable stemming arabic text link python capable handling arabic text since support utf unicode link parsing lemmatization arabic text done using snlpg stanford natural language processing group statistical parser link
Testing my classifier on a review,"<p>Okay so I have been able to train my movie review classifier using the NaiveBayes Algorithm. The task is to: </p>

<blockquote>
  <p>Test your classifier against a negative review of the walking dead. <a href=""http://metro.co.uk/2017/02/27/the-walking-dead-season-7-episode-11-hostiles-and-calamities-wasnt-as-exciting-as-it-sounds-6473911/#mv-a"" rel=""nofollow noreferrer"">http://metro.co.uk/2017/02/27/the-walking-dead-season-7-episode-11-hostiles-and-calamities-wasnt-as-exciting-as-it-sounds-6473911/#mv-a</a></p>
</blockquote>

<p>Now my book gave an example of classifying documents and it used <code>classifier.classify(df)</code>....now i understand this was document features and had to be tokenized etc. </p>

<blockquote>
  <p>My question: Is there some way to test my classifier against the review just using the url? Or do i have to highlight all the words of the review, store as a string or document then tokenize etc?</p>
</blockquote>
",Preprocessing of the text & Tokenization,testing classifier review okay able train movie review classifier using naivebayes algorithm task test classifier negative review walking dead book gave example classifying document used understand wa document feature tokenized etc question way test classifier review using url highlight word review store string document tokenize etc
Is there an implementation of the Penn Treebank Tokenizer in Perl?,"<p>I'm looking for a Perl module that is a port of <a href=""http://www.cis.upenn.edu/~treebank/tokenizer.sed"" rel=""nofollow"">this</a> where I can basically create an object, call a tokenize() subroutine, pass in a pile of text and get back a list of tokens.  Something to that effect.  If it doesn't exist I'll do it, but no sense in reinventing the wheel, right? :)  TIA.</p>
",Preprocessing of the text & Tokenization,implementation penn treebank tokenizer perl looking perl module port basically create object call tokenize subroutine pas pile text get back list token something effect exist sense reinventing wheel right tia
how to filter rows that satisfy a regular expression via pandas,"<p>I'm trying to figure out a way to to select only the rows that satisfy my regular expression via Pandas. My actual dataset, data.csv, has one column(the heading is not labeled) and millions of row. The first four rows look like:</p>

<pre><code>5;4Z13H;;L
5;346;4567;;O
5;342;4563;;P
5;3LPH14;4567;;O
</code></pre>

<p>and I wrote the following regular expression </p>

<pre><code>([1-9][A-Z](.*?);|[A-Z][A-Z](.*?);|[A-Z][1-9](.*?);)
</code></pre>

<p>which would identify <code>4Z13H;</code> from row 1 and <code>3LPH14;</code> from row 4. Basically I would like pandas to filter my data and select rows 1 and 4. 
So my desired output would be </p>

<pre><code>5;4Z13H;;L
5;3LPH14;4567;;O
</code></pre>

<p>I would then like to save the subset of filter rows into a new csv, filteredData.csv. So far I only have this:</p>

<pre><code>import pandas as pd
import numpy as np
import sys
import re


sys.stdout=open(""filteredData.csv"",""w"")

def Process(filename, chunksize):
    for chunk in pd.read_csv(filename, chunksize=chunksize):
        df[0] = df[0].re.compile(r""([1-9][A-Z]|[A-Z][A-Z]|[A-Z][1-9])(.*?);"")
        sys.stdout.close()


if __name__ == ""__main__"":
    Process('data.csv', 10 ** 4)
</code></pre>

<p>I'm still relatively new to python so the code above has some syntax issues(I'm still trying to figure out how to use pandas chunksize). However the main issue is filtering the rows by the regular expression. I'd greatly appreciate anyone's advice</p>
",Preprocessing of the text & Tokenization,filter row satisfy regular expression via panda trying figure way select row satisfy regular expression via panda actual dataset data csv ha one column heading labeled million row first four row look like wrote following regular expression would identify row row basically would like panda filter data select row desired output would would like save subset filter row new csv filtereddata csv far still relatively new python code ha syntax issue still trying figure use panda chunksize however main issue filtering row regular expression greatly appreciate anyone advice
R function to find subset of words based on letter,"<p>I'm looking for a way to find a way to create a subset of words from a list of words that contains a specific letter. </p>

<p>Right now I know that I can use the grepexpr function to find whether or not a letter exists in a word, but I'm not able to create a subset of words that contain a specific letter. </p>

<p>I've been able to find the total number of letters within a list of words:</p>

<pre><code>&gt; letters_table2&lt;-table(unlist(strsplit(newdata2, """"), use.names=FALSE))
&gt; letters_table2

 a  b  c  d  e  f  g  h  i  j  k  l  m  n  o  p  q  r  s  t  u  v  w  x  y  z 
14  9 11  8 11  6  4  7 12  3  3  9 14  7  9  8  6 13 13  6  7  8  4  7  8  3 
</code></pre>

<p>I'd like to create a list of words that only contain a, b, c, etc... from newdata2.</p>

<pre><code>    newdata2
  [1] ""ae"" ""aj"" ""al"" ""an"" ""av"" ""av"" ""ay"" ""ba"" ""bd"" ""bd"" ""bk"" ""bl"" ""bv"" ""ca"" ""cl"" ""cm"" ""co""
 [18] ""cr"" ""cy"" ""dh"" ""dl"" ""dm"" ""ea"" ""ec"" ""ef"" ""er"" ""ex"" ""ex"" ""ez"" ""fm"" ""fo"" ""ft"" ""gi"" ""gy""
 [35] ""hb"" ""hm"" ""hr"" ""hr"" ""hs"" ""id"" ""in"" ""io"" ""iq"" ""ir"" ""ir"" ""it"" ""iz"" ""ja"" ""js"" ""kn"" ""lc""
 [52] ""ld"" ""le"" ""lp"" ""ls"" ""me"" ""mg"" ""mh"" ""mi"" ""mi"" ""mm"" ""mo"" ""ms"" ""nf"" ""nw"" ""ny"" ""ok"" ""op""
 [69] ""ox"" ""pa"" ""pi"" ""pr"" ""ps"" ""ps"" ""py"" ""qc"" ""qf"" ""qm"" ""qu"" ""qy"" ""rn"" ""rr"" ""rs"" ""rt"" ""ru""
 [86] ""sa"" ""so"" ""ss"" ""ts"" ""uc"" ""us"" ""uu"" ""ux"" ""vb"" ""vc"" ""vv"" ""vw"" ""wb"" ""wg"" ""xe"" ""xo"" ""xt""
[103] ""yd"" ""yt"" ""za""
</code></pre>
",Preprocessing of the text & Tokenization,r function find subset word based letter looking way find way create subset word list word contains specific letter right know use grepexpr function find whether letter exists word able create subset word contain specific letter able find total number letter within list word like create list word contain b c etc newdata
Find the similar texts across the python dataframe,"<p>Suppose I have a python dataframe as follows,</p>

<pre><code>data['text']

abc.google.com
d-2667808233512566908.ampproject.net
d-27973032622323999654.ampproject.net
def.google.com
d-28678547673442325000.ampproject.net
i1-j4-20-1-1-13960-2081004232-s.init.cedexis-radar.net
d-29763453703185417167.ampproject.net
poi.google.com
d-3064948553577027059.ampproject.net
i1-io-0-4-1-20431-1341659986-s.init.cedexis-radar.net
d-2914631797784843280.ampproject.net
i1-j1-18-24-1-11326-1053733564-s.init.cedexis-radar.net
</code></pre>

<p>I want to find the similar common texts and group it. for example, abc.google.com, def.google.com, poi.google.com will point to google.com and etc. </p>

<p>The required output is,</p>

<pre><code>google.com
ampproject.net
ampproject.net
google.com
ampproject.net
s.init.cedexis-radar.net
ampproject.net
google.com
ampproject.net
s.init.cedexis-radar.net
ampproject.net
s.init.cedexis-radar.net
</code></pre>

<p>It's more like a data cleaning exercise where I can clean the unwanted parts. One way is to manually inspect and code for every possible group. But I would be having millions of text. So is there a way / package in python to do this?</p>

<p>Sorry for asking this without trying anything. I've tried to research on this without much success. Not sure how I should start. If anybody can let me know the approach that needs to be taken also, it would be helpful for me.</p>

<p>Thanks</p>
",Preprocessing of the text & Tokenization,find similar text across python dataframe suppose python dataframe follows want find similar common text group example abc google com def google com poi google com point google com etc required output like data cleaning exercise clean unwanted part one way manually inspect code every possible group would million text way package python sorry asking without trying anything tried research without much success sure start anybody let know approach need taken also would helpful thanks
keep trailing punctuation in python nltk.word_tokenize,"<p>There's a ton available about removing punctuation, but I can't seem to find anything keeping it.</p>

<p>If I do:</p>

<pre><code>from nltk import word_tokenize
test_str = ""Some Co Inc. Other Co L.P.""
word_tokenize(test_str)
Out[1]: ['Some', 'Co', 'Inc.', 'Other', 'Co', 'L.P', '.']
</code></pre>

<p>the last ""."" is pushed into its own token. However, if instead there is another word at the end, the last ""."" is preserved:</p>

<pre><code>from nltk import word_tokenize
test_str = ""Some Co Inc. Other Co L.P. Another Co""
word_tokenize(test_str)
Out[1]: ['Some', 'Co', 'Inc.', 'Other', 'Co', 'L.P.', 'Another', 'Co']
</code></pre>

<p>I'd like this to always perform as the second case. For now, I'm hackishly doing:</p>

<pre><code>from nltk import word_tokenize
test_str = ""Some Co Inc. Other Co L.P.""
word_tokenize(test_str + "" |||"")
</code></pre>

<p>since I feel pretty confident in throwing away ""|||"" at any given time, but don't know what other punctuation I might want to preserve that could get dropped. Is there a better way to accomplish this ? </p>
",Preprocessing of the text & Tokenization,keep trailing punctuation python nltk word tokenize ton available removing punctuation seem find anything keeping last pushed token however instead another word end last preserved like always perform second case hackishly since feel pretty confident throwing away given time know punctuation might want preserve could get dropped better way accomplish
textmining graph sentences in python,"<p>I'm trying to solve a text mining problem in python which consist on:</p>

<p><strong>Target:</strong> Create a graph composed of nodes(sentences) by tokenizing a paragraph into sentences, their edges would be their similarity.</p>

<p>This isn't new at all, but the crux of the question is not very treated on the Internet. So, after getting the sentences from a paragraph the interesting point would be to compute a matrix of similarity between sentences(all combinations) to draw the graph. </p>

<p>Is there any package to perform the similairty between several vectors in an easy way?, even with a given list of strings make a graph of similarity...</p>

<p>A reproducible example:</p>

<pre><code># tokenize into sentences
&gt;&gt;&gt; from nltk import tokenize
&gt;&gt;&gt; p = ""Help my code works. The graph isn't still connected. The code computes the relationship in a graph. ""
&gt;&gt;&gt; sentences=tokenize.sent_tokenize(p)
  ['Help my code works.', ""The graph isn't still connected."", 'The code computes the relationship in a graph.']
&gt;&gt;&gt; len (sentences)
3

# compute similarity with dice coeffcient.
&gt;&gt;&gt;def dice_coefficient(a, b):
    """"""dice coefficient 2nt/na + nb.""""""
    a_bigrams = set(a)
    b_bigrams = set(b)
    overlap = len(a_bigrams &amp; b_bigrams)
    return overlap * 2.0/(len(a_bigrams) + len(b_bigrams)
&gt;&gt;&gt;dice_coefficient(sentences[1],sentences[2])
0.918918918918919
</code></pre>

<p>So, with this function I can do it manually and later make the graph with the nodes and the edges. But always a global solution (with n sentences) is the best one.</p>

<p>Any suggestion?</p>
",Preprocessing of the text & Tokenization,textmining graph sentence python trying solve text mining problem python consist target create graph composed node sentence tokenizing paragraph sentence edge would similarity new crux question treated internet getting sentence paragraph interesting point would compute matrix similarity sentence combination draw graph package perform similairty several vector easy way even given list string make graph similarity reproducible example function manually later make graph node edge always global solution n sentence best one suggestion
Extract sentences using regexpr in R,"<p>From a body of text, I would like to extract a sentence that starts with the word &quot;Meds:&quot; with regular expressions. I pattern that I used was:</p>
<pre><code>    &quot;[:blank:]Meds:[^.]*\\.&quot; 
</code></pre>
<p>so that my sentence would start just before the word &quot;Meds:&quot; followed by more words and end at the period. It didn't work the way I hoped it would.</p>
<p>Here's what happened:</p>
<pre><code>[1] Patient X came with the complain of pain Meds:ASA 81mg PO qd, 
toprol XL 25 mg PO Gen: Healthy appearing, overweight.
</code></pre>
<ol>
<li><p>My output didn't start at the word &quot;Meds:&quot; rather a whole sentence before it and continued to the next sentence until it matched the next 'period'. So it looked like this:</p>
</li>
<li><p>I would like it to stop at '25 mg PO' but I understand that it didn't do so because it couldn't find a 'period' there.</p>
<p>Any suggestions would be highly appreciated.</p>
</li>
</ol>
",Preprocessing of the text & Tokenization,extract sentence using regexpr r body text would like extract sentence start word med regular expression pattern used wa sentence would start word med followed word end period work way hoped would happened output start word med rather whole sentence next sentence matched next period looked like would like stop mg po understand find period suggestion would highly appreciated
Regular expression on tagged words,"<p>Given a string representing a sentence like this followed by tagging the string using OpenNLP. </p>

<p><code>String sentence = ""His plays remain highly popular, and are constantly studied."";</code></p>

<p>I get this below. My question is how do I know apply a regular expression to it to filter out tags? What is throwing me off is the word prepended to each hyphen. If it were just tags I can do something like <code>(VBP|VBN)+</code> for example, the words in front would vary.  </p>

<p><code>His_PRP$ plays_NNS remain_VBP highly_RB popular,_JJ and_CC are_VBP constantly_RB studied._VBN</code> </p>

<p>For example, how would I write a regular expression to keep all <code>NN</code> and <code>CC</code>? 
So given the tagged string as shown above how do I get <code>plays_NNS and_CC</code>? </p>
",Preprocessing of the text & Tokenization,regular expression tagged word given string representing sentence like followed tagging string using opennlp get question know apply regular expression filter tag throwing word prepended hyphen tag something like example word front would vary example would write regular expression keep given tagged string shown get
Predicting product&#39;s category by search term,"<p><strong>Problem:</strong> User performs product's search using search term, we should define most related category(categories is descending order) to that search term.</p>

<p><strong>Given:</strong> Products set, around 50000(Could be ten times more) products. Products contains title, description, and list of categories it belongs to.</p>

<p><strong>Model:</strong></p>

<p><strong>Pre-processing</strong> Perform stemming and remove stopwords from product's title and description. Put all unique stemmed words in WORDS list of size N. Put all categories to CATEGORIES list of size M.</p>

<p><strong>Fitting</strong> Use neural network which has N input neurons and M outputs.</p>

<p><strong>Training</strong> For product which has words w1, w3, w4, w6 input will be x=[1 0 1 1 0 1 ...] in which elements which index corresponds to thouse words index in WORDS will be set to 1. If product belongs to categories c1, c3, c25 it corresponds to y =[1 0 1 ... 1(25-th position)...] Predicting step. As input put user search term stemmed tokens that should as output give us prediction of most related category.</p>

<p>Is this model correct way for solving such a problem? What are the recommendation for hidden NN layers configuration. Any advice will be helpful, I'm completely new to Machine Learning.</p>

<p>Thank you!</p>
",Preprocessing of the text & Tokenization,predicting product category search term problem user performs product search using search term define related category category descending order search term given product set around could ten time product product contains title description list category belongs model pre processing perform stemming remove stopwords product title description put unique stemmed word word list size n put category category list size fitting use neural network ha n input neuron output training product ha word w w w w input x element index corresponds thouse word index word set product belongs category c c c corresponds th position predicting step input put user search term stemmed token output give u prediction related category model correct way solving problem recommendation hidden nn layer configuration advice helpful completely new machine learning thank
How to lemmatize with stanford-nlp tools?,"<p>I have installed the PHP API to the NLP Stanford tools (from <a href=""https://github.com/agentile/PHP-Stanford-NLP"" rel=""nofollow noreferrer"">https://github.com/agentile/PHP-Stanford-NLP</a>) and I have managed to obtain POS tagging using the code example included there:</p>

<pre><code>$pos = new \StanfordNLP\POSTagger(
  '/path/to/stanford-postagger-2014-08-27/models/english-left3words-distsim.tagger',
  '/path/to/stanford-postagger-2014-08-27/stanford-postagger.jar'
);
$result = $pos-&gt;tag(explode(' ', ""What does the fox say?""));
var_dump($result);
</code></pre>

<p>However, I also need lemmas. My question is: Is it possible to get them with the POS tagger? Or should I use the <a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">CoreNLP</a> for that? </p>

<p>If the latter is the case, how do I run it in PHP? </p>

<p>Thanks a lot.</p>
",Preprocessing of the text & Tokenization,lemmatize stanford nlp tool installed php api nlp stanford tool managed obtain po tagging using code example included however also need lemma question possible get po tagger use corenlp latter case run php thanks lot
How to filter out words in a corpus from a constrained vocabulary with gensim?,"<p>I am using gensim for topic modeling. I've created a corpus using </p>

<pre><code>wordDict = corpora.Dictionary(trimmedTextTokens)

gsCorpus = [wordDict.doc2bow(text) for text in trimmedTextTokens]
</code></pre>

<p>where trimmedTextTokens are the result of removing stop words. Now I want to filter out the terms from the corpus that are not in a list of a restricted or constructed vocabulary. Any ideas? Thank you!!</p>
",Preprocessing of the text & Tokenization,filter word corpus constrained vocabulary gensim using gensim topic modeling created corpus using trimmedtexttokens result removing stop word want filter term corpus list restricted constructed vocabulary idea thank
Why this does not work? Stop words in CountVectorizer,"<p>I'm using CountVectorizer to tokenize text and I want to add my own stop words. Why this doesn't work? The word 'de' shouldn't be in the final print.</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(ngram_range=(1,1),stop_words=frozenset([u'de']))
word_tokenizer = vectorizer.build_tokenizer()
print (word_tokenizer(u'Isto é um teste de qualquer coisa.'))

[u'Isto', u'um', u'teste', u'de', u'qualquer', u'coisa']
</code></pre>
",Preprocessing of the text & Tokenization,doe work stop word countvectorizer using countvectorizer tokenize text want add stop word work word de final print
tm package: stemCompletion not working,"<p>I have a simple code to perform text analytics. Before creating the DTM, I am applyting stemCompletion. However, the output of this is something which I am not understanding, whether I am doing it wrong, or this is the only way it behaves.</p>

<p>I have referred this link of rmy help: <a href=""https://stackoverflow.com/questions/16069406/text-mining-with-the-tm-package-word-stemming"">text-mining-with-the-tm-package-word-stemming</a></p>

<p>The issue that I see here is that after stemming, my DTm shrinks and doesn't return the tokens at all (returns 'content' 'meta')</p>

<p><strong>My code and Outputs:</strong></p>

<pre><code>texts &lt;- c(""i am member of the XYZ association"",
           ""apply for our open associate position"", 
           ""xyz memorial lecture takes place on wednesday"", 
           ""vote for the most popular lecturer"")

myCorpus &lt;- Corpus(VectorSource(texts))
myCorpus &lt;- tm_map(myCorpus, content_transformer(tolower))
myCorpus &lt;- tm_map(myCorpus, removePunctuation) 
myCorpus &lt;- tm_map(myCorpus, removeNumbers)
removeURL &lt;- function(x) gsub(""http[[:alnum:]]*"", """", x)
myCorpus &lt;- tm_map(myCorpus, content_transformer(removeURL))  #??
myCorpusCopy &lt;- myCorpus
myCorpus &lt;- tm_map(myCorpus, stemDocument)

for (i in 1:4) {
  cat(paste(""[["", i, ""]] "", sep = """"))
  writeLines(as.character(myCorpus[[i]]))
}

Output:
  [[1]] i am member of the xyz associ
  [[2]] appli for our open associ posit
  [[3]] xyz memori lectur take place on wednesday
  [[4]] vote for the most popular lectur


myCorpus &lt;- tm_map(myCorpus, stemCompletion, dictionary = myCorpusCopy)
for (i in 1:4) {
  cat(paste(""[["", i, ""]] "", sep = """"))
  writeLines(as.character(myCorpus[[i]]))
}

Output:
  [[1]] content
  meta
  [[2]] content
  meta
  [[3]] content
  meta
  [[4]] content
  meta

myCorpus &lt;- tm_map(myCorpus, PlainTextDocument)

dtm &lt;- DocumentTermMatrix(myCorpus, control = list(weighting = weightTf))
dtm
inspect(dtm)

Output:
  &gt; inspect(dtm)
  &lt;&lt;DocumentTermMatrix (documents: 4, terms: 2)&gt;&gt;
    Non-/sparse entries: 8/0
  Sparsity           : 0%
  Maximal term length: 7
  Weighting          : term frequency (tf)

  Terms
  Docs           content meta
  character(0)       1    1
  character(0)       1    1
  character(0)       1    1
  character(0)       1    1
</code></pre>

<p>Expected output: To successfully run stemming (both stemdocument and stemcompletion). I am using tm 0.6 package</p>
",Preprocessing of the text & Tokenization,tm package stemcompletion working simple code perform text analytics creating dtm applyting stemcompletion however output something understanding whether wrong way behaves referred link rmy help href p issue see stemming dtm shrink return token return content meta code output expected output successfully run stemming stemdocument stemcompletion using tm package
how to identify a end of a sentence,"<pre><code>String x="" i am going to the party at 6.00 in the evening. are you coming with me?"";
</code></pre>

<p>if i have the above string, i need that to be broken to sentences by using sentence boundry punctuations(like . and ?)</p>

<p>but it should not split the sentence at 6 because of having an pointer there. is there a way to identify what is the correct sentence boundry place in java? i have tried using stringTokenizer in java.util pakage but it always break the sentence whenever it finds a pointer. Can someone suggest me a method to do this correctly?</p>

<p>This is the method which i have tried in tokenizing a text into sentences. </p>

<pre><code>public static ArrayList&lt;String&gt; sentence_segmenter(String text) {
    ArrayList&lt;String&gt; Sentences = new ArrayList&lt;String&gt;();

    StringTokenizer st = new StringTokenizer(text, "".?!"");
    while (st.hasMoreTokens()) {

        Sentences.add(st.nextToken());
    }
    return Sentences;
}
</code></pre>

<p>also i have a method to segement sentences into phrases, but here also when the program found comma(,) it splits the text. but i dont need to split it when there is a number like 60,000 with a comma in the middle.  following is the method i am using to segment the phrases.</p>

<pre><code>   public static ArrayList&lt;String&gt; phrasesSegmenter(String text) {
    ArrayList&lt;String&gt; phrases = new ArrayList&lt;String&gt;();
    StringTokenizer st = new StringTokenizer(text, "","");
    while (st.hasMoreTokens()) {
        phrases.add(st.nextToken());
    }
    return phrases;
}
</code></pre>
",Preprocessing of the text & Tokenization,identify end sentence string need broken sentence using sentence boundry punctuation like split sentence pointer way identify correct sentence boundry place java tried using stringtokenizer java util pakage always break sentence whenever find pointer someone suggest method correctly method tried tokenizing text sentence also method segement sentence phrase also program found comma split text dont need split number like comma middle following method using segment phrase
How to keep the beginning and end of sentence markers with quanteda,"<p>I'm trying to create 3-grams using R's <code>quanteda</code> package.</p>

<p>I'm struggling to find a way to keep in the n-grams beginning and end of sentence markers, the <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> as in the code below.</p>

<p>I thought that using the <code>keptFeatures</code> with a regular expression that matched those should maintain them but the chevron markers are always removed.</p>

<p>How can I keep the chevron markers from being removed or what is the best way to delimit beginning and end of sentence with <code>quanteda</code>?</p>

<p>As a bonus question what is the advantage of <code>docfreq(mydfm)</code> over <code>colSums(mydfm)</code>, the result of str(colSums(mydfm)) and str(docfreq(mydfm)) is almost identical (<code>Named num [1:n]</code> the former, <code>Named int [1:n]</code> the latter)? </p>

<pre><code>library(quanteda)
text &lt;- ""&lt;s&gt;I'm a sentence and I'd better be formatted properly!&lt;/s&gt;&lt;s&gt;I'm a second sentence&lt;/s&gt;""

qc &lt;- corpus(text)

mydfm  &lt;- dfm(qc, ngram=3, removeNumbers = F, stem=T, keptFeatures=""\\&lt;/?s\\&gt;"")

names(colSums(mydfm))

# Output:
# [1] ""s_i'm_a""    ""i'm_a_sentenc""    ""a_sentenc_and""    ""sentenc_and_i'd""
# [2] ""and_i'd_better""   ""i'd_better_be""    ""better_be_format""   
# [3] ""be_format_proper"" ""format_proper_s""  ""proper_s_s""   ""s_s_i'm""    
# [4] ""i'm_a_second""   ""a_second_sentenc""   ""second_sentenc_s""
</code></pre>

<p><strong>EDIT:</strong></p>

<p>Corrected keepFeatures to keptFeatures in code snippet.</p>
",Preprocessing of the text & Tokenization,keep beginning end sentence marker quanteda trying create gram using r package struggling find way keep n gram beginning end sentence marker code thought using regular expression matched maintain chevron marker always removed keep chevron marker removed best way delimit beginning end sentence bonus question advantage result str colsums mydfm str docfreq mydfm almost identical former latter edit corrected keepfeatures keptfeatures code snippet
NLTK: How to filter stopwords and do stemming on N-gram tokens?,"<p>I have currently following pipe:</p>

<p>1) Text -> <code>from nltk.tokenize import word_tokenize</code> -> unigram tokens ['word1', 'word2', ..]</p>

<p>2) Filter unigram tokens on <code>from nltk.corpus import stopwords</code> and do stemming on each word</p>

<p>3) Get PoS from filtered unigram words</p>

<p>4) Get NE</p>

<p>But I want to handle N-gram tokens which are more informative, but confused when I should filter tokens from garbage AKA <code>from nltk.corpus import stopwords</code> + my own unigram stopwords and normalize words with stemming. Please advise:</p>

<p>1) Text -> Sentence Tokenizer = N-gram tokens</p>

<p>2) N-gram tokens -> PoS -> Named Entities</p>

<p>3) N-gram tokens -> Classification</p>
",Preprocessing of the text & Tokenization,nltk filter stopwords stemming n gram token currently following pipe text unigram token word word filter unigram token stemming word get po filtered unigram word get ne want handle n gram token informative confused filter token garbage aka unigram stopwords normalize word stemming please advise text sentence tokenizer n gram token n gram token po named entity n gram token classification
How to predict next word in sentence using ngram model in R,"<p>I have pre-processed text data into a corpus I would now  like to build a prediction model based on the previous 2 words (so I think a 3-gram model?). Based on my understanding of the articles I have read, here is how I am thinking of doing it:</p>

<p>step 1: enter two word phrase we wish to predict the next word for</p>

<pre><code># phrase our word prediction will be based on
phrase &lt;- ""I love""
</code></pre>

<p>step 2: calculate 3 gram frequencies</p>

<pre><code>library(RWeka)

threegramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))

dtm_threegram &lt;- DocumentTermMatrix(corpus, control=list(tokenize=threegramTokenizer))

threegram_freq &lt;- sort(colSums(as.matrix(dtm_threegram)), decreasing = TRUE)
</code></pre>

<p>The next step is where I am getting stuck. Conceptually, I think I should subset my 3-gram to only include three word combinations that start with ""I love"". Then, I should only keep the highest frequency 3-gram. For instance, if ""I love you"" appeared 12 times in my corpus and ""I love beer"" appeared 15 times, then the probability of ""beer"" being the next word is higher than ""love"" hence the model should return the former. Is this the correct approach and if so, how do I create something like this programmatically? My <code>threegram_freq</code> object appears to be of numeric class with a character attribute which I don't fully understand what that is. Is it possible to use a regular expression to only include elements starting with ""I love"" and then extract the 3rd word of the 3-gram with the highest frequency?</p>

<p>Thank you!</p>
",Preprocessing of the text & Tokenization,predict next word sentence using ngram model r pre processed text data corpus would like build prediction model based previous word think gram model based understanding article read thinking step enter two word phrase wish predict next word step calculate gram frequency next step getting stuck conceptually think subset gram include three word combination start love keep highest frequency gram instance love appeared time corpus love beer appeared time probability beer next word higher love hence model return former correct approach create something like programmatically object appears numeric class character attribute fully understand possible use regular expression include element starting love extract rd word gram highest frequency thank
StanfordNLP OpenIE 4 error,"<p>I've been encountering this error:</p>

<p>I ran the OpenIE 4.1 binary but got the following error: </p>

<pre><code>Exception in thread ""main"" java.lang.NullPointerException at 
com.googlecode.clearnlp.tokenization.EnglishTokenizer.protec‌​tEmoticons
(EnglishTokenizer.java:335) at 
com.googlecode.clearnlp.tokenization.EnglishTokenizer.getTok‌​enList(En 
glishTokenizer.java:109) at 
com.googlecode.clearnlp.tokenization.AbstractTokenizer.getTo‌​kens(AbstractTokenizer.java:58) at 
edu.knowitall.tool.tokenize.ClearTokenizer.tokenize(ClearTok‌​enizer.sc ala:22) 
</code></pre>

<p>I've looked up a few sources and found a comment by Yangrui who also had this problem in the past. But there are no solutions. I've checked my openie.4.1.jar file and the com.googlecode.clearnlp.tokenization.EnglishTokenizer.protectEmoticon exists. </p>

<p>Hope someone can help shed some light on this. Thank you in advance.</p>
",Preprocessing of the text & Tokenization,stanfordnlp openie error encountering error ran openie binary got following error looked source found comment yangrui also problem past solution checked openie jar file com googlecode clearnlp tokenization englishtokenizer protectemoticon exists hope someone help shed light thank advance
Detecting the start and end of dialog sections in prose,"<p>I have looked through a lot of the open source NLP tools (OpenNLP primarily) and I do not see anything which automates the task of detecting the start and end of dialog.</p>

<p>The sentence detection tools find the boundaries of the full sentence. The tokenizers accurately tokenize the punctuation, but still don't detect start and end. I've read many scholarly articles (<a href=""http://www.aclweb.org/anthology/P13-1129"" rel=""nofollow noreferrer"">such as</a>) where dialog detection is assumed. But I don't see any tools which automate this as general purpose dialog detection.</p>

<p>For instance, text like this:</p>

<pre><code>""I am happy,"" she said.
</code></pre>

<p>Should have ""I am happy,"" defined as dialog. Text like this:</p>

<pre><code>""This is a really long piece of dialog spoken by a character.

""That spans across multiple paragraphs.""
</code></pre>

<p>Should have the whole thing identified as dialog (even though the end of the first paragraph is missing the closing quotation mark). Also there are weirder ways of specifying dialog. Such as with dashes:</p>

<pre><code>They were walking when Joe spoke up.
--I really like walking.
</code></pre>

<p>Plus, often internal dialog will be denoted with italics, such as:</p>

<pre><code>Joe walked down the street. *I really hope I don't get hit by a bus.*
</code></pre>

<p>Is there an NLP tool that can detect dialog sections like this? Or a way to do this with OpenNLP that I just missed?</p>
",Preprocessing of the text & Tokenization,detecting start end dialog section prose looked lot open source nlp tool opennlp primarily see anything automates task detecting start end dialog sentence detection tool find boundary full sentence tokenizers accurately tokenize punctuation still detect start end read many scholarly article dialog detection assumed see tool automate general purpose dialog detection instance text like happy defined dialog text like whole thing identified dialog even though end first paragraph missing closing quotation mark also weirder way specifying dialog dash plus often internal dialog denoted italic nlp tool detect dialog section like way opennlp missed
add stemming support to CountVectorizer (sklearn),"<p>I'm trying to add stemming to my pipeline in NLP with sklearn.</p>

<pre><code>from nltk.stem.snowball import FrenchStemmer

stop = stopwords.words('french')
stemmer = FrenchStemmer()


class StemmedCountVectorizer(CountVectorizer):
    def __init__(self, stemmer):
        super(StemmedCountVectorizer, self).__init__()
        self.stemmer = stemmer

    def build_analyzer(self):
        analyzer = super(StemmedCountVectorizer, self).build_analyzer()
        return lambda doc:(self.stemmer.stem(w) for w in analyzer(doc))

stem_vectorizer = StemmedCountVectorizer(stemmer)
text_clf = Pipeline([('vect', stem_vectorizer), ('tfidf', TfidfTransformer()), ('clf', SVC(kernel='linear', C=1)) ])
</code></pre>

<p>When using this pipeline with the CountVectorizer of sklearn it works. And if I create manually the features like this it works also.<br/></p>

<pre><code>vectorizer = StemmedCountVectorizer(stemmer)
vectorizer.fit_transform(X)
tfidf_transformer = TfidfTransformer()
X_tfidf = tfidf_transformer.fit_transform(X_counts)
</code></pre>

<p><strong>EDIT</strong>:</p>

<p>If I try this pipeline on my IPython Notebook it displays the [*] and nothing happens. When I look at my terminal, it gives this error :<br/></p>

<pre><code>Process PoolWorker-12:
Traceback (most recent call last):
  File ""C:\Anaconda2\lib\multiprocessing\process.py"", line 258, in _bootstrap
    self.run()
  File ""C:\Anaconda2\lib\multiprocessing\process.py"", line 114, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Anaconda2\lib\multiprocessing\pool.py"", line 102, in worker
    task = get()
  File ""C:\Anaconda2\lib\site-packages\sklearn\externals\joblib\pool.py"", line 360, in get
    return recv()
AttributeError: 'module' object has no attribute 'StemmedCountVectorizer'
</code></pre>

<p><em>Example</em></p>

<p>Here is the complete example</p>

<pre><code>from sklearn.pipeline import Pipeline
from sklearn import grid_search
from sklearn.svm import SVC
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from nltk.stem.snowball import FrenchStemmer

stemmer = FrenchStemmer()
analyzer = CountVectorizer().build_analyzer()

def stemming(doc):
    return (stemmer.stem(w) for w in analyzer(doc))

X = ['le chat est beau', 'le ciel est nuageux', 'les gens sont gentils', 'Paris est magique', 'Marseille est tragique', 'JCVD est fou']
Y = [1,0,1,1,0,0]

text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SVC())])
parameters = { 'vect__analyzer': ['word', stemming]}

gs_clf = grid_search.GridSearchCV(text_clf, parameters, n_jobs=-1)
gs_clf.fit(X, Y)
</code></pre>

<p>If you remove stemming from the parameters it works otherwise it doesn't work.</p>

<p><strong>UPDATE</strong>:</p>

<p>The problem seems to be in the parallelization process because when removing <strong>n_jobs=-1</strong> the problem disappear.</p>
",Preprocessing of the text & Tokenization,add stemming support countvectorizer sklearn trying add stemming pipeline nlp sklearn using pipeline countvectorizer sklearn work create manually feature like work also edit try pipeline ipython notebook display nothing happens look terminal give error example complete example remove stemming parameter work otherwise work update problem seems parallelization process removing n job problem disappear
"Why NLTK uses regular expressions for word tokenization, but training for sentence tokenization?","<p>I am using NLTK in python. I understood that it uses regular expressions in its word tokenization functions, such as TreebankWordTokenizer.tokenize(), but it uses trained models (pickle files) for sentence tokenization. I don't understand why they don't use training for word tokenization? Does it imply that sentence tokenization is a harder task?</p>
",Preprocessing of the text & Tokenization,nltk us regular expression word tokenization training sentence tokenization using nltk python understood us regular expression word tokenization function treebankwordtokenizer tokenize us trained model pickle file sentence tokenization understand use training word tokenization doe imply sentence tokenization harder task
What is the formal process of cleaning unstructured data,"<p>I needed help with a couple of things.. I am new to NLP and unstructured data cleaning.. can someone answer the following questions... Thanks</p>

<ol>
<li>need help with regex to identify words like _male and female_ or more generic like _word and word_ or _something_something_something and get rid of the underscore that is present in the beginning or the end but not in the middle.</li>
<li>I wanted to know the formal process of cleaning the data, like are there any steps that we have to follow for cleaning unstructured data, im asking this because I am doing lemmatization (with POS) and replacing the commonly occurring words like (something, something) to something_something. so what steps should I follow? I am doing the following right now-tokenize_clean>remove_numbers>remove_url>remove_slash>remove_cross>remove_garbage>replace_hypen_with_underscore>lemmatize_sentence>change_words_to_bigrams>remove_smaller_than_3(words with len smaller then 3)>remove_simlutaneous( words that occurred simultaneously many times eg, death death death)>remove_location>remove_bullets>remove_stop>remove_simlutaneous</li>
</ol>

<p>Should I do something different in these steps?</p>

<ol start=""3"">
<li>I also have words like (group'shealthplanbecauseeitheroneofthefollowingqualifyingeventshappens) , (whenyouuseanon_networkprovider) ,(per\xad) ,(vlfldq\x10vxshuylvhg)
how should I handle them? ignore them completely or try to improve them?</li>
</ol>

<p>My final goal is to classify the documents into Yes and No class. Any suggestions are welcomed. </p>

<p>Will provide more examples and explanation if required.</p>
",Preprocessing of the text & Tokenization,formal process cleaning unstructured data needed help couple thing new nlp unstructured data cleaning someone answer following question thanks need help regex identify word like male female generic like word word something something something get rid underscore present beginning end middle wanted know formal process cleaning data like step follow cleaning unstructured data im asking lemmatization po replacing commonly occurring word like something something something something step follow following right tokenize clean remove number remove url remove slash remove cross remove garbage replace hypen underscore lemmatize sentence change word bigram remove smaller word len smaller remove simlutaneous word occurred simultaneously many time eg death death death remove location remove bullet remove stop remove simlutaneous something different step also word like group shealthplanbecauseeitheroneofthefollowingqualifyingeventshappens whenyouuseanon networkprovider per xad vlfldq x vxshuylvhg handle ignore completely try improve final goal classify document yes class suggestion welcomed provide example explanation required
NLP - why is &quot;not&quot; a stop word?,"<p>I am trying to remove stop words before performing topic modeling. I noticed that some negation words (not, nor, never, none etc..) are usually considered to be stop words. For example, NLTK, spacy and sklearn include ""not"" on their stop word lists. However, if we remove ""not"" from these sentences below they lose the significant meaning and that would not be accurate for topic modeling or sentiment analysis.</p>

<pre><code>1). StackOverflow is helpful      =&gt; StackOverflow helpful
2). StackOverflow is not helpful  =&gt; StackOverflow helpful
</code></pre>

<p>Can anyone please explain why these negation words are typically considered to be stop words? </p>
",Preprocessing of the text & Tokenization,nlp stop word trying remove stop word performing topic modeling noticed negation word never none etc usually considered stop word example nltk spacy sklearn include stop word list however remove sentence lose significant meaning would accurate topic modeling sentiment analysis anyone please explain negation word typically considered stop word
issue recognizing NEs with StanfordNER in python NLTK,"<p>This happens when there is a potential NE followed by a comma, for example if my strings are something like,</p>

<blockquote>
  <p>""These names  Praveen Kumar,,  David Harrison,  Paul Harrison, blah ""</p>
</blockquote>

<p>or </p>

<blockquote>
  <p>""California, United States""</p>
</blockquote>

<p>my output is something as follows, respectively.</p>

<blockquote>
  <p>[[(u'These', u'O'), (u'names', u'O'), (u'Praveen', u'O'), (u'Kumar,,', u'O'), (u'David', u'PERSON'), (u'Harrison,', u'O'), (u'Paul', u'PERSON'), (u'Harrison,', u'O'), (u'blah', u'O')]]</p>
</blockquote>

<p>or </p>

<blockquote>
  <p>[[(u'California,', u'O'), (u'United', u'LOCATION'), (u'States', u'LOCATION')]] </p>
</blockquote>

<p>why it doesn't recognize potential NEs such as ""Praveen Kumar"", ""Harrison"" and ""California""? </p>

<p>Here is how is use it in the code:</p>

<pre><code>from nltk.tag.stanford import NERTagger
st = NERTagger('stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')

tags = st.tag(""California, United States"".split())
</code></pre>

<p>Is it because I tokenize the input stirng with <code>split()</code> ? How can I resolve this as it's working fine when tried in Java?</p>
",Preprocessing of the text & Tokenization,issue recognizing ne stanfordner python nltk happens potential ne followed comma example string something like name praveen kumar david harrison paul harrison blah california united state output something follows respectively u u u name u u praveen u u kumar u u david u person u harrison u u paul u person u harrison u u blah u u california u u united u location u state u location recognize potential ne praveen kumar harrison california use code tokenize input stirng resolve working fine tried java
building a lexer with very many tokens,"<p>I've been searching for two hours now And I don't really know what to do.</p>

<p>I'm trying to build a analyser which use a lexer that can match several thousand words. Those are natural language words, that's why they are so many.</p>

<p>I tried first in a simple way with just 1000 differents matches for one token :</p>

<pre><code>    TOKEN :
{
    &lt;VIRG: "",""&gt;
|   &lt;COORD: ""et""&gt;
|   &lt;ADVERBE: ""vraiment""&gt;
|   &lt;DET: ""la""&gt;
|   &lt;ADJECTIF: ""bonne""&gt;
|   &lt;NOM: ""pomme""
        |   ""émails""
        |   ""émaux""
        |   ""APL""
        |   ""APLs""
        |   ""Acide""
        |   ""Acides""
        |   ""Inuk""

[...]
</code></pre>

<p>After javac compilation it returns that the code is too large.</p>

<p>So, how could I manage thousands tokens in my lexer ?</p>

<ol>
<li><p>I've read that it is more efficient to use n tokens for each word, than using one token for n words. But in this case I will have rules with 1000+ tokens, which doesn't look like a better idea;</p></li>
<li><p>I could modify the token manager, or build one, so it just matchs words in a list;</p></li>
<li><p>Here I know that the lexer is a finite state machine, and this is why it's not possible, so is there anyway to use an other lexer ? ;</p></li>
<li><p>I could automatically generate a huge regular expression which match every word, but that wouldn't let me handle the words independantly afterwards, and I'm not sure that writing a 60-lines-regular-expression would be a great idea;</p></li>
<li><p>Maybe is there a way to load the token from a file, this solution is pretty close to solutions 2 and 3;</p></li>
<li><p>Maybe I should use another language ? I'm trying to migrate from XLE (which can handle a lexicon of more than 70 000 tokens) to java, and what is interesting here is to generate java files !</p></li>
</ol>

<p>So here it is, I can find my way to handle several thousands tokens with a javacc lexer. That would be great if any one is use to that and have an idea ?</p>

<p>Best</p>

<p>Corentin</p>
",Preprocessing of the text & Tokenization,building lexer many token searching two hour really know trying build analyser use lexer match several thousand word natural language word many tried first simple way differents match one token javac compilation return code large could manage thousand token lexer read efficient use n token word using one token n word case rule token look like better idea could modify token manager build one match word list know lexer finite state machine possible anyway use lexer could automatically generate huge regular expression match every word let handle word independantly afterwards sure writing line regular expression would great idea maybe way load token file solution pretty close solution maybe use another language trying migrate xle handle lexicon token java interesting generate java file find way handle several thousand token javacc lexer would great one use idea best corentin
add Lucene PorterStemmer into MLlib pipeline,"<p>I am trying to write a pipe using spark.ml.feature, basically tokenizer, stopwords, and stemmer. Because spark doesn't have stemmer, I plan to add Lucene's. My two questions are:</p>

<p>1) Can anyone show me the syntax for Lucene stemmer pipeline? I found one which must put in an argument PorterStemFilter(TokenStream in). But this doesn't fit into pipeline (without any arguments yet). </p>

<p>2) Is Lucene's API fully compatible with spark's</p>
",Preprocessing of the text & Tokenization,add lucene porterstemmer mllib pipeline trying write pipe using spark ml feature basically tokenizer stopwords stemmer spark stemmer plan add lucene two question anyone show syntax lucene stemmer pipeline found one must put argument porterstemfilter tokenstream fit pipeline without argument yet lucene api fully compatible spark
What would be the right way to extract intent from natural language input in this case?,"<p>Before posting this question, I spent a whole day reading stuff from machine learning and nlp under tags section on Stackoverflow.</p>

<p>I have an input statement of the following form</p>

<pre><code>""I am looking for an iPhone 6S possibly rose gold with 16 GB memory, what is the best deal that I can get on this""
</code></pre>

<p>Here is what I want from that line</p>

<pre><code>{intent: ""discount"", brand: ""Apple"", productLine: ""iPhone"", model: ""6S"", color: ""rose gold"", memory: ""16GB""}
</code></pre>

<p>My query could be about phones, laptops, anything and may or may not be specific about a particular model. For example, it could have been ""What is the best mobile phone to buy"" </p>

<p>Here is what I am planning to do but would love some feedback or suggestions if you guys think there is a better way to do it
<strong>Stage 1</strong>
Cleaning text, tokenize, remove stop words
<strong>Stage 2</strong>
Extract category, brand, model,product line from this sentence. I believe I will need a database of some sort that has all this information and I will simply have to do a fuzzy match with the brand name inside the sentence. Not sure how to do this in the most efficient manner. </p>

<p>One approach is to scan the complete database with possibly 1000s of models and then take the sentence and check if the brand word is present or not. I believe this has to be a fuzzy search just in case the person writes i-Ball instead of iBall</p>

<p><strong>Stage 3</strong>
Feature extraction such as rose gold and 16 GB memory. Should I use a regex here or are there more sophisticated methods to extract such info. </p>

<p>One approach that I thought of was to extract unigrams, bigrams and trigrams from the input sentence then compare it with the product specification in a fuzzy manner. What about record linkage libraries for this?</p>

<p><strong>Stage 4</strong></p>

<p>How do I free the sentence of all the extra junk such as product name and features and classify it into a discount or a price range or review type query? I am assuming that a classifier works nicely when the sentences are not stuffed with product info inside it otherwise the classifier is going to need a huge training set.</p>

<p><strong>Stage 5</strong>
How do I know when to show a specific product and when to show generic stuff. For example a query about iPhone above is quite specific whereas if I am asking about the best mobile phone, its a generic one. Should I use a Naive Bayesian classifier for this or logistic regression.</p>

<p><strong>The Ultimate question</strong>
What is the best way to go about such an implementation
NLTK + Scikitlearn
TFLearn
TensorFlow</p>

<p>I am assuming that neural networks are only going to accept numbers and output numbers. Does that mean I will have to convert the input to a vector representation.</p>

<p>Thank you for your suggestions in advance.</p>
",Preprocessing of the text & Tokenization,would right way extract intent natural language input case posting question spent whole day reading stuff machine learning nlp tag section stackoverflow input statement following form want line query could phone laptop anything may may specific particular model example could best mobile phone buy planning would love feedback suggestion guy think better way stage cleaning text tokenize remove stop word stage extract category brand model product line sentence believe need database sort ha information simply fuzzy match brand name inside sentence sure efficient manner one approach scan complete database possibly model take sentence check brand word present believe ha fuzzy search case person writes ball instead iball stage feature extraction rose gold gb memory use regex sophisticated method extract info one approach thought wa extract unigrams bigram trigram input sentence compare product specification fuzzy manner record linkage library stage free sentence extra junk product name feature classify discount price range review type query assuming classifier work nicely sentence stuffed product info inside otherwise classifier going need huge training set stage know show specific product show generic stuff example query iphone quite specific whereas asking best mobile phone generic one use naive bayesian classifier logistic regression ultimate question best way go implementation nltk scikitlearn tflearn tensorflow assuming neural network going accept number output number doe mean convert input vector representation thank suggestion advance
Is it possible to do lemmatization independently in spacy?,"<p>I'm using spacy to preprocess the data for sentiment analysis.</p>

<p>What I want to do is:</p>

<p>1) Lemmatization<br>
2) POS tagging on lemmatized words</p>

<p>But since spacy does all the process at once when the parser is called it's doing all the calculations twice. Is there an option to disable non-required calculations?</p>
",Preprocessing of the text & Tokenization,possible lemmatization independently spacy using spacy preprocess data sentiment analysis want lemmatization po tagging lemmatized word since spacy doe process parser called calculation twice option disable non required calculation
BioNLP stanford - tokenization,"<p>I try to tokenize a biomedical text so I decided to use <a href=""http://nlp.stanford.edu/software/eventparser.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/eventparser.shtml</a>. I used the stand-alone program RunBioNLPTokenizer that does what I want.</p>

<p>Now, I want to create my own program that uses Stanford libraries. So, I read the code from RunBioNLPTokenizer describing below.</p>

<pre><code>package edu.stanford.nlp.ie.machinereading.domains.bionlp;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.PrintStream;
import java.util.Collection;
import java.util.List;
import java.util.Properties;

import edu.stanford.nlp.ie.machinereading.GenericDataSetReader;
import edu.stanford.nlp.ie.machinereading.msteventextractor.DataSet;
import edu.stanford.nlp.ie.machinereading.msteventextractor.EpigeneticsDataSet;
import edu.stanford.nlp.ie.machinereading.msteventextractor.GENIA11DataSet;
import edu.stanford.nlp.ie.machinereading.msteventextractor.InfectiousDiseasesDataSet;
import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.util.StringUtils;

/**
 * Standalone program to run our BioNLP tokenizer and save its output
 */
public class RunBioNLPTokenizer extends GenericDataSetReader {

  public static void main(String[] args) throws IOException {
    Properties props = StringUtils.argsToProperties(args);
    String basePath = props.getProperty(""base.directory"", ""/u/nlp/data/bioNLP/2011/originals/"");

    DataSet dataset = new GENIA11DataSet();
    dataset.getFilesystemInformation().setTokenizer(""stanford"");
    runTokenizerForDirectory(dataset, basePath + ""genia/training"");
    runTokenizerForDirectory(dataset, basePath + ""genia/development"");
    runTokenizerForDirectory(dataset, basePath + ""genia/testing"");

    dataset = new EpigeneticsDataSet();
    dataset.getFilesystemInformation().setTokenizer(""stanford"");
    runTokenizerForDirectory(dataset, basePath + ""epi/training"");
    runTokenizerForDirectory(dataset, basePath + ""epi/development"");
    runTokenizerForDirectory(dataset, basePath + ""epi/testing"");

    dataset = new InfectiousDiseasesDataSet();
    dataset.getFilesystemInformation().setTokenizer(""stanford"");
    runTokenizerForDirectory(dataset, basePath + ""infect/training"");
    runTokenizerForDirectory(dataset, basePath + ""infect/development"");
    runTokenizerForDirectory(dataset, basePath + ""infect/testing"");
  }

  private static void runTokenizerForDirectory(DataSet dataset, String path) throws IOException {
    System.out.println(""Input directory: "" + path);
    BioNLPFormatReader reader = new BioNLPFormatReader();    
    for (File rawFile : reader.getRawFiles(path)) {
      System.out.println(""Input filename: "" + rawFile.getName());
      String rawText = IOUtils.slurpFile(rawFile);

      String docId = rawFile.getName().replace(""."" + BioNLPFormatReader.TEXT_EXTENSION, """");
      String parentPath = rawFile.getParent();

      runTokenizer(dataset.getFilesystemInformation().getTokenizedFilename(parentPath, docId), rawText);
    }
  }

  private static void runTokenizer(String tokenizedFilename, String text) {
    System.out.println(""Tokenized filename: "" + tokenizedFilename);
    Collection&lt;String&gt; sentences = BioNLPFormatReader.splitSentences(text);

    PrintStream os = null;
    try {
      os = new PrintStream(new FileOutputStream(tokenizedFilename));
    } catch (IOException e) {
      System.err.println(""ERROR: cannot save online tokenization to "" + tokenizedFilename);
      e.printStackTrace();
      System.exit(1);
    }

    for (String sentence : sentences) {
      BioNLPFormatReader.BioNLPTokenizer tokenizer = new BioNLPFormatReader.BioNLPTokenizer(sentence);
      List&lt;CoreLabel&gt; tokens = tokenizer.tokenize();
      for (CoreLabel l : tokens) {
        os.print(l.word() + "" "");
      }
      os.println();
    }
    os.close();
  }
}
</code></pre>

<p>I wrote the below code. I achieved to split the text into sentences but I can't use the BioNLPTokenizer as it is used in RunBioNLPTokenizer.</p>

<pre><code>public static void main(String[] args) throws Exception {
  // TODO code application logic here
  Collection&lt;String&gt; c =BioNLPFormatReader.splitSentences("".."");
  for (String sentence : c) {
    System.out.println(sentence);
    BioNLPFormatReader.BioNLPTokenizer x = BioNLPFormatReader.BioNLPTokenizer(sentence);
  }
} 
</code></pre>

<p>I took this error</p>

<blockquote>
  <p>Exception in thread ""main"" java.lang.RuntimeException: Uncompilable source code - edu.stanford.nlp.ie.machinereading.domains.bionlp.BioNLPFormatReader.BioNLPTokenizer has protected access in edu.stanford.nlp.ie.machinereading.domains.bionlp.BioNLPFormatReader</p>
</blockquote>

<p>My question is. How can I tokenize a biomedical sentence according to Stanford libraries without using RunBioNLPTokenizer?</p>
",Preprocessing of the text & Tokenization,bionlp stanford tokenization try tokenize biomedical text decided use used stand alone program runbionlptokenizer doe want want create program us stanford library read code runbionlptokenizer describing wrote code achieved split text sentence use bionlptokenizer used runbionlptokenizer took error exception thread main java lang runtimeexception uncompilable source code edu stanford nlp ie machinereading domain bionlp bionlpformatreader bionlptokenizer ha protected access edu stanford nlp ie machinereading domain bionlp bionlpformatreader question tokenize biomedical sentence according stanford library without using runbionlptokenizer
"Splitting sentences using nltk.sent_tokenize, it does not provide correct result","<p>I am trying to split some customers' comments to sentences using <code>nltk.sent_tokenize</code>. I already tried to solve some of the problems using the following code: </p>

<pre><code>comment = comment.replace('?', '? ').replace('!', '! ').replace('..','.').replace('.', '. ')
</code></pre>

<p>But I do not know how to solve the following problems: </p>

<ol>
<li><p>Customer used several <code>"".""</code> after some sentences. For example:</p>

<pre><code>Think tool is a huge factor in this....i have only
</code></pre></li>
<li><p>Customer used several <code>""!""</code> after some sentences, such as <code>auditory subject everyday!!!!!</code> </p></li>
<li><p>some of them used combination of <code>""!""</code> and <code>"".""</code> at the end of sentences. </p></li>
<li><p>Because I already used <code>replace('.', '. ')</code>, it also causes the following problem:</p>

<p>Weight gain <code>(20lbs.)</code>, was split to <code>(20lbs.</code>  <code>)</code></p></li>
</ol>

<p>Any suggestion? I am using Python.</p>
",Preprocessing of the text & Tokenization,splitting sentence using nltk sent tokenize doe provide correct result trying split customer comment sentence using already tried solve problem using following code know solve following problem customer used several sentence example customer used several sentence used combination end sentence already used also cause following problem weight gain wa split suggestion using python
Snowball Stemming: defining Null Region,"<p>I'm trying to understand the snowball stemming algorithmus. <a href=""https://stackoverflow.com/questions/31848056/snowball-stemming-defining-regions"">HW90</a> has had a similar question with examples, but not mine. The algorithmus is using two regions R1 and R2 that are definied as follows:</p>

<blockquote>
  <p>R1 is the region after the first non-vowel following a vowel, or is
  the null region at the end of the word if there is no such non-vowel.</p>
  
  <p>R2 is the region after the first non-vowel following a vowel in R1, or
  is the null region at the end of the word if there is no such
  non-vowel.</p>
  
  <p><a href=""http://snowball.tartarus.org/texts/r1r2.html"" rel=""nofollow noreferrer"">http://snowball.tartarus.org/texts/r1r2.html</a></p>
</blockquote>

<p>I don't understand, what ""the null region at the end of the word"" is. Could anybody give me some examples for that, please?</p>
",Preprocessing of the text & Tokenization,snowball stemming defining null region trying understand snowball stemming algorithmus understand null region end word could anybody give example please
Lemmatization java,"<p>I am looking for a <a href=""http://en.wikipedia.org/wiki/Lemmatisation"" rel=""noreferrer"">lemmatisation</a> implementation for English in Java. I found a few already, but I need something that does not need to much memory to run (1 GB top).
Thanks. I do not need a stemmer.</p>
",Preprocessing of the text & Tokenization,lemmatization java looking lemmatisation implementation english java found already need something doe need much memory run gb top thanks need stemmer
NLTK - nltk.tokenize.RegexpTokenizer - regex not working as expected,"<p>I am trying to Tokenize text using RegexpTokenizer.</p>

<p><strong>Code:</strong></p>

<pre><code>from nltk.tokenize import RegexpTokenizer
#from nltk.tokenize import word_tokenize

line = ""U.S.A Count U.S.A. Sec.of U.S. Name:Dr.John Doe J.Doe 1.11 1,000 10--20 10-20""
pattern = '[\d|\.|\,]+|[A-Z][\.|A-Z]+\b[\.]*|[\w]+|\S'
tokenizer = RegexpTokenizer(pattern)

print tokenizer.tokenize(line)
#print word_tokenize(line)
</code></pre>

<p><strong>Output:</strong></p>

<blockquote>
  <p>['U', '.', 'S', '.', 'A', 'Count', 'U', '.', 'S', '.', 'A', '.', 'Sec', '.', 'of', 'U', '.', 'S', '.', 'Name', ':', 'Dr', '.', 'John', 'Doe', 'J', '.', 'Doe', '1.11', '1,000', '10', '-', '-', '20', '10', '-', '20']</p>
</blockquote>

<p><strong>Expected Output:</strong></p>

<blockquote>
  <p>['U.S.A', 'Count', 'U.S.A.', 'Sec', '.', 'of', 'U.S.', 'Name', ':', 'Dr', '.', 'John', 'Doe', 'J.', 'Doe', '1.11', '1,000', '10', '-', '-', '20', '10', '-', '20']</p>
</blockquote>

<p>Why tokenizer is also spiltting my expected tokens ""U.S.A"" , ""U.S.""?
How can I resolve this issue?</p>

<p>My regex : <a href=""https://regex101.com/r/dS1jW9/1"" rel=""noreferrer"">https://regex101.com/r/dS1jW9/1</a></p>
",Preprocessing of the text & Tokenization,nltk nltk tokenize regexptokenizer regex working expected trying tokenize text using regexptokenizer code output u count u sec u name dr john doe j doe expected output u count u sec u name dr john doe j doe tokenizer also spiltting expected token u u resolve issue regex
Negation Marking with Regular Expressions in Python,"<p>I'm struggling to implement negation marking using regex in Python, a la Christopher Potts' <a href=""http://sentiment.christopherpotts.net/lingstruc.html#fig:tokenizer_accuracy_neg"" rel=""nofollow noreferrer"">sentiment analysis tutorial</a>.</p>
<p>The definition of a negation, taken from his tutorial is:</p>
<pre><code>(?:
    ^(?:never|no|nothing|nowhere|noone|none|not|
        havent|hasnt|hadnt|cant|couldnt|shouldnt|
        wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint
    )$
)
|
n't
</code></pre>
<p>and the definition of clause-level punctuation is:</p>
<pre><code>^[.:;!?]$
</code></pre>
<p>The idea is to capture words between a negation and clause-level punctuation and then to modify them to indicate that they are being negated, eg:</p>
<pre><code>No one enjoys it.
</code></pre>
<p>should become something like:</p>
<pre><code>No one_NEG enjoys_NEG it_NEG.
</code></pre>
<p>Any suggestions would be appreciated.</p>
",Preprocessing of the text & Tokenization,negation marking regular expression python struggling implement negation marking using regex python la christopher potts sentiment analysis tutorial definition negation taken tutorial definition clause level punctuation idea capture word negation clause level punctuation modify indicate negated eg become something like suggestion would appreciated
"Count vectorizing into bigrams for one document, and then taking the average","<p>I'm trying to write a function that takes in one document, count vectorizes the bigrams for that document. This shouldn't have any zeroes, as I'm only doing this to one document at a time. Then I want to take the average of those numbers to get a sense of bigram repetition.</p>

<p>Any problems with this code?</p>

<pre><code>def avg_bigram(x):
    bigram_vectorizer =  CountVectorizer(stop_words='english', ngram_range=(2,2))
    model = bigram_vectorizer.fit_transform(x)
    vector = model.toarray()
    return vector.mean()
</code></pre>

<p>I've tested it with text that I know contains more than stop words, and I get back</p>

<p>""empty vocabulary; perhaps the documents only contain stop words""</p>

<p>Thank you for any help!</p>
",Preprocessing of the text & Tokenization,count vectorizing bigram one document taking average trying write function take one document count vectorizes bigram document zero one document time want take average number get sense bigram repetition problem code tested text know contains stop word get back empty vocabulary perhaps document contain stop word thank help
"How can I perform word tokenization of a text, using the tokenize annotator, with pycorenlp (Python wrapper for Stanford CoreNLP), without ssplit?","<p>I am trying to run <a href=""https://github.com/smilli/py-corenlp"" rel=""nofollow noreferrer"">pycorenlp</a>, which is a Python wrapper for <a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">Stanford CoreNLP</a>, to perform word tokenization of a text, using the <a href=""http://stanfordnlp.github.io/CoreNLP/tokenize.html#description"" rel=""nofollow noreferrer""><code>tokenize</code></a> annotator.</p>

<p>I first launch a Stanford CoreNLP: </p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 50000
</code></pre>

<p>then run:</p>

<pre><code>from pycorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('http://localhost:9000')

text_input = 'this is a test.'
print('text_input: {0}'.format(text_input))
text_output = nlp.annotate(text_input, properties={
                    'annotators': 'tokenize',
                    'outputFormat': 'json'
                })
print('text_output: {0}'.format(text_output))
</code></pre>

<p>Surprisingly, this gives no output:</p>

<pre><code>text_input: this is a test.
text_output: {}
</code></pre>

<p>Why?</p>

<hr>

<p>If I add the <a href=""http://stanfordnlp.github.io/CoreNLP/ssplit.html"" rel=""nofollow noreferrer""><code>ssplit</code></a>, then <code>text_output</code> isn't empty anymore:</p>

<pre><code>text_input = 'this is a test.'
print('text_input: {0}'.format(text_input))
text_output = nlp.annotate(text_input, properties={
                    'annotators': 'tokenize,ssplit',
                    'outputFormat': 'json'
                })
print('text_output: {0}'.format(text_output))
</code></pre>

<p>outputs:</p>

<pre><code>text_input: this is a test.
text_output: {u'sentences': [{u'parse': u'SENTENCE_SKIPPED_OR_UNPARSABLE', u'index': 0, u'tokens': [{u'index': 1, u'word': u'this', u'after': u' ', u'characterOffsetEnd': 4, u'characterOffsetBegin': 0, u'originalText': u'this', u'before': u''}, {u'index': 2, u'word': u'is', u'after': u' ', u'characterOffsetEnd': 7, u'characterOffsetBegin': 5, u'originalText': u'is', u'before': u' '}, {u'index': 3, u'word': u'a', u'after': u' ', u'characterOffsetEnd': 9, u'characterOffsetBegin': 8, u'originalText': u'a', u'before': u' '}, {u'index': 4, u'word': u'test', u'after': u'', u'characterOffsetEnd': 14, u'characterOffsetBegin': 10, u'originalText': u'test', u'before': u' '}, {u'index': 5, u'word': u'.', u'after': u'', u'characterOffsetEnd': 15, u'characterOffsetBegin': 14, u'originalText': u'.', u'before': u''}]}]}
</code></pre>

<p>Can't I use the <a href=""http://stanfordnlp.github.io/CoreNLP/tokenize.html#description"" rel=""nofollow noreferrer""><code>tokenize</code></a> annotator without having to use the <a href=""http://stanfordnlp.github.io/CoreNLP/ssplit.html"" rel=""nofollow noreferrer""><code>ssplit</code></a> annotator?</p>

<p>The  <a href=""http://stanfordnlp.github.io/CoreNLP/dependencies.html"" rel=""nofollow noreferrer"">overview of the annotator dependencies</a> seems to say I should be able to use the <a href=""http://stanfordnlp.github.io/CoreNLP/tokenize.html#description"" rel=""nofollow noreferrer""><code>tokenize</code></a> annotator alone:</p>

<p><a href=""https://i.sstatic.net/cqPgh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cqPgh.png"" alt=""enter image description here""></a></p>
",Preprocessing of the text & Tokenization,perform word tokenization text using tokenize annotator pycorenlp python wrapper stanford corenlp without ssplit trying run pycorenlp python wrapper stanford corenlp perform word tokenization text using annotator first launch stanford corenlp run surprisingly give output add empty anymore output use annotator without use annotator overview annotator dependency seems say able use annotator alone
Find the Number of Distinct Topics After LDA in Python/ R,"<p>As far as I know, I need to fix the number of topics for LDA modeling in Python/ R. However, say I set <code>topic=10</code> while the results show that, for a document, nine topics are all about 'health' and the distinct number of topics for this document is <code>2</code> indeed. How can I spot it without examining the key words of each topic and manually count the real distinct topics?</p>

<p>P.S. I googled and learned that there are Vocabulary Word Lists (Word Banks) by Theme, and I could pair each topic with a theme according to the word lists. If several topics fall into the same theme, then I can combine them into one distinct topic. I guess it's an approach worth trying and I'm looking for smarter ideas, thanks.</p>
",Preprocessing of the text & Tokenization,find number distinct topic lda python r far know need fix number topic lda modeling python r however say set result show document nine topic health distinct number topic document indeed spot without examining key word topic manually count real distinct topic p googled learned vocabulary word list word bank theme could pair topic theme according word list several topic fall theme combine one distinct topic guess approach worth trying looking smarter idea thanks
"Text Mining - most common words, normalized","<p>I am a researcher and have about 17,000 free-text documents of which around 30-40% are associated with my outcome. Is there an open-source tool I can use to determine the most common words (or even phrases, but not necessary) that are associated with the outcome, normalizing for the frequency of words that are already occurring? All of the documents are written by health care workers, so it will be important to normalize since there will be technical language across both documents and also would want to screen out words like ""the"", ""it"", etc. </p>

<p>What I am trying to do is build a tool using regular expressions or NLP that will then use these words to identify the outcome based on new documents. I'm not planning on spending a huge amount of time customizing an NLP tool, so something with reasonable accuracy is good enough. </p>

<p>I know SAS, SQL (am using postgreSQL), and Python, but could potentially get by in R. I haven't done any NLP before. Is there any software I could use that doesn't have too steep of a learning curve? Thanks!</p>
",Preprocessing of the text & Tokenization,text mining common word normalized researcher free text document around associated outcome open source tool use determine common word even phrase necessary associated outcome normalizing frequency word already occurring document written health care worker important normalize since technical language across document also would want screen word like etc trying build tool using regular expression nlp use word identify outcome based new document planning spending huge amount time customizing nlp tool something reasonable accuracy good enough know sa sql using postgresql python could potentially get r done nlp software could use steep learning curve thanks
delete stop words in a file in python,"<p>I have a file which consists of stop words (each in a new line) and another file (a corpus actually) which consists of a lot of sentences each in a new line. I have to delete the stop words in the corpus and return each line of that without stop words. I wrote a code but it just returns one sentence. (The language is Persian). How can fix it that it returns all of the sentences?  </p>

<pre><code>with open (""stopwords.txt"", encoding = ""utf-8"") as f1:
   with open (""train.txt"", encoding = ""utf-8"") as f2:
      for i in f1:
          for line in f2:
              if i in line:
                 line= line.replace(i, """")
with open (""NoStopWordsTrain.txt"", ""w"", encoding = ""utf-8"") as f3:
   f3.write (line)
</code></pre>
",Preprocessing of the text & Tokenization,delete stop word file python file consists stop word new line another file corpus actually consists lot sentence new line delete stop word corpus return line without stop word wrote code return one sentence language persian fix return sentence
Storing list in a pandas DataFrame column,"<p>I am trying to do some text processing using NLTK and Pandas. </p>

<p>I have DataFrame with column 'text'. I want to add column 'text_tokenized' that will be stored as a nested list.</p>

<p>My code for tokenizing text is: </p>

<pre><code>def sent_word_tokenize(text):
    text = unicode(text, errors='replace')
    sents = sent_tokenize(text)
    tokens = map(word_tokenize, sents)

    return tokens
</code></pre>

<p>Currently, I am trying to apply this function as following:</p>

<pre><code>df['text_tokenized'] = df.apply(lambda row: sent_word_tokenize(row.text), axis=1)
</code></pre>

<p>Which gives me error:</p>

<pre><code>ValueError: Shape of passed values is (100, 3), indices imply (100, 21)
</code></pre>

<p>Not sure how to fix it and what is wrong here.</p>
",Preprocessing of the text & Tokenization,storing list panda dataframe column trying text processing using nltk panda dataframe column text want add column text tokenized stored nested list code tokenizing text currently trying apply function following give error sure fix wrong
applying word2vec on small text files,"<p>I'm totally new to word2vec so please bear it with me. I have a set of text files each containing a set of tweets, between 1000-3000. I have chosen a common keyword (<code>""kw1""</code>) and I want to find semantically relevant terms for <code>""kw1""</code> using word2vec. For example if the keyword is <code>""apple""</code> I would expect to see related terms such as <code>""ipad"" ""os"" ""mac""</code>... based on the input file. So this set of related terms for <code>""kw1""</code> would be different for each input file as word2vec would be trained on individual files (eg., 5 input files, run word2vec 5 times on each file). </p>

<p>My goal is to find sets of related terms for each input file given the common keyword (<code>""kw1""</code>), which would be used for some other purposes. </p>

<p>My questions/doubts are:</p>

<ul>
<li>Does it make sense to use word2vec for a task like this? is it technically right to use considering the small size of an input file?</li>
</ul>

<p>I have downloaded the code from code.google.com: <a href=""https://code.google.com/p/word2vec/"" rel=""nofollow"">https://code.google.com/p/word2vec/</a> and have just given it a dry run as follows:</p>

<pre><code> time ./word2vec -train $file -output vectors.bin -cbow 1 -size 200 -window 10 -negative 25 -hs 1 -sample 1e-3 -threads 12 -binary 1 -iter 50

./distance vectors.bin 
</code></pre>

<ul>
<li><p>From my results I saw I'm getting many noisy terms (stopwords) when I'm using the <code>'distance'</code> tool to get related terms to <code>""kw1""</code>. So I did remove stopwords and other noisy terms such as user mentions. But I haven't seen anywhere that word2vec requires cleaned input data?</p></li>
<li><p>How do you choose right parameters? I see the results (from running the <code>distance</code> tool) varies greatly when I change parameters such as <code>'-window'</code>, <code>'-iter'</code>. Which technique should I use to find the correct values for the parameters. (manual trial and error is not possible for me as I'll be scaling up the dataset). </p></li>
</ul>
",Preprocessing of the text & Tokenization,applying word vec small text file totally new word vec please bear set text file containing set tweet chosen common keyword want find semantically relevant term using word vec example keyword would expect see related term based input file set related term would different input file word vec would trained individual file eg input file run word vec time file goal find set related term input file given common keyword would used purpose question doubt doe make sense use word vec task like technically right use considering small size input file downloaded code code google com given dry run follows result saw getting many noisy term stopwords using tool get related term remove stopwords noisy term user mention seen anywhere word vec requires cleaned input data choose right parameter see result running tool varies greatly change parameter technique use find correct value parameter manual trial error possible scaling dataset
R - Package tm - Which terms correspond to each common root after stemming?,"<p>Corpus created, stopwords defined, cleansing done (removePunctuation, removeNumbers, tolower...).</p>

<p>The corpus is now ready to be stemmed. The function is executed correctly and all works as it should, but...</p>

<p><strong>I need to know which words are being stemmed to each common root. Is that possible using the tm package? Or any other package?</strong></p>

<p>For example, <em>TermA1, TermA2, TermB1, TermB2, TermB3</em>, all of them are stemmed to <em>Term</em> and my new Corpus reflect only <em>Term</em>. However, I need also to know which words are associated with each root word, and therefore an optimal output should be:</p>

<pre><code>Term     Stemm
TermA1   Term
TermA2   Term
TermB1   Term
TermB2   Term
TermB3   Term
...
WordA1   Word
WordB1   Word
WordB2   Word
WordB3   Word
WordC1   Word
</code></pre>
",Preprocessing of the text & Tokenization,r package tm term correspond common root stemming corpus created stopwords defined cleansing done removepunctuation removenumbers tolower corpus ready stemmed function executed correctly work need know word stemmed common root possible using tm package package example terma terma termb termb termb stemmed term new corpus reflect term however need also know word associated root word therefore optimal output
Keyword suggestion Algorithm,"<p>I have been working on a project which asks me to give keyword/keyphrase suggestion based on description of the product. </p>

<p>What I have currently: Description of the Product, Category of product(May or may not be present). </p>

<p>What I want: Machine generated keywords/keyphrases based on description.</p>

<p>What research I have done: (NLP Based approach) This problem can be broken down into two separate approaches.     </p>

<ul>
<li>Not using the past Data : Just summarizing on current description</li>
<li>Method: - Tokenization, stemming, stopwords removal etc. (Preprocessing)</li>
<li>Shallow NLP (Constituency Parsing) and retain only NP &amp; JJ phrases.</li>
</ul>

<p>This would be an approach which doesn't use description present in database.</p>

<p>What I was looking for is a better approach which uses ML algorithms and also uses my past product description data.</p>

<p>I was thinking about applying shallow parsing on entire dataset, and then give keywords which encounters in more than N number of products.    </p>

<p>What algorithm or approach would come in handy? 
How can I use my data?</p>
",Preprocessing of the text & Tokenization,keyword suggestion algorithm working project asks give keyword keyphrase suggestion based description product currently description product category product may may present want machine generated keywords keyphrases based description research done nlp based approach problem broken two separate approach using past data summarizing current description method tokenization stemming stopwords removal etc preprocessing shallow nlp constituency parsing retain np jj phrase would approach use description present database wa looking better approach us ml algorithm also us past product description data wa thinking applying shallow parsing entire dataset give keywords encounter n number product algorithm approach would come handy use data
How to extract special characters using NLTK RegexpParser Chunk for POS_tagged words in Python,"<p>I have some text for example say: <code>80% of $300,000 Each Human Resource/IT Department.</code></p>

<p>I would need to extract <code>$300,000</code> along with the words <code>Each Human Resource/IT Department</code></p>

<p>I have used pos tagging to tag the words after tokenizing. I was able to extract 300,000 but not able to extract $ sign along with it.</p>

<p>What I have so far:</p>

<pre><code>text = '80% of $300,000 Each Human Resource/IT Department'
train_text = text
sample_text = text
custom_sent_tokenizer = PunktSentenseTokenizer(train_text)
tokenized = custom_sent_tokenizer.tokenize(sample_text)

for i in tokenized:
    words = nltk.word_tokenize(i)
    tagged = nltk.pos_tag(words)

chunkGram = r""""""chunk: {&lt;DT&gt;+&lt;NN.*&gt;+&lt;NN.*&gt;?|&lt;NNP&gt;?|&lt;CD&gt;+&lt;NN&gt;?|&lt;NNP&gt;?}""""""


chunkParser = nltk.RegexpParser(chunkGram)
chunked = chunkParser.parse(tagged)
</code></pre>

<p>chunked output when coverted to list  - <code>['80 %', '300,000', 'Each Human Resource/IT Department']</code></p>

<p>What I wanted : <code>['80 %', '**$**300,000', 'Each Human Resource/IT Department']</code></p>

<p>I tried </p>

<p><code>chunkGram = r""""""chunk: {**&lt;/$CD&gt;|**&lt;DT&gt;+&lt;NN.*&gt;+&lt;NN.*&gt;?|&lt;NNP&gt;?|&lt;CD&gt;+&lt;NN&gt;?|</code>?}""""""</p>

<p>It still doesn't work. So, all I need is a <strong>$</strong> along with <strong>CD</strong></p>
",Preprocessing of the text & Tokenization,extract special character using nltk regexpparser chunk po tagged word python text example say would need extract along word used po tagging tag word tokenizing wa able extract able extract sign along far chunked output coverted list wanted tried still work need along cd
while performing stopwords on dataset in python3 getting error in printing function,"<p>I'm trying to remove stop words using <code>NLTK</code>. I have a syntax error in the fourth line, but the first three lines are working fine.</p>

<pre><code>  File ""&lt;stdin&gt;"", line 1
   print [i for i in senten
           ^
 SyntaxError: invalid syntax
</code></pre>

<p>My code:</p>

<pre><code>from nltk.corpus import stopwords
stop = stopwords.words('english')
sentence = ""this is a foo bar sentence""
print [i for i in sentence.split() if i not in stop]
</code></pre>
",Preprocessing of the text & Tokenization,performing stopwords dataset python getting error printing function trying remove stop word using syntax error fourth line first three line working fine code
algorithm for sentence matching after calculating word similarity using nltk,"<p>Aim- User inputs a string. I need to compare this input with Sentence 1 and Sentence 2 and find the maximum similarity with either of these sentences.</p>

<p>Current Approach- I tokenize the input and both sentences, find synonym sets of each token and compare maximum similarity by adding similarity for each token using nltk .path_similarity(token1,token2).</p>

<p>Problem- If sentence 1 is short and sentence 2 is long with many tokens, since I sum up individual similarities, the similarity of sentence 2 with input is always more even if most of tokens of input match with sentence 1.</p>

<p>One solution- I can divide the similarity of each sentence with length of sentence and hence I get similarity per token of Sentence. But this approach is too aggressive. Is there an industry standard approach for this?</p>
",Preprocessing of the text & Tokenization,algorithm sentence matching calculating word similarity using nltk aim user input string need compare input sentence sentence find maximum similarity either sentence current approach tokenize input sentence find synonym set token compare maximum similarity adding similarity token using nltk path similarity token token problem sentence short sentence long many token since sum individual similarity similarity sentence input always even token input match sentence one solution divide similarity sentence length sentence hence get similarity per token sentence approach aggressive industry standard approach
Stanford NLP: punctuation error identification,"<p>I've just started working with Stanford NLP core. </p>

<p>My problem is that many of the sentences in my corpus do not end with a period (full stop). </p>

<p>Frankly, a bit of string parsing with regular expressions could probably fix the issue, but with some degree of error.</p>

<p>I am curious as to whether Stanford NLP can identify missing periods.</p>
",Preprocessing of the text & Tokenization,stanford nlp punctuation error identification started working stanford nlp core problem many sentence corpus end period full stop frankly bit string parsing regular expression could probably fix issue degree error curious whether stanford nlp identify missing period
Where to find an exhaustive list of stop words?,"<p>Where could I find an exhaustive list of stop words? The one I have is quite short and it seems to be inapplicable to scientific texts. 
I am creating lexical chains to extract key topics from scientific papers. The problem is that words like <code>based</code>, <code>regarding</code>, etc. should also be considered as stop words as they do not deliver much sense.</p>
",Preprocessing of the text & Tokenization,find exhaustive list stop word could find exhaustive list stop word one quite short seems inapplicable scientific text creating lexical chain extract key topic scientific paper problem word like etc also considered stop word deliver much sense
Python regex: return the whole sentence with a certain word in it from period to period,"<p>Below is the sentence I want to process. It consists of IDs which start with two hashes at the front (<code>##2312435</code>) plus remaining text. I need a regular expression that would find sentences with word <code>likely</code> in them and would retrieve the whole sentence plus the ID. </p>

<p>Sentence:</p>

<blockquote>
  <p>##2312435 Jon is not home. John is likely to come home after midnight. Another not related sentence. ##2233442 Mark is very angry. Mark is likely to have a beer tonight.</p>
</blockquote>

<p>I managed to write this:</p>

<pre><code>(?=.\*((?&lt;=##)\d+))(?=.*([^.]+(likely)+[^.]+))
</code></pre>

<p>but that does not retrieve the full sentence.</p>

<p>Expected result would look like: 2312435 John is likely to come home after midnight, 2233442 Mark is likely to have a beer tonight</p>

<p>I am asking this just for general knowledge, because I am struggling to get in grip with lookarounds. I already solved this without regex, but I thought I could give it a go and try to retrieve the needed info using regex. Thanks </p>
",Preprocessing of the text & Tokenization,python regex return whole sentence certain word period period sentence want process consists id start two hash front plus remaining text need regular expression would find sentence word would retrieve whole sentence plus id sentence jon home john likely come home midnight another related sentence mark angry mark likely beer tonight managed write doe retrieve full sentence expected result would look like john likely come home midnight mark likely beer tonight asking general knowledge struggling get grip lookarounds already solved without regex thought could give go try retrieve needed info using regex thanks
Porter stemmer gives different results when calculating semantic similarity,"<p>I am  making some tests with ws4j library. In particular I want to calculate similarity between two test words ""university"" and ""teaching"". When I apply stemming, it gives me 0 similarity... When I do not apply stemming, the result is higher than 0.
On the other hand, when I check the similarity between ""genders"" and ""sex"", then stemming has a reverse impact: when I use it, it gives a positive similarity. Otherwise the similarity is equal to 0.</p>

<p>Why does it happen and which would be a more generic approach that would give similar results for both examples? </p>

<pre><code>public class TestWs4j
{    
    private static ILexicalDatabase db = new NictWordNet();
    private static RelatednessCalculator[] rcs = {
            new WuPalmer(db), // new HirstStOnge(db), new LeacockChodorow(db), new Lesk(db),
            new JiangConrath(db), new Path(db) // new Resnik(db), new Lin(db),
    };

    private static void run( String word1, String word2 ) {
        WS4JConfiguration.getInstance().setMFS(true);
        for ( RelatednessCalculator rc : rcs ) {
            double s = rc.calcRelatednessOfWords(word1, word2);
            System.out.println( rc.getClass().getName()+""\t""+s );
        }
    }
    public static void main(String[] args) {
        long t0 = System.currentTimeMillis();
        PorterStemmer stemmer = new PorterStemmer();
//        String w1 = stemmer.stemWord(""university"");
//        String w2 = stemmer.stemWord(""teaching"");
//        run(w1,w2);
        run(""university"",""teaching"");
        long t1 = System.currentTimeMillis();
        System.out.println( ""Done in ""+(t1-t0)+"" msec."" );
    }
}
</code></pre>
",Preprocessing of the text & Tokenization,porter stemmer give different result calculating semantic similarity making test w j library particular want calculate similarity two test word university teaching apply stemming give similarity apply stemming result higher hand check similarity gender sex stemming ha reverse impact use give positive similarity otherwise similarity equal doe happen would generic approach would give similar result example
Techniques for extracting regular expressions out of a labeled data set,"<p>Let's suppose I have a data set of several hundred thousand strings (which happen to be natural language sentences, if it matters) which are each tagged with a certain ""label"". Each sentence is tagged with exactly one label, and there are about 10 labels, each with approximately 10% of the data set belonging to them. There is a high degree of similarity to the structure of sentences within a label.</p>

<p>I know the above sounds like a classical example of a machine learning problem, but I want to ask a slightly different question. <strong>Are there any known techniques for programatically generating a set of regular expressions for each label, which can successfully classify the training data while still generalizing to future test data?</strong></p>

<p>I would be very happy with references to the literature; I realize that this will not be a straightforward algorithm :)</p>

<p><strong>PS:</strong> I know that the normal way to do classification is with machine learning techniques like an SVM or such. I am, however, explicitly looking for a way to generate <em>regular expressions</em>. (I would be happy with with machine learning techniques for generating the regular expressions, just not with machine learning techniques for doing the classification itself!)</p>
",Preprocessing of the text & Tokenization,technique extracting regular expression labeled data set let suppose data set several hundred thousand string happen natural language sentence matter tagged certain label sentence tagged exactly one label label approximately data set belonging high degree similarity structure sentence within label know sound like classical example machine learning problem want ask slightly different question known technique programatically generating set regular expression label successfully classify training data still generalizing future test data would happy reference literature realize straightforward algorithm p know normal way classification machine learning technique like svm however explicitly looking way generate regular expression would happy machine learning technique generating regular expression machine learning technique classification
Difference between Python&#39;s collections.Counter and nltk.probability.FreqDist,"<p>I want to calculate the term-frequencies of words in a text corpus. I've been using NLTK's word_tokenize followed by probability.FreqDist for some time to get this done. The word_tokenize returns a list, which is converted to a frequency distribution by FreqDist. However, I recently came across the Counter function in collections (collections.Counter), which seems to be doing the exact same thing. Both FreqDist and Counter have a most_common(n) function which return the n most common words. Does anyone know if there's a difference between these two? Is one faster than the other? Are there cases where one would work and the other wouldn't?</p>
",Preprocessing of the text & Tokenization,difference python collection counter nltk probability freqdist want calculate term frequency word text corpus using nltk word tokenize followed probability freqdist time get done word tokenize return list converted frequency distribution freqdist however recently came across counter function collection collection counter seems exact thing freqdist counter common n function return n common word doe anyone know difference two one faster case one would work
find job role from text data,"<p>I have a text file from which i have to extract on what role the people are working. ""Mechanical engineer"",""software developer"" etc.
I have used NLTK to extract this using grammer like,</p>

<pre><code>grammer= r""""""
          NP: {&lt;NN.*|JJ&gt;*&lt;NN.*&gt;}  """"""
</code></pre>

<p>the result i am getting is good, but still for few documnets junk is coming. for those lines i want to apply Regular expressions.</p>

<p>my sample texts are like this.</p>

<ul>
<li>""I am software developement engineer in microsoft""</li>
<li>""I am mechanical engineer with 10 years experience""</li>
</ul>

<p>what i want is, I will extract two or three words before ""Engineer"".
I m using regular expression like,</p>

<pre><code>regex=re.compile('|'.join([r'(?:\S+\s)?\S*[eE]ngineer']))
</code></pre>

<p>but, it extracts only one word before the specific word. How to make it to extract two or more words.?</p>

<p>i tried putting {2-3} in place of ""?"" in expression. but i am not getting desired result.</p>

<p>Is my approach correct ?
or any other approach to extract this specific phrase in better way ?</p>
",Preprocessing of the text & Tokenization,find job role text data text file extract role people working mechanical engineer software developer etc used nltk extract using grammer like result getting good still documnets junk coming line want apply regular expression sample text like software developement engineer microsoft mechanical engineer year experience want extract two three word engineer using regular expression like extract one word specific word make extract two word tried putting place expression getting desired result approach correct approach extract specific phrase better way
Word Association In R,"<p>I am searching for a solution/library or any function that finds the most frequent word associations within a paragraph. For example: </p>

<blockquote>
  <p>This tree gives red apple. Bananas are yellow. The apple I ate was red. </p>
</blockquote>

<p>In the above text, we should be able to get Association of each word with all other words in the sentence (after removing stop words and stemming). So lets say the above text gives association as:</p>

<p>tree - red      : 0.41
tree - apple    : 0.46
bananas - yellow: 0.30
apple - red     : 0.8</p>

<p>The most frequent two words repeated in the text are ""apple - red"" combination since both of the words occur in two sentences. </p>

<p>The two solutions I have tried are :</p>

<ol>
<li><p>findAssoc() of tm library: </p>

<pre><code>      Word AssociatedWord Association  
1    apple            red           1
2    apple            ate         0.5
3    apple           tree         0.5
4      red          apple           1
5      red            ate         0.5
6      red           tree         0.5
7      ate          apple         0.5 
8      ate            red         0.5  
9  bananas         yellow           1  
10    tree          apple         0.5 
11    tree            red         0.5
12  yellow        bananas           1
</code></pre>

<p>The result shown above is the output of text given above. Sentences are entered individually as it does not find association on a single line text.</p></li>
<li><p>A customized solution using most frequent n-grams: this is not feasible since it only checks consecutively occurring words. </p></li>
</ol>

<p>I am just looking for a solution that gives the most frequent word association. I can't break the text into multiple lines so would there be any solution of such kind?
Any help would be appreciated.</p>
",Preprocessing of the text & Tokenization,word association r searching solution library function find frequent word association within paragraph example tree give red apple banana yellow apple ate wa red text able get association word word sentence removing stop word stemming let say text give association tree red tree apple banana yellow apple red frequent two word repeated text apple red combination since word occur two sentence two solution tried findassoc tm library result shown output text given sentence entered individually doe find association single line text customized solution using frequent n gram feasible since check consecutively occurring word looking solution give frequent word association break text multiple line would solution kind help would appreciated
python tokenization UnicodeDecodeError,"<p>I'm trying to tokenize some documents but I have this error</p>

<blockquote>
  <p>UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position
  6: ordinal not in range(128)</p>
</blockquote>

<pre><code>import nltk
import pandas as pd

df = pd.DataFrame(pd.read_csv('status2.csv'))
documents = df['status']

result = [nltk.word_tokenize(sent) for sent in documents]
</code></pre>

<p>I think it's the unicode problem so I added</p>

<pre><code>documents = unicode(documents, 'utf-8')
</code></pre>

<p>another error</p>

<blockquote>
  <p>TypeError: coercing to Unicode: need string or buffer, Series found</p>
</blockquote>

<pre><code>print documents

1      Brandon Cachia ,All I know is that,you're so n...
2      Melissa Zejtunija:HAM AND CHEESE BIEX INI??? *...
3                         .........Where is my mind?????
4      Having a philosophical discussion with Trudy D...
</code></pre>
",Preprocessing of the text & Tokenization,python tokenization unicodedecodeerror trying tokenize document error unicodedecodeerror ascii codec decode byte xef position ordinal range think unicode problem added another error typeerror coercing unicode need string buffer series found
How to omit tokenize and ssplit annotators for Sentiment Analysis,"<p>For the task of sentiment analysis on a text, I am using the following annotators to create a pipeline:</p>

<p>annotators = tokenize, ssplit, parse, sentiment</p>

<p>After reading the documentation on annotators, I realized that tokenize and ssplit take the whole text and break it up into separate sentences to be consdiered for further parsing.
The problem on which I am working currently is sentiment analysis of tweets. Since tweets most of the times do not exceed more than a line, using a tokenize and ssplit annotator before parse seems overkill. </p>

<p>I tried to exclude the first two but it won't let me do giving out a message Exception in thread ""main"" java.lang.IllegalArgumentException: annotator ""parse"" requires annotator ""tokenize""</p>

<p>Is there any way to avoid using the tokenize and ssplit annotators to imrpove efficiency ?</p>
",Preprocessing of the text & Tokenization,omit tokenize ssplit annotator sentiment analysis task sentiment analysis text using following annotator create pipeline annotator tokenize ssplit parse sentiment reading documentation annotator realized tokenize ssplit take whole text break separate sentence consdiered parsing problem working currently sentiment analysis tweet since tweet time exceed line using tokenize ssplit annotator parse seems overkill tried exclude first two let giving message exception thread main java lang illegalargumentexception annotator parse requires annotator tokenize way avoid using tokenize ssplit annotator imrpove efficiency
Modify NLTK word_tokenize to prevent tokenization of parenthesis,"<p>I have the following <code>main.py</code>.</p>

<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/env python
# vim: set noexpandtab tabstop=2 shiftwidth=2 softtabstop=-1 fileencoding=utf-8:

import nltk
import string
import sys
for token in nltk.word_tokenize(''.join(sys.stdin.readlines())):
    #print token
    if len(token) == 1 and not token in string.punctuation or len(token) &gt; 1:
        print token
</code></pre>

<p>The output is the following.</p>

<pre class=""lang-none prettyprint-override""><code>./main.py &lt;&lt;&lt; 'EGR1(-/-) mouse embryonic fibroblasts'
EGR1
-/-
mouse
embryonic
fibroblasts
</code></pre>

<p>I want to slightly change the tokenizer so that it will recognize <code>EGR1(-/-)</code> as one token (without any other changes). Does anybody know if there is a such way to slighly modify the tokenizer? Thanks.</p>
",Preprocessing of the text & Tokenization,modify nltk word tokenize prevent tokenization parenthesis following output following want slightly change tokenizer recognize one token without change doe anybody know way slighly modify tokenizer thanks
figuring out if an apostrophe is a quote or contraction,"<p>I am looking for a way to go through a sentence to see if an apostrophe is a quote or a contraction so I can remove punctuation from the string, and then normalize all words.</p>

<p>My test sentence is: <code>don't frazzel the horses. 'she said wow'.</code></p>

<p>In my attempts I have split the sentence into words parts tokonizing on words and non words like so:</p>

<pre><code>contractionEndings = [""d"", ""l"", ""ll"", ""m"", ""re"", ""s"", ""t"", ""ve""]

sentence = ""don't frazzel the horses. 'she said wow'."".split(/(\w+)|(\W+)/i).reject! { |word| word.empty? }
</code></pre>

<p>This returns <code>[""don"", ""'"", ""t"", "" "", ""frazzel"", "" "", ""the"", "" "", ""horses"", "". '"", ""she"", "" "", ""said"", "" "", ""wow"", ""'.""]</code></p>

<p>Next I want to be able to iterate sentence looking for apostrophes <code>'</code> and when one is found, compare the next element to see if it is included in the <code>contractionEndings</code> array. If it is included I want to join the prefix, the apostrophe <code>'</code>, and the suffix into one index, else remove the apostrophes.</p>

<p>In this example, <code>don</code>, <code>'</code>, and <code>t</code> would be joined into <code>don't</code> as a single index, but <code>. '</code> and <code>'.</code> would be removed.</p>

<p>Afterwards I can run a regex to remove other punctuation from the sentence so that I can pass it into my stemmer to normalize the input.</p>

<p>The final output I am after is <code>don't frazzel the horses she said wow</code> in which all punctuation will be removed besides apostrophes for contractions.</p>

<p>If anyone has any suggestions to make this work or have a better idea on how to solve this problem I would like to know.</p>

<p>Overall I want to remove all punctuation from the sentence except for contractions.</p>

<p>Thanks</p>
",Preprocessing of the text & Tokenization,figuring apostrophe quote contraction looking way go sentence see apostrophe quote contraction remove punctuation string normalize word test sentence attempt split sentence word part tokonizing word non word like return next want able iterate sentence looking apostrophe one found compare next element see included array included want join prefix apostrophe suffix one index else remove apostrophe example would joined single index would removed afterwards run regex remove punctuation sentence pas stemmer normalize input final output punctuation removed besides apostrophe contraction anyone ha suggestion make work better idea solve problem would like know overall want remove punctuation sentence except contraction thanks
How to add a new language Support in Lucene Solr,"<p>It seems to me that lucene currently does not support bengali; how can i add support for bengali content?
and what nlp methods(stemming, tokenizing etc.) lucene uses for indexing and search?</p>

<p>my content will be mainly in bengali and with some english words</p>

<p>thanks</p>

<p>edit: it seems there are some info here, but it does not contain enough detail.</p>

<pre><code>http://wiki.apache.org/lucene-java/IndexingOtherLanguages
</code></pre>
",Preprocessing of the text & Tokenization,add new language support lucene solr seems lucene currently doe support bengali add support bengali content nlp method stemming tokenizing etc lucene us indexing search content mainly bengali english word thanks edit seems info doe contain enough detail
Why stemDocument change an ending &#39;y&#39; to &#39;i&#39;? How to stop it?,"<p>When using stemDocument{SnowBallC} in R, I found it would change a letter 'y' at then end of a word to letter 'i', for example:</p>

<pre><code>&gt; stemDocument('sleepy', language='english')
[1] ""sleepi""
</code></pre>

<p>I'm wondering how can we prevent this in stemming?</p>

<p>Thanks,
Ming</p>
",Preprocessing of the text & Tokenization,stemdocument change ending stop using stemdocument snowballc r found would change letter end word letter example wondering prevent stemming thanks ming
How to resolve &quot;Missing the manifest.properties&quot; in OpenNLP?,"<p>I'm trying to use OpenNLP for tokenization. I don't know what's the problem. Following is the exception:</p>

<pre>
opennlp.tools.util.InvalidFormatException: Missing the manifest.properties!
        at opennlp.tools.util.model.BaseModel.validateArtifactMap(BaseModel.java:209)
        at opennlp.tools.tokenize.TokenizerModel.validateArtifactMap(TokenizerModel.java:108)
        at opennlp.tools.util.model.BaseModel.(BaseModel.java:142)
        at opennlp.tools.tokenize.TokenizerModel.(TokenizerModel.java:93)
        at pk.edu.kics.JSFController.getAllTokens(JSFController.java:105)
        at pk.edu.kics.JSFController.(JSFController.java:47)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at java.lang.Class.newInstance0(Class.java:355)
        at java.lang.Class.newInstance(Class.java:308)
        at com.sun.faces.mgbean.BeanBuilder.newBeanInstance(BeanBuilder.java:188)
        at com.sun.faces.mgbean.BeanBuilder.build(BeanBuilder.java:102)
        at com.sun.faces.mgbean.BeanManager.createAndPush(BeanManager.java:405)
        at com.sun.faces.mgbean.BeanManager.create(BeanManager.java:267)
        at com.sun.faces.el.ManagedBeanELResolver.getValue(ManagedBeanELResolver.java:86)
        at javax.el.CompositeELResolver.getValue(CompositeELResolver.java:175)
        at com.sun.faces.el.FacesCompositeELResolver.getValue(FacesCompositeELResolver.java:72)
        at com.sun.el.parser.AstIdentifier.getValue(AstIdentifier.java:99)
        at com.sun.el.parser.AstValue.getValue(AstValue.java:158)
        at com.sun.el.ValueExpressionImpl.getValue(ValueExpressionImpl.java:219)
        at com.sun.faces.facelets.el.TagValueExpression.getValue(TagValueExpression.java:102)
        at javax.faces.component.ComponentStateHelper.eval(ComponentStateHelper.java:190)
        at javax.faces.component.ComponentStateHelper.eval(ComponentStateHelper.java:178)
        at javax.faces.component.UIOutput.getValue(UIOutput.java:168)
        at com.sun.faces.renderkit.html_basic.HtmlBasicInputRenderer.getValue(HtmlBasicInputRenderer.java:205)
        at com.sun.faces.renderkit.html_basic.HtmlBasicRenderer.getCurrentValue(HtmlBasicRenderer.java:338)
        at com.sun.faces.renderkit.html_basic.HtmlBasicRenderer.encodeEnd(HtmlBasicRenderer.java:164)
        at javax.faces.component.UIComponentBase.encodeEnd(UIComponentBase.java:878)
        at javax.faces.component.UIComponent.encodeAll(UIComponent.java:1620)
        at javax.faces.render.Renderer.encodeChildren(Renderer.java:168)
        at javax.faces.component.UIComponentBase.encodeChildren(UIComponentBase.java:848)
        at javax.faces.component.UIComponent.encodeAll(UIComponent.java:1613)
        at javax.faces.component.UIComponent.encodeAll(UIComponent.java:1616)
        at javax.faces.component.UIComponent.encodeAll(UIComponent.java:1616)
        at com.sun.faces.application.view.FaceletViewHandlingStrategy.renderView(FaceletViewHandlingStrategy.java:380)
        at com.sun.faces.application.view.MultiViewHandler.renderView(MultiViewHandler.java:126)
        at com.sun.faces.lifecycle.RenderResponsePhase.execute(RenderResponsePhase.java:127)
        at com.sun.faces.lifecycle.Phase.doPhase(Phase.java:101)
        at com.sun.faces.lifecycle.LifecycleImpl.render(LifecycleImpl.java:139)
        at javax.faces.webapp.FacesServlet.service(FacesServlet.java:313)
        at org.apache.catalina.core.StandardWrapper.service(StandardWrapper.java:1523)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:279)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:188)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:641)
        at com.sun.enterprise.web.WebPipeline.invoke(WebPipeline.java:97)
        at com.sun.enterprise.web.PESessionLockingStandardPipeline.invoke(PESessionLockingStandardPipeline.java:85)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:185)
        at org.apache.catalina.connector.CoyoteAdapter.doService(CoyoteAdapter.java:325)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:226)
        at com.sun.enterprise.v3.services.impl.ContainerMapper.service(ContainerMapper.java:165)
        at com.sun.grizzly.http.ProcessorTask.invokeAdapter(ProcessorTask.java:791)
        at com.sun.grizzly.http.ProcessorTask.doProcess(ProcessorTask.java:693)
        at com.sun.grizzly.http.ProcessorTask.process(ProcessorTask.java:954)
        at com.sun.grizzly.http.DefaultProtocolFilter.execute(DefaultProtocolFilter.java:170)
        at com.sun.grizzly.DefaultProtocolChain.executeProtocolFilter(DefaultProtocolChain.java:135)
        at com.sun.grizzly.DefaultProtocolChain.execute(DefaultProtocolChain.java:102)
        at com.sun.grizzly.DefaultProtocolChain.execute(DefaultProtocolChain.java:88)
        at com.sun.grizzly.http.HttpProtocolChain.execute(HttpProtocolChain.java:76)
        at com.sun.grizzly.ProtocolChainContextTask.doCall(ProtocolChainContextTask.java:53)
        at com.sun.grizzly.SelectionKeyContextTask.call(SelectionKeyContextTask.java:57)
        at com.sun.grizzly.ContextTask.run(ContextTask.java:69)
        at com.sun.grizzly.util.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:330)
        at com.sun.grizzly.util.AbstractThreadPool$Worker.run(AbstractThreadPool.java:309)
        at java.lang.Thread.run(Thread.java:662)
</pre>
",Preprocessing of the text & Tokenization,resolve missing manifest property opennlp trying use opennlp tokenization know problem following exception opennlp tool util invalidformatexception missing manifest property opennlp tool util model basemodel validateartifactmap basemodel java opennlp tool tokenize tokenizermodel validateartifactmap tokenizermodel java opennlp tool util model basemodel basemodel java opennlp tool tokenize tokenizermodel tokenizermodel java pk edu kics jsfcontroller getalltokens jsfcontroller java pk edu kics jsfcontroller jsfcontroller java sun reflect nativeconstructoraccessorimpl newinstance native method sun reflect nativeconstructoraccessorimpl newinstance nativeconstructoraccessorimpl java sun reflect delegatingconstructoraccessorimpl newinstance delegatingconstructoraccessorimpl java java lang reflect constructor newinstance constructor java java lang class newinstance class java java lang class newinstance class java com sun face mgbean beanbuilder newbeaninstance beanbuilder java com sun face mgbean beanbuilder build beanbuilder java com sun face mgbean beanmanager createandpush beanmanager java com sun face mgbean beanmanager create beanmanager java com sun face el managedbeanelresolver getvalue managedbeanelresolver java javax el compositeelresolver getvalue compositeelresolver java com sun face el facescompositeelresolver getvalue facescompositeelresolver java com sun el parser astidentifier getvalue astidentifier java com sun el parser astvalue getvalue astvalue java com sun el valueexpressionimpl getvalue valueexpressionimpl java com sun face facelets el tagvalueexpression getvalue tagvalueexpression java javax face component componentstatehelper eval componentstatehelper java javax face component componentstatehelper eval componentstatehelper java javax face component uioutput getvalue uioutput java com sun face renderkit html basic htmlbasicinputrenderer getvalue htmlbasicinputrenderer java com sun face renderkit html basic htmlbasicrenderer getcurrentvalue htmlbasicrenderer java com sun face renderkit html basic htmlbasicrenderer encodeend htmlbasicrenderer java javax face component uicomponentbase encodeend uicomponentbase java javax face component uicomponent encodeall uicomponent java javax face render renderer encodechildren renderer java javax face component uicomponentbase encodechildren uicomponentbase java javax face component uicomponent encodeall uicomponent java javax face component uicomponent encodeall uicomponent java javax face component uicomponent encodeall uicomponent java com sun face application view faceletviewhandlingstrategy renderview faceletviewhandlingstrategy java com sun face application view multiviewhandler renderview multiviewhandler java com sun face lifecycle renderresponsephase execute renderresponsephase java com sun face lifecycle phase dophase phase java com sun face lifecycle lifecycleimpl render lifecycleimpl java javax face webapp facesservlet service facesservlet java org apache catalina core standardwrapper service standardwrapper java org apache catalina core standardwrappervalve invoke standardwrappervalve java org apache catalina core standardcontextvalve invoke standardcontextvalve java org apache catalina core standardpipeline invoke standardpipeline java com sun enterprise web webpipeline invoke webpipeline java com sun enterprise web pesessionlockingstandardpipeline invoke pesessionlockingstandardpipeline java org apache catalina core standardhostvalve invoke standardhostvalve java org apache catalina connector coyoteadapter doservice coyoteadapter java org apache catalina connector coyoteadapter service coyoteadapter java com sun enterprise v service impl containermapper service containermapper java com sun grizzly com sun grizzly com sun grizzly com sun grizzly com sun grizzly defaultprotocolchain executeprotocolfilter defaultprotocolchain java com sun grizzly defaultprotocolchain execute defaultprotocolchain java com sun grizzly defaultprotocolchain execute defaultprotocolchain java com sun grizzly com sun grizzly protocolchaincontexttask docall protocolchaincontexttask java com sun grizzly selectionkeycontexttask call selectionkeycontexttask java com sun grizzly contexttask run contexttask java com sun grizzly util abstractthreadpool worker dowork abstractthreadpool java com sun grizzly util abstractthreadpool worker run abstractthreadpool java java lang thread run thread java
How can I extract datetime from freeform text?,"<p>I'm trying to come up with something along the lines of Google Calendar (or even some gmail messages), where freeform text will be parsed and converted to specific dates/times.</p>

<p>Some examples (assume for simplicity that right now is January 01, 2013 at 1am):</p>

<pre><code>""I should call Mom tomorrow to wish her a happy birthday"" -&gt; ""tomorrow"" = ""2013-01-02""
""The super bowl is on Feb 3rd at 6:30pm"" -&gt; ""Feb 3rd at 6:30"" =&gt; ""2013-02-03T06:30:00Z""
""Remind me to take out the trash on Friday"" =&gt; ""Friday"" =&gt; ""2013-01-04""
</code></pre>

<p>First of all I'll ask this - are there any already existing open source libraries that this (or part of this). If not, what sort of approaches do you think I should take?</p>

<p>I am thinking of a few different possibilities:</p>

<ol>
<li>Lots of regular expressions, as many as I can come up with for each different use case</li>
<li>Some sort of Bayesian Net that looks at n-grams and categorizes them into different scenarios like ""relative date"", ""relative day of week"", ""specific date"", ""date and time"", and then runs it through a rules engine (maybe more regex) to figure out the actual date.</li>
<li>Sending it to a Google search and try to extract meaningful information from the search results (this one is probably not realistic)</li>
</ol>
",Preprocessing of the text & Tokenization,extract datetime freeform text trying come something along line google calendar even gmail message freeform text parsed converted specific date time example assume simplicity right january first ask already existing open source library part sort approach think take thinking different possibility lot regular expression many come different use case sort bayesian net look n gram categorizes different scenario like relative date relative day week specific date date time run rule engine maybe regex figure actual date sending google search try extract meaningful information search result one probably realistic
PorterStemmer Stemmer Not Stemming Words Unless At End of String (in Python),"<p>I am creating a bot that talks about chess, and right now I am preprocessing. The problem is, when the Python program stems the words in the string, it will not do anything unless the word is the final word. That is the output. What I expected is for the stemmer to stem every word in the sentence that the user types in. What am I doing wrong? Here is the code:</p>

<pre><code>import re
from nltk.stem import PorterStemmer
port = PorterStemmer()

print(""Hello! I am the ChessBot. I will talk about chess, but can only talk     about chess. Nothing else. I can't even understand a greeting! Now, initiate this conversation!"")

while True:
    userinput = input('&gt; ')
    userinput = re.sub('(\?|\.|!|,)', '', userinput)
    userinput = str.lower(userinput)
    userinput = userinput.split('\s')
    userinput = "" "".join([port.stem(word) for word in userinput])
    print(userinput)
</code></pre>
",Preprocessing of the text & Tokenization,porterstemmer stemmer stemming word unless end string python creating bot talk chess right preprocessing problem python program stem word string anything unless word final word output expected stemmer stem every word sentence user type wrong code
Regular expression for two following proper nouns,"<p>I am trying to build a script that should give you a list of the proper names of a text. I want to build a regular expression that matchs every word that starts with a capital letter and that doesn't start a stence. An example from the text:</p>

<blockquote>
  <p>""But while he thought about these things, behold, an angel of the Lord
  appeared to him in a dream, saying: Joseph, son of David, do not be
  afraid to take to you Mary your wife, for that which is conceived in
  her is of the Holy Spirit. She is going to have a son. You must give him the name Jesus. That’s because he will save his people from their sins.</p>
</blockquote>

<p>I want that the regexp matches: ""Lord"", ""Joseph"", ""David"", ""Mary"", ""Holy"" and ""Spirit"". It should not match ""But"", ""She"", ""You"" or ""That"". So I did this:</p>

<pre><code>[a-z,;:] +([A-Z][a-z]+)[\W]
</code></pre>

<p>This matchs Lord, Joseph, David, Mary and Holy, BUT not Spirit. And that is my problem: after finding ""Holy"", the regexp keeps going searching in the text the pattern; to find ""Spirit"", the regexp should take the ""y"" of ""Holy"" in consideration for the first square bracket. So, after finding ""Holy"" should restart not after ""Holy"". I have tried it also with non matching groups, like:</p>

<pre><code>(?:[a-z,;:]) +([A-Z][a-z]+)[\W]
</code></pre>

<p>But that is also not the answer. I would be very thankful if anyone would help.
Regards!</p>
",Preprocessing of the text & Tokenization,regular expression two following proper noun trying build script give list proper name text want build regular expression match every word start capital letter start stence example text thought thing behold angel lord appeared dream saying joseph son david afraid take mary wife conceived holy spirit going son must give name jesus save people sin want regexp match lord joseph david mary holy spirit match match lord joseph david mary holy spirit problem finding holy regexp keep going searching text pattern find spirit regexp take holy consideration first square bracket finding holy restart holy tried also non matching group like also answer would thankful anyone would help regard
Extracting proper nouns from stemmed text in java,"<p>I am using OpenNLP for extracting proper nouns from a sentence. Here is my code:</p>

<pre><code>import java.io.FileInputStream;
import java.io.InputStream;
import java.util.HashSet;
import java.util.Set;

import opennlp.tools.cmdline.parser.ParserTool;
import opennlp.tools.parser.Parse;
import opennlp.tools.parser.Parser;
import opennlp.tools.parser.ParserFactory;
import opennlp.tools.parser.ParserModel;

public class ParserTest {

    static Set&lt;String&gt; nounPhrases = new HashSet&lt;&gt;();

    private static String line = ""iran india pai oil due euro delhi iran ask indian refin essar oil mangalor refineri petrochem mrpl clear oil due amount billion euro month lift sanction iran told indian author three year mechan pai cent oil import bill rupe keep remain cent pend payment channel clear end."";

    public void getNounPhrases(Parse p) {
        if (p.getType().equals(""NNP"") || p.getType().equals(""NNPS"")) {
             nounPhrases.add(p.getCoveredText());
             System.out.println(p.getCoveredText());
        }

        for (Parse child : p.getChildren()) {
             getNounPhrases(child);
        }
    }


    public void parserAction() throws Exception {
        InputStream is = new FileInputStream(""C:\\Users\\asus\\Downloads\\en-parser-chunking.bin"");
        ParserModel model = new ParserModel(is);
        Parser parser = ParserFactory.create(model);
        Parse topParses[] = ParserTool.parseLine(line, parser, 1);
        for (Parse p : topParses){
            //p.show();
            getNounPhrases(p);
        }
    }
    public static void main(String[] args) throws Exception {
        new ParserTest().parserAction();
        System.out.println(""List of Noun Parse : ""+nounPhrases);
    }
}
</code></pre>

<p>The problem is its a stemmed text (I used Porter Stemming Algorithm) so every word is in lower case. Because of that the proper nouns are not getting extracted. Is my above approach of extracting proper nouns is correct? If yes, then what changes I have to make in the code to make it work? If no, then suggest me a new approach along with a sample code would help me do it.</p>

<p>Thank you. </p>
",Preprocessing of the text & Tokenization,extracting proper noun stemmed text java using opennlp extracting proper noun sentence code problem stemmed text used porter stemming algorithm every word lower case proper noun getting extracted approach extracting proper noun correct yes change make code make work suggest new approach along sample code would help thank
How to skip phrases when tokenizing sentences in OpenNLP?,"<p>I am using OpenNLP JAVA API for Sentence Tokenization and it is using space character to tokenize the sentence and is splitting every word. </p>

<p>Is there any way to i can skip the word splitting or tokenization for some specific words.</p>

<p>For Example in a sentence. ""A quick brown fox jumping over the lazy dog"". OpenNLP split/tokenize the sentence as</p>

<p>a<br>
quick<br>
brown<br>
fox<br>
jumping<br>
over<br>
the 
lazy<br>
dog</p>

<p>i want to skip tokenization for the word ""quick brown fox"" and ""lazy dog"" , so the expected output will be</p>

<p>a<br>
quick brown fox<br>
jumping<br>
over<br>
the<br>
lazy dog  </p>
",Preprocessing of the text & Tokenization,skip phrase tokenizing sentence opennlp using opennlp java api sentence tokenization using space character tokenize sentence splitting every word way skip word splitting tokenization specific word example sentence quick brown fox jumping lazy dog opennlp split tokenize sentence quick brown fox jumping lazy dog want skip tokenization word quick brown fox lazy dog expected output quick brown fox jumping lazy dog
Need help in Web scraping webpages and its links by automatic funciton in R,"<p>I am interested to extract the data of paranormal activity reported in news, so that i can analyze the 
data of space and time of appearance for any correlations. This project is just for fun, to learn and use web scraping, text extraction and spatial and time correlation analysis. So please forgive me for deciding on this topic, I wanted to do something interesting and challenging work.
First I found this website has some collection of the reported paranormal incidences, they have collection for 2009,2010,2011 and 2012.
The structure of the website goes like this in every year they have 1..10 pages...and links goes like this
for year2009
link <a href=""http://paranormal.about.com/od/paranormalgeneralinfo/tp/2009-paranormal-activity.htm"" rel=""nofollow"">http://paranormal.about.com/od/paranormalgeneralinfo/tp/2009-paranormal-activity.htm</a></p>

<p>In each page they have collected the stories under the heading like this
Internal structure
Paranormal Activity, Posted 03-14-09
each of these head lines has two pages inside it..goes like this
link <a href=""http://paranormal.about.com/od/paranormalgeneralinfo/a/news_090314n.htm"" rel=""nofollow"">http://paranormal.about.com/od/paranormalgeneralinfo/a/news_090314n.htm</a></p>

<p>On each of these pages they have actual reported stories collected on various headlines..and the actual websites link for those stories. I am interested in collecting those reported text and extract information regarding the kind of paranormal activity like ghost, demon or UFOs and  the time, date and place of incidents. I wish to analyze this data for any spatial and time correlations. If UFO or Ghosts are real they must have some behavior and correlations in space or time in their movements. This is long shot of the story...</p>

<p><em><strong>I need help in web scraping the text form the above said pages. Here i have wrote down the code to follow one page and its link down to last final text i want. Can anyone let me know is there any better and efficient way to get the clean text from the final page. Also automation of the collecting text by following all 10 pages for whole 2009.</em></strong></p>

<pre><code>library(XML)
#source of paranormal news from about.com
#first page to start
#2009 -  http://paranormal.about.com/od/paranormalgeneralinfo/tp/2009-paranormal-activity.htm
pn.url&lt;-""http://paranormal.about.com/od/paranormalgeneralinfo/tp/2009-paranormal-activity.htm""
pn.html&lt;-htmlTreeParse(pn.url,useInternalNodes=T)
pn.h3=xpathSApply(pn.html,""//h3"",xmlValue)
#extracting the links of the headlines to follow to the story
pn.h3.links=xpathSApply(pn.html,""//h3/a/@href"")
#Extracted the links of the Internal structure to follow ...
#Paranormal Activity, Posted 01-03-09 (following this head line)
#http://paranormal.about.com/od/paranormalgeneralinfo/a/news_090314n.htm
pn.l1.url&lt;-pn.h3.links[1]
pn.l1.html&lt;-htmlTreeParse(pn.l1.url,useInternalNodes=T)
pn.l1.links=xpathSApply(pn.l1.html,""//p/a/@href"")
#Extracted the links of the Internal structure to follow ...
#British couple has 'black-and-white-twins' twice (following this head line)
#http://www.msnbc.msn.com/id/28471626/
pn.l1.f1.url=pn.l1.links[7]
pn.l1.f1.html=htmlTreeParse(pn.l1.f1.url,useInternalNodes=T)
pn.l1.f1.text=xpathSApply(pn.l1.f1.html,""//text()[not(ancestor::script)][not(ancestor::style)]"",xmlValue)
</code></pre>

<p>I sincerely thanks in advance for reading my post and your time for helping me.
I will be great full for any expert who would like to mentor me in this whole project.</p>

<p>Regards
Sathish</p>
",Preprocessing of the text & Tokenization,need help web scraping webpage link automatic funciton r interested extract data paranormal activity reported news analyze data space time appearance correlation project fun learn use web scraping text extraction spatial time correlation analysis please forgive deciding topic wanted something interesting challenging work first found website ha collection reported paranormal incidence collection structure website go like every year page link go like year link page collected story heading like internal structure paranormal activity posted head line ha two page inside go like link page actual reported story collected various headline actual website link story interested collecting reported text extract information regarding kind paranormal activity like ghost demon ufo time date place incident wish analyze data spatial time correlation ufo ghost real must behavior correlation space time movement long shot story need help web scraping text form said page wrote code follow one page link last final text want anyone let know better efficient way get clean text final page also automation collecting text following page whole sincerely thanks advance reading post time helping great full expert would like whole project regard sathish
Tokenize sentences based on dictionary,"<p>I have few key words say - RAM, Speaker, Brand, Display etc. I have made a dictionary of all possible values these keys can have. Like - RAM can have 12, 12gb, 12GB; Display can have 12inch, 12"", 12 inch.</p>

<p>I am given a title say - ""Apple iPhone 5s (Space Grey, 16GB)""</p>

<p>I have to extract tokens from these titles and map to the dictionary values and assign appropriate keyword to each token.</p>

<p>Like for this - Brand - Apple, Color - Grey, Storage - 16GB</p>

<p>How should i tokenize these sentences? Doing it just by space will not be enough- like in this title - ""Samsung Guru Music 2 SM-B310E (White)""
2 will be one token and can map to RAM,Display anything.</p>

<p>Will any NLP library help with this? I am using python to code and new to NLP.</p>

<pre><code>    final_dict = {'Width': [81.28, 49.0], 'Brand': ['wd', 'lenovo', 'western digital'], 'Weight': [960.0], 'Height': [111.76, 109.22, 170.6]}
    map = {'Ram_gb' : ['gb','GB'],
           'Storage_gb' : ['gb','GB'],
           'FrontCamera' : ['MP','mp'],
           'BackCamera' : ['MP','mp'],
           'Display Resolution' : ['p','pixels'],
           'Weight' : ['gram','g','kg','Gram','KG'],
           'Generation' : ['gen','G','nd']
           }
    add_words = ["""","" "",""-""]
    for key,value in self.final_dict.iteritems():
        if(key in map):
            adverb = map.get(key)
            new_dict = []
            for val in value:
                for ad in adverb:
                    for btw in add_words:
                        new_value = """"
                        new_value = str(val) + str(btw) + str(ad)
                        new_dict.append(new_value)
            self.final_dict[key].extend(new_dict)
            print self.final_dict[key]
</code></pre>

<p>This is the code for making all permutations of words possible in a title.</p>

<p>I have few titles -</p>

<pre><code>apple macbook pro md101hn/a 13-inch laptop (core i5/4gb/500gb/intel hd graphics), silver;
lenovo g40-45 laptop(amd e1/ 2gb ram/ 500gb hdd/ win 8.1);
lenovo g40-80 notebook(4th gen- ci3/ 4gb ram/ 500gb hdd/ win 8.1) (black);
lenovo g50-80 notebook(5th gen- ci3/ 4gb ram/ 1tb hdd/ dos) (black);
dell inspiron 3542 notebook (4th gen- ci3/ 4gb ram/ 1tb hdd/ dos), silver;
</code></pre>

<p>How to extract data from these titles and map to the keys in dictionary.</p>

<p>Like for title one -</p>

<pre><code>{
'Brand' : 'apple',
'processor' : 'core',
'RAM' : '4gb',
'color' : 'silver',
'Display' : '13-inch',
'Storage' : '500gb'
}
</code></pre>
",Preprocessing of the text & Tokenization,tokenize sentence based dictionary key word say ram speaker brand display etc made dictionary possible value key like ram gb gb display inch inch given title say apple iphone space grey gb extract token title map dictionary value assign appropriate keyword token like brand apple color grey storage gb tokenize sentence space enough like title samsung guru music sm b e white one token map ram display anything nlp library help using python code new nlp code making permutation word possible title title extract data title map key dictionary like title one
Is stemming used when gensim creates a dictionary for tf-idf model?,"<p>I am using Gensim python toolkit to build tf-idf model for documents. So I need to create a dictionary for all documents first. However, I found Gensim does not use stemming before creating the dictionary and corpus. Am I right ? </p>
",Preprocessing of the text & Tokenization,stemming used gensim creates dictionary tf idf model using gensim python toolkit build tf idf model document need create dictionary document first however found gensim doe use stemming creating dictionary corpus right
How do I discover list of words from corpus which distinguish from another corpus? Python,"<p>I have two lists of unstructured text input, and I want to find the words that distinguish <code>listA</code> from <code>listB</code>. 
For example, if <code>listA</code> were the text of ""Harry Potter"" and <code>listB</code> were the text of ""Ender's Game"", the distinguishes elements for <code>listA</code> would be <code>[wand, magic, wizard, . . .]</code> and the distinguishing elements for <code>listB</code> would be <code>[ender, buggers, battle, . . .]</code></p>

<p>I've tried a bit with the python-nltk module, and am able to easily find the most common words in each list, but that is not exactly what I'm after.</p>
",Preprocessing of the text & Tokenization,discover list word corpus distinguish another corpus python two list unstructured text input want find word distinguish example text harry potter text ender game distinguishes element would distinguishing element would tried bit python nltk module able easily find common word list exactly
Getting NLTK tree leaf values as a string,"<p>I'm trying to get leaf values in the Tree object as a string. The tree object here is the output of the Stanford Parser.</p>

<p>Here is my code:</p>

<pre><code>from nltk.parse import stanford
Parser = stanford.StanfordParser(""path"")


example = ""Selected variables by univariate/multivariate analysis, constructed logistic regression, calibrated the low defaults portfolio to benchmark ratings, performed back""
sentences = Parser.raw_parse(example)
for line in sentences:
    for sentence in line:
        tree = sentence
</code></pre>

<p>And this is how I extract the VP (Verb Phrases) leaves.</p>

<pre><code>VP=[]

VP_tree = list(tree.subtrees(filter=lambda x: x.label()=='VP'))

for i in VP_tree:
    VP.append(' '.join(i.flatten()))
</code></pre>

<p>Here is what i.flatten() looks like: (it returns parsed word list)</p>

<pre><code>(VP
  constructed
  logistic
  regression
  ,
  calibrated
  the
  low
  defaults
  portfolio
  to
  benchmark
  ratings)
</code></pre>

<p>Becuase I could only get them as a list of parsed words, I joined them with '  '. Therefore there is a space between 'regression' and ','. </p>

<pre><code>In [33]: VP
Out [33]: [u'constructed logistic regression , calibrated the low defaults portfolio to benchmark ratings']
</code></pre>

<p>I'd like to get the Verb Phrase as a string (not as a list of parsed words) without having to join them by ' '.</p>

<p>I have looked at methods under Tree class (<a href=""http://www.nltk.org/_modules/nltk/tree.html"" rel=""nofollow"">http://www.nltk.org/_modules/nltk/tree.html</a>) however got no luck so far.</p>
",Preprocessing of the text & Tokenization,getting nltk tree leaf value string trying get leaf value tree object string tree object output stanford parser code extract vp verb phrase leaf flatten look like return parsed word list becuase could get list parsed word joined therefore space regression like get verb phrase string list parsed word without join looked method tree class however got luck far
Python re.split() vs nltk word_tokenize and sent_tokenize,"<p>I was going through <a href=""https://stackoverflow.com/questions/7501609/python-re-split-vs-split/7501659#7501659"">this question</a>.</p>

<p>Am just wondering whether NLTK would be faster than regex in word/sentence tokenization.</p>
",Preprocessing of the text & Tokenization,python split v nltk word tokenize sent tokenize wa going href question wondering whether nltk would faster regex word sentence tokenization
Item description keyword extraction,"<p>I'm playing around with a recommendation system that takes key descriptive words and phrases and matches them against others. Specifically, I'm focusing on flavors in beer, with an algorithm searching for things like <code>malty</code> or <code>medium bitterness</code>, pulling those out, and then comparing against other beers to come up with flavor recommendations.</p>

<p>Currently, I'm struggling with the extraction. What are some techniques for identifying words and standardizing them for later processing? </p>

<p>How do I pull out <code>hoppy</code> and <code>hops</code> and treat them as the same word, but also keeping in mind that <code>very hoppy</code> and <code>not enough hops</code> have different meanings that are modified by the preceding word(s)? I believe I can use stemming for things like plurals and suffixed/prefixed words, but what about pairs or more complicated patterns? What techniques exist for this?</p>
",Preprocessing of the text & Tokenization,item description keyword extraction playing around recommendation system take key descriptive word phrase match others specifically focusing flavor beer algorithm searching thing like pulling comparing beer come flavor recommendation currently struggling extraction technique identifying word standardizing later processing pull treat word also keeping mind different meaning modified preceding word believe use stemming thing like plural suffixed prefixed word pair complicated pattern technique exist
Item description keyword extraction,"<p>I'm playing around with a recommendation system that takes key descriptive words and phrases and matches them against others. Specifically, I'm focusing on flavors in beer, with an algorithm searching for things like <code>malty</code> or <code>medium bitterness</code>, pulling those out, and then comparing against other beers to come up with flavor recommendations.</p>

<p>Currently, I'm struggling with the extraction. What are some techniques for identifying words and standardizing them for later processing? </p>

<p>How do I pull out <code>hoppy</code> and <code>hops</code> and treat them as the same word, but also keeping in mind that <code>very hoppy</code> and <code>not enough hops</code> have different meanings that are modified by the preceding word(s)? I believe I can use stemming for things like plurals and suffixed/prefixed words, but what about pairs or more complicated patterns? What techniques exist for this?</p>
",Preprocessing of the text & Tokenization,item description keyword extraction playing around recommendation system take key descriptive word phrase match others specifically focusing flavor beer algorithm searching thing like pulling comparing beer come flavor recommendation currently struggling extraction technique identifying word standardizing later processing pull treat word also keeping mind different meaning modified preceding word believe use stemming thing like plural suffixed prefixed word pair complicated pattern technique exist
Python: Regular Expression not working properly,"<p>I'm using following regex, it suppose to find the string <code>'U.S.A.'</code>, but it only gets <code>'A.'</code>, is anyone know what's wrong?</p>

<pre><code>#INPUT
import re

text = 'That U.S.A. poster-print costs $12.40...'

print re.findall(r'([A-Z]\.)+', text)

#OUTPUT
['A.']
</code></pre>

<p>Expected Output:</p>

<pre><code>['U.S.A.']
</code></pre>

<p>I'm following the NLTK Book, Chapter 3.7 <a href=""http://www.nltk.org/book/ch03.html#tab-re-symbols"" rel=""nofollow"">here</a>, it has a set of regex but it just not workin. I've tried it in both Python 2.7 and 3.4.</p>

<pre><code>&gt;&gt;&gt; text = 'That U.S.A. poster-print costs $12.40...'
&gt;&gt;&gt; pattern = r'''(?x)    # set flag to allow verbose regexps
...     ([A-Z]\.)+        # abbreviations, e.g. U.S.A.
...   | \w+(-\w+)*        # words with optional internal hyphens
...   | \$?\d+(\.\d+)?%?  # currency and percentages, e.g. $12.40, 82%
...   | \.\.\.            # ellipsis
...   | [][.,;""'?():-_`]  # these are separate tokens; includes ], [
... '''
&gt;&gt;&gt; nltk.regexp_tokenize(text, pattern)
['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']
</code></pre>

<p><strong>nltk.regexp_tokenize()</strong> works the same as <strong>re.findall()</strong>, I think somehow my python here does not recognize the regex as expected. The regex listed above output this:</p>

<pre><code>[('', '', ''),
 ('A.', '', ''),
 ('', '-print', ''),
 ('', '', ''),
 ('', '', '.40'),
 ('', '', '')]
</code></pre>
",Preprocessing of the text & Tokenization,python regular expression working properly using following regex suppose find string get anyone know wrong expected output following nltk book chapter ha set regex workin tried python nltk regexp tokenize work findall think somehow python doe recognize regex expected regex listed output
Input matches no features in training set; how much more training data do I need?,"<p>I am new to Text Mining. I am working on Spam filter. I did text cleaning, removed stop words. n-grams are my features. So I build a frequency matrix and build model using Naive Bayes.  I have very limited set of training data, so I am facing the following problem.</p>

<p>When a sentence comes to me for classification and if none of its features match with the existing features in training then my frequency vector has only zeros. </p>

<p>When I send this vector for classification, I obviously get a useless result.</p>

<p>What can be ideal size of training data to expect better results?</p>
",Preprocessing of the text & Tokenization,input match feature training set much training data need new text mining working spam filter text cleaning removed stop word n gram feature build frequency matrix build model using naive bayes limited set training data facing following problem sentence come classification none feature match existing feature training frequency vector ha zero send vector classification obviously get useless result ideal size training data expect better result
Stop words and stemmer in java,"<p>I'm thinking of putting a stop words in my similarity program and then a stemmer (going for porters 1 or 2 depends on what easiest to implement)</p>

<p>I was wondering that since I read my text from files as whole lines and save them as a long string, so if I got two strings ex.</p>

<pre><code>String one = ""I decided buy something from the shop."";
String two = ""Nevertheless I decidedly bought something from a shop."";
</code></pre>

<p>Now that I got those strings </p>

<p>Stemming:
Can I just use the stemmer algoritmen directly on it, save it as a String and then continue working on the similarity like I did before implementing the stemmer in the program, like running one.stem(); kind of thing? </p>

<p>Stop word:
How does this work out? O.o
Do I just use; one.replaceall(""I"", """"); or is there some specific way to use for this proces? I want to keep working with the string and get a string before using the similarity algorithms on it to get the similarity. Wiki doesn't say a lot. </p>

<p>Hope you can help me out! Thanks. </p>

<p>Edit: It is for a school-related project where I'm writing a paper on similarity between different algorithms so I don't think I'm allowed to use lucene or other libraries that does the work for me. Plus I would like to try and understand how it works before I start using the libraries like Lucene and co. Hope it's not too much a bother ^^</p>
",Preprocessing of the text & Tokenization,stop word stemmer java thinking putting stop word similarity program stemmer going porter depends easiest implement wa wondering since read text file whole line save long string got two string ex got string stemming use stemmer algoritmen directly save string continue working similarity like implementing stemmer program like running one stem kind thing stop word doe work use one replaceall specific way use proces want keep working string get string using similarity algorithm get similarity wiki say lot hope help thanks edit school related project writing paper similarity different algorithm think allowed use lucene library doe work plus would like try understand work start using library like lucene co hope much bother
How to modify text that matches a particular regular expression in Python?,"<p>I need to mark negative contexts in a sentence. The algorithm goes as follows:</p>

<ol>
<li>Detect a negator (not/never/ain't/don't/ etc)</li>
<li>Detect a clause ending punctuation (.;:!?)</li>
<li>Add _NEG to all the words in between this.</li>
</ol>

<p>Now, I have defined a regex to pick out all such occurences:</p>

<pre><code>def replacenegation(text):
    match=re.search(r""((\b(never|no|nothing|nowhere|noone|none|not|havent|hasnt|hadnt|cant|couldnt|shouldnt|wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint)\b)|\b\w+n't\b)((?![.:;!?]).)*[.:;!?\b]"", text)
    if match:
        s=match.group()
        print s
        news=""""
        wlist=re.split(r""[.:;!? ]"" , s)
        wlist=wlist[1:]
        print wlist
        for w in wlist:
            if w:
                news=news+"" ""+w+""_NEG""
        print news
</code></pre>

<p>I can detect and replace the matched group. However, I don't know how to recreate the complete sentence after this operation. Also for multiple matches, match.groups() gives me wrong output.</p>

<p>For example, if my input sentence is:</p>

<pre><code>I don't like you at all; I should not let you know my happiest secret.
</code></pre>

<p>Output should be:</p>

<pre><code>I don't like_NEG you_NEG at_NEG all_NEG ; I should not let_NEG you_NEG know_NEG my_NEG happiest_NEG secret_NEG .
</code></pre>

<p>How do I do this?</p>
",Preprocessing of the text & Tokenization,modify text match particular regular expression python need mark negative context sentence algorithm go follows detect negator never etc detect clause ending punctuation add neg word defined regex pick occurences detect replace matched group however know recreate complete sentence operation also multiple match match group give wrong output example input sentence output
How to keep only the highly significant variables in r and combine two entire corpus to make predictions?,"<p>I'm using R and I have a file of blog posts with 3 variables: <strong>title</strong> (the title of the post), <strong>body</strong> (the text of the post) and <strong>great</strong> (1 if the post received five stars, 0 otherwise). I already created a corpus for the title, remove punctuation, converted it to lowercase, etc. as follows:</p>

<pre><code>title = Corpus(VectorSource(posts$title))
title = tm_map(title, tolower)
title = tm_map(title, PlainTextDocument)
title = tm_map(title, removePunctuation)
title = tm_map(title, removeWords, stopwords(""english""))
title = tm_map(title, stemDocument)
dtm = DocumentTermMatrix(title)
sparseTerms = removeSparseTerms(dtm, 0.99)
title = as.data.frame(as.matrix(sparseTerms))
title$great = posts$great
</code></pre>

<p>I did the same process for the <strong>body</strong> variable. After that, I used <code>sample.split</code> to separate a training and a testing set (for the title) and used the <code>glm()</code> function to use a logistic regression:</p>

<pre><code>library(caTools)
spl = sample.split(title$great, 0.7)
train = subset(title, spl = TRUE)
test = subset(title, spl = FALSE) 
Log = glm(great ~ ., data=train, family=binomial)
summary(Log)
</code></pre>

<p>Now when I use <code>summary()</code> I can see which variables are significant (3 stars). And, as you can see, I only used the <strong>title</strong> variable at <code>glm()</code>. So I'd like to know:</p>

<ol>
<li>How to keep only the variables with 3 stars?</li>
<li>How to combine the title and the body corpus so that I can use the glm() function with all the data?</li>
</ol>

<p>Thanks in advance.</p>
",Preprocessing of the text & Tokenization,keep highly significant variable r combine two entire corpus make prediction using r file blog post variable title title post body text post great post received five star otherwise already created corpus title remove punctuation converted lowercase etc follows process body variable used separate training testing set title used function use logistic regression use see variable significant star see used title variable like know keep variable star combine title body corpus use glm function data thanks advance
Tagging words based on some pre-defined categories,"<p>Consider we have 16 different categories, e.g., Computer, Science, Art, Business etc. We have some words under each category as synonyms, homonyms etc which describes the possible meaning of each topic and its range. Consequently there might be similar or even same words which falls in more than one categories. Our aim is to submit a query (with max length of 3, after stop word removal) to a system and ask the system to put this word into the category with highest similarity. So my question is, beside Cosine similarity, is there any good technique for doing this?</p>

<p>I know already about <code>WordNet</code> and its extended version, <code>extjwnl</code>, however, I wish to implement one which gives enough flexibility to me for small usages.</p>
",Preprocessing of the text & Tokenization,tagging word based pre defined category consider different category e g computer science art business etc word category synonym homonym etc describes possible meaning topic range consequently might similar even word fall one category aim submit query max length stop word removal system ask system put word category highest similarity question beside cosine similarity good technique know already extended version however wish implement one give enough flexibility small usage
break compound based on dictionary,"<p>I have a grammar which says that <code>'A'</code> can be replaced with `'A','aa','aA','Aa','AA'. (Sanskrit Grammar to be precise).</p>

<p>I want to split a compound word into its possible components, e.g. <code>'samADAna'</code> -> <code>['sam+ADAna','sama+ADAna']</code>.</p>

<pre><code>lstrep = [('A',('A','aa','aA','Aa','AA'))]
</code></pre>

<p>My dictionary sample is</p>

<pre><code>['sam','sama','ADAna']
</code></pre>

<p>The actual dictionary is 450,000 words list.</p>

<p><a href=""https://stackoverflow.com/questions/34108900/optionally-replacing-a-substring-python"">Optionally replacing a substring python</a> has shown a way to create a list of all possible permutations after replacing the <code>'A'</code> at all places. </p>

<p>As can be seen, it would give a 25 member list. After this I use Generic Human's code at <a href=""https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words"">How to split text without spaces into list of words?</a> to infer the break in the compound based on my dictionary.</p>

<p>Practically the code has to run 25 times. It is not a big problem at this juncture.</p>

<p>But if my input string was <code>'samADAnApA'</code> - the permutations would be 625. Code would have to iter for 625 times. It is a heavy cost on memory and time.</p>

<p><strong>Question</strong> - Is there a way by which I can restrict the possible permutations to the words allowable by the dictionary. e.g. the dictionary doesn't have <code>'samA'</code>. </p>

<p>Therefore <code>samADAna, samAaDAna, samAADAna</code> etc would not be included in permutations?</p>

<p>My try:</p>

<pre><code>if __name__==""__main__"":
    perm = permut(sys.argv[1],lstrep,words) # function permut creates all possible permutations of replacements.
    output = []
    for mem in perm:
        split = infer_spaces(mem) # Code of Generic Human 
        if split is not False:
            output.append(split)
    output = sorted(output,key=len)
    print output
</code></pre>
",Preprocessing of the text & Tokenization,break compound based dictionary grammar say replaced aa aa aa aa sanskrit grammar precise want split compound word possible component e g dictionary sample actual dictionary word list infer break compound based dictionary practically code ha run time big problem juncture input string wa permutation would code would iter time heavy cost memory time question way restrict possible permutation word allowable dictionary e g dictionary therefore etc would included permutation try
How to tokenize all currency symbols using Regex in python?,"<p>I want to tokenize all the symbols of currency by using NLTK tokenize with regex.</p>

<p>For example this is my sentence:</p>

<pre><code>The price of it is $5.00.
The price of it is RM5.00.
The price of it is €5.00.
</code></pre>

<p>I used this pattern of regex:</p>

<pre><code>pattern = r'''(['()""""\w]+|\.+|\?+|\,+|\!+|\$?\d+(\.\d+)?%?)'''
tokenize_list = nltk.regexp_tokenize(sentence, pattern)
</code></pre>

<p>But as we can see it only considers $.</p>

<p>I tried to use <code>\p{Sc}</code> as explained in <a href=""https://stackoverflow.com/questions/25978771/what-is-regex-for-currency-symbol"">What is regex for currency symbol?</a> but it is still not working for me.</p>
",Preprocessing of the text & Tokenization,tokenize currency symbol using regex python want tokenize symbol currency using nltk tokenize regex example sentence used pattern regex see considers tried use explained href regex currency symbol still working
NLTK stemmer returns a list of NoneTypes,"<p>I am preprocessing text data,(twitter data to be exact), but whenever I apply the NLTK stemmer I get a list of NoneTypes. I can't figure out why this happens and I have no idea how to solve it.</p>

<p>This is how my text data looks through processing:</p>

<p><strong>Before processing:</strong></p>

<pre><code>In [10]:
undefined



import pandas as pd
import numpy as np
import glob
import os
import nltk
dir = ""C:\Users\Anonymous\Desktop\KAGA FOLDER\Hashtags""
train = np.array(pd.read_csv(os.path.join(dir,""train.csv"")))[:,1]
def clean_the_text(data):
    alist = []
    data = nltk.word_tokenize(data)
    for j in data:
        alist.append(j.rstrip('\n'))
    alist = "" "".join(alist)
    return alist

def stemmer(data):
    stemmer = nltk.stem.PorterStemmer()
    new_list = []
    new_list = [new_list.append(stemmer.stem(word)) for word in data]
    return new_list
def loop_data(data):
    for i in range(len(data)):
        data[i] = clean_the_text(data[i])
    return data
train


Out[10]:
array(['Jazz for a Rainy Afternoon:  {link}',
       'RT: @mention: I love rainy days.',
       'Good Morning Chicago! Time to kick the Windy City in the nuts and head back West!',
       ...,
       'OMG #WeatherForecast for tomm 80 degrees &amp; Sunny &amp;lt;=== #NeedThat #Philly #iMustSeeItToBelieveIt yo',
       ""@mention Oh no! We had cold weather early in the week, but now it's getting warmer! Hoping the rain holds out to Saturday!"",
       'North Cascades Hwy to reopen Wed.: quite late after a long, deep winter. Only had to clear snow 75 ft deep {link}'], dtype=object)
</code></pre>

<p><strong>After tokenizing and cleaning text:</strong></p>

<pre><code>train = loop_data(train)

In [12]:
undefined



train
Out[12]:
array(['Jazz for a Rainy Afternoon : { link }',
       'RT : @ mention : I love rainy days .',
       'Good Morning Chicago ! Time to kick the Windy City in the nuts and head back West !',
       ...,
       'OMG # WeatherForecast for tomm 80 degrees &amp; Sunny &amp; lt ; === # NeedThat # Philly # iMustSeeItToBelieveIt yo',
       ""@ mention Oh no ! We had cold weather early in the week , but now it 's getting warmer ! Hoping the rain holds out to Saturday !"",
       'North Cascades Hwy to reopen Wed. : quite late after a long , deep winter. Only had to clear snow 75 ft deep { link }'], dtype=object)
</code></pre>

<p><strong>And finally after stemming:</strong></p>

<pre><code>In [13]:
undefined



train = stemmer(train)
train
Out[13]:
[None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
</code></pre>
",Preprocessing of the text & Tokenization,nltk stemmer return list nonetypes preprocessing text data twitter data exact whenever apply nltk stemmer get list nonetypes figure happens idea solve text data look processing processing tokenizing cleaning text finally stemming
Clustering list of list with Strings,"<p>So I have a my data set currently looks like the following:</p>

<pre><code>['microsoft','bizspark'],
['microsoft'],
['microsoft', 'skype'],
['amazon', 's3'],
['amazon', 'zappos'],
['amazon'],
</code></pre>

<p>....
etc. </p>

<p>Now what I would love to do is cluster these in regards to one another, using the Levenstein distance to calculate word scores.</p>

<p>Now I would iterate through all of the lists and compare the distance to the following lists. </p>

<pre><code>microsoft -&gt; ['microsoft','bizspark'], ['microsoft'], ['microsoft', 'skype'],
amazon -&gt; ['amazon', 's3'], ['amazon', 'zappos'], ['amazon'], ....
</code></pre>

<p>The question is how to do this? Should I calculate each levenstein distance on a word by word basis i.e. for ['amazon', 'zappos'] and ['microsoft','bizspark'], I would firstly get pairs: (amazon, microsoft), (amazon, bizspark), (zappos, microsoft, (zappos, bizspark) and calculate the distance of each pair.</p>

<p>Or should I really just create strings from these and then calculate the distance?</p>

<p>What I should then end up with is an NXN matrix with the distance:</p>

<pre><code>                            ['microsoft','bizspark'] | ['amazon', 'zappos'] ....
    ['microsoft','bizspark']           1             |          ?
    _-------------------------------------------------------------------------
    ['amazon', 'zappos']               ?             |          1
            ...
            ....
</code></pre>

<p>Then how do I apply clustering to this to determine a cut-off threshold?</p>

<p>One such suggestion using single words is discussed  <a href=""https://stackoverflow.com/a/12287285"">here</a></p>

<p>But I'm not sure how to go about it with regards to word lists!?</p>

<p>Please note, in regards to implementation I am using Python libaries, such as Numpy, Scipy, Pandas and as needed.</p>
",Preprocessing of the text & Tokenization,clustering list list string data set currently look like following etc would love cluster regard one another using levenstein distance calculate word score would iterate list compare distance following list question calculate levenstein distance word word basis e amazon zappos microsoft bizspark would firstly get pair amazon microsoft amazon bizspark zappos microsoft zappos bizspark calculate distance pair really create string calculate distance end nxn matrix distance apply clustering determine cut threshold one suggestion using single word discussed href p sure go regard word list please note regard implementation using python libaries numpy scipy panda needed
Lemmatizer supporting german language (for commercial and research purpose),"<p>I am searching for a lemmatization software which:</p>

<ul>
<li>supports the german language</li>
<li>has a license that allows it to be used for commercial and research purpose. LGPL license would be good.</li>
<li>should preferably be implemented in Java. Implementations in other programming languages would also be OK.</li>
</ul>

<p>Does anybody know about such a lemmatizer?</p>

<p>Regards,</p>

<p><strong>UPDATE</strong>: Hi Daniel, At first, thank you for the great work you are providing with the LanguageTool. </p>

<p>We would like to index german Texts into elasticsearch (ES) and pre-analyze the texts using either
 an ES-built-in german stemmer (please see <a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stemmer-tokenfilter.html"" rel=""nofollow"">https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stemmer-tokenfilter.html</a>) 
or 
the following plugin <a href=""https://github.com/jprante/elasticsearch-analysis-baseform"" rel=""nofollow"">https://github.com/jprante/elasticsearch-analysis-baseform</a>. The latter uses your morphology file under <a href=""http://www.danielnaber.de/morphologie/morphy-mapping-20110717.latin1.gz"" rel=""nofollow"">http://www.danielnaber.de/morphologie/morphy-mapping-20110717.latin1.gz</a> and that is why I thought you maybe have some evaluation data in order to know what is the trade-off when using the lemmatization based on your morphology file instead of an ES-built-in stemmer. Do you maybe have some figures in terms of precision/ coverage of your german morphology? Or comparative data with the german stemmers used in Elasticsearch?</p>

<p>Best regards</p>
",Preprocessing of the text & Tokenization,lemmatizer supporting german language commercial research purpose searching lemmatization software support german language ha license allows used commercial research purpose lgpl license would good preferably implemented java implementation programming language would also ok doe anybody know lemmatizer regard update hi daniel first thank great work providing languagetool would like index german text elasticsearch e pre analyze text using either e built german stemmer please see following plugin latter us morphology file thought maybe evaluation data order know trade using lemmatization based morphology file instead e built stemmer maybe figure term precision coverage german morphology comparative data german stemmer used elasticsearch best regard
word_tokenize TypeError: expected string or buffer,"<p>When calling <code>word_tokenize</code> I get the following error:</p>

<pre><code>File ""C:\Python34\lib\site-packages\nltk\tokenize\punkt.py"", line 1322,
    in _slices_from_text for match in
    self._lang_vars.period_context_re().finditer(text):
TypeError: expected string or buffer
</code></pre>

<p>I have a large text file (1500.txt) from which I want to remove stop words.
My code is as follows:</p>

<pre><code>from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

with open('E:\\Book\\1500.txt', ""r"", encoding='ISO-8859-1') as File_1500:
    stop_words = set(stopwords.words(""english""))
    words = word_tokenize(File_1500)
    filtered_sentence = [w for w in words if not w in stop_words]
    print(filtered_sentence)
</code></pre>
",Preprocessing of the text & Tokenization,word tokenize typeerror expected string buffer calling get following error large text file txt want remove stop word code follows
Why NLTK lemmatization has wrong output even if verb.exc has added right value?,"<p>When I open verb.exc, I can see</p>

<pre><code>saw see
</code></pre>

<p>While I use lemmatization in code</p>

<pre><code>&gt;&gt;&gt;print lmtzr.lemmatize('saw', 'v')
saw
</code></pre>

<p>How can this happen? Do I misunderstand in revising wordNet?</p>
",Preprocessing of the text & Tokenization,nltk lemmatization ha wrong output even verb exc ha added right value open verb exc see use lemmatization code happen misunderstand revising wordnet
Is there a stop word list for twitter?,"<p>I want to do some mining on tweets. Is there any more specific stop word list for tweets such as removing ""lol"" and other twitter smiley?</p>
",Preprocessing of the text & Tokenization,stop word list twitter want mining tweet specific stop word list tweet removing lol twitter smiley
"Extracting email addresses, phone numbers using Stanford CoreNLP","<p>I have been looking for a solution to extract email addresses, phone numbers, ... from a text using Stanford CoreNLP (RegexNERAnnotator). Can anyone please provide any example?</p>

<p><strong>UPDATE : 04/11/2015:</strong>
Actually i should asked instead if there is a way Stanford RegexNERAnnotator can supports Java Regular expression.</p>

<p>Example Usage:</p>

<pre><code>       final String EMAIL_PATTERN = 
            ""^[_A-Za-z0-9-\\+]+(\\.[_A-Za-z0-9-]+)*@""
            + ""[A-Za-z0-9-]+(\\.[A-Za-z0-9]+)*(\\.[A-Za-z]{2,})$"";

       List&lt;CoreLabel&gt; tokens = ...;
       TokenSequencePattern pattern = TokenSequencePattern.compile(EMAIL_PATTERN);
       TokenSequenceMatcher matcher = pattern.getMatcher(tokens);

       while (matcher.find()) {
         String matchedString = matcher.group();
         List&lt;CoreMap&gt; matchedTokens = matcher.groupNodes();
         ...
       }
</code></pre>

<p><strong>It seems that it doesn't support Java Regular expression:</strong></p>

<pre><code>Exception in thread ""main"" edu.stanford.nlp.ling.tokensregex.parser.TokenMgrError: Lexical error at line 1, column 1.  Encountered: ""^"" (94), after : """"
    at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParserTokenManager.getNextToken(TokenSequenceParserTokenManager.java:1029)
    at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.jj_ntk(TokenSequenceParser.java:3228)
    at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegexBasic(TokenSequenceParser.java:784)
    at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegexDisjConj(TokenSequenceParser.java:973)
    at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegex(TokenSequenceParser.java:743)
    at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegexWithAction(TokenSequenceParser.java:1596)
    at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.parseSequenceWithAction(TokenSequenceParser.java:37)
    at edu.stanford.nlp.ling.tokensregex.TokenSequencePattern.compile(TokenSequencePattern.java:186)
    at edu.stanford.nlp.ling.tokensregex.TokenSequencePattern.compile(TokenSequencePattern.java:169)
</code></pre>
",Preprocessing of the text & Tokenization,extracting email address phone number using stanford corenlp looking solution extract email address phone number text using stanford corenlp regexnerannotator anyone please provide example update actually asked instead way stanford regexnerannotator support java regular expression example usage seems support java regular expression
Tool to normalize text source and build original source from normalized one,"<p>Does somebody know a tool/project on Java which can normalize text (and store normalization log) and then build original source text?</p>

<p>Any approach is appreciated.</p>

<p>The problem:
In order to process input data, we need to normalize it. </p>

<p>The process engine receives normalized text and returns matched positions.</p>

<p>After this step, we need to recover original source equivalent by normalized positions.</p>

<p>Example:</p>

<pre><code>Source:
Lorem ipsum ad his scripta blandit partiendo, eum fastidii accumsan euripidis in, eum liber hendrerit an ... ütf Wórd èxämplé

Normalized text (approx):
lorem ipsum scripta blandit partiendo, fastidi accumsan euripidis, liber hendrerit utf word example

Engine output:
lorem ipsum scripta begin 0 end 19
euripidis           begin 56 end 65

Original source equivalent:
Lorem ipsum ad his scripta begin 0 end 26
euripidis                  begin 69 end 78
</code></pre>

<p>Thanks for help</p>
",Preprocessing of the text & Tokenization,tool normalize text source build original source normalized one doe somebody know tool project java normalize text store normalization log build original source text approach appreciated problem order process input data need normalize process engine receives normalized text return matched position step need recover original source equivalent normalized position example thanks help
How to remove stopwords efficiently from a list of ngram tokens in R,"<p>Here's an appeal for a better way to do something that I can already do inefficiently: <strong>filter a series of n-gram tokens using ""stop words""</strong> so that the occurrence of any stop word term in an n-gram triggers removal.</p>

<p>I'd very much like to have one solution that works for both unigrams and n-grams, although it would be ok to have two versions, one with a ""fixed"" flag and one with a ""regex"" flag.  I'm putting the two aspects of the question together since someone may have a solution that tries a different approach that addresses both fixed and regular expression stopword patterns.</p>

<p>Formats:  </p>

<ul>
<li><p><strong>tokens</strong> are a list of character vectors, which may be unigrams, or n-grams concatenated by a <code>_</code> (underscore) character.  </p></li>
<li><p><strong>stopwords</strong> are a character vector.  Right now I am content to let this be a fixed string, but it would be a nice bonus to be able to implement this using regular expression formatted stopwords too.</p></li>
</ul>

<p><strong>Desired Output:</strong> A list of characters matching the input <strong>tokens</strong> but with any component token matching a stop word being removed.  (This means a unigram match, or a match to one of the terms which the n-gram comprises.)</p>

<p><strong>Examples, test data, and working code and benchmarks to build on:</strong></p>

<pre><code>tokens1 &lt;- list(text1 = c(""this"", ""is"", ""a"", ""test"", ""text"", ""with"", ""a"", ""few"", ""words""), 
                text2 = c(""some"", ""more"", ""words"", ""in"", ""this"", ""test"", ""text""))
tokens2 &lt;- list(text1 = c(""this_is"", ""is_a"", ""a_test"", ""test_text"", ""text_with"", ""with_a"", ""a_few"", ""few_words""), 
                text2 = c(""some_more"", ""more_words"", ""words_in"", ""in_this"", ""this_text"", ""text_text""))
tokens3 &lt;- list(text1 = c(""this_is_a"", ""is_a_test"", ""a_test_text"", ""test_text_with"", ""text_with_a"", ""with_a_few"", ""a_few_words""),
                text2 = c(""some_more_words"", ""more_words_in"", ""words_in_this"", ""in_this_text"", ""this_text_text""))
stopwords &lt;- c(""is"", ""a"", ""in"", ""this"")

# remove any single token that matches a stopword
removeTokensOP1 &lt;- function(w, stopwords) {
    lapply(w, function(x) x[-which(x %in% stopwords)])
}

# remove any word pair where a single word contains a stopword
removeTokensOP2 &lt;- function(w, stopwords) {
    matchPattern &lt;- paste0(""(^|_)"", paste(stopwords, collapse = ""(_|$)|(^|_)""), ""(_|$)"")
    lapply(w, function(x) x[-grep(matchPattern, x)])
}

removeTokensOP1(tokens1, stopwords)
## $text1
## [1] ""test""  ""text""  ""with""  ""few""   ""words""
## 
## $text2
## [1] ""some""  ""more""  ""words"" ""test""  ""text"" 

removeTokensOP2(tokens1, stopwords)
## $text1
## [1] ""test""  ""text""  ""with""  ""few""   ""words""
## 
## $text2
## [1] ""some""  ""more""  ""words"" ""test""  ""text"" 

removeTokensOP2(tokens2, stopwords)
## $text1
## [1] ""test_text"" ""text_with"" ""few_words""
## 
## $text2
## [1] ""some_more""  ""more_words"" ""text_text"" 

removeTokensOP2(tokens3, stopwords)
## $text1
## [1] ""test_text_with""
## 
## $text2
## [1] ""some_more_words""

# performance benchmarks for answers to build on
require(microbenchmark)
microbenchmark(OP1_1 = removeTokensOP1(tokens1, stopwords),
               OP2_1 = removeTokensOP2(tokens1, stopwords),
               OP2_2 = removeTokensOP2(tokens2, stopwords),
               OP2_3 = removeTokensOP2(tokens3, stopwords),
               unit = ""relative"")
## Unit: relative
## expr      min       lq     mean   median       uq      max neval
## OP1_1 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000   100
## OP2_1 5.119066 3.812845 3.438076 3.714492 3.547187 2.838351   100
## OP2_2 5.230429 3.903135 3.509935 3.790143 3.631305 2.510629   100
## OP2_3 5.204924 3.884746 3.578178 3.753979 3.553729 8.240244   100
</code></pre>
",Preprocessing of the text & Tokenization,remove stopwords efficiently list ngram token r better way something already inefficiently filter series n gram token using stop word occurrence stop word term n gram trigger removal much like one solution work unigrams n gram although would ok two version one fixed flag one regex flag putting two aspect question together since someone may solution try different approach address fixed regular expression stopword pattern format token list character vector may unigrams n gram concatenated underscore character stopwords character vector right content let fixed string would nice bonus able implement using regular expression formatted stopwords desired output list character matching input token component token matching stop word removed mean unigram match match one term n gram comprises example test data working code benchmark build
How to remove the stop words in a meaningful way using Lucene,"<p>I'm using <code>org.apache.lucene.analysis.en.EnglishAnalyzer</code> to remove the stop words and also stem a phrase. However what happens is when I have a phrase like ""heart and heart disease"", that the analyzer does is it removes the ""and"" from the middle of the phrase which turns it into a meaningless phrase (the output is ""heart heart disease""). How can I cope this problem and remove stop words only if it is necessary?</p>
",Preprocessing of the text & Tokenization,remove stop word meaningful way using lucene using remove stop word also stem phrase however happens phrase like heart heart disease analyzer doe remove middle phrase turn meaningless phrase output heart heart disease cope problem remove stop word necessary
n-gram counting in MySQL,"<p>I am building a MySQL database that will have roughly 10,000 records.  Each record will contain a textual document (a few pages of text in most cases).  I want to do all sorts of n-gram counting across the entire database.  I have algorithms already written in Python that will what I want against a directory containing a large number of text files, but  to do that I will need to extract 10,000 text files from the database - this will have performance issues.</p>

<p>I'm a rookie with MySQL, so I'm not sure if it has any built-in features that do n-gram analysis, or whether there are good plugins out there that would do it.  Please note that I need to go up to at least 4-grams (preferably 5-grams) in my analysis, so the simple 2-gram plugins I've seen won't work here.  I also need to have the ability to remove the stopwords from the textual documents before doing the n-gram counting.</p>

<p>Any ideas from the community?</p>

<p>Thanks,</p>

<p>Ron</p>
",Preprocessing of the text & Tokenization,n gram counting mysql building mysql database roughly record record contain textual document page text case want sort n gram counting across entire database algorithm already written python want directory containing large number text file need extract text file database performance issue rookie mysql sure ha built feature n gram analysis whether good plugins would please note need go least gram preferably gram analysis simple gram plugins seen work also need ability remove stopwords textual document n gram counting idea community thanks ron
Is it possible to tokenize all except pre-defined words?,"<p>I want to tokenise a sentence but keep the pre-defined words intact. e.g.</p>

<pre><code>""i went to university of abc and had a wonderful time there!""
</code></pre>

<p>into</p>

<pre><code>[""i"", ""went"", ""to"", ""university of abc"", ""and"", ""had"", ""a"", ""wonderful"", ""time"", ""there"", ""!""]
</code></pre>

<p>As <code>""university of abc""</code> being the pre-defined words.</p>

<p>I couldn't find such parameter or control in any of the NLTK tokenisers. Any way I can hack to achieve this? Thanks!</p>
",Preprocessing of the text & Tokenization,possible tokenize except pre defined word want tokenise sentence keep pre defined word intact e g pre defined word find parameter control nltk tokenisers way hack achieve thanks
How to convert a Noun Phrase into a keyword,"<p>I'm working on a keyword extraction task in which the candidate keywords are the noun phrases that I obtain after applying part of speech tagging (pos) and chunking. However I polish the noun phrase a little bit more. That is, I remove the stop words from their beginning (or their end). I also need to apply stemming. However I'm not sure how to apply stemming on multiple-word phrases. does it make sense to apply stemming on multiple-word phrases at all or I should leave them as is? apart from stop word removal and possible stemming, is there anything else that I can do?</p>
",Preprocessing of the text & Tokenization,convert noun phrase keyword working keyword extraction task candidate keywords noun phrase obtain applying part speech tagging po chunking however polish noun phrase little bit remove stop word beginning end also need apply stemming however sure apply stemming multiple word phrase doe make sense apply stemming multiple word phrase leave apart stop word removal possible stemming anything else
Fast shell command to remove stop words in a text file,"<p>I have a 2GB text file. I am trying to remove frequently occurring english stop words from this file.</p>

<p>I have stopwords.txt containing like this..</p>

<pre><code>a
an
the
for
and
I
</code></pre>

<p>What is the fast method to do this using shell command such as tr, sed or awk? </p>
",Preprocessing of the text & Tokenization,fast shell command remove stop word text file gb text file trying remove frequently occurring english stop word file stopwords txt containing like fast method using shell command tr sed awk
Simple stemming and lemmatization in python,"<pre><code>from nltk.stem.snowball import SnowballStemmer
def check():
    stemmer = SnowballStemmer(""english"")
    lemmatizer = nltk.WordNetLemmatizer()
    temp_sent = ""Several women told me I have lying eyes.""

    print [stemmer.stem(t) for t in nltk.word_tokenize(temp_sent)]
    print [lemmatizer.lemmatize(t) for t in nltk.word_tokenize(temp_sent)]
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>[u'sever', u'women', u'told', 'me', 'i', u'have', u'lie', u'eye', '.']
['Several', u'woman', 'told', 'me', 'I', 'have', 'lying', u'eye', '.']
</code></pre>

<p>Can someone suggest a better alternative?  Stemmer fails with <code>several</code> and lemmatizer with <code>lying</code>. I would prefer lemmatizer I have a lot of time to run ,i.e, <code>accuracy&gt; efficiency</code>.</p>
",Preprocessing of the text & Tokenization,simple stemming lemmatization python output someone suggest better alternative stemmer fails lemmatizer would prefer lemmatizer lot time run e
NLTK importError when using Porter Stemmer,"<p>I am importing nltk, but it gives the following error. </p>

<pre><code>Traceback (most recent call last):
  File ""/home/control/Work/Intelligence/Datasets/whats cooking/new.py"", line 4, in &lt;module&gt;
    import nltk
  File ""/usr/local/lib/python2.7/dist-packages/nltk-3.0.5-py2.7.egg/nltk/__init__.py"", line 137, in &lt;module&gt;
    from nltk.stem import *
  File ""/usr/local/lib/python2.7/dist-packages/nltk-3.0.5-py2.7.egg/nltk/stem/__init__.py"", line 29, in &lt;module&gt;
    from nltk.stem.snowball import SnowballStemmer
  File ""/usr/local/lib/python2.7/dist-packages/nltk-3.0.5-py2.7.egg/nltk/stem/snowball.py"", line 25, in &lt;module&gt;
    from nltk.stem import porter
ImportError: cannot import name porter
</code></pre>

<p>My nltk was working perfectly a few days ago and i haven't updated or changed anything and I've also installed all the nltk data.</p>
",Preprocessing of the text & Tokenization,nltk importerror using porter stemmer importing nltk give following error nltk wa working perfectly day ago updated changed anything also installed nltk data
Why does `import word_tokenize` from NLTK works in the interpreter but not in my script?,"<p>I am trying to tokenize a sentence using nltk. when i do it through python shell i get the correct answer.</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; sentence = ""Mohanlal made his acting debut in Thiranottam (1978), but the film got released only after 25 years due to censorship issues.""
&gt;&gt;&gt; tokens = nltk.word_tokenize(sentence)
&gt;&gt;&gt; tokens
['Mohanlal', 'made', 'his', 'acting', 'debut', 'in', 'Thiranottam', '(', '1978', ')', ',', 'but', 'the', 'film', 'got', 'released', 'only', 'after', '25', 'years', 'due', 'to', 'censorship', 'issues', '.']
</code></pre>

<p>But when i write the same code in a file and try to run it i got the following error.</p>

<pre><code>    Traceback (most recent call last):
  File ""tokenize.py"", line 1, in &lt;module&gt;
    import nltk
  File ""/usr/local/lib/python2.7/dist-packages/nltk/__init__.py"", line 114, in &lt;module&gt;
    from nltk.collocations import *
  File ""/usr/local/lib/python2.7/dist-packages/nltk/collocations.py"", line 38, in &lt;module&gt;
    from nltk.util import ngrams
  File ""/usr/local/lib/python2.7/dist-packages/nltk/util.py"", line 13, in &lt;module&gt;
    import pydoc
  File ""/usr/lib/python2.7/pydoc.py"", line 55, in &lt;module&gt;
    import sys, imp, os, re, types, inspect, __builtin__, pkgutil, warnings
  File ""/usr/lib/python2.7/inspect.py"", line 39, in &lt;module&gt;
    import tokenize
  File ""/home/gadheyan/Project/Codes/tokenize.py"", line 2, in &lt;module&gt;
    from nltk import word_tokenize
ImportError: cannot import name word_tokenize
</code></pre>

<p>Here's the code that i run.</p>

<pre><code>import nltk
from nltk import word_tokenize

sentence = ""Mohanlal made his acting debut in Thiranottam (1978), but the film got released only after 25 years due to censorship issues.""
tokens = nltk.word_tokenize(sentence)
print tokens
</code></pre>
",Preprocessing of the text & Tokenization,doe nltk work interpreter script trying tokenize sentence using nltk python shell get correct answer write code file try run got following error code run
Why is sorting an array in PHP so slow?,"<p>I'm implementing a class that tries to detect stop words on a corpus. One of the methods in said class is, of course, <code>idf()</code>:</p>

<pre><code>public function idf(){

    // 1) In how many articles does each term appear
    $tokensFreq = $this-&gt;computeTokensFreq();

    // 2) Applies the IDF formula (array format : $term =&gt; $idf)
    $tokensIDF = $this-&gt;computeIDF($tokensFreq);

    // 3) Sort array so that 'stopwords' (smaller idfs) appear at the top
    arsort($tokensIDF);

    return $tokensIDF;

}
</code></pre>

<p>The problem is, this function is veeery slow: it takes more than 5 minutes to go through a relatively small array of 4 006 tokens. </p>

<p>At first, I thought I misimplemented the algorithm (I even replaced all <code>array_key_exists</code> with <code>isset</code>). But after further inspection, it turned out that it's the <code>arsort</code> call who takes the lion part of the execution time.</p>

<p>To help reduce the load, I added the <code>SORT_NUMERIC</code> flag to the <code>arsort</code> function, but in vain. </p>

<p>First of all, is it normal that sorting such an array takes up all that time?
Secondly, and most importantly, what can I do to make things go smoother?</p>
",Preprocessing of the text & Tokenization,sorting array php slow implementing class try detect stop word corpus one method said class course problem function veeery slow take minute go relatively small array token first thought misimplemented algorithm even replaced inspection turned call take lion part execution time help reduce load added flag function vain first normal sorting array take time secondly importantly make thing go smoother
Extract Word from Synset using Wordnet in NLTK 3.0,"<p>Some time ago, someone on SO asked <a href=""https://stackoverflow.com/questions/24664250/how-do-i-print-out-just-the-word-itself-in-a-wordnet-synset-using-python-nltk"">how to retrieve a list of words for a given synset</a> using NLTK's wordnet wrapper. Here is one of the suggested responses:</p>

<pre><code>for synset in wn.synsets('dog'):
    print synset.lemmas[0].name
</code></pre>

<p>Running this code with NLTK 3.0 yields <code>TypeError: 'instancemethod' object is not subscriptable</code>. </p>

<p>I tried each of the previously-proposed solutions (each of the solutions described on the page linked above), but each throws an error. I therefore wanted to ask: Is it possible to print the words for a list of synsets with NLTK 3.0? I would be thankful for any advice others can offer on this question.</p>
",Preprocessing of the text & Tokenization,extract word synset using wordnet nltk time ago someone asked href retrieve list word given synset using nltk wordnet wrapper one suggested response running code nltk yield tried previously proposed solution solution described page linked throw error therefore wanted ask possible print word list synset nltk would thankful advice others offer question
Word Tokenization using Stanford NLP,"<p>I am using Stanford-NLP Java library. I found it most accurate while dealing with stemming, lemmatization and tokenization of sentences into words. </p>

<p>My requirement is to tokenize words also, like ""leatherjacket"" can be correctly interpreted and spitted out as ""leather jacket"". At this level I am not dealing with spelling correction but please let me know if that is possible.</p>
",Preprocessing of the text & Tokenization,word tokenization using stanford nlp using stanford nlp java library found accurate dealing stemming lemmatization tokenization sentence word requirement tokenize word also like leatherjacket correctly interpreted spitted leather jacket level dealing spelling correction please let know possible
How can I merge if-then statement with some text classifier to build such a model that classifies a sentences into different classes?,"<p>I have the following scenario to process and then classify a natural language as the following:-</p>

<p><em>Initially</em>, I have an algorithm <strong>Alg1</strong> which can classify some data/text according to some matrix-scores , I can build some feature-matrix which is scored by somehow like:</p>

<p>POS | Modal verbs | sentence length |  special words (<em>if a sentence has a special word -> score=1</em>) |  special verbs  (<em>if a sentence has one or more of special verbs</em>) | conditions (<em>while,if,then</em>)) </p>

<hr>

<p>then according to these matrix-scores: I can initially classify some sentences into different classes{<strong>class1</strong>,<strong>class2</strong>,<strong>class3</strong>}, just using <strong><em>[if - then]</em></strong> statements, so the question now how can I merge (normalize) this approach to be used with help of  a text classifier algorithm such as (<strong>SVM</strong>) or whatever, in order to get a better precision-recall) what is the idea to implement such mixed-approach. ?</p>
",Preprocessing of the text & Tokenization,merge statement text classifier build model classifies sentence different class following scenario process classify natural language following initially algorithm alg classify data text according matrix score build feature matrix scored somehow like po modal verb sentence length special word sentence ha special word score special verb sentence ha one special verb condition according matrix score initially classify sentence different class class class class using statement question merge normalize approach used help text classifier algorithm svm whatever order get better precision recall idea implement mixed approach
NLTK: Tuning LinearSVC classifier accuracy? - Looking for better approaches/advices,"<p><strong>Problem/Main objective/TLDR:</strong> Train a classifier, then feed it a random review and get the correspondent predicted review rating (number of stars from 1 to 5) - only 60% accuracy! :(</p>

<p>I have a big dataset with around 48000 tech product reviews (from many different writers and from different products - here this is not so important (?)) and corresponding ratings (1 to 5 stars)
I randomly selected some reviews within each class:</p>

<ul>
<li>1 star: 173 reviews (could not pick 1000 because there were 173)</li>
<li>2 stars: 1000 reviews</li>
<li>3 stars: 1000 reviews</li>
<li>4 stars: 1000 reviews</li>
<li>5 stars: 1000 reviews</li>
</ul>

<p>Total: 4173 reviews - this data is organized in one file (all_reviews_labeled.txt) in tuple format, one review and rating for line:</p>

<ul>
<li>(‘review text’, ‘x star’)</li>
<li>(‘review text’, ‘x star’)</li>
<li>(‘review text’, ‘x star’)</li>
<li>(‘review text’, ‘x star’)</li>
<li>…</li>
</ul>

<p>My 1st “dummie” aproach was:</p>

<ol>
<li>Tokenize review text</li>
<li>POS tagging</li>
<li><p>Get most frequent bigrams that folowing some POS tags rules for
most frequent trigrams (I have seen this rules - using this POS
patterns in “Automatic Star-rating Generation from Text Reviews” -
pag.7 - paper from Chong-U Lim, Pablo Ortiz and Sang-Woo Jun):</p>

<pre><code>for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(text):
    if (t1 == 'JJ' or t1 == 'JJS' or t1 == 'JJR') and (t2 == 'NN' or t2 == 'NNS'):
        bi = unicode(w1 + ' ' + w2).encode('utf-8')
        bigrams.append(bi)
    elif (t1 == 'RB' or t1 == 'RBR' or t1 == 'RBS') and (t2 == 'JJ' or t2 == 'JJS' or t2 == 'JJR') and (t3 != 'NN' or t3 != 'NNS'):
        bi = unicode(w1 + ' ' + w2).encode('utf-8')
        bigrams.append(bi)
    elif (t1 == 'JJ' or t1 == 'JJS' or t1 == 'JJR') and (t2 == 'JJ' or t2 == 'JJS' or t2 == 'JJRS') and (t3 != 'NN' or t3 != 'NNS'):
        bi = unicode(w1 + ' ' + w2).encode('utf-8')
        bigrams.append(bi)
    elif (t1 == 'NN' or t1 == 'NNS') and (t2 == 'JJ' or t2 == 'JJS' or t2 == 'JJRS') and (t3 != 'NN' or t3 != 'NNS'):
        bi = unicode(w1 + ' ' + w2).encode('utf-8')
        bigrams.append(bi)
    elif (t1 == 'RB' or t1 == 'RBR' or t1 == 'RBS') and (t2 == 'VB' or t2 == 'VBD' or t2 == 'VBN' or t2 == 'VBG'):
        bi = unicode(w1 + ' ' + w2).encode('utf-8')
        bigrams.append(bi)
    elif (t1 == 'DT') and (t2 == 'JJ' or t2 == 'JJS' or t2 == 'JJRS'):
        bi = unicode(w1 + ' ' + w2).encode('utf-8')
        bigrams.append(bi)
    elif (t1 == 'VBZ') and (t2 == 'JJ' or t2 == 'JJS' or t2 == 'JJRS'):
        bi = unicode(w1 + ' ' + w2).encode('utf-8')
        bigrams.append(bi)
    else:
        continue
</code></pre></li>
<li><p>Extract features (here is where I have more doubts - should I
only look for this two features?):</p>

<pre><code>features={}
for bigram,freq in word_features:
    features['contains(%s)' % unicode(bigram).encode('utf-8')] = True
    features[""count({})"".format(unicode(bigram).encode('utf-8'))] = freq
return features
</code></pre>

<p>featuresets = [(review_features(review), rating) for (review, rating) in tuples_labeled_reviews]</p></li>
<li><p>Splits the training data into training size and testing size
(90% training - 10% testing):</p>

<pre><code>numtrain = int(len(tuples_labeled_reviews) * 90 / 100)
train_set, test_set = featuresets[:numtrain], featuresets[numtrain:]
</code></pre></li>
<li><p>Train SVMc:</p>

<pre><code>classifier = nltk.classify.SklearnClassifier(LinearSVC())
classifier.train(train_set)
</code></pre></li>
<li><p>Evaluate the classifier:</p>

<pre><code>errors = 0
correct = 0
for review, rating in test_set:
    tagged_rating = classifier.classify(review)
    if tagged_rating == rating:
        correct += 1
        print(""Correct"")
        print ""Guess: "", tagged_rating
        print ""Correct: "", rating
    else:
        errors += 1
</code></pre></li>
</ol>

<p>So far I get only 60% accuracy… 
What can I do to improve my prediction results? Is something before, some text/reviews preprocessing (like removing stopwords/punctuation?) that is missing? Could you suggest me some other approaches? I am still a bit confused if is really a classification problem or a regression one... :/</p>

<p>Please simple explanations, or give me a link to “machine learning for dummies”, or be my mentor, I promise to learn fast!
My background in machine learning/language processing/data mining is very light, I have played a couple of times with weka (Java), but now I need to stick with Python (nltk + scikit-learn)!</p>

<p><strong>EDIT:</strong></p>

<ol>
<li>Now I am also extracting unigrams as features, unigrams POS-tagged as 'JJ', 'NN','VB' and 'RB'. It improved a little the accuracy to 65%.</li>
<li>I applied also do stemming and lemmatization in text before POS tagging. It improved the accuracy to +70%.</li>
</ol>

<p><strong>EDIT 2:</strong></p>

<ol start=""3"">
<li><p>I have feed the classifier all my reviews, the 48000, split into 90% training and 10% testing and the accuracy was 91%.</p></li>
<li><p>Now I have 32000 new reviews (also labeled) and feed them all for testing and the mean accuracy was 62 % ... my confusion matrix is something like this image below (i divided by equal errors of +1/-1 star point, +2/-2, +3/-3 - because it is just an illustration):</p></li>
</ol>

<p><a href=""https://i.sstatic.net/efSpy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/efSpy.png"" alt=""enter image description here""></a>
What is happening? Why accuracy drops so much at 3 and 5 stars?</p>
",Preprocessing of the text & Tokenization,nltk tuning linearsvc classifier accuracy looking better approach advice problem main objective train classifier feed random review get correspondent predicted review rating number star accuracy big dataset around tech product review many different writer different product important corresponding rating star randomly selected review within class star review could pick star review star review star review star review total review data organized one file review labeled txt tuple format one review rating line review text x star review text x star review text x star review text x star st dummie aproach wa tokenize review text po tagging get frequent bigram folowing po tag rule frequent trigram seen rule using po pattern automatic star rating generation text review pag paper chong u lim pablo ortiz sang woo jun extract feature doubt look two feature featuresets review feature review rating review rating tuples labeled review split training data training size testing size training testing train svmc evaluate classifier far get accuracy improve prediction result something text review preprocessing like removing stopwords punctuation missing could suggest approach still bit confused really classification problem regression one please simple explanation give link machine learning dummy promise learn fast background machine learning language processing data mining light played couple time weka java need stick python nltk scikit learn edit also extracting unigrams feature unigrams po tagged jj nn vb rb improved little accuracy applied also stemming lemmatization text po tagging improved accuracy edit feed classifier review split training testing accuracy wa new review also labeled feed testing mean accuracy wa confusion matrix something like image divided equal error star point illustration happening accuracy drop much star
Stemming algorithm that produces real words,"<p>I need to take a paragraph of text and extract from it a list of ""tags"".  Most of this is quite straight forward. However I need some help now stemming the resulting word list to avoid duplicates. Example: Community / Communities</p>

<p>I've used an implementation of Porter Stemmer algorithm (I'm writing in PHP by the way):</p>

<p><a href=""http://tartarus.org/~martin/PorterStemmer/php.txt"" rel=""noreferrer"">http://tartarus.org/~martin/PorterStemmer/php.txt</a></p>

<p>This works, up to a point, but doesn't return ""real"" words.  The example above is stemmed to ""commun"".</p>

<p>I've tried ""Snowball"" (suggested within another Stack Overflow thread).</p>

<p><a href=""http://snowball.tartarus.org/demo.php"" rel=""noreferrer"">http://snowball.tartarus.org/demo.php</a></p>

<p>For my example (community / communities), Snowball stems to ""communiti"".</p>

<p><strong>Question</strong></p>

<p>Are there any other stemming algorithms that will do this? Has anyone else solved this problem?</p>

<p><em>My current thinking is that I could use a stemming algorithm to avoid duplicates and then pick the shortest word I encounter to be the actual word to display.</em></p>
",Preprocessing of the text & Tokenization,stemming algorithm produce real word need take paragraph text extract list tag quite straight forward however need help stemming resulting word list avoid duplicate example community community used implementation porter stemmer algorithm writing php way work point return real word example stemmed commun tried snowball suggested within another stack overflow thread example community community snowball stem communiti question stemming algorithm ha anyone else solved problem current thinking could use stemming algorithm avoid duplicate pick shortest word encounter actual word display
NLTK words lemmatizing,"<p>I am trying to do lemmatization on words with <code>NLTK</code>.  </p>

<p>What I can find now is that I can use the <code>stem</code> package to get some results like transform ""cars"" to ""car"" and ""women"" to ""woman"", however I cannot do lemmatization on some words with affixes like ""acknowledgement"".  </p>

<p>When using <code>WordNetLemmatizer()</code> on ""acknowledgement"", it returns ""acknowledgement"" and using <code>.PorterStemmer()</code>, it returns ""acknowledg"" rather than ""acknowledge"".  </p>

<p>Can anyone tell me how to eliminate the affixes of words?<br>
Say, when input is ""acknowledgement"", the output to be ""acknowledge""</p>
",Preprocessing of the text & Tokenization,nltk word lemmatizing trying lemmatization word find use package get result like transform car car woman woman however lemmatization word affix like acknowledgement using acknowledgement return acknowledgement using return acknowledg rather acknowledge anyone tell eliminate affix word say input acknowledgement output acknowledge
How to recognize URLs using Stanford CoreNLP,"<p>I am using Stanford CoreNLP to extract various types of information from a given document. I am trying to detect URL patterns and I can see that links beginning with http:// or https:// are recognized properly, but links beginning with ftp://, svn:// etc are broken at ':' and 'ftp' or 'svn' becomes a token instead of the complete link being recognized a token. Due to this, I am not able to use any regex for match.
I know there is a way to tokenize words with whitespaces using tokenize.whitespace.
Is there a way to suppress ':' tokenizing the URL so that the complete link is recognized as a token?</p>
",Preprocessing of the text & Tokenization,recognize url using stanford corenlp using stanford corenlp extract various type information given document trying detect url pattern see link beginning recognized properly link beginning ftp svn etc broken ftp svn becomes token instead complete link recognized token due able use regex match know way tokenize word whitespaces using tokenize whitespace way suppress tokenizing url complete link recognized token
Classifying text into different classes depending on similarity,"<p>I am working on very large documents {NEWS + Articles} using modeling Natural Sentences into classes, please look at the following example:</p>

<pre><code>1- The System enables a user to shut down the server remotely ==&gt; class 1

2- The Application allows a customer to to close the machine online ==&gt; (must be also) class 1 , why ?
</code></pre>

<p>because both sentences have many similar synonyms {System ~Application,enables ~ allows ,user ~ customer ,shut down ~ close,server ~ machine,remotely~online}
so I am  doing  classifier train    on some data depending on the similarity rules or synonyms of the words + stemming + may be (lemmatization) the most number of rules the most result we can get.</p>

<p>so the question what is the best strategy to configure/adjust the classifier to that ideas ?
Thank you in advance</p>
",Preprocessing of the text & Tokenization,classifying text different class depending similarity working large document news article using modeling natural sentence class please look following example sentence many similar synonym system application enables allows user customer shut close server machine remotely online classifier train data depending similarity rule synonym word stemming may lemmatization number rule result get question best strategy configure adjust classifier idea thank advance
steamming words with r,"<p>I'm having a difficulties to understand R stemming word process.</p>

<p>In my example, i created the following corpus object</p>

<pre><code>a &lt;- Corpus(VectorSource(""device so much more funand  unlike most android torrent download clients""))
</code></pre>

<p>So a is</p>

<pre><code>a[[1]]$content

[1] ""device so much more funand  unlike most android torrent download clients""
</code></pre>

<p>The first word in this string is ""device"", I created my term matrix</p>

<pre><code>b &lt;- TermDocumentMatrix(a, control = list(stemming = TRUE)) 
</code></pre>

<p>and got this as an output</p>

<pre><code>dimnames(b)$Terms
[1] ""android""  ""client""   ""devic""    ""download"" ""funand""   ""more""     ""most""      ""much""     ""torrent"" 
[10] ""unlik""
</code></pre>

<p>What i like to know is why i lost the ""e"" at ""device"" and ""unlike"" but did not loss it at ""more"".</p>

<p>how can i avoid this from happening in this word and in some others?</p>

<p>Thanks.</p>
",Preprocessing of the text & Tokenization,steamming word r difficulty understand r stemming word process example created following corpus object first word string device created term matrix got output like know lost e device unlike loss avoid happening word others thanks
How to determine whether 2 code snippets are functionally same?,"<p>Given 2 code snippets I want to check whether they are functionally similar or not. By functional similarity I mean that they should yield same output when provided with same input.
I am extracting feature set from a given code snippet using : </p>

<ol>
<li><strong>Syntactic Approach</strong> : Using basic NLP techniques like stemming, splitting etc.</li>
<li><strong>Semantic Approach</strong> : Using AST to normalize a code snippet eg : converting ‘for’ to ‘while’ etc.</li>
</ol>

<p>After forming tokens, I am using topic modelling algorithms like Latent Dirichlet Allocation, probabilistic latent semantic indexing etc. for finding topics in a given code snippet and matching it with the other code snippet’s topic. Though I understand it’s a hard problem, the accuracy of the approach is much lower than I expected.</p>

<p>It would be great if I can get any pointers/ideas on better algorithms / techniques which will be more effective.</p>

<p><strong>Edit</strong> : I am not looking forward for a generic approach. Any approach which gives approximate results with certain accuracy would do.</p>
",Preprocessing of the text & Tokenization,determine whether code snippet functionally given code snippet want check whether functionally similar functional similarity mean yield output provided input extracting feature set given code snippet using syntactic approach using basic nlp technique like stemming splitting etc semantic approach using ast normalize code snippet eg converting etc forming token using topic modelling algorithm like latent dirichlet allocation probabilistic latent semantic indexing etc finding topic given code snippet matching code snippet topic though understand hard problem accuracy approach much lower expected would great get pointer idea better algorithm technique effective edit looking forward generic approach approach give approximate result certain accuracy would
Lucene lemmatization,"<p>I'm indexing some English texts in a Java application with Lucene, and I need to lemmatization them with Lucene 4_1_0. I've found stemming (PorterStemFilter and SnowballFilter), but it not enough. </p>

<p>After lemmatizations I wanted to use a thesaurus for query expansion, does Lucene also include a thesaurus?</p>

<p>If it is not possible I will use the StanfordCoreNLP and WordNet instead. </p>

<p>Do you think that lemmatization may influence the search using Lucene library?</p>

<p>Thanks</p>
",Preprocessing of the text & Tokenization,lucene lemmatization indexing english text java application lucene need lemmatization lucene found stemming porterstemfilter snowballfilter enough lemmatizations wanted use thesaurus query expansion doe lucene also include thesaurus possible use stanfordcorenlp wordnet instead think lemmatization may influence search using lucene library thanks
Sentence tokenization for texts that contains quotes,"<p>Code:</p>

<pre><code>from nltk.tokenize import sent_tokenize           
pprint(sent_tokenize(unidecode(text)))
</code></pre>

<p>Output:</p>

<pre><code>[After Du died of suffocation, her boyfriend posted a heartbreaking message online: ""Losing consciousness in my arms, your breath and heartbeat became weaker and weaker.',
 'Finally they pushed you out of the cold emergency room.',
 'I failed to protect you.',
 '""Li Na, 23, a migrant worker from a farming family in Jiangxi province, was looking forward to getting married in 2015.',]
</code></pre>

<p>Input:</p>

<blockquote>
  <p>After Du died of suffocation, her boyfriend posted a heartbreaking
  message online: ""Losing consciousness in my arms, your breath and
  heartbeat became weaker and weaker. Finally they pushed you out of the
  cold emergency room. I failed to protect you.""</p>
  
  <p>Li Na, 23, a migrant worker from a farming family in Jiangxi province,
  was looking forward to getting married in 2015.</p>
</blockquote>

<p>Quotes should be included in previous sentence. Instead of  <code>"" Li.</code></p>

<p>It fails at <code>.""</code> How to fix this?</p>

<p><strong>Edit:</strong>
Explaining the extraction of text.</p>

<pre><code>html = open(path, ""r"").read()                           #reads html code
article = extractor.extract(raw_html=html)              #extracts content
text = unidecode(article.cleaned_text)                  #changes encoding 
</code></pre>

<p>Here, article.cleaned_text is in unicode. The idea behind using this to change characters “ to "".</p>

<p>Solutions @alvas Incorrect Result:</p>

<pre><code>['After Du died of suffocation, her boyfriend posted a heartbreaking message online: ""Losing consciousness in my arms, your breath and heartbeat became weaker and weaker.',
 'Finally they pushed you out of the cold emergency room.',
 'I failed to protect you.',
 '""',
 'Li Na, 23, a migrant worker from a farming family in Jiangxi province, was looking forward to getting married in 2015.'
]
</code></pre>

<p><strong>Edit2:</strong>
(Updated) nltk and python version</p>

<pre><code>python -c ""import nltk; print nltk.__version__""
3.0.4
python -V
Python 2.7.9
</code></pre>
",Preprocessing of the text & Tokenization,sentence tokenization text contains quote code output input du died suffocation boyfriend posted heartbreaking message online losing consciousness arm breath heartbeat became weaker weaker finally pushed cold emergency room failed protect li na migrant worker farming family jiangxi province wa looking forward getting married quote included previous sentence instead fails fix edit explaining extraction text article cleaned text unicode idea behind using change character solution alvas incorrect result edit updated nltk python version
Regular expression to match line that doesn&#39;t contain alphabet and numeric?,"<p>I want to use the regular expression to check the string that doesn't contain alphabet or numeric for removing it.</p>

<pre><code>Ex: ""().;"" =&gt; false
    ""(9).;"" =&gt; true
    "")+&amp;^%"" =&gt; false
    ""A)%$#"" =&gt; true
</code></pre>

<p>Thanks and regards,
Hien Su</p>
",Preprocessing of the text & Tokenization,regular expression match line contain alphabet numeric want use regular expression check string contain alphabet numeric removing thanks regard hien su
Stanford part-of-speech tagger cannot tag parentheses and quotation marks in pre-tokenized text,"<p>I have a pre-tokenized text as the input to Stanford part-of-speech tagger. It cannot tag parentheses and quotation marks correctly at all. I don't want Stanford Tagger's default tokenization, so I disabled it, using <code>-tokenize false</code> option. </p>

<p>I know we should <code>escape characters</code> the way the <code>Penn Treebank</code> does during tokenization, such as turning parentheses into <code>-LRB-</code> and <code>-RRB-</code>, which is supported in <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#q"" rel=""nofollow"">standalone Stanford Parser</a>. </p>

<p>Is there a way to nicely handle it in Stanford part-of-speech tagger?</p>
",Preprocessing of the text & Tokenization,stanford part speech tagger tag parenthesis quotation mark pre tokenized text pre tokenized text input stanford part speech tagger tag parenthesis quotation mark correctly want stanford tagger default tokenization disabled using option know way doe tokenization turning parenthesis supported standalone stanford parser way nicely handle stanford part speech tagger
how to extend/modify the dictionary in Stanford NLP?,"<p>i've got special words and compound nouns
that should be identified as nouns in my system. </p>

<p>eg.: </p>

<p>i.) ""XYZBrand"" a brand name that's not in the standard dictionary</p>

<p>ii.) ""copper wire"" should be treated as 1 token all throughout the system. </p>

<p>both """"XYZBrand"" and ""copper wire"" above should be treated as a noun each. 
or, rather, 
it's all fine if they are treated the way any other noun, say 
""jacket"" is. </p>

<p>as far as i could see, StanfordNLP is doing just so for the first case above. 
when it sees a token that's not in the dictionary, it's 
taking it as a noun(?).
so, this is all about the compound nouns-- like ""copper wire"" above. </p>

<p>from what i see, there's the following ways of doing it:</p>

<p>1.) add the compound words to the dictionary from the start: 
so that StanfordNLP will tokenize them so and process on from the beginning. 
how to do this? this is the way i'm looking to go.</p>

<p>2.) define a <code>GrammaticalRelation</code> in <code>UniversalEnglishGrammaticalRelations</code> to 
see these compound words. this would work. however i wont always need <code>parser</code> in the pipeline and this doesn't ""feel"" like a proper twist.</p>

<p>3.) tokenize them so-- whenever there's ""copper"" and ""wire"" side by side, take the two as one token. 
however, this takes tempering with <code>edu.stanford.nlp.process.PTBLexer</code> 
and i'm not much willing to do so. </p>

<p>4.) write an annotator to go in right after <code>TokenizerAnnotator</code>.</p>

<p>5.) pre-process the text before feeding into Stanford-NLP: turn ""copper wire""
into ""copper-wire"" for instance. this is the solution i least like. </p>

<p>how to add new terms to the dictionary in the system?</p>

<p>TIA. </p>

<p>//===========================</p>

<p>EDIT: </p>

<p>""copper"" and ""wire"" when appearing side by side separated by whitespace(s) 
is <strong>always</strong> meaningful as one token-- ""copper wire"" in our context. </p>

<p>looking up the type dependency to see whether it's a 
compound noun with the ""right"" 
kind of other token(s) isn't a favorable choice:</p>

<ul>
<li><p>our ""compound nouns"" aren't always groups of nouns. ""small contanier"" 
should also be one token in our system.</p></li>
<li><p>will take looking up the token, say ""copper"" and checking 
its dependency relations 
with other tokens and will slow it down. 
this is looking up our set of words and 
tracing the dependencies-- ""amod"" for ""container"" and ""compound"" for ""wire""
just about in every step. </p></li>
<li><p>i'll look to turn around other things. Eg.: in </p>

<pre><code>""some sugar and a bit of salt"" 
</code></pre></li>
</ul>

<p>""sugar"" and ""bit"" are ""conj:and""-ed accurately by the grammer. 
however, ""conj:and""-ing ""sugar"" and ""salt"" is more accurate in our context. </p>

<p>once i find a fine way to inject in my own dictionary to override 
the dictionary the tokenizer and tagger are using, or rather, to override 
the trained behaviour of the tokenizer and tagger, i can bend the system 
to process on our own set of terms. this Q is to look for that way to do this bend. </p>
",Preprocessing of the text & Tokenization,extend modify dictionary stanford nlp got special word compound noun identified noun system eg xyzbrand brand name standard dictionary ii copper wire treated token throughout system xyzbrand copper wire treated noun rather fine treated way noun say jacket far could see stanfordnlp first case see token dictionary taking noun compound noun like copper wire see following way add compound word dictionary start stanfordnlp tokenize process beginning way looking go define see compound word would work however wont always need pipeline feel like proper twist tokenize whenever copper wire side side take two one token however take tempering much willing write annotator go right pre process text feeding stanford nlp turn copper wire copper wire instance solution least like add new term dictionary system tia edit copper wire appearing side side separated whitespace always meaningful one token copper wire context looking type dependency see whether compound noun right kind token favorable choice compound noun always group noun small contanier also one token system take looking token say copper checking dependency relation token slow looking set word tracing dependency amod container compound wire every step look turn around thing eg sugar bit conj ed accurately grammer however conj ing sugar salt accurate context find fine way inject dictionary override dictionary tokenizer tagger using rather override trained behaviour tokenizer tagger bend system process set term q look way bend
Snowball Stemming: defining Regions,"<p>I'm trying to understand the snoball stemming algorithmus. The algorithmus is using two regions R1 and R2 that are definied as follows:</p>
<blockquote>
<p>R1 is the region after the first non-vowel following a vowel, or is
the null region at the end of the word if there is no such non-vowel.</p>
<p>R2 is the region after the first non-vowel following a vowel in R1, or
is the null region at the end of the word if there is no such
non-vowel.</p>
<p><a href=""http://snowball.tartarus.org/texts/r1r2.html"" rel=""nofollow noreferrer"">http://snowball.tartarus.org/texts/r1r2.html</a></p>
</blockquote>
<p>Examples are</p>
<pre><code>    b   e   a   u   t   i   f   u   l
                      |&lt;-------------&gt;|    R1
                              |&lt;-----&gt;|    R2

   b   e   a   u   t   y
                     |&lt;-&gt;|    R1
                       -&gt;|&lt;-  R2

   a   n   i   m   a   d   v   e   r   s   i   o   n
        |&lt;-----------------------------------------&gt;|    R1
                |&lt;---------------------------------&gt;|    R2

   s   p   r   i   n   k   l   e   d
                     |&lt;-------------&gt;|    R1
                                   -&gt;|&lt;-  R2

    e   u   c   h   a   r   i   s   t
            |&lt;---------------------&gt;|    R1
                        |&lt;---------&gt;|    R2
</code></pre>
<p>My question is, why is &quot;kled&quot; in springkled and &quot;harist&quot; in eucharist defined as R1? I thought the correct result would be &quot;inkled&quot; and &quot;arist&quot;?</p>
",Preprocessing of the text & Tokenization,snowball stemming defining region trying understand snoball stemming algorithmus algorithmus using two region r r definied follows r region first non vowel following vowel null region end word non vowel r region first non vowel following vowel r null region end word non vowel example question kled springkled harist eucharist defined r thought correct result would inkled arist
sentiment analysis and efficient clustering of raw text with minimal context,"<p>Say I have an email chain where 2 people discuss about a problem and its solution. I have some context too. Example, the email chain is about some problem in using iPhone 6 with iOS 7. Thats it. From the content/text of these emails, I need to figure out what exactly the problem is and what exactly is the solution proposed. </p>

<p>Now, if we port this problem to big data i.e. millions of such email chains, I want to know how to classify or cluster them. </p>

<p>I am using Apache Spark's MLlib - LDA, FPgrowth and Kmeans (+ a huge list of stop words). But my results dont look correct. Playing around with params for these algorithms is just giving me knowledge but not good results. My biggest problem is not having training data. Unfortunately, most solutions I see online use manually created training data. Any help?</p>
",Preprocessing of the text & Tokenization,sentiment analysis efficient clustering raw text minimal context say email chain people discus problem solution context example email chain problem using iphone io thats content text email need figure exactly problem exactly solution proposed port problem big data e million email chain want know classify cluster using apache spark mllib lda fpgrowth kmeans huge list stop word result dont look correct playing around params algorithm giving knowledge good result biggest problem training data unfortunately solution see online use manually created training data help
"limit text files to a certain word length, but keep complete sentences","<p>I have a corpus of text files that I need to copy, but limiting each file to roughly the same word length, while maintaining complete sentences. Treating any punctuation within <code>{.?!}</code> as a sentence boundary is acceptable. I could do this with python, but I am trying to learn bash, so suggestions are welcome. The approach I have been considering is to overshoot my target word length by a few words and then trim the result to the last sentence boundary.</p>

<p>I am familiar with <code>head</code> and <code>wc</code>, but I can't come up with a way to combine the two. The <code>man</code> file for <code>head</code> does not indicate a way to use word-counts, and the <code>man</code> file for <code>wc</code> does not indicate a way to split the file.</p>

<p>Context:
I am working on a text classification task with machine-learning (using <code>weka</code>, for the record). I want to make sure that text length (which varies widely in my data) is not influencing the outcomes too much. To do this, I am trying to normalize my text lengths before I perform feature extraction.</p>
",Preprocessing of the text & Tokenization,limit text file certain word length keep complete sentence corpus text file need copy limiting file roughly word length maintaining complete sentence treating punctuation within sentence boundary acceptable could python trying learn bash suggestion welcome approach considering overshoot target word length word trim result last sentence boundary familiar come way combine two file doe indicate way use word count file doe indicate way split file context working text classification task machine learning using record want make sure text length varies widely data influencing outcome much trying normalize text length perform feature extraction
How to retrieve all variants of a lexeme in Java?,"<p>I am searching for a way to retrieve all variants of the lexeme of a specific word.</p>

<p>Example: running -> (run, runs, ran, running…)</p>

<p>I tried out Stanford NLP according to <a href=""https://stackoverflow.com/questions/1578062/lemmatization-java"">this post</a>. However, the lemma-annotator only retrieves the lemma (running -> run), not the complete set of variants. Is there a way to do this with Stanford NLP or another Java Lib/Framework?</p>

<p>Clarification: I do not search for a stemmer. Also, I would like to avoid programming a new algorithm from scratch to crawl WordNet or similar dictionaries.</p>
",Preprocessing of the text & Tokenization,retrieve variant lexeme java searching way retrieve variant lexeme specific word example running run run ran running tried stanford nlp according href post however lemma annotator retrieves lemma running run complete set variant way stanford nlp another java lib framework clarification search stemmer also would like avoid programming new algorithm scratch crawl wordnet similar dictionary
"Using Wordnet to generate superlative, comparative and adjectives","<p>I have a wordnet database setup, and I'm trying to generate synonyms for various words.</p>

<p>For example, the word, ""greatest"". I'll look through and find several different synonyms, but none of them really fit the definition - for example, one is ""superlative"".</p>

<p>I'm guessing that I need to do some sort of check by frequency in a given language or stemming a word to get the base word (for example, greatest -> great, great -> best).</p>

<p>What table should I be using to ensure my words make some modicum of sense?</p>
",Preprocessing of the text & Tokenization,using wordnet generate superlative comparative adjective wordnet database setup trying generate synonym various word example word greatest look find several different synonym none really fit definition example one superlative guessing need sort check frequency given language stemming word get base word example greatest great great best table using ensure word make modicum sense
Corenlp document level multithreading,"<p>I have 8 million wikipedia articles to parse. I want to run 7 operations: tokenize,ssplit,pos,lemma,ner,parse,dcoref. Each document is taking approx 20 secs. In this rate it will take months to parse the whole data set in single thread. There is a 'nthreads' option for simultaneously parsing successive sentences. But co-reference analyzer cannot work on single sentence level. I can split my documents in multiple buckets and run corenlp on each of them simultaneously but that is resource hungry. Is there any simpler way to run multi-threaded corenlp at document level (not sentence) ? (I have  100 GB ram and 50 cores). </p>
",Preprocessing of the text & Tokenization,corenlp document level multithreading million wikipedia article parse want run operation tokenize ssplit po lemma ner parse dcoref document taking approx sec rate take month parse whole data set single thread nthreads option simultaneously parsing successive sentence co reference analyzer work single sentence level split document multiple bucket run corenlp simultaneously resource hungry simpler way run multi threaded corenlp document level sentence gb ram core
A confusion about the porter stemming algorithm,"<p>I am trying to implement porter stemming algorithm, but I stumbled at this point</p>

<blockquote>
  <p>where the square brackets denote
  arbitrary presence of their contents.
  Using (VC){m} to denote VC repeated m
  times, this may again be written as</p>

<pre><code>[C](VC){m}[V].
</code></pre>
  
  <p>m will be called the \measure\ of any
  word or word part when represented in
  this form. The case m = 0 covers the
  null word. Here are some examples:</p>

<pre><code>m=0    TR,  EE,  TREE,  Y,  BY.
m=1    TROUBLE,  OATS,  TREES,  IVY.
m=2    TROUBLES,  PRIVATE,  OATEN,  ORRERY.
</code></pre>
</blockquote>

<p>I don't understand what is this ""measure"" and what does it stand for?</p>
",Preprocessing of the text & Tokenization,confusion porter stemming algorithm trying implement porter stemming algorithm stumbled point square bracket denote arbitrary presence content using vc denote vc repeated time may written called measure word word part represented form case cover null word example understand measure doe stand
stemming problems in python,"<p>I want to find the stems of Persian language verbs. For that first I made a file containing some current and exception stems. I want first, my code searches in the file and if the stem was there it returns the stem and if not, it goes through the rest of the code and by deleting suffixes and prefixes it returns the stem. The problem 1) is that it doesn't pay attention to the file and ignoring it, it just goes through the rest of the code and outputs a wrong stem because exceptions are in the file. 2) because I used ""for"", the suffixes and prefixes of verbs influence on other verbs and omit other verbs' suffixes and prefixes which sometimes outputs a wrong stem. How should I change the code that each ""for"" loop works independently and doesn't affect the others? (I have to just write one function and call just it)</p>

<p>I reduced some suffixes and prefixes.</p>

<pre><code>def stemmer (verb, file):
   with open (file, encoding = ""utf-8"") as f:   
      f = f.read().split()
      for i in f:
           if i in verb:
           return i
           else:
               for i in suffix1:      
                    if verb.endswith(i):
                        verb = verb[:-len(i)]
                        return verb
</code></pre>
",Preprocessing of the text & Tokenization,stemming problem python want find stem persian language verb first made file containing current exception stem want first code search file stem wa return stem go rest code deleting suffix prefix return stem problem pay attention file ignoring go rest code output wrong stem exception file used suffix prefix verb influence verb omit verb suffix prefix sometimes output wrong stem change code loop work independently affect others write one function call reduced suffix prefix
Filter based on expression match,"<p>I've been combing over regex related websites and tutorials to figure out how to accomplish this but unfortunately still can't get a clear grasp on regular expressions.  I have forum posts that have been tokenized and am trying to filter them based on posts that may have age related info.  </p>

<p>For example, I used this snippet to try to filter:</p>

<pre><code>re.search(r'.*(daughter|ds|son|dd|) (is|was|is turning|turned) ([0-9]{2})*', post)
</code></pre>

<p>This works on text such as the string</p>

<pre><code>'my son was 7 when this incident occurred'
</code></pre>

<p>but also on the string</p>

<pre><code>'is'
</code></pre>

<p>To filter,</p>

<pre><code>r = re.compile('.*(daughter|ds|son|dd|)(is|was|is turning|turned)([0-9]{2}).*')
filter(r.match, ['my daughter is 7', 'is'])
</code></pre>

<p>but this returns an empty list when there should be a match with the first string.</p>

<p>What is wrong with the code snippet and what are some pointers to help with regular expression matches for my filtering?</p>
",Preprocessing of the text & Tokenization,filter based expression match combing regex related website tutorial figure accomplish unfortunately still get clear grasp regular expression forum post tokenized trying filter based post may age related info example used snippet try filter work text string also string filter return empty list match first string wrong code snippet pointer help regular expression match filtering
Extracting more similar words from a list of words,"<p>So I have a list of words describing a particular group. For example, one group is based around pets. </p>

<p>The words for the example group pets, are as follows: </p>

<p><code>[pets, pet, kitten, cat, cats, kitten, puppies, puppy, dog, dogs, dog walking, begging, catnip, lol, catshit, thug life, poop, lead, leads, bones, garden, mouse, bird, hamster, hamsters, rabbits, rabbit, german shepherd, moggie, mongrel, tomcat, lolcatz, bitch, icanhazcheeseburger, bichon frise, toy dog, poodle, terrier, russell, collie, lab, labrador, persian, siamese, rescue, Celia Hammond, RSPCA, battersea dogs home, rescue home, battersea cats home, animal rescue, vets, vet, supervet, Steve Irwin, pugs, collar, worming, fleas, ginger, maine coon, smelly cat, cat people, dog person, Calvin and Hobbes, Calvin &amp; Hobbes, cat litter, catflap, cat flap, scratching post, chew toy, squeaky toy, pets at home, cruft's, crufts, corgi, best in show, animals, Manchester dogs' home, manchester dogs home, cocker spaniel, labradoodle, spaniel, sheepdog, Himalayan, chinchilla, tabby, bobcat, ragdoll, short hair, long hair, tabby cat, calico, tabbies, looking for a good home, neutring, missing, spayed, neutered, declawing, deworming, declawed, pet insurance, pet plan, guinea pig, guinea pigs, ferret, hedgehogs, minipigs, mastiff, leonburger, great dane, four-legged friend, walkies, goldfish, terrapin, whiskas, mr dog, sheba, iams]</code></p>

<p>Now I plan on enriching this list using NLTK.</p>

<p>So as a start I can get the synset of each word. If we take <code>cats</code>, as an example we obtain:</p>

<pre><code>Synset('cat.n.01')
Synset('guy.n.01')
Synset('cat.n.03')
Synset('kat.n.01')
Synset('cat-o'-nine-tails.n.01')
Synset('caterpillar.n.02')
Synset('big_cat.n.01')
Synset('computerized_tomography.n.01')
Synset('cat.v.01')
Synset('vomit.v.01')
</code></pre>

<p>For this we user <code>nltk's wordnet</code>, <code>from nltk.corpus import wordnet as wn</code>.</p>

<p>We can then obtain the lemmas for each synset. By simply adding these lemma's I inturn add quite a bit of noise, how ever I also add some interesting words.</p>

<p>But what I would like to look at is noise reduction, and would appreciate any suggestions or alternate methods to the above.</p>

<p>One such idea, I am trying is to see if the word 'cats' appears in the synset name or definition, to include or exclude those lemmas.</p>
",Preprocessing of the text & Tokenization,extracting similar word list word list word describing particular group example one group based around pet word example group pet follows plan enriching list using nltk start get synset word take example obtain user obtain lemma synset simply adding lemma inturn add quite bit noise ever also add interesting word would like look noise reduction would appreciate suggestion alternate method one idea trying see word cat appears synset name definition include exclude lemma
How to Train an Input File containing lines of text in NLTK Python,"<p>I need help in training a data set which can then be tagged by tokenizing using pos tagger.
My Input File is - kon_set1.txt
containing text in Konkani(Indian Language).</p>

<pre><code>ताजो स्वास आनी चकचकीत दांत तुमचें व्यक्तीमत्व परजळायतात.
दांत आशिल्ल्यान तुमचो आत्मविश्वासय वाडटा.
आमच्या हड्ड्यां आनी दांतां मदीं बॅक्टेरिया आसतात.
त्यो दांत बुरशे आनी स्वास घाणयारो करतात.
हांगा दिल्ल्या कांय सोंप्या सुचोवण्यांच्या आदारान तुमी तुमचे दांत नितळ आनी स्वास ताजो दवरूंक शकतात.
</code></pre>

<p>I would like to know how training of this data set can be done.
So that I can later use the trained data to tokenize using POS tagger.
Thanking You. Awaiting for a positive response.</p>
",Preprocessing of the text & Tokenization,train input file containing line text nltk python need help training data set tagged tokenizing using po tagger input file kon set txt containing text konkani indian language would like know training data set done later use trained data tokenize using po tagger thanking awaiting positive response
Separately tokenizing and pos-tagging with CoreNLP,"<p>I'm having few problems with the way Stanford CoreNLP divides text into sentences, namely:</p>

<ol>
<li>It treats ! and ? (exclamation and question marks) inside a quoted text as a sentence end where it shouldn't, e.g.: He shouted ""Alice! Alice!"" - here it treats the ! after the first Alice as a sentence end and divides the text into two sentences.</li>
<li>It doesn't recognize ellipses as a sentence end.</li>
</ol>

<p>In NLTK we would deal with these problems by simply normalizing text before and after dividing into sentences, that is, replacing the said marks with other symbols before dividing and returning them after to send them down the pipeline in a proper form. </p>

<p>However, the tokenizer in CoreNLP tokenizes before dividing into sentences and that doesn't leave much room to tweak the process. So, my first question: is it possible to ""correct"" the tokenizer without rewriting it to account for such cases?</p>

<p>If it's not, can we at least separate tokenization from the rest of the pipeline (in my case it's pos, lemma, and parse), so that we can change the tokens themselves before sending them further down?</p>

<p>Thanks!</p>
",Preprocessing of the text & Tokenization,separately tokenizing po tagging corenlp problem way stanford corenlp divide text sentence namely treat exclamation question mark inside quoted text sentence end e g shouted alice alice treat first alice sentence end divide text two sentence recognize ellipsis sentence end nltk would deal problem simply normalizing text dividing sentence replacing said mark symbol dividing returning send pipeline proper form however tokenizer corenlp tokenizes dividing sentence leave much room tweak process first question possible correct tokenizer without rewriting account case least separate tokenization rest pipeline case po lemma parse change token sending thanks
keywords in NEGATIVE Sentiment using sentiment Analysis(stanfordNLP),"<p>My requirement is to read the input text and undergo sentiment Analysis. If the sentiment analysis is NEGATIVE i should get only the negative keywords used in the context.</p>

<p>The following is the code snippet to achieve the sentiment analysis:</p>

<p>String tweet=""the movie is worst"";<br>
Properties props = new Properties();
    //   props.put(""pos.model"", ""D:\Stanford_API's\english-left3words-distsim.tagger"");
            props.setProperty(""annotators"", ""tokenize, ssplit, pos,lemma,parse, sentiment"");</p>

<pre><code>     pipeline = new StanfordCoreNLP(props);

        int mainSentiment = 0;
        if (tweet != null &amp;&amp; tweet.length() &gt; 0) {
            int longest = 0;

            Annotation annotation = pipeline.process(tweet);
            for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) 
            {
                Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
                int sentiment = RNNCoreAnnotations.getPredictedClass(tree);

               SimpleMatrix sd= RNNCoreAnnotations.getNodeVector(tree);

               System.out.println(""---sentiment----""+sd.negative().eig());
               //System.out.println(""---sd.get(0)----""+sd.get(0));

                String partText = sentence.toString();
                if (partText.length() &gt; longest) 
                {
                    mainSentiment = sentiment;
                    longest = partText.length();
                }

           }
        }
</code></pre>

<p>Pls suggest how i can get the keywords from the sentence if the sentiment analysis is NEGATIVE ?</p>
",Preprocessing of the text & Tokenization,keywords negative sentiment using sentiment analysis stanfordnlp requirement read input text undergo sentiment analysis sentiment analysis negative get negative keywords used context following code snippet achieve sentiment analysis string tweet movie worst property prop new property prop put po model stanford api english left word distsim tagger prop setproperty annotator tokenize ssplit po lemma parse sentiment pls suggest get keywords sentence sentiment analysis negative
Stemming unstructured text in NLTK,"<p>I tried the regex stemmer, but I get hundreds of unrelated tokens. I'm just interested in the ""play"" stem. Here is the code I'm working with:</p>

<pre class=""lang-python prettyprint-override""><code>import nltk
from nltk.book import *
f = open('tupac_original.txt', 'rU')
text = f.read()
text1 = text.split()
tup = nltk.Text(text1)
lowtup = [w.lower() for w in tup if w.isalpha()]
import sys, re
tupclean = [w for w in lowtup if not w in nltk.corpus.stopwords.words('english')]
from nltk import stem
tupstem = stem.RegexpStemmer('az$|as$|a$')
[tupstem.stem(i) for i in tupclean] 
</code></pre>

<p>The result of the above is;</p>

<pre><code>['like', 'ed', 'young', 'black', 'like'...]
</code></pre>

<p>I'm trying to clean up <code>.txt</code> files (all lowercase, remove stopwords, etc), normalize multiple spellings of a word into one and do a frequency dist/count. I know how to do <code>FreqDist</code>, but any suggestions as to where I'm going wrong with the stemming?</p>
",Preprocessing of the text & Tokenization,stemming unstructured text nltk tried regex stemmer get hundred unrelated token interested play stem code working result trying clean file lowercase remove stopwords etc normalize multiple spelling word one frequency dist count know suggestion going wrong stemming
Counting words in list using a dictionary,"<p>I have a list of dictionaries containing a word and some misspellings of the word. I am trying to go through a list of strings and first count the occurrences of the the word and then count the occurrences of each misspelling. I have tried using <code>if word in string</code> but this ends up giving me an incorrect count as many misspellings contain the actual word itself within them. Is it possible to use pythons <code>counter</code> here or would regex make more sense?</p>

<p>For example I have </p>

<pre><code>words = [{'word':'achieve','misspellings':  ['acheive','acheiv','achiev']},

        {'word':'apparently','misspellings':['apparantly','apparintly']}]
</code></pre>

<p>I am looking to go through a list of strings and at the end have a total count of each word and its misspellings. I am having problems on misspellings like achiev which when using <code>if word in string</code> would count mess up the count because achiev in in achieve so the counts would be off.</p>
",Preprocessing of the text & Tokenization,counting word list using dictionary list dictionary containing word misspelling word trying go list string first count occurrence word count occurrence misspelling tried using end giving incorrect count many misspelling contain actual word within possible use python would regex make sense example looking go list string end total count word misspelling problem misspelling like achiev using would count mess count achiev achieve count would
Keep digits in Mallet topic modeling,"<p>I am using Mallet for topic modeling. A large amount of words in my input text include both letters and digits; e.g., A54, D892. I just noticed that Mallet automatically removes the digits and only keeps the letters in the words. I even do not use the --remove-stopwords option when importing my text file. Does anyone know how I can fix this problem.</p>
",Preprocessing of the text & Tokenization,keep digit mallet topic modeling using mallet topic modeling large amount word input text include letter digit e g noticed mallet automatically remove digit keep letter word even use remove stopwords option importing text file doe anyone know fix problem
The &#177; 2 window in Word similarity of NLP,"<p>There is a question illustrate below:</p>

<p>//--------question start---------------------</p>

<p>Consider the following small corpus consisting of three sentences:</p>

<p><code>The judge struck the gavel to silence the court. Buying the cheap saw is false
economy. The nail was driven in when the hammer struck it hard.</code></p>

<p>Use distributional similarity to determine whether the word gavel is more similar in mean-
ing to hammer or saw . To compute distributional similarity you must (1) use bag-of-words
in <strong>a ± 2 window</strong> around the target as features, (2) not alter the context words in any way
(e.g. by stemming or removing stop words) and (3) use the Dice measure to compare
the feature vectors. Make sure to show all stages of your working.</p>

<p>//--------question end---------------------</p>

<p>I don't understand what is a ± 2 window in (1). Would someone explain for me? Thank you guys very much.</p>
",Preprocessing of the text & Tokenization,window word similarity nlp question illustrate question start consider following small corpus consisting three sentence use distributional similarity determine whether word gavel similar mean ing hammer saw compute distributional similarity must use bag word window around target feature alter context word way e g stemming removing stop word use dice measure compare feature vector make sure show stage working question end understand window would someone explain thank guy much
stemming words in python,"<p>I'm using this code to stem words, here is how it works, first there's a list of suffixes, the program checks if the word has the ending same as the one in the list if positive it removes the suffix, however, when I run the code I get this result:  </p>

<pre><code>suffixes = ['ing']
def stem(word):
for suff in suffixes:
    return word[:-len(suff)]

stem ('having')
print (stem)
</code></pre>
",Preprocessing of the text & Tokenization,stemming word python using code stem word work first list suffix program check word ha ending one list positive remove suffix however run code get result
Splitting text based on words in a list,"<p>I have a bunch of text documents from the congressional record of the senate. Speakers are identified by ""Mr/Mrs/Ms. [name in all caps]."" (note the period at the end). Basically there are lines starting with ""Mr. REID."" or some other senator to identify who is speaking. See <a href=""https://www.congress.gov/congressional-record/2011/10/20/senate-section/article/S6874-1"" rel=""nofollow"">here for a full example</a>. </p>

<p>For each document, I want to create a dictionary with keys {speaker: what they said}. If the same speaker is in the text twice, I want to just concatenate each time he spoke. </p>

<p>One idea was to split based on name, using a list of all senator names. Originally I was going to use something with isupper() to find where speaker changes, but this gets thrown of with names like McCONNELL. </p>

<p>Any thoughts?  </p>

<p>Edit: To give a small example, consider this toy example </p>

<pre><code>Mr. REID. Mr. President, thanks for whatever whatever

Mr. McCONNELL. I disagree yadda yaddaa yadaa

Mr. REID. I don't care 'bout yo Mitch

Mr. PAUL. Guyssssss 
</code></pre>

<p>This should give a dictionary: </p>

<pre><code>{""REID"": ""Mr. President, thanks for whatever whatever I don't care 'bout yo Mitch"",
 ""McCONNEL"": ""I disagree yadda yaddaa yadaa"",
 ""PAUL"": ""Guyssssss""}
</code></pre>
",Preprocessing of the text & Tokenization,splitting text based word list bunch text document congressional record senate speaker identified mr mr name cap note period end basically line starting mr reid senator identify speaking see full example document want create dictionary key speaker said speaker text twice want concatenate time spoke one idea wa split based name using list senator name originally wa going use something isupper find speaker change get thrown name like mcconnell thought edit give small example consider toy example give dictionary
Python : How to optimize comparison between two large sets?,"<p>I salute you ! I'm new here, and I've got a little problem trying to optimize this part of code.</p>

<p>I'm reading from two files :</p>

<p>Corpus.txt -----> Contains my text (of 1.000.000 words)</p>

<p>Stop_words.txt -----> Contains my stop_list (of 4000 words)</p>

<p>I must compare each word from my corpus with every word in the stop_list, because I want to have a text without stop words, so I've :
1.000.000*4000 comparisons to do with the code below : </p>

<pre><code>fich= open(""Corpus.txt"", ""r"")
text = fich.readlines()

fich1= open(""stop_words.txt"", ""r"")
stop = fich1.read()

tokens_stop = nltk.wordpunct_tokenize(stop)
tokens_stop=sorted(set(tokens_stop))

for line in text :
    tokens_rm = nltk.wordpunct_tokenize(line)
    z = [val for val in tokens_rm if val not in tokens_stop]
    for i in z:
        print i
</code></pre>

<p>My question is : Is there anything to do it differently ? Any structure to optimize it ? </p>
",Preprocessing of the text & Tokenization,python optimize comparison two large set salute new got little problem trying optimize part code reading two file corpus txt contains text word stop word txt contains stop list word must compare word corpus every word stop list want text without stop word comparison code question anything differently structure optimize
Only ignore stop words for ngram_range=1,"<p>I am using CountVectorizer from sklearn...looking to provide a list of stop words and apply the count vectorizer for ngram_range of (1,3). </p>

<p>From what I can tell, if a word - say ""me"" - is in the list of stop words, then it doesn't get seen for higher ngrams i.e., ""tell me"" would not be a feature. Is there a way that I can specify something like, ""consider stop words only when ngram is 1""?</p>
",Preprocessing of the text & Tokenization,ignore stop word ngram range using countvectorizer sklearn looking provide list stop word apply count vectorizer ngram range tell word say list stop word get seen higher ngrams e tell would feature way specify something like consider stop word ngram
Python convert list of multiple words to single words,"<p>I have a list of words for example:</p>

<p><code>words = ['one','two','three four','five','six seven']</code> # quote was missing</p>

<p>And I am trying to create a new list where each item in the list is just one word so I would have:</p>

<p><code>words = ['one','two','three','four','five','six','seven']</code></p>

<p>Would the best thing to do be join the entire list into a string and then tokenize the string? Something like this:</p>

<p><code>word_string = ' '.join(words)
tokenize_list = nltk.tokenize(word_string)</code></p>

<p>Or is there a better option?</p>
",Preprocessing of the text & Tokenization,python convert list multiple word single word list word example quote wa missing trying create new list item list one word would would best thing join entire list string tokenize string something like better option
Format an entire text with pattern.en?,"<p>I need to analyse some texts for a machine learning purpose. A data scientist I know advised me to use <a href=""http://www.clips.ua.ac.be/pages/pattern-en"" rel=""nofollow"">pattern.en</a> for my project.</p>

<p>I will give my program a keyword (<strong>Example</strong> : pizza), and it has to sort some ""trends"" from several texts I give him. (<strong>Example</strong> : I give him texts which talk about nutella on pizzas, so the program would identify that nutella is a growing trend.)</p>

<p>So for the start, I have to ""clean"" the text. I know that <em>pattern.en</em> can identify words as a noun, verb, adverb etc., I want to remove all determiners, articles and other ""meaningless"" words for my analyses, but I don't know how to do that. I tried to <code>parse()</code> so I can get :</p>

<pre><code>s = ""Hello, how is it going ? I am tired actually, did not sleep enough... That is bad for work, definitely""
parsedS = parse(s)
print(parsedS)
</code></pre>

<p>That outputs :</p>

<pre><code>Hello/UH/hello ,/,/, how/WRB/how is/VBZ/be it/PRP/it going/VBG/go ?/./?
I/PRP/i am/VBP/be tired/VBN/tire actually/RB/actually ,/,/, did/VBD/do not/RB/not sleep/VB/sleep enough/RB/enough .../:/...
That/DT/that is/VBZ/be bad/JJ/bad for/IN/for work/NN/work ,/,/, definitely/RB/definitely
</code></pre>

<p>So I'd like to remove words with tags ""UH"", "","", ""PRP"" etc. but I don't know how to do it, and without messing with the sentences (For analyzing purposes, I will ignore sentences without the word ""pizza"" in my <strong>Example</strong>)</p>

<p>I don't know if I am very clear in my explanations, feel free to ask me if you didn't understand something.</p>

<p><strong>EDIT - UPDATE :</strong> After <em>canyon289</em>'s answer, I would like to do it sentence by sentence, not for the entire text. I tried :</p>

<pre><code>for sentence in Text(s):
    sentence = sentence.split("" "")
    print(""SENTENCE :"")
    for word in sentence:
        if not any(tag in word for tag in dont_want):
            print(word)
</code></pre>

<p>But I have the following error :</p>

<pre><code>AttributeError: 'Sentence' object has no attribute 'split'
</code></pre>

<p>How can I solve the problem ?</p>
",Preprocessing of the text & Tokenization,format entire text pattern en need analyse text machine learning purpose data scientist know advised use pattern en project give program keyword example pizza ha sort trend several text give example give text talk nutella pizza program would identify nutella growing trend start clean text know pattern en identify word noun verb adverb etc want remove determiner article meaningless word analysis know tried get output like remove word tag uh prp etc know without messing sentence analyzing purpose ignore sentence without word pizza example know clear explanation feel free ask understand something edit update canyon answer would like sentence sentence entire text tried following error solve problem
Simple tokenization issue in NTLK,"<p>I want to tokenize the following text:</p>

<pre><code>In Düsseldorf I took my hat off. But I can't put it back on.


'In', 'Düsseldorf', 'I', 'took', 'my', 'hat', 'off', '.', 'But', 'I', 
'can't', 'put', 'it', 'back', 'on', '.'
</code></pre>

<p>But to my surprise none of the <a href=""http://text-processing.com/demo/tokenize/"" rel=""nofollow"">NLTK tokenizers work</a>. How can I accomplish did? Is it possible to use a combination of these tokenizers somehow to achieve the above?</p>
",Preprocessing of the text & Tokenization,simple tokenization issue ntlk want tokenize following text surprise none nltk tokenizers work accomplish possible use combination tokenizers somehow achieve
SnowballC in R stems &quot;many&quot; and &quot;only&quot;,"<p>I am using SnowballC to process a text document, but realize it stems words such as ""many"" and ""only"" even though they are not supposed to be stemmed.</p>

<pre><code>&gt; library(SnowballC)
&gt; 
&gt; str &lt;- c(""many"", ""only"", ""things"")
&gt; str.stemmed &lt;- stemDocument(str)
&gt; str.stemmed
[1] ""mani""  ""onli""  ""thing""
&gt; 
&gt; dic &lt;- c(""many"", ""only"", ""online"", ""things"")
&gt; str.complete &lt;- stemCompletion(str.stemmed, dic)
&gt; str.complete
    mani     onli    thing 
      """" ""online"" ""things"" 
</code></pre>

<p>You can see that after stemming, ""many"" and ""only"" became ""mani"" and ""onli"", which cannot be completed back with stemCompletion later on, since letters in ""many"" is not inclusive of ""mani"". Notice how ""onli"" gets completed to ""online"" instead of the original ""only"".</p>

<p>Why is that? Is that a way to fix this? </p>
",Preprocessing of the text & Tokenization,snowballc r stem many using snowballc process text document realize stem word many even though supposed stemmed see stemming many became mani onli completed back stemcompletion later since letter many inclusive mani notice onli get completed online instead original way fix
How can Topic Modeling noise be removed?,"<p>I am working on Topic Modeling where the given text corpus have lots of noise in form of supporting words after removal of stop words. These words have high term frequency but does not help in forming topic terms by using LDA along with other words with high frequency that are useful . How can this noise be removed?</p>
",Preprocessing of the text & Tokenization,topic modeling noise removed working topic modeling given text corpus lot noise form supporting word removal stop word word high term frequency doe help forming topic term using lda along word high frequency useful noise removed
CLUTO doc2mat specified stop word list not working,"<p>I am trying to convert my documents into vector-space format using <a href=""http://glaros.dtc.umn.edu/gkhome/files/fs/sw/cluto/doc2mat.html"" rel=""nofollow"" title=""doc2mat"">doc2mat</a></p>

<p>On the website, it says I can use my specified text file where words are white-space separated or on multiple lines. So, I use some code similar to this one:</p>

<p><code>./doc2mat -mystoplist=stopword.txt -skipnumeric mydocuments.txt myvectorspace.txt</code></p>

<p>However, when I check the output <code>.clabel</code> file, it still has stop words that's in <code>stopword.txt</code>.</p>

<p>I really do not know how to do this. Someone help me out please? Thank you!</p>
",Preprocessing of the text & Tokenization,cluto doc mat specified stop word list working trying convert document vector space format using doc mat website say use specified text file word white space separated multiple line use code similar one however check output file still ha stop word really know someone help please thank
How to output NLTK chunks to file?,"<p>I have this python script where I am using nltk library to parse,tokenize,tag and chunk some lets say random text from the web.</p>

<p>I need to format and write in a file the output of <code>chunked1</code>,<code>chunked2</code>,<code>chunked3</code>. These have type <code>class 'nltk.tree.Tree'</code></p>

<p>More specifically I need to write only the lines that match the regular expressions <code>chunkGram1</code>, <code>chunkGram2</code>, <code>chunkGram3</code>.</p>

<p>How can i do that?</p>

<pre><code>#! /usr/bin/python2.7

import nltk
import re
import codecs

xstring = [""An electronic library (also referred to as digital library or digital repository) is a focused collection of digital objects that can include text, visual material, audio material, video material, stored as electronic media formats (as opposed to print, micro form, or other media), along with means for organizing, storing, and retrieving the files and media contained in the library collection. Digital libraries can vary immensely in size and scope, and can be maintained by individuals, organizations, or affiliated with established physical library buildings or institutions, or with academic institutions.[1] The electronic content may be stored locally, or accessed remotely via computer networks. An electronic library is a type of information retrieval system.""]


def processLanguage():
    for item in xstring:
        tokenized = nltk.word_tokenize(item)
        tagged = nltk.pos_tag(tokenized)
        #print tokenized
        #print tagged

        chunkGram1 = r""""""Chunk: {&lt;JJ\w?&gt;*&lt;NN&gt;}""""""
        chunkGram2 = r""""""Chunk: {&lt;JJ\w?&gt;*&lt;NNS&gt;}""""""
        chunkGram3 = r""""""Chunk: {&lt;NNP\w?&gt;*&lt;NNS&gt;}""""""

        chunkParser1 = nltk.RegexpParser(chunkGram1)
        chunked1 = chunkParser1.parse(tagged)

        chunkParser2 = nltk.RegexpParser(chunkGram2)
        chunked2 = chunkParser2.parse(tagged)

        chunkParser3 = nltk.RegexpParser(chunkGram3)
        chunked3 = chunkParser2.parse(tagged)

        #print chunked1
        #print chunked2
        #print chunked3

        # with codecs.open('path\to\file\output.txt', 'w', encoding='utf8') as outfile:

            # for i,line in enumerate(chunked1):
                # if ""JJ"" in line:
                    # outfile.write(line)
                # elif ""NNP"" in line:
                    # outfile.write(line)



processLanguage()
</code></pre>

<p>For the time being when I am trying to run it I get error:</p>

<pre><code>`Traceback (most recent call last):
  File ""sentdex.py"", line 47, in &lt;module&gt;
    processLanguage()
  File ""sentdex.py"", line 40, in processLanguage
    outfile.write(line)
  File ""C:\Python27\lib\codecs.py"", line 688, in write
    return self.writer.write(data)
  File ""C:\Python27\lib\codecs.py"", line 351, in write
    data, consumed = self.encode(object, self.errors)
TypeError: coercing to Unicode: need string or buffer, tuple found`
</code></pre>

<p><strong>edit:</strong> After @Alvas answer I managed to do what I wanted. However now, I would like to know how I could strip all non-ascii characters from a text <em>corpus</em>. example:</p>

<pre><code>#store cleaned file into variable
with open('path\to\file.txt', 'r') as infile:
    xstring = infile.readlines()
infile.close

    def remove_non_ascii(line):
        return ''.join([i if ord(i) &lt; 128 else ' ' for i in line])

    for i, line in enumerate(xstring):
        line = remove_non_ascii(line)

#tokenize and tag text
def processLanguage():
    for item in xstring:
        tokenized = nltk.word_tokenize(item)
        tagged = nltk.pos_tag(tokenized)
        print tokenized
        print tagged
processLanguage()
</code></pre>

<p>This above is taken from another answer here in S/O. However it doesn't seem to work. What might be wrong? The error I am getting is:</p>

<pre><code>UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position
not in range(128)
</code></pre>
",Preprocessing of the text & Tokenization,output nltk chunk file python script using nltk library parse tokenize tag chunk let say random text web need format write file output type specifically need write line match regular expression time trying run get error edit alvas answer managed wanted however would like know could strip non ascii character text corpus example taken another answer however seem work might wrong error getting
clustering semantically related words from a list of words,"<p>I have a word list containing about 30000 <strong>unique</strong> words.<Br>
I would like to group this list of words based on how similar these words tend to be.
Can I create a ontology tree using this list and with possibly with the help of WordNet?</p>

<p>So essentially what I want to do is aggregate these words in some meaningful way to reduce the size of the list.<br> What kind of techniques can I use to do this?</p>
",Preprocessing of the text & Tokenization,clustering semantically related word list word word list containing unique word would like group list word based similar word tend create ontology tree using list possibly help wordnet essentially want aggregate word meaningful way reduce size list kind technique use
Most light-way library to stem and lemmatise a word in java,"<p>I have a simple project that needs to achieve this kind of things.</p>

<pre><code>Sports - &gt; Sport
Walking -&gt; Walk

and ideally also do things like:

good -&gt; better
better -&gt; good 
person -&gt; people
people -&gt; person
</code></pre>

<p>Could someone point me to the most light-way library that can achieve this? (I know there is lib like Lucene, CoreNLP etc..) but these are quite HEAVY and I really just need a stemmer / lemmatiser</p>

<p>Thank you!</p>
",Preprocessing of the text & Tokenization,light way library stem lemmatise word java simple project need achieve kind thing could someone point light way library achieve know lib like lucene corenlp etc quite heavy really need stemmer lemmatiser thank
what is the most efficient way of removing stop words from huge text corpus ?,"<p>i want to know the efficient way to remove the stop words from huge text corpus. 
currently my approach is to convert stopword in to regex match the lines of text with regex and remove it. </p>

<p>e.g </p>

<pre><code>String regex =""\\b(?:a|an|the|was|i)\\b\\s*"";
 String line = ""hi this is regex approach of stop word removal"";
 String lineWithoutStopword = line.replaceAll(regex,"""");
</code></pre>

<p>Is there and other efficient approach present to remove stopwords from huge corupus.</p>

<p>thanks </p>
",Preprocessing of the text & Tokenization,efficient way removing stop word huge text corpus want know efficient way remove stop word huge text corpus currently approach convert stopword regex match line text regex remove e g efficient approach present remove stopwords huge corupus thanks
A development platform for unicode spell checker?,"<p>I have decided to develop a (Unicode) spell checker for my final year project for a south Asian language. I want to develop it as a plugin or a web service. But I need to decide a suitable development platform for it. (This will not just check for a dictionary file, morphological analysis / generation modules (a stemmer) will also be used).</p>

<p>Would java script be able to handle such processing with a fair response time? </p>

<p>Will I be able to process a large dictionary on client side?  </p>

<p>Is there any better suggestions that you can make?</p>
",Preprocessing of the text & Tokenization,development platform unicode spell checker decided develop unicode spell checker final year project south asian language want develop plugin web service need decide suitable development platform check dictionary file morphological analysis generation module stemmer also used would java script able handle processing fair response time able process large dictionary client side better suggestion make
removing custom stop words form a phrase in python,"<p>I am trying to remove certain phrases and words form a user input before I further process the input and while trying to do that I'm running into a problem of getting an ""index out of range"" error and am completely stuck. How do I solve this? 
I get my input phrase as a string which I convert to a list to compare every word and I have my stop words as a predefined list.<br>
Example inputs:<br>
[""well"",""you"",""know"",""the"",""weather"",""is"",""awful""]<br>
[""you"", ""know"", ""what"", ""i"", ""mean"", ""so"", ""just"", ""turn"", ""the"", ""lights"", ""on""]</p>

<pre><code>#Gets user input and removes the selected stop words from it and returns a filtered phrase back.    
def stop_word_remover(phrase_list):

    stop_words_lst = [""yo"", ""so"", ""well"", ""um"", ""a"", ""the"",""you know"", ""i mean""]

    #initalize clean phrase string
    clean_input_phrase= """"

    #copying phrase_list into a new variable for stopword removal.
    Copy_phrase_list = list(phrase_list)

    #Cleanup loop

    for i in range(1,len(phrase_list)):
        has_stop_words = False

        for x in range(len(stop_words_lst)):
            has_stop_words = False

            #if one of the stop words matches the word passed by the first main loop      the  flag is raised.
            if (phrase_list[i-1]+"" ""+phrase_list[i]) == stop_words_lst[x].strip():
                has_stop_words = True    

            # this if statement adds the word of the phrase only if the flag is not raised thus making sure all the stop words are filtered out         
            if has_stop_words == True:
                Copy_phrase_list.remove(Copy_phrase_list[i-1])
                Copy_phrase_list.remove(Copy_phrase_list[i-1])

    #first for loop takes a individual words of the phrase given and makes a loop until the whole phrase goes through one word at a time
    for i in range(len(Copy_phrase_list)):
        #flag initialized for marking stop words
        has_stop_words = False

        #second loop takes all the stop words and compares them to the first word passed on by the first loop to sheck for a stop word
        for x in range(len(stop_words_lst)):
            #if one of the stop words matches the word passed by the first main loop the  flag is raised.
            if Copy_phrase_list[i] == stop_words_lst[x].strip():
            has_stop_words = True    

        # this if statement adds the word of the phrase only if the flag is not raised thus making sure all the stop words are filtered out        
        if has_stop_words == False:
            clean_input_phrase += str(Copy_phrase_list[i]) +"" ""


return clean_input_phrase
</code></pre>
",Preprocessing of the text & Tokenization,removing custom stop word form phrase python trying remove certain phrase word form user input process input trying running problem getting index range error completely stuck solve get input phrase string convert list compare every word stop word predefined list example input well know weather awful know mean turn light
Regular Expression to limit a string to the shortest match versus the longest match (non-greedy group)?,"<p>I'm searching within paragraphs of text.</p>

<p>I'd like to find strings within those paragraphs that start with a specific word, and then grab the text that immediately follows that matching word. I'd like to stop when encountering the first period, exclamation mark, question mark, or new line ... If none of these are found within 100 characters of the search word, I'd like to cut the string off at the word boundary closest to the 100 character limit.</p>

<p>How can I do this?</p>

<p><strong>EXAMPLE</strong></p>

<pre><code>string: ""A test sentence containing an ngram and ending with a period. Another sentence that does not have the word we're searching for and runs on until we're past 100 characters.""

regex: /\bngram(.{0,100})(\.|\b)/i

desired output: ' and ending with a period'
</code></pre>

<p>In this case, my regex returns "" and ending with a period. Another sentence that does not have the word we're searching for and runs."" It goes on longer than I wanted because it's the period/word-boundary capture group is greedy (maybe?). I don't know how to limit to the shorter match, versus the longest match.</p>
",Preprocessing of the text & Tokenization,regular expression limit string shortest match versus longest match non greedy group searching within paragraph text like find string within paragraph start specific word grab text immediately follows matching word like stop encountering first period exclamation mark question mark new line none found within character search word like cut string word boundary closest character limit example case regex return ending period another sentence doe word searching run go longer wanted period word boundary capture group greedy maybe know limit shorter match versus longest match
How to intelligently remove similar parts out of a set of strings?,"<p>For example, this is what I want to input:</p>

<pre><code>input = [
   '&lt;html&gt;&lt;head&gt;&lt;title&gt;Albert Einstein - Minipedia&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;b&gt;Welcome to Minipedia! You are viewing page 1&lt;/b&gt; Albert Einstein was a scientist&lt;/body&gt;&lt;/html&gt;',
   '&lt;html&gt;&lt;head&gt;&lt;title&gt;Ludwig Van Beethoven - Minipedia&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;b&gt;Welcome to Minipedia! You are viewing page 2&lt;/b&gt; Ludwig van Beethoven was a Musician&lt;/body&gt;&lt;/html&gt;',
   '&lt;html&gt;&lt;head&gt;&lt;title&gt;Red - Minipedia&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;b&gt;Welcome to Minipedia! You are viewing page 3&lt;/b&gt; Red is a color.&lt;/body&gt;&lt;/html&gt;'
]
</code></pre>

<p>and what I'm looking for as output is:</p>

<pre><code>output = [
    ['Albert Einstein', 'Albert Einstein was a scientist'],
    ['Ludwig Van Beethoven', 'Ludwig Van Beethoven was a musician'],
    ['Red', 'Red is a color']
]
</code></pre>

<p>The logic I'm looking for approximately that if there are substrings of each document that have significant overlap (i.e. small enough edit distance), they should be taken out and used to tokenize the remaining strings that have sufficient difference.</p>

<p>Are there any libraries for this?</p>
",Preprocessing of the text & Tokenization,intelligently remove similar part set string example want input looking output logic looking approximately substring document significant overlap e small enough edit distance taken used tokenize remaining string sufficient difference library
How can I compare a word against a list of chosen words to find the word that correlates the strongest?,"<p>I am looking to design a search box that will take any input and return the most appropriate output from a chosen list of outputs.</p>

<p>As an example, my chosen list of outputs are <code>animal</code>,<code>vehicle</code> and <code>place</code>.</p>

<p>If the user searches for <code>cat</code>, I would like the code to run <code>cat</code> vs <code>animal</code>,<code>vehicle</code> and <code>place</code>. A correlation/matching score will be determined for each. With <code>animal</code> generating the highest correlation. The output will then be <code>animal</code>.</p>

<p>Similarly, typing in <code>car</code>, will output <code>vehicle</code> from the list.</p>

<p>Any ideas on what is the best way to generate this correlation score?
My output list consists of 100 different terms. </p>
",Preprocessing of the text & Tokenization,compare word list chosen word find word correlate strongest looking design search box take input return appropriate output chosen list output example chosen list output user search would like code run v correlation matching score determined generating highest correlation output similarly typing output list idea best way generate correlation score output list consists different term
Diminutive words stemming / lemmatization,"<p>Currently I use 'lucene' and 'elasticsearch', and have next problem.
I need get stemmed form or lemma for <a href=""http://en.wikipedia.org/wiki/Diminutive"">diminutive</a> word. For instance :</p>

<ul>
<li><em>doggy -> dog</em> </li>
<li><em>kitty -> cat</em></li>
</ul>

<p>etc.</p>

<p>But I get next results :</p>

<ul>
<li><em>doggy -> doggi</em> </li>
<li><em>kitty -> kitti</em></li>
</ul>

<p>Is there any way (not important ready to use library, any algorithm, approach etc.) to get root / original word form for <a href=""http://en.wikipedia.org/wiki/Diminutive"">diminutive</a> word forms?</p>

<p>Target language : Russian.
For example :</p>

<ul>
<li><em>собачка -> собака</em></li>
<li><em>кошечка -> кошка</em></li>
</ul>

<p>Thanks in advance!</p>
",Preprocessing of the text & Tokenization,diminutive word stemming lemmatization currently use lucene elasticsearch next problem need get stemmed form lemma target language russian example thanks advance
"nltk sentence tokenizer, consider new lines as sentence boundary","<p>I am using nltk's <code>PunkSentenceTokenizer</code> to tokenize a text to a set of sentences. However, the tokenizer doesn't seem to consider new paragraph or new lines as a new sentence.</p>

<pre><code>&gt;&gt;&gt; from nltk.tokenize.punkt import PunktSentenceTokenizer
&gt;&gt;&gt; tokenizer = PunktSentenceTokenizer()
&gt;&gt;&gt; tokenizer.tokenize('Sentence 1 \n Sentence 2. Sentence 3.')
['Sentence 1 \n Sentence 2.', 'Sentence 3.']
&gt;&gt;&gt; tokenizer.span_tokenize('Sentence 1 \n Sentence 2. Sentence 3.')
[(0, 24), (25, 36)]
</code></pre>

<p>I would like it to to consider new lines as a boundary of sentences as well. Anyway to do this (I need to save the offsets too)?</p>
",Preprocessing of the text & Tokenization,nltk sentence tokenizer consider new line sentence boundary using nltk tokenize text set sentence however tokenizer seem consider new paragraph new line new sentence would like consider new line boundary sentence well anyway need save offset
NLP - Error while Tokenization and Tagging etc,"<p>I want to identify all the Tokens and also PartsOfSpeech Tagging using the Stanford NLP jar file. I have added all the required jar files into the build path of the project..The error which I am getting is..</p>

<pre><code>Exception in thread ""main"" java.lang.UnsupportedClassVersionError: edu/stanford/nlp/pipeline/StanfordCoreNLP : Unsupported major.minor version 52.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(Unknown Source)
    at java.security.SecureClassLoader.defineClass(Unknown Source)
    at java.net.URLClassLoader.defineClass(Unknown Source)
    at java.net.URLClassLoader.access$100(Unknown Source)
    at java.net.URLClassLoader$1.run(Unknown Source)
    at java.net.URLClassLoader$1.run(Unknown Source)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at Test.testing(Test.java:19)
    at mainFunction.main(mainFunction.java:29)
</code></pre>
",Preprocessing of the text & Tokenization,nlp error tokenization tagging etc want identify token also partsofspeech tagging using stanford nlp jar file added required jar file build path project error getting
Lucene 5.0 UnsupportedOperationException,"<p>I use Lucene 5.0 and I want to add my own stopwords. </p>

<pre><code>    CharArraySet stopSet = StandardAnalyzer.STOP_WORDS_SET;
    stopSet.add(""i"");
    stopSet.add(""m"");
    stopSet.add(""t"");
    stopSet.add(""can"");
    stopSet.add(""said"");
    tokenStream = new StopFilter(tokenStream, stopSet);
</code></pre>

<p>And this found in <a href=""https://stackoverflow.com/questions/18008999/how-to-add-custom-stop-words-using-lucene-in-java"">how to add custom stop words using lucene in java</a>. But it does not work in my work. The problem is as follows.</p>

<pre><code>Exception in thread ""main"" java.lang.UnsupportedOperationException
at org.apache.lucene.analysis.util.CharArrayMap$UnmodifiableCharArrayMap.put(CharArrayMap.java:606)
at org.apache.lucene.analysis.util.CharArraySet.add(CharArraySet.java:115)
at Main.main(Main.java:23)
</code></pre>

<p>How could I solve it. Thanks very much. </p>
",Preprocessing of the text & Tokenization,lucene unsupportedoperationexception use lucene want add stopwords found href add custom stop word using lucene java doe work work problem follows could solve thanks much
how to handle floating point values going beyond the range in Python?,"<p>I am writing a unigram naive bayes classifier in Python. I am calculating probabilities and multiplying those. Such cumulative multiplication is reducing the value to 0 after reaching some number. I am trying to get rid of extremely low values by removing stop words or not considering the denominator of prior in Naive Bayes classifier. 
I don't however, know how to make approximation or handle floating point values beyond the lower limit.Following is a snippet of code </p>

<pre><code>for word in para:
            probofwordgivenclass=int(likelihood[key][word])
            if(probofwordgivenclass&gt;10):
                mult=(float(probofwordgivenclass+1)/float(totalwordcount+vocabularsize))
                prob= prob*mult 
</code></pre>

<p>In above code prob is cumulative probability. It reduces to 0 at the end. What approximations would you suggest? or how to handle it in Python?</p>
",Preprocessing of the text & Tokenization,handle floating point value going beyond range python writing unigram naive bayes classifier python calculating probability multiplying cumulative multiplication reducing value reaching number trying get rid extremely low value removing stop word considering denominator prior naive bayes classifier however know make approximation handle floating point value beyond lower limit following snippet code code prob cumulative probability reduces end approximation would suggest handle python
"How can i determine the head word in a Sentence, using NLP?","<p>For example, if I have been given a  sentence:</p>

<blockquote>
  <p>A British soldier was killed in the fighting in Afghanistan</p>
</blockquote>

<p>The head word of that sentence is ""killed"".</p>

<p>How can I find it, given the nltk package in Python? I am not talking about stemming, I refer to the head word.</p>
",Preprocessing of the text & Tokenization,determine head word sentence using nlp example given sentence british soldier wa killed fighting afghanistan head word sentence killed find given nltk package python talking stemming refer head word
Base word stemming instead of root word stemming in R,"<p>Is there any way to get base word instead of root word in stemming using NLP in R? </p>

<p>Code:</p>

<pre><code>&gt; #Loading libraries
&gt; library(tm)
&gt; library(slam)
&gt; 
&gt; #Vector
&gt; Vec=c(""happyness happies happys"",""sky skies"")
&gt; 
&gt; #Creating Corpus
&gt; Txt=Corpus(VectorSource(Vec))
&gt; 
&gt; #Stemming
&gt; Txt=tm_map(Txt, stemDocument)
&gt; 
&gt; #Checking result
&gt; inspect(Txt)
A corpus with 2 text documents

The metadata consists of 2 tag-value pairs and a data frame
Available tags are:
  create_date creator 
Available variables in the data frame are:
  MetaID 

[[1]]
happi happi happi

[[2]]
sky sky

&gt; 
</code></pre>

<p>Can I get base word ""happy"" (base word) instead of ""happi"" (root word) for ""happyness happies happys"" using R.</p>
",Preprocessing of the text & Tokenization,base word stemming instead root word stemming r way get base word instead root word stemming using nlp r code get base word happy base word instead happi root word happyness happies happys using r
R - Tokenization - single and two letter words in a TermDocumentMatrix,"<p>I am currently trying to do a little bit of text processing and I would like to get the one and two letter words in a TermDocumentMatrix.</p>

<p>The issue is that it seems to display only 3 letter words and more.</p>

<pre><code>    library(tm)
    library(RWeka)

    test&lt;-'This is a test.'

    testmyCorpus&lt;-Corpus(VectorSource(test))
    testTDF&lt;-TermDocumentMatrix(testmyCorpus, control=list(tokenize=AlphabeticTokenizer))
    inspect(testTDF)
</code></pre>

<p>Only the words ""this"" and ""test"" are displayed. Any ideas?</p>

<p>Thanks a lot for you help!
Robert</p>
",Preprocessing of the text & Tokenization,r tokenization single two letter word termdocumentmatrix currently trying little bit text processing would like get one two letter word termdocumentmatrix issue seems display letter word word test displayed idea thanks lot help robert
Sentiment Analysis - should I normalise positive and negative word lists when they differ in length?,"<p>I'm implementing a simple sentiment analysis algorithm where the authors of the paper have a word list for positive and negative words and simply count the number of occurrences of each in the analysed document and give it a sentiment score the document with:</p>

<p>sentiment = (#positive_matches - #negative_matches) / (document_word_count)</p>

<p>This is normalising the sentiment score by document length BUT the corpus of negative words is 6 times larger than the positive word corpus (around 300 positive words and 1800 negative words) so by the measure above, the sentiment score will likely be negatively biased since there are more negative words to match than positive words.</p>

<p>How can I correct for the imbalance in the length of the positive vs. negative corpuses?</p>

<p>Should I adjust the sentiment score to normalize each positive/negative count by the respective corpus lengths such that:</p>

<p>sentiment* = ((#positive_matches/#words_in_positive_corpus) - (#negative_matches/#words_in_negative_corpus)) / (document_word_count)</p>

<p>Any thoughts / sanity check / advice much appreciated :)</p>
",Preprocessing of the text & Tokenization,sentiment analysis normalise positive negative word list differ length implementing simple sentiment analysis algorithm author paper word list positive negative word simply count number occurrence analysed document give sentiment score document sentiment positive match negative match document word count normalising sentiment score document length corpus negative word time larger positive word corpus around positive word negative word measure sentiment score likely negatively biased since negative word match positive word correct imbalance length positive v negative corpus adjust sentiment score normalize positive negative count respective corpus length sentiment positive match word positive corpus negative match word negative corpus document word count thought sanity check advice much appreciated
Sequence of vowels count,"<p>This is not a homework question, it is an exam preparation question.</p>

<p>I should deﬁne a function <code>syllables(word)</code> that counts the number of syllables in
A word in the following way:</p>

<p>• a maximal sequence of vowels is a syllable;</p>

<p>• a ﬁnal <strong>e</strong> in a word is not a syllable (or the vowel sequence it is a part
Of).</p>

<p>I do not have to deal with any special cases, such as a ﬁnal <strong>e</strong> in a
One-syllable word (e.g., ’be’ or ’bee’).</p>

<pre><code>&gt;&gt;&gt; syllables(’honour’)
2
&gt;&gt;&gt; syllables(’decode’)
2
&gt;&gt;&gt; syllables(’oiseau’)
2
</code></pre>

<p>Should I use regular expression here or just list comprehension ?</p>
",Preprocessing of the text & Tokenization,sequence vowel count homework question exam preparation question de ne function count number syllable word following way maximal sequence vowel syllable nal e word syllable vowel sequence part deal special case nal e one syllable word e g bee use regular expression list comprehension
NLP: How to correctly normalise a feature for gender classification?,"<p><strong>NOTE</strong> Before I begin, this F-measure is not related to precision and recall, and its title and definition is taken from this <a href=""http://pespmc1.vub.ac.be/papers/contextuality-fos.pdf"" rel=""nofollow"">paper</a>. </p>

<p>I have a feature known as the F-measure, which is used to measure formality in a given text. It is mostly used in gender classification of text which is what I'm working on as a project. </p>

<p>The <strong>F-measure</strong> is defined as:</p>

<p>F = 0.5 * (noun freq. + adjective freq. + preposition freq. + article freq. – pronoun
freq. – verb freq. – adverb freq. – interjection freq. + 100)</p>

<p>where the frequencies are taken from a given text (for example, a blog post).</p>

<p>I would like to normalize this feature for use in a classification task. Initially, my first thought was that since the value <em>F</em> is bound by the number of words in the given text (text_length), I thought of first taking <em>F</em> and dividing by text_length. Secondly, and finally, since this measure can take on both positive and negative values (as can be inferred from the equation) I then thought of squaring (<em>F</em>/text_length) to only get a positive value.</p>

<p>Trying this I found that the normalised values did not seem to be too correct as I started getting really small values in (below 0.10) for all the cases I tested the feature with and I am thinking that the reason might be because I am squaring the value which would essentially make it smaller since its the square of a fraction. However this is required if I want to guarantee positive values only. I am not sure what else to consider to improve the normalisation such that a nice distribution within [0,1] is produced, and would like to know if there is some kind of strategy involved to correctly normalise NLP features.</p>

<p>How should I approach the normalisation of my feature, and what might I be doing wrong?</p>
",Preprocessing of the text & Tokenization,nlp correctly normalise feature gender classification note begin f measure related precision recall title definition taken paper feature known f measure used measure formality given text mostly used gender classification text working project f measure defined f noun freq adjective freq preposition freq article freq pronoun freq verb freq adverb freq interjection freq frequency taken given text example blog post would like normalize feature use classification task initially first thought wa since value f bound number word given text text length thought first taking f dividing text length secondly finally since measure take positive negative value inferred equation thought squaring f text length get positive value trying found normalised value seem correct started getting really small value case tested feature thinking reason might squaring value would essentially make smaller since square fraction however required want guarantee positive value sure else consider improve normalisation nice distribution within produced would like know kind strategy involved correctly normalise nlp feature approach normalisation feature might wrong
Preserve punctuation characters when using Lucene&#39;s StandardTokenizer,"<p>I'm thinking of leveraging Lucene's <a href=""https://lucene.apache.org/core/4_0_0/analyzers-common/org/apache/lucene/analysis/standard/StandardTokenizer.html"" rel=""nofollow"">StandardTokenizer</a> for word tokenization in a non-IR context.</p>

<p>I understand that this tokenizer removes punctuation characters. Would anybody know (or happen to have experience with) making it output punctuation characters as separate tokens?</p>

<p>Example of current behaviour:</p>

<pre><code>Welcome, Dr. Chasuble! =&gt; Welcome Dr. Chasuble
</code></pre>

<p>Example of desired behaviour:</p>

<pre><code>Welcome, Dr. Chasuble! =&gt; Welcome , Dr. Chasuble !
</code></pre>
",Preprocessing of the text & Tokenization,preserve punctuation character using lucene standardtokenizer thinking leveraging lucene standardtokenizer word tokenization non ir context understand tokenizer remove punctuation character would anybody know happen experience making output punctuation character separate token example current behaviour example desired behaviour
Is it possible to get a natural word after it has been stemmed?,"<p>I have a word <strong><em>play</em></strong> which after stemming has become <strong><em>plai</em></strong>. Now I want to get <strong><em>play</em></strong> again. Is it possible? I have used Porter's Stemmer.</p>
",Preprocessing of the text & Tokenization,possible get natural word ha stemmed word play stemming ha become plai want get play possible used porter stemmer
I am working on text analytics. Please suggest how to proceed with UIMA?,"<p>I download UIMA for ubuntu 12.4 and perform tokenization. Now I want to preform lemmatisation on some text using UIMA.  Please suggest proper approach.</p>
",Preprocessing of the text & Tokenization,working text analytics please suggest proceed uima download uima ubuntu perform tokenization want preform lemmatisation text using uima please suggest proper approach
I am working on text analytics. Please suggest how to proceed with UIMA?,"<p>I download UIMA for ubuntu 12.4 and perform tokenization. Now I want to preform lemmatisation on some text using UIMA.  Please suggest proper approach.</p>
",Preprocessing of the text & Tokenization,working text analytics please suggest proceed uima download uima ubuntu perform tokenization want preform lemmatisation text using uima please suggest proper approach
Built-in Porter Stemmer in java opennlp toolkit,"<p>Like there are built in Stemmers like porter Stemmer in python nltk (<a href=""https://stackoverflow.com/a/10369407/3096792"">https://stackoverflow.com/a/10369407/3096792</a> ), I want to know if there is any built-in Porter Stemmer in java apache opennlp though there is an interface under this package [opennlp.tools.stemmer.Stemmer]. Since I dont know or need to know the algorithm behind stemming as of now,I need a standard package under apache opennlp toolkit. If not present, whats the alternative in java ?</p>
",Preprocessing of the text & Tokenization,built porter stemmer java opennlp toolkit like built stemmer like porter stemmer python nltk href want know built porter stemmer java apache opennlp though interface package opennlp tool stemmer stemmer since dont know need know algorithm behind stemming need standard package apache opennlp toolkit present whats alternative java p
How Do I Use BrazilianStemmer in Lucene 4?,"<p>i'm trying to tokenize and stem a portuguese sentence using Lucene 4.</p>

<p>Based on this [thread] (<a href=""https://stackoverflow.com/questions/6334692/how-to-use-a-lucene-analyzer-to-tokenize-a-string"">How to use a Lucene Analyzer to tokenize a String?</a>) i was abble to correctly tokenize a portuguese sentence. However, no stemming were been applied. Thus, reading the Lucene 4 documentation, i found this class [BrazilianStemmer] (<a href=""https://lucene.apache.org/core/4_0_0/analyzers-common/org/apache/lucene/analysis/br/BrazilianStemmer.html"" rel=""nofollow noreferrer"">https://lucene.apache.org/core/4_0_0/analyzers-common/org/apache/lucene/analysis/br/BrazilianStemmer.html</a>).</p>

<p>I alter my code to use this BrazilianStemmer class.</p>

<pre><code>    public static StringBuffer tokenizeString(StringBuffer text) {
    StringBuffer result = new StringBuffer();

    try {

        Analyzer analyzer = new PortugueseAnalyzer();

        TokenStream stream  = analyzer.tokenStream(null, new StringReader(text.toString()));
        stream.reset();

        BrazilianStemFilter filter = new BrazilianStemFilter(stream);

        while (filter.incrementToken()) {
            result.append(filter.getAttribute(CharTermAttribute.class).toString());
            result.append("" "");
        }

        filter.close();
        analyzer.close();
    } catch (IOException e) {
        throw new RuntimeException(e);
    }
    return result;
}
</code></pre>

<p>But, I'm not sure that it is working properly. Is this the right and better way to achieve a stemming from Lucene for foreign languages?</p>
",Preprocessing of the text & Tokenization,use brazilianstemmer lucene trying tokenize stem portuguese sentence using lucene based thread alter code use brazilianstemmer class sure working properly right better way achieve stemming lucene foreign language
phonology transformational method,"<p>I have a word list of a particular language in <a href=""http://en.m.wikipedia.org/wiki/International_Phonetic_Alphabet"" rel=""nofollow"">IPA symbols</a>. I also have some rules in regular expressions to convert the input word list into a new word list of another language in IPA symbols. How can I convert that?</p>

<p>My regular expression rules are like this </p>

<pre><code>y=&gt; j / $_
</code></pre>

<p>which means <em>'y' is replaced by 'j' when it is in first place in a particular word</em>.</p>

<p>And the input word list contains some words ready to be converted to a new word list in IPA symbols.</p>
",Preprocessing of the text & Tokenization,phonology transformational method word list particular language ipa symbol also rule regular expression convert input word list new word list another language ipa symbol convert regular expression rule like mean replaced j first place particular word input word list contains word ready converted new word list ipa symbol
Hunspell affix condition regex format. Any way to match the start?,"<p>Good day.</p>

<p>I'm trying to use Hunspell as a stemmer in my application. I don't quite like porter and snowball stemming because of their ""chopped"" words results like ""abus"", ""exampl"". Lemmatizing seems like a good alternative, but I don't know any good CoreNLP alternatives, and I'm certainly not ready to port my project's source code to Java or use bridges yet. Ideally I would like to see initial, like-in-the-dictionary form of the given word.</p>

<p>As I've noticed most of the dictionaries has separate words in .dic file for: bid and bidding, set and setting, get and getting, etc. I'm not that experienced in Hunspell, but isn't there any clever way to handle double d or t for 3-letter word? Is there a way to make it think that ""setting"" is actually is derivated from ""set""?</p>

<p>My current particular problem with Hunspell is I can't get a good comprehensive documentation for creating/editing an affix file. That's what documentations says here: <a href=""http://manpages.ubuntu.com/manpages/dapper/man4/hunspell.4.html"" rel=""nofollow"">http://manpages.ubuntu.com/manpages/dapper/man4/hunspell.4.html</a></p>

<pre><code>(4) condition.

Zero stripping or affix are indicated by zero. Zero condition is
indicated   by   dot. Condition is a simplified, regular
expression-like pattern, which must be met before the affix  can
be  applied. (Dot  signs  an arbitrary character. Characters in
braces sign an arbitrary character from  the  character  subset.
Dash  hasn’t  got  special  meaning, but circumflex (^) next the
first brace sets the complementer character set.)
</code></pre>

<p>Default one is this:</p>

<pre><code>SFX G Y 2
SFX G   e     ing        e
SFX G   0     ing        [^e] 
</code></pre>

<p>I've tried this one:</p>

<pre><code>SFX G Y 4
SFX G   e     ing        e
SFX G   0     ing        [^e] 
SFX G   0     ting       [bcdfghjklmnpqrstvwxz][aeiou]t 
SFX G   0     ding       [bcdfghjklmnpqrstvwxz][aeiou]d 
</code></pre>

<p>but it clearly will also match asSET. Is there any way to get around it somehow? I've tried ^ symbol at the start of regexp, but it seems like it's not working. What can I do to make it work?</p>

<p>Thanks in advance.</p>
",Preprocessing of the text & Tokenization,hunspell affix condition regex format way match start good day trying use hunspell stemmer application quite like porter snowball stemming chopped word result like abus exampl lemmatizing seems like good alternative know good corenlp alternative certainly ready port project source code java use bridge yet ideally would like see initial like dictionary form given word noticed dictionary ha separate word dic file bidding set setting get getting etc experienced hunspell clever way handle double letter word way make think setting actually derivated set current particular problem hunspell get good comprehensive documentation creating editing affix file documentation say default one tried one clearly also match asset way get around somehow tried symbol start regexp seems like working make work thanks advance
Clean the data in an efficient way in Python,"<p>I have a data in the following format : </p>

<p>TOP  (S  (PP-LOC <strong>(IN In)</strong>  (NP  (NP <strong>(DT an)</strong> <strong>(NNP Oct.)</strong> <strong>(CD 19)</strong> <strong>(NN review)</strong> ) (PP <strong>(IN of)</strong>  (NP <strong>(<code></code>)</strong>  (NP-TTL <strong>(DT The)</strong> <strong>(NN Misanthrope)</strong> ) <strong>('' '')</strong>  (PP-LOC <strong>(IN at)</strong>  (NP  (NP <strong>(NNP Chicago)</strong> <strong>(POS 's)</strong> ) <strong>(NNP Goodman)</strong> <strong>(NNP Theatre)</strong> )))) (PRN  <strong>(-LRB- -LRB-)</strong> <strong>(<code></code>)</strong>  (S-HLN  (NP-SBJ <strong>(VBN Revitalized)</strong> <strong>(NNS Classics)</strong> ) (VP <strong>(VBP Take)</strong>  (NP <strong>(DT the)</strong> <strong>(NN Stage)</strong> ) (PP-LOC <strong>(IN in)</strong>  (NP <strong>(NNP Windy)</strong> <strong>(NNP City)</strong> )))) <strong>(, ,)</strong> <strong>('' '')</strong>  (NP-TMP <strong>(NN Leisure)</strong> <strong>(CC &amp;)</strong> <strong>(NNS Arts)</strong> ) <strong>(-RRB- -RRB-)</strong> ))) <strong>(, ,)</strong>  (NP-SBJ-2  (NP  (NP <strong>(DT the)</strong> <strong>(NN role)</strong> ) (PP <strong>(IN of)</strong>  (NP <strong>(NNP Celimene)</strong> ))) <strong>(, ,)</strong>  (VP <strong>(VBN played)</strong>  (NP <strong>(-NONE- *)</strong> ) (PP <strong>(IN by)</strong>  (NP-LGS <strong>(NNP Kim)</strong> <strong>(NNP Cattrall)</strong> ))) <strong>(, ,)</strong> ) (VP <strong>(VBD was)</strong>  (VP  (ADVP-MNR <strong>(RB mistakenly)</strong> ) <strong>(VBN attributed)</strong>  (NP <strong>(-NONE- *-2)</strong> ) (PP-CLR <strong>(TO to)</strong>  (NP <strong>(NNP Christina)</strong> <strong>(NNP Haag)</strong> )))) <strong>(. .)</strong> ))</p>

<p>(TOP  (S  (NP-SBJ <strong>(NNP Ms.)</strong> <strong>(NNP Haag)</strong> ) (VP <strong>(VBZ plays</strong>)  (NP (<strong>NNP Elianti)</strong> )) <strong>(. .)</strong> )) </p>

<p>..... (There are 7000 more..)</p>

<p>This data was taken from a newspaper. A new line is a new sentence (begins with 'TOP')
From this data I need only the bold parts (without the parenthesis) for each sentence:</p>

<pre><code>(IN In)(DT an) (NNP Oct.) (CD 19) (NN review) (IN of) (`` ``) (DT The) (NN Misanthrope)   ('' '')  (IN at)  (NNP Chicago) (POS 's) (NNP Goodman) (NNP Theatre)(-LRB- -LRB-) (`` ``)     (VBN Revitalized) (NNS Classics) (VBP Take) (DT the) (NN Stage)  (IN in)   (NNP Windy) (NNP    City) (, ,) ('' '') (NN Leisure) (CC &amp;) (NNS Arts) (-RRB- -RRB-)(, ,) (DT the) (NN role)(IN of)  (NNP Celimene) (, ,) (VBN played) (-NONE- *)(IN by)(NNP Kim) (NNP Cattrall) (, ,) (VBD was)  (RB mistakenly)(VBN attributed) (-NONE- *-2) (TO to)(NNP Christina) (NNP Haag) (. .)

(NNP Ms.) (NNP Haag) (VBZ plays)(NNP Elianti)(. .)
</code></pre>

<p>I tried the following:</p>

<pre><code>f = open('filename')
data = f.readlines()
f.close()
</code></pre>

<p>this part is to crate an array of tuples for each row (using regular expressions):</p>

<pre><code>tag_word_train = numpy.empty((5000), dtype = 'object')
for i in range(0,5000) :
    tag_word_train[i] = re.findall(r'\(([\w.-]+)\s([\w.-]+)\)',data[i])
</code></pre>

<p>it takes a very long time so I couldn't tell if it is even correct</p>

<p>Do you have any idea how to do it in an efficient way?</p>

<p>Thanks,</p>

<p>Hadas</p>
",Preprocessing of the text & Tokenization,clean data efficient way python data following format top pp loc np np dt nnp oct cd nn review pp np np ttl dt nn misanthrope pp loc np np nnp chicago po nnp goodman nnp theatre prn lrb lrb hln np sbj vbn revitalized nns classic vp vbp take np dt nn stage pp loc np nnp windy nnp city np tmp nn leisure cc nns art rrb rrb np sbj np np dt nn role pp np nnp celimene vp vbn played np none pp np lgs nnp kim nnp cattrall vp vbd wa vp advp mnr rb mistakenly vbn attributed np none pp clr np nnp christina nnp haag top np sbj nnp nnp haag vp vbz play np nnp elianti data wa taken newspaper new line new sentence begin top data need bold part without parenthesis sentence tried following part crate array tuples row using regular expression take long time tell even correct idea efficient way thanks hadas
How to remove punctuation?,"<p>I am using the tokenizer from <strong>NLTK in Python</strong>.</p>

<p>There are whole bunch of answers for removing punctuations on the forum already. However, none of them address all of the following issues together:</p>

<ol>
<li><em>More than one symbol in a row</em>. For example, the sentence: He said,""that's it."" Because there's a comma followed by quotation mark, the tokenizer won't remove ."" in the sentence. The tokenizer will give ['He', 'said', ',""', 'that', 's', 'it.'] instead of ['He','said', 'that', 's', 'it']. Some other examples include '...', '--', '!?', ',""', and so on.</li>
<li><em>Remove symbol at the end of the sentence</em>. i.e. the sentence: Hello World. The tokenizer will give ['Hello', 'World.'] instead of ['Hello', 'World']. Notice the period at the end of the word 'World'. Some other examples include '--',',' in the beginning, middle, or end of any character.</li>
<li><em>Remove characters with symbols in front and after</em>. i.e. <code>'*u*', '''','""""'</code></li>
</ol>

<p>Is there an elegant way of solving both problems?</p>
",Preprocessing of the text & Tokenization,remove punctuation using tokenizer nltk python whole bunch answer removing punctuation forum already however none address following issue together one symbol row example sentence said comma followed quotation mark tokenizer remove sentence tokenizer give said instead said example include remove symbol end sentence e sentence hello world tokenizer give hello world instead hello world notice period end word world example include beginning middle end character remove character symbol front e elegant way solving problem
How to parse product titles (unstructured) into structured data?,"<p>I am looking to parse unstructured product titles like “Canon D1000 4MP Camera 2X Zoom LCD” into structured data like <code>{brand: canon, model number: d1000, lens: 4MP zoom: 2X, display type: LCD}</code>.</p>

<p>So far I have:</p>

<ol>
<li>Removed stopwords and cleaned up (remove characters like <code>-</code> <code>;</code> <code>:</code> <code>/</code>)</li>
<li>Tokenizing long strings into words.</li>
</ol>

<p>Any techniques/library/methods/algorithms would be much appreciated!</p>

<p>EDIT: There is no heuristic for the product titles. A seller can input <strong>anything</strong> as a title. For eg: 'Canon D1000' can just be the title. Also, this exercise is not only for camera datasets, the title can be of any product.  </p>
",Preprocessing of the text & Tokenization,parse product title unstructured structured data looking parse unstructured product title like canon mp camera x zoom lcd structured data like far removed stopwords cleaned remove character like tokenizing long string word technique library method algorithm would much appreciated edit heuristic product title seller input anything title eg canon title also exercise camera datasets title product
Custom Language Stemmer for Elasticsearch,"<p>Is there any way how to create new stemmer? There is for example analyzer for czech language already built in with czech language stemmer. This algorithm was made by some guys in Netherlands. It's not that bad, but for the native speaker it is clear that those honorable guys does not speak the language. If I would like to create my own stemming algorithm, how can I do it in the Elasticsearch?</p>

<p>Thanks.</p>
",Preprocessing of the text & Tokenization,custom language stemmer elasticsearch way create new stemmer example analyzer czech language already built czech language stemmer algorithm wa made guy netherlands bad native speaker clear honorable guy doe speak language would like create stemming algorithm elasticsearch thanks
Python Converting output into sentence,"<p>I just started learning python. I was trying to clean a sentence by breaking into words and joining back to a sentence. the document big.txt has some words like youth, caretaker etc. The problem is in the final procedure : looper , This produces an output by each line.</p>

<p>Correct is an another procedure defined before this code that corrects each word</p>

<p>here is the code :</p>

<pre><code>zebra = 'Yout caretak taking care of something'

count = len(re.findall(r'\w+', zebra))

def looper(a,count):
words = nltk.word_tokenize(zebra)
for i in range(len(words)):
    X = correct(words[i])
    print (X)

final = looper(zebra)
</code></pre>

<p>The output it produces:</p>

<pre><code>youth
caretaker
walking
car
in
something
</code></pre>

<p>How should I take all the individual outputs and make a sentence:</p>

<p>Expected Result:</p>

<pre><code>youth caretaker walking car in something
</code></pre>

<p>Please let me know if you need additional details.</p>

<p>Thanks in advance</p>
",Preprocessing of the text & Tokenization,python converting output sentence started learning python wa trying clean sentence breaking word joining back sentence document big txt ha word like youth caretaker etc problem final procedure looper produce output line correct another procedure defined code corrects word code output produce take individual output make sentence expected result please let know need additional detail thanks advance
how to develop a tool for phonological transformation purpose?,"<p>I want to develop a tool which will accept phonological transformation rules and input word list and subsequently it will produce output word list.For this how the algorithm will look like?</p>
",Preprocessing of the text & Tokenization,develop tool phonological transformation purpose want develop tool accept phonological transformation rule input word list subsequently produce output word list algorithm look like
Replacement by synsets in Python pattern packatge,"<p>My goal is to create a system that will be able to take any random text, extract sentences, remove punctuations, and then, on the bare sentence (one of them), to randomly replace NN or VB tagged words with their meronym, holonym or synonim as well as with a similar word from a WordNet synset. There is a lot of work ahead, but I have a problem at the very beginning.</p>

<p>For this I use pattern and TextBlob packages. This is what I have done so far...</p>

<pre><code>from pattern.web import URL, plaintext
from pattern.text import tokenize
from pattern.text.en import wordnet
from textblob import TextBlob
import string

s = URL('http://www.fangraphs.com/blogs/the-fringe-five-baseballs-most-compelling-fringe-prospects-35/#more-157570').download()
s = plaintext(s, keep=[])
secam = (tokenize(s, punctuation=""""))
simica = secam[15].strip(string.punctuation)
simica = simica.replace("","", """")

simica = TextBlob(simica)
simicaTg = simica.words

synsimica = wordnet.synsets(simicaTg[3])[0]
djidja = synsimica.hyponyms()
</code></pre>

<p>Now everything works the way I want but when I try to extract the i.e. hyponym from this <code>djidja</code> variable it proves to be impossible since it is a <code>Synset</code> object, and I can't manipulate it anyhow. </p>

<p>Any idea how to extract a the very word that is reported in hyponyms list (i.e. <code>print(djidja[2])</code> displays <code>Synset(u'bowler')</code>...so how to extract only <code>'bowler'</code> from this)?</p>
",Preprocessing of the text & Tokenization,replacement synset python pattern packatge goal create system able take random text extract sentence remove punctuation bare sentence one randomly replace nn vb tagged word meronym holonym synonim well similar word wordnet synset lot work ahead problem beginning use pattern textblob package done far everything work way want try extract e hyponym variable prof impossible since object manipulate anyhow idea extract word reported hyponym list e display extract
Data structure/Algorithm for Streaming Data and identifying topics,"<p>I want to know the effective algorithms/data structures to identify the below information in streaming data.  </p>

<p>Consider a real-time streaming data like twitter. I am mainly interested in the below queries rather than storing the actual data. </p>

<blockquote>
  <p>I need my queries to run on actual data but not any of the duplicates.</p>
</blockquote>

<p>As I am not interested in storing the complete data, it will be difficult for me to identify the duplicate posts. However, I can hash all the posts and check against them. But I would like to identify near duplicate posts also. How can I achieve this.  </p>

<blockquote>
  <p>Identify the top k topics being discussed by the users.  </p>
</blockquote>

<p>I want to identify the top topics being discussed by users. I don't want the top frequency words as shown by twitter. Instead I want to give some high level topic name of the most frequent words.  </p>

<blockquote>
  <p>I would like my system to be real-time. I mean, my system should be able to handle any amount of traffic.  </p>
</blockquote>

<p>I can think of map reduce approach but I am not sure how to handle synchronization issues. For example, duplicate posts can reach different nodes and both of them could store them in the index.  </p>

<blockquote>
  <p>In a typical news source, one will be removing any stop words in the data. In my system I would like to update my stop words list by identifying top frequent words across a wide range of topics.  </p>
</blockquote>

<p>What will be effective algorithm/data structure to achieve this.  </p>

<blockquote>
  <p>I would like to store the topics over a period of time to retrieve interesting patterns in the data. Say, friday evening everyone wants to go to a movie. what will be the efficient way to store this data.  </p>
</blockquote>

<p>I am thinking of storing it in hadoop distributed file system, but over a period of time, these indexes become so large that I/O will be my major bottleneck.  </p>

<blockquote>
  <p>Consider multi-lingual data from tweets around the world. How can I identify similar topics being discussed across a geographical area?  </p>
</blockquote>

<p>There are 2 problems here. One is identifying the language being used. It can be identified based on the person tweeting. But this information might affect the privacy of the users. Other idea, could be running it through a training algorithm. What is the best method currently followed for this. Other problem is actually looking up the word in a dictionary and associating it to common intermediate language like say english. How to take care of word sense disambiguation like a same word being used in different contests.  </p>

<blockquote>
  <p>Identify the word boundaries</p>
</blockquote>

<p>One possibility is to use some kind of training algorithm. But what is the best approach followed. This is some way similar to word sense disambiguation, because you will be able to  identify word boundaries based on the actual sentence.  </p>

<p>I am thinking of developing a prototype and evaluating the system rather than the concrete implementation. I think its not possible to scrap the real-time twitter data. I am thinking this approach can be tested on some data freely available online. Any ideas, where I can get this data.</p>

<p>Your feedback is appreciated.</p>

<p>Thanks for your time.</p>

<p>-- Bala</p>
",Preprocessing of the text & Tokenization,data structure algorithm streaming data identifying topic want know effective algorithm data structure identify information streaming data consider real time streaming data like twitter mainly interested query rather storing actual data need query run actual data duplicate interested storing complete data difficult identify duplicate post however hash post check would like identify near duplicate post also achieve identify top k topic discussed user want identify top topic discussed user want top frequency word shown twitter instead want give high level topic name frequent word would like system real time mean system able handle amount traffic think map reduce approach sure handle synchronization issue example duplicate post reach different node could store index typical news source one removing stop word data system would like update stop word list identifying top frequent word across wide range topic effective algorithm data structure achieve would like store topic period time retrieve interesting pattern data say friday evening everyone want go movie efficient way store data thinking storing hadoop distributed file system period time index become large major bottleneck consider multi lingual data tweet around world identify similar topic discussed across geographical area problem one identifying language used identified based person tweeting information might affect privacy user idea could running training algorithm best method currently followed problem actually looking word dictionary associating common intermediate language like say english take care word sense disambiguation like word used different contest identify word boundary one possibility use kind training algorithm best approach followed way similar word sense disambiguation able identify word boundary based actual sentence thinking developing prototype evaluating system rather concrete implementation think possible scrap real time twitter data thinking approach tested data freely available online idea get data feedback appreciated thanks time bala
removing multiple \n in python before sentence tokenizing,"<p>I'm brand new to programming and I am teaching myself out of a book and Stack Overflow.  I'm trying to remove multiple instances of \n in a a chat corpus and then tokenize the sentences.  If I don't remove the \n, the strings look like this: </p>

<pre><code>['answers for 10-19-20sUser139 ... hi 10-19-20sUser101 ;)\n\n\n\n\n\n\n\n\n\nI like it when you do it, 10-19-20sUser83\n\n\n\n\n\n\n\n\n\n\n\niamahotnipwithpics\n\n\n\n10-19-20sUser20 go plan the wedding!']
</code></pre>

<p>I've tried several different methods like chomps, line, rstrip, etc and none of them seem to work. It could be I am using them wrong.  The whole code looks like this:</p>

<pre><code>import nltk, re, pprint
from nltk.corpus import nps_chat
chat= nltk.Text(nps_chat.words())
from nltk.corpus import NPSChatCorpusReader
from bs4 import BeautifulSoup
chat=nltk.corpus.nps_chat.raw()
soup= BeautifulSoup(chat)
soup.get_text()
text =soup.get_text()
print(text[:40])
print(len(text))
from nltk.tokenize import sent_tokenize
sent_chat = sent_tokenize(text)
len(sent_chat)
text[:] = [line.rstrip('\n') for line in text]
print(len(sent_chat))
print(sent_chat[:40])
</code></pre>

<p>When I use the line method I get this error:</p>

<pre><code>Traceback (most recent call last):
File ""C:\Python34\Lib\idlelib\testsubjects\sentencelen.py"", line 57, in &lt;module&gt;
text[:] = [line.rstrip('\n') for line in text]
TypeError: 'str' object does not support item assignment
</code></pre>

<p>Help?</p>
",Preprocessing of the text & Tokenization,removing multiple n python sentence tokenizing brand new programming teaching book stack overflow trying remove multiple instance n chat corpus tokenize sentence remove n string look like tried several different method like chomp line rstrip etc none seem work could using wrong whole code look like use line method get error help
Python NLTK interpret a fixed pattern of sentence and tokenize it,"<p>I have an application where NLTK needs to interpret speech delivered by humans, and find meaningful chunks from it. The sentence which needs to be interpreted is of the form <code>from &lt;somewhere&gt;, to &lt;somewhere&gt; on &lt;some_date&gt;, &lt;class_of_travel,like AC_CHAIR_CAR&gt;</code>. As you understand, this can be expressed in myriads of ways, for example,</p>

<ol>
<li><p>I want to go to New York from Atlanta, business class, 25th July 2014.</p></li>
<li><p>I want to travel via business class, to Atlanta on 25th July from New York.</p></li>
<li><p>I have a dream that I will one day board a plane, travel in business class, descend at New York, the source being at Atlanta, preferably on 25th July.</p></li>
<li><p>25th July Atlanta to New York, business class.</p></li>
</ol>

<p>You get the idea. What I want to extract are few tidbits of information - source, destination, class, date. Some may be missing, which have to be identified, or appropriately assumed. Like if the source is found missing, identify that. Or if the year is missing, chalk it up to the current year. And all the while ignore the useless information (like the <strong>I have a dream</strong> part, much as I adore Martin Luther).</p>

<p>Is there any way I can achieve this in NLTK? I am aware that there are taggers available, and there are ways to train taggers, but I don't have sufficient knowledge on that. Is it possible to cover more or less all possible cases that can mean such a sentence, and extract the information like this? If so, a little guidance would be appreciated.</p>
",Preprocessing of the text & Tokenization,python nltk interpret fixed pattern sentence tokenize application nltk need interpret speech human find meaningful chunk sentence need interpreted form understand expressed myriad way example want go new york atlanta business class th july want travel via business class atlanta th july new york dream one day board plane travel business class descend new york source atlanta preferably th july th july atlanta new york business class get idea want extract tidbit information source destination class date may missing identified appropriately assumed like source found missing identify year missing chalk current year ignore useless information like dream part much adore martin luther way achieve nltk aware tagger available way train tagger sufficient knowledge possible cover le possible case mean sentence extract information like little guidance would appreciated
Extracting tuples with nltk?,"<p>Reading the documentation of nltk i found that is possible to extract tuples with <code>str2tuple()</code>. As an instance assume i have the following sentence(clearly is a much larger file):</p>

<pre><code>sent = ""pero pero CC "" \
        ""tan tan RG "" \
        ""antigua antiguo AQ0FS0 "" \
        ""que que CS "" \
        ""según según SPS00 "" \
        ""mi mi  DP1CSS "" \
        ""madre madre NCFS000""
</code></pre>

<p>I would like to extract a list of tuples, e.g.:</p>

<pre><code>&gt; ([antigua, AQ0FS0],[madre, NCFS000])
</code></pre>

<p>The female adjective tag <code>(AQ0FS0)</code> and the female noun tag <code>(NCFS000)</code>. Is this possible with <code>str2tuple()</code> or a better aproach could be using a regular expression?</p>

<p>This is what i have tried:</p>

<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-

import nltk as nl

sent = ""pero pero CC "" \
              ""tan tan RG "" \
              ""antigua antiguo AQ0FS0 "" \
              ""que que CS "" \
              ""según según SPS00 "" \
              ""mi mi  DP1CSS "" \
              ""madre madre NCFS000""

nl.tag.str2tuple(t) for t in sent.split()
</code></pre>
",Preprocessing of the text & Tokenization,extracting tuples nltk reading documentation nltk found possible extract tuples instance assume following sentence clearly much larger file would like extract list tuples e g female adjective tag female noun tag possible better aproach could using regular expression tried
tm Package error: Error definining Document Term Matrix,"<p>I am analyzing the Reuters 21578 corpus, all the Reuters news articles from 1987, using the ""tm"" package. After importing the XML files into an R data file, I clean the text--convert to plaintext, convert to lwer case, remove stop words etc. (as seen below)--then I try to convert the corpus to a document term matrix but receive an error message: </p>

<p>Error in UseMethod(""Content"", x) : 
  no applicable method for 'Content' applied to an object of class ""character""</p>

<p>All pre-processing steps work correctly up until document term matrix.</p>

<p>I created a non-random subset of the corpus (with 4000 documents) and the document term matrix command works fine on that.</p>

<p>My code is below. Thanks for the help.</p>

<pre><code>##Import
file &lt;- ""reut-full.xml"" 
reuters &lt;- Corpus(ReutersSource(file), readerControl = list(reader = readReut21578XML))

## Convert to Plain Text Documents
reuters &lt;- tm_map(reuters, as.PlainTextDocument)

## Convert to Lower Case
reuters &lt;- tm_map(reuters, tolower)

## Remove Stopwords
reuters &lt;- tm_map(reuters, removeWords, stopwords(""english""))

## Remove Punctuations
reuters &lt;- tm_map(reuters, removePunctuation)

## Stemming
reuters &lt;- tm_map(reuters, stemDocument)

## Remove Numbers
reuters &lt;- tm_map(reuters, removeNumbers)

## Eliminating Extra White Spaces
reuters &lt;- tm_map(reuters, stripWhitespace)

## create a term document matrix
dtm &lt;- DocumentTermMatrix(reuters)

Error in UseMethod(""Content"", x) : 
  no applicable method for 'Content' applied to an object of class ""character""
</code></pre>
",Preprocessing of the text & Tokenization,tm package error error definining document term matrix analyzing reuters corpus reuters news article using tm package importing xml file r data file clean text convert plaintext convert lwer case remove stop word etc seen try convert corpus document term matrix receive error message error usemethod content x applicable method content applied object class character pre processing step work correctly document term matrix created non random subset corpus document document term matrix command work fine code thanks help
Address Extraction from text in Objective C,"<p>I am working on a Business Card reader app which has some extra features other than business card reading. Presently, I am stuck in address extraction process. The addresses I am working on are not of any specific country. They are global addresses. 
The app is meant to run offline. So, I can't consume any online API to detect the address. I have gone trough several blogs and tried to implement most of them but with no effect. I tried with several regular expressions as well but nothing seemed to work so far. I have some Business Card reader apps installed in my mobile which detect the addresses beautifully. I need a logic which can extract address from an unstructured text.</p>
",Preprocessing of the text & Tokenization,address extraction text objective c working business card reader app ha extra feature business card reading presently stuck address extraction process address working specific country global address app meant run offline consume online api detect address gone trough several blog tried implement effect tried several regular expression well nothing seemed work far business card reader apps installed mobile detect address beautifully need logic extract address unstructured text
How do I implement lemmatization on a large amounts of data?,"<p>I need to implement lemmatization on a large amounts of data(~10 million lines). Python nltk library is very slow on large amounts of data. Is there any other alternative?</p>
",Preprocessing of the text & Tokenization,implement lemmatization large amount data need implement lemmatization large amount data million line python nltk library slow large amount data alternative
"Are there any other sentence tokenizers in NLTK, other than punkt tokenizer","<p>I am using NLTK to tokenize articles from wikipedia into sentences. But the punkt tokenizer is not giving very good results as sometime it is creating problems like sentences are getting tokenized when <code>etc.</code> appears, or problems occurs when double inverted commas appear in text like <code>['as they say ""harry is a good boy.', '"" He thinks']</code> and so on.</p>

<p>I want to stick to NLTK itself as this is something which is sandwiched between some other processes.</p>

<p>Are there any other classifiers that can be used?</p>

<p>I don't mind using any other library in python as well.</p>
",Preprocessing of the text & Tokenization,sentence tokenizers nltk punkt tokenizer using nltk tokenize article wikipedia sentence punkt tokenizer giving good result sometime creating problem like sentence getting tokenized appears problem occurs double inverted comma appear text like want stick nltk something sandwiched process classifier used mind using library python well
Sentence segmentation with Regex in Python,"<p>I am writing a script to split the text into sentences with Python. However I am quite bad with writing more complex regular expressions.</p>

<p>There are 5 rules according to which I wish to split the sentences. I want to split sentences if they:</p>

<pre><code>* end with ""!""  or
* end with ""?""  or
* end with ""...""  or
* end with ""."" and the full stop is not followed by a number  or
* end with ""."" and the full stop is followed by a whitespace
</code></pre>

<p>What would be the regular expression for this for Python?</p>
",Preprocessing of the text & Tokenization,sentence segmentation regex python writing script split text sentence python however quite bad writing complex regular expression rule according wish split sentence want split sentence would regular expression python
NLP: lemmatization with lemmaGen c++,"<p>I am enhancing a chatbot and I wish to find the lemma of the words from the input sentence. The chatbot is written is c++ and I have found a free open source lemmatization tool called LemmaGen. I have download version 2.2 for c++ but it comes with no documentation on how it is referenced or even used.</p>

<p>Has anyone had any experience with LemmaGen for c++ in the past? Any information will be helpful.
Many Thanks</p>
",Preprocessing of the text & Tokenization,nlp lemmatization lemmagen c enhancing chatbot wish find lemma word input sentence chatbot written c found free open source lemmatization tool called lemmagen download version c come documentation referenced even used ha anyone experience lemmagen c past information helpful many thanks
How to tweak the NLTK sentence tokenizer,"<p>I'm using NLTK to analyze a few classic texts and I'm running in to trouble tokenizing the text by sentence. For example, here's what I get for a snippet from <em><a href=""http://www.gutenberg.org/cache/epub/2701/pg2701.txt"">Moby Dick</a></em>:</p>

<pre><code>import nltk
sent_tokenize = nltk.data.load('tokenizers/punkt/english.pickle')

'''
(Chapter 16)
A clam for supper? a cold clam; is THAT what you mean, Mrs. Hussey?"" says I, ""but
that's a rather cold and clammy reception in the winter time, ain't it, Mrs. Hussey?""
'''
sample = 'A clam for supper? a cold clam; is THAT what you mean, Mrs. Hussey?"" says I, ""but that\'s a rather cold and clammy reception in the winter time, ain\'t it, Mrs. Hussey?""'

print ""\n-----\n"".join(sent_tokenize.tokenize(sample))
'''
OUTPUT
""A clam for supper?
-----
a cold clam; is THAT what you mean, Mrs.
-----
Hussey?
-----
"" says I, ""but that\'s a rather cold and clammy reception in the winter time, ain\'t it, Mrs.
-----
Hussey?
-----
""
'''
</code></pre>

<p>I don't expect perfection here, considering that Melville's syntax is a bit dated, but NLTK ought to be able to handle terminal double quotes and titles like ""Mrs."" Since the tokenizer is the result of an unsupervised training algo, however, I can't figure out how to tinker with it.</p>

<p>Anyone have recommendations for a better sentence tokenizer? I'd prefer a simple heuristic that I can hack rather than having to train my own parser. </p>
",Preprocessing of the text & Tokenization,tweak nltk sentence tokenizer using nltk analyze classic text running trouble tokenizing text sentence example get snippet anyone recommendation better sentence tokenizer prefer simple heuristic hack rather train parser
Is there any list of english stop words for blogs?,"<p>I am working on blogs to analyses their content. I am using basic english stop word list but it is not enough because of blog specific frequent but useless words like ""archive"", ""comment"" and such. Do you know pre-created list of stop-words targeting blogs?</p>
",Preprocessing of the text & Tokenization,list english stop word blog working blog analysis content using basic english stop word list enough blog specific frequent useless word like archive comment know pre created list stop word targeting blog
How to stop NLTK stemmer from removing the trailing &quot;e&quot;?,"<p>I'm using NLTK stemmer to remove grammatical variations of a stem word.
However, the Port or Snowball stemmers remove the trailing ""e"" of the original form of a noun or verb, e.g., Profile becomes Profil.</p>

<p>How can I prevent this from happening? I know I can use a conditional to guard against this. But obviously it will fail on different cases.</p>

<p>Is there an option or another API for what I want?</p>
",Preprocessing of the text & Tokenization,stop nltk stemmer removing trailing e using nltk stemmer remove grammatical variation stem word however port snowball stemmer remove trailing e original form noun verb e g profile becomes profil prevent happening know use conditional guard obviously fail different case option another api want
"Document or Text Clustering using EM algorithm for GMM, how to do?","<p>I am trying to make a project of Document Clustering (in Java). There can be maximum 1 million documents and I want to make unsupervised cluster. To do, I am trying to implement EM algorithm with Gaussian Mixture Model. </p>

<p>But, I am not sure how to make the document vector.</p>

<p>I am thinking something like this, first I will calculate TF/IDF for each word in the document(after removing stop words and stemming done). </p>

<p>Then I will normalize each vector. At this stage, the question arises, how shall I represent a vector by a point? Is it possible? </p>

<p>I have learned about EM algorithm from this (<a href=""https://www.youtube.com/watch?v=iQoXFmbXRJA"" rel=""nofollow"">https://www.youtube.com/watch?v=iQoXFmbXRJA</a>) video where 1-D points are used for GMM and to be used in EM. </p>

<p>Can any one explain how to convert a vector in a 1-D point to implement EM for GMM?</p>

<p>If my approach is wrong, can you explain how to do the whole thing in simple words? Sorry for my long question. Thanks for your help!</p>
",Preprocessing of the text & Tokenization,document text clustering using em algorithm gmm trying make project document clustering java maximum million document want make unsupervised cluster trying implement em algorithm gaussian mixture model sure make document vector thinking something like first calculate tf idf word document removing stop word stemming done normalize vector stage question arises shall represent vector point possible learned em algorithm video point used gmm used em one explain convert vector point implement em gmm approach wrong explain whole thing simple word sorry long question thanks help
Want Regex to stop at first occurrence of &quot;.&quot; and &quot;;&quot;,"<p>I am trying to extract sentence to from a paragraph, with pattern like</p>

<pre><code> Current. time is six thirty at Scotland. Past. time was five thirty at India; Current. time is five thirty at Scotland. Past. time was five thirty at Scotland. Current. time is five ten at Scotland.
</code></pre>

<p>When I Use Regex as</p>

<pre><code>/current\..*scotland\./i
</code></pre>

<p>This matches to all string </p>

<pre><code>Current. time is six thirty at Scotland. Past. time was six thirty at India; Current. time is five thirty at Scotland. Past. time was five thirty at Scotland. Current. time is five ten at Scotland.
</code></pre>

<p>Instead I want to stop at first occurrence of ""."" to all capture groups like</p>

<pre><code> Current. time is six thirty at Scotland.
 Current. time is five ten at Scotland. 
</code></pre>

<p>Similarly for text like</p>

<pre><code> Past. time was five thirty at India; Current. time is six thirty at Scotland. Past. time was five thirty at Scotland. Past. time was five ten at India;    
</code></pre>

<p>When I Use Regex Like</p>

<pre><code> /past\..*india\;/i
</code></pre>

<p>This matches will whole string </p>

<pre><code> Past. time was five thirty at India; Current. time is six thirty at Scotland. Past. time was five thirty at Scotland. Past. time was five ten at India; 
</code></pre>

<p>Here I want to capture all groups or first group like following, and How to stop at first occurrence of "";""</p>

<pre><code>Past. time was five thirty at India; 
Past. time was five ten at India; 
</code></pre>

<p>How I can make regular expression to stop at "","" or "";"" with above examples?</p>
",Preprocessing of the text & Tokenization,want regex stop first occurrence trying extract sentence paragraph pattern like use regex match string instead want stop first occurrence capture group like similarly text like use regex like match whole string want capture group first group like following stop first occurrence make regular expression stop example
"Really common linking words, verbs and pronouns list?","<p>I require a simple word list to filter some sentences. What I need is to find only the meaningful words from a sentence. Like this:</p>

<pre><code>In mathematics, and more specifically in graph theory, a graph is a representation of a set of objects where some pairs of objects are connected by links.
</code></pre>

<p>I want to get this:</p>

<pre><code>[mathematics, graph, theory, representation, set, objects, pairs, connected, links]
</code></pre>

<p>I figured that a really common word list would help me eliminate what I don't need, but I can't seem to find a good one.</p>

<p>Any ideas where I could find such a list?</p>
",Preprocessing of the text & Tokenization,really common linking word verb pronoun list require simple word list filter sentence need find meaningful word sentence like want get figured really common word list would help eliminate need seem find good one idea could find list
"What is proper Tokenization algorithm? &amp; Error: TypeError: coercing to Unicode: need string or buffer, list found","<p>I'm doing an Information Retrieval Task. As part of pre-processing I want to doing. </p>

<ol>
<li>Stopword removal</li>
<li>Tokenization</li>
<li>Stemming (Porter Stemmer)</li>
</ol>

<p>Initially, I skipped tokenization. As a result I got terms like this:</p>

<pre><code>broker
broker'
broker,
broker.
broker/deal
broker/dealer'
broker/dealer,
broker/dealer.
broker/dealer;
broker/dealers),
broker/dealers,
broker/dealers.
brokerag
brokerage,
broker-deal
broker-dealer,
broker-dealers,
broker-dealers.
brokered.
brokers,
brokers.
</code></pre>

<p>So, Now I realized importance of tokenization. Is there any standard algorithm for tokenization for English language? Based on <code>string.whitespace</code> and commonly used puncuation marks. I wrote </p>

<pre><code>def Tokenize(text):
    words = text.split(['.',',', '?', '!', ':', ';', '-','_', '(', ')', '[', ']', '\'', '`', '""', '/',' ','\t','\n','\x0b','\x0c','\r'])    
    return [word.strip() for word in words if word.strip() != '']
</code></pre>

<ol>
<li>I'm getting <code>TypeError: coercing to Unicode: need string or buffer, list found</code> error! </li>
<li>How can this Tokenization routine be improved?</li>
</ol>
",Preprocessing of the text & Tokenization,proper tokenization algorithm error typeerror coercing unicode need string buffer list found information retrieval task part pre processing want stopword removal tokenization stemming porter stemmer initially skipped tokenization result got term like realized importance tokenization standard algorithm tokenization english language based commonly used puncuation mark wrote getting error tokenization routine improved
How to go back to the original word from the output of Lucene&#39;s Porterstemmer (or any word stemmer),"<p>I'm using the PorterStemmer with Lucene to stem words for some text analysis. I keep getting words like 'de' and 'en' that rank among the top. In order to remove them, I need to know what the original words were. Is there any way to find out?</p>
",Preprocessing of the text & Tokenization,go back original word output lucene porterstemmer word stemmer using porterstemmer lucene stem word text analysis keep getting word like de en rank among top order remove need know original word way find
Tokenizing first and last name as one token,"<p>Is is possible to tokenize a text in tokens such that first and last name are combined in one token?
For example if my text is:</p>

<pre><code>text = ""Barack Obama is the President""
</code></pre>

<p>Then:</p>

<pre><code>text.split()
</code></pre>

<p>results in: </p>

<pre><code>['Barack', 'Obama', 'is', 'the, 'President']
</code></pre>

<p>how can I recognize the first and last name? So I get only <code>['Barack Obama', 'is', 'the', 'President']</code> as tokens.</p>

<p>Is there a way to achieve it in Python?</p>
",Preprocessing of the text & Tokenization,tokenizing first last name one token possible tokenize text token first last name combined one token example text result recognize first last name get token way achieve python
Why cowardly becomes cowardli after stemming?,"<p>I noticed that after applying Porter stemming (from NLTK library) I get strange stems such as <em>""cowardli""</em> or <em>""contrari""</em>. For me they don't look like stems at all. </p>

<p>Is it okay? Could it be that I made a mistake smwhere?</p>

<p>Here is my code:</p>

<pre><code>string = string.lower()
tokenized = nltk.tokenize.regexp_tokenize(string,""[a-z]+"")
filtered = [w for w in tokenized if w not in nltk.corpus.stopwords.words(""english"")]


stemmer = nltk.stem.porter.PorterStemmer()
stemmed = []
for w in filtered:
    stemmed.append(stemmer.stem(w))
</code></pre>

<p>And here is the text I used for processing <a href=""http://pastebin.com/XUMNCYAU"" rel=""nofollow"">http://pastebin.com/XUMNCYAU</a> (beginning of ""crime and punishment"" book by Dostoevsky).</p>
",Preprocessing of the text & Tokenization,cowardly becomes cowardli stemming noticed applying porter stemming nltk library get strange stem cowardli contrari look like stem okay could made mistake smwhere code text used processing beginning crime punishment book dostoevsky
Method for distinguishing between word and non-words,"<p>I'm working with the Stack exchange data dump and attempting to identify unique and novel words in the corpus. I'm doing this be referencing a very large wordlist and extracting the words not present in my reference word list.</p>

<p>The problem I am running up against is a number of the unique tokens are non-words, like directory names, error codes, and other strings.</p>

<p>Is there a good method of identifying differentiating word-like strings from non-word-like strings? </p>

<p>I'm using NLTK, but am not limited to that toolkit. </p>
",Preprocessing of the text & Tokenization,method distinguishing word non word working stack exchange data dump attempting identify unique novel word corpus referencing large wordlist extracting word present reference word list problem running number unique token non word like directory name error code string good method identifying differentiating word like string non word like string using nltk limited toolkit
Should I remove Stop words with POS tagging?,"<p>I'm new to this NLP stuff but all the examples of POS tagging and Sentence Chunking I have seen don't seem to have removed stops words. So question I have if I'm doing POS tagging and Chunking is does this remove the need to remove stopwords (and also Stem)?</p>
",Preprocessing of the text & Tokenization,remove stop word po tagging new nlp stuff example po tagging sentence chunking seen seem removed stop word question po tagging chunking doe remove need remove stopwords also stem
"Lemmatizer in R or python (am, are, is -&gt; be?)","<p>I'm not a [computational] linguistic, so please excuse my supper dummy-ness in this topic.</p>

<p>According to Wikipedia, lemmatisation is defined as:</p>

<blockquote>
  <p>Lemmatisation (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.</p>
</blockquote>

<p>Now my question is, is the lemmatised version of any member of the set {am, is, are} supposed to be ""be""? If not, why not?</p>

<p>Second question: How do I get that in R or python? I've tried methods like <a href=""https://stackoverflow.com/questions/14952215/wordnet-lemmatizer-for-r"">this</a> link, but non of them gives ""be"" given ""are"". I guess at least for the purpose of classifying text documents, this makes sense to be true.</p>

<p>I also couldn't do that with any of the given demos <a href=""http://text-processing.com/demo/"" rel=""nofollow noreferrer"">here</a>.</p>

<p>What am I doing/assuming wrong?</p>
",Preprocessing of the text & Tokenization,lemmatizer r python computational linguistic please excuse supper dummy ness topic according wikipedia lemmatisation defined lemmatisation lemmatization linguistics process grouping together different inflected form word analysed single item question lemmatised version member set supposed second question get r python tried method like also given demo assuming wrong
Using NLP to clean up user generated content automatically?,"<p>Is it possible to use NLP to automatically improve user generated content?</p>

<p>The goal is to take a bad UGC sentence like:</p>

<p><code>no low beams or running lights..bulbs ok and high beams work!!!</code></p>

<p>And try to improve it into something like:</p>

<p><code>No running lights or low beams. Bulbs are alright and the high beams are working!</code></p>

<p>Any advice would be greatly appreciated.</p>
",Preprocessing of the text & Tokenization,using nlp clean user generated content automatically possible use nlp automatically improve user generated content goal take bad ugc sentence like try improve something like advice would greatly appreciated
Exactly replicating R text preprocessing in python,"<p>I would like to preprocess a corpus of documents using Python in the same way that I can in R. For example, given an initial corpus, <code>corpus</code>, I would like to end up with a preprocessed corpus that corresponds to the one produced using the following R code:</p>

<pre><code>library(tm)
library(SnowballC)

corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(""myword"", stopwords(""english"")))
corpus = tm_map(corpus, stemDocument)
</code></pre>

<p>Is there a simple or straightforward — preferably pre-built — method of doing this in Python? Is there a way to ensure exactly the same results?</p>

<hr>

<p>For example, I would like to preprocess</p>

<blockquote>
  <p>@Apple ear pods are AMAZING! Best sound from in-ear headphones I've
  ever had!</p>
</blockquote>

<p>into</p>

<blockquote>
  <p>ear pod  amaz best sound  inear headphon ive ever</p>
</blockquote>
",Preprocessing of the text & Tokenization,exactly replicating r text preprocessing python would like preprocess corpus document using python way r example given initial corpus would like end preprocessed corpus corresponds one produced using following r code simple straightforward preferably pre built method python way ensure exactly result example would like preprocess apple ear pod amazing best sound ear headphone ever ear pod amaz best sound inear headphon ive ever
Matching one tokenized set with many others,"<p>I have a data quering problem and am a little stuck as to how to solve it. I have a 2 lists of urls. One, we can call 'positive hits', and another called 'unknown hits'. The goal is to tokenize each sets of pages and find any matching tokens in the unknown set of urls with the positive set of urls. For example, if the first url in the positive set has the token 'hello' found on it, that token should be checked against the tokenized versions of all unknown url pages (in tokenized form) to see if there is a hit. Then move on to the second url in the positive set... etc.</p>

<p>I can tokenize the pages no problem and even do an intersection for each page in the positive url sets with every page in the unknown set but this is very slow. Is there another method of computation i can use to achieve my goals?</p>
",Preprocessing of the text & Tokenization,matching one tokenized set many others data quering problem little stuck solve list url one call positive hit another called unknown hit goal tokenize set page find matching token unknown set url positive set url example first url positive set ha token hello found token checked tokenized version unknown url page tokenized form see hit move second url positive set etc tokenize page problem even intersection page positive url set every page unknown set slow another method computation use achieve goal
"Bash Script to process irregular text, count occurrences, cut at a threshold","<p>I have a large sample of text that is pretty irregular and I'd like to tokenize it to single words, and count occurrences of each words, and have an output where occurrence > threshold_value </p>

<pre><code>if [ $# -ne 3 ]; then
        echo 'Usage &lt;file&gt; &lt;output_file&gt; &lt;threshold&gt;'
        exit 1
fi

clean_and_rank () {
    tr -dc [:graph:][:cntrl:][:space:] &lt; $1 \
    | tr -d [:punct:] \
    | tr -s ' ' \
    | tr ' ' '\n' \
    | tr '[A-Z]' '[a-z]' \
    | grep -v '^$' \
    | sort \
    | uniq -c \
    | sort -nr
}

cut_below_threshold () {
        $THRESHOLD=$1
        awk '$1 &gt; '$THRESHOLD' { print $1, $2 }'
}

clean_and_rank $1 \
| cut_below_threshold $3
| sort -nr &gt; $2
</code></pre>

<p>but for some reason I'm running into trouble with the cut_below_threshold() function --</p>

<p>Also once I get this done, I want to be able to compare it to another sample (my data is 2 samples of several rows of labeled text snippets and I want to score words independently for prevalence in sample A/sample B)</p>

<p>Is there a better way to go about this? Ultimately, I'm looking for insights along the lines of ""$WORD is in sample 1 1000 times, out of 100000 total words, it is in sample 2 100 times out of 10000 words""</p>
",Preprocessing of the text & Tokenization,bash script process irregular text count occurrence cut threshold large sample text pretty irregular like tokenize single word count occurrence word output occurrence threshold value reason running trouble cut threshold function also get done want able compare another sample data sample several row labeled text snippet want score word independently prevalence sample sample b better way go ultimately looking insight along line word sample time total word sample time word
How to use python to tokenize and chunk tagged sentences line by line,"<p>I'm a Linguistist and would want to use python to tokenize sentences in a csv document line by line and also tell the tag and the token position in the tag(B-beginning or I-inside) just like the example below.  </p>

<pre><code>""id"", ""sentence""
""1"", ""&lt;person&gt;Claire&lt;/person&gt;lived in&lt;location&gt;London UK&lt;/location&gt;for&lt;time&gt;2 years&lt;/time&gt;""
""2"", ""&lt;location&gt;UK&lt;/location&gt; is in&lt;location&gt;Europe&lt;/location&gt;""
 ...........
 ...........


 dataframe = pd.read_csv(document)
 sentences = dataframe['sentence']
 for line in sentences :
     #print token position tag

 &gt;&gt; Claire  B-Per  person 
    lived   null   null   
    in      null   null
    London  B-Loc  location
    UK      I-Loc  location
    for     null   null
    2       B-Tim   time
    years   I-Tim   time 

    UK      B-Loc  location
    is      null   null                
    in      null   null
    Europe  B-Loc  location 
</code></pre>
",Preprocessing of the text & Tokenization,use python tokenize chunk tagged sentence line line linguistist would want use python tokenize sentence csv document line line also tell tag token position tag b beginning inside like example
Which stemming algorithm should I use?,"<p>We are developing a vertical search engine as our BTech project. We want to use a stemmer to convert words on webpages to their root words. We tried using Porter Stemmer but it is not giving expected outcome.</p>

<p>Porter Stemmer falsely converts e.g.</p>

<pre><code>1. goes -&gt; goe
2. ponies -&gt; poni
3. happily -&gt; happili
</code></pre>

<p>So can anybody suggest which algorithm should we use?</p>
",Preprocessing of the text & Tokenization,stemming algorithm use developing vertical search engine btech project want use stemmer convert word webpage root word tried using porter stemmer giving expected outcome porter stemmer falsely convert e g anybody suggest algorithm use
How to get informal synonyms (ie technology =&gt; tech)?,"<p>How can I get informal synonyms or abbreviations for a word? I tried using stemmers (like the Porter filter) and thesauruses, but they don't seem to recognize ""informal"" synonyms for a word. I guess my examples below are not really synonyms, but rather abbreviations.</p>

<p>Examples include:</p>

<ol>
<li>Technology => Tech</li>
<li>Business => Biz</li>
<li>Applications => Apps</li>
</ol>
",Preprocessing of the text & Tokenization,get informal synonym ie technology tech get informal synonym abbreviation word tried using stemmer like porter filter thesaurus seem recognize informal synonym word guess example really synonym rather abbreviation example include technology tech business biz application apps
Natural Language Processing using nltk,"<p>I need an algorithm to tokenize given sentence into words which are correctly tagged to its grammar meaning. </p>

<p>for example: ""People took to the streets and protested""
people-noun
took- adjective
and-conjunction
to- ...and so on</p>
",Preprocessing of the text & Tokenization,natural language processing using nltk need algorithm tokenize given sentence word correctly tagged grammar meaning example people took street protested people noun took adjective conjunction
Using regular expression to extract leaf nodes in phrase structure trees,"<p>I want to use regular expression in Java to extract leaf nodes in sentence or phrase structure trees.
For example,
Give a sentence ""This is an easy sentence."",</p>

<p>I have syntactic information </p>

<p>Input:
<code>(ROOT (S (NP (DT This)) (VP (VBZ is) (NP (DT an) (JJ easy) (NN sentence))) (. .)))</code></p>

<p>I want to use regular expression to extract leaf nodes</p>

<p>Output:</p>

<pre><code>DT This
VBZ is
DT an
JJ easy
NN sentence
.  .
</code></pre>
",Preprocessing of the text & Tokenization,using regular expression extract leaf node phrase structure tree want use regular expression java extract leaf node sentence phrase structure tree example give sentence easy sentence syntactic information input want use regular expression extract leaf node output
NLTK (python) and Greek encoding,"<p>I try to use NLTK pagkage in Greek text and I deal with a great problem with the encoding. My code is below</p>

<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os, string, re, nltk

def find_bigrams(input_list):
   bigram_list = []
   for i in range(len(input_list)-1):
       bigram_list.append((input_list[i], input_list[i+1]))
       return bigram_list

def get_nice_string(list_or_iterator):
   return ""["" + "" , "".join( str(x) for x in list_or_iterator) + ""]""

def stripText(rawText):
   text = rawText
    rules = [
    {r'{[^)]*\}' : ''},             # remove curly brackets
    {r'\([^)]*\)' : ''},            # remove parentheses
    {r'^https?:\/\/.*[\r\n]*' : ''},# remove urls
    {r' +' : ' '},                  # remove multiple whitespaces
    {r'^\s+': ''},                  # remove whitespaces beginning
    {r'\.\.+' : '.'}                # remove multiple fullstops
    ]

for rule in rules:
    for (k, v) in rule.items():
        regex = re.compile(k)
        text = regex.sub(v, text)

sentenceClean = text.translate(string.maketrans('', ''), '{}[]|?""=\'')
return sentenceClean

if __name__ == '__main__':
    f = open('C:\\Users\\Dimitris\\Desktop\\1.txt', 'r').readlines()

    newFile = open('C:\\Users\\Dimitris\\Desktop\\corpus.txt', 'w')
    newFile1 = open('C:\\Users\\Dimitris\\Desktop\\words.txt', 'w')

    words = ['jpg', 'jpeg', 'File', 'Image']

for line in f:
    sentences = stripText(line)
    whitespaces = sentences.count(' ')
    if any(word in sentences for word in words):
        continue
    elif whitespaces &lt; 20:
        continue
    else:
        newFile.write(sentences+'\n')

        b = nltk.word_tokenize(sentences)
        print get_nice_string(b)
        get_nice_string(nltk.bigrams(b))
        print get_nice_string(nltk.bigrams(b))

        newFile1.write(get_nice_string(b))


newFile.close()
newFile1.close()
</code></pre>

<p>When I try to print the output from nltk.word_tokenize(sentences), the result is like something that (('\xe5\xe3\xea\xfe\xec\xe9\xe1', '\xe3\xe9')), but if I use the  get_nice_string() function and I turn list into a string, then the result is normal greek text. 
So far so good. </p>

<p>But whether I use find_bigrams() function or nltk.bigrams() I get strings like the above (('\xe5\xe3\xea\xfe\xec\xe9\xe1', '\xe3\xe9')), even if I use get_nice_string() function, in order to turn list into string.</p>

<p>Also, I have tried to open file with the codecs.open() function, like this</p>

<pre><code>f = codecs.open('C:\\Users\\Dimitris\\Desktop\\1.txt', 'r', 'utf-8').readlines()  
</code></pre>

<p>but the problem persists.</p>

<p>Any ideas?</p>
",Preprocessing of the text & Tokenization,nltk python greek encoding try use nltk pagkage greek text deal great problem encoding code try print output nltk word tokenize sentence result like something xe xe xea xfe xec xe xe xe xe use get nice string function turn list string result normal greek text far good whether use find bigram function nltk bigram get string like xe xe xea xfe xec xe xe xe xe even use get nice string function order turn list string also tried open file codecs open function like problem persists idea
Lucene Standard Analyzer vs Snowball,"<p>Just getting started with Lucene.Net.  I indexed 100,000 rows using standard analyzer, ran some test queries, and noticed plural queries don't return results if the original term was singular.  I understand snowball analyzer adds stemming support, which sounds nice.  However, I'm wondering if there are any drawbacks to gong with snowball over standard?  Am I losing anything by going with it?  Are there any other analyzers out there to consider?</p>
",Preprocessing of the text & Tokenization,lucene standard analyzer v snowball getting started lucene net indexed row using standard analyzer ran test query noticed plural query return result original term wa singular understand snowball analyzer add stemming support sound nice however wondering drawback gong snowball standard losing anything going analyzer consider
How do I partition a sequence into varying-length tuples according to an existing pattern distribution?,"<p>I have a prior probability distribution for various parts of speech patterns, held in <code>patterns</code>. I also have a sequence of lists of parts of speech tokens, obtained from word tokenization, in <code>sequences</code>.</p>

<p>I would like to partition each list in <code>sequences</code> into any number of disjoint segments, where each segment exists in <code>patterns</code> and the joint probability is maximized.</p>

<p>For example, the sequence <code>['NN', 'VBG', 'CC', 'VBG']</code> would ideally be partitioned into the following: <code>[('NN',), ('VBG',), ('VBG',)]</code>.</p>

<p>I can't think of an approach that isn't horribly inefficient. Perhaps if <code>patterns</code> were organized in some sort of tree structure would help? </p>

<pre><code>patterns = {('NN',): 0.40132345717065276,
            ('VBG',): 0.22273379631859294,
            ('JJ', 'NN'): 0.075111492116086656,
            ('NN', 'NN'): 0.056656296053708859,
            ...
            ('NN', 'NN', 'VBG'): 0.00039491807857906547,
            ('RB', 'VBD'): 0.00033712518903090955,
            ('NN', 'CD'): 0.00019264296516051976,
            ('VBG', 'NN'): 0.0017337866864446778}

sequences = [['NN', 'VBG', 'CC', 'VBG'],
             ['JJ', 'NNS', 'VBP', 'RB', 'JJ', 'JJ', 'NN'],
             ['JJ', 'NN'],
             ['JJ', 'NNP', 'JJ', 'NNS']]
</code></pre>
",Preprocessing of the text & Tokenization,partition sequence varying length tuples according existing pattern distribution prior probability distribution various part speech pattern held also sequence list part speech token obtained word tokenization would like partition list number disjoint segment segment exists joint probability maximized example sequence would ideally partitioned following think approach horribly inefficient perhaps organized sort tree structure would help
How to match text tokens from a lookup table?,"<p>I'm trying to do text preprocessing on Tweets. What I'm interested in is to match tokens from tweet's text to that of a lookup table. For example I have a table/dictionary of sentiments words and I want to check if a tweet contains one of those words.</p>

<p>My text preprocessing at the moment is the following:</p>

<ol>
<li>Store the tweet's text in a String.</li>
<li>Tokenize the text at whitespace characters (<code>String.split("" "")</code>) and remove all numeric and weird characters tokens. Also I remove all mentions and stopwords. Then store the tokens in a String array.</li>
</ol>

<p>This approach leads to some problems and here is the explanation:</p>

<ul>
<li>One straightforward approach is just to compare the tokens with the strings in the table and check if they match. That is okay but this leads to the next problem.</li>
<li>I also want to check for if the text contains emoticons (which are also stored in a dictionary). Now that I already removed all weird characters tokens, I can't simply do the previous comparisons approach. One could say ok, then just don't remove the numeric and weird characters tokens, but this leads to the following problem.</li>
<li>I also want to cluster the text, so keeping the numeric tokens (or urls) is not an option since it screws the clustering quality.</li>
<li>Keeping two versions of the tokens has a memory problem since everything is duplicated.</li>
</ul>

<p>I was wondering if there is a way to match stuff that doesn't require duplicating stuff. Maybe using regex?</p>
",Preprocessing of the text & Tokenization,match text token lookup table trying text preprocessing tweet interested match token tweet text lookup table example table dictionary sentiment word want check tweet contains one word text preprocessing moment following store tweet text string tokenize text whitespace character remove numeric weird character token also remove mention stopwords store token string array approach lead problem explanation one straightforward approach compare token string table check match okay lead next problem also want check text contains emoticon also stored dictionary already removed weird character token simply previous comparison approach one could say ok remove numeric weird character token lead following problem also want cluster text keeping numeric token url option since screw clustering quality keeping two version token ha memory problem since everything duplicated wa wondering way match stuff require duplicating stuff maybe using regex
feature selection within large data set,"<p>I want to know what are the most acceptable ways to find features(special words) within large data set. When I say special words, I mean words which are most used in a specific field.</p>

<p>For example, I have two books:</p>

<ol>
<li>book1: a book about economics</li>
<li>book2: a book about art</li>
</ol>

<p>Now, I choose book1 and want to see which words are most related to it. I guess such words as  'financial', 'dollar', 'revenue' etc. will dominate the top of the most used words list. Even though the words may occur in the book2 too, frequencies will be less than book1.</p>

<p>On the other hand, choosing book2 is supposed to yield words such as 'abstract', 'renaissance', 'romanticism', 'culture' etc.</p>

<p>Of course the result depends on context(in the above example, it depends on book1 and book2).</p>

<p>It is obvious, chosen algorithm must be able to eliminate stop-words.</p>

<p>So, I am wondering which methods are being used for this problem.</p>
",Preprocessing of the text & Tokenization,feature selection within large data set want know acceptable way find feature special word within large data set say special word mean word used specific field example two book book book economics book book art choose book want see word related guess word financial dollar revenue etc dominate top used word list even though word may occur book frequency le book hand choosing book supposed yield word abstract renaissance romanticism culture etc course result depends context example depends book book obvious chosen algorithm must able eliminate stop word wondering method used problem
Why do I need a tokenizer for each language?,"<p>When processing text, why would one need a tokenizer specialized for the language? </p>

<p>Wouldn't tokenizing by whitespace be enough? What are the cases where it is not good idea to use simply a white space tokenization?</p>
",Preprocessing of the text & Tokenization,need tokenizer language processing text would one need tokenizer specialized language tokenizing whitespace enough case good idea use simply white space tokenization
Parse sentence Stanford Parser by passing String not an array of strings,"<p>Is it possible to parse a sentence using the Stanford Parser by passing a string and not an array of strings. This is the example they gave in their short tutorial (<a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/lexparser/package-summary.html"" rel=""nofollow"">See Docs</a>) :</p>

<p>Here's example:</p>

<pre><code>    import java.util.*;
    import edu.stanford.nlp.ling.*;
    import edu.stanford.nlp.trees.*;
    import edu.stanford.nlp.parser.lexparser.LexicalizedParser;

    class ParserDemo {
      public static void main(String[] args) {
        LexicalizedParser lp = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"");
        lp.setOptionFlags(new String[]{""-maxLength"", ""80"", ""-retainTmpSubcategories""});

        String[] sent = { ""This"", ""is"", ""an"", ""easy"", ""sentence"", ""."" }; // This is the sentence to be parsed
        List&lt;CoreLabel&gt; rawWords = Sentence.toCoreLabelList(sent);
        Tree parse = lp.apply(rawWords);
        parse.pennPrint();
        System.out.println();

        TreebankLanguagePack tlp = new PennTreebankLanguagePack();
        GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
        GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
        List&lt;TypedDependency&gt; tdl = gs.typedDependenciesCCprocessed();
        System.out.println(tdl);
        System.out.println();

      }

}
</code></pre>

<p>I am trying to see if I can do this because I need to get sentences from a MySQL database and parse them directly as strings. I could tokezine the sentences and add the words, commas, and period to a String Array, However, to tokenize these sentences, I would have to use the Stanford Tokenizer , PTBTokenizer. The constructor of this tokenizer as listed here</p>

<p>(<a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/PTBTokenizer.html#PTBTokenizer%28java.io.Reader,%20edu.stanford.nlp.process.LexedTokenFactory,%20java.lang.String%29"" rel=""nofollow"">See Docs</a>)</p>

<p>requires a ""java.io.FileReader"" Object, but I am not reading a file from directory. So I am wondering if there is a way to either Parse the sentence directly by passing a string, or if I can solve my problem by tokenizing the sentence without requiring a ""java.io.FileReader"" Object.</p>
",Preprocessing of the text & Tokenization,parse sentence stanford parser passing string array string possible parse sentence using stanford parser passing string array string example gave short tutorial see doc example trying see need get sentence mysql database parse directly string could tokezine sentence add word comma period string array however tokenize sentence would use stanford tokenizer ptbtokenizer constructor tokenizer listed see doc requires java io filereader object reading file directory wondering way either parse sentence directly passing string solve problem tokenizing sentence without requiring java io filereader object
Is there a method to determine if a document is a file of text sentences?,"<p>I'm processing hundreds of thousands of files. Potentially millions later on down the road. A bad file will contain a text version of an excel spreadsheet or other text that isn't binary but also isn't sentences. Such files cause CoreNLP to blow up (technically, these files take a long time to process such as 15 seconds per kilobyte of text.) I'd love to detect these files and discard them in sub-second time.</p>

<p>What I am considering is taking a few thousand files at random, examining the first, say, 200 characters and looking for the distribution of characters to determine what is legic and what is an outlier. Example, if there are no punctuation marks or too many of them. Does this seem like a good approach? Is there a better one that has been proven? I think, for sure, this will work well enough, possibly throwing out potentially good files but rarely.</p>

<p>Another idea is to simply run with annotators tokenize and ssplit and do word and sentence count. That seems to do a good job as well and returns quickly. I can think of cases where this might fail as well, possibly.</p>
",Preprocessing of the text & Tokenization,method determine document file text sentence processing hundred thousand file potentially million later road bad file contain text version excel spreadsheet text binary also sentence file cause corenlp blow technically file take long time process second per kilobyte text love detect file discard sub second time considering taking thousand file random examining first say character looking distribution character determine legic outlier example punctuation mark many doe seem like good approach better one ha proven think sure work well enough possibly throwing potentially good file rarely another idea simply run annotator tokenize ssplit word sentence count seems good job well return quickly think case might fail well possibly
How to get sentence number from input?,"<p>It seems hard to detect a sentence boundary in a text. Quotation marks like .!? may be used to delimite sentences but not so accurate as there may be ambiguous words and quotations such as U.S.A or Prof. or Dr.  I am studying Tperlregex library and Regular Expression Cookbook by <a href=""http://www.regular-expressions.info/delphi.html"" rel=""nofollow"">Jan Goyvaerts</a> but I do not know how to write the expression that detects sentence?</p>

<p>What may be comparatively accurate expression using Tperlregex in delphi?</p>

<p>Thanks</p>
",Preprocessing of the text & Tokenization,get sentence number input seems hard detect sentence boundary text quotation mark like may used delimite sentence accurate may ambiguous word quotation u prof dr studying tperlregex library regular expression cookbook jan goyvaerts know write expression detects sentence may comparatively accurate expression using tperlregex delphi thanks
Split text file at sentence boundary,"<p>I have to process a text file (an e-book). I'd like to process it so that there is one sentence per line (a ""newline-separated file"", yes?). How would I do this task using sed the UNIX utility? Does it have a symbol for ""sentence boundary"" like a symbol for ""word boundary"" (I think the GNU version has that). Please note that the sentence can end in a period, ellipsis, question or exclamation mark, the last two in combination (for example, ?, !, !?, !!!!! are all valid ""sentence terminators""). The input file is formatted in such a way that some sentences contain newlines that have to be removed.</p>

<p>I thought about a script like <code>s/...|. |[!?]+ |/\n/g</code> (unescaped for better reading). But it does not remove the newlines from inside the sentences.</p>

<p>How about in C#? Would it be remarkably faster if I use regular expressions like in sed? (I think not). Is there an other faster way?</p>

<p>Either way (sed or C#) is fine. Thank you.</p>
",Preprocessing of the text & Tokenization,split text file sentence boundary process text file e book like process one sentence per line newline separated file yes would task using sed unix utility doe symbol sentence boundary like symbol word boundary think gnu version ha please note sentence end period ellipsis question exclamation mark last two combination example valid sentence terminator input file formatted way sentence contain newlines removed thought script like unescaped better reading doe remove newlines inside sentence c would remarkably faster use regular expression like sed think faster way either way sed c fine thank
Can I get BigramCollocationFinder (nltk) to honour document boundaries?,"<p>I am using NLTK to do some analysis of a number of distinct documents. The content of these documents means that they all tend to end and start with the same tokens.</p>

<p>I tokenize the documents into a list of lists and then use BigramCollocationFinder.from_documents to create the finder. When I score the ngrams by raw frequency, I notice that the most common occurence is the end character/start character. This would suggest that it is running all the documents into one and finding ngrams on the whole lot which I don't want.</p>

<p>A sample of the code:</p>

<pre><code>line_tokenizer = nltk.RegexpTokenizer('\{|\}|[^,""}]+')
seqs = [""{B,C}"", ""{B,A}"", ""{A,B,C}""]
documents = [line_tokenizer.tokenize(s) for s in seqs]
finder = BigramCollocationFinder.from_documents(documents)
bigram_measures = nltk.collocations.BigramAssocMeasures()
print(finder.score_ngrams(bigram_measures.raw_freq))
</code></pre>

<p>This results in the following output:</p>

<pre><code>[(('B', 'C'), 0.15384615384615385), 
 (('C', '}'), 0.15384615384615385), 
 (('{', 'B'), 0.15384615384615385), 
 (('}', '{'), 0.15384615384615385), 
 (('A', 'B'), 0.07692307692307693), 
 (('A', '}'), 0.07692307692307693), 
 (('B', 'A'), 0.07692307692307693), 
 (('{', 'A'), 0.07692307692307693)]
</code></pre>

<p>The ngram }{ shows up in the list which it shouldn't as }{ never appear next to each other.</p>

<p>Is there an alternative way to approach this problem to avoid }{ showing up in the list?</p>
",Preprocessing of the text & Tokenization,get bigramcollocationfinder nltk honour document boundary using nltk analysis number distinct document content document mean tend end start token tokenize document list list use bigramcollocationfinder document create finder score ngrams raw frequency notice common occurence end character start character would suggest running document one finding ngrams whole lot want sample code result following output ngram show list never appear next alternative way approach problem avoid showing list
Generating words relevant to a word,"<p>my question is pretty straight forward, I've spent a few hours searching the web of existing methods of generating keywords for a topic/word.
For example, if my input is:</p>

<p>Object oriented programming</p>

<p>I want my output to be along the lines of:</p>

<p>classes, objects, friend functions, static variables etc</p>

<p>My current idea of a solution is to google the specific subject I'm interested in generating keywords for, grabbing the first x(many) result pages, removing all tags and stop words from them, passing each word through the Python nltk lemmatizer to get its basic form so I dont count words that mean essentially the same thing more than once (""performance"" and ""performer"" will both become ""perform""), then counting the number of occurances of each word, and grabbing the top x% as the most relevant words to my search topic. </p>

<p>The first issue with this, is that it wont generate any phrases because it treats each word individually, the second is that there Must be something already done in this field and the results I've come up with during my research are : context vectors(seem quite similar to what I want but in reality they arent really...i think... :p) the second thing is Porter stemmer algorithm, but then I realized lemmatization is much better for my cause... I also saw alot of ""keyword generators"" for sites to increase their traffic, but I highly doubt I can use any of those for what Im trying to do.</p>

<p>If anyone could point me in a direction of an algorithm or existing research on this, or anything at all, Id be really grateful :)</p>
",Preprocessing of the text & Tokenization,generating word relevant word question pretty straight forward spent hour searching web existing method generating keywords topic word example input object oriented programming want output along line class object friend function static variable etc current idea solution google specific subject interested generating keywords grabbing first x many result page removing tag stop word passing word python nltk lemmatizer get basic form dont count word mean essentially thing performance performer become perform counting number occurances word grabbing top x relevant word search topic first issue wont generate phrase treat word individually second must something already done field result come research context vector seem quite similar want reality arent really think p second thing porter stemmer algorithm realized lemmatization much better cause also saw alot keyword generator site increase traffic highly doubt use im trying anyone could point direction algorithm existing research anything id really grateful
Why does NLTK mis-tokenize quote at end of sentence?,"<p>Given a string:</p>

<pre><code>c = 'A problem. She said: ""I don\'t know about it.""'
</code></pre>

<p>And an attempt to tokenize it:</p>

<pre><code>&gt;&gt;&gt; for sindex,sentence in enumerate(sent_tokenize(c)):
...     print str(sindex)+"": ""+sentence
...
0: A problem.
1: She said: ""I don't know about it.
2: ""
&gt;&gt;&gt;
</code></pre>

<p>Why does NLTK put the end quote of sentence 2 into its own sentence 3? Is there a way to correct this behavior?</p>
",Preprocessing of the text & Tokenization,doe nltk mi tokenize quote end sentence given string attempt tokenize doe nltk put end quote sentence sentence way correct behavior
How to match words from a list in a huge corpus using regexp (in Perl or *nix terminal)?,"<p>from a given noun list in a .txt file, where nouns are separated by new lines, such as this one: </p>

<pre><code>hooligan
football
brother
bollocks
</code></pre>

<p>...and a separate .txt file containing a series of regular expressions separated by new lines, like this:</p>

<pre><code>[a-z]+\tNN(S)?
[a-z]+\tJJ(S)?
</code></pre>

<p>...I would like to run the regular expressions through each sentence of a corpus and, every time the regexp matches a pattern, if that pattern contains one of the nouns in the list of nouns, I would like to print that noun in the output and (separated it by tab) the regular expression that matched it. Here is an example of how the resulting output could be:</p>

<pre><code>football    [a-z]+NN(S)?\'s POS[a-z]+NN(S)?
hooligan    [a-z]+NN(S)?,,[a-z]+JJ[a-z]+NN(S)?
hooligan    [a-z]+NN(S)?,,[a-z]+JJ[a-z]+NN(S)?
football    [a-z]+NN(S)?[a-z]+NN(S)?
brother [a-z]+PP$[a-z]+NN(S)?
bollocks    [a-z]+DT[a-z]+NN(S)?
football    [a-z]+NN(s)?(be)VBZnotRB
</code></pre>

<p>The corpus I would use is huge (tens of GB) and has the following format (each sentence is contained in the tag <code>&lt;s&gt;</code>):</p>

<pre><code>&lt;s&gt;
Hooligans   hooligan    NNS 1   4   NMOD
,   ,   ,   2   4   P
unbridled   unbridled   JJ  3   4   NMOD
passion passion NN  4   0   ROOT
-   -   :   5   4   P
and and CC  6   4   CC
no  no  DT  7   9   NMOD
executive   executive   JJ  8   9   NMOD
boxes   box NNS 9   4   COORD
.   .   SENT    10  0   ROOT
&lt;/s&gt;
&lt;s&gt;
Hooligans   hooligan    NNS 1   4   NMOD
,   ,   ,   2   4   P
unbridled   unbridled   JJ  3   4   NMOD
passion passion NN  4   0   ROOT
-   -   :   5   4   P
and and CC  6   4   CC
no  no  DT  7   9   NMOD
executive   executive   JJ  8   9   NMOD
boxes   box NNS 9   4   COORD
.   .   SENT    10  0   ROOT
&lt;/s&gt;
&lt;s&gt;
Portsmouth  Portsmouth  NP  1   2   SBJ
bring   bring   VVP 2   0   ROOT
something   something   NN  3   2   OBJ
entirely    entirely    RB  4   5   AMOD
different   different   JJ  5   3   NMOD
to  to  TO  6   5   AMOD
the the DT  7   12  NMOD
Premiership Premiership NP  8   12  NMOD
:   :   :   9   12  P
football    football    NN  10  12  NMOD
's  's  POS 11  10  NMOD
past    past    NN  12  6   PMOD
.   .   SENT    13  2   P
&lt;/s&gt;
&lt;s&gt;
This    this    DT  1   2   SBJ
is  be  VBZ 2   0   ROOT
one one CD  3   2   PRD
of  of  IN  4   3   NMOD
Britain Britain NP  5   10  NMOD
's  's  POS 6   5   NMOD
most    most    RBS 7   8   AMOD
ardent  ardent  JJ  8   10  NMOD
football    football    NN  9   10  NMOD
cities  city    NNS 10  4   PMOD
:   :   :   11  2   P
think   think   VVP 12  2   COORD
Liverpool   Liverpool   NP  13  0   ROOT
or  or  CC  14  13  CC
Newcastle   Newcastle   NP  15  19  SBJ
in  in  IN  16  15  ADV
miniature   miniature   NN  17  16  PMOD
,   ,   ,   18  15  P
wound   wind    VVD 19  13  COORD
back    back    RB  20  19  ADV
three   three   CD  21  22  NMOD
decades decade  NNS 22  19  OBJ
.   .   SENT    23  2   P
&lt;/s&gt;
</code></pre>

<p>I started to work to a script in PERL to achieve my goal, and in order to not run out of memory with such a huge dataset I used the module <a href=""http://search.cpan.org/~toddr/Tie-File-0.98/lib/Tie/File.pm"" rel=""nofollow"">Tie::File</a> so that my script would read one line at a time (instead of trying to open the entire corpus file in memory). This would work perfectly with a corpus where each sentence corresponds to one single line, but not in the current case where sentences are spread on more lines and delimited by a tag.</p>

<p>Is there a way to achieve what I want using a combination unix terminal commands (e.g. cat and grep)? Alternatively, which would be the best solution for this issue? (Some code examples would be great).</p>
",Preprocessing of the text & Tokenization,match word list huge corpus using regexp perl nix terminal given noun list txt file noun separated new line one separate txt file containing series regular expression separated new line like would like run regular expression sentence corpus every time regexp match pattern pattern contains one noun list noun would like print noun output separated tab regular expression matched example resulting output could corpus would use huge ten gb ha following format sentence contained tag started work script perl achieve goal order run memory huge dataset used module tie file script would read one line time instead trying open entire corpus file memory would work perfectly corpus sentence corresponds one single line current case sentence spread line delimited tag way achieve want using combination unix terminal command e g cat grep alternatively would best solution issue code example would great
Recognizing email fields without using regular expressions,"<p>We have a tokenizer which tokenizes a text file .The logic followed is quite weird but necessary in our context. </p>

<p>An email such as 
<code>xyz.zyx@gmail.com</code></p>

<p>will result in the following tokens :
<code>xyz</code>
<code>.</code> 
<code>zyx</code>
<code>@</code>
<code>gmail</code></p>

<p>I would like to know how can we recognize the field as email if we are allowed to use only these tokens. No regex is allowed. We are allowed only to use the tokens and their surrounding tokens to figure out if the field is an email field</p>
",Preprocessing of the text & Tokenization,recognizing email field without using regular expression tokenizer tokenizes text file logic followed quite weird necessary context email result following token would like know recognize field email allowed use token regex allowed allowed use token surrounding token figure field email field
is punctuation kept in a bag of words?,"<p>I'm creating a bag of words module from the scratch. I'm not sure whether it's best practice in this approach whether to remove punctuation. Consider the sentence</p>

<pre><code>I've been ""DMX world center"" for long time ago.Are u?
</code></pre>

<p><strong>Question</strong>: For the bag of words, should I consider </p>

<ul>
<li>the token <code>DMX</code> (no quotation mark) or <code>""DMX</code> (which includes the left quotation mark)</li>
<li><code>u</code> (without the question mark) or <code>u?</code> (with the question mark)</li>
</ul>

<p>In short, <strong>should I remove all the punctuation marks when getting distinct words?</strong></p>

<p>Thanks in advance</p>

<p>Updated
This is the code of what I have implemented</p>

<p>Sample text : <code>ham  , im .. On the snowboarding trip. I was wondering if your planning to get everyone together befor we go..a meet and greet kind of affair? Cheers,</code></p>

<pre><code>   HashSet&lt;String&gt; bagOfWords = new HashSet&lt;String&gt;();
   BufferedReader reader = new BufferedReader(new FileReader(path));
   while (reader.ready()) {
       String msg = reader.readLine().split(""\t"", 2)[1].toLowerCase(); // I get only the 2nd part. 1st part indicate wether message is spam or ham
       String[] words = msg.split(""[\\s+\n.\t!?+,]""); // this is the regex that I've used to split words
       for (String word : words) {
           bagOfWords.add(word);
       }
   }
</code></pre>
",Preprocessing of the text & Tokenization,punctuation kept bag word creating bag word module scratch sure whether best practice approach whether remove punctuation consider sentence question bag word consider token quotation mark includes left quotation mark without question mark question mark short remove punctuation mark getting distinct word thanks advance updated code implemented sample text
Why it&#39;s difficult for a stemmer to work on some non-regular words,"<p>This question may have more things to do with philosophy than with coding. The nltk provides both a tag package that marks each word of a sentence with a tagger, and a stem package that turn a word to its base form. I noted that the stemmer often fails to turn some non-regular words to its base form. For example, ""It's"" to ""It is"", ""knew"" to ""know"", ""got"" to ""get"", and so on. This is a bit surprising given the fact that the tagger can correctly tag ""'s"" as a verb in its right form (""VBZ""), and recognize the difference between ""knew"" (""VBD"") and ""know"" (""VB""). Since we already know ""knew"" is a verb in its past tense, why it's difficult for a stemmer to correctly return its base form? It seems the best stemmer available is the snowball stemmer, though its performance is not satisfying.</p>
",Preprocessing of the text & Tokenization,difficult stemmer work non regular word question may thing philosophy coding nltk provides tag package mark word sentence tagger stem package turn word base form noted stemmer often fails turn non regular word base form example knew know got get bit surprising given fact tagger correctly tag verb right form vbz recognize difference knew vbd know vb since already know knew verb past tense difficult stemmer correctly return base form seems best stemmer available snowball stemmer though performance satisfying
Regex: How to find and extract acronyms and corresponding definition of acronym from a text?,"<p>I would like to do something like suggested in this question – but on a more general level:
<a href=""https://stackoverflow.com/questions/4593376/regular-expression-for-acronyms"">Regular Expression for Acronyms</a></p>

<p>Input example:</p>

<blockquote>
  <p>""In a seminal set of papers, <strong>Feddersen and Pesendorfer</strong> (1996, 1999), hereafter <strong>FP</strong>, incorporate ... has been labeled the “<strong>swing voter’s curse</strong>,” from now on <strong>SVC</strong>. The prediction ... the best way to begin using a <strong>Static Application Security Testing</strong> (<strong>SAST</strong>) tool.. from Latin <strong>ante meridiem</strong> (<strong>A.M.</strong>) meaning before noon...""</p>
</blockquote>

<p>Result:</p>

<ol>
<li>['Feddersen and Pesendorfer', 'FP']</li>
<li>['swing voter’s curse', 'SVC']</li>
<li>['Static Application Security Testing', 'SAST']</li>
<li>['ante meridiem', 'A.M.']</li>
</ol>

<p>There are of course many possible 'signals' of an acronym. I've listed some below.:</p>

<ul>
<li><em>Parenthesis:</em> ... (...)</li>
<li>... hereafter ...</li>
<li>... from now on ...</li>
<li>... after this ...</li>
<li>... refered to as ...</li>
<li>... subsequently ...</li>
<li>... hence ...</li>
<li>... henceforth ...</li>
<li>... hereinafter ...</li>
<li><em>etc.</em></li>
</ul>

<p>Perhaps it would be beneficial to have two regular expressions; one for the parenthesis, and one for all the others, since they differ quite substantially in their structure.</p>

<p>Only focusing on first letter acronyms, ie. ignoring cases such as <strong>sonar</strong>, created from <strong>SOund Navigation And Ranging</strong>.</p>

<p>Is it possible to do such a think with regex, and if so how would you go about it?</p>
",Preprocessing of the text & Tokenization,regex find extract acronym corresponding definition acronym text would like something like suggested question general level feddersen pesendorfer hereafter fp incorporate ha labeled swing voter curse svc prediction best way begin using static application security testing sast tool latin ante meridiem meaning noon result feddersen pesendorfer fp swing voter curse svc static application security testing sast ante meridiem course many possible signal acronym listed parenthesis hereafter refered subsequently hence henceforth hereinafter etc perhaps would beneficial two regular expression one parenthesis one others since differ quite substantially structure focusing first letter acronym ie ignoring case sonar created sound navigation ranging possible think regex would go
How to represent bi-grams,"<p>I'm working on a system/algorithm that will detect topics in a stream of tweets. </p>

<p>What I'll do is remove the stop words, emoticons, urls, etc. and I'm thinking about representing the tweet as follows:</p>

<pre><code>terms = (t1, t2, ..., tk)
hashtags = (h1, h2, ..., hn)
date = date of tweet
</code></pre>

<p>and then use some similarity measures between the tweets when applying some clustering algorithms, combining those 3 values. This will be a little more complex than that, since I'll handle replies (eg. when you reply to some tweet, most of the time you keep talking about the same topics, etc).</p>

<p>I don't know if that will work or not, but the problem I'm seeing so far is that I'm not identifying <strong>n-grams</strong>, so <em>Barack Obama</em> appear most of time together, and in my system it will be two separate terms (<em>Barack</em> and <em>Obama</em>).</p>

<p>My question is:</p>

<p><strong>How can I also represent bi-grams? I mean, how is it usually modeled?</strong></p>

<p>I thought about having something like the following:</p>

<pre><code>Tweet = `Some words here`
terms = `[some, words, here, some words, words here]`
...
</code></pre>

<p>but I don't know if that is the correct way to go, if I have to do that for every possible bi-gram, etc.</p>

<p><strong>Edit</strong>:</p>

<p>In my database, I will have all the terms stored. Should I also store the bi-grams as if they were terms?</p>
",Preprocessing of the text & Tokenization,represent bi gram working system algorithm detect topic stream tweet remove stop word emoticon url etc thinking representing tweet follows use similarity measure tweet applying clustering algorithm combining value little complex since handle reply eg reply tweet time keep talking topic etc know work problem seeing far identifying n gram barack obama appear time together system two separate term barack obama question also represent bi gram mean usually modeled thought something like following know correct way go every possible bi gram etc edit database term stored also store bi gram term
NLP: Calculating probability a document belongs to a topic (with a bag of words)?,"<p>Given a topic, how can I calculate the probability a document ""belongs"" to that topic(ie sports)</p>

<p>This is what I have to work with:</p>

<p>1) I know the common words in documents associated with that topics (eliminating all STOP words), and the % of documents that have that word 
For instance if the topic is sports, I know:</p>

<pre><code>75% of sports documents have the word ""play""
70% have the word ""stadium""
40% have the word ""contract""
30% have the word ""baseball""
</code></pre>

<p>2) Given this, and a document with a bunch of words, how can I calculate the probability this document belongs to that topic? </p>
",Preprocessing of the text & Tokenization,nlp calculating probability document belongs topic bag word given topic calculate probability document belongs topic ie sport work know common word document associated topic eliminating stop word document word instance topic sport know given document bunch word calculate probability document belongs topic
Regular expressions for getting all kinds of tokens including hyphens,"<p>I want to split a sentence into words and special characters. I am using the regular expression below:</p>

<pre><code>@""((\b[^\s]+\b)((?&lt;=\.\w).)?)
</code></pre>

<p>But it returns only words and not special characters such as space-separated hyphens or colons.</p>

<p>Ideally, for the sentence:</p>

<blockquote>
  <p>""Right now!"" she shouted, and hands fluttered in the air - amid a few cheers - for 
  about two minutes.</p>
</blockquote>

<p>I should get:</p>

<pre>
Right
now
she
shouted
and
hands
fluttered
in
the
air
-
amid
a
few
cheers
-
for
about
two
minutes
</pre>
",Preprocessing of the text & Tokenization,regular expression getting kind token including hyphen want split sentence word special character using regular expression return word special character space separated hyphen colon ideally sentence right shouted hand fluttered air amid cheer two minute get right shouted hand fluttered air amid cheer two minute
Split in to Sentences and Wrap With Tags,"<p>I'm trying to build a text fixing page for normalising text written in all capital letters, all lower case or an ungrammatical mixture of both. </p>

<p>What I'm currently trying to do is write a regular expression to find all full stops, question marks and line breaks, then split the string in to various strings containing all of the words up to and including each full stop.</p>

<p>Then I'm going to wrap them with <code>&lt;span&gt;</code> tags and use CSS <code>:first-letter</code> and <code>text-transform:capitalize;</code> to capitalise the first letter of each sentence. </p>

<p>The last stage will be writing a dictionary function to find user-specified words for capitalisation.</p>

<p>This question only concerns the part about writing a regex and splitting in to strings.</p>

<p>I've tried too many methods to post here, with varying results, but here's my current attempt:</p>

<pre><code>for(var i=0; i &lt; DoIt.length; i++){ 
    DoIt[i].onclick = function(){

        var offendingtext = input.value.toString();
        var keeplinebreaks = offendingtext.replace(/\r?\n/g, '&lt;br /&gt;');
        var smalltext = keeplinebreaks.toLowerCase();
        //split at each character I specify
        var breakitup = smalltext.split(/[/.?\r\n]/g);

        breakitup.forEach(function(i){
            var i;
            console.log(i);
            var packagedtogo = document.createElement('span');
            packagedtogo.className = 'sentence';
            packagedtogo.innerHTML = breakitup[i];
            output.appendChild(packagedtogo);
            i++;
        });
    }
}
</code></pre>

<p>It was splitting at the right places before, but it was printing <code>undefined</code> in the output area between the tags. I've been at this for days, please could someone give me a hand.</p>

<p><strong>How can I split a string in to multiple string sentences, and then wrap each string with html tags?</strong></p>
",Preprocessing of the text & Tokenization,split sentence wrap tag trying build text fixing page normalising text written capital letter lower case ungrammatical mixture currently trying write regular expression find full stop question mark line break split string various string containing word including full stop going wrap tag use cs capitalise first letter sentence last stage writing dictionary function find user specified word capitalisation question concern part writing regex splitting string tried many method post varying result current attempt wa splitting right place wa printing output area tag day please could someone give hand split string multiple string sentence wrap string html tag
A custom tokenizer for Java,"<p>I am developing an application in which I need to process text files containing emails. I need all the tokens from the text and the following is the definition of token:</p>

<ol>
<li>Alphanumeric</li>
<li>Case-sensitive (case to be preserved)</li>
<li>'!' and '$' are to be considered as constituent characters. Ex: <code>FREE!!</code>, <code>$50</code> are tokens</li>
<li><p>'.' (dot) and ',' comma are to be considered as constituent characters if they occur between numbers. For ex:</p>

<p>192.168.1.1, $24,500</p>

<p>are tokens.</p></li>
</ol>

<p>and so on..</p>

<p>Please suggest me some open-source tokenizers for Java which are easy to customize to suit my needs. Will simply using StringTokenizer and regex be enough? I have to perform stopping also and that's why I was looking for an open source tokenizer which will also perform some extra things like stopping, stemming.</p>
",Preprocessing of the text & Tokenization,custom tokenizer java developing application need process text file containing email need token text following definition token alphanumeric case sensitive case preserved considered constituent character ex token dot comma considered constituent character occur number ex token please suggest open source tokenizers java easy customize suit need simply using stringtokenizer regex enough perform stopping also wa looking open source tokenizer also perform extra thing like stopping stemming
Paring an index down to &quot;interesting&quot; words for future search terms,"<p>I have a list of about 18,000 unique words scraped from a database of government transcripts that I would like to make searchable in a web app. The catch: This web app must be client-side. (AJAX is permissible.)</p>

<p>All the original transcripts are in neat text files on my server, so the index file of words will list which files contain each word and how many times, like so: </p>

<pre><code>ADMINSTRATION   {""16"": 4, ""11"": 5, ""29"": 4, ""14"": 2}
ADMIRAL {""34"": 12, ""12"": 2, ""15"": 9, ""16"": 71, ""17"": 104, ""18"": 37, ""19"": 23}
AMBASSADOR  {""2"": 15, ""3"": 10, ""5"": 37, ""8"": 5, ""41"": 10, ""10"": 2, ""16"": 6, ""17"": 6, ""50"": 4, ""20"": 5, ""22"": 17, ""40"": 10, ""25"": 14}
</code></pre>

<p>I have this reduced to a trie-structure in its final form to save space and speed up retrieval, but even so, 18K words is about 5MB of data with the locations, even with stop words removed. But no one is reasonably going to search for out-of-context adjectives and subordinating conjunctions.</p>

<p>I realize this is something of a language question as much as a coding question, but I'm wondering if there is a common solution in NLP for reducing a text to words that are meaningful out of context. </p>

<p>I tried running each word through the Python NLTK POS tagger, but there's a high error rate when the words stand by themselves, as one would expect.</p>
",Preprocessing of the text & Tokenization,paring index interesting word future search term list unique word scraped database government transcript would like make searchable web app catch web app must client side ajax permissible original transcript neat text file server index file word list file contain word many time like reduced trie structure final form save space speed retrieval even k word mb data location even stop word removed one reasonably going search context adjective subordinating conjunction realize something language question much coding question wondering common solution nlp reducing text word meaningful context tried running word python nltk po tagger high error rate word stand one would expect
Extract only complete setences from emails using Python?,"<p>I have thousands of emails stored in either plain text or HTML.  All of the plain text emails are formatted pretty much the same, so extracting just the actual email message has been simple.</p>

<p>But the HTML emails are all over the place, and I'm finding it difficult to come up with a mathod of extracting the body message only.  There's a lot of other junk in the email that >I don't want, such as ""This email was generated by..."" and a bunch of other non-user generated text that changes from email to email.</p>

<p>Is there some way for Python to identify what resembles a body of text or complete sentences?</p>

<p>I've already tried using regular expressions found here:
<a href=""https://stackoverflow.com/questions/8465335/a-regex-for-extracting-sentence-from-a-paragraph-in-python"">a Regex for extracting sentence from a paragraph in python</a></p>

<p>But the problem with that was that I have a lot of lines that look like this:</p>

<p>Title* : Mr.</p>

<p>Which the regular expression thinks is a sentence and I don't want extracted.</p>

<p>I've also tried combining that regular expression with NLTK's POS tagger to only print out sentences that have both a Noun and a Verb, but I it doesn't seem to work to well as it's just the built in POS tagger and not trained on any dataset.</p>

<p>So I guess my question is: how can I fix my problem?  Am I missing something?</p>
",Preprocessing of the text & Tokenization,extract complete setences email using python thousand email stored either plain text html plain text email formatted pretty much extracting actual email message ha simple html email place finding difficult come mathod extracting body message lot junk email want email wa generated bunch non user generated text change email email way python identify resembles body text complete sentence already tried using regular expression found href regex extracting sentence paragraph python problem wa lot line look like title mr regular expression think sentence want extracted also tried combining regular expression nltk po tagger print sentence noun verb seem work well built po tagger trained dataset guess question fix problem missing something
korean language tokenizer,"<p>What is the best tokenizer exist for processing Korean language?  </p>

<p>I have tried <strong><em><a href=""http://lucene.apache.org/core/old_versioned_docs/versions/2_9_0/api/all/org/apache/lucene/analysis/cjk/CJKTokenizer.html"" rel=""noreferrer"">CJKTokenizer</a> in Solr4.0</em></strong>. It is doing the tokenization, but accuracy is very low.</p>
",Preprocessing of the text & Tokenization,korean language tokenizer best tokenizer exist processing korean language tried cjktokenizer solr tokenization accuracy low
Splitting NLP logic into server and client parts,"<p>I have to create NLP engine with a website frontend. The user should type a sentence in the editbox and receive an answer after sentence analyzing on a server side. I use PHP, Apache, Linux hosting, PostgreSQL. Text analyzer is made as a C++ application with FastCGI technology. Should I make sentence preprocessing on a client side in order to reduce server load?
I mean tokenizing into words, spell checking. If so, what technologies should I use - Javascript, ""invisible"" Java applet..? I intentionally don't describe server parameters, because Linux hosting can be changed. </p>
",Preprocessing of the text & Tokenization,splitting nlp logic server client part create nlp engine website frontend user type sentence editbox receive answer sentence analyzing server side use php apache linux hosting postgresql text analyzer made c application fastcgi technology make sentence preprocessing client side order reduce server load mean tokenizing word spell checking technology use javascript invisible java applet intentionally describe server parameter linux hosting changed
python regex module not working with utf-8 (Devnagari),"<p>I am using python 2.7 for NLP in Bodo Language (uses <a href=""http://en.wikipedia.org/wiki/Devnagari"" rel=""nofollow"">Devnagari</a> script)</p>

<p>In the process of stop word removal, I made a list of stop words in a file separated by newline (""\n""). I used codecs module to read this file and convert to a list.</p>

<pre><code>raw_txt = codecs.open('stopwords.txt', 'r', 'utf-8')
stopWords = []
while(1):
    line = raw_txt.readline()
    if not line:
        break
    line = u''.join(line.strip())
    stopWords.append(line)
</code></pre>

<p>Now I compiled a regular expression to find the matched words:</p>

<pre><code>def addWordBoundary(word):       
    return u''.join(r""\b"" + word + r""\b"")

reg = regex.compile(r""(%s)"" % ""|"".join(map(addWordBoundary, stopWords)), regex.UNICODE)
</code></pre>

<p>I read the corpus (text file) using codecs module to a string and regex.sub() and then write it to a file using codecs itself. But it missed some words. I could not figure out why.</p>

<pre><code>fl = codecs.open('corpus.txt', 'r', 'utf-8')
rawFile = fl.read()

cleanText = reg.sub('', rawFile, regex.U)

wr = codecs.open('output.txt', 'w', 'utf-8')
wr.write(cleanText)
wr.close()
</code></pre>

<p>For testing purpose use this as both stopwords.txt and corpus.txt</p>

<blockquote>
  <p>माब्लानिफ्रायथो<br>
  फारसेनिफ्रायबो<br>
  ﻿रावनिफ्रायबो<br>
  माब्लानिफ्राय<br>
  जेब्लानिफ्राय<br>
  अब्लानिफ्राय<br>
  ﻿इफोरनिफ्राय<br>
  नोंनिफ्रायबो<br>
  फारसेनिफ्राय<br>
  नोंनिफ्रायनो  </p>
</blockquote>

<p>The output.txt file must be a empty file, but it contains:</p>

<blockquote>
  <p>﻿रावनिफ्रायबो<br>
  ﻿इफोरनिफ्राय  </p>
</blockquote>

<p>This code works good for English text (ASCII), so may be I am doing something wrong with utf-8 processing. Please suggest.</p>
",Preprocessing of the text & Tokenization,python regex module working utf devnagari using python nlp bodo language us devnagari script process stop word removal made list stop word file separated newline n used codecs module read file convert list compiled regular expression find matched word read corpus text file using codecs module string regex sub write file using codecs missed word could figure testing purpose use stopwords txt corpus txt output txt file must empty file contains code work good english text ascii may something wrong utf processing please suggest
Package tm stop-word parameter,"<p>I am trying to filter stop-words from the following documents using package <code>tm</code>.</p>

<pre><code>library(tm)
documents &lt;- c(""the quick brown fox jumps over the lazy dog"", ""i am the walrus"")
corpus &lt;- Corpus(VectorSource(documents))
matrix &lt;- DocumentTermMatrix(corpus,control=list(stopwords=TRUE))
</code></pre>

<p>However, when I run this code I still get the following in the <code>DocumentTermMatrix</code>.</p>

<pre><code>colnames(matrix)
[1] ""brown""  ""dog""    ""fox""    ""jumps""  ""lazy""   ""over""   ""quick""  ""the""    ""walrus""
</code></pre>

<p>""The"" is listed as a stop-word in the list that package <code>tm</code> uses. Am I doing something wrong regarding the <code>stopwords</code> parameter, or is this a bug in the <code>tm</code> package?</p>

<p><strong>EDIT:</strong> I contacted Ingo Feinerer and he noted that it is technically not a bug:</p>

<blockquote>
  <p>User-provided options are processed first, and then all remaining
  options. Hence stopword removal is done before tokenization (as
  already written by Vincent Zoonekynd on stackoverflow.com) which gives
  exactly your result.</p>
</blockquote>

<p>Therefore, the solution is to explicitly list the default tokenizing option prior to the <code>stopwords</code> parameter, for example:</p>

<pre><code>library(tm)
documents &lt;- c(""the quick brown fox jumps over the lazy dog"", ""i am the walrus"")
corpus &lt;- Corpus(VectorSource(documents))
matrix &lt;- DocumentTermMatrix(corpus,control=list(tokenize=scan_tokenizer,stopwords=TRUE))
colnames(matrix)
</code></pre>
",Preprocessing of the text & Tokenization,package tm stop word parameter trying filter stop word following document using package however run code still get following listed stop word list package us something wrong regarding parameter bug package edit contacted ingo feinerer noted technically bug user provided option processed first remaining option hence stopword removal done tokenization already written vincent zoonekynd stackoverflow com give exactly result therefore solution explicitly list default tokenizing option prior parameter example
News Article Categorization (Subject / Entity Analysis via NLP?); Preferably in Node.js,"<p>Objective: a node.js function that can be passed a news article (title, text, tags, etc.) and will return a category for that article (""Technology"", ""Fashion"", ""Food"", etc.)</p>

<p>I'm not picky about exactly what categories are returned, as long as the list of possible results is finite and reasonable (10-50).</p>

<p>There are Web APIs that do this (eg, <a href=""http://www.alchemyapi.com/"" rel=""nofollow"">alchemy</a>), but I'd prefer not to incur the extra cost (both in terms of external HTTP requests and also $$) if possible.</p>

<p>I've had a look at the node module ""<a href=""https://github.com/NaturalNode/natural"" rel=""nofollow"">natural</a>"". I'm a bit new to NLP, but it seems like maybe I could achieve this by training a BayesClassifier on a reasonable word list. Does this seem like a good/logical approach? Can you think of anything better?</p>
",Preprocessing of the text & Tokenization,news article categorization subject entity analysis via nlp preferably node j objective node j function passed news article title text tag etc return category article technology fashion food etc picky exactly category returned long list possible result finite reasonable web apis eg alchemy prefer incur extra cost term external http request also possible look node module natural bit new nlp seems like maybe could achieve training bayesclassifier reasonable word list doe seem like good logical approach think anything better
Dealing with homographs when counting n-grams in scikit-learn,"<p>I'm using TfIdfVectorizer to count n-grams in the text, but I need to lemmatize it first. One written form can correspond to different lemmas, so all of them should be counted. How can I deal with it within scikit-learn context? Do I need to write an analyser and pass it to TfIdfVectorizer? How does it work?</p>
",Preprocessing of the text & Tokenization,dealing homograph counting n gram scikit learn using tfidfvectorizer count n gram text need lemmatize first one written form correspond different lemma counted deal within scikit learn context need write analyser pas tfidfvectorizer doe work
Clustering Words,"<p>I have a list of words. I am looking for a way to cluster these words together semantically. Can anyone tell me about a library or tool that accomplishes this? I have been searching this on net a lot but nothing suits my requirements. Most of the words are technical and so won't be found in any dictionary. I want to perform ontological clustering on the list of words.</p>
",Preprocessing of the text & Tokenization,clustering word list word looking way cluster word together semantically anyone tell library tool accomplishes searching net lot nothing suit requirement word technical found dictionary want perform ontological clustering list word
Link a negative in a sentence to the following word by an undercore,"<p>I'm trying to count the number of positive reviews on a website. Consider the following strings:</p>

<pre><code>$str_1 = ""This is great"";
$str_2 = ""This is not great after all"";
$str_3 = ""That isn't good and I will not return to this store."";
</code></pre>

<p>They mean the opposite. In automatic classification, $str_2 and 3 would be counted as positives by most classifiers (who simply count the number of positive words in a sentence). I want to circumvent this error by linking ""not"" with ""great"", ""isn't"" with ""good and ""not with ""return"", as follows:</p>

<pre><code>$str_1 = ""This is great"";
$str_2 = ""This is not_great after all"";
$str_3 = ""That isn't_good and I will not_return to this store."";
</code></pre>

<p>I started by tokenizing the strings:</p>

<pre><code>$tokens = explode("""", $str_3);
</code></pre>

<p>But I don't know how to proceed. How do I link the word AFTER a negative (""not"", ""isn't"") with the following word? Isn't a regex of better use here?</p>
",Preprocessing of the text & Tokenization,link negative sentence following word undercore trying count number positive review website consider following string mean opposite automatic classification str would counted positive classifier simply count number positive word sentence want circumvent error linking great good return follows started tokenizing string know proceed link word negative following word regex better use
Java NLP: Extracting Indicies When Tokenizing Text,"<p>When tokenizing a string of text, I need to extract the indexes of the tokenized words. For example, given:</p>

<pre><code>""Mary didn't kiss John""
</code></pre>

<p>I would need something like:</p>

<pre><code>[(Mary, 0), (did, 5), (n't, 8), (kiss, 12), (John, 17)]
</code></pre>

<p>Where 0, 5, 8, 12 and 17 correspond to the index (in the original string) where the token began. I cannot rely on just whitespace, since some words become 2 tokens. Further, I cannot just search for the token in the string, since the word likely will appear multiple times. </p>

<p>One giant obstacle is that I'm working with ""dirty"" text. Here is a real example from the corpus, and its tokenization:</p>

<p>String:</p>

<pre><code>The child some how builds a boaty  c capable of getting scrtoacross the sea, even after findingovercoming many treachrous rous obsittalcles.
</code></pre>

<p>Tokens:</p>

<pre><code>The, child, some, how, builds, a, boaty, , , c, , capable, of, getting, scrto, , across, the, sea, ,, even, after, finding, , , , , overcoming, many, treachrous, rous, obsittalcles, .
</code></pre>

<p>I'm currently using OpenNLP to tokenize the text, but am ambivalent about which API to utilize for tokenization. It does need to be Java, though, so (unfortunately) Python's NLTK is out of the picture.</p>

<p>Any ideas would be greatly appreciated! Thanks!</p>
",Preprocessing of the text & Tokenization,java nlp extracting indicies tokenizing text tokenizing string text need extract index tokenized word example given would need something like correspond index original string token began rely whitespace since word become token search token string since word likely appear multiple time one giant obstacle working dirty text real example corpus tokenization string token currently using opennlp tokenize text ambivalent api utilize tokenization doe need java though unfortunately python nltk picture idea would greatly appreciated thanks
Is it possible to search for words inside a Lucene index by part of speech,"<p>I have a large set of documents stored inside a Lucene index and I am using a customAnalyzer which basically does tokenization and stemming for the documents content.</p>

<p>Now, if I search inside the documents for the word ""love"", I get results where love is being used either as a noun or a verb, while I want only those documents which use love only as a verb.</p>

<p>How can such s feature be implemented where I could also mention the part-of-speech of the word along with the word so that the results have only love used as a verb and not as a noun?</p>

<p>I can think of a way to initially part-of-speech tag each word of the document and store it by appending the POS with the word with a '_' or something and then to search accordingly, but wanted to know if there is a smarter way to do this in Lucene.</p>
",Preprocessing of the text & Tokenization,possible search word inside lucene index part speech large set document stored inside lucene index using customanalyzer basically doe tokenization stemming document content search inside document word love get result love used either noun verb want document use love verb feature implemented could also mention part speech word along word result love used verb noun think way initially part speech tag word document store appending po word something search accordingly wanted know smarter way lucene
NLTK Regular Expressions and CFGs,"<p>Is there any practical difference in power between a 'regular expression' as exampled by NLTK's docs and a CFG from the same?  There definitely should be, since there are context-free languages which are not regular, but I can't find a concrete example where the CFG approach outshines a regular expression.</p>

<p><a href=""http://nltk.org/book/ch07.html"" rel=""nofollow"">http://nltk.org/book/ch07.html</a></p>
",Preprocessing of the text & Tokenization,nltk regular expression cfgs practical difference power regular expression exampled nltk doc cfg definitely since context free language regular find concrete example cfg approach outshines regular expression
How to replace and count frequency of a word or word sequence?,"<p>I need to do two things, first, find a given text which are the most used word <strong>and</strong> word sequences (limited to <em>n</em>).
Example:</p>

<blockquote>
  <p><strong>Lorem</strong> *<em>ipsum</em>* dolor <strong>sit amet</strong>, consectetur adipiscing elit. Nunc auctor urna sed urna mattis nec interdum magna ullamcorper. Donec ut <strong>lorem</strong> eros, id rhoncus nisl. Praesent sodales <strong>lorem</strong> vitae sapien volutpat et accumsan <strong>lorem</strong> viverra. Proin lectus <strong>elit</strong>, cursus ut feugiat ut, porta <strong>sit amet</strong> leo. Cras est nisl, aliquet quis lobortis <strong>sit amet</strong>, viverra non erat. Vestibulum ante <strong>ipsum</strong> primis in faucibus orci luctus et ultrices posuere cubilia Curae; Integer euismod scelerisque quam, et aliquet nibh dignissim at. <strong>Pellentesque</strong> ut <strong>elit</strong> neque. Etiam facilisis nisl eu mauris luctus in consequat libero volutpat. <strong>Pellentesque</strong> auctor, justo in suscipit mollis, erat justo sollicitudin <strong>ipsum</strong>, in cursus erat <strong>ipsum</strong> id turpis. In tincidunt hendrerit scelerisque.</p>
</blockquote>

<p>(some words my have been omited, but it's an example).</p>

<p>I'd like to result with <strong>sit amet</strong> and not <strong>sit</strong> and <strong>amet</strong></p>

<p>Any ideas on how to start?</p>

<p>Second, I need to wrap all the words or word sequences matched from a given list in a given file.</p>

<p>For this, I think to order the result by desceding length and then process each string in replace function, to avoid having <strong>sit amet</strong> wrapped if I have another <strong>sit</strong> word in my list.
Is it a good way to do?!</p>

<p>Thank you</p>
",Preprocessing of the text & Tokenization,replace count frequency word word sequence need two thing first find given text used word word sequence limited n example lorem ipsum dolor sit amet elit nunc auctor urna sed urna mattis nec interdum magna ullamcorper donec ut lorem eros id rhoncus nisl praesent sodales lorem vitae sapien volutpat et accumsan lorem viverra proin lectus elit cursus ut feugiat ut porta sit amet leo cras est nisl aliquet quis lobortis sit amet viverra non erat vestibulum ante ipsum primis faucibus orci luctus et ultrices posuere cubilia curae integer euismod scelerisque quam et aliquet nibh dignissim pellentesque ut elit neque etiam facilisis nisl eu mauris luctus consequat libero volutpat pellentesque auctor justo suscipit mollis erat justo sollicitudin ipsum cursus erat ipsum id turpis tincidunt hendrerit scelerisque word omited example like result sit amet sit amet idea start second need wrap word word sequence matched given list given file think order result desceding length process string replace function avoid sit amet wrapped another sit word list good way thank
sentiment analysis for arabic language,"<p>I'm looking for analysing sentiment in arabic language , to do that I collect some status from facebook and I classify them into positive and negative , I'm using RapidMiner software so I do the tokenization and stemmig and stop words remove , now I want that my corpus consider the emoticons "" :( and :) "" as a positive and begative sentiment , how to add that to my corpus.</p>

<p><img src=""https://i.sstatic.net/uyvaZ.png"" alt=""enter image description here""></p>

<p>this the model that I used , what operator I have to incorporate in order to deal with smileys as positive and negative sentiment  </p>
",Preprocessing of the text & Tokenization,sentiment analysis arabic language looking analysing sentiment arabic language collect status facebook classify positive negative using rapidminer software tokenization stemmig stop word remove want corpus consider emoticon positive begative sentiment add corpus model used operator incorporate order deal smiley positive negative sentiment
List of stopwords for NLP,"<p>Is there a list of stop words that people usually use to remove punctuations and close class words (such as <code>he, she, it</code>) when performing NLP or IR/IE related task?</p>

<p>I have been trying out topic modeling using gibbs sampling for word sense disambiguation and it keeps giving punctuations and close class words high probabilities just because they appear frequently in the corpus. <a href=""https://github.com/christianscheible/BNB/blob/master/nb_gibbs.py"" rel=""nofollow"">https://github.com/christianscheible/BNB/blob/master/nb_gibbs.py</a> </p>
",Preprocessing of the text & Tokenization,list stopwords nlp list stop word people usually use remove punctuation close class word performing nlp ir ie related task trying topic modeling using gibbs sampling word sense disambiguation keep giving punctuation close class word high probability appear frequently corpus
How is stemming useful?,"<p>Simple question: When do we stem or lemmatize the words? Is stemming helpful for all nlp processes or are there applications where using full form of words might result in better accuracy or precision?</p>
",Preprocessing of the text & Tokenization,stemming useful simple question stem lemmatize word stemming helpful nlp process application using full form word might result better accuracy precision
using regular expression to find polysyllabic words,"<p>i'm trying to use regexp to find the number of polysyllabic words in a piece of text, my code works most of the time but doesn't pick up on some of the poly words:</p>

<pre><code>polySyllableCount = lWords2.replace(/(?:[^laeiouy\s]es|ed|[^laeiouy\s]e)$/, '');
</code></pre>

<p>is what I use to count the syllables, and</p>

<pre><code>polySyllableCount = lWords2.replace(/^y/, '');
</code></pre>

<p>to replace the leading Y's so they are not counted,</p>

<p>and finally:</p>

<pre><code>try 
{
polySyllables = polySyllableCount.match(/[aeiouy]\S[aeiouy]\S[aeiouy]/g).length;
}
catch(err)
{
console.log(""No Poly Words"")
}
</code></pre>

<p>to count the number of polysyllabic words.</p>

<p>My thought process is that it will find any 3 vowels in a (modified) word, separated by anything except a whitespace, to give me the number of polysyllabic words</p>
",Preprocessing of the text & Tokenization,using regular expression find polysyllabic word trying use regexp find number polysyllabic word piece text code work time pick poly word use count syllable replace leading counted finally count number polysyllabic word thought process find vowel modified word separated anything except whitespace give number polysyllabic word
Does stemming harm precision in text classification?,"<p>I have read stemming harms precision but improves recall in text classification. How does that happen? When you stem you increase the number of matches between the query and the sample documents right?</p>
",Preprocessing of the text & Tokenization,doe stemming harm precision text classification read stemming harm precision improves recall text classification doe happen stem increase number match query sample document right
Tokenize and label text,"<p>Here's a simple scanner, that tokenizes text according to certain rules, and labels the tokens.</p>

<ol>
<li>What is the best way to handle unknown characters, and label them as
unknown?</li>
<li>Is there a recommended way/library to speed things up while
accomplishing similar results and remaining relatively simple.</li>
</ol>

<p><em>Example:</em></p>

<pre><code>import re

def alpha(scanner,token):
    return token, 'a'

def numeric(scanner,token):
    return token,'rn'

def punctuation(scanner,token):
    return token, 'p'

def superscript(scanner,token):
    return token, 'sn'

scanner = re.Scanner([
    (u""[a-zA-Z]+"", alpha),
    (u""[.,:;!?]"", punctuation),
    (u""[0-9]+"", numeric),
    (u""[\xb9\u2070\xb3\xb2\u2075\u2074\u2077\u2076\u2079\u2078]"", superscript),
    (r""[\s\n]+"", None), # whitespace, newline
    ])

tokens, _ = scanner.scan(""This is a little test? With 7,9 and 6."")
print tokens
</code></pre>

<p><em>out:</em></p>

<pre><code>[('This', 'a'), ('is', 'a'), ('a', 'a'), ('little', 'a'), ('test', 'a'),
 ('?', 'p'), ('With', 'a'), ('7', 'rn'), (',', 'p'), ('9', 'rn'), 
 ('and', 'a'), ('6', 'rn'), ('.', 'p')]
</code></pre>

<p><em>ps!</em> Defined functions will probably try to categorize the tokens further.</p>
",Preprocessing of the text & Tokenization,tokenize label text simple scanner tokenizes text according certain rule label token best way handle unknown character label unknown recommended way library speed thing accomplishing similar result remaining relatively simple example p defined function probably try categorize token
Stemming that avoids matching popular words with different meanings,"<p>I'm currently using the PorterStemmer to identify derived words. However, I'm facing an issue with words that don't have the same meaning but appear to have. For example:<br>
<code>Market</code> and <code>Marketing</code><br>
<code>Wine</code> and <code>Winning</code><br>
etc..<br>
have different meanings, but the PorterStemmer identifies them as the same.</p>

<p>Which open tools are able to overcome such issues? A dictionary with corner-cases? A more advanced stemmer?</p>

<p>Preferably something that is easily accessible through PHP. </p>
",Preprocessing of the text & Tokenization,stemming avoids matching popular word different meaning currently using porterstemmer identify derived word however facing issue word meaning appear example etc different meaning porterstemmer identifies open tool able overcome issue dictionary corner case advanced stemmer preferably something easily accessible php
Stanford coreNLP - split words ignoring apostrophe,"<p>I'm trying to split a sentence into words using Stanford coreNLP .
I'm having problem with words that contains apostrophe.</p>

<p>For example, the sentence:
I'm 24 years old.</p>

<p>Splits like this:
[I] ['m] [24] [years] [old]</p>

<p>Is it possible to split it like this using Stanford coreNLP?:
[I'm] [24] [years] [old]</p>

<p>I've tried using tokenize.whitespace, but it doesn't split on other punctuation marks like: '?' and ','</p>
",Preprocessing of the text & Tokenization,stanford corenlp split word ignoring apostrophe trying split sentence word using stanford corenlp problem word contains apostrophe example sentence year old split like year old possible split like using stanford corenlp year old tried using tokenize whitespace split punctuation mark like
lucene support for common NLP tasks,"<p>I am working on a text mining project which plans to integrate Lucene later. My current implementation uses openNLP for the common NLP tasks, such as tokenization, building n-gram features. I am curious to know that whether Lucene can support these functionalities? Does Lucene can achieve high efficiency for large scale document collections when comparing to openNLP?</p>
",Preprocessing of the text & Tokenization,lucene support common nlp task working text mining project plan integrate lucene later current implementation us opennlp common nlp task tokenization building n gram feature curious know whether lucene support functionality doe lucene achieve high efficiency large scale document collection comparing opennlp
Is there a set of adjective word list for positive or negative polarity,"<p>I am working on sentiment analysis. I thought if there is any available set of adjectives indicating positive/negative(like for positive: good,awesome,amazing,) meaning? and the second thing is a set of data from which i can use as a test case.</p>
",Preprocessing of the text & Tokenization,set adjective word list positive negative polarity working sentiment analysis thought available set adjective indicating positive negative like positive good awesome amazing meaning second thing set data use test case
Oracle text curly braces behavior,"<p>I'm using oracle text to do a readahead (according to the spec writer) in the search bar.</p>

<p>Basically, a user can start typing text and we fill the suggestions bar with likely matches.</p>

<p>I tried using oracle text for this, and ran into some issues, and the latest one being:</p>

<p>Table contains this entry for answertext: ... we offer many pricing options ...</p>

<pre><code>SELECT
    questiontext as qtext,
   answertext as text,
   questionid FROM question
   WHERE contains(answertext, '{pric}', 1) &gt; 0

;
</code></pre>

<p>This query returns nothing. But using {pricing} will return the correct result.</p>

<p>And suggestion why this is happening would be great!</p>

<p>Edit: just wanted to add that using stemming does not work for me because the user wants to differentiate between ""report"" and ""reporting"" and they want the matching substring to be highlighted which can be done if I can find the substring among the returned results.</p>

<p>Edit 2: I have my guess, that oracle tokenizes each word using word boundary of some sort in the index, and thus without any wildcards it looks for a token that equals = 'pric' and therefore does not find it (because there is a token 'pricing'). So, if that guess is correct I would love if someone can chime in for how I can make the query above work with the example entry while still maintaining whitespace so if type 'pricing options' it should return but if i type 'many options' it should not...</p>
",Preprocessing of the text & Tokenization,oracle text curly brace behavior using oracle text readahead according spec writer search bar basically user start typing text fill suggestion bar likely match tried using oracle text ran issue latest one table contains entry answertext offer many pricing option query return nothing using pricing return correct result suggestion happening would great edit wanted add using stemming doe work user want differentiate report reporting want matching substring highlighted done find substring among returned result edit guess oracle tokenizes word using word boundary sort index thus without wildcards look token equal pric therefore doe find token pricing guess correct would love someone chime make query work example entry still maintaining whitespace type pricing option return type many option
Matching multiple overlapping n-grams with a single regular expression,"<p>I am trying to perform regular expressions to match multiple n-grams in a document. 
I first get a list of n-grams, I compile them into a regex like so:</p>

<pre><code>sNgrams = '|'.join(('\s+'.join(re.escape(gram) for gram in nGram.split())) for nGram in aNgrams)
</code></pre>

<p>(Splitting the n-grams to tokens on any whitespace character, re.escape these tokens and join them with '\s+'-es (so I can match the ngrams over newlines, double spaces, tabs and whatnot), and then join the n-grams with '|' for the regex)</p>

<p>My regex looks like this:</p>

<pre><code>reNgram = re.compile('(\A|\W+)(' + sNgrams + ')(?=\W+|\Z)',flags=re.UNICODE|re.IGNORECASE)
</code></pre>

<p>Now, this works fine for most cases, however, when an n-gram overlaps with another one, only one match is found:</p>

<pre><code>doc = 'aap noot mies'

aNgrams = ['aap','noot','aap noot']
sNgrams = 'aap|noot|aap\\s+noot'
re.findall(reNgram,doc)
[('', 'aap'), (' ', 'noot')]

aNgrams = ['mies','aap noot']
re.findall(reNgram,doc)
[('', 'aap noot'), (' ', 'mies')]
</code></pre>

<ul>
<li><p>Is there any way to solve this? To return all (sub)strings that match
in the document? </p></li>
<li><p>Furthermore, speed/efficiency is of high importance
(I'm firing tens of thousands of these regexes), is there anything I
can do to optimize? I've read that pre-compiling the regexes doesn't
do much, as the 'on-the-fly' compiled regexes are cached anyway, are there any (other) obvious steps I can take to speed up these expressions?</p></li>
</ul>
",Preprocessing of the text & Tokenization,matching multiple overlapping n gram single regular expression trying perform regular expression match multiple n gram document first get list n gram compile regex like splitting n gram token whitespace character escape token join e match ngrams newlines double space tab whatnot join n gram regex regex look like work fine case however n gram overlap another one one match found way solve return sub string match document furthermore speed efficiency high importance firing ten thousand regexes anything optimize read pre compiling regexes much fly compiled regexes cached anyway obvious step take speed expression
Real time text processing using Python,"<p>Real time text processing using Python. For e.g. consider this sentance</p>

<pre>
I am going to schol today
</pre>

<p>I want to do the following (real time):</p>

<pre>
1) tokenize 
2) check spellings
3) stem(nltk.PorterStemmer()) 
4) lemmatize (nltk.WordNetLemmatizer())
</pre> 

<p>Currently I am using <a href=""http://www.nltk.org/"" rel=""nofollow"">NLTK</a> library to do these operations, but its not real time (meaning its taking few seconds to complete these operations). I am processing 1 sentence at a time, Is it possible to make it efficient</p>

<p>Update:
Profiling: </p>

<pre>
Fri Jul  8 17:59:32 2011    srj.profile

         105503 function calls (101919 primitive calls) in 1.743 CPU seconds

   Ordered by: internal time
   List reduced from 1797 to 10 due to restriction 

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
     7450    0.136    0.000    0.208    0.000 sre_parse.py:182(__next)
  602/179    0.130    0.000    0.583    0.003 sre_parse.py:379(_parse)
23467/22658    0.122    0.000    0.130    0.000 {len}
 1158/142    0.092    0.000    0.313    0.002 sre_compile.py:32(_compile)
    16152    0.081    0.000    0.081    0.000 {method 'append' of 'list' objects}
     6365    0.070    0.000    0.249    0.000 sre_parse.py:201(get)
     4947    0.058    0.000    0.086    0.000 sre_parse.py:130(__getitem__)
 1641/639    0.039    0.000    0.055    0.000 sre_parse.py:140(getwidth)
      457    0.035    0.000    0.103    0.000 sre_compile.py:207(_optimize_charset)
     6512    0.034    0.000    0.034    0.000 {isinstance}

</pre>

<p>timit:</p>

<pre>
t = timeit.Timer(main)
print t.timeit(1000)

=> 3.7256231308
</pre>
",Preprocessing of the text & Tokenization,real time text processing using python real time text processing using python e g consider sentance going schol today want following real time tokenize check spelling stem nltk porterstemmer lemmatize nltk wordnetlemmatizer currently using nltk library operation real time meaning taking second complete operation processing sentence time possible make efficient update profiling fri jul srj profile function call primitive call cpu second ordered internal time list reduced due restriction ncalls tottime percall cumtime percall filename lineno function sre parse py next sre parse py parse len sre compile py compile method append list object sre parse py get sre parse py getitem sre parse py getwidth sre compile py optimize charset isinstance timit timeit timer main print timeit
nltk tokenizing a 175 mb file,"<p>Does nltk have some serialization format for writing out tokenized text ? I have a 175mb text file and getting it into the <code>nltk.Text</code> object takes me 4 minutes (on a macbook retina -- i.e., cutting edge processor, 8 gigs of ram and a SSD). Loading the raw file from disk is instantaneous almost.</p>

<p>The functions that do the work are as follows : </p>

<pre><code>def _load_all_text(self):
    if not self._text_loaded:
        file = open(""all_posts"",""r"")
        self._text = file.read()
        self._text_loaded = True

def nltk_text(self):
    self._load_all_text()
    return nltk.Text(nltk.word_tokenize(self._text))
</code></pre>

<p>I can't believe it takes 4 minutes to get done, I guess it's because of the python garbage collector and the list object, which nltk builds on. I don't know much about pickling, would pickling the list do the trick (--i.e., the list in question is the result of <code>word_tokenise</code>)? </p>
",Preprocessing of the text & Tokenization,nltk tokenizing mb file doe nltk serialization format writing tokenized text mb text file getting object take minute macbook retina e cutting edge processor gig ram ssd loading raw file disk instantaneous almost function work follows believe take minute get done guess python garbage collector list object nltk build know much pickling would pickling list trick e list question result
How do i Tokenize a string?,"<p>I want my program to know if the string is a Noun, verb , adjective or etc. and it needs to be tokenized but how do i do that?</p>

<p>Do i need a Database for the definition of words? using Free context grammar?</p>

<p>Thank you.</p>
",Preprocessing of the text & Tokenization,tokenize string want program know string noun verb adjective etc need tokenized need database definition word using free context grammar thank
Compatibility of Stemmers between NLTK and Lucene,"<p>I'm using Lucene in Java to index a corpus and extract stemmed wordlists from it. I stem using the EnglishAnalyzer. Then I hand the wordlist to Python to do some things with NLTK.
Is there a stemmer in NLTK that is fully compatible with the stemmer used by Lucene's EnglishAnalyzer?</p>

<p>I know I could also use PyLucene to circumvent this, but I would like to minimize dependencies.</p>
",Preprocessing of the text & Tokenization,compatibility stemmer nltk lucene using lucene java index corpus extract stemmed wordlists stem using englishanalyzer hand wordlist python thing nltk stemmer nltk fully compatible stemmer used lucene englishanalyzer know could also use pylucene circumvent would like minimize dependency
Stanford POS Tagger: How to preserve newlines in the output?,"<p>My input.txt file contains the following sample text:</p>

<p>you have to let's<br>
come and see me.</p>

<p>Now if I invoke the Stanford POS tagger with the default command:</p>

<pre><code>java -classpath stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -model models/wsj-0-18-bidirectional-distsim.tagger -textFile input.txt &gt; output.txt
</code></pre>

<p>I get the following in my output.txt file:</p>

<pre><code>you_PRP have_VBP to_TO let_VB 's_POS come_VB and_CC see_VB me_PRP ._.
</code></pre>

<p>The problem with the above output is that I have lost my original newline delimiter used in the input file.</p>

<p>Now, if I use the following command to preserve my newline sentence delimiter in the output file I have to set -tokenize option to false:</p>

<pre><code>java -classpath stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -model models/wsj-0-18-bidirectional-distsim.tagger -sentenceDelimiter newline -tokenize false -textFile input.txt &gt; output.txt 
</code></pre>

<p>The problem with this code is that it totally messed up the output:</p>

<pre><code>you_PRP have_VBP to_TO let's_NNS  
come_VB and_CC see_VB me._NN
</code></pre>

<p>Here let's and me. are tagged inappropriately.</p>

<p>My question is how can I preserve the newline delimiters in the output file without messing up the tokenization?</p>
",Preprocessing of the text & Tokenization,stanford po tagger preserve newlines output input txt file contains following sample text let come see invoke stanford po tagger default command get following output txt file problem output lost original newline delimiter used input file use following command preserve newline sentence delimiter output file set tokenize option false problem code totally messed output let tagged inappropriately question preserve newline delimiters output file without messing tokenization
"Tokenization, and indexing with Lucene, how to handle external tokenize and part-of-speech?","<p>i would like to build my own - here am not sure which one - tokenizer (from Lucene point of view) or my own analyzer. I already write a code that tokenize my documents in word (as a List &lt; String > or a List &lt; <em>Word</em> > where <em>Word</em> is a class with only a kind of container with 3 public String : word, pos, lemma - pos stand for part-of-speech tag).</p>

<p>i'm not sure what i am going to index, maybe only ""<em>Word.lemma</em>"" or something like ""<em>Word.lemma + '#' + Word.pos</em>"", probably i will do some filtering from a stop word list based on part-of-speech.</p>

<p>btw here is my misunderstanding : i am not sure where i should plug to the Lucene API,</p>

<p>should i wrap my own tokenizer inside a new tokenizer ? should i rewrite TokenStream ? should i consider that this is the job of the analyzer rather than the tokenizer ? or shoud i bypass everything and directly build my index by adding my word directly inside index, using IndexWriter, Fieldable and so on ? (if so do you know of any documentation on how to create it's own index from scratch when bypass ing the analysis process)</p>

<p>best regards</p>

<p><strong>EDIT</strong> : may be the simplest way should be to org.apache.commons.lang.StringUtils.join my <em>Word</em>-s with a space on the exit of my personal tokenizer/analyzer and rely on the WhiteSpaceTokenizer to feed Lucene (and other classical filters) ?</p>

<p><strong>EDIT</strong> : so, i have read <strong>EnglishLemmaTokenizer</strong> pointed by <strong>Larsmans</strong>... but where i am still confused, is the fact that i end my own analysis/tokenization process with a complete *List &lt; Word > * (<em>Word</em> class wrapping <em>.form/.pos/.lemma</em>) , this process rely on an external binary that i had wrapped in Java (this is a must do / can not do otherwise - it is not on a consumer point of view, i get the full list as a result) and i still not see how i should wrap it again to get back to the normal Lucene analysis process.</p>

<p>also i will be using the TermVector feature with TF.IDF like scoring (may be redefining my own), i may also be interested in the proximty searching, thus, discarding some words from their part-of- speech before providing them to a Lucene built-in tokenizer or internal analyzer may seem a bad idea.  And i have difficulties in thinking of a ""proper"" way to wrap a Word.form / Word.pos / Word.lemma (or even other Word.anyOtherUnterestingAttribute) to the Lucene way.</p>

<p><strong>EDIT:</strong> 
BTW, here is a piece of code that i write inspired by the one of @Larsmans :</p>

<pre><code>class MyLuceneTokenizer extends TokenStream {

    private PositionIncrementAttribute posIncrement;
    private CharTermAttribute termAttribute;

    private List&lt;TaggedWord&gt; tagged;
    private int position;

    public MyLuceneTokenizer(Reader input, String language, String pathToExternalBinary) {
        super();

        posIncrement = addAttribute(PositionIncrementAttribute.class);
        termAttribute = addAttribute(CharTermAttribute.class); // TermAttribute is deprecated!

        // import com.google.common.io.CharStreams;            
        text = CharStreams.toString(input); //see http://stackoverflow.com/questions/309424/in-java-how-do-i-read-convert-an-inputstream-to-a-string
        tagged = MyTaggerWrapper.doTagging(text, language, pathToExternalBinary);
        position = 0;
    }

    public final boolean incrementToken()
            throws IOException {
        if (position &gt; tagged.size() -1) {
            return false;
        }

        int increment = 1; // will probably be changed later depending upon any POS filtering or insertion @ same place...
        String form = (tagged.get(position)).word;
        String pos = (tagged.get(position)).pos;
        String lemma = (tagged.get(position)).lemma;

        // logic filtering should be here...
        // BTW we have broken the idea behing the Lucene nested filters or analyzers! 
        String kept = lemma;

        if (kept != null) {
            posIncrement.setPositionIncrement(increment);
            char[] asCharArray = kept.toCharArray();
            termAttribute.copyBuffer(asCharArray, 0, asCharArray.length);
            //termAttribute.setTermBuffer(kept);
            position++;
        }

        return true;
    }
}

class MyLuceneAnalyzer extends Analyzer {
    private String language;
    private String pathToExternalBinary;

    public MyLuceneAnalyzer(String language, String pathToExternalBinary) {
        this.language = language;
        this.pathToExternalBinary = pathToExternalBinary;
    }

    @Override
    public TokenStream tokenStream(String fieldname, Reader input) {
        return new MyLuceneTokenizer(input, language, pathToExternalBinary);
    }
}
</code></pre>
",Preprocessing of the text & Tokenization,tokenization indexing lucene handle external tokenize part speech would like build sure one tokenizer lucene point view analyzer already write code tokenize document word list string list word word class kind container public string word po lemma po stand part speech tag sure going index maybe word lemma something like word lemma word po probably filtering stop word list based part speech btw misunderstanding sure plug lucene api wrap tokenizer inside new tokenizer rewrite tokenstream consider job analyzer rather tokenizer shoud bypass everything directly build index adding word directly inside index using indexwriter fieldable know documentation create index scratch bypass ing analysis process best regard edit may simplest way org apache common lang stringutils join word space exit personal tokenizer analyzer rely whitespacetokenizer feed lucene classical filter edit read englishlemmatokenizer pointed larsmans still confused fact end analysis tokenization process complete list word word class wrapping form po lemma process rely external binary wrapped java must otherwise consumer point view get full list result still see wrap get back normal lucene analysis process also using termvector feature tf idf like scoring may redefining may also interested proximty searching thus discarding word part speech providing lucene built tokenizer internal analyzer may seem bad idea difficulty thinking proper way wrap word form word po word lemma even word anyotherunterestingattribute lucene way edit btw piece code write inspired one larsmans
NLTK CorpusReader tokenize one file at the time,"<p>I have corpus of several hundred of documents and I am using NLTK PlaintextCorpusReader to process these files. The only problem is that I need to handle one file at the time in <code>for</code> cycle so I could count the similarity of these documents.</p>

<p>If I initialize the reader like this
<code>corpusReader = PlaintextCorpusReader(root, fileids = "".*"")</code> it just consumes all the documents and I can't find a way how to iterate over files instead of tokens.</p>

<p>One solution could be to initialize corpusReader for each file, iterate over its tokens and then again create new reader for another file but I think that this isn't very efficient way to process such large data.</p>

<p>Thanks for any advice :)</p>
",Preprocessing of the text & Tokenization,nltk corpusreader tokenize one file time corpus several hundred document using nltk plaintextcorpusreader process file problem need handle one file time cycle could count similarity document initialize reader like consumes document find way iterate file instead token one solution could initialize corpusreader file iterate token create new reader another file think efficient way process large data thanks advice
Identifying important words and phrases in text,"<p>I have text stored in a python string.</p>

<p><strong>What I Want</strong></p>

<ol>
<li>To identify key words in that text.</li>
<li>to identify N-grams in that text (ideally more than just bi and tri grams).</li>
</ol>

<p>Keep in mind...</p>

<ul>
<li>The text might be small (e.g. tweet sized)</li>
<li>The text might be middle (e.g. news article sized)</li>
<li>The text might be large (e.g. book or chapter sized)</li>
</ul>

<p><strong>What I Have</strong></p>

<p>I'm already using <a href=""http://www.nltk.org/"" rel=""noreferrer"">nltk</a> to break the corpus into tokens and remove stopwords:</p>

<pre><code>    # split across any non-word character
    tokenizer = nltk.tokenize.RegexpTokenizer('[^\w\']+', gaps=True)

    # tokenize
    tokens = tokenizer.tokenize(text)

    # remove stopwords
    tokens = [w for w in tokens if not w in nltk.corpus.stopwords.words('english')]
</code></pre>

<p>I'm aware of the BigramCollocationFinder and TrigramCollectionFinder which does exaclty what I'm looking for for those two cases.</p>

<p><strong>The Question</strong></p>

<p>I need advice for n-grams of higher order, improving the kinds of results that come from BCF and TCF, and advice on the best way to identify the most unique individual key words.</p>

<p>Many thanks!</p>
",Preprocessing of the text & Tokenization,identifying important word phrase text text stored python string want identify key word text identify n gram text ideally bi tri gram keep mind text might small e g tweet sized text might middle e g news article sized text might large e g book chapter sized already using nltk break corpus token remove stopwords aware bigramcollocationfinder trigramcollectionfinder doe exaclty looking two case question need advice n gram higher order improving kind result come bcf tcf advice best way identify unique individual key word many thanks
What area of machine learning should I look into to automatically extract certain info from messages,"<p>I have an app that extracts information from incoming messages.  The messages all contain the same information, but they have different forms depending on the source that sent them. </p>

<p>Example:</p>

<p>Message from source A : </p>

<pre><code>A: You spent $50.00 at Macy's on 2/20/12
</code></pre>

<p>Message from source B : </p>

<pre><code>Purchase, $50.00, Macy's, 2Feb2012, Balance $5000.00
</code></pre>

<p>Every message from a single source has the same form though. So at the moment, I'm doing it by writing a set of regular expressions to first identify which message I'm trying to decode (i.e. what source it came from so I know what the form of the message is), and then extracting the necessary information from the message (in the above example, I want to know the transaction amount, the store where the transaction happened, and the date).  If I discover a new source for a message, or a source changes the format of their message (doesn't happen very often, but could happen), I need to manually write the regular expressions for that message. I'm sure however that I could automate this using some kind of machine learning technique.  I just don't know much about machine learning, and I don't know where to even start looking for a technique that would apply to my problem.  I would like someone to just point me in the right direction on where to start reading.</p>
",Preprocessing of the text & Tokenization,area machine learning look automatically extract certain info message app extract information incoming message message contain information different form depending source sent example message source message source b every message single source ha form though moment writing set regular expression first identify message trying decode e source came know form message extracting necessary information message example want know transaction amount store transaction happened date discover new source message source change format message happen often could happen need manually write regular expression message sure however could automate using kind machine learning technique know much machine learning know even start looking technique would apply problem would like someone point right direction start reading
How to implement query searching in a specific cluster after document clustering?,"<p>I have two clusters as a class which has </p>

<pre><code>Cluster : class

DocumentList : List&lt;Document&gt;
centroidVector : Map&lt;String,Double&gt;
</code></pre>

<p>Now the problem is that when the query is searched it is parsed as a file and then made into a document object , added to documentIndex and its index is constructed along with other documents . I did that because it had to go through the same procedure i.e tokenizing ,stemming etc. But now i want to implement query search in a specific cluster with which the query vector is most similar with , i.e dot product ~ 0.5 -1 . So i would have to take a dot product between the query vector and the cluster vector to do that. But i dont know how to implement it because the index is created in memory and is not stored in the database. Still in the process of doing that .</p>

<p>Thank you </p>
",Preprocessing of the text & Tokenization,implement query searching specific cluster document clustering two cluster class ha problem query searched parsed file made document object added documentindex index constructed along document go procedure e tokenizing stemming etc want implement query search specific cluster query vector similar e dot product would take dot product query vector cluster vector dont know implement index created memory stored database still process thank
Using WordNet to determine semantic similarity between two texts?,"<p>How can you determine the semantic similarity between two texts in python using WordNet? </p>

<p>The obvious preproccessing would be removing stop words and stemming, but then what?</p>

<p>The only way I can think of would be to calculate the WordNet path distance between each word in the two texts. This is standard for unigrams. But these are large (400 word) texts, that are natural language documents, with words that are not in any particular order or structure (other than those imposed by English grammar). So, which words would you compare between texts? How would you do this in python? </p>
",Preprocessing of the text & Tokenization,using wordnet determine semantic similarity two text determine semantic similarity two text python using wordnet obvious preproccessing would removing stop word stemming way think would calculate wordnet path distance word two text standard unigrams large word text natural language document word particular order structure imposed english grammar word would compare text would python
What approch for simple text processing in Haskell?,"<p>I am trying to do some simple text processing in Haskell, and I am wondering what might me the best way to go about this in an FP language. I looked at the parsec module, but this seems much more sophisticated than I am looking for as a new Haskeller. What would be the best way to strip all the punctuation from a corpus of text? My naive approach was to make a function like this:</p>

<pre><code>removePunc str = [c | c &lt;- str, c /= '.',
                                 c /= '?',
                                 c /= '.',
                                 c /= '!',
                                 c /= '-',
                                 c /= ';',
                                 c /= '\'',
                                 c /= '\""',]
</code></pre>
",Preprocessing of the text & Tokenization,approch simple text processing haskell trying simple text processing haskell wondering might best way go fp language looked parsec module seems much sophisticated looking new haskeller would best way strip punctuation corpus text naive approach wa make function like
Discover user behind multiple different user accounts according to words he uses,"<p>I would like to create algorithm to distinguish the persons writing on forum under different nicknames.</p>

<p>The goal is to discover people registring new account to flame forum anonymously, not under their main account.</p>

<p>Basicaly I was thinking about stemming words they use and compare users according to similarities or these words.</p>

<p><img src=""https://i.sstatic.net/ggqW0.png"" alt=""Users using words""></p>

<p>As shown on the picture there is user3 and user4 who uses same words. It means there is probably one person behind the computer.</p>

<p>Its clear that there are lot of common words which are being used by all users. So I should focus on ""user specific"" words.</p>

<p>Input is (related to the image above):</p>

<pre><code>&lt;word1, user1&gt;
&lt;word2, user1&gt;
&lt;word2, user2&gt;
&lt;word3, user2&gt;
&lt;word4, user2&gt;
&lt;word5, user3&gt;
&lt;word5, user4&gt;
... etc. The order doesnt matter
</code></pre>

<p>Output should be:</p>

<pre><code>user1
user2
user3 = user4
</code></pre>

<p>I am doing this in Java but I want this question to be language independent.</p>

<p><strong>Any ideas how to do it?</strong></p>

<p>1) how to store words/users? What data structures?</p>

<p>2) how to get rid of common words everybody use? I have to somehow ignore them among user specific words. Maybe I could just ignore them because they get lost. I am afraid that they will hide significant difference of ""user specific words""</p>

<p>3) how to recognize same users? - somehow count same words between each user?</p>

<p>I am very thankful for every advice in advance.</p>
",Preprocessing of the text & Tokenization,discover user behind multiple different user account according word us would like create algorithm distinguish person writing forum different nickname goal discover people registring new account flame forum anonymously main account basicaly wa thinking stemming word use compare user according similarity word shown picture user user us word mean probably one person behind computer clear lot common word used user focus user specific word input related image output java want question language independent idea store word user data structure get rid common word everybody use somehow ignore among user specific word maybe could ignore get lost afraid hide significant difference user specific word recognize user somehow count word user thankful every advice advance
Phrase corpus for sentimental analysis,"<p>Good day,
I'm attempting to write a sentimental analysis application in python (Using naive-bayes classifier) with the aim to categorize phrases from news as being positive or negative.
And I'm having a bit of trouble finding an appropriate corpus for that.
I tried using ""General Inquirer"" (http://www.wjh.harvard.edu/~inquirer/homecat.htm) which works OK but I have one big problem there.
Since it is a word list, not a phrase list I observe the following problem when trying to label the following sentence:</p>

<blockquote>
  <p>He is not expected to win.</p>
</blockquote>

<p>This sentence is categorized as being positive, which is wrong. The reason for that is that ""win"" is positive, but ""not"" does not carry any meaning since ""not win"" is a phrase.
Can anyone suggest either a corpus or a work around for that issue?
Your help and insight is greatly appriciated. </p>
",Preprocessing of the text & Tokenization,phrase corpus sentimental analysis good day attempting write sentimental analysis application python using naive bayes classifier aim categorize phrase news positive negative bit trouble finding appropriate corpus tried using general inquirer work ok one big problem since word list phrase list observe following problem trying label following sentence expected win sentence categorized positive wrong reason win positive doe carry meaning since win phrase anyone suggest either corpus work around issue help insight greatly appriciated
Regex replace for large number of search replace pairs,"<p>I want to be able to do large scale search and replace across documents for purposes of text normalisation.</p>

<p>For example: </p>

<ol>
<li>Find all uses of <strong>U.S.A</strong>, <strong>USA</strong> and replace with <strong>United States Of America</strong></li>
<li>Find all ampersands (&amp;) and replace with the word <strong>and</strong>    </li>
</ol>

<p>I also want to be able to add new rules to the system without having to change any code. So the search replace pairs are stored in a data store that means anyone could add,update,remove the rules. </p>

<p>I've been working with the Python re module which is quite nice and ideally I would want to pass a list of tuples to the sub command for it to then go through each one and do the replacements. Is there a better way to do this other than iterating over a list of tuples and then creating a regular expression for each - it's very slow and inefficient especially with large documents:</p>

<pre><code>replacements = [
  r('USA','United States Of America'),
  (r'U\.S\.A','United States Of America'),
  (r'US of A', 'United States of America')]

for replacement in replacements:
  document = re.sub(replacement[0],replacement[1],document
</code></pre>
",Preprocessing of the text & Tokenization,regex replace large number search replace pair want able large scale search replace across document purpose text normalisation example find us u usa replace united state america find ampersand replace word also want able add new rule system without change code search replace pair stored data store mean anyone could add update remove rule working python module quite nice ideally would want pas list tuples sub command go one replacement better way iterating list tuples creating regular expression slow inefficient especially large document
Regexp for Tokenizing English Text,"<p>What would be the best regular expression for tokenizing an English text?</p>

<p>By an English token, I mean an atom consisting of maximum number of characters that can be meaningfully used for NLP purposes. An analogy is a ""token"" in any programming language (e.g. in C, '{', '[', 'hello', '&amp;', etc. can be tokens). There is one restriction: Though English punctuation characters can be ""meaningful"", let's ignore them for the sake of simplicity when they do not appear in the middle of \w+. So, ""Hello, world."" yields 'hello' and 'world'; similarly, ""You are good-looking."" may yield either [you, are, good-looking] or [you, are, good, looking].</p>
",Preprocessing of the text & Tokenization,regexp tokenizing english text would best regular expression tokenizing english text english token mean atom consisting maximum number character meaningfully used nlp purpose analogy token programming language e g c hello etc token one restriction though english punctuation character meaningful let ignore sake simplicity appear middle w hello world yield hello world similarly good looking may yield either good looking good looking
Any better pre processing library or implementation in python?,"<p>I need to pre-process some text documents so that I can apply classification techniques like fcm e.t.c and other topic modeling techniques like latent dirichlet allocation e.t.c</p>

<p>To elaborate a bit in preprocessing I need to remove the stop words, extract the nouns and keywords and perform stemming. The code which I used for this purpose is:</p>

<pre><code>#--------------------------------------------------------------------------
#Extracting nouns
#--------------------------------------------------------------------------
for i in range (0,len(a)) :
    x=a[i]          
    text=nltk.pos_tag(nltk.Text(nltk.word_tokenize(x)))
    for noun in text:
        if(noun[1]==""NN"" or noun[1]==""NNS""):
            temp+=noun[0]
            temp+=' '
documents.append(temp)
print documents

#--------------------------------------------------------------------------
#remove unnecessary words and tags
#--------------------------------------------------------------------------

texts = [[word for word in document.lower().split() if word not in stoplist]for    document in documents]
allTokens = sum(texts, [])
tokensOnce = set(word for word in set(allTokens) if allTokens.count(word)== 0)
texts = [[word for word in text if word not in tokensOnce]for text in texts]
print texts

#--------------------------------------------------------------------------
#Stemming
#--------------------------------------------------------------------------

for i in texts:
    for j in range (0,len(i)):        
        k=porter.stem(i[j])
        i[j]=k
print texts
</code></pre>

<p>The problem with the code I mentioned above is </p>

<ol>
<li>The nltk module used for extracting nouns and keywords is missing many words.
For example the pre-processing was performed on some documents and names like 'Sachin' were not recognized as keywords and missed after pre-processing.</li>
<li>The words are not properly stemmed. There is either too much stemming (network and networking to net) and some times nouns are also stemmed.</li>
</ol>

<p>Is there any better module for the functions needed or is there any better implementation of the same module?
Kindly help  </p>
",Preprocessing of the text & Tokenization,better pre processing library implementation python need pre process text document apply classification technique like fcm e c topic modeling technique like latent dirichlet allocation e c elaborate bit preprocessing need remove stop word extract noun keywords perform stemming code used purpose problem code mentioned nltk module used extracting noun keywords missing many word example pre processing wa performed document name like sachin recognized keywords missed pre processing word properly stemmed either much stemming network networking net time noun also stemmed better module function needed better implementation module kindly help
Better approach to filtering Wikipedia edits,"<p>When you are watching for news of particular <a href=""http://en.wikipedia.org/wiki/Kristen_Stewart"" rel=""nofollow"">Wikipedia article</a> via its <a href=""http://en.wikipedia.org/w/index.php?title=Kristen_Stewart&amp;feed=atom&amp;action=history"" rel=""nofollow"">RSS channel</a>,
its annoying without filtering the information, because most of the edits is spam,
vandalism, minor edits etc.</p>

<p>My approach is to create filters. I decided to remove all edits that don't contain a nickname of the contributor but are identified only by the IP address of the contributor, because most of such edits is spam (though there are some good contributions). This was easy to do with regular expressions.
I also removed edits that contained vulgarisms and other typical spam keywords.</p>

<p>Do you know some better approach utilizing algorithms or heuristics with regular expressions, AI, text-processing techniques etc.? The approach should be able to detect bad posts (minor edits or vandalisms) and should be able to incrementally learn what is good/bad contribution and update its database.</p>

<p>thank you</p>
",Preprocessing of the text & Tokenization,better approach filtering wikipedia edits watching news particular wikipedia article via r channel annoying without filtering information edits spam vandalism minor edits etc approach create filter decided remove edits contain nickname contributor identified ip address contributor edits spam though good contribution wa easy regular expression also removed edits contained vulgarism typical spam keywords know better approach utilizing algorithm heuristic regular expression ai text processing technique etc approach able detect bad post minor edits vandalism able incrementally learn good bad contribution update database thank
Difference between word stemming and depluralization,"<p>In understanding string matching: What is the exact difference between <em>word stemming</em> and <em>depluralization?</em></p>

<p>Or do they mean the same thing?</p>
",Preprocessing of the text & Tokenization,difference word stemming depluralization understanding string matching exact difference word stemming depluralization mean thing
Why the 30 topics identified by Stanford Topic Modeling Toolkit so similar to each other?,"<p>What can be the possible reasons why the 30 topics identified by Stanford
Topic Modeling Toolkit (it took ~4 hours) on the corpus of 19,500
articles (shared by Twitter users) so similar to each other? They
have pretty much the same terms, and frequencies => essentially, I
just have a single topic :)</p>

<p>The topics are identified can be found <a href=""http://www.cs.princeton.edu/~asuleime/data/summary.txt"" rel=""nofollow"">here</a></p>

<p>I do standard prep of text docs before learning and inferring stages:
removing stop words, collapsing whitespaces, lowercasing everything,
etc.</p>

<p>Some of my params:</p>

<ul>
<li>numTopics = 30</li>
<li>TermMinimumDocumentCountFilter = (10) ~>  // filter terms which occur in &lt; 10 docs</li>
<li>TermDynamicStopListFilter(30) ~> // filter out 30 most common terms</li>
<li>DocumentMinimumLengthFilter(10) // take only docs with >= 10 terms</li>
<li>topicSmoothing = SymmetricDirichletParams(0.01)</li>
<li>termSmoothing = SymmetricDirichletParams(0.01)</li>
<li>maxIterations = 10</li>
</ul>
",Preprocessing of the text & Tokenization,topic identified stanford topic modeling toolkit similar possible reason topic identified stanford topic modeling toolkit took hour corpus article shared twitter user similar pretty much term frequency essentially single topic topic identified found standard prep text doc learning inferring stage removing stop word collapsing whitespaces lowercasing everything etc params numtopics termminimumdocumentcountfilter filter term occur doc termdynamicstoplistfilter filter common term documentminimumlengthfilter take doc term topicsmoothing symmetricdirichletparams termsmoothing symmetricdirichletparams maxiterations
"Extracting text from webpage, processing with Perl/Python, then rebuilding the page with links added","<p>I'm building a web app that processes the text in a web page, adds links to certain entities, then re-displays the page exactly as it was, but with some links added. My server-side code is in Perl and Python, and I'm currently using HTML::Parser to extract the text from a page. I can clean the markup, extract, and process the text without issue, but I want to display the original page exactly as it was, only with some links added to previously unlinked text. </p>

<p>I'm hoping to find out the best way to redisplay the exact same page with links added to certain words or phrases in the text. All of the original markup should be preserved exactly as it was before the text was extracted. </p>

<p>I've searched thoroughly, but I cannot find a precise solution to this issue. Any help would be greatly appreciated.  </p>
",Preprocessing of the text & Tokenization,extracting text webpage processing perl python rebuilding page link added building web app process text web page add link certain entity display page exactly wa link added server side code perl python currently using html parser extract text page clean markup extract process text without issue want display original page exactly wa link added previously unlinked text hoping find best way redisplay exact page link added certain word phrase text original markup preserved exactly wa text wa extracted searched thoroughly find precise solution issue help would greatly appreciated
Algorithm to match natural text in mail,"<p>I need to separate natural, coherent text/sentences in emails from lists, signatures, greetings and so on  before further processing.</p>

<p>example:</p>

<blockquote>
  <p>Hi tom,</p>
  
  <p><em><strong>last monday we did bla bla, lore Lorem ipsum dolor sit amet, consectetur adipisici elit, sed eiusmod tempor incidunt ut labore et
  dolore magna aliqua.</em></strong></p>
  
  <ul>
  <li>list item 2</li>
  <li>list item 3</li>
  <li>list item 3</li>
  </ul>
  
  <p><em><strong>Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquid x ea commodi consequat. Quis aute iure reprehenderit
  in voluptate velit</em></strong></p>
  
  <p>regards, K.</p>
  
  <p>---line-of-funny-characters-#######</p>
  
  <p>example inc. </p>
  
  <p>33 evil street, london</p>
  
  <p>mobile: 00 234534/234345</p>
</blockquote>

<p>Ideally the algorithm would match only the bold parts. </p>

<p>Is there any recommended approach - or are there even existing algorithms for that problem? Should I try approximate regular expressions or more statistical stuff based on number of punctation marks, length and so on?</p>
",Preprocessing of the text & Tokenization,algorithm match natural text mail need separate natural coherent text sentence email list signature greeting processing example hi tom last monday bla bla lore lorem ipsum dolor sit amet adipisici elit sed eiusmod tempor incidunt ut labore et dolore magna aliqua list item list item list item ut enim ad minim veniam quis nostrud exercitation ullamco laboris nisi ut aliquid x ea commodi consequat quis aute iure reprehenderit voluptate velit regard k line funny character example inc evil street london mobile ideally algorithm would match bold part recommended approach even existing algorithm problem try approximate regular expression statistical stuff based number punctation mark length
How to determine the proper weights for metric scores,"<p>I'm doing some personal research into text analysis, and have come up with close to 70 metrics (pronoun usage frequency, reading levels, vowel frequency, use of bullet points, etc) to ""score"" a piece of text. </p>

<p>Ideally, separate pieces of text from the same author would have similar scores. The ultimate goal is to index a great deal of authors, and use scores to guess at who wrote a separate, anonymous piece of text. </p>

<p>I'd like the scores to normalize from 0 to 100 and represent a percentage of how ""similar"" two pieces of text are in writing style. Questions like <a href=""https://stackoverflow.com/questions/7041867/how-to-decide-on-weights"">How to decide on weights?</a> and <a href=""https://stackoverflow.com/questions/994819/how-to-calculate-scores"">How to calculate scores?</a> describe the math behind scoring metrics and how to normalize, but assume every metric is weighted the same.</p>

<p><strong>My question is this: how do I determine the proper weight to use when scoring each metric, to ensure that the cumulative score per-user most accurately describes the writing from that specific user?</strong></p>

<p>Also, weights can be assigned per-user. If syllables per word most aptly describes who wrote a piece for Alice, while the frequency of two-letter words is the best for Bob, I'd like Alice's heaviest weight to be on syllables per word, and Bob's to be on frequency of two-letter words.</p>
",Preprocessing of the text & Tokenization,determine proper weight metric score personal research text analysis come close metric pronoun usage frequency reading level vowel frequency use bullet point etc score piece text ideally separate piece text author would similar score ultimate goal index great deal author use score guess wrote separate anonymous piece text like score normalize represent percentage similar two piece text writing style question like describe math behind scoring metric normalize assume every metric weighted question determine proper weight use scoring metric ensure cumulative score per user accurately describes writing specific user also weight assigned per user syllable per word aptly describes wrote piece alice frequency two letter word best bob like alice heaviest weight syllable per word bob frequency two letter word
Retrival of character string from email addresses,"<p>I am trying to come up with a way in flex through which I can extract only the character series from a list of email addresses.
The email addresses are valid. Example</p>

<pre><code>Input: a12l3i.ce@example.com
output: alice
</code></pre>

<p>So far, I came with the regular expression <code>([^@]+)</code> through which I can extract the username part of the email. However, without the use of input/unput (which is not allowed in flex) can we retrive the text from the email username ? If yes, can you suggest me a regular expression or any other method for that ? (Perhaps a grammar in Bison)</p>
",Preprocessing of the text & Tokenization,retrival character string email address trying come way flex extract character series list email address email address valid example far came regular expression extract username part email however without use input unput allowed flex retrive text email username yes suggest regular expression method perhaps grammar bison
MySQL routine for porter or porter 2 stemming algorithm,"<p>I'm looking for a port of the Porter or Porter 2 stemming algorithm written as a MySQL routine (user defined function).  Has anybody seen one?</p>

<p>Thanks very much!</p>
",Preprocessing of the text & Tokenization,mysql routine porter porter stemming algorithm looking port porter porter stemming algorithm written mysql routine user defined function ha anybody seen one thanks much
Stop-word elimination and stemmer in python,"<p>I have a somewhat large document and want to do stop-word elimination and stemming on the words of this document with Python. Does anyone know an of the shelf package for these?
If not a code which is fast enough for large documents is also welcome.
Thanks</p>
",Preprocessing of the text & Tokenization,stop word elimination stemmer python somewhat large document want stop word elimination stemming word document python doe anyone know shelf package code fast enough large document also welcome thanks
Efficient Lemmatizer that avoids dictionary lookup,"<p>I want to convert string like 'eat' to 'eating', 'eats'. I searched and found the lemmatization as the solution, but all the lemmatizer tools that I have come across uses wordlist or dictionary-lookup. Is there any lemmatizer which avoids dictionary lookup and gives high efficiency, may be a lemmatizer that is based on rules. Yes and I am not looking for ""stemmer"".</p>
",Preprocessing of the text & Tokenization,efficient lemmatizer avoids dictionary lookup want convert string like eat eating eats searched found lemmatization solution lemmatizer tool come across us wordlist dictionary lookup lemmatizer avoids dictionary lookup give high efficiency may lemmatizer based rule yes looking stemmer
Regular expression to match object dimensions,"<p>I'll put it right out there: I'm terrible with regular expressions. I've tried to come up with one to solve my problem but I really don't know much about them. . .</p>

<p>Imagine some sentences along the following lines:</p>

<blockquote>
  <ul>
  <li>Hello blah blah. It's around 11 1/2"" x 32"".</li>
  <li>The dimensions are 8 x 10-3/5!</li>
  <li>Probably somewhere in the region of 22"" x 17"".</li>
  <li>The roll is quite large: 42 1/2"" x 60 yd.</li>
  <li>They are all 5.76 by 8 frames.</li>
  <li>Yeah, maybe it's around 84cm long.</li>
  <li>I think about 13/19"".</li>
  <li>No, it's probably 86 cm actually.</li>
  </ul>
</blockquote>

<p>I want to, as cleanly as possible, extract item dimensions from within these sentences. In a perfect world the regular expression would output the following:</p>

<blockquote>
  <ul>
  <li>11 1/2"" x 32""</li>
  <li>8 x 10-3/5</li>
  <li>22"" x 17""</li>
  <li>42 1/2"" x 60 yd</li>
  <li>5.76 by 8</li>
  <li>84cm</li>
  <li>13/19""</li>
  <li>86 cm</li>
  </ul>
</blockquote>

<p>I imagine a world where the following rules apply:</p>

<ul>
<li>The following are valid units: <code>{cm, mm, yd, yards, "", ', feet}</code>, though I'd prefer a solution that considers an arbitrary set of units rather than an explicit solution for the above units.</li>
<li>A dimension is always described numerically, may or may not have units following it and may or may not have a fractional or decimal part. Being made up of a fractional part on it's own is allowed, e.g., <code>4/5""</code>.</li>
<li>Fractional parts always have a <code>/</code> separating the numerator / denominator, and one can assume there is no space between the parts (though if someone takes that in to account that's great!).</li>
<li>Dimensions may be one-dimensional or two-dimensional, in which case one can assume the following are acceptable for separating two dimensions: <code>{x, by}</code>. If a dimension is only one-dimensional it <strong>must</strong> have units from the set above, i.e., <code>22 cm</code> is OK, <code>.333</code> is not, nor is <code>4.33 oz</code>.</li>
</ul>

<p>To show you how useless I am with regular expressions (and to show I at least tried!), I got this far. . .</p>

<pre><code>[1-9]+[/ ][x1-9]
</code></pre>

<p><strong>Update (2)</strong></p>

<p>You guys are very fast and efficient! I'm going to add an extra few of test cases that haven't been covered by the regular expressions below:</p>

<blockquote>
  <ul>
  <li>The last but one test case is 12 yd x.</li>
  <li>The last test case is 99 cm by.</li>
  <li>This sentence doesn't have dimensions in it: 342 / 5553 / 222.</li>
  <li>Three dimensions? 22"" x 17"" x 12 cm</li>
  <li>This is a product code: c720 with another number 83 x better.  </li>
  <li>A number on its own 21.</li>
  <li>A volume shouldn't match 0.332 oz.</li>
  </ul>
</blockquote>

<p>These should result in the following (# indicates nothing should match):</p>

<blockquote>
  <ul>
  <li>12 yd</li>
  <li>99 cm</li>
  <li>#</li>
  <li>22"" x 17"" x 12 cm</li>
  <li>#</li>
  <li>#</li>
  <li>#</li>
  </ul>
</blockquote>

<p>I've adapted <a href=""https://stackoverflow.com/a/8434591/646300"">M42's</a> answer below, to:</p>

<pre><code>\d+(?:\.\d+)?[\s-]*(?:\d+)?(?:\/\d+)?(?:cm|mm|yd|""|'|feet)(?:\s*x\s*|\s*by\s*)?(?:\d+(?:\.\d+)?[\s*-]*(?:\d+(?:\/\d+)?)?(?:cm|mm|yd|""|'|feet)?)?
</code></pre>

<p>But while that resolves some new test cases it now fails to match the following others. It reports:</p>

<blockquote>
  <ul>
  <li>11 1/2"" x 32"" PASS</li>
  <li>(nothing) FAIL</li>
  <li>22"" x 17"" PASS</li>
  <li>42 1/2"" x 60 yd PASS</li>
  <li>(nothing) FAIL</li>
  <li>84cm PASS</li>
  <li>13/19"" PASS</li>
  <li>86 cm PASS</li>
  <li>22"" PASS</li>
  <li>(nothing) FAIL</li>
  <li><p>(nothing) FAIL</p></li>
  <li><p>12 yd x FAIL</p></li>
  <li>99 cm by FAIL</li>
  <li>22"" x 17"" [and also, but separately '12 cm'] FAIL</li>
  <li><h1>PASS</h1></li>
  <li><h1>PASS</h1></li>
  </ul>
</blockquote>
",Preprocessing of the text & Tokenization,regular expression match object dimension put right terrible regular expression tried come one solve problem really know much imagine sentence along following line hello blah blah around x dimension x probably somewhere region x roll quite large x yd frame yeah maybe around cm long think probably cm actually want cleanly possible extract item dimension within sentence perfect world regular expression would output following x x x x yd cm cm imagine world following rule apply following valid unit though prefer solution considers arbitrary set unit rather explicit solution unit dimension always described numerically may may unit following may may fractional decimal part made fractional part allowed e g fractional part always separating numerator denominator one assume space part though someone take account great dimension may one dimensional two dimensional case one assume following acceptable separating two dimension dimension one dimensional must unit set e ok show useless regular expression show least tried got far update guy fast efficient going add extra test case covered regular expression last one test case yd x last test case cm sentence dimension three dimension x x cm product code c another number x better number volume match result following indicates nothing match yd cm x x cm adapted nothing fail x pas x yd pas nothing fail cm pas pas cm pas pas nothing fail nothing fail yd x fail cm fail x also separately cm fail pas pas
translate by replacing words inside existing text,"<p>What are common approaches for translating certain words (or expressions) inside a given text, when the text must be reconstructed (with punctuations and everythin.) ?</p>

<p>The translation comes from a lookup table, and covers words, collocations, and emoticons like L33t, CUL8R, :-), etc.</p>

<p>Simple string search-and-replace is not enough since it can replace part of longer words (cat > dog  ≠>  caterpillar > dogerpillar).</p>

<p>Assume the following input:</p>

<pre><code>s = ""dogbert, started a dilbert dilbertion proces cat-bert :-)""
</code></pre>

<p>after translation, i should receive something like:</p>

<blockquote>
  <p>result = ""<strong>anna</strong>, started a <strong>george</strong> dilbertion <strong>process</strong> cat-bert <strong>smiley</strong>""</p>
</blockquote>

<p>I can't simply tokenize, since i loose <strong>punctuations</strong> and <strong>word positions</strong>. </p>

<p>Regular expressions, works for normal words, but don't catch special expressions like the smiley :-) but it does .</p>

<pre><code>re.sub(r'\bword\b','translation',s) ==&gt; translation
re.sub(r'\b:-\)\b','smiley',s) ==&gt; :-)
</code></pre>

<p>for now i'm using the above mentioned regex, and simple replace for the non-alphanumeric words, but it's far from being bulletproof.</p>

<p>(p.s. i'm using python)</p>
",Preprocessing of the text & Tokenization,translate replacing word inside existing text common approach translating certain word expression inside given text text must reconstructed punctuation everythin translation come lookup table cover word collocation emoticon like l cul r etc simple string search replace enough since replace part longer word cat dog caterpillar dogerpillar assume following input translation receive something like result anna started george dilbertion process cat bert smiley simply tokenize since loose punctuation word position regular expression work normal word catch special expression like smiley doe using mentioned regex simple replace non alphanumeric word far bulletproof p using python
Could you recommend a NLP toolkit in Prolog?,"<p>I need to parse or tokenize English sentences.
Is there any NLP toolkit in Prolog?
Thanks.</p>
",Preprocessing of the text & Tokenization,could recommend nlp toolkit prolog need parse tokenize english sentence nlp toolkit prolog thanks
Regular expression for PennTreeBank tags in Java,"<p>Please consider some examples of PennTreeBank Tags:</p>

<pre><code>ADJP -ADV ADVP -BNF CC CD -CLF -CLR -HLN PRP$ PR-P$ NP
</code></pre>

<p>Please consider an instance of my program execution. </p>

<pre><code>Enter your regex: ^-{0,1}[A-Z]{1,6}-{0,1}[A-Z]{0,1}\${0,1}
Enter input string to search: -HLN
I found the text ""-HLN"" starting at index 0 and ending at index 4.
</code></pre>

<p>It works fine.</p>

<p>My task actually is to successfully identify any tag (please refer to tag examples above) except the ""NP"" tag. I wrote the regex as below.</p>

<pre><code>Enter your regex: (^-{0,1}[A-Z]{1,6}-{0,1}[A-Z]{0,1}\${0,1})&amp;&amp;^(NP)
Enter input string to search: -HLN
No match found.
</code></pre>

<p>It is not the desired outcome.</p>

<p>Could someone help me modify the regex to suit the task?</p>

<p>Thank you.</p>
",Preprocessing of the text & Tokenization,regular expression penntreebank tag java please consider example penntreebank tag please consider instance program execution work fine task actually successfully identify tag please refer tag example except np tag wrote regex desired outcome could someone help modify regex suit task thank
where to download multi language word list from Wiktionary?,"<p>I was wondering if there was a place to download multi-language word lists from Wiktionary? </p>
",Preprocessing of the text & Tokenization,download multi language word list wiktionary wa wondering wa place download multi language word list wiktionary
Most efficient way to generate a list of Unigrams from a text field in MongoDB,"<p>I need to generate a vector of unigrams, i.e. a vector of all the unique words which appear in a specific text field that I have stored as part of a broader JSON object in MongoDB.</p>

<p>I'm not really sure what's the easiest and most efficient way to generate this vector. I was thinking of writing a simple Java app which could handle the tokenization (using something like OpenNLP), however I think that a better approach may be to try to tackle this using Mongo's Map-Reduce feature... However I'm not really sure how I could go about this. </p>

<p>Another option would be to use Apache Lucene indexing, but it would mean I'd still need to export this data in one by one. Which is really the same issue I would have with the custom Java or Ruby approach... </p>

<p>Map reduce sounds good however the Mongo data is growing by the day as more document are inserted. This isn't really a one off task as there are new documents being added all the time. Updates are very rare. I really don't want to run a Map-Reduce over the millions of documents every time I want to update my Unigram vector as I fear this will be very inefficient use of resources... </p>

<p>What would be the most efficient way to generate the unigram vector and then keep it updated?</p>

<p>Thanks!</p>
",Preprocessing of the text & Tokenization,efficient way generate list unigrams text field mongodb need generate vector unigrams e vector unique word appear specific text field stored part broader json object mongodb really sure easiest efficient way generate vector wa thinking writing simple java app could handle tokenization using something like opennlp however think better approach may try tackle using mongo map reduce feature however really sure could go another option would use apache lucene indexing would mean still need export data one one really issue would custom java ruby approach map reduce sound good however mongo data growing day document inserted really one task new document added time update rare really want run map reduce million document every time want update unigram vector fear inefficient use resource would efficient way generate unigram vector keep updated thanks
What language can be recommended for text mining/parsing?,"<p>I'm doing some text mining in web pages. Currently I'm working with Java, but maybe there is more appropriate languages to do what I want.</p>

<p>Example of some things I want to do:</p>

<p>Determine the char type of a word based on it parts (letter, digit, symbols, etc.) as Alphabetic, Number, Alphanumeric, Symbol, etc.(there is more types).</p>

<p>Discover stop words based on statistics.</p>

<p>Discover some gramatical class (verb, noun, preposition, conjuntion) based on statistics and some logics.</p>

<p>I was thinking about using Prolog and R (I don't know much about these languages), but I don't know if they are good for this or maybe, another language more appropriate.</p>

<p>Which can I use? Good libs for Java are welcome too.</p>
",Preprocessing of the text & Tokenization,language recommended text mining parsing text mining web page currently working java maybe appropriate language want example thing want determine char type word based part letter digit symbol etc alphabetic number alphanumeric symbol etc type discover stop word based statistic discover gramatical class verb noun preposition conjuntion based statistic logic wa thinking using prolog r know much language know good maybe another language appropriate use good libs java welcome
How to create a bag of words using Weka?,"<p>I have a corpus of documents and I want to represent each document as a vector. Basically, the vector would have 1 for words that are present inside a document and for other words (which are present in other documents in the corpus and not in this particular document) it would have a 0. How do I create this vector for all the documents in Weka?</p>

<p>Is there a quick way to do this using Weka? I also want Weka to remove stopwords and so some pre-processing if possible before it creates this vector.</p>

<p>Thanks
Abhishek S</p>
",Preprocessing of the text & Tokenization,create bag word using weka corpus document want represent document vector basically vector would word present inside document word present document corpus particular document would create vector document weka quick way using weka also want weka remove stopwords pre processing possible creates vector thanks abhishek
ANTLR for Writing JAPE Grammar,"<p>I am using GATE to process texts written in natural language. I have to extract height, weight, bp etc from the text and store it in structured form. Now, these things(i.e height, weight etc) can be written in many forms which is unknown to me. Writing JAPE grammar for all the different ways I can come up with, is merely hard-coding it. Do I have any other option to process the docs in a more flexible way?</p>

<p>Also, I am confused as to whether I can use ANTLR to generate jape grammar by presenting different ways of writing height, weight etc? </p>

<p>The text will contain the following:</p>

<pre><code>Vitals: Height: 72 inches, Weight is 170 pounds, T is 89.9 degree Fahrenheit. OR
Vitals: He is 184 cm tall, his weight was 67.8 Kg, RR 16 (its respiration rate). OR
Vitals: height is 6ft 3 in, he weighs 70 kg, pulse is 67, Temperature 99.8 degrees.
</code></pre>

<p>Now I use JAPE grammar and ANNIE to tokenize the text to extract these vitals and store in structured form. Will using ANTLR be flexible to tokenize this such text? Flexible in the sense that I should not hard-code it for each way of representation, as you see height, weight etc can be represented in many forms.</p>

<p>Will that be a good idea to search for or develop any grammar generator that would generate jape grammars?</p>

<p>If you require any detail to better understand the problem pls let me know.</p>

<p>Thanks a lot!!</p>
",Preprocessing of the text & Tokenization,antlr writing jape grammar using gate process text written natural language extract height weight bp etc text store structured form thing e height weight etc written many form unknown writing jape grammar different way come merely hard coding option process doc flexible way also confused whether use antlr generate jape grammar presenting different way writing height weight etc text contain following use jape grammar annie tokenize text extract vitals store structured form using antlr flexible tokenize text flexible sense hard code way representation see height weight etc represented many form good idea search develop grammar generator would generate jape grammar require detail better understand problem pls let know thanks lot
The best IR software for my use?,"<p>I want to take what people chat about in a chat room and do the following information retrieval:</p>

<ol>
<li>Get the keywords</li>
<li>Ignore all noise words, keep verb an nouns mainly</li>
<li>Perform stemming on the keywords so that I don't store the same keyword in many forms</li>
<li>If a synonym keyword is already stored in my storage then the existing synonym should be used instead of the new keyword</li>
<li>Store the processed keyword in a persistant storage with a reference to the chat message it was located in and the user who uttered it</li>
</ol>

<p>With this prosessed information I want to slowly get an idea of what people are talking about in chatrooms, and then use this to automatically find related chatrooms etc. based on these keywords.</p>

<p>My question to you is a follows: What is the best C/C++ or .NET tools for doing the above?</p>
",Preprocessing of the text & Tokenization,best ir software use want take people chat chat room following information retrieval get keywords ignore noise word keep verb noun mainly perform stemming keywords store keyword many form synonym keyword already stored storage existing synonym used instead new keyword store processed keyword persistant storage reference chat message wa located user uttered prosessed information want slowly get idea people talking chatroom use automatically find related chatroom etc based keywords question follows best c c net tool
Lucene Porter Stemmer Thread Safe?,"<p>Quick question, is the porter stemmer from Lucene packages (Java) thread safe?</p>

<p>I'm guessing the answer is no as you need to set the current string, invoke stem method then get the current block to get the stemmed word. But perhaps I'm missing something - Is there are thread safe method to do stemming of a single word or string from Lucene?</p>

<p>Does anyone from experience know if it is faster to instantiate one Porter Stemmer instance and then use a synchronized block over that stemmer instance and do the <code>setCurrent(""...""); stem(); get();</code> routine or is it just faster to create a new porter stemmer instance for each string/document you want to process. </p>

<p>In this instance I have many 1000s of documents which are each taken up by a pool of threads (i.e. 1 thread has one document). </p>

<p>Edit FYI - Example usage pattern:</p>

<pre><code>import org.tartarus.snowball.ext.PorterStemmer;
...
private String stem(String word){
       PorterStemmer stem = new PorterStemmer();
       stem.setCurrent(word);
       stem.stem();
       return stem.getCurrent();
    }
</code></pre>

<p>Cheers!</p>
",Preprocessing of the text & Tokenization,lucene porter stemmer thread safe quick question porter stemmer lucene package java thread safe guessing answer need set current string invoke stem method get current block get stemmed word perhaps missing something thread safe method stemming single word string lucene doe anyone experience know faster instantiate one porter stemmer instance use synchronized block stemmer instance routine faster create new porter stemmer instance string document want process instance many document taken pool thread e thread ha one document edit fyi example usage pattern cheer
Suggestions for obtaining Google search results and cleaning HTML tags,"<p>I am working on a project to get Google search web pages and then clean HTML tags to obtain pure text content.</p>

<p>Any suggestion for available tools (esp. Python tools)</p>

<p>many thanks.</p>
",Preprocessing of the text & Tokenization,suggestion obtaining google search result cleaning html tag working project get google search web page clean html tag obtain pure text content suggestion available tool esp python tool many thanks
Building/Running a Streaming Weka Text Classifer in Java,"<p>We have been using the Weka Explorer GUI to build a few classifier models. Now Testing is complete we would like to implement this model within a Java application so it can take new messages.</p>

<p>So for new messages we need to tokenize the message, match up tokens in the message with tokens used to build the word vector for the model and then parse this word vector to the model. </p>

<p>How should we go about this process? Are there any examples available? </p>

<p>How do we deal with new tokens (i.e. words that appear in new text messages which are not a part of the word vector used to build the model)?</p>

<p>For the classifier preprocessing/tokenising we are using the NGram Tokenizer, Stemmer and IDF Transform. So we need to figure out how to do these steps before we can create a new instancebased on the text we would like to classify. </p>

<p>As a side When building a classifier in the explorer, under more options there is a button to choose 'output classifier code' which sounds like it outputs Java source code to build and use the model however this option is disabled. Tested with a number of different classifiers (RF, NB) and it doesnt change. I'm guessing its not implemented for these?</p>

<p>Cheers!</p>
",Preprocessing of the text & Tokenization,building running streaming weka text classifer java using weka explorer gui build classifier model testing complete would like implement model within java application take new message new message need tokenize message match token message token used build word vector model parse word vector model go process example available deal new token e word appear new text message part word vector used build model classifier preprocessing tokenising using ngram tokenizer stemmer idf transform need figure step create new instancebased text would like classify side building classifier explorer option button choose output classifier code sound like output java source code build use model however option disabled tested number different classifier rf nb doesnt change guessing implemented cheer
Extracting keywords from an article,"<p>I have articles and keywords stored inside MySQL. The site will preprocess the new articles to find how many matching keywords there are and then update a table which stores the relevant keywords related to the article. This will then be used on the front-end by highlighting keywords within the article and will link users to articles with the same matching keywords.</p>

<p>My concern here is how to do this processing efficiently. My idea is: when processing new articles, it finds the ngrams of the text (up to 3- or 4-gram) and then search each against the keywords table in the MySQL database. This may end up being a slow mess, I haven't tried. But maybe I'm approaching this the wrong way?</p>

<p>Any resources on how to do this efficiently would be awesome. Language used here is primarily PHP.</p>
",Preprocessing of the text & Tokenization,extracting keywords article article keywords stored inside mysql site preprocess new article find many matching keywords update table store relevant keywords related article used front end highlighting keywords within article link user article matching keywords concern processing efficiently idea processing new article find ngrams text gram search keywords table mysql database may end slow mess tried maybe approaching wrong way resource efficiently would awesome language used primarily php
Is string.GetHashCode() sufficent for word dictionaries in nlp?,"<p>can I use the <code>string.GetHashCode()</code> function for storing stemming data reliably?</p>
",Preprocessing of the text & Tokenization,string gethashcode sufficent word dictionary nlp use function storing stemming data reliably
"Algorithms or libraries for textual analysis, specifically: dominant words, phrases across text, and collection of text","<p>I'm working on a project where I need to analyze a page of text and collections of pages of text to determine dominant words.   I'd like to know if there is a library (prefer c# or java) that will handle the heavy lifting for me.  If not, is there an algorithm or multiple that would achieve my goals below.  </p>

<p>What I want to do is similar to word clouds built from a url or rss feed that you find on the web, except I don't want the visualization.  They are used all the time for analyzing the presidential candidate speeches to see what the theme or most used words are.  </p>

<p>The complication, is that I need to do this on thousands of short documents, and then collections or categories of these documents.  </p>

<p>My initial plan was to parse the document out, then filter common words - of, the, he, she, etc..  Then count the number of times the remaining words show up in the text (and overall collection/category).  </p>

<p>The problem is that in the future, I would like to handle stemming, plural forms, etc..   I would also like to see if there is a way to identify important phrases. (Instead of a count of a word, the count of a phrase being 2-3 words together)</p>

<p>Any guidance on a strategy, libraries or algorithms that would help are appreciated.  </p>
",Preprocessing of the text & Tokenization,algorithm library textual analysis specifically dominant word phrase across text collection text working project need analyze page text collection page text determine dominant word like know library prefer c java handle heavy lifting algorithm multiple would achieve goal want similar word cloud built url r feed find web except want visualization used time analyzing candidate speech see theme used word complication need thousand short document collection category document initial plan wa parse document filter common word etc count number time remaining word show text overall collection category problem future would like handle stemming plural form etc would also like see way identify important phrase instead count word count phrase word together guidance strategy library algorithm would help appreciated
python data mining,"<p>I am not too much onto data mining but I require some ideas on clustering. Let me first describe my problem.</p>

<p>I have a around 100 data sheets which contain user reviews. I am trying to find for instances words that describe quality. One can say it is amazing quality another person can say great quality now I have to cluster those documents which describe those similar sentences and get the frequency of such sentences. What concept to apply here?</p>

<p>Guess I have to specify some stop words and synonyms. I am not too familiar with this concept.</p>

<p>Can some one give me some detailed links or explanation? and what tool to be used? I am basically a python programmer so any python module would be appreciated.</p>

<p>Thank You</p>
",Preprocessing of the text & Tokenization,python data mining much onto data mining require idea clustering let first describe problem around data sheet contain user review trying find instance word describe quality one say amazing quality another person say great quality cluster document describe similar sentence get frequency sentence concept apply guess specify stop word synonym familiar concept one give detailed link explanation tool used basically python programmer python module would appreciated thank
Split string into sentences using regular expression,"<p>I need to match a string like ""one. two.    three. four. five.  six. seven. eight. nine. ten. eleven"" into groups of four sentences.  I need a regular expression to break the string into a group after every fourth period. Something like: </p>

<pre><code>  string regex = @""(.*.\s){4}"";

  System.Text.RegularExpressions.Regex exp = new System.Text.RegularExpressions.Regex(regex);

  string result = exp.Replace(toTest, "".\n"");
</code></pre>

<p>doesn't work because it will replace the text before the periods, not just the periods themselves.  How can I count just the periods and replace them with a period and new line character?</p>
",Preprocessing of the text & Tokenization,split string sentence using regular expression need match string like one two three four five six seven eight nine ten eleven group four sentence need regular expression break string group every fourth period something like work replace text period period count period replace period new line character
NLP programming tools using PHP?,"<p>Since big web applications came into existence, searching for data (and doing it lightning fast and accurate) has been one of the most important problems in web applications. For a while, I've worked using <a href=""http://incubator.apache.org/lucene.net/"">Lucene.NET</a>, which is a C# port of the <a href=""http://lucene.apache.org/java/"">Lucene project</a>. </p>

<p>I also work using PHP using <a href=""http://framework.zend.com/manual/en/zend.search.lucene.html"">Zend Framework's Lucene API</a>, which brings me to my question. Most times for providing good indexing we need to perform some NLP tools like <strong>tokenizing</strong>, <strong>lemmatizing</strong>, and many more, the question is:</p>

<p>Do you know of any good NLP programming framework/toolset using PHP?</p>

<p>PS: I'm very aware of the Zend API for Lucene, but indexing data properly is not just storing and relying in Lucene, you need to perform some extra tasks, like those above.</p>
",Preprocessing of the text & Tokenization,nlp programming tool using php since big web application came existence searching data lightning fast accurate ha one important problem web application worked using also work using php using href framework lucene api brings question time providing good indexing need perform nlp tool like tokenizing lemmatizing many question know good nlp programming framework toolset using php p aware zend api lucene indexing data properly storing relying lucene need perform extra task like
put sentences into list - python,"<p>I understand that nltk can split sentences and print it out using the following code.
but how do i put the sentences into a list instead of outputing onto the screen?</p>

<pre><code>import nltk.data
from nltk.tokenize import sent_tokenize
import os, sys, re, glob
cwd = './extract_en' #os.getcwd()
for infile in glob.glob(os.path.join(cwd, 'fileX.txt')):
    (PATH, FILENAME) = os.path.split(infile)
    read = open(infile)
    for line in read:
        sent_tokenize(line)
</code></pre>

<p>the sent_tokenize(line) prints it out. how do i put it into a list?</p>
",Preprocessing of the text & Tokenization,put sentence list python understand nltk split sentence print using following code put sentence list instead outputing onto screen sent tokenize line print put list
another porter stemming algorithm implementation question?,"<p>I am trying to implement porter stemming algorithm, but i am having difficualties understanding this point </p>

<blockquote>
  <p>Step 1c</p>

<pre><code>(*v*) Y -&gt; I                    happy        -&gt;  happi
                                sky          -&gt;  sky
</code></pre>
</blockquote>

<p>Isn't that the the opposite of what we want to do , why does the algorithim convert the Y into I.</p>

<p>for the complete algorithm here <a href=""http://tartarus.org/~martin/PorterStemmer/def.txt"" rel=""nofollow"">http://tartarus.org/~martin/PorterStemmer/def.txt</a></p>

<p>Thanks</p>
",Preprocessing of the text & Tokenization,another porter stemming algorithm implementation question trying implement porter stemming algorithm difficualties understanding point step c opposite want doe algorithim convert complete algorithm thanks
Storing tokenized text in the db?,"<p>I have a simple question. I'm doing some light crawling so new content arrives every few days. I've written a tokenizer and would like to use it for some text mining purposes. Specifically, I'm using Mallet's topic modeling tool and one of the pipe is to tokenize the text into tokens before further processing can be done. With the amount of text in my database, it takes a substantial amount of time tokenizing the text (I'm using regex here). </p>

<p>As such, is it a norm to store the tokenized text in the db so that tokenized data can be readily available and tokenizing can be skipped if I need them for other text mining purposes such as Topic modeling, POS tagging? What are the cons of this approach? </p>
",Preprocessing of the text & Tokenization,storing tokenized text db simple question light crawling new content arrives every day written tokenizer would like use text mining purpose specifically using mallet topic modeling tool one pipe tokenize text token processing done amount text database take substantial amount time tokenizing text using regex norm store tokenized text db tokenized data readily available tokenizing skipped need text mining purpose topic modeling po tagging con approach
Find the words in a long stream of characters. Auto-tokenize,"<p>How would you find the correct words in a long stream of characters?</p>

<p>Input :</p>

<pre><code>""The revised report onthesyntactictheoriesofsequentialcontrolandstate""
</code></pre>

<p>Google's Output: </p>

<pre><code>""The revised report on syntactic theories sequential controlandstate""
</code></pre>

<p>(which is close enough considering the time that they produced the output)</p>

<p>How do you think Google does it? 
How would you increase the accuracy? </p>
",Preprocessing of the text & Tokenization,find word long stream character auto tokenize would find correct word long stream character input google output close enough considering time produced output think google doe would increase accuracy
What is the default chunker for NLTK toolkit in Python?,"<p>I am using their default POS tagging and default tokenization..and it seems sufficient.  I'd like their default chunker too.</p>

<p>I am reading the NLTK toolkit book, but it does not seem like they have a default chunker?</p>
",Preprocessing of the text & Tokenization,default chunker nltk toolkit python using default po tagging default tokenization seems sufficient like default chunker reading nltk toolkit book doe seem like default chunker
Regular expression for counting sentences in a block of text,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/2158296/php-how-to-split-a-paragraph-into-sentences"">PHP - How to split a paragraph into sentences.</a>  </p>
</blockquote>



<p>I have a block of text that I would like to separate into sentences, what would be the best way of doing this? I thought of looking for '.','!','?' characters, but I realized there were some problems with this, such as when people use acronyms, or end a sentence with something like !?.  What would be the best way to handle this? I figured there would be some regex that could handle this, but I'm open to a non-regex solution if that fits the problem better.</p>
",Preprocessing of the text & Tokenization,regular expression counting sentence block text possible duplicate href split paragraph sentence block text would like separate sentence would best way thought looking character realized problem people use acronym end sentence something like would best way handle figured would regex could handle open non regex solution fit problem better
Algorithm for sentence analysis and tokenization,"<p>I need to analyze a document and compile statistics as to how many times each a sequence of words is used (so the analysis is not on single words but of batch of recurring words).  I read that compression algorithms do something similar to what I want - creating dictionaries of blocks of text with a piece of information reporting its frequency.
It should be something similar to <a href=""http://www.codeproject.com/KB/recipes/Patterns.aspx"" rel=""nofollow noreferrer"">http://www.codeproject.com/KB/recipes/Patterns.aspx</a>
Do you have anything written in C#?</p>
",Preprocessing of the text & Tokenization,algorithm sentence analysis tokenization need analyze document compile statistic many time sequence word used analysis single word batch recurring word read compression algorithm something similar want creating dictionary block text piece information reporting frequency something similar anything written c
"Given a document, select a relevant snippet","<p>When I ask a question here, the tool tips for the question returned by the auto search given the first little bit of the question, but a decent percentage of them don't give any text that is any more useful for understanding the question than the title. Does anyone have an idea about how to make a filter to trim out useless bits of a question?</p>

<p>My first idea is to trim any leading sentences that contain only words in some list (for instance, stop words, plus words from the title, plus words from the SO corpus that have very weak correlation with tags, that is that are equally likely to occur in any question regardless of it's tags)</p>
",Preprocessing of the text & Tokenization,given document select relevant snippet ask question tool tip question returned auto search given first little bit question decent percentage give text useful understanding question title doe anyone idea make filter trim useless bit question first idea trim leading sentence contain word list instance stop word plus word title plus word corpus weak correlation tag equally likely occur question regardless tag
TDD and the Bayesian Spam Filter problem,"<p>It's well known that Bayesian classifiers are an effective way to filter spam. These can be fairly concise (our one is only a few hundred LoC) but all core code needs to be written up-front before you get any results at all.</p>

<p>However, the TDD approach mandates that only the minimum amount of code to pass a test can be written, so given the following method signature:</p>

<pre><code>bool IsSpam(string text)
</code></pre>

<p>And the following string of text, which is clearly spam:</p>

<pre><code>""Cheap generic viagra""
</code></pre>

<p>The minimum amount of code I could write is:</p>

<pre><code>bool IsSpam(string text)
{
    return text == ""Cheap generic viagra""
}
</code></pre>

<p>Now maybe I add another test message, e.g.</p>

<pre><code>""Online viagra pharmacy""
</code></pre>

<p>I could change the code to:</p>

<pre><code>bool IsSpam(string text)
{
    return text.Contains(""viagra"");
}
</code></pre>

<p>...and so on, and so on. Until at some point the code becomes a mess of string checks, regular expressions, etc. because we've <em>evolved</em> it instead of thinking about it and writing it in a different way from the start.</p>

<p>So how is TDD supposed to work with this type of situation where evolving the code from the simplest possible code to pass the test is not the right approach? (Particularly if it is known in advance that the best implementations cannot be trivially evolved).</p>
",Preprocessing of the text & Tokenization,tdd bayesian spam filter problem well known bayesian classifier effective way filter spam fairly concise one hundred loc core code need written front get result however tdd approach mandate minimum amount code pas test written given following method signature following string text clearly spam minimum amount code could write maybe add another test message e g could change code point code becomes mess string check regular expression etc evolved instead thinking writing different way start tdd supposed work type situation evolving code simplest possible code pas test right approach particularly known advance best implementation trivially evolved
Non regular context-free language and infinite regular sublanguages,"<p>I had a work for the university which basically said:  </p>

<p>""Demonstrates that the non-regular language L={0^n 1^n : n natural} had no infinite regular sublanguages.""</p>

<p>I demonstrated this by contradiction.  I basically said that there is a language S which is a sublanguage of L and it is a regular language.  Since the possible Regular expressions for S are 0*, 1*, (1+0)* and (0o1)*.  I check each grammar and demonstrate that none of them are part of the language L.  </p>

<p>However, how I could prove that ANY non regular context free language could not contain any regular infinite sublanguages?</p>

<p>I don't want the prove per se, I just want to be pointed in the right direction.</p>
",Preprocessing of the text & Tokenization,non regular context free language infinite regular sublanguages work university basically said demonstrates non regular language l n n n natural infinite regular sublanguages demonstrated contradiction basically said language sublanguage l regular language since possible regular expression check grammar demonstrate none part language l however could prove non regular context free language could contain regular infinite sublanguages want prove per se want pointed right direction
Stemming - code examples or open source projects?,"<p>Stemming is something that's needed in tagging systems.  I use delicious, and I don't have time to manage and prune my tags.  I'm a bit more careful with my blog, but it isn't perfect.  I write software for embedded systems that would be much more functional (helpful to the user) if they included stemming.</p>

<p>For instance:<br>
Parse<br>
Parser<br>
Parsing  </p>

<p>Should all mean the same thing to whatever system I'm putting them into.</p>

<p>Ideally there's a BSD licensed stemmer somewhere, but if not, where do I look to learn the common algorithms and techniques for this?</p>

<p>Aside from BSD stemmers, what other open source licensed stemmers are out there?</p>

<p>-Adam</p>
",Preprocessing of the text & Tokenization,stemming code example open source project stemming something needed tagging system use delicious time manage prune tag bit careful blog perfect write software embedded system would much functional helpful user included stemming instance parse parser parsing mean thing whatever system putting ideally bsd licensed stemmer somewhere look learn common algorithm technique aside bsd stemmer open source licensed stemmer adam
How does Google&#39;s In Quotes work?,"<p>I find Google's <a href=""http://labs.google.com/inquotes/"" rel=""nofollow noreferrer"">In Quotes</a> a really nifty application, and as a CS guy, I have to understand how it works. How do you think it turns news articles into a list of quotes attributed to specific persons?
Sure, there are some mistakes, but their algorithm seems to be smarter than just a simple heuristic or multiple regular expressions. For example, a quote can be attributed to someone even though his/her name was only mentioned in the last paragraph.</p>

<p>Any ideas? Any known paper on the subject?</p>
",Preprocessing of the text & Tokenization,doe google quote work find google quote really nifty application c guy understand work think turn news article list quote attributed specific person sure mistake algorithm seems smarter simple heuristic multiple regular expression example quote attributed someone even though name wa mentioned last paragraph idea known paper subject
End user tool for generating a regular expression,"<p>We have a SaaS application requirement to allow a user responsible for building a CMS site to define up to 10 custom fields in a form. 
As part of this field definition we want to add a field validation option which we will store (and apply at runtime) as a reg-ex.</p>

<p>Are there any tools, code samples or similar that offer a wizard style front end for building a reg-ex. We are looking to embed a control or code into our .NET site that will generate the reg-ex from (pseudo) user friendly terms (close to natural language if possible).</p>

<p>e.g.
Field 1 = (5 alphanumerics) followed-by (1 to 3 numerics) followed by ""-"" followed by 1 alpha</p>
",Preprocessing of the text & Tokenization,end user tool generating regular expression saas application requirement allow user responsible building cm site define custom field form part field definition want add field validation option store apply runtime reg ex tool code sample similar offer wizard style front end building reg ex looking embed control code net site generate reg ex pseudo user friendly term close natural language possible e g field alphanumerics followed numerics followed followed alpha
